{"id": "4015", "revid": "42867806", "url": "https://en.wikipedia.org/wiki?curid=4015", "title": "BASIC", "text": "BASIC (Beginners' All-purpose Symbolic Instruction Code) is a family of general-purpose, high-level programming languages designed for ease of use. The original version was created by John G. Kemeny and Thomas E. Kurtz at Dartmouth College in 1963. They wanted to enable students in non-scientific fields to use computers. At the time, nearly all computers required writing custom software, which only scientists and mathematicians tended to learn.\nIn addition to the programming language, Kemeny and Kurtz developed the Dartmouth Time-Sharing System (DTSS), which allowed multiple users to edit and run BASIC programs simultaneously on remote terminals. This general model became popular on minicomputer systems like the PDP-11 and Data General Nova in the late 1960s and early 1970s. Hewlett-Packard produced an entire computer line for this method of operation, introducing the HP2000 series in the late 1960s and continuing sales into the 1980s. Many early video games trace their history to one of these versions of BASIC.\nThe emergence of microcomputers in the mid-1970s led to the development of multiple BASIC dialects, including Microsoft BASIC in 1975. Due to the tiny main memory available on these machines, often 4\u00a0KB, a variety of Tiny BASIC dialects were also created. BASIC was available for almost any system of the era, and became the \"de facto\" programming language for home computer systems that emerged in the late 1970s. These PCs almost always had a BASIC interpreter installed by default, often in the machine's firmware or sometimes on a ROM cartridge.\nBASIC declined in popularity in the 1990s, as more powerful microcomputers came to market and programming languages with advanced features (such as Pascal and C) became tenable on such computers. By then, most nontechnical personal computer users relied on pre-written applications rather than writing their own programs. In 1991, Microsoft released Visual Basic, combining an updated version of BASIC with a visual forms builder. This reignited use of the language and \"VB\" remains a major programming language in the form of VB.NET, while a hobbyist scene for BASIC more broadly continues to exist.\nOrigin.\nJohn G. Kemeny was the chairman of the Dartmouth College Mathematics Department. Based largely on his reputation as an innovator in math teaching, in 1959 the college won an Alfred P. Sloan Foundation award for $500,000 to build a new department building. Thomas E. Kurtz had joined the department in 1956, and from the 1960s Kemeny and Kurtz agreed on the need for programming literacy among students outside the traditional STEM fields. Kemeny later noted that \"Our vision was that every student on campus should have access to a computer, and any faculty member should be able to use a computer in the classroom whenever appropriate. It was as simple as that.\"\nKemeny and Kurtz had made two previous experiments with simplified languages, DARSIMCO (Dartmouth Simplified Code) and DOPE (Dartmouth Oversimplified Programming Experiment). These did not progress past a single freshman class. New experiments using Fortran and ALGOL followed, but Kurtz concluded these languages were too tricky for what they desired. As Kurtz noted, Fortran had numerous oddly formed commands, notably an \"almost impossible-to-memorize convention for specifying a loop: . Is it '1, 10, 2' or '1, 2, 10', and is the comma after the line number required or not?\"\nMoreover, the lack of any sort of immediate feedback was a key problem; the machines of the era used batch processing and took a long time to complete a run of a program. While Kurtz was visiting MIT, John McCarthy suggested that time-sharing offered a solution; a single machine could divide up its processing time among many users, giving them the illusion of having a (slow) computer to themselves. Small programs would return results in a few seconds. This led to increasing interest in a system using time-sharing and a new language specifically for use by non-STEM students.\nKemeny wrote the first version of BASIC. The acronym \"BASIC\" comes from the name of an unpublished paper by Thomas Kurtz. The new language was heavily patterned on FORTRAN II; statements were one-to-a-line, numbers were used to indicate the target of loops and branches, and many of the commands were similar or identical to Fortran. However, the syntax was changed wherever it could be improved. For instance, the difficult to remember codice_1 loop was replaced by the much easier to remember , and the line number used in the DO was instead indicated by the codice_2. Likewise, the cryptic codice_3 statement of Fortran, whose syntax matched a particular instruction of the machine on which it was originally written, became the simpler . These changes made the language much less idiosyncratic while still having an overall structure and feel similar to the original FORTRAN.\nThe project received a $300,000 grant from the National Science Foundation, which was used to purchase a GE-225 computer for processing, and a Datanet-30 realtime processor to handle the Teletype Model 33 teleprinters used for input and output. A team of a dozen undergraduates worked on the project for about a year, writing both the DTSS system and the BASIC compiler. The first version BASIC language was released on 1 May 1964.\nInitially, BASIC concentrated on supporting straightforward mathematical work, with matrix arithmetic support from its initial implementation as a batch language, and character string functionality being added by 1965. Usage in the university rapidly expanded, requiring the main CPU to be replaced by a GE-235, and still later by a GE-635. By the early 1970s there were hundreds of terminals connected to the machines at Dartmouth, some of them remotely.\nWanting use of the language to become widespread, its designers made the compiler available free of charge. In the 1960s, software became a chargeable commodity; until then, it was provided without charge as a service with expensive computers, usually available only to lease. They also made it available to high schools in the Hanover, New Hampshire, area and regionally throughout New England on Teletype Model 33 and Model 35 teleprinter terminals connected to Dartmouth via dial-up phone lines, and they put considerable effort into promoting the language. In the following years, as other dialects of BASIC appeared, Kemeny and Kurtz's original BASIC dialect became known as \"Dartmouth BASIC\".\nNew Hampshire recognized the accomplishment in 2019 when it erected a highway historical marker in Hanover describing the creation of \"the first user-friendly programming language\".\nSpread on time-sharing services.\nThe emergence of BASIC took place as part of a wider movement toward time-sharing systems. First conceptualized during the late 1950s, the idea became so dominant in the computer industry by the early 1960s that its proponents were speaking of a future in which users would \"buy time on the computer much the same way that the average household buys power and water from utility companies\".\nGeneral Electric, having worked on the Dartmouth project, wrote their own underlying operating system and launched an online time-sharing system known as Mark I. It featured BASIC as one of its primary selling points. Other companies in the emerging field quickly followed suit; Tymshare introduced SUPER BASIC in 1968, CompuServe had a version on the DEC-10 at their launch in 1969, and by the early 1970s BASIC was largely universal on general-purpose mainframe computers. Even IBM eventually joined the club with the introduction of VS-BASIC in 1973.\nAlthough time-sharing services with BASIC were successful for a time, the widespread success predicted earlier was not to be. The emergence of minicomputers during the same period, and especially low-cost microcomputers in the mid-1970s, allowed anyone to purchase and run their own systems rather than buy online time which was typically billed at dollars per minute.\nSpread on minicomputers.\nBASIC, by its very nature of being small, was naturally suited to porting to the minicomputer market, which was emerging at the same time as the time-sharing services. These machines had small main memory, perhaps as little as 4\u00a0KB in modern terminology, and lacked high-performance storage like hard drives that make compilers practical. On these systems, BASIC was normally implemented as an interpreter rather than a compiler due to its lower requirement for working memory.\nA particularly important example was HP Time-Shared BASIC, which, like the original Dartmouth system, used two computers working together to implement a time-sharing system. The first, a low-end machine in the HP 2100 series, was used to control user input and save and load their programs to tape or disk. The other, a high-end version of the same underlying machine, ran the programs and generated output. For a cost of about $100,000, one could own a machine capable of running between 16 and 32 users at the same time. The system, bundled as the HP 2000, was the first mini platform to offer time-sharing and was an immediate runaway success, catapulting HP to become the third-largest vendor in the minicomputer space, behind DEC and Data General (DG).\nDEC, the leader in the minicomputer space since the mid-1960s, had initially ignored BASIC. This was due to their work with RAND Corporation, who had purchased a PDP-6 to run their JOSS language, which was conceptually very similar to BASIC. This led DEC to introduce a smaller, cleaned up version of JOSS known as FOCAL, which they heavily promoted in the late 1960s. However, with timesharing systems widely offering BASIC, and all of their competition in the minicomputer space doing the same, DEC's customers were clamoring for BASIC. After management repeatedly ignored their pleas, David H. Ahl took it upon himself to buy a BASIC for the PDP-8, which was a major success in the education market. By the early 1970s, FOCAL and JOSS had been forgotten and BASIC had become almost universal in the minicomputer market. DEC would go on to introduce their updated version, BASIC-PLUS, for use on the RSTS/E time-sharing operating system.\nDuring this period a number of simple text-based games were written in BASIC, most notably Mike Mayfield's \"Star Trek\". David Ahl collected these, some ported from FOCAL, and published them in an educational newsletter he compiled. He later collected a number of these into book form, \"101 BASIC Computer Games\", published in 1973. During the same period, Ahl was involved in the creation of a small computer for education use, an early personal computer. When management refused to support the concept, Ahl left DEC in 1974 to found the seminal computer magazine, \"Creative Computing\". The book remained popular, and was re-published on several occasions.\nExplosive growth: the home computer era.\nThe introduction of the first microcomputers in the mid-1970s was the start of explosive growth for BASIC. It had the advantage that it was fairly well known to the young designers and computer hobbyists who took an interest in microcomputers, many of whom had seen BASIC on minis or mainframes. Despite Dijkstra's famous judgement in 1975, \"It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration\", BASIC was one of the few languages that was both high-level enough to be usable by those without training and small enough to fit into the microcomputers of the day, making it the \"de facto\" standard programming language on early microcomputers.\nThe first microcomputer version of BASIC was co-written by Bill Gates, Paul Allen and Monte Davidoff for their newly formed company, Micro-Soft. This was released by MITS in punch tape format for the Altair 8800 shortly after the machine itself, immediately cementing BASIC as the primary language of early microcomputers. Members of the Homebrew Computer Club began circulating copies of the program, causing Gates to write his Open Letter to Hobbyists, complaining about this early example of software piracy.\nPartially in response to Gates's letter, and partially to make an even smaller BASIC that would run usefully on 4\u00a0KB machines, Bob Albrecht urged Dennis Allison to write their own variation of the language. How to design and implement a stripped-down version of an interpreter for the BASIC language was covered in articles by Allison in the first three quarterly issues of the \"People's Computer Company\" newsletter published in 1975 and implementations with source code published in \"\". This led to a wide variety of Tiny BASICs with added features or other improvements, with versions from Tom Pittman and Li-Chen Wang becoming particularly well known.\nMicro-Soft, by this time Microsoft, ported their interpreter for the MOS 6502, which quickly become one of the most popular microprocessors of the 8-bit era. When new microcomputers began to appear, notably the \"1977 trinity\" of the TRS-80, Commodore PET and Apple II, they either included a version of the MS code, or quickly introduced new models with it. Ohio Scientific's personal computers also joined this trend at that time. By 1978, MS BASIC was a \"de facto\" standard and practically every home computer of the 1980s included it in ROM. Upon boot, a BASIC interpreter in direct mode was presented.\nCommodore Business Machines includes Commodore BASIC, based on Microsoft BASIC. The Apple II and TRS-80 each have two versions of BASIC: a smaller introductory version with the initial releases of the machines and a Microsoft-based version introduced as interest in the platforms increased. As new companies entered the field, additional versions were added that subtly changed the BASIC family. The Atari 8-bit computers use the 8\u00a0KB Atari BASIC which is not derived from Microsoft BASIC. Sinclair BASIC was introduced in 1980 with the Sinclair ZX80, and was later extended for the Sinclair ZX81 and the Sinclair ZX Spectrum. The BBC published BBC BASIC, developed by Acorn Computers, incorporates extra structured programming keywords and floating-point features.\nAs the popularity of BASIC grew in this period, computer magazines published complete source code in BASIC for video games, utilities, and other programs. Given BASIC's straightforward nature, it was a simple matter to type in the code from the magazine and execute the program. Different magazines were published featuring programs for specific computers, though some BASIC programs were considered universal and could be used in machines running any variant of BASIC (sometimes with minor adaptations). Many books of type-in programs were also available, and in particular, Ahl published versions of the original 101 BASIC games converted into the Microsoft dialect and published it from \"Creative Computing\" as \"BASIC Computer Games\". This book, and its sequels, provided hundreds of ready-to-go programs that could be easily converted to practically any BASIC-running platform. The book reached the stores in 1978, just as the home computer market was starting off, and it became the first million-selling computer book. Later packages, such as Learn to Program BASIC would also have gaming as an introductory focus. On the business-focused CP/M computers which soon became widespread in small business environments, Microsoft BASIC (MBASIC) was one of the leading applications.\nIn 1978, David Lien published the first edition of \"The BASIC Handbook: An Encyclopedia of the BASIC Computer Language\", documenting keywords across over 78 different computers. By 1981, the second edition documented keywords from over 250 different computers, showcasing the explosive growth of the microcomputer era.\nIBM PC and compatibles.\nWhen IBM was designing the IBM PC, they followed the paradigm of existing home computers in having a built-in BASIC interpreter. They sourced this from Microsoft \u2013 IBM Cassette BASIC \u2013 but Microsoft also produced several other versions of BASIC for MS-DOS/PC DOS including IBM Disk BASIC (BASIC D), IBM BASICA (BASIC A), GW-BASIC (a BASICA-compatible version that did not need IBM's ROM) and QBasic, all typically bundled with the machine. In addition they produced the Microsoft QuickBASIC Compiler (1985) for power users and hobbyists, and the Microsoft BASIC Professional Development System (PDS) for professional programmers. Turbo Pascal-publisher Borland published Turbo Basic 1.0 in 1985 (successor versions were marketed under the name PowerBASIC). \nOn Unix-like systems, specialized implementations were created such as XBasic and X11-Basic. XBasic was ported to Microsoft Windows as XBLite, and cross-platform variants such as SmallBasic, yabasic, Bywater BASIC, nuBasic, MyBasic, Logic Basic, Liberty BASIC, and wxBasic emerged. FutureBASIC and Chipmunk Basic meanwhile targeted the Apple Macintosh, while yab is a version of yaBasic optimized for BeOS, ZETA and Haiku.\nThese later variations introduced many extensions, such as improved string manipulation and graphics support, access to the file system and additional data types. More important were the facilities for structured programming, including additional control structures and proper subroutines supporting local variables. The addition of an integrated development environment (IDE) and electronic Help files made the products easier to work with and supported learning tools and school curriculum.\nIn 1989, Microsoft Press published \"Learn BASIC Now\", a book-and-software system designed to teach BASIC programming to self-taught learners who were using IBM-PC compatible systems and the Apple Macintosh. \"Learn BASIC Now\" included software disks containing the Microsoft QuickBASIC Interpreter and a programming tutorial written by Michael Halvorson and David Rygmyr. Learning systems like \"Learn BASIC Now\" popularized structured BASIC and helped QuickBASIC reach an installed base of four million active users.\nBy the late 1980s, many users were using pre-made applications written by others rather than learning programming themselves, and professional developers had a wide range of advanced languages available on small computers. C and later C++ became the languages of choice for professional \"shrink wrap\" application development.\nA niche that BASIC continued to fill was for hobbyist video game development, as game creation systems and readily available game engines were still in their infancy. The Atari ST had STOS BASIC while the Amiga had AMOS BASIC for this purpose. Microsoft first exhibited BASIC for game development with DONKEY.BAS for GW-BASIC, and later GORILLA.BAS and NIBBLES.BAS for QuickBASIC. QBasic maintained an active game development community, which helped later spawn the QB64 and FreeBASIC implementations. An early example of this market is the QBasic software package Microsoft Game Shop (1990), a hobbyist-inspired release that included six \"arcade-style\" games that were easily customizable in QBasic.\nIn 2013, a game written in QBasic and compiled with QB64 for modern computers entitled \"Black Annex\" was released on Steam. Blitz Basic, Dark Basic, SdlBasic, Super Game System Basic, PlayBASIC, CoolBasic, AllegroBASIC, ethosBASIC, GLBasic and Basic4GL further filled this demand, right up to the modern RCBasic, NaaLaa, AppGameKit, Monkey 2, and Cerberus-X.\nVisual Basic.\nIn 1991, Microsoft introduced Visual Basic, an evolutionary development of QuickBASIC. It included constructs from that language such as block-structured control statements, parameterized subroutines and optional static typing as well as object-oriented constructs from other languages such as \"With\" and \"For Each\". The language retained some compatibility with its predecessors, such as the Dim keyword for declarations, \"Gosub\"/Return statements and optional line numbers which could be used to locate errors. An important driver for the development of Visual Basic was as the new macro language for Microsoft Excel, a spreadsheet program. To the surprise of many at Microsoft who still initially marketed it as a language for hobbyists, the language came into widespread use for small custom business applications shortly after the release of VB version 3.0, which is widely considered the first relatively stable version. Microsoft also spun it off as Visual Basic for Applications and Embedded Visual Basic.\nWhile many advanced programmers still scoffed at its use, VB met the needs of small businesses efficiently as by that time, computers running Windows 3.1 had become fast enough that many business-related processes could be completed \"in the blink of an eye\" even using a \"slow\" language, as long as large amounts of data were not involved. Many small business owners found they could create their own small, yet useful applications in a few evenings to meet their own specialized needs. Eventually, during the lengthy lifetime of VB3, knowledge of Visual Basic had become a marketable job skill. Microsoft also produced VBScript in 1996 and Visual Basic .NET in 2001. The latter has essentially the same power as C# and Java but with syntax that reflects the original Basic language, and also features some cross-platform capability through implementations such as Mono-Basic. The IDE, with its event-driven GUI builder, was also influential on other rapid application development tools, most notably Borland Software's Delphi for Object Pascal and its own descendants such as Lazarus.\nMainstream support for the final version 6.0 of the original Visual Basic ended on March 31, 2005, followed by extended support in March 2008. Owing to its persistent remaining popularity, third-party attempts to further support it exist. On February 2, 2017, Microsoft announced that development on VB.NET would no longer be in parallel with that of C#, and on March 11, 2020, it was announced that evolution of the VB.NET language had also concluded. Even so, the language was still supported.\nPost-1990 versions and dialects.\nMany other BASIC dialects have also sprung up since 1990, including the open source QB64 and FreeBASIC, inspired by QBasic, and the Visual Basic-styled RapidQ, HBasic, Basic For Qt and Gambas. Modern commercial incarnations include PureBasic, PowerBASIC, Xojo, Monkey X and True BASIC (the direct successor to Dartmouth BASIC from a company controlled by Kurtz).\nSeveral web-based simple BASIC interpreters also now exist, including Microsoft's Small Basic and Google's wwwBASIC. A number of compilers also exist that convert BASIC into JavaScript. such as NS Basic.\nBuilding from earlier efforts such as Mobile Basic, many dialects are now available for smartphones and tablets.\nOn game consoles, an application for the Nintendo 3DS and Nintendo DSi called \"Petit Computer\" allows for programming in a slightly modified version of BASIC with DS button support. A version has also been released for Nintendo Switch, which has also been supplied a version of the Fuze Code System, a BASIC variant first implemented as a custom Raspberry Pi machine. Previously BASIC was made available on consoles as Family BASIC (for the Nintendo Famicom) and PSX Chipmunk Basic (for the original PlayStation), while yabasic was ported to the PlayStation 2 and FreeBASIC to the original Xbox.\nCalculators.\nVariants of BASIC are available on graphing and otherwise programmable calculators made by Texas Instruments (TI-BASIC), HP (HP BASIC), Casio (Casio BASIC), and others.\nWindows command-line.\nQBasic, a version of Microsoft QuickBASIC without the linker to make EXE files, is present in the Windows NT and DOS-Windows 95 streams of operating systems and can be obtained for more recent releases like Windows 7 which do not have them. Prior to DOS 5, the Basic interpreter was GW-Basic. QuickBasic is part of a series of three languages issued by Microsoft for the home and office power user and small-scale professional development; QuickC and QuickPascal are the other two. For Windows 95 and 98, which do not have QBasic installed by default, they can be copied from the installation disc, which will have a set of directories for old and optional software; other missing commands like Exe2Bin and others are in these same directories.\nOther.\nThe various Microsoft, Lotus, and Corel office suites and related products are programmable with Visual Basic in one form or another, including LotusScript, which is very similar to VBA 6. The Host Explorer terminal emulator uses WWB as a macro language; or more recently the programme and the suite in which it is contained is programmable in an in-house Basic variant known as Hummingbird Basic. The VBScript variant is used for programming web content, Outlook 97, Internet Explorer, and the Windows Script Host. WSH also has a Visual Basic for Applications (VBA) engine installed as the third of the default engines along with VBScript, JScript, and the numerous proprietary or open source engines which can be installed like PerlScript, a couple of Rexx-based engines, Python, Ruby, Tcl, Delphi, XLNT, PHP, and others; meaning that the two versions of Basic can be used along with the other mentioned languages, as well as LotusScript, in a WSF file, through the component object model, and other WSH and VBA constructions. VBScript is one of the languages that can be accessed by the 4Dos, 4NT, and Take Command enhanced shells. SaxBasic and WWB are also very similar to the Visual Basic line of Basic implementations. The pre-Office 97 macro language for Microsoft Word is known as WordBASIC. Excel 4 and 5 use Visual Basic itself as a macro language. Chipmunk Basic, an old-school interpreter similar to BASICs of the 1970s, is available for Linux, Microsoft Windows and macOS.\nLegacy.\nThe ubiquity of BASIC interpreters on personal computers was such that textbooks once included simple \"Try It In BASIC\" exercises that encouraged students to experiment with mathematical and computational concepts on classroom or home computers. Popular computer magazines of the day typically included type-in programs.\nFuturist and sci-fi writer David Brin mourned the loss of ubiquitous BASIC in a 2006 \"Salon\" article as have others who first used computers during this era. In turn, the article prompted Microsoft to develop and release Small Basic; it also inspired similar projects like Basic-256 and the web based Quite Basic. Dartmouth held a 50th anniversary celebration for BASIC on 1 May 2014. The pedagogical use of BASIC has been followed by other languages, such as Pascal, Java and particularly Python.\nDartmouth College celebrated the 50th anniversary of the BASIC language with a day of events on April 30, 2014. A short documentary film was produced for the event.\nSyntax.\nData types and variables.\nMinimal versions of BASIC had only integer variables and one- or two-letter variable names, which minimized requirements of limited and expensive memory (RAM). More powerful versions had floating-point arithmetic, and variables could be labelled with names six or more characters long. There were some problems and restrictions in early implementations; for example, Applesoft BASIC allowed variable names to be several characters long, but only the first two were significant, thus it was possible to inadvertently write a program with variables \"LOSS\" and \"LOAN\", which would be treated as being the same; assigning a value to \"LOAN\" would silently overwrite the value intended as \"LOSS\". Keywords could not be used in variables in many early BASICs; \"SCORE\" would be interpreted as \"SC\" OR \"E\", where OR was a keyword. String variables are usually distinguished in many microcomputer dialects by having $ suffixed to their name as a sigil, and values are often identified as strings by being delimited by \"double quotation marks\". Arrays in BASIC could contain integers, floating point or string variables.\nSome dialects of BASIC supported matrices and matrix operations, which can be used to solve sets of simultaneous linear algebraic equations. These dialects would directly support matrix operations such as assignment, addition, multiplication (of compatible matrix types), and evaluation of a determinant. Many microcomputer BASICs did not support this data type; matrix operations were still possible, but had to be programmed explicitly on array elements.\nExamples.\nUnstructured BASIC.\nNew BASIC programmers on a home computer might start with a simple program, perhaps using the language's PRINT statement to display a message on the screen; a well-known and often-replicated example is Kernighan and Ritchie's \"Hello, World!\" program:\n10 PRINT \"Hello, World!\"\n20 END\nAn infinite loop could be used to fill the display with the message:\n10 PRINT \"Hello, World!\"\n20 GOTO 10\nNote that the codice_58 statement is optional and has no action in most dialects of BASIC. It was not always included, as is the case in this example. This same program can be modified to print a fixed number of messages using the common codice_59 statement:\n10 LET N=10\n20 FOR I=1 TO N\n30 PRINT \"Hello, World!\"\n40 NEXT I\nMost home computers BASIC versions, such as MSX BASIC and GW-BASIC, supported simple data types, loop cycles, and arrays. The following example is written for GW-BASIC, but will work in most versions of BASIC with minimal changes:\n10 INPUT \"What is your name: \"; U$\n20 PRINT \"Hello \"; U$\n30 INPUT \"How many stars do you want: \"; N\n40 S$ = \"\"\n50 FOR I = 1 TO N\n60 S$ = S$ + \"*\"\n70 NEXT I\n80 PRINT S$\n90 INPUT \"Do you want more stars? \"; A$\n100 IF LEN(A$) = 0 THEN GOTO 90\n110 A$ = LEFT$(A$, 1)\n120 IF A$ = \"Y\" OR A$ = \"y\" THEN GOTO 30\n130 PRINT \"Goodbye \"; U$\n140 END\nThe resulting dialog might resemble:\n What is your name: Mike\n Hello Mike\n How many stars do you want: 7\n Do you want more stars? yes\n How many stars do you want: 3\n Do you want more stars? no\n Goodbye Mike\nThe original Dartmouth Basic was unusual in having a matrix keyword, MAT. Although not implemented by most later microprocessor derivatives, it is used in this example from the 1968 manual which averages the numbers that are input:\n5 LET S = 0\n10 MAT INPUT V \n20 LET N = NUM \n30 IF N = 0 THEN 99 \n40 FOR I = 1 TO N \n45 LET S = S + V(I) \n50 NEXT I \n60 PRINT S/N \n70 GO TO 5 \n99 END\nStructured BASIC.\nSecond-generation BASICs (for example, VAX Basic, SuperBASIC, True BASIC, QuickBASIC, BBC BASIC, Pick BASIC, PowerBASIC, Liberty BASIC, QB64 and (arguably) COMAL) introduced a number of features into the language, primarily related to structured and procedure-oriented programming. Usually, line numbering is omitted from the language and replaced with labels (for GOTO) and procedures to encourage easier and more flexible design. In addition keywords and structures to support repetition, selection and procedures with local variables were introduced.\nThe following example is in Microsoft QuickBASIC:\nREM QuickBASIC example\nREM Forward declaration - allows the main code to call a\nREM subroutine that is defined later in the source code\nDECLARE SUB PrintSomeStars (StarCount!)\nREM Main program follows\nINPUT \"What is your name: \", UserName$\nPRINT \"Hello \"; UserName$\nDO\n INPUT \"How many stars do you want: \", NumStars\n CALL PrintSomeStars(NumStars)\n DO\n INPUT \"Do you want more stars? \", Answer$\n LOOP UNTIL Answer$ &lt;&gt; \"\"\n Answer$ = LEFT$(Answer$, 1)\nLOOP WHILE UCASE$(Answer$) = \"Y\"\nPRINT \"Goodbye \"; UserName$\nEND\nREM subroutine definition\nSUB PrintSomeStars (StarCount)\n REM This procedure uses a local variable called Stars$\n Stars$ = STRING$(StarCount, \"*\")\n PRINT Stars$\nEND SUB\nObject-oriented BASIC.\nThird-generation BASIC dialects such as Visual Basic, Xojo, Gambas, StarOffice Basic, BlitzMax and PureBasic introduced features to support object-oriented and event-driven programming paradigm. Most built-in procedures and functions are now represented as \"methods\" of standard objects rather than \"operators\". Also, the operating system became increasingly accessible to the BASIC language.\nThe following example is in Visual Basic .NET:\nPublic Module StarsProgram\n Private Function Ask(prompt As String) As String\n Console.Write(prompt)\n Return Console.ReadLine()\n End Function\n Public Sub Main()\n Dim userName = Ask(\"What is your name: \")\n Console.WriteLine(\"Hello {0}\", userName)\n Dim answer As String\n Do\n Dim numStars = CInt(Ask(\"How many stars do you want: \"))\n Dim stars As New String(\"*\"c, numStars)\n Console.WriteLine(stars)\n Do\n answer = Ask(\"Do you want more stars? \")\n Loop Until answer &lt;&gt; \"\"\n Loop While answer.StartsWith(\"Y\", StringComparison.OrdinalIgnoreCase)\n Console.WriteLine(\"Goodbye {0}\", userName)\n End Sub\nEnd Module"}
{"id": "4016", "revid": "38596647", "url": "https://en.wikipedia.org/wiki?curid=4016", "title": "List of Byzantine emperors", "text": "The foundation of Constantinople in 330 AD marks the conventional start of the Eastern Roman Empire, which fell to the Ottoman Empire in 1453 AD. Only the emperors who were recognized as legitimate rulers and exercised sovereign authority are included, to the exclusion of junior co-emperors (\"symbasileis\") who never attained the status of sole or senior ruler, as well as of the various usurpers or rebels who claimed the imperial title.\nThe following list starts with Constantine the Great, the first Christian emperor, who rebuilt the city of Byzantium as an imperial capital, Constantinople, and who was regarded by the later emperors as the model ruler. Modern historians distinguish this later phase of the Roman Empire as Byzantine due to the imperial seat moving from Rome to Byzantium, the Empire's integration of Christianity, and the predominance of Greek instead of Latin.\nThe Byzantine Empire was the direct legal continuation of the eastern half of the Roman Empire following the division of the Roman Empire in 395. Emperors listed below up to Theodosius I in 395 were sole or joint rulers of the entire Roman Empire. The Western Roman Empire continued until 476. Byzantine emperors considered themselves to be Roman emperors in direct succession from Augustus; the term \"Byzantine\" became convention in Western historiography in the 19th century. The use of the title \"Roman Emperor\" by those ruling from Constantinople was not contested until after the papal coronation of the Frankish Charlemagne as Holy Roman emperor (25 December 800).\nThe title of all Emperors preceding Heraclius was officially \"\"Augustus\", although other titles such as \"Dominus\" were also used. Their names were preceded by \"Imperator Caesar\" and followed by \"Augustus\". Following Heraclius, the title commonly became the Greek \"Basileus\" (Gr. \u0392\u03b1\u03c3\u03b9\u03bb\u03b5\u03cd\u03c2), which had formerly meant sovereign, though \"Augustus\" continued to be used in a reduced capacity. Following the establishment of the rival Holy Roman Empire in Western Europe, the title \"Autokrator\"\" (Gr. \u0391\u1f50\u03c4\u03bf\u03ba\u03c1\u03ac\u03c4\u03c9\u03c1) was increasingly used. In later centuries, the Emperor could be referred to by Western Christians as the \"Emperor of the Greeks\". Towards the end of the Empire, the standard imperial formula of the Byzantine ruler was \"[Emperor's name] in Christ, Emperor and Autocrat of the Romans\" (cf. \u1fec\u03c9\u03bc\u03b1\u1fd6\u03bf\u03b9 and R\u00fbm). \nDynasties were a common tradition and structure for rulers and government systems in the Medieval period. The principle or formal requirement for hereditary succession was not a part of the Empire's governance; hereditary succession was a custom and tradition, carried on as habit and benefited from some sense of legitimacy, but not as a \"rule\" or inviolable requirement for office at the time."}
{"id": "4021", "revid": "4440889", "url": "https://en.wikipedia.org/wiki?curid=4021", "title": "B.C.E.", "text": ""}
{"id": "4022", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=4022", "title": "B.C.E", "text": ""}
{"id": "4024", "revid": "14410800", "url": "https://en.wikipedia.org/wiki?curid=4024", "title": "Butterfly effect", "text": "In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state.\nThe term is closely associated with the work of the mathematician and meteorologist Edward Norton Lorenz. He noted that the butterfly effect is derived from the example of the details of a tornado (the exact time of formation, the exact path taken) being influenced by minor perturbations such as a distant butterfly flapping its wings several weeks earlier. Lorenz originally used a seagull causing a storm but was persuaded to make it more poetic with the use of a butterfly and tornado by 1972. He discovered the effect when he observed runs of his weather model with initial condition data that were rounded in a seemingly inconsequential manner. He noted that the weather model would fail to reproduce the results of runs with the unrounded initial condition data. A very small change in initial conditions had created a significantly different outcome.\nThe idea that small causes may have large effects in weather was earlier acknowledged by the French mathematician and physicist Henri Poincar\u00e9. The American mathematician and philosopher Norbert Wiener also contributed to this theory. Lorenz's work placed the concept of \"instability\" of the Earth's atmosphere onto a quantitative base and linked the concept of instability to the properties of large classes of dynamic systems which are undergoing nonlinear dynamics and deterministic chaos.\nThe concept of the butterfly effect has since been used outside the context of weather science as a broad term for any situation where a small change is supposed to be the cause of larger consequences.\nHistory.\nIn \"The Vocation of Man\" (1800), Johann Gottlieb Fichte says \"you could not remove a single grain of sand from its place without thereby ... changing something throughout all parts of the immeasurable whole\".\nChaos theory and the sensitive dependence on initial conditions were described in numerous forms of literature. This is evidenced by the case of the three-body problem by Poincar\u00e9 in 1890. He later proposed that such phenomena could be common, for example, in meteorology.\nIn 1898, Jacques Hadamard noted general divergence of trajectories in spaces of negative curvature. Pierre Duhem discussed the possible general significance of this in 1908.\nIn 1950, Alan Turing noted: \"The displacement of a single electron by a billionth of a centimetre at one moment might make the difference between a man being killed by an avalanche a year later, or escaping.\"\nThe idea that the death of one butterfly could eventually have a far-reaching ripple effect on subsequent historical events made its earliest known appearance in \"A Sound of Thunder\", a 1952 short story by Ray Bradbury. \"A Sound of Thunder\" features time travel.\nMore precisely, though, almost the exact idea and the exact phrasing \u2014of a tiny insect's wing affecting the entire atmosphere's winds\u2014 was published in a children's book which became extremely successful and well-known globally in 1962, the year before Lorenz published:\nIn 1961, Lorenz was running a numerical computer model to redo a weather prediction from the middle of the previous run as a shortcut. He entered the initial condition 0.506 from the printout instead of entering the full precision 0.506127 value. The result was a completely different weather scenario.\nLorenz wrote:\nIn 1963, Lorenz published a theoretical study of this effect in a highly cited, seminal paper called \"Deterministic Nonperiodic Flow\" (the calculations were performed on a Royal McBee LGP-30 computer). Elsewhere he stated:\nFollowing proposals from colleagues, in later speeches and papers, Lorenz used the more poetic butterfly. According to Lorenz, when he failed to provide a title for a talk he was to present at the 139th meeting of the American Association for the Advancement of Science in 1972, Philip Merilees concocted \"Does the flap of a butterfly's wings in Brazil set off a tornado in Texas?\" as a title. Although a butterfly flapping its wings has remained constant in the expression of this concept, the location of the butterfly, the consequences, and the location of the consequences have varied widely.\nThe phrase refers to the effect of a butterfly's wings creating tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay, accelerate, or even prevent the occurrence of a tornado in another location. The butterfly does not power or directly create the tornado, but the term is intended to imply that the flap of the butterfly's wings can \"cause\" the tornado: in the sense that the flap of the wings is a part of the initial conditions of an interconnected complex web; one set of conditions leads to a tornado, while the other set of conditions doesn't. The flapping wing creates a small change in the initial condition of the system, which cascades to large-scale alterations of events (compare: domino effect). Had the butterfly not flapped its wings, the trajectory of the system might have been vastly different\u2014but it's also equally possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado.\nThe butterfly effect presents an obvious challenge to prediction, since initial conditions for a system such as the weather can never be known to complete accuracy. This problem motivated the development of ensemble forecasting, in which a number of forecasts are made from perturbed initial conditions.\nSome scientists have since argued that the weather system is not as sensitive to initial conditions as previously believed. David Orrell argues that the major contributor to weather forecast error is model error, with sensitivity to initial conditions playing a relatively small role. Stephen Wolfram also notes that the Lorenz equations are highly simplified and do not contain terms that represent viscous effects; he believes that these terms would tend to damp out small perturbations. Recent studies using generalized Lorenz models that included additional dissipative terms and nonlinearity suggested that a larger heating parameter is required for the onset of chaos.\nWhile the \"butterfly effect\" is often explained as being synonymous with sensitive dependence on initial conditions of the kind described by Lorenz in his 1963 paper (and previously observed by Poincar\u00e9), the butterfly metaphor was originally applied to work he published in 1969 which took the idea a step further. Lorenz proposed a mathematical model for how tiny motions in the atmosphere scale up to affect larger systems. He found that the systems in that model could only be predicted up to a specific point in the future, and beyond that, reducing the error in the initial conditions would not increase the predictability (as long as the error is not zero). This demonstrated that a deterministic system could be \"observationally indistinguishable\" from a non-deterministic one in terms of predictability. Recent re-examinations of this paper suggest that it offered a significant challenge to the idea that our universe is deterministic, comparable to the challenges offered by quantum physics.\nIn the book entitled \"The Essence of Chaos\" published in 1993, Lorenz defined butterfly effect as: \"The phenomenon that a small alteration in the state of a dynamical system will cause subsequent states to differ greatly from the states that would have followed without the alteration.\" This feature is the same as sensitive dependence of solutions on initial conditions (SDIC) in . In the same book, Lorenz applied the activity of skiing and developed an idealized skiing model for revealing the sensitivity of time-varying paths to initial positions. A predictability horizon is determined before the onset of SDIC.\nTheory and mathematical definition.\nRecurrence, the approximate return of a system toward its initial conditions, together with sensitive dependence on initial conditions, are the two main ingredients for chaotic motion. They have the practical consequence of making complex systems, such as the weather, difficult to predict past a certain time range (approximately a week in the case of weather) since it is impossible to measure the starting atmospheric conditions completely accurately.\nA dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate. The definition is not topological, but essentially metrical. Lorenz defined sensitive dependence as follows:\n\"The property characterizing an orbit (i.e., a solution) if most other orbits that pass close to it at some point do not remain close to it as time advances.\"\nIf \"M\" is the state space for the map formula_1, then formula_1 displays sensitive dependence to initial conditions if for any x in \"M\" and any \u03b4\u00a0&gt;\u00a00, there are y in \"M\", with distance such that formula_3 and such that\nfor some positive parameter \"a\". The definition does not require that all points from a neighborhood separate from the base point \"x\", but it requires one positive Lyapunov exponent. In addition to a positive Lyapunov exponent, boundedness is another major feature within chaotic systems.\nThe simplest mathematical framework exhibiting sensitive dependence on initial conditions is provided by a particular parametrization of the logistic map:\nwhich, unlike most chaotic maps, has a closed-form solution:\nwhere the initial condition parameter formula_7 is given by formula_8. For rational formula_7, after a finite number of iterations formula_10 maps into a periodic sequence. But almost all formula_7 are irrational, and, for irrational formula_7, formula_10 never repeats itself \u2013 it is non-periodic. This solution equation clearly demonstrates the two key features of chaos \u2013 stretching and folding: the factor 2\"n\" shows the exponential growth of stretching, which results in sensitive dependence on initial conditions (the butterfly effect), while the squared sine function keeps formula_10 folded within the range\u00a0[0,\u00a01].\nIn physical systems.\nIn weather.\nOverview.\nThe butterfly effect is most familiar in terms of weather; it can easily be demonstrated in standard weather prediction models, for example. The climate scientists James Annan and William Connolley explain that chaos is important in the development of weather prediction methods; models are sensitive to initial conditions. They add the caveat: \"Of course the existence of an unknown butterfly flapping its wings has no direct bearing on weather forecasts, since it will take far too long for such a small perturbation to grow to a significant size, and we have many more immediate uncertainties to worry about. So the direct impact of this phenomenon on weather prediction is often somewhat wrong.\"\nDifferentiating types of butterfly effects.\nThe concept of the butterfly effect encompasses several phenomena. The two kinds of butterfly effects, including the sensitive dependence on initial conditions, and the ability of a tiny perturbation to create an organized circulation at large distances, are not exactly the same. In Palmer et al., a new type of butterfly effect is introduced, highlighting the potential impact of small-scale processes on finite predictability within the Lorenz 1969 model. Additionally, the identification of ill-conditioned aspects of the Lorenz 1969 model points to a practical form of finite predictability. These two distinct mechanisms suggesting finite predictability in the Lorenz 1969 model are collectively referred to as the third kind of butterfly effect. The authors in have considered Palmer et al.'s suggestions and have aimed to present their perspective without raising specific contentions.\nThe third kind of butterfly effect with finite predictability, as discussed in, was primarily proposed based on a convergent geometric series, known as Lorenz's and Lilly's formulas. Ongoing discussions are addressing the validity of these two formulas for estimating predictability limits in.\nA comparison of the two kinds of butterfly effects and the third kind of butterfly effect has been documented. In recent studies, it was reported that both meteorological and non-meteorological linear models have shown that instability plays a role in producing a butterfly effect, which is characterized by brief but significant exponential growth resulting from a small disturbance.\nRecent debates on butterfly effects.\nThe first kind of butterfly effect (BE1), known as SDIC (Sensitive Dependence on Initial Conditions), is widely recognized and demonstrated through idealized chaotic models. However, opinions differ regarding the second kind of butterfly effect, specifically the impact of a butterfly flapping its wings on tornado formation, as indicated in two 2024 articles. In more recent discussions published by \"Physics Today\", it is acknowledged that the second kind of butterfly effect (BE2) has never been rigorously verified using a realistic weather model. While the studies suggest that BE2 is unlikely in the real atmosphere, its invalidity in this context does not negate the applicability of BE1 in other areas, such as pandemics or historical events.\nFor the third kind of butterfly effect, the limited predictability within the Lorenz 1969 model is explained by scale interactions in one article and by system ill-conditioning in another more recent study.\nFinite predictability in chaotic systems.\nAccording to Lighthill (1986), the presence of SDIC (commonly known as the butterfly effect) implies that chaotic systems have a finite predictability limit. In a literature review, it was found that Lorenz's perspective on the predictability limit can be condensed into the following statement:\nRecently, a short video has been created to present Lorenz's perspective on predictability limit.\nA recent study refers to the two-week predictability limit, initially calculated in the 1960s with the Mintz-Arakawa model's five-day doubling time, as the \"Predictability Limit Hypothesis.\" Inspired by Moore's Law, this term acknowledges the collaborative contributions of Lorenz, Mintz, and Arakawa under Charney's leadership. The hypothesis supports the investigation into extended-range predictions using both partial differential equation (PDE)-based physics methods and Artificial Intelligence (AI) techniques.\nRevised perspectives on chaotic and non-chaotic systems.\nBy revealing coexisting chaotic and non-chaotic attractors within Lorenz models, Shen and his colleagues proposed a revised view that \"weather possesses chaos and order\", in contrast to the conventional view of \"weather is chaotic\". As a result, sensitive dependence on initial conditions (SDIC) does not always appear. Namely, SDIC appears when two orbits (i.e., solutions) become the chaotic attractor; it does not appear when two orbits move toward the same point attractor. The above animation for double pendulum motion provides an analogy. For large angles of swing the motion of the pendulum is often chaotic. By comparison, for small angles of swing, motions are non-chaotic.\nMultistability is defined when a system (e.g., the double pendulum system) contains more than one bounded attractor that depends only on initial conditions. The multistability was illustrated using kayaking in Figure on the right side (i.e., Figure 1 of ) where the appearance of strong currents and a stagnant area suggests instability and local stability, respectively. As a result, when two kayaks move along strong currents, their paths display SDIC. On the other hand, when two kayaks move into a stagnant area, they become trapped, showing no typical SDIC (although a chaotic transient may occur). Such features of SDIC or no SDIC suggest two types of solutions and illustrate the nature of multistability.\nBy taking into consideration time-varying multistability that is associated with the modulation of large-scale processes (e.g., seasonal forcing) and aggregated feedback of small-scale processes (e.g., convection), the above revised view is refined as follows:\n\"The atmosphere possesses chaos and order; it includes, as examples, emerging organized systems (such as tornadoes) and time varying forcing from recurrent seasons.\"\nIn quantum mechanics.\nThe potential for sensitive dependence on initial conditions (the butterfly effect) has been studied in a number of cases in semiclassical and quantum physics, including atoms in strong fields and the anisotropic Kepler problem. Some authors have argued that extreme (exponential) dependence on initial conditions is not expected in pure quantum treatments; however, the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by Martin Gutzwiller and John B. Delos and co-workers. The random matrix theory and simulations with quantum computers prove that some versions of the butterfly effect in quantum mechanics do not exist.\nOther authors suggest that the butterfly effect can be observed in quantum systems. Zbyszek P. Karkuszewski et al. consider the time evolution of quantum systems which have slightly different Hamiltonians. They investigate the level of sensitivity of quantum systems to small changes in their given Hamiltonians. David Poulin et al. presented a quantum algorithm to measure fidelity decay, which \"measures the rate at which identical initial states diverge when subjected to slightly different dynamics\". They consider fidelity decay to be \"the closest quantum analog to the (purely classical) butterfly effect\". Whereas the classical butterfly effect considers the effect of a small change in the position and/or velocity of an object in a given Hamiltonian system, the quantum butterfly effect considers the effect of a small change in the Hamiltonian system with a given initial position and velocity. This quantum butterfly effect has been demonstrated experimentally. Quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos.\nIn popular culture.\nThe butterfly effect has appeared across mediums such as literature (for instance, \"A Sound of Thunder\"), films and television (such as \"The Simpsons\"), video games (such as \"Life Is Strange\"), webcomics (such as \"Homestuck\"), AI-driven expansive language models, and more."}
{"id": "4025", "revid": "169380", "url": "https://en.wikipedia.org/wiki?curid=4025", "title": "B.C.", "text": ""}
{"id": "4026", "revid": "378390", "url": "https://en.wikipedia.org/wiki?curid=4026", "title": "Buckminister Fuller", "text": ""}
{"id": "4027", "revid": "82286", "url": "https://en.wikipedia.org/wiki?curid=4027", "title": "Borland", "text": "Borland Software Corporation was a computing technology company founded in 1983 by Niels Jensen, Ole Henriksen, Mogens Glad, and Philippe Kahn. Its main business was developing and selling software development and software deployment products. Borland was first headquartered in Scotts Valley, California, then in Cupertino, California, and then in Austin, Texas. In 2009, the company became a full subsidiary of the British firm Micro Focus International plc. In 2023, Micro Focus (including Borland) was acquired by Canadian firm OpenText, which later absorbed Borland's portfolio into its application delivery management division.\nHistory.\nThe 1980s: Foundations.\nBorland Ltd. was founded in August 1981 by three Danish citizens Niels Jensen, Ole Henriksen, and Mogens Glad to develop products like Word Index for the CP/M operating system using an off-the-shelf company. However, the response to the company's products at the CP/M-82 show in San Francisco showed that a U.S. company would be needed to reach the American market. They met Philippe Kahn, who had just moved to Silicon Valley and had been a key developer of the Micral. Kahn was chairman, president, and CEO of Borland Inc. at its inception in 1983 and until 1995. The company name \"Borland\" was a creation of Kahn's, taking inspiration from the name of an American Astronaut and then-Eastern Air Lines chairperson Frank Borman. \nThe first name for the company was not Borland. It was MIT. The acronym MIT stood for \"Market In Time\". The name \"Borland\" originated from a small company in Ireland, which was one of MIT initial customers. After they went bankrupt, MIT sought permission to acquire and use the name \"Borland\" in the U.S., following a legal recommendation during a rebranding prompted by a letter from MIT (Massachusetts Institute of Technology).\nThe main shareholders at the incorporation of Borland were Niels Jensen (250,000 shares), Ole Henriksen (160,000), Mogens Glad (100,000), and Kahn (80,000).\nBorland International, Inc. era.\nBorland developed various software development tools. Its first product was Turbo Pascal in 1983, developed by Anders Hejlsberg (who later developed .NET and C# for Microsoft) and before Borland acquired the product which was sold in Scandinavia under the name \"Compas Pascal\". In 1984, Borland launched Sidekick, a time organization, notebook, and calculator utility that was an early terminate-and-stay-resident program (TSR) for MS-DOS compatible operating systems.\nBy the mid-1980s, the company had an exhibit at the 1985 West Coast Computer Faire along with IBM and AT&amp;T. Bruce Webster reported that \"the legend of Turbo Pascal has by now reached mythic proportions, as evidenced by the number of firms that, in marketing meetings, make plans to become 'the next Borland'\". After Turbo Pascal and Sidekick, the company launched other applications such as SuperKey and Lightning, all developed in Denmark. While the Danes remained majority shareholders, board members included Kahn, Tim Berry, John Nash, and David Heller. With the assistance of John Nash and David Heller, both British members of the Borland Board, the company was taken public on London's Unlisted Securities Market (USM) in 1986.\nSchroders was the lead investment banker. According to the London IPO filings, the management team was Philippe Kahn as president, Spencer Ozawa as VP of Operations, Marie Bourget as CFO, and Spencer Leyton as VP of sales and business development. All software development continued to take place in Denmark and later London as the Danish co-founders moved there. A first US IPO followed in 1989 after Ben Rosen joined the Borland board with Goldman Sachs as the lead banker and a second offering in 1991 with Lazard as the lead banker.\nIn 1985, Borland acquired Analytica and its Reflex database product. The engineering team of Analytica, managed by Brad Silverberg and including Reflex co-founder Adam Bosworth, became the core of Borland's engineering team in the US. Brad Silverberg was VP of engineering until he left in early 1990 to head up the Personal Systems division at Microsoft. Adam Bosworth initiated and headed up the Quattro project until moving to Microsoft later in 1990 to take over the project which eventually became Access.\nIn 1987, Borland purchased Wizard Systems and incorporated portions of the Wizard C technology into Turbo C. Bob Jervis, the author of Wizard C became a Borland employee. Turbo C was released on May 18, 1987. This drove a wedge between Borland and Niels Jensen and the other members of his team who had been working on a brand-new series of compilers at their London development centre. They reached an agreement and spun off a company named Jensen &amp; Partners International (JPI), later TopSpeed. JPI first launched an MS-DOS compiler named JPI Modula-2, which later became TopSpeed Modula-2, and followed up with TopSpeed C, TopSpeed C++, and TopSpeed Pascal compilers for both the MS-DOS and OS/2 operating systems. The TopSpeed compiler technology still exists as the underlying technology of the Clarion 4GL programming language, a Windows development tool.\nIn September 1987, Borland purchased Ansa-Software, including their Paradox (version 2.0) database management tool. Richard Schwartz, a cofounder of Ansa, became Borland's CTO and Ben Rosen joined the Borland board.\nThe Quattro Pro spreadsheet was launched in 1989. Lotus Development, under the leadership of Jim Manzi, sued Borland for copyright infringement (see Look and feel). The litigation, \"Lotus Dev. Corp. v. Borland Int'l, Inc.\", brought forward Borland's open standards position as opposed to Lotus' closed approach. Borland, under Kahn's leadership, took a position of principle and announced that they would defend against Lotus' legal position and \"fight for programmer's rights\". After a decision in favour of Borland by the First Circuit Court of Appeals, the case went to the United States Supreme Court. Because Justice John Paul Stevens had recused himself, only eight justices heard the case, and concluded in a 4\u20134 tie. As a result, the First Circuit Court decision remained standing but did not bind any other court and set no national precedent.\nAdditionally, Borland's approach towards software piracy and intellectual property (IP) included its \"Borland no-nonsense license agreement\"; allowing the developer/user to utilize its products \"just like a book\". The user was allowed to make multiple copies of a program, as long as it was the only copy in use at any point in time.\nThe 1990s: Rise and change.\nIn September 1991, Borland purchased Ashton-Tate, bringing the dBASE and InterBase databases to the house, in an all-stock transaction. However, competition with Microsoft was fierce. Microsoft launched the competing database Microsoft Access and bought the dBASE clone FoxPro in 1992, undercutting Borland's prices. During the early 1990s, Borland's implementation of C and C++ outsold Microsoft's. Borland survived as a company, but no longer dominated the software tools that it once had. It went through a radical transition in products, financing, and staff, and became a very different company from the one which challenged Microsoft and Lotus in the early 1990s.\nThe internal problems that arose with the Ashton-Tate merger were a large part of the downfall. Ashton-Tate's product portfolio proved to be weak, with no provision for evolution into the GUI environment of Windows. Almost all product lines were discontinued. The consolidation of duplicate support and development offices was costly and disruptive. Worst of all, the highest revenue earner of the combined company was dBASE with no Windows version ready. Borland had an internal project to clone dBASE which was intended to run on Windows and was part of the strategy of the acquisition, but by late 1992 this was abandoned due to technical flaws and the company had to constitute a replacement team (the ObjectVision team, redeployed) headed by Bill Turpin to redo the job.\nBorland lacked the financial strength to project its marketing and move internal resources off other products to shore up the dBASE/W effort. Layoffs occurred in 1993 to keep the company afloat, the third instance of this in five years. By the time dBASE for Windows eventually shipped, the developer community had moved on to other products such as Clipper or FoxBase, and dBASE never regained a significant share of Ashton-Tate's former market. This happened against the backdrop of the rise in Microsoft's combined Office product marketing.\nA change in market conditions also contributed to Borland's fall from prominence. In the 1980s, companies had few people who understood the growing personal computer phenomenon and so most technical people were given free rein to purchase whatever software they thought they needed. Borland had done an excellent job marketing to those with a highly technical bent. By the mid-1990s, however, companies were beginning to ask what the return was on the investment they had made in this loosely controlled PC software buying spree. Company executives were starting to ask questions that were hard for technically minded staff to answer, and so corporate standards began to be created. This required new kinds of marketing and support materials from software vendors, but Borland remained focused on the technical side of its products.\nIn 1993 Borland explored ties with WordPerfect as a possible way to form a suite of programs to rival Microsoft's nascent integration strategy. WordPerfect itself was struggling with a late and troubled transition to Windows. The eventual joint company effort, named Borland Office for Windows (a combination of the WordPerfect word processor, Quattro Pro spreadsheet, and Paradox database) was introduced at the 1993 Comdex computer show. Borland Office never made significant inroads against Microsoft Office. WordPerfect was then bought by Novell. In October 1994, Borland sold Quattro Pro and rights to sell up to a million copies of Paradox to Novell for $140 million in cash, repositioning the company on its core software development tools and the Interbase database engine and shifting toward client-server scenarios in corporate applications. This later proved a good foundation for the shift to web development tools.\nPhilippe Kahn and the Borland board disagreed on how to focus the company, and Kahn resigned as chairman, CEO and president, after 12 years, in January 1995. Kahn remained on the board until November 7, 1996. Borland named Gary Wetsel as CEO, but he resigned in July 1996. William F. Miller was interim CEO until September of that year, when Whitney G. Lynn (the current chairman at mergers &amp; acquisitions company XRP Healthcare) became interim president and CEO (along with other executive changes), followed by a succession of CEOs including Dale Fuller and Tod Nielsen.\nThe Delphi 1 rapid application development (RAD) environment was launched in 1995, under the leadership of Anders Hejlsberg.\nIn 1996 Borland acquired Open Environment Corporation, a Cambridge-based company founded by John J. Donovan.\nOn November 25, 1996, Del Yocam was hired as Borland CEO and chairman.\nIn 1997, Borland sold Paradox to Corel, but retained all development rights for the core BDE. In November 1997, Borland acquired Visigenic, a middleware company that was focused on implementations of CORBA.\nInprise Corporation era.\nIn April 1998, Borland International, Inc. announced it had become Inprise Corporation.\nFor several years, before and during the Inprise name, Borland suffered from serious financial losses and poor public image. When the name was changed to Inprise, many thought Borland had gone out of business. In March 1999, dBASE was sold to KSoft, Inc. which was soon renamed dBASE Inc. (In 2004 dBASE Inc. was renamed to DataBased Intelligence, Inc.).\nIn 1999, Dale L. Fuller replaced Yocam. At this time Fuller's title was \"interim president and CEO\". The \"interim\" was dropped in December 2000. Keith Gottfried served in senior executive positions with the company from 2000 to 2004.\nA proposed merger between Inprise and Corel was announced in February 2000, aimed at producing Linux-based products. The plan was abandoned when Corel's shares fell and it became clear that there was no strategic fit.\nInterBase 6.0 was made available as open-source software in July 2000.\nIn November 2000, Inprise Corporation announced the company intended to officially change its name to Borland Software Corporation. The legal name of the company would continue to be Inprise Corporation until the completion of the renaming process during the first quarter of 2001. Once the name change was completed, the company would also expect to change its Nasdaq market symbol from \"INPR\" to \"BORL\".\nBorland Software Corporation era.\nOn January 2, 2001, Borland Software Corporation announced it had completed its name change from Inprise Corporation. Effective at the opening of trading on Nasdaq, the company's Nasdaq market symbol would also be changed from \"INPR\" to \"BORL\".\nUnder the Borland name and a new management team headed by president and CEO Dale L.\u00a0Fuller, a now-smaller and profitable Borland refocused on Delphi and created a version of Delphi and C++Builder for Linux, both under the name Kylix. This brought Borland's expertise in integrated development environments to the Linux platform for the first time. Kylix was launched in 2001.\nPlans to spin off the InterBase division as a separate company were abandoned after Borland and the people who were to run the new company could not agree on terms for the separation. Borland stopped open-source releases of InterBase and has developed and sold new versions at a fast pace.\nIn 2001, Delphi 6 became the first integrated development environment to support web services. All of the company's development platforms now support web services.\nC#Builder was released in 2003 as a native C# development tool, competing with Visual Studio .NET. By the 2005 release, C#Builder, Delphi for Win32, and Delphi for .NET were combined into one IDE named \"Borland Developer Studio\", though it was still \npopularly known as \"Delphi\". In late 2002 Borland purchased design tool vendor TogetherSoft and tool publisher Starbase, makers of the StarTeam configuration management tool and the CaliberRM requirements management tool (eventually, CaliberRM was renamed as \"Caliber\"). The latest releases of JBuilder and Delphi integrate these tools to give developers a broader set of tools for development.\nFormer CEO Dale Fuller quit in July 2005, but remained on the board of directors. Former COO Scott Arnold took the title of interim president and chief executive officer until November 8, 2005, when it was announced that Tod Nielsen would take over as CEO effective November 9, 2005. Nielsen remained with the company until January 2009, when he accepted the position of chief operating officer at VMware; CFO Erik Prusch then took over as acting president and CEO.\nIn early 2007 Borland announced new branding for its focus around open application life-cycle management. In April 2007 Borland announced that it would relocate its headquarters and development facilities to Austin, Texas. It also had development centers in Singapore, Santa Ana, California, and Linz, Austria.\nOn May 6, 2009, the company announced it was to be acquired by Micro Focus for $75\u00a0million. The transaction was approved by Borland shareholders on July 22, 2009, with Micro Focus acquiring the company for $1.50 per share. Following Micro Focus shareholder approval and the required corporate filings, the transaction was completed in late July 2009. Borland was estimated to have 750 employees at the time.\nOn April 5, 2015, Micro Focus announced the completion of integrating the Attachmate Group of companies that was merged on November 20, 2014. During the integration period, the affected companies were merged into one organization. In the announced reorganization, Borland products would be part of the Micro Focus portfolio.\nProducts.\nRecent.\nThe products acquired from Segue Software include Silk Central, Silk Performer, and Silk Test. The Silk line was first announced in 1997. Other programs are:\nMarketing.\nRenaming to Inprise Corporation.\nAlong with renaming from Borland International, Inc. to Inprise Corporation, the company refocused its efforts on targeting enterprise applications development. Borland hired a marketing firm Lexicon Branding to come up with a new name for the company. Yocam explained that the new name, Inprise, was meant to evoke \"integrating the enterprise\". The idea was to integrate Borland's tools, Delphi, C++Builder, and JBuilder with enterprise environment software, including Visigenic's implementations of CORBA, Visibroker for C++ and Java, and the new product, Application Server.\nFrank Borland.\nFrank Borland is a mascot character for Borland products. According to Philippe Kahn, the mascot first appeared in advertisements and the cover of Borland Sidekick 1.0 manual, which was in 1984 during Borland International, Inc. era. Frank Borland also appeared in Turbo Tutor - A Turbo Pascal Tutorial, Borland JBuilder 2.\nA live action version of Frank Borland was made after Micro Focus plc had acquired Borland Software Corporation. This version was created by True Agency Limited. An introductory film was also made about the mascot."}
{"id": "4031", "revid": "1270536276", "url": "https://en.wikipedia.org/wiki?curid=4031", "title": "Buckminster Fuller", "text": "Richard Buckminster Fuller (; July 12, 1895\u00a0\u2013 July 1, 1983) was an American architect, systems theorist, writer, designer, inventor, philosopher, and futurist. He styled his name as R. Buckminster Fuller in his writings, publishing more than 30 books and coining or popularizing such terms as \"Spaceship Earth\", \"Dymaxion\" (e.g., Dymaxion house, Dymaxion car, Dymaxion map), \"ephemeralization\", \"synergetics\", and \"tensegrity\".\nFuller developed numerous inventions, mainly architectural designs, and popularized the widely known geodesic dome; carbon molecules known as fullerenes were later named by scientists for their structural and mathematical resemblance to geodesic spheres. He also served as the second World President of Mensa International from 1974 to 1983.\nFuller was awarded 28 United States patents and many honorary doctorates. In 1960, he was awarded the Frank P. Brown Medal from The Franklin Institute. He was elected an honorary member of Phi Beta Kappa in 1967, on the occasion of the 50-year reunion of his Harvard class of 1917 (from which he had been expelled in his first year). He was elected a Fellow of the American Academy of Arts and Sciences in 1968. The same year, he was elected into the National Academy of Design as an Associate member. He became a full Academician in 1970, and he received the Gold Medal award from the American Institute of Architects the same year. Also in 1970, Fuller received the title of Master Architect from Alpha Rho Chi (APX), the national fraternity for architecture and the allied arts.\nIn 1976, he received the St. Louis Literary Award from the Saint Louis University Library Associates. In 1977, he received the Golden Plate Award of the American Academy of Achievement. He also received numerous other awards, including the Presidential Medal of Freedom, presented to him on February 23, 1983, by President Ronald Reagan.\nLife and work.\nFuller was born on July 12, 1895, in Milton, Massachusetts, the son of Richard Buckminster Fuller, a prosperous leather and tea merchant, and Caroline Wolcott Andrews. He was a grand-nephew of Margaret Fuller, an American journalist, critic, and women's rights advocate associated with the American transcendentalism movement. The unusual middle name, Buckminster, was an ancestral family name. As a child, Richard Buckminster Fuller tried numerous variations of his name. He used to sign his name differently each year in the guest register of his family summer vacation home at Bear Island, Maine. He finally settled on R. Buckminster Fuller.\nFuller spent much of his youth on Bear Island, in Penobscot Bay off the coast of Maine. He attended Froebelian Kindergarten He was dissatisfied with the way geometry was taught in school, disagreeing with the notions that a chalk dot on the blackboard represented an \"empty\" mathematical point, or that a line could stretch off to infinity. To him these were illogical, and led to his work on synergetics. He often made items from materials he found in the woods, and sometimes made his own tools. He experimented with designing a new apparatus for human propulsion of small boats. By age 12, he had invented a 'push pull' system for propelling a rowboat by use of an inverted umbrella connected to the transom with a simple oar lock which allowed the user to face forward to point the boat toward its destination. Later in life, Fuller took exception to the term \"invention.\" \nYears later, he decided that this sort of experience had provided him with not only an interest in design, but also a habit of being familiar with and knowledgeable about the materials that his later projects would require. Fuller earned a machinist's certification, and knew how to use the press brake, stretch press, and other tools and equipment used in the sheet metal trade.\nEducation.\nFuller attended Milton Academy in Massachusetts, and after that began studying at Harvard College, where he was affiliated with Adams House. He was expelled from Harvard twice: first for spending all his money partying with a vaudeville troupe, and then, after having been readmitted, for his \"irresponsibility and lack of interest.\" By his own appraisal, he was a non-conforming misfit in the fraternity environment.\nWartime experience.\nBetween his sessions at Harvard, Fuller worked in Canada as a mechanic in a textile mill, and later as a laborer in the meat-packing industry. He also served in the U.S. Navy in World War I, as a shipboard radio operator, as an editor of a publication, and as commander of the crash rescue boat USS \"Inca\". After discharge, he worked again in the meat-packing industry, acquiring management experience. In 1917, he married Anne Hewlett. During the early 1920s, he and his father-in-law developed the Stockade Building System for producing lightweight, weatherproof, and fireproof housing\u2014although the company would ultimately fail in 1927.\nDepression and epiphany.\nFuller recalled 1927 as a pivotal year of his life. His daughter Alexandra had died in 1922 of complications from polio and spinal meningitis just before her fourth birthday. Barry Katz, a Stanford University scholar who wrote about Fuller, found signs that around this time in his life Fuller had developed depression and anxiety. Fuller dwelled on his daughter's death, suspecting that it was connected with the Fullers' damp and drafty living conditions. This provided motivation for Fuller's involvement in Stockade Building Systems, a business which aimed to provide affordable, efficient housing.\nIn 1927, at age 32, Fuller lost his job as president of Stockade. The Fuller family had no savings, and the birth of their daughter Allegra in 1927 added to the financial challenges. Fuller drank heavily and reflected upon the solution to his family's struggles on long walks around Chicago. During the autumn of 1927, Fuller contemplated suicide by drowning in Lake Michigan, so that his family could benefit from a life insurance payment.\nFuller said that he had experienced a profound incident which would provide direction and purpose for his life. He felt as though he was suspended several feet above the ground enclosed in a white sphere of light. A voice spoke directly to Fuller, and declared:\nFuller stated that this experience led to a profound re-examination of his life. He ultimately chose to embark on \"an experiment, to find what a single individual could contribute to changing the world and benefiting all humanity.\"\nSpeaking to audiences later in life, Fuller would frequently recount the story of his Lake Michigan experience, and its transformative impact on his life.\nRecovery.\nIn 1927, Fuller resolved to think independently which included a commitment to \"the search for the principles governing the universe and help advance the evolution of humanity in accordance with them\u00a0... finding ways of \"doing more with less\" to the end that all people everywhere can have more and more.\" By 1928, Fuller was living in Greenwich Village and spending much of his time at the popular caf\u00e9 Romany Marie's, where he had spent an evening in conversation with Marie and Eugene O'Neill several years earlier. Fuller accepted a job decorating the interior of the caf\u00e9 in exchange for meals, giving informal lectures several times a week, and models of the Dymaxion house were exhibited at the caf\u00e9. Isamu Noguchi arrived during 1929\u2014Constantin Br\u00e2ncu\u0219i, an old friend of Marie's, had directed him there\u2014and Noguchi and Fuller were soon collaborating on several projects, including the modeling of the Dymaxion car based on recent work by Aurel Persu. It was the beginning of their lifelong friendship.\nGeodesic domes.\nFuller taught at Black Mountain College in North Carolina during the summers of 1948 and 1949, serving as its Summer Institute director in 1949. Fuller had been shy and withdrawn, but he was persuaded to participate in a theatrical performance of Erik Satie's \"Le pi\u00e8ge de M\u00e9duse\" produced by John Cage, who was also teaching at Black Mountain. During rehearsals, under the tutelage of Arthur Penn, then a student at Black Mountain, Fuller broke through his inhibitions to become confident as a performer and speaker.\nAt Black Mountain, with the support of a group of professors and students, he began reinventing a project that would make him famous: the geodesic dome. Although the geodesic dome had been created, built and awarded a German patent on June 19, 1925, by Dr. Walther Bauersfeld, Fuller was awarded United States patents. Fuller's patent application made no mention of Bauersfeld's self-supporting dome built some 26 years prior. Although Fuller undoubtedly popularized this type of structure he is mistakenly given credit for its design.\nOne of his early models was first constructed in 1945 at Bennington College in Vermont, where he lectured often. Although Bauersfeld's dome could support a full skin of concrete it was not until 1949 that Fuller erected a geodesic dome building that could sustain its own weight with no practical limits. It was in diameter and constructed of aluminium aircraft tubing and a vinyl-plastic skin, in the form of an icosahedron. To prove his design, Fuller suspended from the structure's framework several students who had helped him build it. The U.S. government recognized the importance of this work, and employed his firm Geodesics, Inc. in Raleigh, North Carolina to make small domes for the Marines. Within a few years, there were thousands of such domes around the world.\nFuller's first \"continuous tension \u2013 discontinuous compression\" geodesic dome (full sphere in this case) was constructed at the University of Oregon Architecture School in 1959 with the help of students. These continuous tension \u2013 discontinuous compression structures featured single force compression members (no flexure or bending moments) that did not touch each other and were 'suspended' by the tensional members.\nDymaxion Chronofile.\nFor half of a century, Fuller developed many ideas, designs, and inventions, particularly regarding practical, inexpensive shelter and transportation. He documented his life, philosophy, and ideas scrupulously by a daily diary (later called the \"Dymaxion Chronofile\"), and by twenty-eight publications. Fuller financed some of his experiments with inherited funds, sometimes augmented by funds invested by his collaborators, one example being the Dymaxion car project.\nWorld stage.\nInternational recognition began with the success of huge geodesic domes during the 1950s. Fuller lectured at North Carolina State University in Raleigh in 1949, where he met James Fitzgibbon, who would become a close friend and colleague. Fitzgibbon was director of Geodesics, Inc. and Synergetics, Inc. the first licensees to design geodesic domes. Thomas C. Howard was lead designer, architect, and engineer for both companies. Richard Lewontin, a new faculty member in population genetics at North Carolina State University, provided Fuller with computer calculations for the lengths of the domes' edges.\nFuller began working with architect Shoji Sadao in 1954, together designing a hypothetical Dome over Manhattan in 1960, and in 1964 they co-founded the architectural firm Fuller &amp; Sadao Inc., whose first project was to design the large geodesic dome for the U.S. Pavilion at Expo 67 in Montreal. This building is now the \"Montreal Biosph\u00e8re\".\nIn 1962, the artist and searcher John McHale wrote the first monograph on Fuller, published by George Braziller in New York.\nAfter employing several Southern Illinois University Carbondale (SIU) graduate students to rebuild his models following an apartment fire in the summer of 1959, Fuller was recruited by longtime friend Harold Cohen to serve as a research professor of \"design science exploration\" at the institution's School of Art and Design. According to SIU architecture professor Jon Davey, the position was \"unlike most faculty appointments\u00a0... more a celebrity role than a teaching job\" in which Fuller offered few courses and was only stipulated to spend two months per year on campus. Nevertheless, his time in Carbondale was \"extremely productive\", and Fuller was promoted to university professor in 1968 and distinguished university professor in 1972.\nWorking as a designer, scientist, developer, and writer, he continued to lecture for many years around the world. He collaborated at SIU with John McHale. In 1965, they inaugurated the World Design Science Decade (1965 to 1975) at the meeting of the International Union of Architects in Paris, which was, in Fuller's own words, devoted to \"applying the principles of science to solving the problems of humanity.\"\nFrom 1972 until retiring as university professor emeritus in 1975, Fuller held a joint appointment at Southern Illinois University Edwardsville, where he had designed the dome for the campus Religious Center in 1971. During this period, he also held a joint fellowship at a consortium of Philadelphia-area institutions, including the University of Pennsylvania, Bryn Mawr College, Haverford College, Swarthmore College, and the University City Science Center; as a result of this affiliation, the University of Pennsylvania appointed him university professor emeritus in 1975.\nFuller believed human societies would soon rely mainly on renewable sources of energy, such as solar- and wind-derived electricity. He hoped for an age of \"omni-successful education and sustenance of all humanity.\" Fuller referred to himself as \"the property of universe\" and during one radio interview he gave later in life, declared himself and his work \"the property of all humanity.\" For his lifetime of work, the American Humanist Association named him the 1969 Humanist of the Year.\nIn 1976, Fuller was a key participant at UN Habitat I, the first UN forum on human settlements.\nLast filmed appearance.\nFuller was interviewed on film on June 21, 1983, in which he spoke at Norman Foster's Royal Gold Medal for architecture ceremony. His speech can be watched in the archives of the AA School of Architecture, in which he spoke after Sir Robert Sainsbury's introductory speech and Foster's keynote address.\nIn May, 1983 Buckminster Fuller participated in an interview with futurist Barbara Marx Hubbard. The hour-long DVD, \"Our Spiritual Experience: A Conversation with Buckminster Fuller and Barbara Marx Hubbard\" was produced by David L. Smith and was hosted by Michael Toms of New Dimensions Radio. The program was recorded at Xavier University in Cincinnati, Ohio. It can be viewed at Spiritual Visionaries.com, a new website expected to go \"public\" in February, 2025.[David L. Smith Productions]\nDeath.\nIn the year of his death, Fuller described himself as follows:\nFuller died on July 1, 1983, 11 days before his 88th birthday. During the period leading up to his death, his wife had been lying comatose in a Los Angeles hospital, dying of cancer. It was while visiting her there that he exclaimed, at a certain point: \"She is squeezing my hand!\" He then stood up, had a heart attack, and died an hour later, at age 87. His wife of 66 years died 36 hours later. They are buried in Mount Auburn Cemetery in Cambridge, Massachusetts.\nPhilosophy.\nBuckminster Fuller was a Unitarian, and, like his grandfather Arthur Buckminster Fuller (brother of Margaret Fuller), a Unitarian minister. Fuller was also an early environmental activist, aware of Earth's finite resources, and promoted a principle he termed \"ephemeralization\", which, according to futurist and Fuller disciple Stewart Brand, was defined as \"doing more with less\". Resources and waste from crude, inefficient products could be recycled into making more valuable products, thus increasing the efficiency of the entire process. Fuller also coined the word synergetics, a catch-all term used broadly for communicating experiences using geometric concepts, and more specifically, the empirical study of systems in transformation; his focus was on total system behavior unpredicted by the behavior of any isolated components.\nFuller was a pioneer in thinking globally and explored energy and material efficiency in the fields of architecture, engineering, and design. In his book \"Critical Path\" (1981) he cited the opinion of Fran\u00e7ois de Chaden\u00e8des (1920\u20131999) that petroleum, from the standpoint of its replacement cost in our current energy \"budget\" (essentially, the net incoming solar flux), had cost nature \"over a million dollars\" per U.S. gallon ($300,000 per litre) to produce. From this point of view, its use as a transportation fuel by people commuting to work represents a huge net loss compared to their actual earnings. An encapsulation quotation of his views might best be summed up as: \"There is no energy crisis, only a crisis of ignorance.\"\nThough Fuller was concerned about sustainability and human survival under the existing socioeconomic system, he remained optimistic about humanity's future. Defining wealth in terms of knowledge as the \"technological ability to protect, nurture, support, and accommodate all growth needs of life\", his analysis of the condition of \"Spaceship Earth\" caused him to conclude that at a certain time during the 1970s, humanity had attained an unprecedented state. He was convinced that the accumulation of relevant knowledge, combined with the quantities of major recyclable resources that had already been extracted from the earth, had attained a critical level, such that competition for necessities had become unnecessary. Cooperation had become the optimum survival strategy. He declared: \"selfishness is unnecessary and hence-forth unrationalizable\u00a0... War is obsolete.\" He criticized previous utopian schemes as too exclusive and thought this was a major source of their failure. To work, he felt that a utopia needed to include everyone.\nFuller was influenced by Alfred Korzybski's idea of general semantics. In the 1950s, Fuller attended seminars and workshops organized by the Institute of General Semantics, and he delivered the annual Alfred Korzybski Memorial Lecture in 1955. Korzybski is mentioned in the Introduction of his book \"Synergetics\". The two shared a remarkable amount of similarity in their general semantics formulations.\nIn his 1970 book, \"I Seem To Be a Verb\", he wrote: \"I live on Earth at present, and I don't know what I am. I know that I am not a category. I am not a thing\u2014a noun. I seem to be a verb, an evolutionary process\u2014an integral function of the universe.\"\nFuller wrote that the universe's natural analytic geometry was based on tetrahedra arrays. He developed this in several ways, from the close-packing of spheres and the number of compressive or tensile members required to stabilize an object in space. One confirming result was that the strongest possible homogeneous truss is cyclically tetrahedral.\nHe had become a guru of the design, architecture, and \"alternative\" communities, such as Drop City, the community of experimental artists to whom he awarded the 1966 \"Dymaxion Award\" for \"poetically economic\" domed living structures.\nMajor design projects.\nThe geodesic dome.\nFuller was most famous for his lattice shell structures \u2013 geodesic domes, which have been used as parts of military radar stations, civic buildings, environmental protest camps, and exhibition attractions. An examination of the geodesic design by Walther Bauersfeld for the Zeiss-Planetarium, built some 28 years prior to Fuller's work, reveals that Fuller's Geodesic Dome patent (U.S. 2,682,235; awarded in 1954) is the same design as Bauersfeld's.\nTheir construction is based on extending some basic principles to build simple \"tensegrity\" structures (tetrahedron, octahedron, and the closest packing of spheres), making them lightweight and stable. The geodesic dome was a result of Fuller's exploration of nature's constructing principles to find design solutions. The Fuller Dome is referenced in the Hugo Award-winning 1968 novel \"Stand on Zanzibar\" by John Brunner, in which a geodesic dome is said to cover the entire island of Manhattan, and it floats on air due to the hot-air balloon effect of the large air-mass under the dome (and perhaps its construction of lightweight materials).\nTransportation.\nThe Dymaxion car was a vehicle designed by Fuller, featured prominently at Chicago's 1933-1934 Century of Progress World's Fair. During the Great Depression, Fuller formed the \"Dymaxion Corporation\" and built three prototypes with noted naval architect Starling Burgess and a team of 27 workmen \u2014 using donated money as well as a family inheritance.\nFuller associated the word \"Dymaxion\", a blend of the words dynamic\", maximum\", and \"tension\" to sum up the goal of his study, \"maximum gain of advantage from minimal energy input\".\nThe Dymaxion was not an automobile but rather the 'ground-taxying mode' of a vehicle that might one day be designed to fly, land and drive \u2014 an \"Omni-Medium Transport\" for air, land and water. Fuller focused on the landing and taxiing qualities, and noted severe limitations in its handling. The team made improvements and refinements to the platform, and Fuller noted the Dymaxion \"was an invention that could not be made available to the general public without considerable improvements\".\nThe bodywork was aerodynamically designed for increased fuel efficiency and its platform featured a lightweight cromoly-steel hinged chassis, rear-mounted V8 engine, front-drive, and three-wheels. The vehicle was steered via the third wheel at the rear, capable of 90\u00b0 steering lock. Able to steer in a tight circle, the Dymaxion often caused a sensation, bringing nearby traffic to a halt.\nShortly after launch, a prototype rolled over and crashed, killing the Dymaxion's driver and seriously injuring its passengers. Fuller blamed the accident on a second car that collided with the Dymaxion. Eyewitnesses reported, however, that the other car hit the Dymaxion only after it had begun to roll over.\nDespite courting the interest of important figures from the auto industry, Fuller used his family inheritance to finish the second and third prototypes \u2014 eventually selling all three, dissolving \"Dymaxion Corporation\" and maintaining the Dymaxion was never intended as a commercial venture. One of the three original prototypes survives.\nHousing.\nFuller's energy-efficient and inexpensive Dymaxion house garnered much interest, but only two prototypes were ever produced. Here the term \"Dymaxion\" is used in effect to signify a \"radically strong and light tensegrity structure\". One of Fuller's Dymaxion Houses is on display as a permanent exhibit at the Henry Ford Museum in Dearborn, Michigan. Designed and developed during the mid-1940s, this prototype is a round structure (not a dome), shaped something like the flattened \"bell\" of certain jellyfish. It has several innovative features, including revolving dresser drawers, and a fine-mist shower that reduces water consumption. According to Fuller biographer Steve Crooks, the house was designed to be delivered in two cylindrical packages, with interior color panels available at local dealers. A circular structure at the top of the house was designed to rotate around a central mast to use natural winds for cooling and air circulation.\nConceived nearly two decades earlier, and developed in Wichita, Kansas, the house was designed to be lightweight, adapted to windy climates, cheap to produce and easy to assemble. Because of its light weight and portability, the Dymaxion House was intended to be the ideal housing for individuals and families who wanted the option of easy mobility. The design included a \"Go-Ahead-With-Life Room\" stocked with maps, charts, and helpful tools for travel \"through time and space\". It was to be produced using factories, workers, and technologies that had produced World War II aircraft. It looked ultramodern at the time, built of metal, and sheathed in polished aluminum. The basic model enclosed of floor area. Due to publicity, there were many orders during the early Post-War years, but the company that Fuller and others had formed to produce the houses failed due to management problems.\nIn 1967, Fuller developed a concept for an offshore floating city named Triton City and published a report on the design the following year. Models of the city aroused the interest of President Lyndon B. Johnson who, after leaving office, had them placed in the Lyndon Baines Johnson Library and Museum.\nIn 1969, Fuller began the Otisco Project, named after its location in Otisco, New York. The project developed and demonstrated concrete spray with mesh-covered wireforms for producing large-scale, load-bearing spanning structures built on-site, without the use of pouring molds, other adjacent surfaces, or hoisting. The initial method used a circular concrete footing in which anchor posts were set. Tubes cut to length and with ends flattened were then bolted together to form a duodeca-rhombicahedron (22-sided hemisphere) geodesic structure with spans ranging to . The form was then draped with layers of \u00bc-inch wire mesh attached by twist ties. Concrete was sprayed onto the structure, building up a solid layer which, when cured, would support additional concrete to be added by a variety of traditional means. Fuller referred to these buildings as monolithic ferroconcrete geodesic domes. However, the tubular frame form proved problematic for setting windows and doors. It was replaced by an iron rebar set vertically in the concrete footing and then bent inward and welded in place to create the dome's wireform structure and performed satisfactorily. Domes up to three stories tall built with this method proved to be remarkably strong. Other shapes such as cones, pyramids, and arches proved equally adaptable.\nThe project was enabled by a grant underwritten by Syracuse University and sponsored by U.S. Steel (rebar), the Johnson Wire Corp (mesh), and Portland Cement Company (concrete). The ability to build large complex load bearing concrete spanning structures in free space would open many possibilities in architecture, and is considered one of Fuller's greatest contributions.\nDymaxion map and World Game.\nFuller, along with co-cartographer Shoji Sadao, also designed an alternative projection map, called the Dymaxion map. This was designed to show Earth's continents with minimum distortion when projected or printed on a flat surface.\nIn the 1960s, Fuller developed the World Game, a collaborative simulation game played on a 70-by-35-foot Dymaxion map, in which players attempt to solve world problems. The object of the simulation game is, in Fuller's words, to \"make the world work, for 100% of humanity, in the shortest possible time, through spontaneous cooperation, without ecological offense or the disadvantage of anyone\".\nAppearance and style.\nBuckminster Fuller wore thick-lensed spectacles to correct his extreme hyperopia, a condition that went undiagnosed for the first five years of his life. Fuller's hearing was damaged during his naval service in World War I and deteriorated during the 1960s. After experimenting with bullhorns as hearing aids during the mid-1960s, Fuller adopted electronic hearing aids from the 1970s onward.\nIn public appearances, Fuller always wore dark-colored suits, appearing like \"an alert little clergyman\". Previously, he had experimented with unconventional clothing immediately after his 1927 epiphany, but found that breaking social fashion customs made others devalue or dismiss his ideas. Fuller learned the importance of physical appearance as part of one's credibility, and decided to become \"the invisible man\" by dressing in clothes that would not draw attention to himself. With self-deprecating humor, Fuller described this black-suited appearance as resembling a \"second-rate bank clerk\".\nWriter Guy Davenport met him in 1965 and described him thus:\nLifestyle.\nFollowing his global prominence from the 1960s onward, Fuller became a frequent flier, often crossing time zones to lecture. In the 1960s and 1970s, he wore three watches simultaneously; one for the time zone of his office at Southern Illinois University, one for the time zone of the location he would next visit, and one for the time zone he was currently in. In the 1970s, Fuller was only in 'homely' locations (his personal home in Carbondale, Illinois; his holiday retreat in Bear Island, Maine; and his daughter's home in Pacific Palisades, California) roughly 65 nights per year\u2014the other 300 nights were spent in hotel beds in the locations he visited on his lecturing and consulting circuits.\nIn the 1920s, Fuller experimented with polyphasic sleep, which he called \"Dymaxion sleep\". Inspired by the sleep habits of animals such as dogs and cats, Fuller worked until he was tired, and then slept short naps. This generally resulted in Fuller sleeping 30-minute naps every 6 hours. This allowed him \"twenty-two thinking hours a day\", which aided his work productivity. Fuller reportedly kept this Dymaxion sleep habit for two years, before quitting the routine because it conflicted with his business associates' sleep habits. Despite no longer personally partaking in the habit, in 1943 Fuller suggested Dymaxion sleep as a strategy that the United States could adopt to win World War II.\nDespite only practicing true polyphasic sleep for a period during the 1920s, Fuller was known for his stamina throughout his life. He was described as \"tireless\" by Barry Farrell in \"Life\" magazine, who noted that Fuller stayed up all night replying to mail during Farrell's 1970 trip to Bear Island. In his seventies, Fuller generally slept for 5\u20138 hours per night.\nFuller documented his life copiously from 1915 to 1983, approximately of papers in a collection called the Dymaxion Chronofile. He also kept copies of all incoming and outgoing correspondence. The enormous R. Buckminster Fuller Collection is currently housed at Stanford University.\nLanguage and neologisms.\nBuckminster Fuller spoke and wrote in a unique style and said it was important to describe the world as accurately as possible. Fuller often created long run-on sentences and used unusual compound words (omniwell-informed, intertransformative, omni-interaccommodative, omniself-regenerative), as well as terms he himself invented. His style of speech was characterized by progressively rapid and breathless delivery and rambling digressions of thought, which Fuller described as \"thinking out loud\". The effect, combined with Fuller's dry voice and non-rhotic New England accent, was varyingly considered \"hypnotic\" or \"overwhelming\".\nFuller used the word \"Universe\" without the definite or indefinite article (\"the\" or \"a\") and always capitalized the word. Fuller wrote that \"by Universe I mean: the aggregate of all humanity's consciously apprehended and communicated (to self or others) Experiences\".\nThe words \"down\" and \"up\", according to Fuller, are awkward in that they refer to a planar concept of direction inconsistent with human experience. The words \"in\" and \"out\" should be used instead, he argued, because they better describe an object's relation to a gravitational center, the Earth. \"I suggest to audiences that they say, 'I'm going \"outstairs\" and \"instairs.\"' At first that sounds strange to them; They all laugh about it. But if they try saying in and out for a few days in fun, they find themselves beginning to realize that they are indeed going inward and outward in respect to the center of Earth, which is our Spaceship Earth. And for the first time they begin to feel real 'reality.'\"\nFuller preferred the term \"world-around\" to replace \"worldwide\". The general belief in a flat Earth died out in classical antiquity, so using \"wide\" is an anachronism when referring to the surface of the Earth\u2014a spheroidal surface has area and encloses a volume but has no width. Fuller held that unthinking use of obsolete scientific ideas detracts from and misleads intuition. Other neologisms collectively invented by the Fuller family, according to Allegra Fuller Snyder, are the terms \"sunsight\" and \"sunclipse\", replacing \"sunrise\" and \"sunset\" to overturn the geocentric bias of most pre-Copernican celestial mechanics.\nFuller also invented the word \"livingry\", as opposed to weaponry (or \"killingry\"), to mean that which is in support of all human, plant, and Earth life. \"The architectural profession\u2014civil, naval, aeronautical, and astronautical\u2014has always been the place where the most competent thinking is conducted regarding livingry, as opposed to weaponry.\"\nAs well as contributing significantly to the development of tensegrity technology, Fuller invented the term \"tensegrity\", a portmanteau of \"tensional integrity\". \"Tensegrity describes a structural-relationship principle in which structural shape is guaranteed by the finitely closed, comprehensively continuous, tensional behaviors of the system and not by the discontinuous and exclusively local compressional member behaviors. Tensegrity provides the ability to yield increasingly without ultimately breaking or coming asunder.\"\n\"Dymaxion\" is a portmanteau of \"dynamic maximum tension\". It was invented around 1929 by two admen at Marshall Field's department store in Chicago to describe Fuller's concept house, which was shown as part of a house of the future store display. They created the term using three words that Fuller used repeatedly to describe his design \u2013 dynamic, maximum, and tension.\nFuller also helped to popularize the concept of Spaceship Earth: \"The most important fact about Spaceship Earth: an instruction manual didn't come with it.\"\nIn the preface for his \"cosmic fairy tale\" \"Tetrascroll: Goldilocks and the Three Bears\", Fuller stated that his distinctive speaking style grew out of years of embellishing the classic tale for the benefit of his daughter, allowing him to explore both his new theories and how to present them. The \"Tetrascroll\" narrative was eventually transcribed onto a set of tetrahedral lithographs (hence the name), as well as being published as a traditional book.\nFuller's language posed problems for his credibility. John Julius Norwich recalled commissioning a 600-word introduction for a planned history of world architecture from him, and receiving a 3500-word proposal which ended: \nNorwich commented: \"On reflection, I asked Dr. Nikolaus Pevsner instead.\"\nConcepts and buildings.\nHis concepts and buildings include:\nInfluence and legacy.\nAmong the many people who were influenced by Buckminster Fuller are:\nConstance Abernathy,\nRuth Asawa,\nJ. Baldwin,\nMichael Ben-Eli, Pierre Cabrol,\nJohn Cage,\nJoseph Clinton,\nPeter Floyd,\nNorman Foster,\nMedard Gabel,\nMichael Hays,\nTed Nelson,\nDavid Johnston,\nPeter Jon Pearce,\nShoji Sadao,\nEdwin Schlossberg,\nKenneth Snelson,\nRobert Anton Wilson, Stewart Brand, Jason McLennan, and John Denver.\nAn allotrope of carbon, fullerene\u2014and a particular molecule of that allotrope C60 (buckminsterfullerene or buckyball) has been named after him. The Buckminsterfullerene molecule, which consists of 60 carbon atoms, very closely resembles a spherical version of Fuller's geodesic dome. The 1996 Nobel Prize in Chemistry was given to Kroto, Curl, and Smalley for their discovery of the fullerene.\nOn July 12, 2004, the United States Post Office released a new commemorative stamp honoring R. Buckminster Fuller on the 50th anniversary of his patent for the geodesic dome and by the occasion of his 109th birthday. The stamp's design replicated the January 10, 1964, cover of \"Time\" magazine.\nFuller was the subject of two documentary films: \"The World of Buckminster Fuller\" (1971) and \"\" (1996). Additionally, filmmaker Sam Green and the band Yo La Tengo collaborated on a 2012 \"live documentary\" about Fuller, \"The Love Song of R. Buckminster Fuller\".\nIn June 2008, the Whitney Museum of American Art presented \"Buckminster Fuller: Starting with the Universe\", the most comprehensive retrospective to date of his work and ideas. The exhibition traveled to the Museum of Contemporary Art, Chicago in 2009. It presented a combination of models, sketches, and other artifacts, representing six decades of the artist's integrated approach to housing, transportation, communication, and cartography. It also featured the extensive connections with Chicago from his years spent living, teaching, and working in the city.\nIn 2009, a number of US companies decided to repackage spherical magnets and sell them as toys. One company, Maxfield &amp; Oberton, told \"The New York Times\" that they saw the product on YouTube and decided to repackage them as \"Buckyballs\", because the magnets could self-form and hold together in shapes reminiscent of the Fuller inspired buckyballs. The buckyball toy launched at New York International Gift Fair in 2009 and sold in the hundreds of thousands, but by 2010 began to experience problems with toy safety issues and the company was forced to recall the packages that were labelled as toys.\nIn 2012, the San Francisco Museum of Modern Art hosted \"The Utopian Impulse\" \u2013 a show about Buckminster Fuller's influence in the Bay Area. Featured were concepts, inventions and designs for creating \"free energy\" from natural forces, and for sequestering carbon from the atmosphere. The show ran January through July.\nIn popular culture.\nFuller is quoted in \"The Tower of Babble\" from the musical \"Godspell\": \"Man is a complex of patterns and processes.\"\nBelgian rock band dEUS released the song \"The Architect\", inspired by Fuller, on their 2008 album \"Vantage Point\".\nIndie band Driftless Pony Club titled their 2011 album \"Buckminster\" after Fuller. Each of the album's songs is based upon his life and works.\nThe design podcast \"99% Invisible\" (2010\u2013present) takes its title from a Fuller quote: \"Ninety-nine percent of who you are is invisible and untouchable.\"\nFuller is briefly mentioned in \"\" (2014) when Kitty Pryde is giving a lecture to a group of students regarding utopian architecture.\nRobert Kiyosaki's 2009 book \"Conspiracy of the Rich\" and 2015 book \"Second Chance\" both concern Kiyosaki's interactions with Fuller as well as Fuller's unusual final book, \"Grunch of Giants\".\nIn \"The House of Tomorrow\" (2017), based on Peter Bognanni's 2010 novel of the same name, Ellen Burstyn's character is obsessed with Fuller and provides retro-futurist tours of her geodesic home that include videos of Fuller sailing and talking with Burstyn, who had in real life befriended Fuller.\nIn \"The Simpsons\"' Treehouse of Horror episode airing on October 29, 1992, a scan over Springfield graveyard reveals graves for American workmanship, Drexell's class, slapstick, and Buckminster Fuller."}
{"id": "4032", "revid": "1268886133", "url": "https://en.wikipedia.org/wiki?curid=4032", "title": "Bill Watterson", "text": "William Boyd Watterson II (born July 5, 1958) is an American cartoonist who authored the comic strip \"Calvin and Hobbes\". The strip was syndicated from 1985 to 1995. Watterson concluded \"Calvin and Hobbes\" with a short statement to newspaper editors and his readers that he felt he had achieved all he could in the medium. Watterson is known for his negative views on comic syndication and licensing, his efforts to expand and elevate the newspaper comic as an art form, and his move back into private life after \"Calvin and Hobbes\" ended. Watterson was born in Washington, D.C., and grew up in Chagrin Falls, Ohio. The suburban Midwestern United States setting of Ohio was part of the inspiration for the setting of \"Calvin and Hobbes\". Watterson lives in Cleveland Heights, Ohio as of January 2024.\nEarly life.\nBill Watterson was born on July 5, 1958, in Washington, D.C., to Kathryn Watterson (1933\u20132022) and James Godfrey Watterson (1932\u20132016). His father worked as a patent attorney. In 1965, six-year-old Watterson and his family moved to Chagrin Falls, Ohio, a suburb of Cleveland. Watterson has a younger brother, Thomas Watterson.\nWatterson drew his first cartoon at age eight and spent much time in his childhood alone, drawing and cartooning. This continued through his school years, during which time he discovered comic strips such as Walt Kelly's \"Pogo\", George Herriman's \"Krazy Kat\", and Charles M. Schulz's \"Peanuts\" which subsequently inspired and influenced his desire to become a professional cartoonist. On one occasion when he was in fourth grade, he wrote a letter to Schulz, who responded, much to Watterson's surprise. This made a big impression on him at the time. His parents encouraged him in his artistic pursuits. Later, they recalled him as a \"conservative child\" \u2014 imaginative, but \"not in a fantasy way\", and certainly nothing like the character of Calvin that he later created. Watterson found avenues for his cartooning talents throughout primary and secondary school, creating high school-themed super hero comics with his friends and contributing cartoons and art to the school newspaper and yearbook.\nAfter high school, Watterson attended Kenyon College, where he majored in political science. He had already decided on a career in cartooning but he felt studying political science would help him move into editorial cartooning. He continued to develop his art skills and during his sophomore year he painted Michelangelo's \"Creation of Adam\" on the ceiling of his dormitory room. He also contributed cartoons to the college newspaper, some of which included the original \"Spaceman Spiff\" cartoons. Watterson graduated from Kenyon in 1980 with a Bachelor of Arts degree.\nLater, when Watterson was creating names for the characters in his comic strip, he decided on Calvin (after the Protestant reformer John Calvin) and Hobbes (after the political philosopher Thomas Hobbes), allegedly as a \"tip of the hat\" to Kenyon's political science department. In \"The Complete Calvin and Hobbes\", Watterson stated that Calvin was named for \"a 16th-century theologian who believed in predestination\", and Hobbes for \"a 17th-century philosopher with a dim view of human nature\".\nCareer.\nEarly work.\nWatterson was inspired by the work of \"The Cincinnati Enquirer\" political cartoonist Jim Borgman, a 1976 graduate of Kenyon College, and decided to try to follow the same career path as Borgman, who in turn offered support and encouragement to the aspiring artist. Watterson graduated in 1980 and was hired on a trial basis at the \"Cincinnati Post\", a competing paper of the \"Enquirer\". Watterson quickly discovered that the job was full of unexpected challenges which prevented him from performing his duties to the standards set for him. Not the least of these challenges was his unfamiliarity with the Cincinnati political scene, as he had never resided in or near the city, having grown up in the Cleveland area and attending college in central Ohio. The \"Post\" fired Watterson before his contract was up.\nHe then joined a small advertising agency and worked there for four years as a designer, creating grocery advertisements while also working on his own projects, including development of his own cartoon strip and contributions to \"Target: The Political Cartoon Quarterly\".\nAs a freelance artist, Watterson has drawn other works for various merchandise, including album art for his brother's band, calendars, clothing graphics, educational books, magazine covers, posters, and post cards.\n\"Calvin and Hobbes\" and rise to success.\nWatterson has said that he works for personal fulfillment. As he told the graduating class of 1990 at Kenyon College, \"It's surprising how hard we'll work when the work is done just for ourselves.\" \"Calvin and Hobbes\" was first published on November 18, 1985. In \"Calvin and Hobbes Tenth Anniversary Book\", he wrote that his influences included \"Peanuts\", \"Pogo\", and \"Krazy Kat\". Watterson wrote the introduction to the first volume of \"The Komplete Kolor Krazy Kat\". Watterson's style also reflects the influence of Winsor McCay's \"Little Nemo in Slumberland\".\nLike many artists, Watterson incorporated elements of his life, interests, beliefs, and values into his work\u2014for example, his hobby as a cyclist, memories of his own father's speeches about \"building character\", and his views on merchandising and corporations. Watterson's cat Sprite very much inspired the personality and physical features of Hobbes.\nWatterson spent much of his career trying to change the climate of newspaper comics. He believed that the artistic value of comics was being undermined, and that the space that they occupied in newspapers continually decreased, subject to arbitrary whims of shortsighted publishers. Furthermore, he opined that art should not be judged by the medium for which it is created (i.e., there is no \"high\" art or \"low\" art\u2014just art).\nWatterson wrote a foreword for \"FoxTrot.\" \nFight against merchandising his characters.\nFor years, Watterson battled against pressure from publishers to merchandise his work, something that he felt would cheapen his comic through compromising the act of creation or reading.\nHe refused to merchandise his creations on the grounds that displaying \"Calvin and Hobbes\" images on commercially sold mugs, stickers, and T-shirts would devalue the characters and their personalities. Watterson said that Universal kept putting pressure on him and that he had signed his contract without fully perusing it because, as a new artist, he was happy just to find a syndicate willing to give him a chance (two other syndicates had previously turned him down). He added that the contract was so one-sided that, if Universal really wanted to, they could license his characters against his will, and could even fire him and continue \"Calvin and Hobbes\" with a new artist. Watterson's position eventually won out and he was able to renegotiate his contract so that he would receive all rights to his work, but later added that the licensing fight exhausted him and contributed to the need for a nine-month sabbatical in 1991.\nDespite Watterson's efforts, many unofficial knockoffs have been found, including items that depict Calvin and Hobbes consuming alcohol or Calvin urinating on a logo. Watterson has said, \"Only thieves and vandals have made money on \"Calvin and Hobbes\" merchandise.\"\nChanging the format of the Sunday strip.\nWatterson was critical of the prevailing format for the Sunday comic strip that was in place when he began drawing (and remained so, to varying degrees). The typical layout consists of three rows with eight total squares, which take up half a page if published with its normal size. Some newspapers are restricted with space for their Sunday features and reduce the size of the strip. One of the more common ways is to cut out the top two panels, which Watterson believed forced him to waste the space on throwaway jokes that did not always fit the strip. \nWhile he was set to return from his first sabbatical, Watterson discussed with his syndicate a new format for \"Calvin and Hobbes\" that would enable him to use his space more efficiently and would almost require the papers to publish it as a half-page. Universal agreed that they would sell the strip as the half-page and nothing else, which garnered anger from papers and criticism for Watterson from both editors and some of his fellow cartoonists (whom he described as \"unnecessarily hot-tempered\"). Eventually, Universal compromised and agreed to offer papers a choice between the full half-page or a reduced-sized version to alleviate concerns about the size issue. Watterson conceded that this caused him to lose space in many papers, but he said that, in the end, it was a benefit because he felt that he was giving the papers' readers a better strip for their money and editors were free not to run \"Calvin and Hobbes\" at their own risk. He added that he was not going to apologize for drawing a popular feature.\nEnd of \"Calvin and Hobbes\".\nOn November 9, 1995, Watterson announced the end of \"Calvin and Hobbes\" with the following letter to newspaper editors:\nThe last strip of \"Calvin and Hobbes\" was published on December 31, 1995.\nAfter \"Calvin and Hobbes\".\nIn the years since \"Calvin and Hobbes\" was ended, many attempts have been made to contact Watterson. Both \"The Plain Dealer\" and the \"Cleveland Scene\" sent reporters, in 1998 and 2003 respectively, but neither were able to make contact with the media-shy Watterson. Since 1995, Watterson has taken up painting, at one point drawing landscapes of the woods with his father. He has kept away from the public eye and shown no interest in resuming the strip, creating new works based on the strip's characters, or embarking on new commercial projects, though he has published several \"Calvin and Hobbes\" \"treasury collection\" anthologies. He does not sign autographs or license his characters. Watterson was once known to sneak autographed copies of his books onto the shelves of the Fireside Bookshop, a family-owned bookstore in his hometown of Chagrin Falls, Ohio. He ended this practice after discovering that some of the autographed books were being sold online for high prices.\nWatterson rarely gives interviews or makes public appearances. His lengthiest interviews include the cover story in \"The Comics Journal\" No. 127 in February 1989, an interview that appeared in a 1987 issue of \"Honk Magazine\", and one in a 2015 Watterson exhibition catalogue.\nOn December 21, 1999, a short piece was published in the \"Los Angeles Times\", written by Watterson to mark the forthcoming retirement of \"Peanuts\" creator Charles M. Schulz.\nCirca 2003, Gene Weingarten of \"The Washington Post\" sent Watterson the first edition of the \"Barnaby\" book as an incentive, hoping to land an interview. Weingarten passed the book to Watterson's parents, along with a message, and declared that he would wait in his hotel for as long as it took Watterson to contact him. Watterson's editor Lee Salem called the next day to tell Weingarten that the cartoonist would not be coming.\nIn 2004, Watterson and his wife Melissa bought a home in the Cleveland suburb of Cleveland Heights, Ohio. In 2005, they completed the move from their home in Chagrin Falls to their new residence.\nIn October 2005, Watterson answered 15 questions submitted by readers. In October 2007, he wrote a review of \"Schulz and Peanuts\", a biography of Charles M. Schulz, in \"The Wall Street Journal\".\nIn 2008, he provided a foreword for the first book collection of Richard Thompson's \"Cul de Sac\" comic strip. In April 2011, a representative for Andrews McMeel received a package from a \"William Watterson in Cleveland Heights, Ohio\" which contained a oil-on-board painting of \"Cul de Sac\" character Petey Otterloop, done by Watterson for the \"Team Cul de Sac\" fundraising project for Parkinson's disease in honor of Richard Thompson, who was diagnosed in 2009. Watterson's syndicate revealed that the painting was the first new artwork of his that the syndicate has seen since \"Calvin and Hobbes\" ended in 1995.\nIn October 2009, Nevin Martell published a book called \"Looking for Calvin and Hobbes,\" which included a story about the author seeking an interview with Watterson. In his search he interviews friends, co-workers and family but never gets to meet the artist himself.\nIn early 2010, Watterson was interviewed by \"The Plain Dealer\" on the 15th anniversary of the end of \"Calvin and Hobbes\". Explaining his decision to discontinue the strip, he said,\nIn October 2013, the magazine \"Mental Floss\" published an interview with Watterson, only the second since the strip ended. Watterson again confirmed that he would not be revisiting \"Calvin and Hobbes\", and that he was satisfied with his decision. He also gave his opinion on the changes in the comic-strip industry and where it would be headed in the future:\nIn 2013 the documentary \"Dear Mr. Watterson\", exploring the cultural impact of \"Calvin and Hobbes\", was released. Watterson himself did not appear in the film.\nOn February 26, 2014, Watterson published his first cartoon since the end of \"Calvin and Hobbes\": a poster for the documentary \"Stripped\".\nIn 2014, Watterson co-authored \"The Art of Richard Thompson\" with \"Washington Post\" cartoonist Nick Galifianakis and David Apatoff.\nIn June 2014, three strips of \"Pearls Before Swine\" (published June 4, June 5, and June 6, 2014) featured guest illustrations by Watterson after mutual friend Nick Galifianakis connected him and cartoonist Stephan Pastis, who communicated via e-mail. Pastis likened this unexpected collaboration to getting \"a glimpse of Bigfoot\". \"I thought maybe Stephan and I could do this goofy collaboration and then use the result to raise some money for Parkinson's research in honor of Richard Thompson. It seemed like a perfect convergence\", Watterson told \"The Washington Post\". The day that Stephan Pastis returned to his own strip, he paid tribute to Watterson by alluding to the final strip of \"Calvin and Hobbes\" from December 31, 1995.\nOn November 5, 2014, a poster was unveiled, drawn by Watterson for the 2015 Angoul\u00eame International Comics Festival where he was awarded the Grand Prix in 2014.\nOn April 1, 2016, for April Fools' Day, Berkeley Breathed posted on Facebook that Watterson had signed \"the franchise over to my 'administration'\". He then posted a comic with Calvin, Hobbes, and Opus all featured. The comic is signed by Watterson, though the degree of his involvement was speculative. Breathed posted another \"Calvin County\" strip featuring Calvin and Hobbes, also \"signed\" by Watterson on April 1, 2017, along with a fake \"New York Times\" story ostensibly detailing the \"merger\" of the two strips. Berkeley Breathed included Hobbes in a November 27, 2017, strip as a stand-in for the character Steve Dallas. Hobbes has also returned in the June 9, 11, and 12, 2021, strips as a stand-in for Bill The Cat.\nExhibitions.\nIn 2001, the Billy Ireland Cartoon Library &amp; Museum at Ohio State University mounted an exhibition of Watterson's Sunday strips. He chose thirty-six of his favorites, displaying them with both the original drawing and the colored finished product, with most pieces featuring personal annotations. Watterson also wrote an accompanying essay that served as the foreword for the exhibit, called \"Calvin and Hobbes: Sunday Pages 1985\u20131995\", which opened on September 10, 2001. It was taken down in January 2002. The accompanying published catalog had the same title.\nFrom March 22 to August 3, 2014, Watterson exhibited again at the Billy Ireland Cartoon Library &amp; Museum at Ohio State University. In conjunction with this exhibition, Watterson also participated in an interview with the school. An exhibition catalog named \"Exploring Calvin and Hobbes\" was released with the exhibit. The book contained a lengthy interview with Bill Watterson, conducted by Jenny Robb, the curator of the museum.\n\"The Mysteries\".\nWatterson released his first published work in 28 years on October 10, 2023, called \"The Mysteries\". It was an illustrated \"fable for grown-ups\" about \"what lies beyond human understanding\". The work was a collaboration with the illustrator and caricaturist John Kascht.\nAwards and honors.\nWatterson was awarded the National Cartoonists Society's Reuben Award in both 1986 and 1988. Watterson's second Reuben win made him the youngest cartoonist to be so honored, and only the sixth person to win twice, following Milton Caniff, Charles M. Schulz, Dik Browne, Chester Gould, and Jeff MacNelly. Gary Larson is the only cartoonist to win a second Reuben since Watterson. \nIn 2014, Watterson was awarded the Grand Prix at the Angoul\u00eame International Comics Festival for his body of work, becoming just the fourth non-European cartoonist to be so honored in the first 41 years of the event.\nBibliography.\nTreasury collections"}
{"id": "4034", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=4034", "title": "Britannica Public Domain", "text": ""}
{"id": "4035", "revid": "46584643", "url": "https://en.wikipedia.org/wiki?curid=4035", "title": "Black", "text": "Black is a color that results from the absence or complete absorption of visible light. It is an achromatic color, without hue, like white and grey. It is often used symbolically or figuratively to represent darkness. Black and white have often been used to describe opposites such as good and evil, the Dark Ages versus the Age of Enlightenment, and night versus day. Since the Middle Ages, black has been the symbolic color of solemnity and authority, and for this reason it is still commonly worn by judges and magistrates.\nBlack was one of the first colors used by artists in Neolithic cave paintings. It was used in ancient Egypt and Greece as the color of the underworld. In the Roman Empire, it became the color of mourning, and over the centuries it was frequently associated with death, evil, witches, and magic. In the 14th century, it was worn by royalty, clergy, judges, and government officials in much of Europe. It became the color worn by English romantic poets, businessmen and statesmen in the 19th century, and a high fashion color in the 20th century. According to surveys in Europe and North America, it is the color most commonly associated with mourning, the end, secrets, magic, force, violence, fear, evil, and elegance.\nBlack is the most common ink color used for printing books, newspapers and documents, as it provides the highest contrast with white paper and thus is the easiest color to read. Similarly, black text on a white screen is the most common format used on computer screens. the darkest material is made by MIT engineers from vertically aligned carbon nanotubes.\nEtymology.\nThe word \"black\" comes from Old English \"bl\u00e6c\" (\"black, dark\", \"also\", \"ink\"), from Proto-Germanic *\"blakkaz\" (\"burned\"), from Proto-Indo-European *\"bhleg-\" (\"to burn, gleam, shine, flash\"), from base *\"bhel-\" (\"to shine\"), related to Old Saxon \"blak\" (\"ink\"), Old High German \"blach\" (\"black\"), Old Norse \"blakkr\" (\"dark\"), Dutch \"blaken\" (\"to burn\"), and Swedish \"bl\u00e4ck\" (\"ink\"). More distant cognates include Latin \"flagrare\" (\"to blaze, glow, burn\"), and Ancient Greek \"phlegein\" (\"to burn, scorch\"). The Ancient Greeks sometimes used the same word to name different colors, if they had the same intensity. \"Kuanos\" could mean both dark blue and black. The Ancient Romans had two words for black: \"ater\" was a flat, dull black, while \"niger\" was a brilliant, saturated black. \"Ater\" has vanished from the vocabulary, but \"niger\" was the source of the country name \"Nigeria,\" the English word \"Negro\", and the word for \"black\" in most modern Romance languages (French: \"noir\"; Spanish and Portuguese: \"negro\"; Italian: \"nero\"; Romanian: \"negru\").\nOld High German also had two words for black: \"swartz\" for dull black and \"blach\" for a luminous black. These are parallelled in Middle English by the terms \"swart\" for dull black and \"blaek\" for luminous black. \"Swart\" still survives as the word \"swarthy\", while \"blaek\" became the modern English \"black\". The former is cognate with the words used for black in most modern Germanic languages aside from English (German: \"schwarz\", Dutch: \"zwart\", Swedish: \"svart\", Danish: \"sort\", Icelandic: \"svartr\"). In heraldry, the word used for the black color is sable, named for the black fur of the sable, an animal.\nArt.\nPrehistoric.\nBlack was one of the first colors used in art. The Lascaux Cave in France contains drawings of bulls and other animals drawn by paleolithic artists between 18,000 and 17,000 years ago. They began by using charcoal, and later achieved darker pigments by burning bones or grinding a powder of manganese oxide.\nAncient.\nFor the ancient Egyptians, black had positive associations; being the color of fertility and the rich black soil flooded by the Nile. It was the color of Anubis, the god of the underworld, who took the form of a black jackal, and offered protection against evil to the dead. To ancient Greeks, black represented the underworld, separated from the living by the river Acheron, whose water ran black. Those who had committed the worst sins were sent to Tartarus, the deepest and darkest level. In the center was the palace of Hades, the king of the underworld, where he was seated upon a black ebony throne. Black was one of the most important colors used by ancient Greek artists. In the 6th century BC, they began making black-figure pottery and later red figure pottery, using a highly original technique. In black-figure pottery, the artist would paint figures with a glossy clay slip on a red clay pot. When the pot was fired, the figures painted with the slip would turn black, against a red background. Later they reversed the process, painting the spaces between the figures with slip. This created magnificent red figures against a glossy black background.\nIn the social hierarchy of ancient Rome, purple was reserved for the emperor; red was the color worn by soldiers (red cloaks for the officers, red tunics for the soldiers); white the color worn by the priests, and black was worn by craftsmen and artisans. The black they wore was not deep and rich; the vegetable dyes used to make black were not solid or lasting, so the blacks often faded to gray or brown.\nIn Latin, the word for black, \"ater\" and to darken, \"atere\", were associated with cruelty, brutality and evil. They were the root of the English words \"atrocious\" and \"atrocity\". For the Romans, black symbolized death and mourning. In the 2nd century BC Roman magistrates wore a dark toga, called a \"toga pulla\", to funeral ceremonies. Later, under the Empire, the family of the deceased also wore dark colors for a long period; then, after a banquet to mark the end of mourning, exchanged the black for a white toga. In Roman poetry, death was called the \"hora nigra\", the black hour.\nThe German and Scandinavian peoples worshipped their own goddess of the night, N\u00f3tt, who crossed the sky in a chariot drawn by a black horse. They also feared Hel, the goddess of the kingdom of the dead, whose skin was black on one side and red on the other. They also held sacred the raven. They believed that Odin, the king of the Nordic pantheon, had two black ravens, Huginn and Muninn, who served as his agents, traveling the world for him, watching and listening.\nPostclassical.\nIn the early Middle Ages, black was commonly associated with darkness and evil. In Medieval paintings, the devil was usually depicted as having human form, but with wings and black skin or hair.\n12th and 13th centuries.\nIn fashion, black did not have the prestige of red, the color of the nobility. It was worn by Benedictine monks as a sign of humility and penitence. In the 12th century a famous theological dispute broke out between the Cistercian monks, who wore white, and the Benedictines, who wore black. A Benedictine abbot, Pierre the Venerable, accused the Cistercians of excessive pride in wearing white instead of black. Saint Bernard of Clairvaux, the founder of the Cistercians responded that black was the color of the devil, hell, \"of death and sin\", while white represented \"purity, innocence and all the virtues\".\nBlack symbolized both power and secrecy in the medieval world. The emblem of the Holy Roman Empire of Germany was a black eagle. The black knight in the poetry of the Middle Ages was an enigmatic figure, hiding his identity, usually wrapped in secrecy.\nBlack ink, invented in China, was traditionally used in the Middle Ages for writing, for the simple reason that black was the darkest color and therefore provided the greatest contrast with white paper or parchment, making it the easiest color to read. It became even more important in the 15th century, with the invention of printing. A new kind of ink, printer's ink, was created out of soot, turpentine and walnut oil. The new ink made it possible to spread ideas to a mass audience through printed books, and to popularize art through black and white prints. Because of its contrast and clarity, black ink on white paper continued to be the standard for printing books, newspapers and documents; and for the same reason black text on a white background is the most common format used on computer screens.\n14th and 15th centuries.\nIn the early Middle Ages, princes, nobles and the wealthy usually wore bright colors, particularly scarlet cloaks from Italy. Black was rarely part of the wardrobe of a noble family. The one exception was the fur of the sable. This glossy black fur, from an animal of the marten family, was the finest and most expensive fur in Europe. It was imported from Russia and Poland and used to trim the robes and gowns of royalty.\nIn the 14th century, the status of black began to change. First, high-quality black dyes began to arrive on the market, allowing garments of a deep, rich black. Magistrates and government officials began to wear black robes, as a sign of the importance and seriousness of their positions. A third reason was the passage of sumptuary laws in some parts of Europe which prohibited the wearing of costly clothes and certain colors by anyone except members of the nobility. The famous bright scarlet cloaks from Venice and the peacock blue fabrics from Florence were restricted to the nobility. The wealthy bankers and merchants of northern Italy responded by changing to black robes and gowns, made with the most expensive fabrics.\nThe change to the more austere but elegant black was quickly picked up by the kings and nobility. It began in northern Italy, where the Duke of Milan and the Count of Savoy and the rulers of Mantua, Ferrara, Rimini and Urbino began to dress in black. It then spread to France, led by Louis I, Duke of Orleans, younger brother of King Charles VI of France. It moved to England at the end of the reign of King Richard II (1377\u20131399), where all the court began to wear black. In 1419\u201320, black became the color of the powerful Duke of Burgundy, Philip the Good. It moved to Spain, where it became the color of the Spanish Habsburgs, of Charles V and of his son, Philip II of Spain (1527\u20131598). European rulers saw it as the color of power, dignity, humility and temperance. By the end of the 16th century, it was the color worn by almost all the monarchs of Europe and their courts.\nModern.\n16th and 17th centuries.\nWhile black was the color worn by the Catholic rulers of Europe, it was also the emblematic color of the Protestant Reformation in Europe and the Puritans in England and America. John Calvin, Philip Melanchthon and other Protestant theologians denounced the richly colored and decorated interiors of Roman Catholic churches. They saw the color red, worn by the pope and his cardinals, as the color of luxury, sin, and human folly. In some northern European cities, mobs attacked churches and cathedrals, smashed the stained glass windows and defaced the statues and decoration. In Protestant doctrine, clothing was required to be sober, simple and discreet. Bright colors were banished and replaced by blacks, browns and grays; women and children were recommended to wear white.\nIn the Protestant Netherlands, Rembrandt used this sober new palette of blacks and browns to create portraits whose faces emerged from the shadows expressing the deepest human emotions. The Catholic painters of the Counter-Reformation, like Rubens, went in the opposite direction; they filled their paintings with bright and rich colors. The new Baroque churches of the Counter-Reformation were usually shining white inside and filled with statues, frescoes, marble, gold and colorful paintings, to appeal to the public. But European Catholics of all classes, like Protestants, eventually adopted a sober wardrobe that was mostly black, brown and gray.\nIn the second part of the 17th century, Europe and America experienced an epidemic of fear of witchcraft. People widely believed that the devil appeared at midnight in a ceremony called a Black Mass or black sabbath, usually in the form of a black animal, often a goat, a dog, a wolf, a bear, a deer or a rooster, accompanied by their familiar spirits, black cats, serpents and other black creatures. This was the origin of the widespread superstition about black cats and other black animals. In medieval Flanders, in a ceremony called \"Kattenstoet,\" black cats were thrown from the belfry of the Cloth Hall of Ypres to ward off witchcraft.\nWitch trials were common in both Europe and America during this period. During the notorious Salem witch trials in New England in 1692\u201393, one of those on trial was accused of being able turn into a \"black thing with a blue cap,\" and others of having familiars in the form of a black dog, a black cat and a black bird. Nineteen women and men were hanged as witches.\n18th and 19th centuries.\nIn the 18th century, during the European Age of Enlightenment, black receded as a fashion color. Paris became the fashion capital, and pastels, blues, greens, yellow and white became the colors of the nobility and upper classes. But after the French Revolution, black again became the dominant color. Black was the color of the industrial revolution, largely fueled by coal, and later by oil. Thanks to coal smoke, the buildings of the large cities of Europe and America gradually turned black. By 1846 the industrial area of the West Midlands of England was \"commonly called 'the Black Country'\". Charles Dickens and other writers described the dark streets and smoky skies of London, and they were vividly illustrated in the wood-engravings of French artist Gustave Dor\u00e9.\nA different kind of black was an important part of the romantic movement in literature. Black was the color of melancholy, the dominant theme of romanticism. The novels of the period were filled with castles, ruins, dungeons, storms, and meetings at midnight. The leading poets of the movement were usually portrayed dressed in black, usually with a white shirt and open collar, and a scarf carelessly over their shoulder, Percy Bysshe Shelley and Lord Byron helped create the enduring stereotype of the romantic poet.\nThe invention of inexpensive synthetic black dyes and the industrialization of the textile industry meant that high-quality black clothes were available for the first time to the general population. In the 19th century black gradually became the most popular color of business dress of the upper and middle classes in England, the Continent, and America. Black dominated literature and fashion in the 19th century, and played a large role in painting. James McNeill Whistler made the color the subject of his most famous painting, \"Arrangement in grey and black number one\" (1871), better known as \"Whistler's Mother\".\nSome 19th-century French painters had a low opinion of black: \"Reject black,\" Paul Gauguin said, \"and that mix of black and white they call gray. Nothing is black, nothing is gray.\" But \u00c9douard Manet used blacks for their strength and dramatic effect. Manet's portrait of painter Berthe Morisot was a study in black which perfectly captured her spirit of independence. The black gave the painting power and immediacy; he even changed her eyes, which were green, to black to strengthen the effect. Henri Matisse quoted the French impressionist Pissarro telling him, \"Manet is stronger than us all \u2013 he made light with black.\"\nPierre-Auguste Renoir used luminous blacks, especially in his portraits. When someone told him that black was not a color, Renoir replied: \"What makes you think that? Black is the queen of colors. I always detested Prussian blue. I tried to replace black with a mixture of red and blue, I tried using cobalt blue or ultramarine, but I always came back to ivory black.\"\nVincent van Gogh used black lines to outline many of the objects in his paintings, such as the bed in the famous painting of his bedroom. making them stand apart. His painting of black crows over a cornfield, painted shortly before he died, was particularly agitated and haunting. In the late 19th century, black also became the color of anarchism. (See the section political movements.)\n20th and 21st centuries.\nIn the 20th century, black was utilised by Italian and German fascism. (See the section political movements). In art, the colour regained some of the territory that it had lost during the 19th century. The Russian painter Kasimir Malevich, a member of the Suprematist movement, created the \"Black Square\" in 1915, is widely considered the first purely abstract painting. He wrote, \"The painted work is no longer simply the imitation of reality, but is this very reality\u00a0... It is not a demonstration of ability, but the materialization of an idea.\"\nBlack was appreciated by Henri Matisse. \"When I didn't know what color to put down, I put down black,\" he said in 1945. \"Black is a force: I used black as ballast to simplify the construction\u00a0... Since the impressionists it seems to have made continuous progress, taking a more and more important part in color orchestration, comparable to that of the double bass as a solo instrument.\"\nIn the 1950s, black came to be a symbol of individuality and intellectual and social rebellion, the color of those who did not accept established norms and values. In Paris, it was worn by Left-Bank intellectuals and performers such as Juliette Gr\u00e9co, and by some members of the Beat Movement in New York and San Francisco. Black leather jackets were worn by motorcycle gangs such as the Hells Angels and street gangs on the fringes of society in the United States. Black as a color of rebellion was celebrated in such films as \"The Wild One\", with Marlon Brando. By the end of the 20th century, black was the emblematic color of the punk subculture punk fashion, and the goth subculture. Goth fashion, which emerged in England in the 1980s, was inspired by Victorian era mourning dress.\nIn men's fashion, black gradually ceded its dominance to navy blue, particularly in business suits. Black evening dress and formal dress in general were worn less and less. In 1960, John F. Kennedy was the last American President to be inaugurated wearing formal dress; Lyndon Johnson and his successors were inaugurated wearing business suits.\nWomen's fashion was revolutionized and simplified in 1926 by the French designer Coco Chanel, who published a drawing of a simple black dress in \"Vogue\" magazine. She famously said, \"A woman needs just three things; a black dress, a black sweater, and, on her arm, a man she loves.\" French designer Jean Patou also followed suit by creating a black collection in 1929. Other designers contributed to the trend of the little black dress. The Italian designer Gianni Versace said, \"Black is the quintessence of simplicity and elegance,\" and French designer Yves Saint Laurent said, \"black is the liaison which connects art and fashion. One of the most famous black dresses of the century was designed by Hubert de Givenchy and was worn by Audrey Hepburn in the 1961 film \"Breakfast at Tiffany's\".\nThe American civil rights movement in the 1950s was a struggle for the political equality of African Americans. It developed into the Black Power movement in the early 1960s until the late 1980s, and the Black Lives Matter movement in the 2010s and 2020s. It also popularized the slogan \"Black is Beautiful\".\nScience.\nPhysics.\nIn the visible spectrum, black is the result of the absorption of all light wavelengths. Black can be defined as the visual impression (or color) experienced when no visible light reaches the eye. Pigments or dyes that absorb light rather than reflect it back to the eye look black. A black pigment can, however, result from a combination of several pigments that collectively absorb all wavelengths of visible light. If appropriate proportions of three primary pigments are mixed, the result reflects so little light as to be called black. This provides two superficially opposite but actually complementary descriptions of black. Black is the color produced by the absorption of all wavelengths of visible light, or an exhaustive combination of multiple colors of pigment.\nIn physics, a black body is a perfect absorber of light, but, by a thermodynamic rule, it is also the best emitter. Thus, the best radiative cooling, out of sunlight, is by using black paint, though it is important that it be black (a nearly perfect absorber) in the infrared as well. In elementary science, far ultraviolet light is called \"black light\" because, while itself unseen, it causes many minerals and other substances to fluoresce.\nAbsorption of light is contrasted by transmission, reflection and diffusion, where the light is only redirected, causing objects to appear transparent, reflective or white respectively. A material is said to be black if most incoming light is absorbed equally in the material. Light (electromagnetic radiation in the visible spectrum) interacts with the atoms and molecules, which causes the energy of the light to be converted into other forms of energy, usually heat. This means that black surfaces can act as thermal collectors, absorbing light and generating heat (see Solar thermal collector).\nAs of September 2019, the darkest material is made from vertically aligned carbon nanotubes. The material was grown by MIT engineers and was reported to have a 99.995% absorption rate of any incoming light. This surpasses any former darkest materials including Vantablack, which has a peak absorption rate of 99.965% in the visible spectrum.\nChemistry.\nPigments.\nThe earliest pigments used by Neolithic man were charcoal, red ocher and yellow ocher. The black lines of cave art were drawn with the tips of burnt torches made of a wood with resin. Different charcoal pigments were made by burning different woods and animal products, each of which produced a different tone. The charcoal would be ground and then mixed with animal fat to make the pigment.\nThe 15th-century painter Cennino Cennini described how this pigment was made during the Renaissance in his famous handbook for artists: \"...there is a black which is made from the tendrils of vines. And these tendrils need to be burned. And when they have been burned, throw some water onto them and put them out and then mull them in the same way as the other black. And this is a lean and black pigment and is one of the perfect pigments that we use.\"\nCennini also noted that \"There is another black which is made from burnt almond shells or peaches and this is a perfect, fine black.\" Similar fine blacks were made by burning the pits of the peach, cherry or apricot. The powdered charcoal was then mixed with gum arabic or the yellow of an egg to make a paint.\nDifferent civilizations burned different plants to produce their charcoal pigments. The Inuit of Alaska used wood charcoal mixed with the blood of seals to paint masks and wooden objects. The Polynesians burned coconuts to produce their pigment.\nDyes.\nGood-quality black dyes were not known until the middle of the 14th century. The most common early dyes were made from bark, roots or fruits of different trees; usually walnuts, chestnuts, or certain oak trees. The blacks produced were often more gray, brown or bluish. The cloth had to be dyed several times to darken the color. One solution used by dyers was add to the dye some iron filings, rich in iron oxide, which gave a deeper black. Another was to first dye the fabric dark blue, and then to dye it black.\nA much richer and deeper black dye was eventually found made from the oak apple or \"gall-nut\". The gall-nut is a small round tumor which grows on oak and other varieties of trees. They range in size from 2\u20135\u00a0cm, and are caused by chemicals injected by the larva of certain kinds of gall wasp in the family Cynipidae. The dye was very expensive; a great quantity of gall-nuts were needed for a very small amount of dye. The gall-nuts which made the best dye came from Poland, eastern Europe, the near east and North Africa. Beginning in about the 14th century, dye from gall-nuts was used for clothes of the kings and princes of Europe.\nAnother important source of natural black dyes from the 17th century onwards was the logwood tree, or Haematoxylum campechianum, which also produced reddish and bluish dyes. It is a species of flowering tree in the legume family, Fabaceae, that is native to southern Mexico and northern Central America. The modern nation of Belize grew from 17th century English logwood logging camps.\nSince the mid-19th century, synthetic black dyes have largely replaced natural dyes. One of the important synthetic blacks is Nigrosin, a mixture of synthetic black dyes (CI 50415, Solvent black 5) made by heating a mixture of nitrobenzene, aniline and aniline hydrochloride in the presence of a copper or iron catalyst. Its main industrial uses are as a colorant for lacquers and varnishes and in marker-pen inks.\nInks.\nThe first known inks were made by the Chinese, and date back to the 23rd century B.C. They used natural plant dyes and minerals such as graphite ground with water and applied with an ink brush. Early Chinese inks similar to the modern inkstick have been found dating to about 256 BC at the end of the Warring States period. They were produced from soot, usually produced by burning pine wood, mixed with animal glue. To make ink from an inkstick, the stick is continuously ground against an inkstone with a small quantity of water to produce a dark liquid which is then applied with an ink brush. Artists and calligraphists could vary the thickness of the resulting ink by reducing or increasing the intensity and time of ink grinding. These inks produced the delicate shading and subtle or dramatic effects of Chinese brush painting.\nIndia ink (or \"Indian ink\" in British English) is a black ink once widely used for writing and printing and now more commonly used for drawing, especially when inking comic books and comic strips. The technique of making it probably came from China. India ink has been in use in India since at least the 4th century BC, where it was called \"masi\". In India, the black color of the ink came from bone char, tar, pitch and other substances.\nThe ancient Romans had a black writing ink they called \"atramentum librarium\". Its name came from the Latin word \"atrare\", which meant to make something black. (This was the same root as the English word \"atrocious\".) It was usually made, like India ink, from soot, although one variety, called \"atramentum elephantinum\", was made by burning the ivory of elephants.\nGall-nuts were also used for making fine black writing ink. Iron gall ink (also known as iron gall nut ink or oak gall ink) was a purple-black or brown-black ink made from iron salts and tannic acids from gall nut. It was the standard writing and drawing ink in Europe, from about the 12th century to the 19th century, and remained in use well into the 20th century.\nAstronomy.\nWhy the night sky and space are black \u2013 Olbers' paradox.\nThe fact that outer space is black is sometimes called Olbers' paradox. In theory, because the universe is full of stars, and is believed to be infinitely large, it would be expected that the light of an infinite number of stars would be enough to brilliantly light the whole universe all the time. However, the background color of outer space is black. This contradiction was first noted in 1823 by German astronomer Heinrich Wilhelm Matthias Olbers, who posed the question of why the night sky was black.\nThe current accepted answer is that, although the universe may be infinitely large, it is not infinitely old. It is thought to be about 13.8 billion years old, so we can only see objects as far away as the distance light can travel in 13.8 billion years. Light from stars farther away has not reached Earth, and cannot contribute to making the sky bright. Furthermore, as the universe is expanding, many stars are moving away from Earth. As they move, the wavelength of their light becomes longer, through the Doppler effect, and shifts toward red, or even becomes invisible. As a result of these two phenomena, there is not enough starlight to make space anything but black.\nThe daytime sky on Earth is blue because light from the Sun strikes molecules in Earth's atmosphere scattering light in all directions. Blue light is scattered more than other colors, and reaches the eye in greater quantities, making the daytime sky appear blue. This is known as Rayleigh scattering.\nThe nighttime sky on Earth is black because the part of Earth experiencing night is facing away from the Sun, the light of the Sun is blocked by Earth itself, and there is no other bright nighttime source of light in the vicinity. Thus, there is not enough light to undergo Rayleigh scattering and make the sky blue. On the Moon, on the other hand, because there is virtually no atmosphere to scatter the light, the sky is black both day and night. This also holds true for other locations without an atmosphere, such as Mercury.\nCulture.\nIn China, the color black is associated with water, one of the five fundamental elements believed to compose all things; and with winter, cold, and the direction north, usually symbolized by a black tortoise. It is also associated with disorder, including the positive disorder which leads to change and new life. When the first emperor of China Qin Shi Huang seized power from the Zhou dynasty, he changed the Imperial color from red to black, saying that black extinguished red. Only when the Han dynasty appeared in 206 BC was red restored as the imperial color.\nIn Japan, black is associated with mystery, the night, the unknown, the supernatural, the invisible and death. Combined with white, it can symbolize intuition. In 10th- and 11th-century Japan, it was believed that wearing black could bring misfortune. It was worn at court by those who wanted to set themselves apart from the established powers or who had renounced material possessions.\nIn Japan black can also symbolize experience, as opposed to white, which symbolizes naivet\u00e9. The black belt in martial arts symbolizes experience, while a white belt is worn by novices. Japanese men traditionally wear a black kimono with some white decoration on their wedding day.\nBlack is associated with depth in Indonesia, as well as the subterranean world, demons, disaster, and the left hand. When combined with white, however, it symbolizes harmony and equilibrium.\nPolitical movements.\nAnarchism.\nAnarchism is a political philosophy, most popular in the late 19th and early 20th centuries, which holds that governments and capitalism are harmful and undesirable. The symbols of anarchism was usually either a black flag or a black letter A. More recently it is usually represented with a bisected red and black flag, to emphasise the movement's socialist roots in the First International. Anarchism was most popular in Spain, France, Italy, Ukraine and Argentina. There were also small but influential movements in the United States, Russia and many other countries all around the world.\nFascism.\nThe Blackshirts () were Fascist paramilitary groups in Italy during the period immediately following World War I and until the end of World War II. The Blackshirts were officially known as the Voluntary Militia for National Security (\"Milizia Volontaria per la Sicurezza Nazionale\", or MVSN).\nInspired by the black uniforms of the Arditi, Italy's elite storm troops of World War I, the Fascist Blackshirts were organized by Benito Mussolini as the military tool of his political movement. They used violence and intimidation against Mussolini's opponents. The emblem of the Italian fascists was a black flag with fasces, an axe in a bundle of sticks, an ancient Roman symbol of authority. Mussolini came to power in 1922 through his March on Rome with the blackshirts.\nBlack was also adopted by Adolf Hitler and the Nazis in Germany. Red, white and black were the colors of the flag of the German Empire from 1870 to 1918. In \"Mein Kampf\", Hitler explained that they were \"revered colors expressive of our homage to the glorious past.\" Hitler also wrote that \"the new flag\u00a0... should prove effective as a large poster\" because \"in hundreds of thousands of cases a really striking emblem may be the first cause of awakening interest in a movement.\" The black swastika was meant to symbolize the Aryan race, which, according to the Nazis, \"was always anti-Semitic and will always be anti-Semitic.\" Several designs by a number of different authors were considered, but the one adopted in the end was Hitler's personal design. Black became the color of the uniform of the SS, the \"Schutzstaffel\" or \"defense corps\", the paramilitary wing of the Nazi Party, and was worn by SS officers from 1932 until the end of World War II.\nThe Nazis used a black triangle to symbolize anti-social elements. The symbol originates from Nazi concentration camps, where every prisoner had to wear one of the Nazi concentration camp badges on their jacket, the color of which categorized them according to \"their kind\". Many Black Triangle prisoners were either mentally disabled or mentally ill. The homeless were also included, as were alcoholics, the Romani people, the habitually \"work-shy\", prostitutes, draft dodgers and pacifists. More recently the black triangle has been adopted as a symbol in lesbian culture and by disabled activists.\nBlack shirts were also worn by the British Union of Fascists before World War II, and members of fascist movements in the Netherlands.\nPatriotic resistance.\nThe L\u00fctzow Free Corps, composed of volunteer German students and academics fighting against Napoleon in 1813, could not afford to make special uniforms and therefore adopted black, as the only color that could be used to dye their civilian clothing without the original color showing. In 1815 the students began to carry a red, black and gold flag, which they believed (incorrectly) had been the colors of the Holy Roman Empire (the imperial flag had actually been gold and black). In 1848, this banner became the flag of the German confederation. In 1866, Prussia unified Germany under its rule, and imposed the red, white and black of its own flag, which remained the colors of the German flag until the end of the Second World War. In 1949 the Federal Republic of Germany returned to the original flag and colors of the students and professors of 1815, which is the flag of Germany today.\nMilitary.\nBlack has been a traditional color of cavalry and armoured or mechanized troops. German armoured troops (Panzerwaffe) traditionally wore black uniforms, and even in others, a black beret is common. In Finland, black is the symbolic color for both armoured troops and combat engineers, and military units of these specialities have black flags and unit insignia.\nThe black beret and the color black is also a symbol of special forces in many countries. Soviet and Russian OMON special police and Russian naval infantry wear a black beret. A black beret is also worn by military police in the Canadian, Czech, Croatian, Portuguese, Spanish and Serbian armies.\nThe silver-on-black skull and crossbones symbol or Totenkopf and a black uniform were used by Hussars and Black Brunswickers, the German Panzerwaffe and the Nazi Schutzstaffel, and U.S. 400th Missile Squadron (crossed missiles), and continues in use with the Estonian Kuperjanov Battalion.\nReligion.\nIn Christian theology, black was the color of the universe before God created light. In many religious cultures, from Mesoamerica to Oceania to India and Japan, the world was created out of a primordial darkness. In the Bible the light of faith and Christianity is often contrasted with the darkness of ignorance and paganism.\nIn Christianity, the devil is often called the \"prince of darkness\". The term was used in John Milton's poem \"Paradise Lost\", published in 1667, referring to Satan, who is viewed as the embodiment of evil. It is an English translation of the Latin phrase \"princeps tenebrarum\", which occurs in the \"Acts of Pilate\", written in the fourth century, in the 11th-century hymn \"Rhythmus de die mortis\" by Pietro Damiani, and in a sermon by Bernard of Clairvaux from the 12th century. The phrase also occurs in \"King Lear\" by William Shakespeare (), Act III, Scene IV, l. 14:\n'The prince of darkness is a gentleman.\"\nPriests and pastors of the Roman Catholic, Eastern Orthodox and Protestant churches commonly wear black, as do monks of the Benedictine Order, who consider it the color of humility and penitence.\nAssociations and symbolism.\nIn the West, black is commonly associated with mourning and bereavement, and usually worn at funerals and memorial services. In some traditional societies, for example in Greece and Italy, some widows wear black for the rest of their lives. In contrast, across much of Africa and parts of Asia like Vietnam, white is a color of mourning.\nA \"black day\" (or week or month) usually refers to tragic date. The Romans marked \"fasti\" days with white stones and \"nefasti\" days with black. The term is often used to remember massacres. Black months include the Black September in Jordan, when large numbers of Palestinians were killed, and Black July in Sri Lanka, the killing of members of the Tamil population by the Sinhalese government. In the financial world, the term often refers to a dramatic drop in the stock market. For example, the Wall Street crash of 1929, the stock market crash on 29 October 1929, which marked the start of the Great Depression, is nicknamed Black Tuesday, and was preceded by Black Thursday, a downturn on 24 October the previous week.\nIn western popular culture, black has long been associated with evil and darkness. It is the traditional color of witchcraft and black magic.\nBlack is frequently used as a color of power, law and authority. In many countries judges and magistrates wear black robes. That custom began in Europe in the 13th and 14th centuries. Jurists, magistrates and certain other court officials in France began to wear long black robes during the reign of Philip IV of France (1285\u20131314), and in England from the time of Edward I (1271\u20131307). The custom spread to the cities of Italy at about the same time, between 1300 and 1320. The robes of judges resembled those worn by the clergy, and represented the law and authority of the King, while those of the clergy represented the law of God and authority of the church. Until the 20th century most police uniforms were black, until they were largely replaced by blue in France, the U.S. and other countries. In the United States, police cars are frequently Black and white. The riot control units of the Basque Autonomous Police in Spain are known as \"beltzak\" (\"blacks\") after their uniform.\nBlack formal attire is still worn at many solemn occasions or ceremonies, from graduations to formal balls. Graduation gowns are copied from the gowns worn by university professors in the Middle Ages, which in turn were copied from the robes worn by judges and priests, who often taught at the early universities. The mortarboard hat worn by graduates is adapted from a square cap called a biretta worn by Medieval professors and clerics.\nIn the 19th and 20th centuries, many machines and devices, large and small, were painted black, to stress their functionality. These included telephones, sewing machines, steamships, railroad locomotives, and automobiles. The Ford Model T, the first mass-produced car, was available only in black from 1914 to 1926. Of means of transportation, only airplanes were rarely ever painted black.\nThe term \"Black\" is often used in the West to describe people whose skin is darker. In the United States, it is particularly used to describe African Americans. Black is also commonly used as a racial description in the United Kingdom, since ethnicity was first measured in the 2001 census. In Canada, census respondents can identify themselves as Black. In Brazil, the Brazilian Institute of Geography and Statistics (IBGE) asks people to identify themselves as \"branco\" (white), \"pardo\" (brown), \"preto\" (black), or \"amarelo\" (yellow).\nBlack and white have often been used to describe opposites, particularly light and darkness and good and evil. In Medieval literature, the white knight usually represented virtue, the black knight something mysterious and sinister. In American westerns, the hero often wore a white hat, the villain a black hat. \nIn philosophy and arguments, the issue is often described as black-and-white, meaning that the issue at hand is dichotomized (having two clear, opposing sides with no middle ground).\nBlack is commonly associated with secrecy.\nBlack is the color most commonly associated with elegance in Europe and the United States. Black first became a fashionable color for men in Europe in the 17th century, in the courts of Italy and Spain. In the 19th century, it was the fashion for men both in business and for evening wear. For women's fashion, the defining moment was the invention of the simple black dress by Coco Chanel in 1926. Thereafter, a long black gown was used for formal occasions, while the simple black dress could be used for everything else. The expression \"X is the new black\" is a reference to the latest trend or fad that is considered a wardrobe basic for the duration of the trend, on the basis that black is always fashionable. The phrase has taken on a life of its own and has become a clich\u00e9."}
{"id": "4036", "revid": "8524693", "url": "https://en.wikipedia.org/wiki?curid=4036", "title": "Black Flag", "text": "Black Flag or black flag may refer to:"}
{"id": "4037", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=4037", "title": "Bletchley Park", "text": "Bletchley Park is an English country house and estate in Bletchley, Milton Keynes (Buckinghamshire), that became the principal centre of Allied code-breaking during the Second World War. The mansion was constructed during the years following 1883 for the financier and politician Herbert Leon in the Victorian Gothic, Tudor and Dutch Baroque styles, on the site of older buildings of the same name.\nDuring World War II, the estate housed the Government Code and Cypher School (GC&amp;CS), which regularly penetrated the secret communications of the Axis Powers most importantly the German Enigma and Lorenz ciphers. The GC&amp;CS team of codebreakers included John Tiltman, Dilwyn Knox, Alan Turing, Harry Golombek, Gordon Welchman, Hugh Alexander, Donald Michie, Bill Tutte and Stuart Milner-Barry.\nAccording to the official historian of British Intelligence, the \"Ultra\" intelligence produced at Bletchley shortened the war by two to four years, and without it the outcome of the war would have been uncertain. The team at Bletchley Park devised automatic machinery to help with decryption, culminating in the development of Colossus, the world's first programmable digital electronic computer. Codebreaking operations at Bletchley Park came to an end in 1946 and all information about the wartime operations was classified until the mid-1970s.\nAfter the war it had various uses including as a teacher-training college and local GPO headquarters. By 1990 the huts in which the codebreakers worked were being considered for demolition and redevelopment. The Bletchley Park Trust was formed in February 1992 to save large portions of the site from development.\nMore recently, Bletchley Park has been open to the public, featuring interpretive exhibits and huts that have been rebuilt to appear as they did during their wartime operations. It receives hundreds of thousands of visitors annually. The separate National Museum of Computing, which includes a working replica Bombe machine and a rebuilt Colossus computer, is housed in Block H on the site.\nHistory.\nThe site appears in the Domesday Book of 1086 as part of the Manor of Eaton. Browne Willis built a mansion there in 1711, but after Thomas Harrison purchased the property in 1793 this was pulled down. It was first known as Bletchley Park after its purchase by the architect Samuel Lipscomb Seckham in 1877, who built a house there. The estate of was bought in 1883 by Sir Herbert Samuel Leon, who expanded the then-existing house into what architect Landis Gores called a \"maudlin and monstrous pile\" combining Victorian Gothic, Tudor, and Dutch Baroque styles. At his Christmas family gatherings there was a fox hunting meet on Boxing Day with glasses of sloe gin from the butler, and the house was always \"humming with servants\". With 40 gardeners, a flower bed of yellow daffodils could become a sea of red tulips overnight. After the death of Herbert Leon in 1926, the estate continued to be occupied by his widow Fanny Leon (n\u00e9e Higham) until her death in 1937.\nIn 1938, the mansion and much of the site was bought by a builder for a housing estate, but in May 1938 Admiral Sir Hugh Sinclair, head of the Secret Intelligence Service (SIS or MI6), bought the mansion and of land for \u00a36,000 (\u00a3 today) for use by GC&amp;CS and SIS in the event of war. He used his own money as the Government said they did not have the budget to do so.\nA key advantage seen by Sinclair and his colleagues (inspecting the site under the cover of \"Captain Ridley's shooting party\") was Bletchley's geographical centrality. It was almost immediately adjacent to Bletchley railway station, where the \"Varsity Line\" between Oxford and Cambridgewhose universities were expected to supply many of the code-breakersmet the main West Coast railway line connecting London, Birmingham, Manchester, Liverpool, Glasgow and Edinburgh. Watling Street, the main road linking London to the north-west (subsequently the A5) was close by, and high-volume communication links were available at the telegraph and telephone repeater station in nearby Fenny Stratford.\nBletchley Park was known as \"B.P.\" to those who worked there.\n\"Station X\" (X = Roman numeral ten), \"London Signals Intelligence Centre\", and \"Government Communications Headquarters\" were all cover names used during the war.\nThe formal posting of the many \"Wrens\"members of the Women's Royal Naval Serviceworking there, was to HMS \"Pembroke V\". Royal Air Force names of Bletchley Park and its outstations included RAF Eastcote, RAF Lime Grove and RAF Church Green. The postal address that staff had to use was \"Room 47, Foreign Office\".\nAfter the war, the Government Code &amp; Cypher School became the Government Communications Headquarters (GCHQ), moving to Eastcote in 1946 and to Cheltenham in the 1950s. The site was used by various government agencies, including the GPO and the Civil Aviation Authority. One large building, block F, was demolished in 1987 by which time the site was being run down with tenants leaving.\nIn 1990 the site was at risk of being sold for housing development. However, Milton Keynes Council made it into a conservation area. Bletchley Park Trust was set up in 1991 by a group of people who recognised the site's importance. The initial trustees included Roger Bristow, Ted Enever, Peter Wescombe, Dr Peter Jarvis of the Bletchley Archaeological &amp; Historical Society, and Tony Sale who in 1994 became the first director of the Bletchley Park Museums.\nPersonnel.\nAdmiral Hugh Sinclair was the founder and head of GC&amp;CS between 1919 and 1938 with Commander Alastair Denniston being operational head of the organization from 1919 to 1942, beginning with its formation from the Admiralty's Room 40 (NID25) and the War Office's MI1b. Key GC&amp;CS cryptanalysts who moved from London to Bletchley Park included John Tiltman, Dillwyn \"Dilly\" Knox, Josh Cooper, Oliver Strachey and Nigel de Grey. These people had a variety of backgroundslinguists and chess champions were common, and Knox's field was papyrology. The British War Office recruited top solvers of cryptic crossword puzzles, as these individuals had strong lateral thinking skills.\nOn the day Britain declared war on Germany, Denniston wrote to the Foreign Office about recruiting \"men of the professor type\". Personal networking drove early recruitments, particularly of men from the universities of Cambridge and Oxford. Trustworthy women were similarly recruited for administrative and clerical jobs. In one 1941 recruiting stratagem, \"The Daily Telegraph\" was asked to organise a crossword competition, after which promising contestants were discreetly approached about \"a particular type of work as a contribution to the war effort\".\nDenniston recognised, however, that the enemy's use of electromechanical cipher machines meant that formally trained mathematicians would also be needed; Oxford's Peter Twinn joined GC&amp;CS in February 1939; Cambridge's Alan Turing and Gordon Welchman began training in 1938 and reported to Bletchley the day after war was declared, along with John Jeffreys. Later-recruited cryptanalysts included the mathematicians Derek Taunt, Jack Good, Bill Tutte, and Max Newman; historian Harry Hinsley, and chess champions Hugh Alexander and Stuart Milner-Barry. Joan Clarke was one of the few women employed at Bletchley as a full-fledged cryptanalyst.\nWhen seeking to recruit more suitably advanced linguists, John Tiltman turned to Patrick Wilkinson of the Italian section for advice, and he suggested asking Lord Lindsay of Birker, of Balliol College, Oxford, S. W. Grose, and Martin Charlesworth, of St John's College, Cambridge, to recommend classical scholars or applicants to their colleges.\nThis eclectic staff of \"Boffins and Debs\" (scientists and debutantes, young women of high society) caused GC&amp;CS to be whimsically dubbed the \"Golf, Cheese and Chess Society\". Among those who worked there and later became famous in other fields were historian Asa Briggs, politician Roy Jenkins and novelist Angus Wilson.\nDuring a morale-boosting visit on 9 September 1941, Winston Churchill reportedly remarked to Denniston or Menzies: \"I told you to leave no stone unturned to get staff, but I had no idea you had taken me so literally.\" Six weeks later, having failed to get sufficient typing and unskilled staff to achieve the productivity that was possible, Turing, Welchman, Alexander and Milner-Barry wrote directly to Churchill. His response was \"Action this day make sure they have all they want on extreme priority and report to me that this has been done.\"\nAfter initial training at the Inter-Service Special Intelligence School set up by John Tiltman (initially at an RAF depot in Buckingham and later in Bedfordwhere it was known locally as \"the Spy School\") staff worked a six-day week, rotating through three shifts: 4\u00a0p.m. to midnight, midnight to 8\u00a0a.m. (the most disliked shift), and 8\u00a0a.m. to 4\u00a0p.m., each with a half-hour meal break. At the end of the third week, a worker went off at 8\u00a0a.m. and came back at 4\u00a0p.m., thus putting in 16 hours on that last day. The irregular hours affected workers' health and social life, as well as the routines of the nearby homes at which most staff lodged. The work was tedious and demanded intense concentration; staff got one week's leave four times a year, but some \"girls\" collapsed and required extended rest. Recruitment took place to combat a shortage of experts in Morse code and German.\nIn January 1945, at the peak of codebreaking efforts, nearly 10,000 personnel were working at Bletchley and its outstations. About three-quarters of these were women. Many of the women came from middle-class backgrounds and held degrees in the areas of mathematics, physics and engineering; they were given chance due to the lack of men, who had been sent to war. They performed calculations and coding and hence were integral to the computing processes. Among them were Eleanor Ireland, who worked on the Colossus computers and Ruth Briggs, a German scholar, who worked within the Naval Section.\nThe female staff in Dilwyn Knox's section were sometimes termed \"Dilly's Fillies\". Knox's methods enabled Mavis Lever (who married mathematician and fellow code-breaker Keith Batey) and Margaret Rock to solve a German code, the Abwehr cipher.\nMany of the women had backgrounds in languages, particularly French, German and Italian. Among them were Rozanne Colchester, a translator who worked mainly for the Italian air forces Section, and Cicely Mayhew, recruited straight from university, who worked in Hut 8, translating decoded German Navy signals, as did Jane Fawcett (n\u00e9e Hughes) who decrypted a vital message concerning the German battleship \"Bismarck\" and after the war became an opera singer and buildings conservationist.\nAlan Brooke (CIGS) in his secret wartime diary frequently refers to \u201cintercepts\u201d:\nFor a long time, the British Government failed to acknowledge the contributions the personnel at Bletchley Park had made. Their work achieved official recognition only in 2009.\nSecrecy.\nProperly used, the German Enigma and Lorenz ciphers should have been virtually unbreakable, but flaws in German cryptographic procedures, and poor discipline among the personnel carrying them out, created vulnerabilities that made Bletchley's attacks just barely feasible. These vulnerabilities, however, could have been remedied by relatively simple improvements in enemy procedures, and such changes would certainly have been implemented had Germany had any hint of Bletchley's success. Thus the intelligence Bletchley produced was considered wartime Britain's \"Ultra secret\"higher even than the normally highest classification and security was paramount.\nAll staff signed the Official Secrets Act (1939) and a 1942 security warning emphasised the importance of discretion even within Bletchley itself: \"Do not talk at meals. Do not talk in the transport. Do not talk travelling. Do not talk in the billet. Do not talk by your own fireside. Be careful even in your Hut\u00a0...\"\nNevertheless, there were security leaks. Jock Colville, the Assistant Private Secretary to Winston Churchill, recorded in his diary on 31 July 1941, that the newspaper proprietor Lord Camrose had discovered Ultra and that security leaks \"increase in number and seriousness\". Without doubt, the most serious of these was that Bletchley Park had been infiltrated by John Cairncross, the notorious Soviet mole and member of the Cambridge Spy Ring, who leaked Ultra material to Moscow.\nDespite the high degree of secrecy surrounding Bletchley Park during the Second World War, unique and hitherto unknown amateur film footage of the outstation at nearby Whaddon Hall came to light in 2020, after being anonymously donated to the Bletchley Park Trust. A spokesman for the Trust noted the film's existence was all the more incredible because it was \"very, very rare even to have [still] photographs\" of the park and its associated sites.\nEarly work.\nThe first personnel of the Government Code and Cypher School (GC&amp;CS) moved to Bletchley Park on 15 August 1939. The Naval, Military, and Air Sections were on the ground floor of the mansion, together with a telephone exchange, teleprinter room, kitchen, and dining room; the top floor was allocated to MI6. Construction of the wooden huts began in late 1939, and Elmers School, a neighbouring boys' boarding school in a Victorian Gothic redbrick building by a church, was acquired for the Commercial and Diplomatic Sections.\nAfter the United States joined World War II, a number of American cryptographers were posted to Hut 3, and from May 1943 onwards there was close co-operation between British and American intelligence. (See 1943 BRUSA Agreement.) In contrast, the Soviet Union was never officially told of Bletchley Park and its activities, a reflection of Churchill's distrust of the Soviets even during the US-UK-USSR alliance imposed by the Nazi threat.\nThe only direct enemy damage to the site was done 2021 November 1940 by three bombs probably intended for Bletchley railway station; Hut\u00a04, shifted two feet off its foundation, was winched back into place as\nwork inside continued.\nIntelligence reporting.\nInitially, when only a very limited amount of Enigma traffic was being read, deciphered non-Naval Enigma messages were sent from Hut\u00a06 to Hut 3 which handled their translation and onward transmission. Subsequently, under Group Captain Eric Jones, Hut 3 expanded to become the heart of Bletchley Park's intelligence effort, with input from decrypts of \"Tunny\" (Lorenz SZ42) traffic and many other sources. Early in 1942 it moved into Block D, but its functions were still referred to as Hut 3.\nHut 3 contained a number of sections: Air Section \"3A\", Military Section \"3M\", a small Naval Section \"3N\", a multi-service Research Section \"3G\" and a large liaison section \"3L\". It also housed the Traffic Analysis Section, SIXTA. An important function that allowed the synthesis of raw messages into valuable Military intelligence was the indexing and cross-referencing of information in a number of different filing systems. Intelligence reports were sent out to the Secret Intelligence Service, the intelligence chiefs in the relevant ministries, and later on to high-level commanders in the field.\nNaval Enigma deciphering was in Hut\u00a08, with translation in Hut\u00a04. Verbatim translations were sent to the Naval Intelligence Division (NID) of the Admiralty's Operational Intelligence Centre (OIC), supplemented by information from indexes as to the meaning of technical terms and cross-references from a knowledge store of German naval technology. Where relevant to non-naval matters, they would also be passed to Hut 3. Hut 4 also decoded a manual system known as the dockyard cipher, which sometimes carried messages that were also sent on an Enigma network. Feeding these back to Hut\u00a08 provided excellent \"cribs\" for Known-plaintext attacks on the daily naval Enigma key.\nListening stations.\nInitially, a wireless room was established at Bletchley Park.\nIt was set up in the mansion's water tower under the code name \"Station\u00a0X\", a term now sometimes applied to the codebreaking efforts at Bletchley as a whole. The \"X\" is the Roman numeral \"ten\", this being the Secret Intelligence Service's tenth such station. Due to the long radio aerials stretching from the wireless room, the radio station was moved from Bletchley Park to nearby Whaddon Hall to avoid drawing attention to the site.\nSubsequently, other listening stationsthe Y-stations, such as the ones at Chicksands in Bedfordshire, Beaumanor Hall, Leicestershire (where the headquarters of the War Office \"Y\" Group was located) and Beeston Hill Y Station in Norfolkgathered raw signals for processing at Bletchley. Coded messages were taken down by hand and sent to Bletchley on paper by motorcycle despatch riders or (later) by teleprinter.\nAdditional buildings.\nThe wartime needs required the building of additional accommodation.\nHuts.\nOften a hut's number became so strongly associated with the work performed inside that even when the work was moved to another building it was still referred to by the original \"Hut\" designation.\nBlocks.\nIn addition to the wooden huts, there were a number of brick-built \"blocks\".\nWork on specific countries' signals.\nGerman signals.\nMost German messages decrypted at Bletchley were produced by one or another version of the Enigma cipher machine, but an important minority were produced by the even more complicated twelve-rotor Lorenz SZ42 on-line teleprinter cipher machine used for high command messages, known as Fish.\nFive weeks before the outbreak of war, Warsaw's Cipher Bureau revealed its achievements in breaking Enigma to astonished French and British personnel. The British used the Poles' information and techniques, and the Enigma clone sent to them in August 1939, which greatly increased their (previously very limited) success in decrypting Enigma messages.\nThe bombe was an electromechanical device whose function was to discover some of the daily settings of the Enigma machines on the various German military networks. Its pioneering design was developed by Alan Turing (with an important contribution from Gordon Welchman) and the machine was engineered by Harold 'Doc' Keen of the British Tabulating Machine Company. Each machine was about high and wide, deep and weighed about a ton.\nAt its peak, GC&amp;CS was reading approximately 4,000 messages per day. As a hedge against enemy attack most bombes were dispersed to installations at Adstock and Wavendon (both later supplanted by installations at Stanmore and Eastcote), and Gayhurst.\nLuftwaffe messages were the first to be read in quantity. The German navy had much tighter procedures, and the capture of code books was needed before they could be broken. When, in February 1942, the German navy introduced the four-rotor Enigma for communications with its Atlantic U-boats, this traffic became unreadable for a period of ten months. Britain produced modified bombes, but it was the success of the US Navy Bombe that was the main source of reading messages from this version of Enigma for the rest of the war. Messages were sent to and fro across the Atlantic by enciphered teleprinter links.\nThe Lorenz messages were codenamed \"Tunny\" at Bletchley Park. They were only sent in quantity from mid-1942. The Tunny networks were used for high-level messages between German High Command and field commanders. With the help of German operator errors, the cryptanalysts in the Testery (named after Ralph Tester, its head) worked out the logical structure of the machine despite not knowing its physical form. They devised automatic machinery to help with decryption, which culminated in Colossus, the world's first programmable digital electronic computer. This was designed and built by Tommy Flowers and his team at the Post Office Research Station at Dollis Hill. The prototype first worked in December 1943, was delivered to Bletchley Park in January and first worked operationally on 5 February 1944. Enhancements were developed for the Mark 2 Colossus, the first of which was working at Bletchley Park on the morning of 1 June in time for D-day. Flowers then produced one Colossus a month for the rest of the war, making a total of ten with an eleventh part-built. The machines were operated mainly by Wrens in a section named the Newmanry after its head Max Newman.\nBletchley's work was essential to defeating the U-boats in the Battle of the Atlantic, and to the British naval victories in the Battle of Cape Matapan and the Battle of North Cape. In 1941, Ultra exerted a powerful effect on the North African desert campaign against German forces under General Erwin Rommel. General Sir Claude Auchinleck wrote that were it not for Ultra, \"Rommel would have certainly got through to Cairo\". While not changing the events, \"Ultra\" decrypts featured prominently in the story of Operation SALAM, L\u00e1szl\u00f3 Alm\u00e1sy's mission across the desert behind Allied lines in 1942. Prior to the Normandy landings on D-Day in June 1944, the Allies knew the locations of all but two of Germany's fifty-eight Western-front divisions.\nItalian signals.\nItalian signals had been of interest since Italy's attack on Abyssinia in 1935.\nDuring the Spanish Civil War the Italian Navy used the K model of the commercial Enigma without a plugboard; this was solved by Knox in 1937.\nWhen Italy entered the war in 1940 an improved version of the machine was used, though little traffic was sent by it and there were \"wholesale changes\" in Italian codes and cyphers.\nKnox was given a new section for work on Enigma variations, which he staffed with women (\"Dilly's girls\"), who included Margaret Rock, Jean Perrin, Clare Harding, Rachel Ronald, Elisabeth Granger; and Mavis Lever.\nMavis Lever solved the signals revealing the Italian Navy's operational plans before the Battle of Cape Matapan in 1941, leading to a British victory.\nAlthough most Bletchley staff did not know the results of their work, Admiral Cunningham visited Bletchley in person a few weeks later to congratulate them.\nOn entering World War II in June 1940, the Italians were using book codes for most of their military messages. The exception was the Italian Navy, which after the Battle of Cape Matapan started using the C-38 version of the Boris Hagelin rotor-based cipher machine, particularly to route their navy and merchant marine convoys to the conflict in North Africa. As a consequence, JRM Butler recruited his former student Bernard Willson to join a team with two others in Hut\u00a04. In June 1941, Willson became the first of the team to decode the Hagelin system, thus enabling military commanders to direct the Royal Navy and Royal Air Force to sink enemy ships carrying supplies from Europe to Rommel's Afrika Korps. This led to increased shipping losses and, from reading the intercepted traffic, the team learnt that between May and September 1941 the stock of fuel for the Luftwaffe in North Africa reduced by 90 per cent.\nAfter an intensive language course, in March 1944 Willson switched to Japanese language-based codes.\nA Middle East Intelligence Centre (MEIC) was set up in Cairo in 1939. When Italy entered the war in June 1940, delays in forwarding intercepts to Bletchley via congested radio links resulted in cryptanalysts being sent to Cairo. A Combined Bureau Middle East (CBME) was set up in November, though the Middle East authorities made \"increasingly bitter complaints\" that GC&amp;CS was giving too little priority to work on Italian cyphers. However, the principle of concentrating high-grade cryptanalysis at Bletchley was maintained. John Chadwick started cryptanalysis work in 1942 on Italian signals at the naval base 'HMS Nile' in Alexandria. Later, he was with GC&amp;CS; in the Heliopolis Museum, Cairo and then in the Villa Laurens, Alexandria.\nSoviet signals.\nSoviet signals had been studied since the 1920s. In 193940, John Tiltman (who had worked on Russian Army traffic from 1930) set up two Russian sections at Wavendon (a country house near Bletchley) and at Sarafand in Palestine. Two Russian high-grade army and navy systems were broken early in 1940. Tiltman spent two weeks in Finland, where he obtained Russian traffic from Finland and Estonia in exchange for radio equipment. In June 1941, when the Soviet Union became an ally, Churchill ordered a halt to intelligence operations against it. In December 1941, the Russian section was closed down, but in late summer 1943 or late 1944, a small GC&amp;CS Russian cypher section was set up in London overlooking Park Lane, then in Sloane Square.\nJapanese signals.\nAn outpost of the Government Code and Cypher School had been set up in Hong Kong in 1935, the Far East Combined Bureau (FECB). The FECB naval staff moved in 1940 to Singapore, then Colombo, Ceylon, then Kilindini, Mombasa, Kenya. They succeeded in deciphering Japanese codes with a mixture of skill and good fortune. The Army and Air Force staff went from Singapore to the Wireless Experimental Centre at Delhi, India.\nIn early 1942, a six-month crash course in Japanese, for 20 undergraduates from Oxford and Cambridge, was started by the Inter-Services Special Intelligence School in Bedford, in a building across from the main Post Office. This course was repeated every six months until war's end. Most of those completing these courses worked on decoding Japanese naval messages in Hut\u00a07, under John Tiltman.\nBy mid-1945, well over 100 personnel were involved with this operation, which co-operated closely with the FECB and the US Signal intelligence Service at Arlington Hall, Virginia. In 1999, Michael Smith wrote that: \"Only now are the British codebreakers (like John Tiltman, Hugh Foss, and Eric Nave) beginning to receive the recognition they deserve for breaking Japanese codes and cyphers\".\nPostwar.\nContinued secrecy.\nAfter the War, the secrecy imposed on Bletchley staff remained in force, so that most relatives never knew more than that a child, spouse, or parent had done some kind of secret war work. Churchill referred to the Bletchley staff as \"the geese that laid the golden eggs and never cackled\". That said, occasional mentions of the work performed at Bletchley Park slipped the censor's net and appeared in print.\nWith the publication of F.\u00a0W. Winterbotham's \"The Ultra Secret\" (1974) public discussion of Bletchley Park's work finally became possible, although some former staff considered themselves bound to silence forever.\nProfessor Brian Randell was researching the history of computer science in Britain in 1975\u201376 for a conference on the history of computing held at the Los Alamos National Laboratory, New Mexico on 10\u201315 June 1976, and received permission to present a paper on wartime development of the Colossi at the Post Office Research Station, Dollis Hill. (In October 1975 the British Government had released a series of captioned photographs from the Public Record Office.) The interest in the \"revelations\" in his paper resulted in a special evening meeting when Randell and Coombs answered further questions. Coombs later wrote that \"no member of our team could ever forget the fellowship, the sense of purpose and, above all, the breathless excitement of those days\". In 1977 Randell published an article \"The First Electronic Computer\" in several journals.\nIn July 2009 the British government announced that Bletchley personnel would be recognised with a commemorative badge.\nSite.\nAfter the war, the site passed through a succession of hands and saw a number of uses, including as a teacher-training college and local GPO headquarters. By 1991, the site was nearly empty and the buildings were at risk of demolition for redevelopment.\nIn February 1992, the Milton Keynes Borough Council declared most of the Park a conservation area, and the Bletchley Park Trust was formed to maintain the site as a museum. The site opened to visitors in 1993, and was formally inaugurated by the Duke of Kent as Chief Patron in July 1994. In 1999 the land owners, the Property Advisors to the Civil Estate and BT, granted a lease to the Trust giving it control over most of the site.\nHeritage attraction.\nJune 2014 saw the completion of an \u00a38 million restoration project by museum design specialist, Event Communications, which was marked by a visit from Catherine, Duchess of Cambridge. The Duchess' paternal grandmother, Valerie, and Valerie's twin sister, Mary (\"n\u00e9e\" Glassborow), both worked at Bletchley Park during the war. The twin sisters worked as Foreign Office Civilians in Hut 6, where they managed the interception of enemy and neutral diplomatic signals for decryption. Valerie married Catherine's grandfather, Captain Peter Middleton. A memorial at Bletchley Park commemorates Mary and Valerie Middleton's work as code-breakers.\nLearning Department.\nThe Bletchley Park Learning Department offers educational group visits with active learning activities for schools and universities. Visits can be booked in advance during term time, where students can engage with the history of Bletchley Park and understand its wider relevance for computer history and national security. Their workshops cover introductions to codebreaking, cyber security and the story of Enigma and Lorenz.\nFunding.\nIn October 2005, American billionaire Sidney Frank donated \u00a3500,000 to Bletchley Park Trust to fund a new Science Centre dedicated to Alan Turing. Simon Greenish joined as Director in 2006 to lead the fund-raising effort in a post he held until 2012 when Iain Standen took over the leadership role. In July 2008, a letter to \"The Times\" from more than a hundred academics condemned the neglect of the site. In September 2008, PGP, IBM and other technology firms announced a fund-raising campaign to repair the facility. On 6 November 2008 it was announced that English Heritage would donate \u00a3300,000 to help maintain the buildings at Bletchley Park, and that they were in discussions regarding the donation of a further \u00a3600,000.\nIn October 2011, the Bletchley Park Trust received a \u00a34.6 million Heritage Lottery Fund grant to be used \"to complete the restoration of the site, and to tell its story to the highest modern standards\" on the condition that \u00a31.7 million of match funding is raised by the Bletchley Park Trust. Just weeks later, Google contributed \u00a3550,000 and by June 2012 the trust had successfully raised \u00a32.4 million to unlock the grants to restore Huts 3 and 6, as well as develop its exhibition centre in Block C.\nAdditional income is raised by renting Block H to the National Museum of Computing, and some office space in various parts of the park to private firms.\nDue to the COVID-19 pandemic the Trust expected to lose more than \u00a32 million in 2020 and be required to cut a third of its workforce. Former MP John Leech asked Amazon, Apple, Google, Facebook and Microsoft to donate \u00a3400,000 each to secure the future of the Trust. Leech had led the successful campaign to pardon Alan Turing and implement Turing's Law.\nOther organisations sharing the campus.\nThe National Museum of Computing.\nThe National Museum of Computing is housed in Block H, which is rented from the Bletchley Park Trust. Its Colossus and Tunny galleries tell an important part of allied breaking of German codes during World War II. There is a working reconstruction of a Bombe and a rebuilt Colossus computer which was used on the high-level Lorenz cipher, codenamed \"Tunny\" by the British.\nThe museum, which opened in 2007, is an independent voluntary organisation that is governed by its own board of trustees. Its aim is \"To collect and restore computer systems particularly those developed in Britain and to enable people to explore that collection for inspiration, learning and enjoyment.\" Through its many exhibits, the museum displays the story of computing through the mainframes of the 1960s and 1970s, and the rise of personal computing in the 1980s. It has a policy of having as many of the exhibits as possible in full working order.\nScience and Innovation Centre.\nThis consisted of serviced office accommodation housed in Bletchley Park's Blocks A and E, and the upper floors of the Mansion. Its aim was to foster the growth and development of dynamic knowledge-based start-ups and other businesses. It closed in 2021 and blocks A and E were taken into use as part of the museum.\nProposed National College of Cyber Security.\nIn April 2020 Bletchley Park Capital Partners, a private company run by Tim Reynolds, Deputy Chairman of the National Museum of Computing, announced plans to sell off the freehold to part of the site containing former Block G for commercial development. Offers of between \u00a34 million and \u00a36 million were reportedly being sought for the 3 acre plot, for which planning permission for employment purposes was granted in 2005. Previously, the construction of a National College of Cyber Security for students aged from 16 to 19 years old had been envisaged on the site, to be housed in Block G after renovation with funds supplied by the Bletchley Park Science and Innovation Centre.\nRSGB National Radio Centre.\nThe Radio Society of Great Britain's National Radio Centre (including a library, radio station, museum and bookshop) are in a newly constructed building close to the main Bletchley Park entrance.\nFinal recognition.\nNot until July 2009 did the British government fully acknowledge the contribution of the many people working for the Government Code and Cypher School (GC&amp;CS) at Bletchley. Only then was a commemorative medal struck to be presented to those involved. The gilded medal bears the inscription \"GC&amp;CS 1939\u20131945 Bletchley Park and its Outstations\".\nLocation.\nBletchley Park is opposite Bletchley railway station. It is close to junctions 13 and 14 of the M1, about northwest of London.\nExternal links.\nMaps"}
{"id": "4038", "revid": "310173", "url": "https://en.wikipedia.org/wiki?curid=4038", "title": "Banach Tarski Paradoxical Decomposition", "text": ""}
{"id": "4040", "revid": "8029", "url": "https://en.wikipedia.org/wiki?curid=4040", "title": "B.C", "text": ""}
{"id": "4041", "revid": "14984434", "url": "https://en.wikipedia.org/wiki?curid=4041", "title": "Bede", "text": "Bede (; ; 672/326 May 735), also known as Saint Bede, the Venerable Bede, and Bede the Venerable (), was an English monk, author and scholar. He was one of the greatest teachers and writers during the Early Middle Ages, and his most famous work, \"Ecclesiastical History of the English People\", gained him the title \"The Father of English History\". He served at the monastery of St Peter and its companion monastery of St Paul in the Kingdom of Northumbria of the Angles.\nBorn on lands belonging to the twin monastery of Monkwearmouth\u2013Jarrow in present-day Tyne and Wear, England, Bede was sent to Monkwearmouth at the age of seven and later joined Abbot Ceolfrith at Jarrow. Both of them survived a plague that struck in 686 and killed the majority of the population there. While Bede spent most of his life in the monastery, he travelled to several abbeys and monasteries across the British Isles, even visiting the archbishop of York and King Ceolwulf of Northumbria.\nHis theological writings were extensive and included a number of Biblical commentaries and other works of exegetical erudition. Another important area of study for Bede was the academic discipline of \"computus\", otherwise known to his contemporaries as the science of calculating calendar dates. One of the more important dates Bede tried to compute was Easter, an effort that was mired in controversy. He also helped popularize the practice of dating forward from the birth of Christ (\"Anno Domini\u2014\"in the year of our Lord), a practice which eventually became commonplace in medieval Europe. He is considered by many historians to be the most important scholar of antiquity for the period between the death of Pope Gregory I in 604 and the coronation of Charlemagne in 800.\nIn 1899, Pope Leo XIII declared him a Doctor of the Church. He is the only native of Great Britain to achieve this designation. Bede was moreover a skilled linguist and translator, and his work made the Latin and Greek writings of the early Church Fathers much more accessible to his fellow Anglo-Saxons, which contributed significantly to English Christianity. Bede's monastery had access to an impressive library which included works by Eusebius, Orosius, and many others.\nLife.\nAlmost everything that is known of Bede's life is contained in the last chapter of his \"Ecclesiastical History of the English People\", a history of the church in England. It was completed in about 731, and Bede implies that he was then in his fifty-ninth year, which would give a birth date in 672 or 673. A minor source of information is the letter by his disciple Cuthbert (not to be confused with the saint, Cuthbert, who is mentioned in Bede's work) which relates Bede's death. Bede, in the \"Historia\", gives his birthplace as \"on the lands of this monastery\". He is referring to the twin monasteries of Monkwearmouth and Jarrow, in modern-day Wearside and Tyneside respectively. There is also a tradition that he was born at Monkton, two miles from the site where the monastery at Jarrow was later built. Bede says nothing of his origins, but his connections with men of noble ancestry suggest that his own family was well-to-do. Bede's first abbot was Benedict Biscop, and the names \"Biscop\" and \"Beda\" both appear in a list of the kings of Lindsey from around 800, further suggesting that Bede came from a noble family.\nBede's name reflects West Saxon \"B\u012beda\" (Anglian \"B\u0113da\"). It is an Old English short name formed on the root of \"b\u0113odan\" \"to bid, command\".\nThe name also occurs in the \"Anglo-Saxon Chronicle\", s.a. 501, as \"Bieda\", one of the sons of the Saxon founder of Portsmouth. The \"Liber Vitae\" of Durham Cathedral names two priests with this name, one of whom is presumably Bede himself. Some manuscripts of the \"Life of Cuthbert\", one of Bede's works, mention that Cuthbert's own priest was named Bede; it is possible that this priest is the other name listed in the \"Liber Vitae\".\nAt the age of seven, Bede was sent as a \"puer oblatus\" to the monastery of Monkwearmouth by his family to be educated by Benedict Biscop and later by Ceolfrith. Bede does not say whether it was already intended at that point that he would be a monk. It was fairly common in Ireland at this time for young boys, particularly those of noble birth, to be fostered out as an oblate; the practice was also likely to have been common among the Germanic peoples in England. Monkwearmouth's sister monastery at Jarrow was founded by Ceolfrith in 682, and Bede probably transferred to Jarrow with Ceolfrith that year.\nThe dedication stone for the church has survived ; it is dated 23 April 685, and as Bede would have been required to assist with menial tasks in his day-to-day life it is possible that he helped in building the original church. In 686, plague broke out at Jarrow. The \"Life of Ceolfrith\", written in about 710, records that only two surviving monks were capable of singing the full offices; one was Ceolfrith and the other a young boy, who according to the anonymous writer had been taught by Ceolfrith. The two managed to do the entire service of the liturgy until others could be trained. The young boy was almost certainly Bede, who would have been about 14.\nWhen Bede was about 17 years old, Adomn\u00e1n, the abbot of Iona Abbey, visited Monkwearmouth and Jarrow. Bede would probably have met the abbot during this visit, and it may be that Adomn\u00e1n sparked Bede's interest in the Easter dating controversy. In about 692, in Bede's nineteenth year, Bede was ordained a deacon by his diocesan bishop, John, who was bishop of Hexham. The canonical age for the ordination of a deacon was 25; Bede's early ordination may mean that his abilities were considered exceptional, but it is also possible that the minimum age requirement was often disregarded. There might have been minor orders ranking below a deacon; but there is no record of whether Bede held any of these offices. In Bede's thirtieth year (about 702), he became a priest, with the ordination again performed by Bishop John.\nIn about 701 Bede wrote his first works, the \"De Arte Metrica\" and \"De Schematibus et Tropis\"; both were intended for use in the classroom. He continued to write for the rest of his life, eventually completing over 60 books, most of which have survived. Not all his output can be easily dated, and Bede may have worked on some texts over a period of many years. His last surviving work is a letter to Ecgbert of York, a former student, written in 734. A 6th-century Greek and Latin manuscript of \"Acts of the Apostles\" that is believed to have been used by Bede survives and is now in the Bodleian Library at the University of Oxford. It is known as the Codex Laudianus.\nBede may have worked on some of the Latin Bibles that were copied at Jarrow, one of which, the Codex Amiatinus, is now held by the Laurentian Library in Florence. Bede was a teacher as well as a writer; he enjoyed music and was said to be accomplished as a singer and as a reciter of poetry in the vernacular. It is possible that he suffered a speech impediment, but this depends on a phrase in the introduction to his verse life of St Cuthbert. Translations of this phrase differ, and it is uncertain whether Bede intended to say that he was cured of a speech problem, or merely that he was inspired by the saint's works.\nIn 708, some monks at Hexham accused Bede of having committed heresy in his work \"De Temporibus\". The standard theological view of world history at the time was known as the Six Ages of the World; in his book, Bede calculated the age of the world for himself, rather than accepting the authority of Isidore of Seville, and came to the conclusion that Christ had been born 3,952 years after the creation of the world, rather than the figure of over 5,000 years that was commonly accepted by theologians. The accusation occurred in front of the bishop of Hexham, Wilfrid, who was present at a feast when some drunken monks made the accusation. Wilfrid did not respond to the accusation, but a monk present relayed the episode to Bede, who replied within a few days to the monk, writing a letter setting forth his defence and asking that the letter also be read to Wilfrid. Bede had another brush with Wilfrid, for the historian says that he met Wilfrid sometime between 706 and 709 and discussed \u00c6thelthryth, the abbess of Ely. Wilfrid had been present at the exhumation of her body in 695, and Bede questioned the bishop about the exact circumstances of the body and asked for more details of her life, as Wilfrid had been her advisor.\nIn 733, Bede travelled to York to visit Ecgbert, who was then bishop of York. The See of York was elevated to an archbishopric in 735, and it is likely that Bede and Ecgbert discussed the proposal for the elevation during his visit. Bede hoped to visit Ecgbert again in 734 but was too ill to make the journey. Bede also travelled to the monastery of Lindisfarne and at some point visited the otherwise unknown monastery of a monk named , a visit that is mentioned in a letter to that monk. Because of his widespread correspondence with others throughout the British Isles, and because many of the letters imply that Bede had met his correspondents, it is likely that Bede travelled to some other places, although nothing further about timing or locations can be guessed.\nIt seems certain that he did not visit Rome, however, as he did not mention it in the autobiographical chapter of his \"Historia Ecclesiastica\". Nothhelm, a correspondent of Bede's who assisted him by finding documents for him in Rome, is known to have visited Bede, though the date cannot be determined beyond the fact that it was after Nothhelm's visit to Rome. Except for a few visits to other monasteries, his life was spent in a round of prayer, observance of the monastic discipline and study of the Sacred Scriptures. He was considered the most learned man of his time.\nBede died at Jarrow on the Feast of the Ascension, 26 May 735 and was buried there. Cuthbert, a disciple of Bede's, wrote a letter to a Cuthwin (of whom nothing else is known), describing Bede's last days and his death. According to Cuthbert, Bede fell ill, \"with frequent attacks of breathlessness but almost without pain\", before Easter. On the Tuesday, two days before Bede died, his breathing became worse and his feet swelled. He continued to dictate to a scribe, however, and despite spending the night awake in prayer he dictated again the following day.\nAt three o'clock, according to Cuthbert, he asked for a box of his to be brought and distributed among the priests of the monastery \"a few treasures\" of his: \"some pepper, and napkins, and some incense\". That night he dictated a final sentence to the scribe, a boy named Wilberht, and died soon afterwards. The account of Cuthbert does not make entirely clear whether Bede died before midnight or after. However, by the reckoning of Bede's time, passage from the old day to the new occurred at sunset, not midnight, and Cuthbert is clear that he died after sunset. Thus, while his box was brought at three o'clock Wednesday afternoon of 25 May, by the time of the final dictation it was considered 26 May, although it might still have been 25 May in modern usage.\nCuthbert's letter also relates a five-line poem in the vernacular that Bede composed on his deathbed, known as \"Bede's Death Song\". It is the most-widely copied Old English poem and appears in 45 manuscripts, but its attribution to Bede is not certain\u2014not all manuscripts name Bede as the author, and the ones that do are of later origin than those that do not. Bede's remains may have been translated to Durham Cathedral in the 11th century; his tomb there was looted in 1541, but the contents were probably re-interred in the Galilee chapel at the cathedral.\nOne further oddity in his writings is that in one of his works, the \"Commentary on the Seven Catholic Epistles\", he writes in a manner that gives the impression he was married. The section in question is the only one in that work that is written in first-person view. Bede says: \"Prayers are hindered by the conjugal duty because as often as I perform what is due to my wife I am not able to pray.\" Another passage, in the \"Commentary on Luke\", also mentions a wife in the first person: \"Formerly I possessed a wife in the lustful passion of desire and now I possess her in honourable sanctification and true love of Christ.\" The historian Benedicta Ward argued that these passages are Bede employing a rhetorical device.\nWorks.\nBede wrote scientific, historical and theological works, reflecting the range of his writings from music and metrics to exegetical Scripture commentaries. He knew patristic literature, as well as Pliny the Elder, Virgil, Lucretius, Ovid, Horace and other classical writers. He knew some Greek. Bede's scriptural commentaries employed the allegorical method of interpretation, and his history includes accounts of miracles, which to modern historians has seemed at odds with his critical approach to the materials in his history. Modern studies have shown the important role such concepts played in the world-view of Early Medieval scholars. Although Bede is mainly studied as a historian now, in his time his works on grammar, chronology, and biblical studies were as important as his historical and hagiographical works. The non-historical works contributed greatly to the Carolingian Renaissance. He has been credited with writing a penitential, though his authorship of this work is disputed.\n\"Ecclesiastical History of the English People\".\nBede's best-known work is the , or \"An Ecclesiastical History of the English People\", completed in about 731. Bede was aided in writing this book by Albinus, abbot of St Augustine's Abbey, Canterbury. The first of the five books begins with some geographical background and then sketches the history of England, beginning with Caesar's invasion in 55 BC. A brief account of Christianity in Roman Britain, including the martyrdom of St Alban, is followed by the story of Augustine's mission to England in 597, which brought Christianity to the Anglo-Saxons.\nThe second book begins with the death of Gregory the Great in 604 and follows the further progress of Christianity in Kent and the first attempts to evangelise Northumbria. These ended in disaster when Penda, the pagan king of Mercia, killed the newly Christian Edwin of Northumbria at the Battle of Hatfield Chase in about 632. The setback was temporary, and the third book recounts the growth of Christianity in Northumbria under kings Oswald of Northumbria and Oswy. The climax of the third book is the account of the Council of Whitby, traditionally seen as a major turning point in English history. The fourth book begins with the consecration of Theodore as Archbishop of Canterbury and recounts Wilfrid's efforts to bring Christianity to the Kingdom of Sussex.\nThe fifth book brings the story up to Bede's day and includes an account of missionary work in Frisia and of the conflict with the British church over the correct dating of Easter. Bede wrote a preface for the work, in which he dedicates it to Ceolwulf, king of Northumbria. The preface mentions that Ceolwulf received an earlier draft of the book; presumably Ceolwulf knew enough Latin to understand it, and he may even have been able to read it. The preface makes it clear that Ceolwulf had requested the earlier copy, and Bede had asked for Ceolwulf's approval; this correspondence with the king indicates that Bede's monastery had connections among the Northumbrian nobility.\nSources.\nThe monastery at Wearmouth-Jarrow had an excellent library. Both Benedict Biscop and Ceolfrith had acquired books from the Continent, and in Bede's day the monastery was a renowned centre of learning. It has been estimated that there were about 200 books in the monastic library.\nFor the period prior to Augustine's arrival in 597, Bede drew on earlier writers, including Solinus. He had access to two works of Eusebius: the \"Historia Ecclesiastica\", and also the \"Chronicon\", though he had neither in the original Greek; instead he had a Latin translation of the \"Historia\", by Rufinus, and Jerome's translation of the \"Chronicon\". He also knew Orosius's \"Adversus Paganus\", and Gregory of Tours' \"Historia Francorum\", both Christian histories, as well as the work of Eutropius, a pagan historian. He used Constantius's \"Life of Germanus\" as a source for Germanus's visits to Britain.\nBede's account of the Anglo-Saxon settlement of Britain is drawn largely from Gildas's \"De Excidio et Conquestu Britanniae\". Bede would also have been familiar with more recent accounts such as Stephen of Ripon's \"Life of Wilfrid\", and anonymous \"Life\" \"of Gregory the Great\" and \"Life of Cuthbert\". He also drew on Josephus's \"Antiquities\", and the works of Cassiodorus, and there was a copy of the \"Liber Pontificalis\" in Bede's monastery. Bede quotes from several classical authors, including Cicero, Plautus, and Terence, but he may have had access to their work via a Latin grammar rather than directly. However, it is clear he was familiar with the works of Virgil and with Pliny the Elder's \"Natural History\", and his monastery also owned copies of the works of Dionysius Exiguus.\nHe probably drew his account of Alban from a life of that saint which has not survived. He acknowledges two other lives of saints directly; one is a life of Fursa, and the other of \u00c6thelburh; the latter no longer survives. He also had access to a life of Ceolfrith. Some of Bede's material came from oral traditions, including a description of the physical appearance of Paulinus of York, who had died nearly 90 years before Bede's \"Historia Ecclesiastica\" was written.\nBede had correspondents who supplied him with material. Albinus, the abbot of the monastery in Canterbury, provided much information about the church in Kent, and with the assistance of Nothhelm, at that time a priest in London, obtained copies of Gregory the Great's correspondence from Rome relating to Augustine's mission. Almost all of Bede's information regarding Augustine is taken from these letters. Bede acknowledged his correspondents in the preface to the \"Historia Ecclesiastica\"; he was in contact with Bishop Daniel of Winchester, for information about the history of the church in Wessex and also wrote to the monastery at Lastingham for information about Cedd and Chad. Bede also mentions an Abbot Esi as a source for the affairs of the East Anglian church, and Bishop Cynibert for information about Lindsey.\nThe historian Walter Goffart argues that Bede based the structure of the \"Historia\" on three works, using them as the framework around which the three main sections of the work were structured. For the early part of the work, up until the Gregorian mission, Goffart feels that Bede used \"De excidio\". The second section, detailing the Gregorian mission of Augustine of Canterbury was framed on \"Life of Gregory the Great\" written at Whitby. The last section, detailing events after the Gregorian mission, Goffart feels was modelled on \"Life of Wilfrid\". Most of Bede's informants for information after Augustine's mission came from the eastern part of Britain, leaving significant gaps in the knowledge of the western areas, which were those areas likely to have a native Briton presence.\nModels and style.\nBede's stylistic models included some of the same authors from whom he drew the material for the earlier parts of his history. His introduction imitates the work of Orosius, and his title is an echo of Eusebius's \"Historia Ecclesiastica\". Bede also followed Eusebius in taking the \"Acts of the Apostles\" as the model for the overall work: where Eusebius used the \"Acts\" as the theme for his description of the development of the church, Bede made it the model for his history of the Anglo-Saxon church. Bede quoted his sources at length in his narrative, as Eusebius had done. Bede also appears to have taken quotes directly from his correspondents at times. For example, he almost always uses the terms \"Australes\" and \"Occidentales\" for the South and West Saxons respectively, but in a passage in the first book he uses \"Meridiani\" and \"Occidui\" instead, as perhaps his informant had done. At the end of the work, Bede adds a brief autobiographical note; this was an idea taken from Gregory of Tours' earlier \"History of the Franks\".\nBede's work as a hagiographer and his detailed attention to dating were both useful preparations for the task of writing the \"Historia Ecclesiastica\". His interest in computus, the science of calculating the date of Easter, was also useful in the account he gives of the controversy between the British and Anglo-Saxon church over the correct method of obtaining the Easter date.\nBede is described by Michael Lapidge as \"without question the most accomplished Latinist produced in these islands in the Anglo-Saxon period\". His Latin has been praised for its clarity, but his style in the \"Historia Ecclesiastica\" is not simple. He knew rhetoric and often used figures of speech and rhetorical forms which cannot easily be reproduced in translation, depending as they often do on the connotations of the Latin words. However, unlike contemporaries such as Aldhelm, whose Latin is full of difficulties, Bede's own text is easy to read. In the words of Charles Plummer, one of the best-known editors of the \"Historia Ecclesiastica\", Bede's Latin is \"clear and limpid\u00a0... it is very seldom that we have to pause to think of the meaning of a sentence\u00a0... Alcuin rightly praises Bede for his unpretending style.\"\nIntent.\nBede's primary intention in writing the \"Historia Ecclesiastica\" was to show the growth of the united church throughout England. The native Britons, whose Christian church survived the departure of the Romans, earn Bede's ire for refusing to help convert the Anglo-Saxons; by the end of the \"Historia\" the English, and their church, are dominant over the Britons. This goal, of showing the movement towards unity, explains Bede's animosity towards the British method of calculating Easter: much of the \"Historia\" is devoted to a history of the dispute, including the final resolution at the Synod of Whitby in 664. Bede is also concerned to show the unity of the English, despite the disparate kingdoms that still existed when he was writing. He also wants to instruct the reader by spiritual example and to entertain, and to the latter end he adds stories about many of the places and people about which he wrote.\nN. J. Higham argues that Bede designed his work to promote his reform agenda to Ceolwulf, the Northumbrian king. Bede painted a highly optimistic picture of the current situation in the Church, as opposed to the more pessimistic picture found in his private letters.\nBede's extensive use of miracles can prove difficult for readers who consider him a more or less reliable historian but do not accept the possibility of miracles. Yet both reflect an inseparable integrity and regard for accuracy and truth, expressed in terms both of historical events and of a tradition of Christian faith that continues. Bede, like Gregory the Great whom Bede quotes on the subject in the \"Historia\", felt that faith brought about by miracles was a stepping stone to a higher, truer faith, and that as a result miracles had their place in a work designed to instruct.\nOmissions and biases.\nBede is somewhat reticent about the career of Wilfrid, a contemporary and one of the most prominent clerics of his day. This may be because Wilfrid's opulent lifestyle was uncongenial to Bede's monastic mind; it may also be that the events of Wilfrid's life, divisive and controversial as they were, simply did not fit with Bede's theme of the progression to a unified and harmonious church.\nBede's account of the early migrations of the Angles and Saxons to England omits any mention of a movement of those peoples across the English Channel from Britain to Brittany described by Procopius, who was writing in the sixth century. Frank Stenton describes this omission as \"a scholar's dislike of the indefinite\"; traditional material that could not be dated or used for Bede's didactic purposes had no interest for him.\nBede was a Northumbrian, and this tinged his work with a local bias. The sources to which he had access gave him less information about the west of England than for other areas. He says relatively little about the achievements of Mercia and Wessex, omitting, for example, any mention of Boniface, a West Saxon missionary to the continent of some renown and of whom Bede had almost certainly heard, though Bede does discuss Northumbrian missionaries to the continent. He is also parsimonious in his praise for Aldhelm, a West Saxon who had done much to convert the native Britons to the Roman form of Christianity. He lists seven kings of the Anglo-Saxons whom he regards as having held \"imperium\", or overlordship; only one king of Wessex, Ceawlin, is listed as Bretwalda, and none from Mercia, though elsewhere he acknowledges the secular power several of the Mercians held. Historian Robin Fleming states that he was so hostile to Mercia because Northumbria had been diminished by Mercian power that he consulted no Mercian informants and included no stories about its saints.\nBede relates the story of Augustine's mission from Rome, and tells how the British clergy refused to assist Augustine in the conversion of the Anglo-Saxons. This, combined with Gildas's negative assessment of the British church at the time of the Anglo-Saxon invasions, led Bede to a very critical view of the native church. However, Bede ignores the fact that at the time of Augustine's mission, the history between the two was one of warfare and conquest, which, in the words of Barbara Yorke, would have naturally \"curbed any missionary impulses towards the Anglo-Saxons from the British clergy.\"\nUse of \"Anno Domini\".\nAt the time Bede wrote the \"Historia Ecclesiastica\", there were two common ways of referring to dates. One was to use indictions, which were 15-year cycles, counting from 312 AD. There were three different varieties of indiction, each starting on a different day of the year. The other approach was to use regnal years\u2014the reigning Roman emperor, for example, or the ruler of whichever kingdom was under discussion. This meant that in discussing conflicts between kingdoms, the date would have to be given in the regnal years of all the kings involved. Bede used both these approaches on occasion but adopted a third method as his main approach to dating: the \"Anno Domini\" method invented by Dionysius Exiguus. Although Bede did not invent this method, his adoption of it and his promulgation of it in \"De Temporum Ratione\", his work on chronology, is the main reason it is now so widely used. Bede's Easter table, contained in \"De Temporum Ratione\", was developed from Dionysius Exiguus' Easter table.\nAssessment.\nThe \"Historia Ecclesiastica\" was copied often in the Middle Ages, and about 160 manuscripts containing it survive. About half of those are located on the European continent, rather than in the British Isles. Most of the 8th- and 9th-century texts of Bede's \"Historia\" come from the northern parts of the Carolingian Empire. This total does not include manuscripts with only a part of the work, of which another 100 or so survive. It was printed for the first time between 1474 and 1482, probably at Strasbourg.\nModern historians have studied the \"Historia\" extensively, and several editions have been produced. For many years, early Anglo-Saxon history was essentially a retelling of the \"Historia\", but recent scholarship has focused as much on what Bede did not write as what he did. The belief that the \"Historia\" was the culmination of Bede's works, the aim of all his scholarship, was a belief common among historians in the past but is no longer accepted by most scholars.\nModern historians and editors of Bede have been lavish in their praise of his achievement in the \"Historia Ecclesiastica\". Stenton regards it as one of the \"small class of books which transcend all but the most fundamental conditions of time and place\", and regards its quality as dependent on Bede's \"astonishing power of co-ordinating the fragments of information which came to him through tradition, the relation of friends, or documentary evidence\u00a0... In an age where little was attempted beyond the registration of fact, he had reached the conception of history.\" Patrick Wormald describes him as \"the first and greatest of England's historians\".\nThe \"Historia Ecclesiastica\" has given Bede a high reputation, but his concerns were different from those of a modern writer of history. His focus on the history of the organisation of the English church, and on heresies and the efforts made to root them out, led him to exclude the secular history of kings and kingdoms except where a moral lesson could be drawn or where they illuminated events in the church. Besides the \"Anglo-Saxon Chronicle\", the medieval writers William of Malmesbury, Henry of Huntingdon, and Geoffrey of Monmouth used his works as sources and inspirations. Early modern writers, such as Polydore Vergil and Matthew Parker, the Elizabethan Archbishop of Canterbury, also utilised the \"Historia\", and his works were used by both Protestant and Catholic sides in the wars of religion.\nSome historians have questioned the reliability of some of Bede's accounts. One historian, Charlotte Behr, thinks that the \"Historia's\" account of the arrival of the Germanic invaders in Kent should not be considered to relate what actually happened, but rather relates myths that were current in Kent during Bede's time.\nIt is likely that Bede's work, because it was so widely copied, discouraged others from writing histories and may even have led to the disappearance of manuscripts containing older historical works.\nOther historical works.\nChronicles.\nAs Chapter 66 of his \"On the Reckoning of Time\", in 725 Bede wrote the \"Greater Chronicle\" (\"chronica maiora\"), which sometimes circulated as a separate work. For recent events the \"Chronicle\", like his \"Ecclesiastical History\", relied upon Gildas, upon a version of the \"Liber Pontificalis\" current at least to the papacy of Pope Sergius I (687\u2013701), and other sources. For earlier events he drew on Eusebius's \"Chronikoi Kanones.\" The dating of events in the \"Chronicle\" is inconsistent with his other works, using the era of creation, the \"Anno Mundi\".\nHagiography.\nHis other historical works included lives of the abbots of Wearmouth and Jarrow, as well as verse and prose lives of St Cuthbert, an adaptation of Paulinus of Nola's \"Life of St Felix\", and a translation of the Greek \"Passion\" of St Anastasius. He also created a listing of saints, the \"Martyrology\".\nTheological works.\nIn his own time, Bede was as well known for his biblical commentaries, and for his exegetical and other theological works. The majority of his writings were of this type and covered the Old Testament and the New Testament. Most survived the Middle Ages, but a few were lost. It was for his theological writings that he earned the title of \"Doctor Anglorum\" and why he was declared a saint.\nBede synthesised and transmitted the learning from his predecessors, as well as made careful, judicious innovation in knowledge (such as recalculating the age of the Earth\u2014for which he was censured before surviving the heresy accusations and eventually having his views championed by Archbishop Ussher in the sixteenth century\u2014see below) that had theological implications. In order to do this, he learned Greek and attempted to learn Hebrew. He spent time reading and rereading both the Old and the New Testaments. He mentions that he studied from a text of Jerome's Vulgate, which itself was from the Hebrew text.\nHe also studied both the Latin and the Greek Fathers of the Church. In the monastic library at Jarrow were numerous books by theologians, including works by Basil, Cassian, John Chrysostom, Isidore of Seville, Origen, Gregory of Nazianzus, Augustine of Hippo, Jerome, Pope Gregory I, Ambrose of Milan, Cassiodorus, and Cyprian. He used these, in conjunction with the Biblical texts themselves, to write his commentaries and other theological works.\nHe had a Latin translation by Evagrius of Athanasius's \"Life of Antony\" and a copy of Sulpicius Severus' \"Life of St Martin\". He also used lesser known writers, such as Fulgentius, Julian of Eclanum, Tyconius, and Prosper of Aquitaine. Bede was the first to refer to Jerome, Augustine, Pope Gregory and Ambrose as the four Latin Fathers of the Church. It is clear from Bede's own comments that he felt his calling was to explain to his students and readers the theology and thoughts of the Church Fathers.\nBede also wrote homilies, works written to explain theology used in worship services. He wrote homilies on the major Christian seasons such as Advent, Lent, or Easter, as well as on other subjects such as anniversaries of significant events.\nBoth types of Bede's theological works circulated widely in the Middle Ages. Several of his biblical commentaries were incorporated into the \"Glossa Ordinaria\", an 11th-century collection of biblical commentaries. Some of Bede's homilies were collected by Paul the Deacon, and they were used in that form in the Monastic Office. Boniface used Bede's homilies in his missionary efforts on the continent.\nBede sometimes included in his theological books an acknowledgement of the predecessors on whose works he drew. In two cases he left instructions that his marginal notes, which gave the details of his sources, should be preserved by the copyist, and he may have originally added marginal comments about his sources to others of his works. Where he does not specify, it is still possible to identify books to which he must have had access by quotations that he uses. A full catalogue of the library available to Bede in the monastery cannot be reconstructed, but it is possible to tell, for example, that Bede was very familiar with the works of Virgil.\nThere is little evidence that he had access to any other of the pagan Latin writers\u2014he quotes many of these writers, but the quotes are almost always found in the Latin grammars that were common in his day, one or more of which would certainly have been at the monastery. Another difficulty is that manuscripts of early writers were often incomplete: it is apparent that Bede had access to Pliny's \"Encyclopaedia\", for example, but it seems that the version he had was missing book xviii, since he did not quote from it in his \"De temporum ratione\".Bede's works included \"Commentary on Revelation\", \"Commentary on the Catholic Epistles\", \"Commentary on Acts\", \"Reconsideration on the Books of Acts\", \"On the Gospel of Mark\", \"On the Gospel of Luke\", and \"Homilies on the Gospels\". At the time of his death he was working on a translation of the Gospel of John into English. He did this for the last 40 days of his life. When the last passage had been translated he said: \"All is finished.\" The works dealing with the Old Testament included \"Commentary on Samuel\", \"Commentary on Genesis\", \"Commentaries on Ezra and Nehemiah\", \"On the Temple\", \"On the Tabernacle\", \"Commentaries on Tobit\", \"Commentaries on Proverbs\", \"Commentaries on the Song of Songs\", \"Commentaries on the Canticle of Habakkuk\". The works on Ezra, the tabernacle and the temple were especially influenced by Gregory the Great's writings.\nHistorical and astronomical chronology.\n\"De temporibus\", or \"On Time\", written in about 703, provides an introduction to the principles of Easter computus. This was based on parts of Isidore of Seville's \"Etymologies\", and Bede also included a chronology of the world which was derived from Eusebius, with some revisions based on Jerome's translation of the Bible. In about 723, Bede wrote a longer work on the same subject, \"On the Reckoning of Time\", which was influential throughout the Middle Ages. He also wrote several shorter letters and essays discussing specific aspects of computus.\n\"On the Reckoning of Time\" (\"De temporum ratione\") included an introduction to the traditional ancient and medieval view of the cosmos, including an explanation of how the spherical Earth influenced the changing length of daylight, of how the seasonal motion of the Sun and Moon influenced the changing appearance of the new moon at evening twilight. Bede also records the effect of the moon on tides. He shows that the twice-daily timing of tides is related to the Moon and that the lunar monthly cycle of spring and neap tides is also related to the Moon's position. He goes on to note that the times of tides vary along the same coast and that the water movements cause low tide at one place when there is high tide elsewhere. Since the focus of his book was the computus, Bede gave instructions for computing the date of Easter from the date of the Paschal full moon, for calculating the motion of the Sun and Moon through the zodiac, and for many other calculations related to the calendar. He gives some information about the months of the Anglo-Saxon calendar.\nAny codex of Bede's Easter table is normally found together with a codex of his \"De temporum ratione\". His Easter table, being an exact extension of Dionysius Exiguus' Paschal table and covering the time interval AD 532\u20131063, contains a 532-year Paschal cycle based on the so-called classical Alexandrian 19-year lunar cycle, being the close variant of bishop Theophilus' 19-year lunar cycle proposed by Annianus and adopted by bishop Cyril of Alexandria around AD 425. The ultimate similar (but rather different) predecessor of this Metonic 19-year lunar cycle is the one invented by Anatolius around AD 260.\nFor calendric purposes, Bede made a new calculation of the age of the world since the creation, which he dated as 3952 BC. Because of his innovations in computing the age of the world, he was accused of heresy at the table of Bishop Wilfrid, his chronology being contrary to accepted calculations. Once informed of the accusations of these \"lewd rustics\", Bede refuted them in his Letter to Plegwin.\nIn addition to these works on astronomical timekeeping, he also wrote \"De natura rerum\", or \"On the Nature of Things\", modelled in part after the work of the same title by Isidore of Seville. His works were so influential that late in the ninth century Notker the Stammerer, a monk of the Monastery of St Gall in Switzerland, wrote that \"God, the orderer of natures, who raised the Sun from the East on the fourth day of Creation, in the sixth day of the world has made Bede rise from the West as a new Sun to illuminate the whole Earth\".\nEducational works.\nBede wrote some works designed to help teach grammar in the abbey school. One of these was \"De arte metrica\", a discussion of the composition of Latin verse, drawing on previous grammarians' work. It was based on Donatus's \"De pedibus\" and Servius's \"De finalibus\" and used examples from Christian poets as well as Virgil. It became a standard text for the teaching of Latin verse during the next few centuries. Bede dedicated this work to Cuthbert, apparently a student, for he is named \"beloved son\" in the dedication, and Bede says \"I have laboured to educate you in divine letters and ecclesiastical statutes.\" \"De orthographia\" is a work on orthography, designed to help a medieval reader of Latin with unfamiliar abbreviations and words from classical Latin works. Although it could serve as a textbook, it appears to have been mainly intended as a reference work. The date of composition for both of these works is unknown.\n\"De schematibus et tropis sacrae scripturae\" discusses the Bible's use of rhetoric. Bede was familiar with pagan authors such as Virgil, but it was not considered appropriate to teach biblical grammar from such texts, and Bede argues for the superiority of Christian texts in understanding Christian literature. Similarly, his text on poetic metre uses only Christian poetry for examples.\nLatin poetry.\nA number of poems have been attributed to Bede. His poetic output has been systematically surveyed and edited by Michael Lapidge, who concluded that the following works belong to Bede: the \"Versus de die iudicii\" (\"verses on the day of Judgement\", found complete in 33 manuscripts and fragmentarily in 10); the metrical \"Vita Sancti Cudbercti\" (\"Life of St Cuthbert\"); and two collections of verse mentioned in the \"Historia ecclesiastica\" V.24.2. Bede names the first of these collections as \"librum epigrammatum heroico metro siue elegiaco\" (\"a book of epigrams in the heroic or elegiac metre\"), and much of its content has been reconstructed by Lapidge from scattered attestations under the title \"Liber epigrammatum\". The second is named as \"liber hymnorum diuerso metro siue rythmo\" (\"a book of hymns, diverse in metre or rhythm\"); this has been reconstructed by Lapidge as containing ten liturgical hymns, one paraliturgical hymn (for the Feast of St \u00c6thelthryth), and four other hymn-like compositions.\nVernacular poetry.\nAccording to his disciple Cuthbert, Bede was \"doctus in nostris carminibus\" (\"learned in our songs\"). Cuthbert's letter on Bede's death, the \"Epistola Cuthberti de obitu Bedae\", moreover, commonly is understood to indicate that Bede composed a five-line vernacular poem known to modern scholars as \"Bede's Death Song\"\nAs Opland notes, however, it is not entirely clear that Cuthbert is attributing this text to Bede: most manuscripts of the latter do not use a finite verb to describe Bede's presentation of the song, and the theme was relatively common in Old English and Anglo-Latin literature. The fact that Cuthbert's description places the performance of the Old English poem in the context of a series of quoted passages from Sacred Scripture might be taken as evidence simply that Bede also cited analogous vernacular texts.\nOn the other hand, the inclusion of the Old English text of the poem in Cuthbert's Latin letter, the observation that Bede \"was learned in our song,\" and the fact that Bede composed a Latin poem on the same subject all point to the possibility of his having written it. By citing the poem directly, Cuthbert seems to imply that its particular wording was somehow important, either since it was a vernacular poem endorsed by a scholar who evidently frowned upon secular entertainment or because it is a direct quotation of Bede's last original composition.\nVeneration.\nThere is no evidence for cult being paid to Bede in England in the 8th century. One reason for this may be that he died on the feast day of Augustine of Canterbury. Later, when he was venerated in England, he was either commemorated after Augustine on 26 May, or his feast was moved to 27 May. However, he was venerated outside England, mainly through the efforts of Boniface and Alcuin, both of whom promoted the cult on the continent. Boniface wrote repeatedly back to England during his missionary efforts, requesting copies of Bede's theological works.\nAlcuin, who was taught at the school set up in York by Bede's pupil Ecgbert, praised Bede as an example for monks to follow and was instrumental in disseminating Bede's works to all of Alcuin's friends. Bede's cult became prominent in England during the 10th-century revival of monasticism and by the 14th century had spread to many of the cathedrals of England. Wulfstan, Bishop of Worcester was a particular devotee of Bede's, dedicating a church to him in 1062, which was Wulfstan's first undertaking after his consecration as bishop.\nHis body was 'translated' (the ecclesiastical term for relocation of relics) from Jarrow to Durham Cathedral around 1020, where it was placed in the same tomb with St Cuthbert. Later Bede's remains were moved to a shrine in the Galilee Chapel at Durham Cathedral in 1370. The shrine was destroyed during the English Reformation, but the bones were reburied in the chapel. In 1831 the bones were dug up and then reburied in a new tomb, which is still there. Other relics were claimed by York, Glastonbury and Fulda.\nHis scholarship and importance to Catholicism were recognised in 1899 when the Vatican declared him a Doctor of the Church. He is the only Englishman named a Doctor of the Church. He is also the only Englishman in Dante's \"Paradise\" (\"Paradiso\" X.130), mentioned among theologians and doctors of the church in the same canto as Isidore of Seville and the Scot Richard of St Victor.\nHis feast day was included in the General Roman Calendar in 1899, for celebration on 27 May rather than on his date of death, 26 May, which was then the feast day of St Augustine of Canterbury. He is venerated in the Catholic Church, in the Church of England and in the Episcopal Church (United States) on 25 May, and in the Eastern Orthodox Church, with a feast day on 27 May (\u0392\u03b5\u03b4\u03ad\u03b1 \u03c4\u03bf\u03c5 \u039f\u03bc\u03bf\u03bb\u03bf\u03b3\u03b7\u03c4\u03bf\u03cd).\nBede became known as \"Venerable Bede\" (Latin: ) by the 9th century because of his holiness, but this was not linked to consideration for sainthood by the Catholic Church. According to a legend, the epithet was miraculously supplied by angels, thus completing his unfinished epitaph. It is first utilised in connection with Bede in the 9th century, where Bede was grouped with others who were called \"venerable\" at two ecclesiastical councils held at Aachen in 816 and 836. Paul the Deacon then referred to him as venerable consistently. By the 11th and 12th century, it had become commonplace.\nModern legacy.\nBede's reputation as a historian, based mostly on the \"Historia Ecclesiastica\", remains strong. Thomas Carlyle called him \"the greatest historical writer since Herodotus\". Walter Goffart says of Bede that he \"holds a privileged and unrivalled place among first historians of Christian Europe\". He is patron of Beda College in Rome which prepares older men for the Roman Catholic priesthood. His life and work have been celebrated with the annual Jarrow Lecture, held at St Paul's Church, Jarrow, since 1958. \nBede has been described as a progressive scholar, who made Latin and Greek teachings accessible to his fellow Anglo-Saxons.\nJarrow Hall (formerly Bede's World), in Jarrow, is a museum that celebrates the history of Bede and other parts of English heritage, on the site where he lived.\nBede Metro station, part of the Tyne and Wear Metro light rail network, is named after him."}
{"id": "4045", "revid": "183684", "url": "https://en.wikipedia.org/wiki?curid=4045", "title": "Bubble tea", "text": "Bubble tea (also known as pearl milk tea, bubble milk tea, tapioca milk tea, boba tea, or boba; , ) is a tea-based drink that originated in Taiwan in the early 1980s. Taiwanese immigrants brought it to the United States in the 1990s, initially in California through regions including Los Angeles County, but the drink has also spread to other countries where there is a large East Asian diaspora population.\nBubble tea most commonly consists of tea accompanied by chewy tapioca balls (\"boba\" or \"pearls\"), but it can be made with other toppings as well, such as grass jelly, aloe vera, red bean, and popping boba. It has many varieties and flavors, but the two most popular varieties are pearl black milk tea and pearl green milk tea (\"pearl\" for the tapioca balls at the bottom).\nDescription.\nBubble teas fall under two categories: teas without milk and milk teas. Both varieties come with a choice of black, green, or oolong tea as the base. Milk teas usually include powdered or fresh milk, but may also use condensed milk, almond milk, soy milk, or coconut milk.\nThe oldest known bubble tea drink consisted of a mixture of hot Taiwanese black tea, tapioca pearls (), condensed milk, and syrup () or honey. Bubble tea is most commonly served cold. The tapioca pearls that give bubble tea its name were originally made from the starch of the cassava, a tropical shrub known for its starchy roots which was introduced to Taiwan from South America during Japanese colonial rule. Larger pearls () quickly replaced these.\nSome caf\u00e9s specialize in bubble tea production. While some caf\u00e9s may serve bubble tea in a glass, most Taiwanese bubble tea shops serve the drink in a plastic cup and use a machine to seal the top of the cup with heated plastic cellophane. The method allows the tea to be shaken in the serving cup and makes it spill-free until a person is ready to drink it. The cellophane is then pierced with an oversized straw, referred to as a boba straw, which is larger than a typical drinking straw to allow the toppings to pass through.\nDue to its popularity, bubble tea has inspired a variety of bubble tea flavored snacks, such as bubble tea ice cream and bubble tea candy. The market size of bubble tea was valued at in 2022 and is projected to reach by the end of 2027. Some of the largest global bubble tea chains include Chatime, CoCo Fresh Tea &amp; Juice and Gong Cha.\nVariants.\nDrink.\nBubble tea comes in many variations which usually consist of black tea, green tea, oolong tea, and sometimes white tea. Another variation, yuenyeung, (, named after the Mandarin duck) originated in Hong Kong and consists of black tea, coffee, and milk.\nOther varieties of the drink include blended tea drinks. These variations are often either blended using ice cream, or are smoothies that contain both tea and fruit. Boba ice cream bars have also been produced.\nThere are many popular flavours of bubble tea, such as taro, mango, coffee, and coconut. Flavouring ingredients such as a syrup or powder determines the flavour and usually the colour of the bubble tea, while other ingredients such as tea, milk and boba are the basis.\nToppings.\nTapioca pearls (boba) are the most common ingredient, although there are other ways to make the chewy spheres found in bubble tea. The pearls vary in color according to the ingredients mixed in with the tapioca. Most pearls are black from brown sugar.\nJelly comes in different shapes: small cubes, stars, or rectangular strips, and flavors such as coconut jelly, konjac, lychee, grass jelly, mango, coffee and green tea. Azuki bean or mung bean paste, typical toppings for Taiwanese shaved ice desserts, give bubble tea an added subtle flavor as well as texture. Aloe, egg pudding (custard), and sago also can be found in many bubble tea shops. Popping boba, or spheres that have fruit juices or syrups inside them, are another popular bubble tea topping. Flavors include mango, strawberry, coconut, kiwi and honey melon.\nSome shops offer milk or cheese foam on top of the drink, giving the drink a consistency similar to that of whipped cream, and a saltier flavor profile. One shop described the effect of the cheese foam as \"neutraliz[ing] the bitterness of the tea...and as you drink it you taste the returning sweetness of the tea\".\nIce and sugar level.\nBubble tea shops often give customers the option of choosing the amount of ice or sugar in their drink. Sugar and ice levels are usually specified ordinally (e.g. no ice, less ice, normal ice, more ice), corresponding to quarterly intervals (0%, 25%, 50%, 75%, 100%).\nPackaging.\nIn Southeast Asia, bubble tea is usually packaged in a plastic takeaway cup, sealed with plastic or a rounded cap. New entrants into the market have attempted to distinguish their products by packaging it in bottles and other shapes. Some have used sealed plastic bags. Nevertheless, the plastic takeaway cup with a sealed cap is still the most common packaging method.\nPreparation method.\nThe tea can be made in batches during the day or the night before. Brewing different types of teas take different amounts of time and temperature. For instance, green tea requires brewing at a lower temperature, typically between with a brewing time of 8\u201310 minutes to extract its optimal flavor. In contrast, black tea needs to be made with hotter water, usually around with a brewing of around 15\u201320 minutes to bring out its sweetness. A tea warmer dispenser allows the tea to remain heated for up to eight hours. \nPearls (boba) are made from tapioca starch. Most bubble tea stores buy packaged tapioca pearls in an uncooked stage. When the boba is uncooked and in the package, it is uncolored and hard. The boba does not turn chewy and dark until they are cooked and sugar is added to bring out its taste. Uncooked tapioca pearls in their package can be stored for around 9 to 12 months, once cooked, they can be stored in a sealed container in the refrigerator. Despite this most bubble tea stores will not sell their boba after 24 hours because it will start to harden and lose its chewyness.\nThe traditional preparation method is to mix the ingredients (sugar, powders and other flavorings) together using a bubble tea shaker cup, by hand. However, many present-day bubble tea shops use a bubble tea shaker machine. This eliminates the need for humans to shake the bubble tea by hand. It also reduces staffing needs as multiple cups of bubble tea may be prepared by a single barista.\nHistory.\nMilk and sugar have been added to tea in Taiwan since the Dutch colonization of Taiwan in 1624\u20131662.\nThere are two competing stories for the discovery of bubble tea. One is associated with the Chun Shui Tang tea room in Taichung. Its founder, Liu Han-Chieh, began serving Chinese tea cold after he observed coffee was served cold in Japan while on a visit in the 1980s. The new style of serving tea propelled his business, and multiple chains serving this tea were established. The company's product development manager, Lin Hsiu Hui, said she created the first bubble tea in 1988 when she poured tapioca balls into her tea during a staff meeting and encouraged others to drink it. The beverage was well received by everyone at the meeting, leading to its inclusion on the menu. It ultimately became the franchise's top-selling product.\nAnother claim for the invention of bubble tea comes from the Hanlin Tea Room (\u7ff0\u6797\u8336\u9928) in Tainan. It claims that bubble tea was invented in 1986 when teahouse owner Tu Tsong-he was inspired by white tapioca balls he saw in the local market of Ah-b\u00f3-li\u00e2u (\u9d28\u6bcd\u5bee, or \"Y\u0101m\u01d4li\u00e1o\" in Mandarin). He later made tea using these traditional Taiwanese snacks. This resulted in what is known as \"pearl tea\".\nPopularity.\nIn the 1990s, bubble tea spread all over East and Southeast Asia with its ever-growing popularity. In regions like Hong Kong, mainland China, Japan, Vietnam, and Singapore, the bubble tea trend expanded rapidly among young people. In some popular shops, people would line up for more than thirty minutes to get a cup of the drink. In recent years, the popularity of bubble tea has gone beyond the beverage itself, with boba lovers inventing various bubble tea flavored-foods, including ice cream, pizza, toast, sushi, and ramen.\nTaiwan.\nIn Taiwan, bubble tea has become not just a beverage, but an enduring icon of the culture and food history for the nation. In 2020, the date April 30 was officially declared as National Bubble Tea Day in Taiwan. That same year, the image of bubble tea was proposed as an alternative cover design for Taiwan's passport. According to Al Jazeera, bubble tea has become synonymous with Taiwan and is an important symbol of Taiwanese identity both domestically and internationally. Bubble tea is used to represent Taiwan in the context of the Milk Tea Alliance. 50 Lan is a bubble tea chain founded in Tainan.\nHong Kong.\nHong Kong is famous for its traditional Hong Kong-style milk tea, which is made with brewed black tea and evaporated milk. While milk tea has long become integrated into people's daily life, the expansion of Taiwanese bubble tea chains, including Tiger Sugar, Youiccha, and Xing Fu Tang, into Hong Kong created a new wave for \"boba tea\".\nMainland China.\nSince the idea of adding tapioca pearls into milk tea was introduced into China in the 1990s, bubble tea has increased in popularity. In 2020 it was estimated that the consumption of bubble tea was 5 times that of coffee in recent years. According to data from QianZhen Industry Research Institute, the value of the tea-related beverage market in China reached (about ) in 2018. In 2019, annual sales from bubble tea shops reached as high as (roughly ). While bubble tea chains from Taiwan (e.g., Gong Cha and Coco) are still popular, more local brands, like Yi Dian Dian, Nayuki, Hey Tea, etc., are now dominating the market.\nIn China, young people's growing obsession with bubble tea shaped their way of social interaction. Buying someone a cup of bubble tea has become a new way of informally thanking someone. It is also a favored topic among friends and on social media.\nJapan.\nBubble tea first entered Japan by the late 1990s, but it failed to leave a lasting impression on the public markets. It was not until the 2010s when the bubble tea trend finally swept Japan. Shops from Taiwan, Korea, and China, as well as local brands, began to pop up in cities, and bubble tea has remained one of the hottest trends since then. Bubble tea has become so commonplace among teenagers that teenage girls in Japan invented slang for it: \"tapiru\" (\u30bf\u30d4\u308b). The word is short for drinking tapioca tea in Japanese, and it won first place in a survey of \"Japanese slang for middle school girls\" in 2018. A bubble tea theme park was open for a limited time in 2019 in Harajuku, Tokyo.\nSingapore.\nKnown locally in Chinese as (), bubble tea is loved by many in Singapore. The drink was sold in Singapore as early as 1992 and became phenomenally popular among young people in 2001. This soon ended because of the intense competition and price wars among shops. As a result, most bubble tea shops closed and bubble tea lost its popularity by 2003. When Taiwanese chains like Koi and Gong Cha came to Singapore in 2007 and 2009, the beverage experienced only short resurgences in popularity. In 2018, the interest in bubble tea rose again at an unprecedented speed in Singapore, as new brands like The Alley and Tiger Sugar entered the market; social media also played an important role in driving this renaissance of bubble tea.\nUnited States.\nIn the 1990s, Taiwanese immigrants began to introduce bubble tea in Taiwanese restaurants in California. Some of the first stand-alone bubble tea shops can be traced to a food court in Arcadia, in Southern California, and Fantasia Coffee &amp; Tea in Cupertino, in Northern California. Chains like Tapioca Express, Quickly, Lollicup and Happy Lemon emerged in the late 1990s and early 2000s, bringing the Taiwanese bubble tea trend to the US. Within the Asian American community, bubble tea is commonly known under its colloquial term \"boba\".\nAs the beverage gained popularity in the US, it gradually became more than a drink, but a cultural identity for Asian Americans. This phenomenon was referred to as \u201cboba life\u201d by Chinese-American brothers Andrew and David Fung in their music video, \u201cBobalife,\u201d released in 2013. Boba symbolizes a subculture that Asian Americans as social minorities could define themselves as, and \u201cboba life\u201d is a reflection of their desire for both cultural and political recognition. It is also used disparagingly in the term boba liberal, a term that derides mainstream Asian-American liberalism.\nOther regions with large concentrations of bubble tea restaurants in the United States are the Northeast and Southwest. This is reflected in the coffeehouse-style teahouse chains that originate from the regions, such as Boba Tea Company from Albuquerque, New Mexico, No. 1 Boba Tea in Las Vegas, Nevada, and Kung Fu Tea from New York City. Albuquerque and Las Vegas have a large concentrations of boba tea restaurants, as the drink is popular especially among the Hispano, Navajo, Pueblo, and other Native American, Hispanic and Latino American communities in the Southwest.\nA massive shipping and supply chain crisis on the U.S. West coast, coupled with the obstruction of the Suez Canal in March 2021, caused a shortage of tapioca pearls for bubble tea shops in the U.S. and Canada. Most of the tapioca consumed in the U.S. is imported from Asia, since the critical ingredient, tapioca starch, is mostly grown in Asia.\nTikTok trends and the Korean Wave also fueled the popularity of bubble tea in the United States.\nVietnam.\nTaiwanese milk tea was introduced to Vietnam in the early 2000s, but it took a few years for this drink to become popular with young people. Roadside stalls and carts rarely served milk tea, and the milk tea trend gradually cooled down in the late 2000s. Many shops had to liquidate or close, while others struggled to survive. Bubble tea also gained controversy because of information about tea of unknown origin, tapioca pearls allegedly being made from polymer plastics, etc.\nBy 2012, Taiwanese brands arrived in Vietnam, still the same old milk tea but served in a completely new style: milk tea with toppings, developing a chain model, and a space designed as well as any famous coffee shop. Also, the halo of Taiwanese milk tea gradually returned, especially around the end of 2016, to the beginning of 2017.\nAccording to a survey by Lozi, in 2017, the Vietnamese milk tea market witnessed an explosion with 100 large and small brands coexisting and over 1,500 points of sale, including major brands from Taiwan such as Ding Tea, Gong Cha, BoBaPop, and Tien Huong. This survey also shows that milk tea is becoming a popular drink in Vietnam when 53% of people are confirmed to drink milk tea at least once a week.\nFrom the consumer perspective, milk tea is characterized by its sweet, creamy taste, suitable for many customers, not only students, but also children and office workers. In addition, milk tea is constantly \"transforming\" to meet all customer needs, from cheese cream tea, fruit tea to low-fat tea. Another important point that makes milk tea popular is the service style. Instead of small shops and school gate carts like in the past, the milk tea is designed into a spacious space, with fixed seats, and cool air conditioning.\nKorea.\nMilk tea is not only a daily drink, but it has also become a \"fever drink\" loved in many countries, including South Korea. In the capital Seoul alone, there are 4 famous milk tea shops, which are popular places for entertainment, dating, and meeting of Korean youth every weekend, which are Gong Cha, Cofioca, Amasvin, and Happy Lemon.\nIn Korea, there are many different large and small milk tea shops, famous brands or just small shops with a drink counter and a table. Although pearl milk tea originated in Taiwan, it took certain changes in Korea. Koreans are very concerned about keeping in shape, every meal they have to check exactly how many calories they take in, so that they can do appropriate exercises to burn off excess fat. Therefore, when entering restaurants or bakeries in Korea, we will see the calorie index recorded very carefully as a way to protect the health of consumers. For example, at Gong Cha milk tea shops there, customers can choose the sweetness of their milk tea by choosing the sugar level (0% - 30% - 50% - 70% and 100%) and similarly choose ice to add personal favorite flavor to their milk tea.\nAustralia.\nIndividual bubble tea shops began to appear in Australia in the 1990s, along with other regional drinks like Eis Cendol. Chains of stores were established as early as 2002, when the Bubble Cup franchise opened its first store in Melbourne. Although originally associated with the rapid growth of immigration from Asia and the vast tertiary student cohort from Asia, in Melbourne and Sydney bubble tea has become popular across many communities.\nMauritius.\nThe first bubble tea shop in Mauritius opened in late 2012, and since then there have been bubble tea shops in most shopping malls on the island. Bubble tea shops have become a popular place for teenagers to hang out.\nCultural influence.\nIn 2020, the Unicode Consortium released the bubble tea emoji as part of its version 13.0 update.\nOn 29 January 2023, Google celebrated Bubble Tea with a doodle.\nIn 2024, Canadian bubble tea company \u2018Bobba\u2019 went on CBC's Dragon's Den to pitch their drinks to potential investors. The company co-owners Sebastien Fiset and Jessica Frenette faced criticism from guest judge Simu Liu regarding their authenticity on the drinks\u2019 cultural origins, to which they defended their intentions by stating their Taiwanese-based suppliers and their desire to modernize the traditional bubble tea drink. This moment in the show\u2019s episode went viral across social platforms and media outlets, sparking debate on whether the westernization of bubble tea had consequently led to it being culturally appropriated. Harmful Asian stereotypes were also recognized from their sales pitch, such as, quote: \"never [being] quite sure about its contents,\" which references the 'Chinese restaurant syndrome' rhetoric.\nPotential health concerns.\nIn July 2019, Singapore's Mount Alvernia Hospital warned against the high sugar content of bubble tea since the drink had become extremely popular in Singapore. While it acknowledged the benefits of drinking green tea and black tea in reducing risk of cardiovascular disease, diabetes, arthritis and cancer, respectively, the hospital cautions that the addition of other ingredients like non-dairy creamer and toppings in the tea could raise the fat and sugar content of the tea and increase the risk of chronic diseases. Non-dairy creamer is a milk substitute that contains trans fat in the form of hydrogenated palm oil. The hospital warned that this oil has been strongly correlated with an increased risk of heart disease and stroke.\nThe other concern about bubble tea is its high calorie content, partially attributed to the high-carbohydrate tapioca pearls (), which can make up to half the calorie-count in a serving of bubble tea."}
{"id": "4049", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=4049", "title": "Battle of Blenheim", "text": "The Battle of Blenheim (; ; ) fought on , was a major battle of the War of the Spanish Succession. The overwhelming Allied victory ensured the safety of Vienna from the Franco-Bavarian army, thus preventing the collapse of the reconstituted Grand Alliance.\nLouis XIV of France sought to knock the Holy Roman Emperor, Leopold, out of the war by seizing Vienna, the Habsburg capital, and gain a favourable peace settlement. The dangers to Vienna were considerable: Maximilian II Emanuel, Elector of Bavaria, and Marshal Ferdinand de Marsin's forces in Bavaria threatened from the west, and Marshal Louis Joseph de Bourbon, duc de Vend\u00f4me's large army in northern Italy posed a serious danger with a potential offensive through the Brenner Pass. Vienna was also under pressure from R\u00e1k\u00f3czi's Hungarian revolt from its eastern approaches. Realising the danger, the Duke of Marlborough resolved to alleviate the peril to Vienna by marching his forces south from Bedburg to help maintain Emperor Leopold within the Grand Alliance.\nA combination of deception and skilled administration \u2013 designed to conceal his true destination from friend and foe alike \u2013 enabled Marlborough to march unhindered from the Low Countries to the River Danube in five weeks. After securing Donauw\u00f6rth on the Danube, Marlborough sought to engage Maximilian's and Marsin's army before Marshal Camille d'Hostun, duc de Tallard, could bring reinforcements through the Black Forest. The Franco-Bavarian commanders proved reluctant to fight until their numbers were deemed sufficient, and Marlborough failed in his attempts to force an engagement. When Tallard arrived to bolster Maximilian's army, and Prince Eugene of Savoy arrived with reinforcements for the Allies, the two armies finally met on the banks of the Danube in and around the small village of Blindheim, from which the English \"Blenheim\" is derived.\nBlenheim was one of the battles that altered the course of the war, which until then was favouring the French and Spanish Bourbons. Although the battle did not win the war, it prevented a potentially devastating loss for the Grand Alliance and shifted the war's momentum, ending French plans of knocking Emperor Leopold out of the war. The French suffered catastrophic casualties in the battle, including their commander-in-chief, Tallard, who was taken captive to England. Before the 1704 campaign ended, the Allies had taken Landau, and the towns of Trier and Trarbach on the Moselle in preparation for the following year's campaign into France itself. This offensive never materialised, for the Grand Alliance's army had to depart the Moselle to defend Li\u00e8ge from a French counter-offensive. The war continued for another decade before ending in 1714.\nBackground.\nBy 1704, the War of the Spanish Succession was in its fourth year. The previous year had been one of successes for France and her allies, most particularly on the Danube, where Marshal Claude-Louis-Hector de Villars and Maximilian II Emanuel, Elector of Bavaria, had created a direct threat to Vienna, the Habsburg capital. Vienna had been saved by dissension between the two commanders, leading to Villars being replaced by the less dynamic Marshal Ferdinand de Marsin. Nevertheless, the threat was still real: R\u00e1k\u00f3czi's Hungarian revolt was threatening the Empire's eastern approaches, and Marshal Louis Joseph, Duke of Vend\u00f4me's forces threatened an invasion from northern Italy. In the courts of Versailles and Madrid, Vienna's fall was confidently anticipated, an event which would almost certainly have led to the collapse of the reconstituted Grand Alliance.\nTo isolate the Danube from any Allied intervention, Marshal Fran\u00e7ois de Neufville, duc de Villeroi's 46,000 troops were expected to pin the 70,000 Dutch and British troops around Maastricht in the Low Countries, while General Robert Jean Antoine de Franquetot de Coigny protected Alsace against surprise with a further corps. The only forces immediately available for Vienna's defence were the imperial army under Margrave Louis William of Baden of 36,000 men stationed in the Lines of Stollhofen to watch Marshal Camille d'Hostun, duc de Tallard, at Strasbourg; and 10,000 men under Prince Eugene of Savoy south of Ulm.\nVarious Allied statesmen, including the Imperial Austrian Ambassador in London, Count Wratislaw, and the Duke of Marlborough realised the implications of the situation on the Danube. To maintain secrecy, Marlborough kept his plans hidden from both the Dutch States General and the Parliament of England. In the Dutch Republic, only a select few \u2013 Grand Pensionary Anthonie Heinsius, Simon van Slingelandt, , and \u2013 were privy to his strategy from the outset. In England, Marlborough confided only in Sidney Godolphin, Queen Anne, and her husband. Marlborough, realising the only way to reinforce the Austrians was by the use of secrecy and guile, pretended to move his troops to the Moselle \u2013 a plan approved of by the Dutch States General \u2013 but once there, he would move further and link up with Austrian forces in southern Germany.\nThe Dutch diplomat and field deputy Van Rechteren-Almelo would come to play an important role. He made sure that on their 450-kilometre-long march, the Allies would nowhere be denied passage by local rulers, nor would they need to look for provisions, horsefeed or new boots. He also saw to it that sufficient stopovers were arranged along the way to ensure that the Allies arrived at their destination in good condition. This was of paramount importance, for the success of the operation depended on a quick elimination of the Bavarian elector. However, it was not possible to make the logistical arrangements in advance that would have been indispensable to supply the Allied army south of the Danube. For this, the Allies should have had access to the free imperial cities of Ulm and Augsburg, but the Bavarian elector had taken these two cities. This could have become a problem for Marlborough had the Elector avoided a battle and instead entrenched himself south of the Danube. Had Villeroy then managed to take advantage of the weakening of Allied forces in the Netherlands by recapturing Li\u00e8ge and besieging Maastricht, it would have validated the concerns of some of his Dutch adversaries, who were against any major weakening of the forces in the Spanish Netherlands.\nPrelude.\nProtagonists march to the Danube.\nMarlborough's march started on 19 May from Bedburg, northwest of Cologne. The army assembled by Marlborough's brother, General Charles Churchill, consisted of 66 squadrons of cavalry, 31 battalions of infantry and 38 guns and mortars, totalling 21,000 men, 16,000 of whom were British. This force was augmented en route, and by the time it reached the Danube it numbered 40,00047 battalions and 88 squadrons. While Marlborough led this army south, the Dutch general, Henry Overkirk, Count of Nassau, maintained a defensive position in the Dutch Republic against the possibility of Villeroi mounting an attack. Marlborough had assured the Dutch that if the French were to launch an offensive he would return in good time, but he calculated that as he marched south, the French army would be drawn after him. In this assumption Marlborough proved correct: Villeroi shadowed him with 30,000 men in 60 squadrons and 42 battalions. Marlborough wrote to Godolphin: \"I am very sensible that I take a great deal upon me, but should I act otherwise, the Empire would be undone ...\"\nIn the meantime, the appointment of Henry Overkirk as Field Marshal caused significant controversy in the Dutch Republic. After the Earl of Athlone's death, the Dutch States General had put Overkirk in charge of the Dutch States Army, which led to much discontent among the other high-ranking Dutch generals. Ernst Wilhelm von Salisch, Dani\u00ebl van Dopff and Menno van Coehoorn threatened to resign or go into the service of other countries, although all were eventually convinced to stay. The new infantry generals were also disgruntled \u2014 the Lord of Slangenburg because he had to serve the less experienced Overkirk; and the Count of Noyelles because he had to serve the orders of the 'insupportable' Slangenburg. Then there was the major problem of the position of the Prince of Orange. The provinces of Friesland and Groningen demanded that their 17-year-old stadtholder be appointed supreme infantry general. This divided the parties so much that a second , as had existed in 1651, was considered. However, after pressure from the other provinces, Friesland and Groningen adjusted their demands and a compromise was found. The Prince of Orange would nominally be appointed infantry general, behind Slangenburg and Noyelles, but he would not really be in command until he was 20.\nWhile the Allies were making their preparations, the French were striving to maintain and re-supply Marsin. He had been operating with Maximilian II against Margrave Louis William, and was somewhat isolated from France: his only lines of communication lay through the rocky passes of the Black Forest. On 14 May, Tallard brought 8,000 reinforcements and vast supplies and munitions through the difficult terrain, whilst outmanoeuvring , the Imperial general who sought to block his path. Tallard then returned with his own force to the Rhine, once again side-stepping Th\u00fcngen's efforts to intercept him.\nOn 26 May, Marlborough reached Coblenz, where the Moselle meets the Rhine. If he intended an attack along the Moselle his army would now have to turn west; instead it crossed to the right bank of the Rhine, and was reinforced by 5,000 waiting Hanoverians and Prussians. The French realised that there would be no campaign on the Moselle. A second possible objective now occurred to theman Allied incursion into Alsace and an attack on Strasbourg. Marlborough furthered this apprehension by constructing bridges across the Rhine at Philippsburg, a ruse that not only encouraged Villeroi to come to Tallard's aid in the defence of Alsace, but one that ensured the French plan to march on Vienna was delayed while they waited to see what Marlborough's army would do.\nEncouraged by Marlborough's promise to return to the Netherlands if a French attack developed there, transferring his troops up the Rhine on barges at a rate of a day, the Dutch States General agreed to release the Danish contingent of seven battalions and 22 squadrons as reinforcements. Marlborough reached Ladenburg, in the plain of the Neckar and the Rhine, and there halted for three days to rest his cavalry and allow the guns and infantry to close up. On 6 June he arrived at Wiesloch, south of Heidelberg. The following day, the Allied army swung away from the Rhine towards the hills of the Swabian Jura and the Danube beyond. At last Marlborough's destination was established without doubt.\nStrategy.\nOn 10 June, Marlborough met for the first time the President of the Imperial War Council, Prince Eugene \u2013 accompanied by Count Wratislaw \u2013 at the village of Mundelsheim, halfway between the Danube and the Rhine. By 13 June, the Imperial Field Commander, Margrave Louis William of Baden, had joined them in Gro\u00dfheppach. The three generals commanded a force of nearly 110,000 men. At this conference, it was decided that Prince Eugene would return with 28,000 men to the Lines of Stollhofen on the Rhine to watch Villeroi and Tallard and prevent them going to the aid of the Franco-Bavarian army on the Danube. Meanwhile, Marlborough's and Margrave Louis William's forces would combine, totalling 80,000 men, and march on the Danube to seek out Maximilian II and Marsin before they could be reinforced.\nKnowing Marlborough's destination, Tallard and Villeroi met at Landau in the Palatinate on 13 June to construct a plan to save Bavaria. The rigidity of the French command system was such that any variations from the original plan had to be sanctioned by Versailles. The Count of M\u00e9rode-Westerloo, commander of the Flemish troops in Tallard's army, wrote \"One thing is certain: we delayed our march from Alsace for far too long and quite inexplicably.\" Approval from King Louis arrived on 27 June: Tallard was to reinforce Marsin and Maximilian II on the Danube via the Black Forest, with 40 battalions and 50 squadrons; Villeroi was to pin down the Allies defending the Lines of Stollhofen, or, if the Allies should move all their forces to the Danube, he was to join with Tallard; Coigny with 8,000 men would protect Alsace. On 1 July Tallard's army of 35,000 re-crossed the Rhine at Kehl and began its march.\nOn 22 June, Marlborough's forces linked up with the Imperial forces at Launsheim, having covered in five weeks. Thanks to a carefully planned timetable, the effects of wear and tear had been kept to a minimum. Captain Parker described the march discipline: \"As we marched through the country of our Allies, commissars were appointed to furnish us with all manner of necessaries for man and horse ... the soldiers had nothing to do but pitch their tents, boil kettles and lie down to rest.\" In response to Marlborough's manoeuvres, Maximilian and Marsin, conscious of their numerical disadvantage with only 40,000 men, moved their forces to the entrenched camp at Dillingen on the north bank of the Danube. Marlborough could not attack Dillingen because of a lack of siege guns \u2013 he had been unable to bring any from the Low Countries, and Margrave Louis William had failed to supply any, despite prior assurances that he would.\nThe Allies needed a base for provisions and a good river crossing. Consequently, on 2 July Marlborough stormed the fortress of Schellenberg on the heights above the town of Donauw\u00f6rth. Count Jean d'Arco had been sent with 12,000 men from the Franco-Bavarian camp to hold the town and grassy hill, but after a fierce battle, with heavy casualties on both sides, Schellenberg fell. This forced Donauw\u00f6rth to surrender shortly afterward. Maximilian, knowing his position at Dillingen was now not tenable, took up a position behind the strong fortifications of Augsburg.\nTallard's march presented a dilemma for Prince Eugene. If the Allies were not to be outnumbered on the Danube, he realised that he had to either try to cut Tallard off before he could get there, or to reinforce Marlborough. If he withdrew from the Rhine to the Danube, Villeroi might also make a move south to link up with Maximilian and Marsin. Prince Eugene compromisedleaving 12,000 troops behind guarding the Lines of Stollhofenhe marched off with the rest of his army to forestall Tallard.\nLacking in numbers, Prince Eugene could not seriously disrupt Tallard's march but the French marshal's progress was proving slow. Tallard's force had suffered considerably more than Marlborough's troops on their march \u2013 many of his cavalry horses were suffering from glanders and the mountain passes were proving tough for the 2,000 wagonloads of provisions. Local German peasants, angry at French plundering, compounded Tallard's problems, leading M\u00e9rode-Westerloo to bemoan \u2013 \"the enraged peasantry killed several thousand of our men before the army was clear of the Black Forest.\"\nAt Augsburg, Maximilian was informed on 14 July that Tallard was on his way through the Black Forest. This good news bolstered his policy of inaction, further encouraging him to wait for the reinforcements. This reticence to fight induced Marlborough to undertake a controversial policy of spoliation in Bavaria, burning buildings and crops throughout the rich lands south of the Danube. This had two aims: firstly to put pressure on Maximilian to fight or come to terms before Tallard arrived with reinforcements; and secondly, to ruin Bavaria as a base from which the French and Bavarian armies could attack Vienna, or pursue Marlborough into Franconia if, at some stage, he had to withdraw northwards. But this destruction, coupled with a protracted siege of the town of Rain over 9 to 16 July, caused Prince Eugene to lament \"...\u00a0since the Donauw\u00f6rth action I cannot admire their performances\", and later to conclude \"If he has to go home without having achieved his objective, he will certainly be ruined.\"\nFinal positioning.\nTallard, with 34,000 men, reached Ulm, joining with Maximilian and Marsin at Augsburg on 5 August, although Maximilian had dispersed his army in response to Marlborough's campaign of ravaging the region. Also on 5 August, Prince Eugene reached H\u00f6chst\u00e4dt, riding that same night to meet with Marlborough at Schrobenhausen. Marlborough knew that another crossing point over the Danube was required in case Donauw\u00f6rth fell to the enemy; so on 7 August, the first of Margrave Louis William's 15,000 Imperial troops left Marlborough's main force to besiege the heavily defended city of Ingolstadt, farther down the Danube, with the remainder following two days later.\nWith Prince Eugene's forces at H\u00f6chst\u00e4dt on the north bank of the Danube, and Marlborough's at Rain on the south bank, Tallard and Maximilian debated their next move. Tallard preferred to bide his time, replenish supplies and allow Marlborough's Danube campaign to flounder in the colder autumn weather; Maximilian and Marsin, newly reinforced, were keen to push ahead. The French and Bavarian commanders eventually agreed to attack Prince Eugene's smaller force. On 9 August, the Franco-Bavarian forces began to cross to the north bank of the Danube. On 10 August, Prince Eugene sent an urgent dispatch reporting that he was falling back to Donauw\u00f6rth. By a series of swift marches Marlborough concentrated his forces on Donauw\u00f6rth and, by noon 11 August, the link-up was complete.\nDuring 11 August, Tallard pushed forward from the river crossings at Dillingen. By 12 August, the Franco-Bavarian forces were encamped behind the small River Nebel near the village of Blenheim on the plain of H\u00f6chst\u00e4dt. On the same day, Marlborough and Prince Eugene carried out a reconnaissance of the French position from the church spire at Tapfheim, and moved their combined forces to M\u00fcnster \u2013 from the French camp. A French reconnaissance under Jacques Joseph Vipart, Marquis de Silly went forward to probe the enemy, but were driven off by Allied troops who had deployed to cover the pioneers of the advancing army, labouring to bridge the numerous streams in the area and improve the passage leading westwards to H\u00f6chst\u00e4dt. Marlborough quickly moved forward two brigades under the command of Lieutenant General John Wilkes and Brigadier Archibald Rowe to secure the narrow strip of land between the Danube and the wooded Fuchsberg hill, at the Schwenningen defile. Tallard's army numbered 56,000 men and 90 guns; the army of the Grand Alliance, 52,000 men and 66 guns. Some Allied officers who were acquainted with the superior numbers of the enemy, and aware of their strong defensive position, remonstrated with Marlborough about the hazards of attacking; but he was resolute \u2013 partly because the Dutch officer Willem Vleertman had scouted the marshy ground before them and reported that the land was perfectly suitable for the troops.\nBattle.\nThe battlefield.\nThe battlefield stretched for nearly . The extreme right flank of the Franco-Bavarian army rested on the Danube, the undulating pine-covered hills of the Swabian Jura lay to their left. A small stream, the Nebel, fronted the French line; the ground either side of this was marshy and only fordable intermittently. The French right rested on the village of Blenheim near where the Nebel flows into the Danube; the village itself was surrounded by hedges, fences, enclosed gardens, and meadows. Between Blenheim and the village of Oberglauheim to the north west the fields of wheat had been cut to stubble and were now ideal for the deployment of troops. From Oberglauheim to the next hamlet of Lutzingen the terrain of ditches, thickets and brambles was potentially difficult ground for the attackers.\nInitial manoeuvres.\nAt 02:00 on 13 August, 40 Allied cavalry squadrons were sent forward, followed at 03:00, in eight columns, by the main Allied force pushing over the River Kessel. At about 06:00 they reached Schwenningen, from Blenheim. The British and German troops who had held Schwenningen through the night joined the march, making a ninth column on the left of the army. Marlborough and Prince Eugene made their final plans. The Allied commanders agreed that Marlborough would command 36,000 troops and attack Tallard's force of 33,000 on the left, including capturing the village of Blenheim, while Prince Eugene's 16,000 men would attack Maximilian and Marsin's combined forces of 23,000 troops on the right. If this attack was pressed hard, it was anticipated that Maximilian and Marsin would feel unable to send troops to aid Tallard on their right. Lieutenant-General John Cutts would attack Blenheim in concert with Prince Eugene's attack. With the French flanks busy, Marlborough could cross the Nebel and deliver the fatal blow to the French at their centre. The Allies would have to wait until Prince Eugene was in position before the general engagement could begin.\nTallard was not anticipating an Allied attack; he had been deceived by intelligence gathered from prisoners taken by de Silly the previous day, and his army's strong position. Tallard and his colleagues believed that Marlborough and Prince Eugene were about to retreat north-westwards towards N\u00f6rdlingen. Tallard wrote a report to this effect to King Louis that morning. Signal guns were fired to bring in the foraging parties and pickets as the French and Bavarian troops drew into battle-order to face the unexpected threat.\nAt around 08:00 the French artillery on their right wing opened fire, answered by Colonel Holcroft Blood's batteries. The guns were heard by Prince Louis in his camp before Ingolstadt. An hour later Tallard, Maximilian, and Marsin climbed Blenheim's church tower to finalise their plans. It was settled that Maximilian and Marsin would hold the front from the hills to Oberglauheim, whilst Tallard would defend the ground between Oberglauheim and the Danube. The French commanders were divided as to how to utilise the Nebel. Tallard's preferred tactic was to lure the Allies across before unleashing his cavalry upon them. This was opposed by Marsin and Maximilian who felt it better to close their infantry right up to the stream itself, so that while the enemy was struggling in the marshes, they would be caught in crossfire from Blenheim and Oberglauheim. Tallard's approach was sound if all its parts were implemented, but in the event it allowed Marlborough to cross the Nebel without serious interference and fight the battle he had planned.\nDeployment.\nThe Franco-Bavarian commanders deployed their forces. In the village of Lutzingen, Count Alessandro de Maffei positioned five Bavarian battalions with a great battery of 16 guns at the village's edge. In the woods to the left of Lutzingen, seven French battalions under C\u00e9sar Armand, Marquis de Rozel moved into place. Between Lutzingen and Oberglauheim Maximilian placed 27 squadrons of cavalry and 14 Bavarian squadrons commanded by d'Arco with 13 more in support nearby under Baron Veit Heinrich Moritz Freiherr von Wolframsdorf. To their right stood Marsin's 40 French squadrons and 12 battalions. The village of Oberglauheim was packed with 14 battalions commanded by , including the effective Irish Brigade known as the \"Wild Geese\". Six batteries of guns were ranged alongside the village. On the right of these French and Bavarian positions, between Oberglauheim and Blenheim, Tallard deployed 64 French and Walloon squadrons, 16 of which were from Marsin, supported by nine French battalions standing near the H\u00f6chst\u00e4dt road. In the cornfield next to Blenheim stood three battalions from the Regiment de Roi. Nine battalions occupied the village itself, commanded by Philippe, Marquis de Cl\u00e9rambault. Four battalions stood to the rear and a further eleven were in reserve. These battalions were supported by Count Gabriel d'Hautefeuille's twelve squadrons of dismounted dragoons. By 11:00 Tallard, Maximilian, and Marsin were in place. Many of the Allied generals were hesitant to attack such a strong position. The Earl of Orkney later said that, \"had I been asked to give my opinion, I had been against it.\"\nPrince Eugene was expected to be in position by 11:00, but due to the difficult terrain and enemy fire, progress was slow. Cutts' column \u2013 which by 10:00 had expelled the enemy from two water mills on the Nebel \u2013 had already deployed by the river against Blenheim, enduring over the next three hours severe fire from a six-gun heavy battery posted near the village. The rest of Marlborough's army, waiting in their ranks on the forward slope, were also forced to bear the cannonade from the French artillery, suffering 2,000 casualties before the attack could even start. Meanwhile, engineers repaired a stone bridge across the Nebel, and constructed five additional bridges or causeways across the marsh between Blenheim and Oberglauheim. Marlborough's anxiety was finally allayed when, just past noon, Colonel William Cadogan reported that Prince Eugene's Prussian and Danish infantry were in place \u2013 the order for the general advance was given. At 13:00, Cutts was ordered to attack the village of Blenheim whilst Prince Eugene was requested to assault Lutzingen on the Allied right flank.\nBlenheim.\nCutts ordered Rowe's brigade to attack. The English infantry rose from the edge of the Nebel, and silently marched towards Blenheim, a distance of some . James Ferguson's Scottish brigade supported Rowe's left, and moved towards the barricades between the village and the river, defended by Hautefeuille's dragoons. As the range closed to within , the French fired a deadly volley. Rowe had ordered that there should be no firing from his men until he struck his sword upon the palisades, but as he stepped forward to give the signal, he fell mortally wounded. The survivors of the leading companies closed up the gaps in their ranks and rushed forward. Small parties penetrated the defences, but repeated French volleys forced the English back and inflicted heavy casualties. As the attack faltered, eight squadrons of elite Gens d'Armes, commanded by the veteran Swiss officer, , fell on the English troops, cutting at the exposed flank of Rowe's own regiment. Wilkes' Hessian brigade, nearby in the marshy grass at the water's edge, stood firm and repulsed the Gens d'Armes with steady fire, enabling the English and Hessians to re-order and launch another attack.\nAlthough the Allies were again repulsed, these persistent attacks on Blenheim eventually bore fruit, panicking Cl\u00e9rambault into making the worst French error of the day. Without consulting Tallard, Cl\u00e9rambault ordered his reserve battalions into the village, upsetting the balance of the French position and nullifying the French numerical superiority. \"The men were so crowded in upon one another\", wrote M\u00e9rode-Westerloo, \"that they couldn't even fire \u2013 let alone receive or carry out any orders\". Marlborough, spotting this error, now countermanded Cutts' intention to launch a third attack, and ordered him simply to contain the enemy within Blenheim; no more than 5,000 Allied soldiers were able to pen in twice the number of French infantry and dragoons.\nLutzingen.\nOn the Allied right, Prince Eugene's Prussian and Danish forces were desperately fighting the numerically superior forces of Maximilian and Marsin. Leopold I, Prince of Anhalt-Dessau led forward four brigades across the Nebel to assault the well-fortified position of Lutzingen. Here, the Nebel was less of an obstacle, but the great battery positioned on the edge of the village enjoyed a good field of fire across the open ground stretching to the hamlet of Schwennenbach. As soon as the infantry crossed the stream, they were struck by Maffei's infantry, and salvoes from the Bavarian guns positioned both in front of the village and in enfilade on the wood-line to the right. Despite heavy casualties the Prussians attempted to storm the great battery, whilst the Danes, under Count Jobst von Scholten, attempted to drive the French infantry out of the copses beyond the village.\nWith the infantry heavily engaged, Prince Eugene's cavalry picked its way across the Nebel. After an initial success, his first line of cavalry, under the Imperial General of Horse, Prince Maximilian of Hanover, were pressed by the second line of Marsin's cavalry and forced back across the Nebel in confusion. The exhausted French were unable to follow up their advantage, and both cavalry forces tried to regroup and reorder their ranks. Without cavalry support, and threatened with envelopment, the Prussian and Danish infantry were in turn forced to pull back across the Nebel. Panic gripped some of Prince Eugene's troops as they crossed the stream. Ten infantry colours were lost to the Bavarians, and hundreds of prisoners taken; it was only through the leadership of Prince Eugene and the Prince Maximilian of Hanover that the Imperial infantry was prevented from abandoning the field.\nAfter rallying his troops near Schwennenbach \u2013 well beyond their starting point \u2013 Prince Eugene prepared to launch a second attack, led by the second-line squadrons under the Duke of W\u00fcrttemberg-Teck. Yet again they were caught in the murderous crossfire from the artillery in Lutzingen and Oberglauheim, and were once again thrown back in disarray. The French and Bavarians were almost as disordered as their opponents, and they too were in need of inspiration from their commander, Maximilian, who was seen \" ... riding up and down, and inspiring his men with fresh courage.\" Anhalt-Dessau's Danish and Prussian infantry attacked a second time but could not sustain the advance without proper support. Once again they fell back across the stream.\nCentre and Oberglauheim.\nWhilst these events around Blenheim and Lutzingen were taking place, Marlborough was preparing to cross the Nebel. Hulsen's brigade of Hessians and Hanoverians and the earl of Orkney's British brigade advanced across the stream and were supported by dismounted British dragoons and ten British cavalry squadrons. This covering force allowed Charles Churchill's Dutch, British and German infantry and further cavalry units to advance and form up on the plain beyond. Marlborough arranged his infantry battalions in a novel manner, with gaps sufficient to allow the cavalry to move freely between them. He ordered the formation forward. Once again Zurlauben's Gens d'Armes charged, looking to rout Henry Lumley's English cavalry who linked Cutts' column facing Blenheim with Churchill's infantry. As the elite French cavalry attacked, they were faced by five English squadrons under Colonel Francis Palmes. To the consternation of the French, the Gens d'Armes were pushed back in confusion and were pursued well beyond the Maulweyer stream that flows through Blenheim. \"What? Is it possible?\" exclaimed Maximilian, \"the gentlemen of France fleeing?\" Palmes attempted to follow up his success but was repulsed by other French cavalry and musket fire from the edge of Blenheim.\nNevertheless, Tallard was alarmed by the repulse of the Gens d'Armes and urgently rode across the field to ask Marsin for reinforcements; but on the basis of being hard pressed by Prince Eugene \u2013 whose second attack was in full flood \u2013 Marsin refused. As Tallard consulted with Marsin, more of his infantry were taken into Blenheim by Cl\u00e9rambault. Fatally, Tallard, although aware of the situation, did nothing to rectify it, leaving him with just the nine battalions of infantry near the H\u00f6chst\u00e4dt road to oppose the massed enemy ranks in the centre. Zurlauben tried several more times to disrupt the Allies forming on Tallard's side of the stream. His front-line cavalry darted forward down the gentle slope towards the Nebel, but the attacks lacked co-ordination, and the Allied infantry's steady volleys disconcerted the French horsemen. During these skirmishes Zurlauben fell mortally wounded; he died two days later. At this stage the time was just after 15:00.\nThe Danish cavalry, under Carl Rudolf, Duke of W\u00fcrttemberg-Neuenstadt, had made slow work of crossing the Nebel near Oberglauheim. Harassed by Marsin's infantry near the village, the Danes were driven back across the stream. Count Horn's Dutch infantry managed to push the French back from the water's edge, but it was apparent that before Marlborough could launch his main effort against Tallard, Oberglauheim would have to be secured.\nCount Horn directed Anton G\u00fcnther, F\u00fcrst von Holstein-Beck to take the village, but his two Dutch brigades were cut down by the French and Irish troops, capturing and badly wounding Holstein-Beck during the action. The battle was now in the balance. If Holstein-Beck's Dutch column were destroyed, the Allied army would be split in two: Prince Eugene's wing would be isolated from Marlborough's, passing the initiative to the Franco-Bavarian forces. Seeing the opportunity, Marsin ordered his cavalry to change from facing Prince Eugene, and turn towards their right and the open flank of Churchill's infantry drawn up in front of Unterglau. Marlborough, who had crossed the Nebel on a makeshift bridge to take personal control, ordered Hulsen's Hanoverian battalions to support the Dutch infantry. A nine-gun artillery battery and a Dutch cavalry brigade under Averock were also called forward, but the cavalry soon came under pressure from Marsin's more numerous squadrons.\nMarlborough now requested Prince Eugene to release Count Hendrick Fugger and his Imperial Cuirassier brigade to help repel the French cavalry thrust. Despite his own difficulties, Prince Eugene at once complied. Although the Nebel stream lay between Fugger's and Marsin's squadrons, the French were forced to change front to meet this new threat, thus preventing Marsin from striking at Marlborough's infantry. Fugger's cuirassiers charged and, striking at a favourable angle, threw back Marsin's squadrons in disorder. With support from Blood's batteries, the Hessian, Hanoverian and Dutch infantry \u2013 now commanded by Count Berensdorf \u2013 succeeded in pushing the French and Irish infantry back into Oberglauheim so that they could not again threaten Churchill's flank as he moved against Tallard. The French commander in the village, de Blainville, was numbered among the heavy casualties.\nBreakthrough.\nBy 16:00, with large parts of the Franco-Bavarian army besieged in Blenheim and Oberglau, the Allied centre of 81 squadrons (nine squadrons had been transferred from Cutts' column) supported by 18 battalions was firmly planted amidst the French line of 64 squadrons and nine battalions of raw recruits. There was now a pause in the battle: Marlborough wanted to attack simultaneously along the whole front, and Prince Eugene, after his second repulse, needed time to reorganise.\nBy just after 17:00 all was ready along the Allied front. Marlborough's two lines of cavalry had now moved to the front of his line of battle, with the two supporting lines of infantry behind them. M\u00e9rode-Westerloo attempted to extricate some French infantry crowded into Blenheim, but Cl\u00e9rambault ordered the troops back into the village. The French cavalry exerted themselves once more against the Allied first line \u2013 Lumley's English and Scots on the Allied left, and Reinhard Vincent Graf von Hompesch's Dutch and German squadrons on the Allied right. Tallard's squadrons, which lacked infantry support and were tired, managed to push the Allied first line back to their infantry support. With the battle still not won, Marlborough had to rebuke one of his cavalry officers who was attempting to leave the field \u2013 \"Sir, you are under a mistake, the enemy lies that way ...\" Marlborough commanded the second Allied line, under and , to move forward, and, driving through the centre, the Allies finally routed Tallard's tired cavalry. The Prussian Life Dragoons' Colonel, Ludwig von Blumenthal, and his second in command, Lieutenant Colonel von Hacke, fell next to each other, but the charge succeeded. With their cavalry in headlong flight, the remaining nine French infantry battalions fought with desperate valour, trying to form a square, but they were overwhelmed by Blood's close-range artillery and platoon fire. M\u00e9rode-Westerloo later wrote \u2013 \"[They] died to a man where they stood, stationed right out in the open plain \u2013 supported by nobody.\"\nThe majority of Tallard's retreating troops headed for H\u00f6chst\u00e4dt but most did not make the safety of the town, plunging instead into the Danube where over 3,000 French horsemen drowned; others were cut down by the pursuing Allied cavalry. The Marquis de Gruignan attempted a counter-attack, but he was brushed aside by the triumphant Allies. After a final rally behind his camp's tents, shouting entreaties to stand and fight, Tallard was caught up in the rout and swept towards Sonderheim. Surrounded by a squadron of Hessian troops, Tallard surrendered to Lieutenant Colonel de Boinenburg, the Prince of Hesse-Kassel's \"aide-de-camp\", and was sent under escort to Marlborough. Marlborough welcomed the French commander \u2013 \"I am very sorry that such a cruel misfortune should have fallen upon a soldier for whom I have the highest regard.\"\nMeanwhile, the Allies had once again attacked the Bavarian stronghold at Lutzingen. Prince Eugene became exasperated with the performance of his Imperial cavalry whose third attack had failed: he had already shot two of his troopers to prevent a general flight. Then, declaring in disgust that he wished to \"fight among brave men and not among cowards\", Prince Eugene went into the attack with the Prussian and Danish infantry, as did Leopold I, waving a regimental colour to inspire his troops. This time the Prussians were able to storm the great Bavarian battery, and overwhelm the guns' crews. Beyond the village, Scholten's Danes defeated the French infantry in a desperate hand-to-hand bayonet struggle. When they saw that the centre had broken, Maximilian and Marsin decided the battle was lost; like the remnants of Tallard's army, they fled the battlefield, albeit in better order than Tallard's men. Attempts to organise an Allied force to prevent Marsin's withdrawal failed owing to the exhaustion of the cavalry, and the growing confusion in the field.\nFall of Blenheim.\nMarlborough now turned his attention from the fleeing enemy to direct Churchill to detach more infantry to storm Blenheim. Orkney's infantry, Hamilton's English brigade and St Paul's Hanoverians moved across the trampled wheat to the cottages. Fierce hand-to-hand fighting gradually forced the French towards the village centre, in and around the walled churchyard which had been prepared for defence. Lord John Hay and Charles Ross's dismounted dragoons were also sent, but suffered under a counter-charge delivered by the regiments of Artois and Provence under command of Colonel de la Silvi\u00e8re. Colonel Belville's Hanoverians were fed into the battle to steady the resolve of the dragoons, who attacked again. The Allied progress was slow and hard, and like the defenders, they suffered many casualties.\nMany of the cottages were now burning, obscuring the field of fire and driving the defenders out of their positions. Hearing the din of battle in Blenheim, Tallard sent a message to Marlborough offering to order the garrison to withdraw from the field. \"Inform Monsieur Tallard\", replied Marlborough, \"that, in the position in which he is now, he has no command.\" Nevertheless, as dusk came the Allied commander was anxious for a quick conclusion. The French infantry fought tenaciously to hold on to their position in Blenheim, but their commander was nowhere to be found. By now Blenheim was under assault from every side by three British generals: Cutts, Churchill, and Orkney. The French had repulsed every attack, but many had seen what had happened on the plain: their army was routed and they were cut off. Orkney, attacking from the rear, now tried a different tactic \u2013 \"...\u00a0it came into my head to beat parley\", he later wrote, \"which they accepted of and immediately their Brigadier de Nouville capitulated with me to be prisoner at discretion and lay down their arms.\" Threatened by Allied guns, other units followed their example. It was not until 21:00 that the Marquis de Blanzac, who had taken charge in Cl\u00e9rambault's absence, reluctantly accepted the inevitability of defeat, and some 10,000 of France's best infantry had laid down their arms.\nDuring these events Marlborough was still in the saddle organising the pursuit of the broken enemy. Pausing for a moment, he scribbled on the back of an old tavern bill a note addressed to his wife, Sarah: \"I have no time to say more but to beg you will give my duty to the Queen, and let her know her army has had a glorious victory.\"\nAftermath.\nFrench losses were immense, with over 27,000 killed, wounded and captured. Moreover, the myth of French invincibility had been destroyed, and King Louis's hopes of a victorious early peace were over. M\u00e9rode-Westerloo summarised the case against Tallard's army: It was a hard-fought contest: Prince Eugene observed that \"I have not a squadron or battalion which did not charge four times at least.\"\nAlthough the war dragged on for years, the Battle of Blenheim was probably its most decisive victory; Marlborough and Prince Eugene had saved the Habsburg Empire and thereby preserved the Grand Alliance from collapse. Munich, Augsburg, Ingolstadt, Ulm and the remaining territory of Bavaria soon fell to the Allies. By the Treaty of Ilbersheim, signed on 7 November, Bavaria was placed under Austrian military rule, allowing the Habsburgs to use its resources for the rest of the conflict.\nThe remnants of Maximilian and Marsin's wing limped back to Strasbourg, losing another 7,000 men through desertion. Despite being offered the chance to remain as ruler of Bavaria, under the strict terms of an alliance with Austria, Maximilian left his country and family in order to continue the war against the Allies from the Spanish Netherlands where he still held the post of governor-general. Tallard \u2013 who, unlike his subordinates, was not ransomed or exchanged \u2013 was taken to England and imprisoned in Nottingham until his release in 1711.\nThe 1704 campaign lasted longer than usual, for the Allies sought to extract the maximum advantage. Realising that France was too powerful to be forced to make peace by a single victory, Prince Eugene, Marlborough and Prince Louis met to plan their next moves. For the following year Marlborough proposed a campaign along the valley of the Moselle to carry the war deep into France. This required the capture of the major fortress of Landau which guarded the Rhine, and the towns of Trier and Trarbach on the Moselle itself. Trier was taken on 27 October and Landau fell on 23 November to Prince Louis and Prince Eugene; with the fall of Trarbach on 20 December, the campaign season for 1704 came to an end. The planned offensive never materialised as the Grand Alliance's army had to depart the Moselle to defend Li\u00e8ge from a French counteroffensive. The war raged on for another decade.\nMarlborough returned to England on 14 December (O.S) to the acclamation of Queen Anne and the country. In the first days of January, the 110 cavalry standards and 128 infantry colours that had been captured during the battle were borne in procession to Westminster Hall. In February 1705, Queen Anne, who had made Marlborough a duke in 1702, granted him the Park of Woodstock Palace and promised a sum of \u00a3240,000 to build a suitable house as a gift from a grateful Crown in recognition of his victory; this resulted in the construction of Blenheim Palace. The British historian Sir Edward Shepherd Creasy considered Blenheim one of the pivotal battles in history, writing: \"Had it not been for Blenheim, all Europe might at this day suffer under the effect of French conquests resembling those of Alexander in extent and those of the Romans in durability.\" The military historian John A. Lynn considers this claim unjustified, for King Louis never had such an objective; the campaign in Bavaria was intended only to bring a favourable peace settlement and not domination over Europe.\nLake Poet Robert Southey criticised the Battle of Blenheim in his anti-war poem \"After Blenheim\", but later praised the victory as \"the greatest victory which had ever done honour to British arms\"."}
{"id": "4050", "revid": "38795418", "url": "https://en.wikipedia.org/wiki?curid=4050", "title": "Battle of Ramillies", "text": "The Battle of Ramillies (), fought on 23 May 1706, was a battle of the War of the Spanish Succession. For the Grand AllianceAustria, England, and the Dutch Republicthe battle had followed an indecisive campaign against the Bourbon armies of King Louis\u00a0XIV of France in 1705. Although the Allies had captured Barcelona that year, they had been forced to abandon their campaign on the Moselle, had stalled in the Spanish Netherlands and suffered defeat in northern Italy. Yet despite his opponents' setbacks LouisXIV wanted peace, but on reasonable terms. Because of this, as well as to maintain their momentum, the French and their allies took the offensive in 1706.\nThe campaign began well for Louis XIV's generals: in Italy Marshal Vend\u00f4me defeated the Austrians at the Battle of Calcinato in April, while in Alsace Marshal Villars forced the Margrave of Baden back across the Rhine. Encouraged by these early gains LouisXIV urged Marshal Villeroi to go over to the offensive in the Spanish Netherlands and, with victory, gain a 'fair' peace. Accordingly, the French Marshal set off from Leuven (\"Louvain\") at the head of 60,000 men and marched towards Tienen (\"Tirlemont\"), as if to threaten Zoutleeuw (\"L\u00e9au\"). Also determined to fight a major engagement, the Duke of Marlborough, commander-in-chief of Anglo-Dutch forces, assembled his armysome 62,000 mennear Maastricht, and marched past Zoutleeuw. With both sides seeking battle, they soon encountered each other on the dry ground between the rivers Mehaigne and Petite Gette, close to the small village of Ramillies.\nIn less than four hours Marlborough's Dutch, English, and Danish forces overwhelmed Villeroi's and Max Emanuel's Franco-Spanish-Bavarian army. The Duke's subtle moves and changes in emphasis during the battlesomething his opponents failed to realise until it was too latecaught the French in a tactical vice. With their foe broken and routed, the Allies were able to fully exploit their victory. Town after town fell, including Brussels, Bruges and Antwerp; by the end of the campaign Villeroi's army had been driven from most of the Spanish Netherlands. With Prince Eugene's subsequent success at the Battle of Turin in northern Italy, the Allies had imposed the greatest loss of territory and resources that LouisXIV would suffer during the war. Thus, the year 1706 proved, for the Allies, to be an \"annus mirabilis\".\nBackground.\nAfter their disastrous defeat at Blenheim in 1704, the French found some respite in next year. The Duke of Marlborough had intended the 1705 campaignan invasion of France through the Moselle valleyto complete the work of Blenheim and persuade King LouisXIV to make peace but the plan had been thwarted by friend and foe alike. The reluctance of his Dutch allies to see their frontiers denuded of troops for another gamble in Germany had denied Marlborough the initiative but of far greater importance was the Margrave of Baden's pronouncement that he could not join the Duke in strength for the coming offensive. This was in part due to the sudden switching of troops from the Rhine to reinforce Prince Eugene in Italy and part due to the deterioration of Baden's health brought on by the re-opening of a severe foot wound he had received at the storming of the Schellenberg the previous year. Marlborough had to cope with the death of Emperor LeopoldI in May and the accession of JosephI, which unavoidably complicated matters for the Grand Alliance.\nThe resilience of the French king and the efforts of his generals also added to Marlborough's problems. Marshal Villeroi, exerting considerable pressure on the Dutch commander, Count Overkirk, along the Meuse, took Huy on 10 June before pressing on towards Li\u00e8ge. With Marshal Villars sitting strong on the Moselle, the Allied commanderwhose supplies had by now become very shortwas forced to call off his campaign on 16 June. \"What a disgrace for Marlborough,\" exulted Villeroi, \"to have made false movements without any result!\" With Marlborough's departure north, the French transferred troops from the Moselle valley to reinforce Villeroi in Flanders, while Villars marched off to the Rhine.\nThe Anglo-Dutch forces gained minor compensation for the failed Moselle campaign with the success at Elixheim and the crossing of the Lines of Brabant in the Spanish Netherlands (Huy was also retaken on 11 July) but a chance to bring the French to a decisive engagement eluded Marlborough. The year 1705 proved almost entirely barren for the Duke, whose military disappointments were only partly compensated by efforts on the diplomatic front where, at the courts of D\u00fcsseldorf, Frankfurt, Vienna, Berlin and Hanover, Marlborough sought to bolster support for the Grand Alliance and extract promises of prompt assistance for the following year's campaign.\nPrelude.\nOn 11 January 1706 Marlborough finally reached London at the end of his diplomatic tour but he had already been planning his strategy for the coming season. The first option (although it is debatable to what extent the Duke was committed to such an enterprise) was a plan to transfer his forces from the Spanish Netherlands to northern Italy; once there, he intended linking up with Prince Eugene in order to defeat the French and safeguard Savoy from being overrun. Savoy would then serve as a gateway into France by way of the mountain passes or an invasion with naval support along the Mediterranean coast via Nice and Toulon, in connexion with redoubled Allied efforts in Spain. It seems that the Duke's favoured scheme was to return to the Moselle valley (where Marshal Marsin had recently taken command of French forces) and once more attempt an advance into the heart of France. But these decisions soon became academic. Shortly after Marlborough landed in the Dutch Republic on 14 April, news arrived of big Allied setbacks in the wider war.\nDetermined to show the Grand Alliance that France was still resolute, LouisXIV prepared to launch a double surprise in Alsace and northern Italy. On the latter front Marshal Vend\u00f4me defeated the Imperial army at Calcinato on 19 April, pushing the Imperialists back in confusion (French forces were now in a position to prepare for the long-anticipated siege of Turin). In Alsace, Marshal Villars took Baden by surprise and captured Haguenau, driving him back across the Rhine in some disorder, thus creating a threat on Landau. With these reverses, the Dutch refused to contemplate Marlborough's ambitious march to Italy or any plan that denuded their borders of the Duke and their army. In the interest of coalition harmony, Marlborough prepared to campaign in the Low Countries.\nOn the move.\nThe Duke left The Hague on 9 May. \"God knows I go with a heavy heart,\" he wrote six days later to his friend and political ally in England, Lord Godolphin, \"for I have no hope of doing anything considerable, unless the French do what I am very confident they will not...\"in other words, court battle. On 17 May the Duke concentrated his Dutch and English troops at Tongeren, near Maastricht. The Hanoverians, Hessians and Danes, despite earlier undertakings, found, or invented, pressing reasons for withholding their support. Marlborough wrote an appeal to the Duke of W\u00fcrttemberg, the commander of the Danish contingent: \"I send you this express to request your Highness to bring forward by a double march your cavalry so as to join us at the earliest moment...\" Additionally, the King \"in\" Prussia, Frederick I, had kept his troops in quarters behind the Rhine while his personal disputes with Vienna and the States General at The Hague remained unresolved. Nevertheless, the Duke could think of no circumstances why the French would leave their strong positions and attack his army, even if Villeroi was first reinforced by substantial transfers from Marsin's command. But in this he had miscalculated. Although LouisXIV wanted peace he wanted it on reasonable terms; for that, he needed victory in the field and to convince the Allies that his resources were by no means exhausted.\nFollowing the successes in Italy and along the Rhine, LouisXIV was now hopeful of similar results in Flanders. Far from standing on the defensive thereforeand unbeknown to MarlboroughLouisXIV was persistently goading his marshal into action. \"[Villeroi] began to imagine,\" wrote St Simon, \"that the King doubted his courage, and resolved to stake all at once in an effort to vindicate himself.\" Accordingly, on 18 May, Villeroi set off from Leuven at the head of 70 battalions, 132 squadrons and 62 cannoncomprising an overall force of some 60,000 troopsand crossed the river Dyle to seek battle with the enemy. Spurred on by his growing confidence in his ability to out-general his opponent, and by Versailles\u2019 determination to avenge Blenheim, Villeroi and his generals anticipated success.\nNeither opponent expected the clash at the exact moment or place where it occurred. The French moved first to Tienen, (as if to threaten Zoutleeuw, abandoned by the French in October 1705), before turning southwards, heading for Jodoignethis line of march took Villeroi's army towards the narrow aperture of dry ground between the rivers Mehaigne and Petite Gette close to the small villages of Ramillies and Taviers; but neither commander quite appreciated how far his opponent had travelled. Villeroi still believed (on 22 May) the Allies were a full day's march away when in fact they had camped near Corswaren waiting for the Danish squadrons to catch up; for his part, Marlborough deemed Villeroi still at Jodoigne when in reality he was now approaching the plateau of Mont St. Andr\u00e9 with the intention of pitching camp near Ramillies (see map at right). However, the Prussian infantry was not there. Marlborough wrote to Lord Raby, the English resident at Berlin: \"If it should please God to give us victory over the enemy, the Allies will be little obliged to the King [Frederick] for the success.\"\nThe following day, at 01:00, Marlborough dispatched Cadogan, his Quartermaster-General, with an advanced guard to reconnoitre the same dry ground that Villeroi's army was now heading toward, country that was well known to the Duke from previous campaigns. Two hours later the Duke followed with the main body: 74 battalions, 123 squadrons, 90 pieces of artillery and 20 mortars, totalling 62,000 troops. About 08:00, after Cadogan had just passed Merdorp, his force made brief contact with a party of French hussars gathering forage on the edge of the plateau of Jandrenouille. After a brief exchange of shots the French retired and Cadogan's dragoons pressed forward. With a short lift in the mist, Cadogan soon discovered the smartly ordered lines of Villeroi's advance guard some off; a galloper hastened back to warn Marlborough. Two hours later the Duke, accompanied by the Dutch field commander Field Marshal Overkirk, General Dani\u00ebl van Dopff, and the Allied staff, rode up to Cadogan where on the horizon to the westward he could discern the massed ranks of the French army deploying for battle along the front. Marlborough later told Bishop Burnet: \"The French army looked the best of any he had ever seen.\"\nBattle.\nBattlefield.\nThe battlefield of Ramillies is very similar to that of Blenheim, for here too there is an immense area of arable land unimpeded by woods or hedges. Villeroi's right rested on the villages of Franquen\u00e9e and Taviers, with the river Mehaigne protecting his flank. A large open plain, about wide, lay between Taviers and Ramillies, but unlike Blenheim, there was no stream to hinder the cavalry. His centre was secured by Ramillies itself, lying on a slight eminence which gave distant views to the north and east. The French left flank was protected by broken country, and by a stream, the Petite Gheete, which runs deep between steep and slippery slopes. On the French side of the stream the ground rises to Offus, the village which, together with Autre-Eglise farther north, anchored Villeroi's left flank. To the west of the Petite Gheete rises the plateau of Mont St. Andr\u00e9; a second plain, the plateau of Jandrenouilleupon which the Anglo-Dutch army amassedrises to the east.\nInitial dispositions.\nAt 11:00 the Duke ordered the army to take standard battle formation. On the far right, towards Foulz, the British battalions and squadrons took up their posts in a double line near the Jeuche stream. The centre was formed by the mass of Dutch, German, Protestant Swiss and Scottish infantryperhaps 30,000 menfacing Offus and Ramillies. Also facing Ramillies Marlborough placed a powerful battery of thirty 24-pounders, dragged into position by a team of oxen; further batteries were positioned overlooking the Petite Gheete. On their left, on the broad plain between Taviers and Ramilliesand where Marlborough thought the decisive encounter must take placeOverkirk drew the 69 squadrons of the Dutch and Danish horse, supported by 19 battalions of Dutch infantry and two artillery pieces.\nMeanwhile, Villeroi deployed his forces. In Taviers on his right, he placed two battalions of the Greder Suisse R\u00e9giment, with a smaller force forward in Franquen\u00e9e; the whole position was protected by the boggy ground of the river Mehaigne, thus preventing an Allied flanking movement. In the open country between Taviers and Ramillies, he placed 82 squadrons under General de Guiscard supported by several interleaved brigades of French, Swiss and Bavarian infantry. Along the Ramillies\u2013Offus\u2013Autre Eglise ridge-line, Villeroi positioned Walloon and Bavarian infantry, supported by the Elector of Bavaria's 50 squadrons of Bavarian and Walloon cavalry placed behind on the plateau of Mont St. Andr\u00e9. Ramillies, Offus and Autre-Eglise were all packed with troops and put in a state of defence, with alleys barricaded and walls loop-holed for muskets. Villeroi also positioned powerful batteries near Ramillies. These guns (some of which were of the three barrelled kind first seen at Elixheim the previous year) enjoyed good arcs of fire, able to fully cover the approaches of the plateau of Jandrenouille over which the Allied infantry would have to pass.\nMarlborough, however, noticed several important weaknesses in the French dispositions. Tactically, it was imperative for Villeroi to occupy Taviers on his right and Autre-Eglise on his left, but by adopting this posture he had been forced to over-extend his forces. Moreover, this dispositionconcave in relation to the Allied armygave Marlborough the opportunity to form a more compact line, drawn up in a shorter front between the 'horns' of the French crescent; when the Allied blow came it would be more concentrated and carry more weight. Additionally, the Duke's disposition facilitated the transfer of troops across his front far more easily than his foe, a tactical advantage that would grow in importance as the events of the afternoon unfolded. Although Villeroi had the option of enveloping the flanks of the Allied army as they deployed on the plateau of Jandrenouillethreatening to encircle their armythe Duke correctly gauged that the characteristically cautious French commander was intent on a defensive battle along the ridge-line.\nTaviers.\nAt 13:00 the batteries went into action; a little later two Allied columns set out from the extremities of their line and attacked the flanks of the Franco-Bavarian army. To the south, 4 Dutch battalions, under the command of Colonel Wertm\u00fcller, came forward with their two field guns to seize the hamlet of Franquen\u00e9e. The small Swiss garrison in the village, shaken by the sudden onslaught and unsupported by the battalions to their rear, were soon compelled back towards the village of Taviers. Taviers was of particular importance to the Franco-Bavarian position: it protected the otherwise unsupported flank of General de Guiscard's cavalry on the open plain, while at the same time, it allowed the French infantry to pose a threat to the flanks of the Dutch and Danish squadrons as they came forward into position. But hardly had the retreating Swiss rejoined their comrades in that village when the Dutch Guards renewed their attack. The fighting amongst the alleys and cottages soon deteriorated into a fierce bayonet and clubbing \"m\u00eal\u00e9e\", but the superiority in Dutch firepower soon told. The accomplished French officer, Colonel de la Colonie, standing on the plain nearby remembered: \"This village was the opening of the engagement, and the fighting there was almost as murderous as the rest of the battle put together.\" By about 15:00 the Swiss had been pushed out of the village into the marshes beyond.\nVilleroi's right flank fell into chaos and was now open and vulnerable. Alerted to the situation de Guiscard ordered an immediate attack with 14 squadrons of French dragoons currently stationed in the rear. Two other battalions of the Greder Suisse R\u00e9giment were also sent, but the attack was poorly co-ordinated and consequently went in piecemeal. The Anglo-Dutch commanders now sent dismounted Dutch dragoons into Taviers, which, together with the Guards and their field guns, poured concentrated musketry- and canister-fire into the advancing French troops. Colonel d\u2019Aubigni, leading his regiment, fell mortally wounded.\nAs the French ranks wavered, the leading squadrons of W\u00fcrttemberg's Danish horsenow unhampered by enemy fire from either villagewere also sent into the attack and fell upon the exposed flank of the Franco-Swiss infantry and dragoons. De la Colonie, with his Grenadiers Rouge regiment, together with the Cologne Guards who were brigaded with them, was now ordered forward from his post south of Ramillies to support the faltering counter-attack on the village. But on his arrival, all was chaos: \"Scarcely had my troops got over when the dragoons and Swiss who had preceded us, came tumbling down upon my battalions in full flight... My own fellows turned about and fled along with them.\" De La Colonie managed to rally some of his grenadiers, together with the remnants of the French dragoons and Greder Suisse battalions, but it was an entirely peripheral operation, offering only fragile support for Villeroi's right flank.\nOffus and Autre-Eglise.\nWhile the attack on Taviers went on the Earl of Orkney launched his first line of English across the Petite Gheete in a determined attack against the barricaded villages of Offus and Autre-Eglise on the Allied right. Villeroi, posting himself near Offus, watched anxiously the redcoats' advance, mindful of the counsel he had received on 6 May from LouisXIV: \"Have particular care to that part of the line which will endure the first shock of the English troops.\" Heeding this advice the French commander began to transfer battalions from his centre to reinforce the left, drawing more foot from the already weakened right to replace them.\nAs the English battalions descended the gentle slope of the Petite Gheete valley, struggling through the boggy stream, they were met by Major General de la Guiche's disciplined Walloon infantry sent forward from around Offus. After concentrated volleys, exacting heavy casualties on the redcoats, the Walloons reformed back to the ridgeline in good order. The English took some time to reform their ranks on the dry ground beyond the stream and press on up the slope towards the cottages and barricades on the ridge. The vigour of the English assault, however, was such that they threatened to break through the line of the villages and out onto the open plateau of Mont St Andr\u00e9 beyond. This was potentially dangerous for the Allied infantry who would then be at the mercy of the Elector's Bavarian and Walloon squadrons patiently waiting on the plateau for the order to move.\nAlthough Henry Lumley's English cavalry had managed to cross the marshy ground around the Petite Gheete, it was soon evident to Marlborough that sufficient cavalry support would not be practicable and that the battle could not be won on the Allied right. The Duke, therefore, called off the attack against Offus and Autre-Eglise. To make sure that Orkney obeyed his order to withdraw, Marlborough sent his Quartermaster-General in person with the command. Despite Orkney's protestations, Cadogan insisted on compliance and, reluctantly, Orkney gave the word for his troops to fall back to their original positions on the edge of the plateau of Jandrenouille. It is still not clear how far Orkney's advance was planned only as a feint; according to historian David Chandler it is probably more accurate to surmise that Marlborough launched Orkney in a serious probe with a view to sounding out the possibilities of the sector. Nevertheless, the attack had served its purpose. Villeroi had given his personal attention to that wing and strengthened it with large bodies of horse and foot that ought to have been taking part in the decisive struggle south of Ramillies.\nRamillies.\nMeanwhile, the Dutch assault on Ramillies was gaining pace. Marlborough's younger brother, General of Infantry Charles Churchill, ordered four brigades of foot to attack the village. The assault consisted of 12 battalions of Dutch infantry commanded by Major Generals Scholten and Sparre; two brigades of Saxons under Count Schulenburg; a Scottish brigade in Dutch service led by the 2nd Duke of Argyle; and a small brigade of Protestant Swiss. The 20 French and Bavarian battalions in Ramillies, supported by the Irish who had left Ireland in the Flight of the Wild Geese to join Clare's Dragoons who fought as infantry and captured a colour from the British 3rd Regiment of Foot and a small brigade of Cologne and Bavarian Guards under the Marquis de Maffei, put up a determined defence, initially driving back the attackers with severe losses as commemorated in the song \"Clare's Dragoons\".\nSeeing that Scholten and Sparre were faltering, Marlborough now ordered Orkney's second-line British and Danish battalions (who had not been used in the assault on Offus and Autre-Eglise) to move south towards Ramillies. Shielded as they were from observation by a slight fold in the land, their commander, Brigadier-General Van Pallandt, ordered the regimental colours to be left in place on the edge of the plateau to convince their opponents they were still in their initial position. Therefore, unbeknown to the French who remained oblivious to the Allies' real strength and intentions on the opposite side of the Petite Gheete, Marlborough was throwing his full weight against Ramillies and the open plain to the south. Villeroi meanwhile, was still moving more reserves of infantry in the opposite direction towards his left flank; crucially, it would be some time before the French commander noticed the subtle change in emphasis of the Allied dispositions.\nAround 15:30 Overkirk advanced his massed squadrons on the open plain in support of the infantry attack on Ramillies. 48 Dutch squadrons, supported on their left by 21 Danish squadrons, led by Count Tilly and Lieutenants Generals Hompesch, d'Auvergne, Ostfriesland and Dopffsteadily advanced towards the enemy (taking care not to prematurely tire the horses), before breaking into a trot to gain the impetus for their charge. The Marquis de Feuqui\u00e8res writing after the battle described the scene: \"They advanced in four lines... As they approached they advanced their second and fourth lines into the intervals of their first and third lines; so that when they made their advance upon us, they formed only one front, without any intermediate spaces.\" This made it nearly impossible for the French cavalry to perform flanking manoeuvres.\nThe initial clash favoured the Dutch and Danish squadrons. The disparity of numbersexacerbated by Villeroi stripping their ranks of infantry to reinforce his left flankenabled Overkirk's cavalry to throw the first line of French horse back in some disorder towards their second-line squadrons. This line also came under severe pressure and, in turn, was forced back to their third-line of cavalry and the few battalions still remaining on the plain. But these French horsemen were amongst the best in LouisXIV's armythe \"Maison du Roi\", supported by four elite squadrons of Bavarian Cuirassiers. Ably led by de Guiscard, the French cavalry rallied, thrusting back the Allied squadrons in successful local counterattacks. On Overkirk's right flank, close to Ramillies, ten of his squadrons suddenly broke ranks and were scattered, riding headlong to the rear to recover their order, leaving the left flank of the Allied assault on Ramillies dangerously exposed. Notwithstanding the lack of infantry support, de Guiscard threw his cavalry forward in an attempt to split the Allied army in two.\nA crisis threatened the centre, but from his vantage point Marlborough was at once aware of the situation. The Allied commander now summoned the cavalry on the right wing to reinforce his centre, leaving only the English squadrons in support of Orkney. Thanks to a combination of battle-smoke and favourable terrain, his redeployment went unnoticed by Villeroi who made no attempt to transfer any of his own 50 unused squadrons. While he waited for the fresh reinforcements to arrive, Marlborough flung himself into the \"m\u00eal\u00e9e\", rallying some of the Dutch cavalry who were in confusion. But his personal involvement nearly led to his undoing. A number of French horsemen, recognising the Duke, came surging towards his party. Marlborough's horse tumbled and the Duke was thrown\"Milord Marlborough was rid over,\" wrote Orkney some time later. It was a critical moment of the battle. \"Major-General Murray,\" recalled one eyewitness: \"...seeing him fall, marched up in all haste with two Swiss battalions to save him and stop the enemy who were hewing all down in their way.\" Samuel Constant de Rebecque helped Marlborough back on his feet, while Marlborough's newly appointed aide-de-camp, Richard Molesworth, galloped to the rescue, mounted the Duke on his horse and made good their escape, before Murray's disciplined ranks threw back the pursuing French troopers.\nAfter a brief pause, Marlborough's equerry, Colonel Bringfield (or Bingfield), led up another of the Duke's spare horses; but while assisting him onto his mount, the unfortunate Bringfield was hit by an errant cannonball that sheared off his head. One account has it that the cannonball flew between the Captain-General's legs before hitting the unfortunate colonel, whose torso fell at Marlborough's feeta moment subsequently depicted in a lurid set of contemporary playing cards. Nevertheless, the danger passed and Overkirk and Tilly restored order among the confused squadrons and ordered them to attack again, enabling the Duke to attend to the positioning of the cavalry reinforcements feeding down from his right flanka change of which Villeroi remained blissfully unaware.\nBreakthrough.\nThe time was about 16:30, and the two armies were in close contact across the whole front, from the skirmishing in the marshes in the south, through the vast cavalry battle on the open plain; to the fierce struggle for Ramillies at the centre, and to the north, where, around the cottages of Offus and Autre-Eglise, Orkney and de la Guiche faced each other across the Petite Gheete ready to renew hostilities.\nThe arrival of the transferring squadrons now began to tip the balance in favour of the Allies. Tired, and suffering a growing list of casualties, the numerical inferiority of Guiscard's squadrons battling on the plain at last began to tell. After earlier failing to hold or retake Franquen\u00e9e and Taviers, Guiscard's right flank had become dangerously exposed and a fatal gap had opened on the right of their line. Taking advantage of this breach, W\u00fcrttemberg's Danish cavalry now swept forward, wheeling to penetrate the flank of the Maison du Roi whose attention was almost entirely fixed on holding back the Dutch. Sweeping forwards, virtually without resistance, the 21 Danish squadrons reformed behind the French around the area of the Tomb of Ottomond, facing north across the plateau of Mont St Andr\u00e9 towards the exposed flank of Villeroi's army.\nThe final Allied reinforcements for the cavalry contest to the south were at last in position; Marlborough's superiority on the left could no longer be denied, and his fast-moving plan took hold of the battlefield. Now, far too late, Villeroi tried to redeploy his 50 unused squadrons, but a desperate attempt to form line facing south, stretching from Offus to Mont St Andr\u00e9, floundered amongst the baggage and tents of the French camp carelessly left there after the initial deployment. The Allied commander ordered his cavalry forward against the now heavily outnumbered French and Bavarian horsemen. De Guiscard's right flank, without proper infantry support, could no longer resist the onslaught and, turning their horses northwards, they broke and fled in complete disorder. Even the squadrons currently being scrambled together by Villeroi behind Ramillies could not withstand the onslaught. \"We had not got forty yards on our retreat,\" remembered Captain Peter Drake, an Irishman serving with the French\"when the words \"sauve qui peut\" went through the great part, if not the whole army, and put all to confusion\"\nIn Ramillies the Allied infantry, now reinforced by the English troops brought down from the north, at last broke through. The R\u00e9giment de Picardie stood their ground but were caught between Colonel Borthwick's Scots-Dutch regiment and the English reinforcements. Borthwick was killed, as was Charles O\u2019Brien, the Irish Viscount Clare in French service, fighting at the head of his regiment. The Marquis de Maffei attempted one last stand with his Bavarian and Cologne Guards, but it proved in vain. Noticing a rush of horsemen fast approaching from the south, he later recalled: \"...I went towards the nearest of these squadrons to instruct their officer, but instead of being listened to [I] was immediately surrounded and called upon to ask for quarter.\"\nPursuit.\nThe roads leading north and west were choked with fugitives. Orkney now sent his English troops back across the Petite Gheete stream to once again storm Offus where de la Guiche's infantry had begun to drift away in the confusion. To the right of the infantry Lord John Hay's 'Scots Greys' also picked their way across the stream and charged the R\u00e9giment du Roi within Autre-Eglise. \"Our dragoons,\" wrote John Deane, \"pushing into the village... made terrible slaughter of the enemy.\" The Bavarian Horse Grenadiers and the Electoral Guards withdrew and formed a shield about Villeroi and the Elector but were scattered by Lumley's cavalry. Stuck in the mass of fugitives fleeing the battlefield, the French and Bavarian commanders narrowly escaped capture by General Cornelius Wood who, unaware of their identity, had to content himself with the seizure of two Bavarian Lieutenant-Generals. Far to the south, the remnants of de la Colonie's brigade headed in the opposite direction towards the French held fortress of Namur.\nThe retreat became a rout. Individual Allied commanders drove their troops forward in pursuit, allowing their beaten enemy no chance to recover. Soon the Allied infantry could no longer keep up, but their cavalry were off the leash, heading through the gathering night for the crossings on the river Dyle. At last, however, Marlborough called a halt to the pursuit shortly after midnight near Meldert, from the field. \"It was indeed a truly shocking sight to see the miserable remains of this mighty army,\" wrote Captain Drake, \"...reduced to a handful.\"\nAftermath.\nWhat was left of Villeroi's army was now broken in spirit; the imbalance of the casualty figures amply demonstrates the extent of the disaster for LouisXIV's army: (\"see below\"). In addition, hundreds of French soldiers were fugitives, many of whom would never remuster to the colours. Villeroi also lost 52 artillery pieces and his entire engineer pontoon train. In the words of Marshal Villars, the French defeat at Ramillies was \"the most shameful, humiliating and disastrous of routs\".\nTown after town now succumbed to the Allies. Leuven fell on 25 May 1706; three days later, the Allies entered Brussels, the capital of the Spanish Netherlands. Marlborough realised the great opportunity created by the early victory of Ramillies: \"We now have the whole summer before us,\" wrote the Duke from Brussels to Robert Harley: \"...and with the blessing of God I shall make the best use of it.\" Malines, Lierre, Ghent, Alost, Damme, Oudenaarde, Bruges, and on 6 June Antwerp, all subsequently fell to Marlborough's victorious army and, like Brussels, proclaimed the Austrian candidate for the Spanish throne, the Archduke Charles, as their sovereign. Villeroi was helpless to arrest the process of collapse. When LouisXIV learnt of the disaster he recalled Marshal Vend\u00f4me from northern Italy to take command in Flanders; but it would be weeks before the command changed hands.\nAs news spread of the Allies' triumph, the Prussians, Hessians and Hanoverian contingents, long delayed by their respective rulers, eagerly joined the pursuit of the broken French and Bavarian forces. \"This,\" wrote Marlborough wearily, \"I take to be owing to our late success.\" Meanwhile, Overkirk took the port of Ostend on 4 July thus opening a direct route to the English Channel for communication and supply, but the Allies were making scant progress against Dendermonde whose governor, the Marquis de Val\u00e9e, was stubbornly resisting. Only later when Cadogan and Churchill went to take charge did the town's defences begin to fail.\nVend\u00f4me formally took over command in Flanders on 4 August; Villeroi would never again receive a major command: \"I cannot foresee a happy day in my life save only that of my death.\" LouisXIV was more forgiving to his old friend: \"At our age, Marshal, we must no longer expect good fortune.\" In the meantime, Marlborough invested the elaborate fortress of Menin which, after a costly siege, capitulated on 22 August. Dendermonde finally succumbed on 6 September followed by Aththe last conquest of 1706on 2 October. By the time Marlborough had closed down the Ramillies campaign he had denied the French most of the Spanish Netherlands west of the Meuse and north of the Sambreit was an unsurpassed operational triumph for the English Duke but once again it was not decisive as these gains did not defeat France.\nThe immediate question for the Allies was how to deal with the Spanish Netherlands, a subject on which the Austrians and the Dutch were diametrically opposed. Emperor JosephI, acting on behalf of his younger brother King CharlesIII, absent in Spain, claimed that reconquered Brabant and Flanders should be put under immediate possession of a governor named by himself. The Dutch, however, who had supplied the major share of the troops and money to secure the victory (the Austrians had produced nothing of either) claimed the government of the region till the war was over, and that after the peace they should continue to garrison Barrier Fortresses stronger than those which had fallen so easily to LouisXIV's forces in 1701. Marlborough mediated between the two parties but favoured the Dutch position. To sway the Duke's opinion, the Emperor offered Marlborough the governorship of the Spanish Netherlands. It was a tempting offer, but in the name of Allied unity, it was one he refused. In the end England and the Dutch Republic took control of the newly won territory for the duration of the war; after which it was to be handed over to the direct rule of CharlesIII, subject to the reservation of a Dutch Barrier, the extent and nature of which had yet to be settled.\nMeanwhile, on the Upper Rhine, Villars had been forced onto the defensive as battalion after battalion had been sent north to bolster collapsing French forces in Flanders; there was now no possibility of his undertaking the re-capture of Landau. Further good news for the Allies arrived from northern Italy where, on 7 September, Prince Eugene had routed a French army before the Piedmontese capital, Turin, driving the Franco-Spanish forces from northern Italy. Only from Spain did LouisXIV receive any good news where Das Minas and Galway had been forced to retreat from Madrid towards Valencia, allowing PhilipV to re-enter his capital on 4 October. All in all though, the situation had changed considerably and LouisXIV began to look for ways to end what was fast becoming a ruinous war for France. For Queen Anne also, the Ramillies campaign had one overriding significance: \"Now we have God be thanked so hopeful a prospect of peace.\" Instead of continuing the momentum of victory, however, cracks in Allied unity would enable LouisXIV to reverse some of the major setbacks suffered at Turin and Ramillies.\nCasualties.\nThe total number of French casualties cannot be calculated precisely, so complete was the collapse of the Franco-Bavarian army that day. David G. Chandler's \"Marlborough as Military Commander\" and \"A Guide to the Battlefields of Europe\" are consistent with regards to French casualty figures, i.e. 12,000 dead and wounded plus some 7,000 taken prisoner. James Falkner, in \"Ramillies 1706: Year of Miracles\", also notes 12,000 dead and wounded and \"up to 10,000\" taken prisoner. In \"Notes on the history of military medicine\", Garrison puts French casualties at 13,000, including 2,000 killed, 3,000 wounded and 6,000 missing. In \"The Collins Encyclopaedia of Military History\", Dupuy puts Villeroi's dead and wounded at 8,000, with a further 7,000 captured. Neil Litten, using French archives, suggests 7,000 killed and wounded and 6,000 captured, with a further 2,000 choosing to desert. John Millner's memoirs\"Compendious Journal\" (1733)is more specific, recording 12,087 of Villeroi's army were killed or wounded, with another 9,729 taken prisoner. In \"Marlborough\", however, Correlli Barnett puts the total casualty figure as high as 30,000\u201315,000 dead and wounded with an additional 15,000 taken captive. Trevelyan estimates Villeroi's casualties at 13,000 but adds \"his losses by desertion may have doubled that number\". La Colonie omits a casualty figure in his \"Chronicles of an old Campaigner\" but Saint-Simon in his \"Memoirs\" states 4,000 killed adding \"many others were wounded and many important persons were taken prisoner\". Voltaire, however, in \"Histoire du si\u00e8cle du LouisXIV\" records \"the French lost there twenty thousand men\". Gaston Bodart states 2,000 killed or wounded, 6,000 captured and 7,000 scattered for a total of 13,000 casualties. P\u00e9rini writes that both sides lost 2 to 3,000 killed or wounded (the Dutch losing precisely 716 killed and 1,712 wounded), and that 5,600 French were captured."}
{"id": "4051", "revid": "48858073", "url": "https://en.wikipedia.org/wiki?curid=4051", "title": "Brian Kernighan", "text": "Brian Wilson Kernighan (; born January 30, 1942) is a Canadian computer scientist.\nHe worked at Bell Labs and contributed to the development of Unix alongside Unix creators Ken Thompson and Dennis Ritchie. Kernighan's name became widely known through co-authorship of the first book on the C programming language (\"The C Programming Language\") with Dennis Ritchie. Kernighan affirmed that he had no part in the design of the C language (\"it's entirely Dennis Ritchie's work\").\nKernighan authored many Unix programs, including ditroff. He is coauthor of the AWK and AMPL programming languages. The \"K\" of K&amp;R C and of AWK both stand for \"Kernighan\".\nIn collaboration with Shen Lin he devised well-known heuristics for two NP-complete optimization problems: graph partitioning and the travelling salesman problem. In a display of authorial equity, the former is usually called the Kernighan\u2013Lin algorithm, while the latter is known as the Lin\u2013Kernighan heuristic.\nKernighan has been a professor of computer science at Princeton University since 2000 and is the director of undergraduate studies in the department of computer science. In 2015, he co-authored the book \"The Go Programming Language.\nEarly life and education.\nKernighan was born in Toronto. He attended the University of Toronto between 1960 and 1964, earning his bachelor's degree in engineering physics. He received his Ph.D. in electrical engineering from Princeton University in 1969, completing a doctoral dissertation titled \"Some graph partitioning problems related to program segmentation\" under the supervision of Peter G. Weiner.\nCareer and research.\nKernighan has held a professorship in the department of computer science at Princeton since 2000. Each fall he teaches a course called \"Computers in Our World\", which introduces the fundamentals of computing to non-majors.\nKernighan was the software editor for Prentice Hall International. His \"Software Tools\" series spread the essence of \"C/Unix thinking\" with makeovers for BASIC, FORTRAN, and Pascal, and most notably his \"Ratfor\" (rational FORTRAN) was put in the public domain.\nHe has said that if stranded on an island with only one programming language it would have to be C.\nKernighan coined the term \"Unix\" and helped popularize Thompson's Unix philosophy. Kernighan is also known for coining the expression \"What You See Is All You Get\" (WYSIAYG), which is a sarcastic variant of the original \"What You See Is What You Get\" (WYSIWYG). Kernighan's term is used to indicate that WYSIWYG systems might throw away information in a document that could be useful in other contexts.\nIn 1972, Kernighan described memory management in strings using \"hello\" and \"world\", in the B programming language, which became the iconic example we know today. Kernighan's original 1978 implementation of was sold at The Algorithm Auction, the world's first auction of computer algorithms.\nIn 1996, Kernighan taught CS50 which is the Harvard University introductory course in computer science. Kernighan was an influence on David J. Malan who subsequently taught the course and scaled it up to run at multiple universities and in multiple digital formats.\nKernighan was elected a member of the National Academy of Engineering in 2002 for contributions to software and to programming languages. He was also elected a member of the American Academy of Arts and Sciences in 2019.\nIn 2022, Kernighan stated that he was actively working on improvements to the AWK programming language, which he took part in creating in 1977."}
{"id": "4052", "revid": "417659", "url": "https://en.wikipedia.org/wiki?curid=4052", "title": "BCPL", "text": "BCPL (\"Basic Combined Programming Language\") is a procedural, imperative, and structured programming language. Originally intended for writing compilers for other languages, BCPL is no longer in common use. However, its influence is still felt because a stripped down and syntactically changed version of BCPL, called B, was the language on which the C programming language was based. BCPL introduced several features of many modern programming languages, including using curly braces to delimit code blocks. BCPL was first implemented by Martin Richards of the University of Cambridge in 1967.\nDesign.\nBCPL was designed so that small and simple compilers could be written for it; reputedly some compilers could be run in 16 kilobytes. Furthermore, the original compiler, itself written in BCPL, was easily portable. BCPL was thus a popular choice for bootstrapping a system. A major reason for the compiler's portability lay in its structure. It was split into two parts: the front end parsed the source and generated O-code, an intermediate language. The back end took the O-code and translated it into the machine code for the target machine. Only of the compiler's code needed to be rewritten to support a new machine, a task that usually took between 2 and 5 person-months. This approach became common practice later (e.g. Pascal, Java).\nThe language is unusual in having only one data type: a word, a fixed number of bits, usually chosen to align with the same platform architecture's machine word and of adequate capacity to represent any valid storage address. For many machines of the time, this data type was a 16-bit word. This choice later proved to be a significant problem when BCPL was used on machines in which the smallest addressable item was not a word but a byte or on machines with larger word sizes such as 32-bit or 64-bit.\nThe interpretation of any value was determined by the operators used to process the values. (For example, codice_1 added two values together, treating them as integers; codice_2 indirected through a value, effectively treating it as a pointer.) In order for this to work, the implementation provided no type checking.\nThe mismatch between BCPL's word orientation and byte-oriented hardware was addressed in several ways. One was by providing standard library routines for packing and unpacking words into byte strings. Later, two language features were added: the bit-field selection operator and the infix byte indirection operator (denoted by codice_3).\nBCPL handles bindings spanning separate compilation units in a unique way. There are no user-declarable global variables; instead, there is a global vector, similar to \"blank common\" in Fortran. All data shared between different compilation units comprises scalars and pointers to vectors stored in a pre-arranged place in the global vector. Thus, the header files (files included during compilation using the \"GET\" directive) become the primary means of synchronizing global data between compilation units, containing \"GLOBAL\" directives that present lists of symbolic names, each paired with a number that associates the name with the corresponding numerically addressed word in the global vector. As well as variables, the global vector contains bindings for external procedures. This makes dynamic loading of compilation units very simple to achieve. Instead of relying on the link loader of the underlying implementation, effectively, BCPL gives the programmer control of the linking process.\nThe global vector also made it very simple to replace or augment standard library routines. A program could save the pointer from the global vector to the original routine and replace it with a pointer to an alternative version. The alternative might call the original as part of its processing. This could be used as a quick \"ad hoc\" debugging aid.\nBCPL was the first brace programming language and the braces survived the syntactical changes and have become a common means of denoting program source code statements. In practice, on limited keyboards of the day, source programs often used the sequences codice_4 and codice_5 or codice_6 and codice_7 in place of the symbols codice_8 and codice_9. The single-line codice_10 comments of BCPL, which were not adopted by C, reappeared in C++ and later in C99.\nThe book \"BCPL: The language and its compiler\" describes the philosophy of BCPL as follows:\nHistory.\nBCPL was first implemented by Martin Richards of the University of Cambridge in 1967. BCPL was a response to difficulties with its predecessor, Cambridge Programming Language, later renamed Combined Programming Language (CPL), which was designed during the early 1960s. Richards created BCPL by \"removing those features of the full language which make compilation difficult\". The first compiler implementation, for the IBM 7094 under Compatible Time-Sharing System, was written while Richards was visiting Project MAC at the Massachusetts Institute of Technology in the spring of 1967. The language was first described in a paper presented to the 1969 Spring Joint Computer Conference.\nBCPL has been rumored to have originally stood for \"Bootstrap Cambridge Programming Language\", but CPL was never created since development stopped at BCPL, and the acronym was later reinterpreted for the BCPL book.\nBCPL is the language in which the original \"Hello, World!\" program was written. The first MUD was also written in BCPL (\"MUD1\").\nSeveral operating systems were written partially or wholly in BCPL (for example, TRIPOS and the earliest versions of AmigaDOS). BCPL was also the initial language used in the Xerox PARC Alto project. Among other projects, the Bravo document preparation system was written in BCPL.\nAn early compiler, bootstrapped in 1969, by starting with a paper tape of the O-code of Richards's Atlas 2 compiler, targeted the ICT 1900 series. The two machines had different word-lengths (48 vs 24 bits), different character encodings, and different packed string representations\u2014and the successful bootstrapping increased confidence in the practicality of the method.\nBy late 1970, implementations existed for the Honeywell 635 and Honeywell 645, IBM 360, PDP-10, TX-2, CDC 6400, UNIVAC 1108, PDP-9, KDF 9 and Atlas 2. In 1974 a dialect of BCPL was implemented at BBN without using the intermediate O-code. The initial implementation was a cross-compiler hosted on BBN's TENEX PDP-10s, and directly targeted the PDP-11s used in BBN's implementation of the second generation IMPs used in the ARPANET.\nThere was also a version produced for the BBC Micro in the mid-1980s, by Richards Computer Products, a company started by John Richards, the brother of Martin Richards. The BBC Domesday Project made use of the language. Versions of BCPL for the Amstrad CPC and Amstrad PCW computers were also released in 1986 by UK software house Arnor Ltd. MacBCPL was released for the Apple Macintosh in 1985 by Topexpress Ltd, of Kensington, England.\nBoth the design and philosophy of BCPL strongly influenced B, which in turn influenced C. Programmers at the time debated whether an eventual successor to C would be called \"D\", the next letter in the alphabet, or \"P\", the next letter in the parent language name. The language most accepted as being C's successor is C++ (with codice_11 being C's increment operator), although meanwhile, a D programming language also exists.\nIn 1979, implementations of BCPL existed for at least 25 architectures; the language gradually fell out of favour as C became popular on non-Unix systems.\nMartin Richards maintains a modern version of BCPL on his website, last updated in 2023. This can be set up to run on various systems including Linux, FreeBSD, and Mac OS X. The latest distribution includes graphics and sound libraries, and there is a comprehensive manual. He continues to program in it, including for his research on musical automated score following.\nA common informal MIME type for BCPL is .\nExamples.\nHello world.\nRichards and Whitby-Strevens provide an example of the \"Hello, World!\" program for BCPL using a standard system header, 'LIBHDR':\nFurther examples.\nIf these programs are run using Richards' current version of Cintsys (December 2018), LIBHDR, START and WRITEF must be changed to lower case to avoid errors.\nPrint factorials:\nCount solutions to the N queens problem:"}
{"id": "4054", "revid": "2490716", "url": "https://en.wikipedia.org/wiki?curid=4054", "title": "Battleship", "text": "A battleship is a large, heavily armored warship with a main battery consisting of large-caliber guns, designed to serve as capital ships with the most intense firepower. Before the rise of supercarriers, battleships were among the largest and most formidable weapon systems ever built.\nThe term \"battleship\" came into use in the late 1880s to describe a type of ironclad warship, now referred to by historians as pre-dreadnought battleships. In 1906, the commissioning of into the United Kingdom's Royal Navy heralded a revolution in the field of battleship design. Subsequent battleship designs, influenced by HMS \"Dreadnought\", were referred to as \"dreadnoughts\", though the term eventually became obsolete as dreadnoughts became the only type of battleship in common use.\nBattleships dominated naval warfare in the late 19th and early 20th centuries, and were a symbol of naval dominance and national might, and for decades were a major intimidation factor for power projection in both diplomacy and military strategy. A global arms race in battleship construction began in Europe in the 1890s and culminated at the decisive Battle of Tsushima in 1905, the outcome of which significantly influenced the design of HMS \"Dreadnought\". The launch of \"Dreadnought\" in 1906 commenced a new naval arms race. Three major fleet actions between steel battleships took place: the long-range gunnery duel at the Battle of the Yellow Sea in 1904, the decisive Battle of Tsushima in 1905 (both during the Russo-Japanese War) and the inconclusive Battle of Jutland in 1916, during the First World War. Jutland was the largest naval battle and the only full-scale clash of dreadnoughts of the war, and it was the last major battle in naval history fought primarily by battleships.\nThe Naval Treaties of the 1920s and 1930s limited the number of battleships, though technical innovation in battleship design continued. Both the Allied and Axis powers built battleships during World War II, though the increasing importance of the aircraft carrier meant that the battleship played a less important role than had been expected in that conflict.\nThe value of the battleship has been questioned, even during their heyday. There were few of the decisive fleet battles that battleship proponents expected and used to justify the vast resources spent on building battlefleets. Even in spite of their huge firepower and protection, battleships were increasingly vulnerable to much smaller and relatively inexpensive weapons: initially the torpedo and the naval mine, and later attack aircraft and the guided missile. The growing range of naval engagements led to the aircraft carrier replacing the battleship as the leading capital ship during World War II, with the last battleship to be launched being in 1944. Four battleships were retained by the United States Navy until the end of the Cold War for fire support purposes and were last used in combat during the Gulf War in 1991, and then struck from the U.S. Naval Vessel Register in the 2000s. Many World War II-era American battleships survive today as museum ships.\nHistory.\nShips of the line.\nA ship of the line was a large, unarmored wooden sailing ship which mounted a battery of up to 120 smoothbore guns and carronades, which came to prominence with the adoption of line of battle tactics in the early 17th century and the end of the sailing battleship's heyday in the 1830s. From 1794, the alternative term 'line of battle ship' was contracted (informally at first) to 'battle ship' or 'battleship'.\nThe sheer number of guns fired broadside meant a ship of the line could wreck any wooden enemy, holing her hull, knocking down masts, wrecking her rigging, and killing her crew. However, the effective range of the guns was as little as a few hundred yards, so the battle tactics of sailing ships depended in part on the wind.\nOver time, ships of the line gradually became larger and carried more guns, but otherwise remained quite similar. The first major change to the ship of the line concept was the introduction of steam power as an auxiliary propulsion system. Steam power was gradually introduced to the navy in the first half of the 19th century, initially for small craft and later for frigates. The French Navy introduced steam to the line of battle with the 90-gun in 1850\u2014the first true steam battleship. \"Napol\u00e9on\" was armed as a conventional ship-of-the-line, but her steam engines could give her a speed of , regardless of the wind. This was a potentially decisive advantage in a naval engagement. The introduction of steam accelerated the growth in size of battleships. France and the United Kingdom were the only countries to develop fleets of wooden steam screw battleships although several other navies operated small numbers of screw battleships, including Russia (9), the Ottoman Empire (3), Sweden (2), Naples (1), Denmark (1) and Austria (1).\nIronclads.\nThe adoption of steam power was only one of a number of technological advances which revolutionized warship design in the 19th century. The ship of the line was overtaken by the ironclad: powered by steam, protected by metal armor, and armed with guns firing high-explosive shells.\nExplosive shells.\nGuns that fired explosive or incendiary shells were a major threat to wooden ships, and these weapons quickly became widespread after the introduction of shell guns as part of the standard armament of French and American line-of-battle ships in 1841. In the Crimean War, six line-of-battle ships and two frigates of the Russian Black Sea Fleet destroyed seven Turkish frigates and three corvettes with explosive shells at the Battle of Sinop in 1853. Later in the war, French ironclad floating batteries used similar weapons against the defenses at the Battle of Kinburn.\nNevertheless, wooden-hulled ships stood up comparatively well to shells, as shown in the 1866 Battle of Lissa, where the modern Austrian steam two-decker ranged across a confused battlefield, rammed an Italian ironclad and took 80 hits from Italian ironclads, many of which were shells, but including at least one shot at point-blank range. Despite losing her bowsprit and her foremast, and being set on fire, she was ready for action again the very next day.\nIron armor and construction.\nThe development of high-explosive shells made the use of iron armor plate on warships necessary. In 1859 France launched , the first ocean-going ironclad warship. She had the profile of a ship of the line, cut to one deck due to weight considerations. Although made of wood and reliant on sail for most journeys, \"Gloire\" was fitted with a propeller, and her wooden hull was protected by a layer of thick iron armor. \"Gloire\" prompted further innovation from the Royal Navy, anxious to prevent France from gaining a technological lead.\nThe superior armored frigate followed \"Gloire\" by only 14 months, and both nations embarked on a program of building new ironclads and converting existing screw ships of the line to armored frigates. Within two years, Italy, Austria, Spain and Russia had all ordered ironclad warships, and by the time of the famous clash of the and the at the Battle of Hampton Roads at least eight navies possessed ironclad ships.\nNavies experimented with the positioning of guns, in turrets (like the USS \"Monitor\"), central-batteries or barbettes, or with the ram as the principal weapon. As steam technology developed, masts were gradually removed from battleship designs. By the mid-1870s steel was used as a construction material alongside iron and wood. The French Navy's , laid down in 1873 and launched in 1876, was a central battery and barbette warship which became the first battleship in the world to use steel as the principal building material.\nPre-dreadnought battleship.\nThe term \"battleship\" was officially adopted by the Royal Navy in the re-classification of 1892. By the 1890s, there was an increasing similarity between battleship designs, and the type that later became known as the 'pre-dreadnought battleship' emerged. These were heavily armored ships, mounting a mixed battery of guns in turrets, and without sails. The typical first-class battleship of the pre-dreadnought era displaced 15,000 to 17,000\u00a0tons, had a speed of , and an armament of four guns in two turrets fore and aft with a mixed-caliber secondary battery amidships around the superstructure. An early design with superficial similarity to the pre-dreadnought is the British of 1871.\nThe slow-firing 12-inch main guns were the principal weapons for battleship-to-battleship combat. The intermediate and secondary batteries had two roles. Against major ships, it was thought a 'hail of fire' from quick-firing secondary weapons could distract enemy gun crews by inflicting damage to the superstructure, and they would be more effective against smaller ships such as cruisers. Smaller guns (12-pounders and smaller) were reserved for protecting the battleship against the threat of torpedo attack from destroyers and torpedo boats.\nThe beginning of the pre-dreadnought era coincided with Britain reasserting her naval dominance. For many years previously, Britain had taken naval supremacy for granted. Expensive naval projects were criticized by political leaders of all inclinations. However, in 1888 a war scare with France and the build-up of the Russian navy gave added impetus to naval construction, and the British Naval Defence Act of 1889 laid down a new fleet including eight new battleships. The principle that Britain's navy should be more powerful than the two next most powerful fleets combined was established. This policy was designed to deter France and Russia from building more battleships, but both nations nevertheless expanded their fleets with more and better pre-dreadnoughts in the 1890s.\nIn the last years of the 19th century and the first years of the 20th, the escalation in the building of battleships became an arms race between Britain and Germany. The German naval laws of 1890 and 1898 authorized a fleet of 38 battleships, a vital threat to the balance of naval power. Britain answered with further shipbuilding, but by the end of the pre-dreadnought era, British supremacy at sea had markedly weakened. In 1883, the United Kingdom had 38 battleships, twice as many as France and almost as many as the rest of the world put together. In 1897, Britain's lead was far smaller due to competition from France, Germany, and Russia, as well as the development of pre-dreadnought fleets in Italy, the United States and Japan. The Ottoman Empire, Spain, Sweden, Denmark, Norway, the Netherlands, Chile and Brazil all had second-rate fleets led by armored cruisers, coastal defence ships or monitors.\nPre-dreadnoughts continued the technical innovations of the ironclad. Turrets, armor plate, and steam engines were all improved over the years, and torpedo tubes were also introduced. A small number of designs, including the American and es, experimented with all or part of the 8-inch intermediate battery superimposed over the 12-inch primary. Results were poor: recoil factors and blast effects resulted in the 8-inch battery being completely unusable, and the inability to train the primary and intermediate armaments on different targets led to significant tactical limitations. Even though such innovative designs saved weight (a key reason for their inception), they proved too cumbersome in practice.\nDreadnought era.\nIn 1906, the British Royal Navy launched the revolutionary . Created as a result of pressure from Admiral Sir John (\"Jackie\") Fisher, HMS \"Dreadnought\" rendered existing battleships obsolete. Combining an \"all-big-gun\" armament of ten 12-inch (305\u00a0mm) guns with unprecedented speed (from steam turbine engines) and protection, she prompted navies worldwide to re-evaluate their battleship building programs. While the Japanese had laid down an all-big-gun battleship, , in 1904 and the concept of an all-big-gun ship had been in circulation for several years, it had yet to be validated in combat. \"Dreadnought\" sparked a new arms race, principally between Britain and Germany but reflected worldwide, as the new class of warships became a crucial element of national power.\nTechnical development continued rapidly through the dreadnought era, with steep changes in armament, armor and propulsion. Ten years after \"Dreadnought\"s commissioning, much more powerful ships, the super-dreadnoughts, were being built.\nOrigin.\nIn the first years of the 20th century, several navies worldwide experimented with the idea of a new type of battleship with a uniform armament of very heavy guns.\nAdmiral Vittorio Cuniberti, the Italian Navy's chief naval architect, articulated the concept of an all-big-gun battleship in 1903. When the \"Regia Marina\" did not pursue his ideas, Cuniberti wrote an article in \"Janes\" proposing an \"ideal\" future British battleship, a large armored warship of 17,000\u00a0tons, armed solely with a single calibre main battery (twelve 12-inch guns), carrying belt armor, and capable of 24 knots (44\u00a0km/h).\nThe Russo-Japanese War provided operational experience to validate the \"all-big-gun\" concept. During the Battle of the Yellow Sea on August 10, 1904, Admiral Togo of the Imperial Japanese Navy commenced deliberate 12-inch gun fire at the Russian flagship \"Tzesarevich\" at . At the Battle of Tsushima on May 27, 1905, Russian Admiral Rozhestvensky's flagship fired the first 12-inch guns at the Japanese flagship \"Mikasa\" at . It is often held that these engagements demonstrated the importance of the 12-inch gun over its smaller counterparts, though some historians take the view that secondary batteries were just as important as the larger weapons when dealing with smaller fast-moving torpedo craft. Such was the case, albeit unsuccessfully, when the at Tsushima had been sent to the bottom by destroyer-launched torpedoes. The 1903\u201304 design also retained traditional triple-expansion steam engines.\nAs early as 1904, Jackie Fisher had been convinced of the need for fast, powerful ships with an all-big-gun armament. If Tsushima influenced his thinking, it was to persuade him of the need to standardise on 12-inch guns. Fisher's concerns were submarines and destroyers equipped with torpedoes, then threatening to outrange battleship guns, making speed imperative for capital ships. Fisher's preferred option was his brainchild, the battlecruiser: lightly armored but heavily armed with eight 12-inch guns and propelled to by steam turbines.\nIt was to prove this revolutionary technology that \"Dreadnought\" was designed in January 1905, laid down in October 1905 and sped to completion by 1906. She carried ten 12-inch guns, had an armor belt, and was the first large ship powered by turbines. She mounted her guns in five turrets; three on the centerline (one forward, two aft) and two on the wings, giving her at her launch twice the broadside of any other warship. She retained a number of 12-pound (3-inch or 76\u00a0mm) quick-firing guns for use against destroyers and torpedo-boats. Her armor was heavy enough for her to go head-to-head with any other ship in a gun battle, and conceivably win.\n\"Dreadnought\" was to have been followed by three s, their construction delayed to allow lessons from \"Dreadnought\" to be used in their design. While Fisher may have intended \"Dreadnought\" to be the last Royal Navy battleship, the design was so successful he found little support for his plan to switch to a battlecruiser navy. Although there were some problems with the ship (the wing turrets had limited arcs of fire and strained the hull when firing a full broadside, and the top of the thickest armor belt lay below the waterline at full load), the Royal Navy promptly commissioned another six ships to a similar design in the and es.\nAn American design, , authorized in 1905 and laid down in December 1906, was another of the first dreadnoughts, but she and her sister, , were not launched until 1908. Both used triple-expansion engines and had a superior layout of the main battery, dispensing with \"Dreadnought\"s wing turrets. They thus retained the same broadside, despite having two fewer guns.\nArms race.\nIn 1897, before the revolution in design brought about by , the Royal Navy had 62 battleships in commission or building, a lead of 26 over France and 50 over Germany. From the 1906 launching of \"Dreadnought\", an arms race with major strategic consequences was prompted. Major naval powers raced to build their own dreadnoughts. Possession of modern battleships was not only seen as vital to naval power, but also, as with nuclear weapons after World War II, represented a nation's standing in the world. Germany, France, Japan, Italy, Austria, and the United States all began dreadnought programmes; while the Ottoman Empire, Argentina, Russia, Brazil, and Chile commissioned dreadnoughts to be built in British and American yards.\nWorld War I.\nBy virtue of geography, the Royal Navy was able to use her imposing battleship and battlecruiser fleet to impose a strict and successful naval blockade of Germany and kept Germany's smaller battleship fleet bottled up in the North Sea: only narrow channels led to the Atlantic Ocean and these were guarded by British forces. Both sides were aware that, because of the greater number of British dreadnoughts, a full fleet engagement would be likely to result in a British victory. The German strategy was therefore to try to provoke an engagement on their terms: either to induce a part of the Grand Fleet to enter battle alone, or to fight a pitched battle near the German coastline, where friendly minefields, torpedo-boats and submarines could be used to even the odds. This did not happen, however, due in large part to the necessity to keep submarines for the Atlantic campaign. Submarines were the only vessels in the Imperial German Navy able to break out and raid British commerce in force, but even though they sank many merchant ships, they could not successfully counter-blockade the United Kingdom; the Royal Navy successfully adopted convoy tactics to combat Germany's submarine counter-blockade and eventually defeated it. This was in stark contrast to Britain's successful blockade of Germany.\nThe first two years of war saw the Royal Navy's battleships and battlecruisers regularly \"sweep\" the North Sea making sure that no German ships could get in or out. Only a few German surface ships that were already at sea, such as the famous light cruiser , were able to raid commerce. Even some of those that did manage to get out were hunted down by battlecruisers, as in the Battle of the Falklands, December 7, 1914. The results of sweeping actions in the North Sea were battles including the Heligoland Bight and Dogger Bank and German raids on the English coast, all of which were attempts by the Germans to lure out portions of the Grand Fleet in an attempt to defeat the Royal Navy in detail. On May 31, 1916, a further attempt to draw British ships into battle on German terms resulted in a clash of the battlefleets in the Battle of Jutland. The German fleet withdrew to port after two short encounters with the British fleet. Less than two months later, the Germans once again attempted to draw portions of the Grand Fleet into battle. The resulting Action of 19 August 1916 proved inconclusive. This reinforced German determination not to engage in a fleet to fleet battle.\nIn the other naval theatres there were no decisive pitched battles. In the Black Sea, engagement between Russian and Ottoman battleships was restricted to skirmishes. In the Baltic Sea, action was largely limited to the raiding of convoys, and the laying of defensive minefields; the only significant clash of battleship squadrons there was the Battle of Moon Sound at which one Russian pre-dreadnought was lost. The Adriatic was in a sense the mirror of the North Sea: the Austro-Hungarian dreadnought fleet remained bottled up by the British and French blockade. And in the Mediterranean, the most important use of battleships was in support of the amphibious assault on Gallipoli.\nIn September 1914, the threat posed to surface ships by German U-boats was confirmed by successful attacks on British cruisers, including the sinking of three British armored cruisers by the German submarine in less than an hour. The British Super-dreadnought soon followed suit as she struck a mine laid by a German U-boat in October 1914 and sank. The threat that German U-boats posed to British dreadnoughts was enough to cause the Royal Navy to change their strategy and tactics in the North Sea to reduce the risk of U-boat attack. Further near-misses from submarine attacks on battleships and casualties amongst cruisers led to growing concern in the Royal Navy about the vulnerability of battleships.\nAs the war wore on however, it turned out that whilst submarines did prove to be a very dangerous threat to older pre-dreadnought battleships, as shown by examples such as the sinking of , which was caught in the Dardanelles by a British submarine and and were torpedoed by \"U-21\" as well as , , etc., the threat posed to dreadnought battleships proved to have been largely a false alarm. HMS \"Audacious\" turned out to be the only dreadnought sunk by a submarine in World War I. While battleships were never intended for anti-submarine warfare, there was one instance of a submarine being sunk by a dreadnought battleship. HMS \"Dreadnought\" rammed and sank the German submarine \"U-29\" on March 18, 1915, off the Moray Firth.\nWhilst the escape of the German fleet from the superior British firepower at Jutland was effected by the German cruisers and destroyers successfully turning away the British battleships, the German attempt to rely on U-boat attacks on the British fleet failed.\nTorpedo boats did have some successes against battleships in World War I, as demonstrated by the sinking of the British pre-dreadnought by during the Dardanelles Campaign and the destruction of the Austro-Hungarian dreadnought by Italian motor torpedo boats in June 1918. In large fleet actions, however, destroyers and torpedo boats were usually unable to get close enough to the battleships to damage them. The only battleship sunk in a fleet action by either torpedo boats or destroyers was the obsolescent German pre-dreadnought . She was sunk by destroyers during the night phase of the Battle of Jutland.\nThe German High Seas Fleet, for their part, were determined not to engage the British without the assistance of submarines; and since the submarines were needed more for raiding commercial traffic, the fleet stayed in port for much of the war.\nInter-war period.\nFor many years, Germany simply had no battleships. The Armistice with Germany required that most of the High Seas Fleet be disarmed and interned in a neutral port; largely because no neutral port could be found, the ships remained in British custody in Scapa Flow, Scotland. The Treaty of Versailles specified that the ships should be handed over to the British. Instead, most of them were scuttled by their German crews on June 21, 1919, just before the signature of the peace treaty. The treaty also limited the German Navy, and prevented Germany from building or possessing any capital ships.\nThe inter-war period saw the battleship subjected to strict international limitations to prevent a costly arms race breaking out.\nWhile the victors were not limited by the Treaty of Versailles, many of the major naval powers were crippled after the war. Faced with the prospect of a naval arms race against the United Kingdom and Japan, which would in turn have led to a possible Pacific war, the United States was keen to conclude the Washington Naval Treaty of 1922. This treaty limited the number and size of battleships that each major nation could possess, and required Britain to accept parity with the U.S. and to abandon the British alliance with Japan. The Washington treaty was followed by a series of other naval treaties, including the First Geneva Naval Conference (1927), the First London Naval Treaty (1930), the Second Geneva Naval Conference (1932), and finally the Second London Naval Treaty (1936), which all set limits on major warships. These treaties became effectively obsolete on September 1, 1939, at the beginning of World War II, but the ship classifications that had been agreed upon still apply. The treaty limitations meant that fewer new battleships were launched in 1919\u20131939 than in 1905\u20131914. The treaties also inhibited development by imposing upper limits on the weights of ships. Designs like the projected British , the first American , and the Japanese \u2014all of which continued the trend to larger ships with bigger guns and thicker armor\u2014never got off the drawing board. Those designs which were commissioned during this period were referred to as treaty battleships.\nRise of air power.\nAs early as 1914, the British Admiral Percy Scott predicted that battleships would soon be made irrelevant by aircraft. By the end of World War I, aircraft had successfully adopted the torpedo as a weapon. In 1921 the Italian general and air theorist Giulio Douhet completed a hugely influential treatise on strategic bombing titled \"The Command of the Air\", which foresaw the dominance of air power over naval units.\nIn the 1920s, General Billy Mitchell of the United States Army Air Corps, believing that air forces had rendered navies around the world obsolete, testified in front of Congress that \"1,000 bombardment airplanes can be built and operated for about the price of one battleship\" and that a squadron of these bombers could sink a battleship, making for more efficient use of government funds. This infuriated the U.S. Navy, but Mitchell was nevertheless allowed to conduct a careful series of bombing tests alongside Navy and Marine bombers. In 1921, he bombed and sank numerous ships, including the \"unsinkable\" German World War I battleship and the American pre-dreadnought .\nAlthough Mitchell had required \"war-time conditions\", the ships sunk were obsolete, stationary, defenseless and had no damage control. The sinking of \"Ostfriesland\" was accomplished by violating an agreement that would have allowed Navy engineers to examine the effects of various munitions: Mitchell's airmen disregarded the rules, and sank the ship within minutes in a coordinated attack. The stunt made headlines, and Mitchell declared, \"No surface vessels can exist wherever air forces acting from land bases are able to attack them.\" While far from conclusive, Mitchell's test was significant because it put proponents of the battleship against naval aviation on the defensive. Rear Admiral William A. Moffett used public relations against Mitchell to make headway toward expansion of the U.S. Navy's nascent aircraft carrier program.\nRearmament.\nThe Royal Navy, United States Navy, and Imperial Japanese Navy extensively upgraded and modernized their World War I\u2013era battleships during the 1930s. Among the new features were an increased tower height and stability for the optical rangefinder equipment (for gunnery control), more armor (especially around turrets) to protect against plunging fire and aerial bombing, and additional anti-aircraft weapons. Some British ships received a large block superstructure nicknamed the \"Queen Anne's castle\", such as in and , which would be used in the new conning towers of the fast battleships. External bulges were added to improve both buoyancy to counteract weight increase and provide underwater protection against mines and torpedoes. The Japanese rebuilt all of their battleships, plus their battlecruisers, with distinctive \"pagoda\" structures, though the received a more modern bridge tower that would influence the new . Bulges were fitted, including steel tube arrays to improve both underwater and vertical protection along the waterline. The U.S. experimented with cage masts and later tripod masts, though after the Japanese attack on Pearl Harbor some of the most severely damaged ships (such as and ) were rebuilt with tower masts, for an appearance similar to their contemporaries. Radar, which was effective beyond visual range and effective in complete darkness or adverse weather, was introduced to supplement optical fire control.\nEven when war threatened again in the late 1930s, battleship construction did not regain the level of importance it had held in the years before World War I. The \"building holiday\" imposed by the naval treaties meant the capacity of dockyards worldwide had shrunk, and the strategic position had changed.\nIn Germany, the ambitious Plan Z for naval rearmament was abandoned in favor of a strategy of submarine warfare supplemented by the use of battlecruisers and commerce raiding (in particular by s). In Britain, the most pressing need was for air defenses and convoy escorts to safeguard the civilian population from bombing or starvation, and re-armament construction plans consisted of five ships of the . It was in the Mediterranean that navies remained most committed to battleship warfare. France intended to build six battleships of the and es, and the Italians four ships. Neither navy built significant aircraft carriers. The U.S. preferred to spend limited funds on aircraft carriers until the . Japan, also prioritising aircraft carriers, nevertheless began work on three mammoth \"Yamato\"s (although the third, , was later completed as a carrier) and a planned fourth was cancelled.\nAt the outbreak of the Spanish Civil War, the Spanish navy included only two small dreadnought battleships, and . \"Espa\u00f1a\" (originally named \"Alfonso XIII\"), by then in reserve at the northwestern naval base of El Ferrol, fell into Nationalist hands in July 1936. The crew aboard \"Jaime I\" remained loyal to the Republic, killed their officers, who apparently supported Franco's attempted coup, and joined the Republican Navy. Thus each side had one battleship; however, the Republican Navy generally lacked experienced officers. The Spanish battleships mainly restricted themselves to mutual blockades, convoy escort duties, and shore bombardment, rarely in direct fighting against other surface units. In April 1937, \"Espa\u00f1a\" ran into a mine laid by friendly forces, and sank with little loss of life. In May 1937, \"Jaime I\" was damaged by Nationalist air attacks and a grounding incident. The ship was forced to go back to port to be repaired. There she was again hit by several aerial bombs. It was then decided to tow the battleship to a more secure port, but during the transport she suffered an internal explosion that caused 300 deaths and her total loss. Several Italian and German capital ships participated in the non-intervention blockade. On May 29, 1937, two Republican aircraft managed to bomb the German pocket battleship outside Ibiza, causing severe damage and loss of life. retaliated two days later by bombarding Almer\u00eda, causing much destruction, and the resulting \"Deutschland\" incident meant the end of German and Italian participation in non-intervention.\nWorld War II.\nThe \u2014an obsolete pre-dreadnought\u2014fired the first shots of World War II with the bombardment of the Polish garrison at Westerplatte; and the final surrender of the Japanese Empire took place aboard a United States Navy battleship, . Between those two events, it had become clear that aircraft carriers were the new principal ships of the fleet and that battleships now performed a secondary role.\nBattleships played a part in major engagements in Atlantic, Pacific and Mediterranean theaters; in the Atlantic, the Germans used their battleships as independent commerce raiders. However, clashes between battleships were of little strategic importance. The Battle of the Atlantic was fought between destroyers and submarines, and most of the decisive fleet clashes of the Pacific war were determined by aircraft carriers.\nIn the first year of the war, armored warships defied predictions that aircraft would dominate naval warfare. and surprised and sank the aircraft carrier off western Norway in June 1940. This engagement marked the only time a fleet carrier was sunk by surface gunnery. In the attack on Mers-el-K\u00e9bir, British battleships opened fire on the French battleships in the harbor near Oran in Algeria with their heavy guns. The fleeing French ships were then pursued by planes from aircraft carriers.\nThe subsequent years of the war saw many demonstrations of the maturity of the aircraft carrier as a strategic naval weapon and its effectiveness against battleships. The British air attack on the Italian naval base at Taranto sank one Italian battleship and damaged two more. The same Swordfish torpedo bombers played a crucial role in sinking the German battleship .\nOn December 7, 1941, the Japanese launched a surprise attack on Pearl Harbor. Within a short time, five of eight U.S. battleships were sunk or sinking, with the rest damaged. All three American aircraft carriers were out to sea, however, and evaded destruction. The sinking of the British battleship and battlecruiser , demonstrated the vulnerability of a battleship to air attack while at sea without sufficient air cover, settling the argument begun by Mitchell in 1921. Both warships were under way and en route to attack the Japanese amphibious force that had invaded Malaya when they were caught by Japanese land-based bombers and torpedo bombers on December 10, 1941.\nAt many of the early crucial battles of the Pacific, for instance Coral Sea and Midway, battleships were either absent or overshadowed as carriers launched wave after wave of planes into the attack at a range of hundreds of miles. In later battles in the Pacific, battleships primarily performed shore bombardment in support of amphibious landings and provided anti-aircraft defense as escort for the carriers. Even the largest battleships ever constructed, Japan's , which carried a main battery of nine guns and were designed as a principal strategic weapon, were never given a chance to show their potential in the decisive battleship action that figured in Japanese pre-war planning.\nThe last battleship confrontation in history was the Battle of Surigao Strait, on October 25, 1944, in which a numerically and technically superior American battleship group destroyed a lesser Japanese battleship group by gunfire after it had already been devastated by destroyer torpedo attacks. All but one of the American battleships in this confrontation had previously been sunk during the attack on Pearl Harbor and subsequently raised and repaired. fired the last major-caliber salvo of this battle. In April 1945, during the battle for Okinawa, the world's most powerful battleship, the \"Yamato\", was sent out on a suicide mission against a massive U.S. force and sunk by overwhelming pressure from carrier aircraft with nearly all hands lost. After that, the Japanese fleet remaining in the home islands was also destroyed by the US naval air force.\nCold War.\nAfter World War II, several navies retained their existing battleships, but they were no longer strategically dominant military assets. It soon became apparent that they were no longer worth the considerable cost of construction and maintenance and only one new battleship was commissioned after the war, . During the war it had been demonstrated that battleship-on-battleship engagements like Leyte Gulf or the sinking of were the exception and not the rule, and with the growing role of aircraft, engagement ranges were becoming longer and longer, making heavy gun armament irrelevant. The armor of a battleship was equally irrelevant in the face of a nuclear attack as tactical missiles with a range of or more could be mounted on the Soviet and s. By the end of the 1950s, smaller vessel classes such as destroyers, which formerly offered no noteworthy opposition to battleships, now were capable of eliminating battleships from outside the range of the ship's heavy guns.\nThe remaining battleships met a variety of ends. and the were sunk during the testing of nuclear weapons in Operation Crossroads in 1946. Both battleships proved resistant to nuclear air burst but vulnerable to underwater nuclear explosions (in the case of \"Arkansas\", \"vulnerable\" due to her proximity to the bomb crushing, flipping, and sinking her in less than a second). The was taken by the Soviets as reparations and renamed \"Novorossiysk\"; she was sunk by a leftover German mine in the Black Sea on October 29, 1955. The two ships were scrapped in 1956. The was scrapped in 1954, in 1968, and in 1970.\nThe United Kingdom's four surviving ships were scrapped in 1957, and followed in 1960. All other surviving British battleships had been sold or broken up by 1949. The Soviet Union's was scrapped in 1953, in 1957 and (back under her original name, , since 1942) in 1956\u201357. Brazil's was scrapped in Genoa in 1953, and her sister ship sank during a storm in the Atlantic \"en route\" to the breakers in Italy in 1951.\nArgentina kept its two ships until 1956 and Chile kept (formerly ) until 1959. The Turkish battlecruiser (formerly , launched in 1911) was scrapped in 1976 after an offer to sell her back to Germany was refused. Sweden had several small coastal-defense battleships, one of which, , survived until 1970. The Soviets scrapped four large incomplete cruisers in the late 1950s, whilst plans to build a number of new s were abandoned following the death of Joseph Stalin in 1953. The three old German battleships , , and all met similar ends. \"Hessen\" was taken over by the Soviet Union and renamed \"Tsel\". She was scrapped in 1960. \"Schleswig-Holstein\" was renamed \"Borodino\", and was used as a target ship until 1960. \"Schlesien\", too, was used as a target ship. She was broken up between 1952 and 1957.\nThe s gained a new lease of life in the U.S. Navy as fire support ships. Radar and computer-controlled gunfire could be aimed with pinpoint accuracy to target. The U.S. recommissioned all four \"Iowa\"-class battleships for the Korean War and the for the Vietnam War. These were primarily used for shore bombardment, \"New Jersey\" firing nearly 6,000 rounds of 16-inch shells and over 14,000 rounds of 5-inch projectiles during her tour on the gunline, seven times more rounds against shore targets in Vietnam than she had fired in the Second World War.\nAs part of Navy Secretary John F. Lehman's effort to build a 600-ship Navy in the 1980s, and in response to the commissioning of by the Soviet Union, the United States recommissioned all four \"Iowa\"-class battleships. On several occasions, battleships were support ships in carrier battle groups, or led their own battleship battle group. These were modernized to carry Tomahawk (TLAM) missiles, with \"New Jersey\" seeing action bombarding Lebanon in 1983 and 1984, while and fired their 16-inch (406\u00a0mm) guns at land targets and launched missiles during Operation Desert Storm in 1991. \"Wisconsin\" served as the TLAM strike commander for the Persian Gulf, directing the sequence of launches that marked the opening of \"Desert Storm\", firing a total of 24 TLAMs during the first two days of the campaign. The primary threat to the battleships were Iraqi shore-based surface-to-surface missiles; \"Missouri\" was targeted by two Iraqi Silkworm missiles, with one missing and another being intercepted by the British destroyer .\nEnd of the battleship era.\nAfter was stricken in 1962, the four \"Iowa-class\" ships were the only battleships in commission or reserve anywhere in the world. There was an extended debate when the four \"Iowa\" ships were finally decommissioned in the early 1990s. and were maintained to a standard whereby they could be rapidly returned to service as fire support vessels, pending the development of a superior fire support vessel. These last two battleships were finally stricken from the U.S. Naval Vessel Register in 2006. The Military Balance and Russian states the U.S. Navy listed one battleship in the reserve (Naval Inactive Fleet/Reserve 2nd Turn) in 2010. The Military Balance states the U.S. Navy listed no battleships in the reserve in 2014.\nWhen the last \"Iowa\"-class ship was finally stricken from the Naval Vessel Registry, no battleships remained in service or in reserve with any navy worldwide. A number are preserved as museum ships, either afloat or in drydock. The U.S. has eight battleships on display: , , , , , , , and . \"Missouri\" and \"New Jersey\" are museums at Pearl Harbor and Camden, New Jersey, respectively. \"Iowa\" is on display as an educational attraction at the Los Angeles Waterfront in San Pedro, California. \"Wisconsin\" now serves as a museum ship in Norfolk, Virginia. \"Massachusetts\", which has the distinction of never having lost a man during service, is on display at the Battleship Cove naval museum in Fall River, Massachusetts. \"Texas\", the first battleship turned into a museum, is normally on display at the San Jacinto Battleground State Historic Site, near Houston, but as of 2021 is closed for repairs. \"North Carolina\" is on display in Wilmington, North Carolina. \"Alabama\" is on display in Mobile, Alabama. The wreck of , sunk during the Pearl Harbor attack in 1941, is designated a historical landmark and national gravesite. The wreck of , also sunk during the attack, is a historic landmark.\nIn the Royal Navy, the end of the battleship era came about in 1960, when its last battleship, HMS \"Vanguard\", was decommissioned and sold for scrap.\nThe only other 20th-century battleship on display is the Japanese pre-dreadnought . A replica of the ironclad battleship was built by the Weihai Port Bureau in 2003 and is on display in Weihai, China.\nFormer battleships that were previously used as museum ships included , , and .\nStrategy and doctrine.\nDoctrine.\nBattleships were the embodiment of sea power. For American naval officer Alfred Thayer Mahan and his followers, a strong navy was vital to the success of a nation, and control of the seas was vital for the projection of force on land and overseas. Mahan's theory, proposed in \"The Influence of Sea Power Upon History, 1660\u20131783\" of 1890, dictated the role of the battleship was to sweep the enemy from the seas. While the work of escorting, blockading, and raiding might be done by cruisers or smaller vessels, the presence of the battleship was a potential threat to any convoy escorted by any vessels other than capital ships. This concept of \"potential threat\" can be further generalized to the mere existence (as opposed to presence) of a powerful fleet tying the opposing fleet down. This concept came to be known as a \"fleet in being\"\u2014an idle yet mighty fleet forcing others to spend time, resource and effort to actively guard against it.\nMahan went on to say victory could only be achieved by engagements between battleships, which came to be known as the \"decisive battle\" doctrine in some navies, while targeting merchant ships (commerce raiding or \"guerre de course\", as posited by the \"Jeune \u00c9cole\") could never succeed.\nMahan was highly influential in naval and political circles throughout the age of the battleship, calling for a large fleet of the most powerful battleships possible. Mahan's work developed in the late 1880s, and by the end of the 1890s it had acquired much international influence on naval strategy; in the end, it was adopted by many major navies (notably the British, American, German, and Japanese). The strength of Mahanian opinion was important in the development of the battleships arms races, and equally important in the agreement of the Powers to limit battleship numbers in the interwar era.\nThe \"fleet in being\" suggested battleships could simply by their existence tie down superior enemy resources. This in turn was believed to be able to tip the balance of a conflict even without a battle. This suggested even for inferior naval powers a battleship fleet could have important strategic effect.\nTactics.\nWhile the role of battleships in both World Wars reflected Mahanian doctrine, the details of battleship deployment were more complex. Unlike ships of the line, the battleships of the late 19th and early 20th centuries had significant vulnerability to torpedoes and mines\u2014because efficient mines and torpedoes did not exist before that\u2014which could be used by relatively small and inexpensive craft. The \"Jeune \u00c9cole\" doctrine of the 1870s and 1880s recommended placing torpedo boats alongside battleships; these would hide behind the larger ships until gun-smoke obscured visibility enough for them to dart out and fire their torpedoes. While this tactic was made less effective by the development of smokeless propellant, the threat from more capable torpedo craft (later including submarines) remained. By the 1890s, the Royal Navy had developed the first destroyers, which were initially designed to intercept and drive off any attacking torpedo boats. During the First World War and subsequently, battleships were rarely deployed without a protective screen of destroyers.\nBattleship doctrine emphasized the concentration of the battlegroup. In order for this concentrated force to be able to bring its power to bear on a reluctant opponent (or to avoid an encounter with a stronger enemy fleet), battlefleets needed some means of locating enemy ships beyond horizon range. This was provided by scouting forces; at various stages battlecruisers, cruisers, destroyers, airships, submarines and aircraft were all used. (With the development of radio, direction finding and traffic analysis would come into play, as well, so even shore stations, broadly speaking, joined the battlegroup.) So for most of their history, battleships operated surrounded by squadrons of destroyers and cruisers. The North Sea campaign of the First World War illustrates how, despite this support, the threat of mine and torpedo attack, and the failure to integrate or appreciate the capabilities of new techniques, seriously inhibited the operations of the Royal Navy Grand Fleet, the greatest battleship fleet of its time.\nStrategic and diplomatic impact.\nThe presence of battleships had a great psychological and diplomatic impact. Similar to possessing nuclear weapons today, the ownership of battleships served to enhance a nation's force projection.\nEven during the Cold War, the psychological impact of a battleship was significant. In 1946, USS \"Missouri\" was dispatched to deliver the remains of the ambassador from Turkey, and her presence in Turkish and Greek waters staved off a possible Soviet thrust into the Balkan region. In September 1983, when Druze militia in Lebanon's Shouf Mountains fired upon U.S. Marine peacekeepers, the arrival of USS \"New Jersey\" stopped the firing. Gunfire from \"New Jersey\" later killed militia leaders.\nValue for money.\nBattleships were the largest and most complex, and hence the most expensive warships of their time; as a result, the value of investment in battleships has always been contested. As the French politician Etienne Lamy wrote in 1879, \"The construction of battleships is so costly, their effectiveness so uncertain and of such short duration, that the enterprise of creating an armored fleet seems to leave fruitless the perseverance of a people\". The \"Jeune \u00c9cole\" school of thought of the 1870s and 1880s sought alternatives to the crippling expense and debatable utility of a conventional battlefleet. It proposed what would nowadays be termed a sea denial strategy, based on fast, long-ranged cruisers for commerce raiding and torpedo boat flotillas to attack enemy ships attempting to blockade French ports. The ideas of the \"Jeune \u00c9cole\" were ahead of their time; it was not until the 20th century that efficient mines, torpedoes, submarines, and aircraft were available that allowed similar ideas to be effectively implemented. The determination of powers such as Germany to build battlefleets with which to confront much stronger rivals has been criticized by historians, who emphasise the futility of investment in a battlefleet that has no chance of matching its opponent in an actual battle."}
{"id": "4055", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=4055", "title": "Bifr\u00f6st", "text": "In Norse mythology, Bifr\u00f6st (), also called Bilr\u00f6st, is a burning rainbow bridge that reaches between Midgard (Earth) and Asgard, the realm of the gods. The bridge is attested as \"Bilr\u00f6st\" in the \"Poetic Edda\", compiled in the 13th century from earlier traditional sources; as \"Bifr\u00f6st\" in the \"Prose Edda\", written in the 13th century by Snorri Sturluson; and in the poetry of skalds. Both the \"Poetic Edda\" and the \"Prose Edda\" alternately refer to the bridge as \u00c1sbr\u00fa (Old Norse \"\u00c6sir's bridge\").\nAccording to the \"Prose Edda\", the bridge ends in heaven at Himinbj\u00f6rg, the residence of the god Heimdall, who guards it from the j\u00f6tnar. The bridge's destruction during Ragnar\u00f6k by the forces of Muspell is foretold. Scholars have proposed that the bridge may have originally represented the Milky Way and have noted parallels between the bridge and another bridge in Norse mythology, Gjallarbr\u00fa.\nEtymology.\nScholar Andy Orchard suggests that \"Bifr\u00f6st\" may mean \"shimmering path.\" He notes that the first element of \"Bilr\u00f6st\"\u2014\"bil\" (meaning \"a moment\")\u2014\"suggests the fleeting nature of the rainbow,\" which he connects to the first element of \"Bifr\u00f6st\"\u2014the Old Norse verb \"bifa\" (meaning \"to shimmer\" or \"to shake\")\u2014noting that the element evokes notions of the \"lustrous sheen\" of the bridge. Austrian Germanist Rudolf Simek says that \"Bifr\u00f6st\" either means \"the swaying road to heaven\" (also citing \"bifa\") or, if \"Bilr\u00f6st\" is the original form of the two (which Simek says is likely), \"the fleetingly glimpsed rainbow\" (possibly connected to \"bil\", perhaps meaning \"moment, weak point\").\nAttestations.\nTwo poems in the \"Poetic Edda\" and two books in the \"Prose Edda\" provide information about the bridge:\n\"Poetic Edda\".\nIn the \"Poetic Edda\", the bridge is mentioned in the poems \"Gr\u00edmnism\u00e1l\" and \"F\u00e1fnism\u00e1l\", where it is referred to as \"Bilr\u00f6st\". In one of two stanzas in the poem \"Gr\u00edmnism\u00e1l\" that mentions the bridge, Gr\u00edmnir (the god Odin in disguise) provides the young Agnarr with cosmological knowledge, including that Bilr\u00f6st is the best of bridges. Later in \"Gr\u00edmnism\u00e1l\", Gr\u00edmnir notes that Asbr\u00fa \"burns all with flames\" and that, every day, the god Thor wades through the waters of K\u00f6rmt and \u00d6rmt and the two Kerlaugar:\nIn \"F\u00e1fnism\u00e1l\", the dying wyrm Fafnir tells the hero Sigurd that, during the events of Ragnar\u00f6k, bearing spears, gods will meet at \u00d3sk\u00f3pnir. From there, the gods will cross Bilr\u00f6st, which will break apart as they cross over it, causing their horses to dredge through an immense river.\n\"Prose Edda\".\nThe bridge is mentioned in the \"Prose Edda\" books \"Gylfaginning\" and \"Sk\u00e1ldskaparm\u00e1l\", where it is referred to as \"Bifr\u00f6st\". In chapter 13 of \"Gylfaginning\", Gangleri (King Gylfi in disguise) asks the enthroned figure of High what way exists between heaven and earth. Laughing, High replies that the question isn't an intelligent one, and goes on to explain that the gods built a bridge from heaven and earth. He incredulously asks Gangleri if he has not heard the story before. High says that Gangleri must have seen it, and notes that Gangleri may call it a rainbow. High says that the bridge consists of three colors, has great strength, \"and is built with art and skill to a greater extent than other constructions.\"\nHigh notes that, although the bridge is strong, it will break when \"Muspell's lads\" attempt to cross it, and their horses will have to make do with swimming over \"great rivers.\" Gangleri says that it doesn't seem that the gods \"built the bridge in good faith if it is liable to break, considering that they can do as they please.\" High responds that the gods do not deserve blame for the breaking of the bridge, for \"there is nothing in this world that will be secure when Muspell's sons attack.\"\nIn chapter 15 of \"Gylfaginning\", Just-As-High says that Bifr\u00f6st is also called \"Asbr\u00fa\", and that every day the gods ride their horses across it (with the exception of Thor, who instead wades through the boiling waters of the rivers K\u00f6rmt and \u00d6rmt) to reach Ur\u00f0arbrunnr, a holy well where the gods have their court. As a reference, Just-As-High quotes the second of the two stanzas in \"Gr\u00edmnism\u00e1l\" that mention the bridge (see above). Gangleri asks if fire burns over Bifr\u00f6st. High says that the red in the bridge is burning fire, and, without it, the frost jotnar and mountain jotnar would \"go up into heaven\" if anyone who wanted could cross Bifr\u00f6st. High adds that, in heaven, \"there are many beautiful places\" and that \"everywhere there has divine protection around it.\"\nIn chapter 17, High tells Gangleri that the location of Himinbj\u00f6rg \"stands at the edge of heaven where Bifrost reaches heaven.\" While describing the god Heimdallr in chapter 27, High says that Heimdallr lives in Himinbj\u00f6rg by Bifr\u00f6st, and guards the bridge from mountain jotnar while sitting at the edge of heaven. In chapter 34, High quotes the first of the two \"Gr\u00edmnism\u00e1l\" stanzas that mention the bridge. In chapter 51, High foretells the events of Ragnar\u00f6k. High says that, during Ragnar\u00f6k, the sky will split open, and from the split will ride forth the \"sons of Muspell\". When the \"sons of Muspell\" ride over Bifr\u00f6st it will break, \"as was said above.\"\nIn the \"Prose Edda\" book \"Sk\u00e1ldskaparm\u00e1l\", the bridge receives a single mention. In chapter 16, a work by the 10th century skald \u00dalfr Uggason is provided, where Bifr\u00f6st is referred to as \"the powers' way.\"\nTheories.\nIn his translation of the \"Prose Edda\", Henry Adams Bellows comments that the \"Gr\u00edmnism\u00e1l\" stanza mentioning Thor and the bridge stanza may mean that \"Thor has to go on foot in the last days of the destruction, when the bridge is burning. Another interpretation, however, is that when Thor leaves the heavens (i.e., when a thunder-storm is over) the rainbow-bridge becomes hot in the sun.\"\nJohn Lindow points to a parallel between Bifr\u00f6st, which he notes is \"a bridge between earth and heaven, or earth and the world of the gods\", and the bridge Gjallarbr\u00fa, \"a bridge between earth and the underworld, or earth and the world of the dead.\" Several scholars have proposed that Bifr\u00f6st may represent the Milky Way.\nAdaptations.\nIn the final scene of Richard Wagner's 1869 opera \"Das Rheingold\", the god Froh summons a rainbow bridge, over which the gods cross to enter Valhalla.\nIn J. R. R. Tolkien's legendarium, the \"level bridge\" of \"The Fall of N\u00famenor\", an early version of the \"Akallabeth\", recalls Bifr\u00f6st. It departs from the earth at a tangent, allowing immortal Elves but not mortal Men to travel the Old Straight Road to the lost earthly paradise of Valinor after the world has been remade. \nBifr\u00f6st appears in comic books associated with the Marvel Comics character Thor and in subsequent adaptations of those comic books. In the Marvel Cinematic Universe film \"Thor\", Jane Foster describes the Bifr\u00f6st as an Einstein\u2013Rosen bridge, which functions as a means of transportation across space in a short period of time."}
{"id": "4057", "revid": "2607788", "url": "https://en.wikipedia.org/wiki?curid=4057", "title": "Battlecruiser", "text": "The battlecruiser (also written as battle cruiser or battle-cruiser) was a type of capital ship of the first half of the 20th century. These were similar in displacement, armament and cost to battleships, but differed in form and balance of attributes. Battlecruisers typically had thinner armour (to a varying degree) and a somewhat lighter main gun battery than contemporary battleships, installed on a longer hull with much higher engine power in order to attain greater speeds. The first battlecruisers were designed in the United Kingdom, as a development of the armoured cruiser, at the same time as the dreadnought succeeded the pre-dreadnought battleship. The goal of the design was to outrun any ship with similar armament, and chase down any ship with lesser armament; they were intended to hunt down slower, older armoured cruisers and destroy them with heavy gunfire while avoiding combat with the more powerful but slower battleships. However, as more and more battlecruisers were built, they were increasingly used alongside the better-protected battleships.\nBattlecruisers served in the navies of the United Kingdom, Germany, the Ottoman Empire, Australia and Japan during World War I, most notably at the Battle of the Falkland Islands and in the several raids and skirmishes in the North Sea which culminated in a pitched fleet battle, the Battle of Jutland. British battlecruisers in particular suffered heavy losses at Jutland, where poor fire safety and ammunition handling practices left them vulnerable to catastrophic magazine explosions following hits to their main turrets from large-calibre shells. This dismal showing led to a persistent general belief that battlecruisers were too thinly armoured to function successfully. By the end of the war, capital ship design had developed, with battleships becoming faster and battlecruisers becoming more heavily armoured, blurring the distinction between a battlecruiser and a fast battleship. The Washington Naval Treaty, which limited capital ship construction from 1922 onwards, treated battleships and battlecruisers identically, and the new generation of battlecruisers planned by the United States, Great Britain and Japan were scrapped or converted into aircraft carriers under the terms of the treaty.\nImprovements in armour design and propulsion created the 1930s \"fast battleship\" with the speed of a battlecruiser and armour of a battleship, making the battlecruiser in the traditional sense effectively an obsolete concept. Thus from the 1930s on, only the Royal Navy continued to use \"battlecruiser\" as a classification for the World War I\u2013era capital ships that remained in the fleet; while Japan's battlecruisers remained in service, they had been significantly reconstructed and were re-rated as full-fledged fast battleships.\nBattlecruisers were put into action again during World War II, and only one survived to the end. There was also renewed interest in large \"cruiser-killer\" type warships, but few were ever begun, as construction of battleships and battlecruisers was curtailed in favor of more-needed convoy escorts, aircraft carriers, and cargo ships. During (and after) the Cold War, the Soviet of large guided missile cruisers have been the only ships termed \"battlecruisers\"; the class is also the only example of a nuclear-powered battlecruiser. As of 2024, Russia operates two units: the \"Pyotr Velikiy\" has remained in active service since its 1998 commissioning, while the \"Admiral Nakhimov\" has been inactive (in storage or refitting) since 1999.\nBackground.\nThe battlecruiser was developed by the Royal Navy in the first years of the 20th century as an evolution of the armoured cruiser. The first armoured cruisers had been built in the 1870s, as an attempt to give armour protection to ships fulfilling the typical cruiser roles of patrol, trade protection and power projection. However, the results were rarely satisfactory, as the weight of armour required for any meaningful protection usually meant that the ship became almost as slow as a battleship. As a result, navies preferred to build protected cruisers with an armoured deck protecting their engines, or simply no armour at all.\nIn the 1890s, new Krupp steel armour meant that it was now possible to give a cruiser side armour which would protect it against the quick-firing guns of enemy battleships and cruisers alike. In 1896\u201397 France and Russia, who were regarded as likely allies in the event of war, started to build large, fast armoured cruisers taking advantage of this. In the event of a war between Britain and France or Russia, or both, these cruisers threatened to cause serious difficulties for the British Empire's worldwide trade.\nBritain, which had concluded in 1892 that it needed twice as many cruisers as any potential enemy to adequately protect its empire's sea lanes, responded to the perceived threat by laying down its own large armoured cruisers. Between 1899 and 1905, it completed or laid down seven classes of this type, a total of 35 ships. This building program, in turn, prompted the French and Russians to increase their own construction. The Imperial German Navy began to build large armoured cruisers for use on their overseas stations, laying down eight between 1897 and 1906. In the period 1889\u20131896, the Royal Navy spent \u00a37.3\u00a0million on new large cruisers. From 1897 to 1904, it spent \u00a326.9\u00a0million. Many armoured cruisers of the new kind were just as large and expensive as the equivalent battleship.\nThe increasing size and power of the armoured cruiser led to suggestions in British naval circles that cruisers should displace battleships entirely. The battleship's main advantage was its 12-inch heavy guns, and heavier armour designed to protect from shells of similar size. However, for a few years after 1900 it seemed that those advantages were of little practical value. The torpedo now had a range of 2,000 yards, and it seemed unlikely that a battleship would engage within torpedo range. However, at ranges of more than 2,000 yards it became increasingly unlikely that the heavy guns of a battleship would score any hits, as the heavy guns relied on primitive aiming techniques. The secondary batteries of 6-inch quick-firing guns, firing more plentiful shells, were more likely to hit the enemy. As naval expert Fred T. Jane wrote in June 1902,Is there anything outside of 2,000 yards that the big gun in its hundreds of tons of medieval castle can affect, that its weight in 6-inch guns without the castle could not affect equally well? And inside 2,000, what, in these days of gyros, is there that the torpedo cannot effect with far more certainty?\nIn 1904, Admiral John \"Jacky\" Fisher became First Sea Lord, the senior officer of the Royal Navy. He had for some time thought about the development of a new fast armoured ship. He was very fond of the \"second-class battleship\" , a faster, more lightly armoured battleship. As early as 1901, there is confusion in Fisher's writing about whether he saw the battleship or the cruiser as the model for future developments. This did not stop him from commissioning designs from naval architect W. H. Gard for an armoured cruiser with the heaviest possible armament for use with the fleet. The design Gard submitted was for a ship between , capable of , armed with four 9.2-inch and twelve guns in twin gun turrets and protected with six inches of armour along her belt and 9.2-inch turrets, on her 7.5-inch turrets, 10 inches on her conning tower and up to on her decks. However, mainstream British naval thinking between 1902 and 1904 was clearly in favour of heavily armoured battleships, rather than the fast ships that Fisher favoured.\nThe Battle of Tsushima proved the effectiveness of heavy guns over intermediate ones and the need for a uniform main caliber on a ship for fire control. Even before this, the Royal Navy had begun to consider a shift away from the mixed-calibre armament of the 1890s pre-dreadnought to an \"all-big-gun\" design, and preliminary designs circulated for battleships with all 12-inch or all 10-inch guns and armoured cruisers with all 9.2-inch guns. In late 1904, not long after the Royal Navy had decided to use 12-inch guns for its next generation of battleships because of their superior performance at long range, Fisher began to argue that big-gun cruisers could replace battleships altogether. The continuing improvement of the torpedo meant that submarines and destroyers would be able to destroy battleships; this in Fisher's view heralded the end of the battleship or at least compromised the validity of heavy armour protection. Nevertheless, armoured cruisers would remain vital for commerce protection.\nFisher's views were very controversial within the Royal Navy, and even given his position as First Sea Lord, he was not in a position to insist on his own approach. Thus he assembled a \"Committee on Designs\", consisting of a mixture of civilian and naval experts, to determine the approach to both battleship and armoured cruiser construction in the future. While the stated purpose of the committee was to investigate and report on future requirements of ships, Fisher and his associates had already made key decisions. The terms of reference for the committee were for a battleship capable of with 12-inch guns and no intermediate calibres, capable of docking in existing drydocks; and a cruiser capable of , also with 12-inch guns and no intermediate armament, armoured like , the most recent armoured cruiser, and also capable of using existing docks.\nFirst battlecruisers.\nUnder the Selborne plan of 1902, the Royal Navy intended to start three new battleships and four armoured cruisers each year. However, in late 1904 it became clear that the 1905\u20131906 programme would have to be considerably smaller, because of lower than expected tax revenue and the need to buy out two Chilean battleships under construction in British yards, lest they be purchased by the Russians for use against the Japanese, Britain's ally. These economic realities meant that the 1905\u20131906 programme consisted only of one battleship, but three armoured cruisers. The battleship became the revolutionary battleship , and the cruisers became the three ships of the . Fisher later claimed, however, that he had argued during the committee for the cancellation of the remaining battleship.\nThe construction of the new class was begun in 1906 and completed in 1908, delayed perhaps to allow their designers to learn from any problems with \"Dreadnought\". The ships fulfilled the design requirement quite closely. On a displacement similar to \"Dreadnought\", the \"Invincible\"s were longer to accommodate additional boilers and more powerful turbines to propel them at . Moreover, the new ships could maintain this speed for days, whereas pre-dreadnought battleships could not generally do so for more than an hour. Armed with eight 12-inch Mk X guns, compared to ten on \"Dreadnought\", they had of armour protecting the hull and the gun turrets. (\"Dreadnought\"s armour, by comparison, was at its thickest.) The class had a very marked increase in speed, displacement and firepower compared to the most recent armoured cruisers but no more armour.\nWhile the \"Invincible\"s were to fill the same role as the armoured cruisers they succeeded, they were expected to do so more effectively. Specifically their roles were:\nConfusion about how to refer to these new battleship-size armoured cruisers set in almost immediately. Even in late 1905, before work was begun on the \"Invincible\"s, a Royal Navy memorandum refers to \"large armoured ships\" meaning both battleships and large cruisers. In October 1906, the Admiralty began to classify all post-Dreadnought battleships and armoured cruisers as \"capital ships\", while Fisher used the term \"dreadnought\" to refer either to his new battleships or the battleships and armoured cruisers together. At the same time, the \"Invincible\" class themselves were referred to as \"cruiser-battleships\", \"dreadnought cruisers\"; the term \"battlecruiser\" was first used by Fisher in 1908. Finally, on 24 November 1911, Admiralty Weekly Order No. 351 laid down that \"All cruisers of the \"Invincible\" and later types are for the future to be described and classified as \"battle cruisers\" to distinguish them from the armoured cruisers of earlier date.\"\nAlong with questions over the new ships' nomenclature came uncertainty about their actual role due to their lack of protection. If they were primarily to act as scouts for the battle fleet and hunter-killers of enemy cruisers and commerce raiders, then the seven inches of belt armour with which they had been equipped would be adequate. If, on the other hand, they were expected to reinforce a battle line of dreadnoughts with their own heavy guns, they were too thin-skinned to be safe from an enemy's heavy guns. The \"Invincible\"s were essentially extremely large, heavily armed, fast armoured cruisers. However, the viability of the armoured cruiser was already in doubt. A cruiser that could have worked with the Fleet might have been a more viable option for taking over that role.\nBecause of the \"Invincible\"s size and armament, naval authorities considered them capital ships almost from their inception\u2014an assumption that might have been inevitable. Complicating matters further was that many naval authorities, including Lord Fisher, had made overoptimistic assessments from the Battle of Tsushima in 1905 about the armoured cruiser's ability to survive in a battle line against enemy capital ships due to their superior speed. These assumptions had been made without taking into account the Russian Baltic Fleet's inefficiency and tactical ineptitude. By the time the term \"battlecruiser\" had been given to the \"Invincible\"s, the idea of their parity with battleships had been fixed in many people's minds.\nNot everyone was so convinced. \"Brasseys Naval Annual\", for instance, stated that with vessels as large and expensive as the \"Invincible\"s, an admiral \"will be certain to put them in the line of battle where their comparatively light protection will be a disadvantage and their high speed of no value.\" Those in favor of the battlecruiser countered with two points\u2014first, since all capital ships were vulnerable to new weapons such as the torpedo, armour had lost some of its validity; and second, because of its greater speed, the battlecruiser could control the range at which it engaged an enemy.\nBattlecruisers in the dreadnought arms race.\nBetween the launching of the \"Invincible\"s to just after the outbreak of the First World War, the battlecruiser played a junior role in the developing dreadnought arms race, as it was never wholeheartedly adopted as the key weapon in British imperial defence, as Fisher had presumably desired. The biggest factor for this lack of acceptance was the marked change in Britain's strategic circumstances between their conception and the commissioning of the first ships. The prospective enemy for Britain had shifted from a Franco-Russian alliance with many armoured cruisers to a resurgent and increasingly belligerent Germany. Diplomatically, Britain had entered the Entente cordiale in 1904 and the Anglo-Russian Entente. Neither France nor Russia posed a particular naval threat; the Russian navy had largely been sunk or captured in the Russo-Japanese War of 1904\u20131905, while the French were in no hurry to adopt the new dreadnought-type design. Britain also boasted very cordial relations with two of the significant new naval powers: Japan (bolstered by the Anglo-Japanese Alliance, signed in 1902 and renewed in 1905), and the US. These changed strategic circumstances, and the great success of the \"Dreadnought\" ensured that she rather than the \"Invincible\" became the new model capital ship. Nevertheless, battlecruiser construction played a part in the renewed naval arms race sparked by the \"Dreadnought\".\nFor their first few years of service, the \"Invincible\"s entirely fulfilled Fisher's vision of being able to sink any ship fast enough to catch them, and run from any ship capable of sinking them. An \"Invincible\" would also, in many circumstances, be able to take on an enemy pre-dreadnought battleship. Naval circles concurred that the armoured cruiser in its current form had come to the logical end of its development and the \"Invincible\"s were so far ahead of any enemy armoured cruiser in firepower and speed that it proved difficult to justify building more or bigger cruisers. This lead was extended by the surprise both \"Dreadnought\" and \"Invincible\" produced by having been built in secret; this prompted most other navies to delay their building programmes and radically revise their designs. This was particularly true for cruisers, because the details of the \"Invincible\" class were kept secret for longer; this meant that the last German armoured cruiser, , was armed with only guns, and was no match for the new battlecruisers.\nThe Royal Navy's early superiority in capital ships led to the rejection of a 1905\u20131906 design that would, essentially, have fused the battlecruiser and battleship concepts into what would eventually become the fast battleship. The 'X4' design combined the full armour and armament of \"Dreadnought\" with the 25-knot speed of \"Invincible\". The additional cost could not be justified given the existing British lead and the new Liberal government's need for economy; the slower and cheaper , a relatively close copy of \"Dreadnought\", was adopted instead. The X4 concept would eventually be fulfilled in the and later by other navies.\nThe next British battlecruisers were the three , slightly improved \"Invincible\"s built to fundamentally the same specification, partly due to political pressure to limit costs and partly due to the secrecy surrounding German battlecruiser construction, particularly about the heavy armour of . This class came to be widely seen as a mistake and the next generation of British battlecruisers were markedly more powerful. By 1909\u20131910 a sense of national crisis about rivalry with Germany outweighed cost-cutting, and a naval panic resulted in the approval of a total of eight capital ships in 1909\u20131910. Fisher pressed for all eight to be battlecruisers, but was unable to have his way; he had to settle for six battleships and two battlecruisers of the . The \"Lion\"s carried eight 13.5-inch guns, the now-standard caliber of the British \"super-dreadnought\" battleships. Speed increased to and armour protection, while not as good as in German designs, was better than in previous British battlecruisers, with armour belt and barbettes. The two \"Lion\"s were followed by the very similar .\nBy 1911 Germany had built battlecruisers of her own, and the superiority of the British ships could no longer be assured. Moreover, the German Navy did not share Fisher's view of the battlecruiser. In contrast to the British focus on increasing speed and firepower, Germany progressively improved the armour and staying power of their ships to better the British battlecruisers. \"Von der Tann\", begun in 1908 and completed in 1910, carried eight 11.1-inch guns, but with 11.1-inch (283\u00a0mm) armour she was far better protected than the \"Invincible\"s. The two s were quite similar but carried ten 11.1-inch guns of an improved design. , designed in 1909 and finished in 1913, was a modified \"Moltke\"; speed increased by one knot to , while her armour had a maximum thickness of 12 inches, equivalent to the s of a few years earlier. \"Seydlitz\" was Germany's last battlecruiser completed before World War I.\nThe next step in battlecruiser design came from Japan. The Imperial Japanese Navy had been planning the ships from 1909, and was determined that, since the Japanese economy could support relatively few ships, each would be more powerful than its likely competitors. Initially the class was planned with the \"Invincible\"s as the benchmark. On learning of the British plans for \"Lion\", and the likelihood that new U.S. Navy battleships would be armed with guns, the Japanese decided to radically revise their plans and go one better. A new plan was drawn up, carrying eight 14-inch guns, and capable of , thus marginally having the edge over the \"Lion\"s in speed and firepower. The heavy guns were also better-positioned, being superfiring both fore and aft with no turret amidships. The armour scheme was also marginally improved over the \"Lion\"s, with nine inches of armour on the turrets and on the barbettes. The first ship in the class was built in Britain, and a further three constructed in Japan. The Japanese also re-classified their powerful armoured cruisers of the \"Tsukuba\" and \"Ibuki\" classes, carrying four 12-inch guns, as battlecruisers; nonetheless, their armament was weaker and they were slower than any battlecruiser.\nThe next British battlecruiser, , was intended initially as the fourth ship in the \"Lion\" class, but was substantially redesigned. She retained the eight 13.5-inch guns of her predecessors, but they were positioned like those of \"Kong\u014d\" for better fields of fire. She was faster (making on sea trials), and carried a heavier secondary armament. \"Tiger\" was also more heavily armoured on the whole; while the maximum thickness of armour was the same at nine inches, the height of the main armour belt was increased. Not all the desired improvements for this ship were approved, however. Her designer, Sir Eustace Tennyson d'Eyncourt, had wanted small-bore water-tube boilers and geared turbines to give her a speed of , but he received no support from the authorities and the engine makers refused his request.\n1912 saw work begin on three more German battlecruisers of the , the first German battlecruisers to mount 12-inch guns. These ships, like \"Tiger\" and the \"Kong\u014d\"s, had their guns arranged in superfiring turrets for greater efficiency. Their armour and speed was similar to the previous \"Seydlitz\" class. In 1913, the Russian Empire also began the construction of the four-ship , which were designed for service in the Baltic Sea. These ships were designed to carry twelve 14-inch guns, with armour up to 12 inches thick, and a speed of . The heavy armour and relatively slow speed of these ships made them more similar to German designs than to British ships; construction of the \"Borodino\"s was halted by the First World War and all were scrapped after the end of the Russian Civil War.\nWorld War I.\nConstruction.\nFor most of the combatants, capital ship construction was very limited during the war. Germany finished the \"Derfflinger\" class and began work on the . The \"Mackensen\"s were a development of the \"Derfflinger\" class, with 13.8-inch guns and a broadly similar armour scheme, designed for .\nIn Britain, Jackie Fisher returned to the office of First Sea Lord in October 1914. His enthusiasm for big, fast ships was unabated, and he set designers to producing a design for a battlecruiser with 15-inch guns. Because Fisher expected the next German battlecruiser to steam at 28 knots, he required the new British design to be capable of 32 knots. He planned to reorder two s, which had been approved but not yet laid down, to a new design. Fisher finally received approval for this project on 28 December 1914 and they became the . With six 15-inch guns but only 6-inch armour they were a further step forward from \"Tiger\" in firepower and speed, but returned to the level of protection of the first British battlecruisers.\nAt the same time, Fisher resorted to subterfuge to obtain another three fast, lightly armoured ships that could use several spare gun turrets left over from battleship construction. These ships were essentially light battlecruisers, and Fisher occasionally referred to them as such, but officially they were classified as \"large light cruisers\". This unusual designation was required because construction of new capital ships had been placed on hold, while there were no limits on light cruiser construction. They became and her sisters and , and there was a bizarre imbalance between their main guns of 15 inches (or in \"Furious\") and their armour, which at thickness was on the scale of a light cruiser. The design was generally regarded as a failure (nicknamed in the Fleet \"Outrageous\", \"Uproarious\" and \"Spurious\"), though the later conversion of the ships to aircraft carriers was very successful. Fisher also speculated about a new mammoth, but lightly built battlecruiser, that would carry guns, which he termed ; this never got beyond the concept stage.\nIt is often held that the \"Renown\" and \"Courageous\" classes were designed for Fisher's plan to land troops (possibly Russian) on the German Baltic coast. Specifically, they were designed with a reduced draught, which might be important in the shallow Baltic. This is not clear-cut evidence that the ships were designed for the Baltic: it was considered that earlier ships had too much draught and not enough freeboard under operational conditions. Roberts argues that the focus on the Baltic was probably unimportant at the time the ships were designed, but was inflated later, after the disastrous Dardanelles Campaign.\nThe final British battlecruiser design of the war was the , which was born from a requirement for an improved version of the \"Queen Elizabeth\" battleship. The project began at the end of 1915, after Fisher's final departure from the Admiralty. While initially envisaged as a battleship, senior sea officers felt that Britain had enough battleships, but that new battlecruisers might be required to combat German ships being built (the British overestimated German progress on the \"Mackensen\" class as well as their likely capabilities). A battlecruiser design with eight 15-inch guns, 8\u00a0inches of armour and capable of 32 knots was decided on. The experience of battlecruisers at the Battle of Jutland meant that the design was radically revised and transformed again into a fast battleship with armour up to 12 inches thick, but still capable of . The first ship in the class, , was built according to this design to counter the possible completion of any of the Mackensen-class ship. The plans for her three sisters, on which little work had been done, were revised once more later in 1916 and in 1917 to improve protection.\nThe Admiral class would have been the only British ships capable of taking on the German \"Mackensen\" class; nevertheless, German shipbuilding was drastically slowed by the war, and while two \"Mackensen\"s were launched, none were ever completed. The Germans also worked briefly on a further three ships, of the , which were modified versions of the \"Mackensen\"s with 15-inch guns. Work on the three additional Admirals was suspended in March 1917 to enable more escorts and merchant ships to be built to deal with the new threat from U-boats to trade. They were finally cancelled in February 1919.\nBattlecruisers in action.\nThe first combat involving battlecruisers during World War I was the Battle of Heligoland Bight in August 1914. A force of British light cruisers and destroyers entered the Heligoland Bight (the part of the North Sea closest to Hamburg) to attack German destroyer patrols. When they met opposition from light cruisers, Vice Admiral David Beatty took his squadron of five battlecruisers into the Bight and turned the tide of the battle, ultimately sinking three German light cruisers and killing their commander, Rear Admiral Leberecht Maass.\nThe German battlecruiser perhaps made the most impact early in the war. Stationed in the Mediterranean, she and the escorting light cruiser evaded British and French ships on the outbreak of war, and steamed to Constantinople (Istanbul) with two British battlecruisers in hot pursuit. The two German ships were handed over to the Ottoman Navy, and this was instrumental in bringing the Ottoman Empire into the war as one of the Central Powers. \"Goeben\" herself, renamed \"Yavuz Sultan Selim\", fought engagements against the Imperial Russian Navy in the Black Sea before being knocked out of the action for the remainder of the war after the Battle of Imbros against British forces in the Aegean Sea in January 1918.\nThe original battlecruiser concept proved successful in December 1914 at the Battle of the Falkland Islands. The British battlecruisers and did precisely the job for which they were intended when they chased down and annihilated the German East Asia Squadron, centered on the armoured cruisers and , along with three light cruisers, commanded by Admiral Maximilian Graf Von Spee, in the South Atlantic Ocean. Prior to the battle, the Australian battlecruiser had unsuccessfully searched for the German ships in the Pacific.\nDuring the Battle of Dogger Bank in 1915, the aftermost barbette of the German flagship \"Seydlitz\" was struck by a British 13.5-inch shell from HMS \"Lion\". The shell did not penetrate the barbette, but it dislodged a piece of the barbette armour that allowed the flame from the shell's detonation to enter the barbette. The propellant charges being hoisted upwards were ignited, and the fireball flashed up into the turret and down into the magazine, setting fire to charges removed from their brass cartridge cases. The gun crew tried to escape into the next turret, which allowed the flash to spread into that turret as well, killing the crews of both turrets. \"Seydlitz\" was saved from near-certain destruction only by emergency flooding of her after magazines, which had been effected by Wilhelm Heidkamp. This near-disaster was due to the way that ammunition handling was arranged and was common to both German and British battleships and battlecruisers, but the lighter protection on the latter made them more vulnerable to the turret or barbette being penetrated. The Germans learned from investigating the damaged \"Seydlitz\" and instituted measures to ensure that ammunition handling minimised any possible exposure to flash.\nApart from the cordite handling, the battle was mostly inconclusive, though both the British flagship \"Lion\" and \"Seydlitz\" were severely damaged. \"Lion\" lost speed, causing her to fall behind the rest of the battleline, and Beatty was unable to effectively command his ships for the remainder of the engagement. A British signalling error allowed the German battlecruisers to withdraw, as most of Beatty's squadron mistakenly concentrated on the crippled armoured cruiser \"Bl\u00fccher\", sinking her with great loss of life. The British blamed their failure to win a decisive victory on their poor gunnery and attempted to increase their rate of fire by stockpiling unprotected cordite charges in their ammunition hoists and barbettes.\nAt the Battle of Jutland on 31 May 1916, both British and German battlecruisers were employed as fleet units. The British battlecruisers became engaged with both their German counterparts, the battlecruisers, and then German battleships before the arrival of the battleships of the British Grand Fleet. The result was a disaster for the Royal Navy's battlecruiser squadrons: \"Invincible\", \"Queen Mary\", and exploded with the loss of all but a handful of their crews. The exact reason why the ships' magazines detonated is not known, but the abundance of exposed cordite charges stored in their turrets, ammunition hoists and working chambers in the quest to increase their rate of fire undoubtedly contributed to their loss. Beatty's flagship \"Lion\" herself was almost lost in a similar manner, save for the heroic actions of Major Francis Harvey.\nThe better-armoured German battlecruisers fared better, in part due to the poor performance of British fuzes (the British shells tended to explode or break up on impact with the German armour). \u2014the only German battlecruiser lost at Jutland\u2014had only 128 killed, for instance, despite receiving more than thirty hits. The other German battlecruisers, , \"Von der Tann\", \"Seydlitz\", and , were all heavily damaged and required extensive repairs after the battle, \"Seydlitz\" barely making it home, for they had been the focus of British fire for much of the battle.\nInterwar period.\nIn the years immediately after World War I, Britain, Japan and the US all began design work on a new generation of ever more powerful battleships and battlecruisers. The new burst of shipbuilding that each nation's navy desired was politically controversial and potentially economically crippling. This nascent arms race was prevented by the Washington Naval Treaty of 1922, where the major naval powers agreed to limits on capital ship numbers. The German navy was not represented at the talks; under the terms of the Treaty of Versailles, Germany was not allowed any modern capital ships at all.\nThrough the 1920s and 1930s only Britain and Japan retained battlecruisers, often modified and rebuilt from their original designs. The line between the battlecruiser and the modern fast battleship became blurred; indeed, the Japanese \"Kong\u014d\"s were formally redesignated as battleships after their very comprehensive reconstruction in the 1930s.\nPlans in the aftermath of World War I.\n\"Hood\", launched in 1918, was the last World War I battlecruiser to be completed. Owing to lessons from Jutland, the ship was modified during construction; the thickness of her belt armour was increased by an average of 50 percent and extended substantially, she was given heavier deck armour, and the protection of her magazines was improved to guard against the ignition of ammunition. This was hoped to be capable of resisting her own weapons\u2014the classic measure of a \"balanced\" battleship. \"Hood\" was the largest ship in the Royal Navy when completed; because of her great displacement, in theory she combined the firepower and armour of a battleship with the speed of a battlecruiser, causing some to refer to her as a fast battleship. However, her protection was markedly less than that of the British battleships built immediately after World War I, the .\nThe navies of Japan and the United States, not being affected immediately by the war, had time to develop new heavy guns for their latest designs and to refine their battlecruiser designs in light of combat experience in Europe. The Imperial Japanese Navy began four s. These vessels would have been of unprecedented size and power, as fast and well armoured as \"Hood\" whilst carrying a main battery of ten 16-inch guns, the most powerful armament ever proposed for a battlecruiser. They were, for all intents and purposes, fast battleships\u2014the only differences between them and the s which were to precede them were less side armour and a increase in speed. The United States Navy, which had worked on its battlecruiser designs since 1913 and watched the latest developments in this class with great care, responded with the . If completed as planned, they would have been exceptionally fast and well armed with eight 16-inch guns, but carried armour little better than the \"Invincible\"s\u2014this after an increase in protection following Jutland. The final stage in the post-war battlecruiser race came with the British response to the \"Amagi\" and \"Lexington\" types: four G3 battlecruisers. Royal Navy documents of the period often described any battleship with a speed of over about as a battlecruiser, regardless of the amount of protective armour, although the G3 was considered by most to be a well-balanced fast battleship.\nThe Washington Naval Treaty meant that none of these designs came to fruition. Ships that had been started were either broken up on the slipway or converted to aircraft carriers. In Japan, \"Amagi\" and were selected for conversion. \"Amagi\" was damaged beyond repair by the 1923 Great Kant\u014d earthquake and was broken up for scrap; the hull of one of the proposed \"Tosa\"-class battleships, , was converted in her stead. The United States Navy also converted two battlecruiser hulls into aircraft carriers in the wake of the Washington Treaty: and , although this was only considered marginally preferable to scrapping the hulls outright (the remaining four: \"Constellation\", \"Ranger\", \"Constitution\" and \"United States\" were scrapped). In Britain, Fisher's \"large light cruisers,\" were converted to carriers. \"Furious\" had already been partially converted during the war and \"Glorious\" and \"Courageous\" were similarly converted.\nRebuilding programmes.\nIn total, nine battlecruisers survived the Washington Naval Treaty, although HMS \"Tiger\" later became a victim of the London Naval Conference 1930 and was scrapped. Because their high speed made them valuable surface units in spite of their weaknesses, most of these ships were significantly updated before World War II. and were modernized significantly in the 1920s and 1930s. Between 1934 and 1936, \"Repulse\" was partially modernized and had her bridge modified, an aircraft hangar, catapult and new gunnery equipment added and her anti-aircraft armament increased. \"Renown\" underwent a more thorough reconstruction between 1937 and 1939. Her deck armour was increased, new turbines and boilers were fitted, an aircraft hangar and catapult added and she was completely rearmed aside from the main guns which had their elevation increased to +30 degrees. The bridge structure was also removed and a large bridge similar to that used in the battleships installed in its place. While conversions of this kind generally added weight to the vessel, \"Renown\"s tonnage actually decreased due to a substantially lighter power plant. Similar thorough rebuildings planned for \"Repulse\" and \"Hood\" were cancelled due to the advent of World War II.\nUnable to build new ships, the Imperial Japanese Navy also chose to improve its existing battlecruisers of the \"Kong\u014d\" class (initially the , , and \u2014the only later as it had been disarmed under the terms of the Washington treaty) in two substantial reconstructions (one for \"Hiei\"). During the first of these, elevation of their main guns was increased to +40 degrees, anti-torpedo bulges and of horizontal armour added, and a \"pagoda\" mast with additional command positions built up. This reduced the ships' speed to . The second reconstruction focused on speed as they had been selected as fast escorts for aircraft carrier task forces. Completely new main engines, a reduced number of boilers and an increase in hull length by allowed them to reach up to 30 knots once again. They were reclassified as \"fast battleships,\" although their armour and guns still fell short compared to surviving World War I\u2013era battleships in the American or the British navies, with dire consequences during the Pacific War, when \"Hiei\" and \"Kirishima\" were easily crippled by US gunfire during actions off Guadalcanal, forcing their scuttling shortly afterwards. Perhaps most tellingly, \"Hiei\" was crippled by medium-caliber gunfire from heavy and light cruisers in a close-range night engagement.\nThere were two exceptions: Turkey's \"Yavuz Sultan Selim\" and the Royal Navy's \"Hood\". The Turkish Navy made only minor improvements to the ship in the interwar period, which primarily focused on repairing wartime damage and the installation of new fire control systems and anti-aircraft batteries. \"Hood\" was in constant service with the fleet and could not be withdrawn for an extended reconstruction. She received minor improvements over the course of the 1930s, including modern fire control systems, increased numbers of anti-aircraft guns, and in March 1941, radar.\nNaval rearmament.\nIn the late 1930s navies began to build capital ships again, and during this period a number of large commerce raiders and small, fast battleships were built that are sometimes referred to as battlecruisers. Germany and Russia designed new battlecruisers during this period, though only the latter laid down two of the 35,000-ton . They were still on the slipways when the Germans invaded in 1941 and construction was suspended. Both ships were scrapped after the war.\nThe Germans planned three battlecruisers of the as part of the expansion of the Kriegsmarine (Plan Z). With six 15-inch guns, high speed, excellent range, but very thin armour, they were intended as commerce raiders. Only one was ordered shortly before World War II; no work was ever done on it. No names were assigned, and they were known by their contract names: 'O', 'P', and 'Q'. The new class was not universally welcomed in the Kriegsmarine. Their abnormally-light protection gained it the derogatory nickname \"Ohne Panzer Quatsch\" (without armour nonsense) within certain circles of the Navy.\nWorld War II.\nThe Royal Navy deployed some of its battlecruisers during the Norwegian Campaign in April 1940. The and the were engaged during the action off Lofoten by \"Renown\" in very bad weather and disengaged after \"Gneisenau\" was damaged. One of \"Renown\"s 15-inch shells passed through \"Gneisenau\"s director-control tower without exploding, severing electrical and communication cables as it went and destroyed the rangefinders for the forward 150\u00a0mm (5.9\u00a0in) turrets. Main-battery fire control had to be shifted aft due to the loss of electrical power. Another shell from \"Renown\" knocked out \"Gneisenau\"s aft turret. The British ship was struck twice by German shells that failed to inflict any significant damage. She was the only pre-war battlecruiser to survive the war.\nIn the early years of the war various German ships had a measure of success hunting merchant ships in the Atlantic. Allied battlecruisers such as \"Renown\", \"Repulse\", and the fast battleships \"Dunkerque\" and were employed on operations to hunt down the commerce-raiding German ships. The one stand-up fight occurred when the battleship and the heavy cruiser sortied into the North Atlantic to attack British shipping and were intercepted by \"Hood\" and the battleship in May 1941 in the Battle of the Denmark Strait. \"Hood\" was destroyed when the \"Bismarck\"s 15-inch shells caused a magazine explosion. Only three men survived.\nThe first battlecruiser to see action in the Pacific War was \"Repulse\" when she was sunk by Japanese torpedo bombers north of Singapore on 10 December 1941 whilst in company with \"Prince of Wales\". She was lightly damaged by a single bomb and near-missed by two others in the first Japanese attack. Her speed and agility enabled her to avoid the other attacks by level bombers and dodge 33 torpedoes. The last group of torpedo bombers attacked from multiple directions and \"Repulse\" was struck by five torpedoes. She quickly capsized with the loss of 27 officers and 486 crewmen; 42 officers and 754 enlisted men were rescued by the escorting destroyers. The loss of \"Repulse\" and \"Prince of Wales\" conclusively proved the vulnerability of capital ships to aircraft without air cover of their own.\nThe Japanese \"Kong\u014d\"-class battlecruisers were extensively used as carrier escorts for most of their wartime career due to their high speed. Their World War I\u2013era armament was weaker and their upgraded armour was still thin compared to contemporary battleships. On 13 November 1942, during the First Naval Battle of Guadalcanal, \"Hiei\" stumbled across American cruisers and destroyers at point-blank range. The ship was badly damaged in the encounter and had to be towed by her sister ship \"Kirishima\". Both were spotted by American aircraft the following morning and \"Kirishima\" was forced to cast off her tow because of repeated aerial attacks. \"Hiei\"s captain ordered her crew to abandon ship after further damage and scuttled \"Hiei\" in the early evening of 14 November. On the night of 14/15 November during the Second Naval Battle of Guadalcanal, \"Kirishima\" returned to Ironbottom Sound, but encountered the American battleships and . While failing to detect \"Washington\", \"Kirishima\" engaged \"South Dakota\" with some effect. \"Washington\" opened fire a few minutes later at short range and badly damaged \"Kirishima\", knocking out her aft turrets, jamming her rudder, and hitting the ship below the waterline. The flooding proved to be uncontrollable and \"Kirishima\" capsized three and a half hours later.\nReturning to Japan after the Battle of Leyte Gulf, \"Kong\u014d\" was torpedoed and sunk by the American submarine on 21 November 1944. \"Haruna\" was moored at Kure, Japan when the naval base was attacked by American carrier aircraft on 24 and 28 July 1945. The ship was only lightly damaged by a single bomb hit on 24 July, but was hit a dozen more times on 28 July and sank at her pier. She was refloated after the war and scrapped in early 1946.\nLarge cruisers or \"cruiser killers\".\nA late renaissance in popularity of ships between battleships and cruisers in size occurred on the eve of World War II. Described by some as battlecruisers, but never classified as capital ships, they were variously described as \"super cruisers\", \"large cruisers\" or even \"unrestricted cruisers\". The Dutch, American, and Japanese navies all planned these new classes specifically to counter the heavy cruisers, or their counterparts, being built by their naval rivals.\nThe first such battlecruisers were the Dutch Design 1047, designed to protect their colonies in the East Indies in the face of Japanese aggression. Never officially assigned names, these ships were designed with German and Italian assistance. While they broadly resembled the German \"Scharnhorst\" class and had the same main battery, they would have been more lightly armoured and only protected against eight-inch gunfire. Although the design was mostly completed, work on the vessels never commenced as the Germans overran the Netherlands in May 1940. The first ship would have been laid down in June of that year.\nThe only class of these late battlecruisers actually built were the United States Navy's \"large cruisers\". Two of them were completed, and ; a third, , was cancelled while under construction and three others, to be named \"Philippines\", \"Puerto Rico\" and \"Samoa\", were cancelled before they were laid down. They were classified as \"large cruisers\" instead of battlecruisers. These ships were named after territories or protectorates. (Battleships, were named after states and cruisers after cities.) With a main armament of nine 12-inch guns in three triple turrets and a displacement of , the \"Alaska\"s were twice the size of s and had guns some 50% larger in diameter. They lacked the thick armoured belt and intricate torpedo defence system of true capital ships. However, unlike most battlecruisers, they were considered a balanced design according to cruiser standards as their protection could withstand fire from their own caliber of gun, albeit only in a very narrow range band. They were designed to hunt down Japanese heavy cruisers, though by the time they entered service most Japanese cruisers had been sunk by American aircraft or submarines. Like the contemporary fast battleships, their speed ultimately made them more useful as carrier escorts and bombardment ships than as the surface combatants they were developed to be.\nThe Japanese started designing the B64 class, which was similar to the \"Alaska\" but with guns. News of the \"Alaska\"s led them to upgrade the design, creating Design B-65. Armed with 356\u00a0mm guns, the B65s would have been the best armed of the new breed of battlecruisers, but they still would have had only sufficient protection to keep out eight-inch shells. Much like the Dutch, the Japanese got as far as completing the design for the B65s, but never laid them down. By the time the designs were ready the Japanese Navy recognized that they had little use for the vessels and that their priority for construction should lie with aircraft carriers. Like the \"Alaska\"s, the Japanese did not call these ships battlecruisers, referring to them instead as super-heavy cruisers.\nCold War\u2013era designs.\nIn spite of the fact that most navies abandoned the battleship and battlecruiser concepts after World War II, Joseph Stalin's fondness for big-gun-armed warships caused the Soviet Union to plan a large cruiser class in the late 1940s. In the Soviet Navy, they were termed \"heavy cruisers\" (\"tjazholyj krejser\"). The fruits of this program were the Project 82 (\"Stalingrad\") cruisers, of standard load, nine guns and a speed of . Three ships were laid down in 1951\u20131952, but they were cancelled in April 1953 after Stalin's death. Only the central armoured hull section of the first ship, \"Stalingrad\", was launched in 1954 and then used as a target.\nThe Soviet is sometimes referred to as a battlecruiser. This description arises from their over displacement, which is roughly equal to that of a First World War battleship and more than twice the displacement of contemporary cruisers; upon entry into service, \"Kirov\" was the largest surface combatant to be built since World War II. The \"Kirov\" class lacks the armour that distinguishes battlecruisers from ordinary cruisers and they are classified as heavy nuclear-powered missile cruisers (\"\u0422\u044f\u0436\u0435\u043b\u044b\u0439 \u0410\u0442\u043e\u043c\u043d\u044b\u0439 \u0420\u0430\u043a\u0435\u0442\u043d\u044b\u0439 \u041a\u0440\u0435\u0439\u0441\u0435\u0440\" (\u0422\u0410\u0420\u041a\u0420)) by Russia, with their primary surface armament consisting of twenty P-700 Granit surface to surface missiles. Four members of the class were completed during the 1980s and 1990s, but due to budget constraints only the is operational with the Russian Navy, though plans were announced in 2010 to return the other three ships to service. As of 2021, was being refitted, but the other two ships are reportedly beyond economical repair."}
{"id": "4059", "revid": "2842084", "url": "https://en.wikipedia.org/wiki?curid=4059", "title": "Bob Hawke", "text": "Robert James Lee Hawke (9 December 1929\u00a0\u2013 16 May 2019) was an Australian politician and trade unionist who served as the 23rd prime minister of Australia from 1983 to 1991. He held office as the leader of the Australian Labor Party (ALP), having previously served as the president of the Australian Council of Trade Unions from 1969 to 1980 and president of the Labor Party national executive from 1973 to 1978.\nHawke was born in Border Town, South Australia. He attended the University of Western Australia and went on to study at University College, Oxford as a Rhodes Scholar. In 1956, Hawke joined the Australian Council of Trade Unions (ACTU) as a research officer. Having risen to become responsible for national wage case arbitration, he was elected as president of the ACTU in 1969, where he achieved a high public profile. In 1973, he was appointed as president of the Labor Party.\nIn 1980, Hawke stood down from his roles as ACTU and Labor Party president to announce his intention to enter parliamentary politics, and was subsequently elected to the Australian House of Representatives as a member of parliament (MP) for the division of Wills at the 1980 federal election. Three years later, he was elected unopposed to replace Bill Hayden as leader of the Australian Labor Party, and within five weeks led Labor to a landslide victory at the 1983 election, and was sworn in as prime minister. He led Labor to victory a further three times, with successful outcomes in 1984, 1987 and 1990 elections, making him the most electorally successful prime minister in the history of the Labor Party.\nThe Hawke government implemented a significant number of reforms, including major economic reforms, the establishment of Landcare, the introduction of the universal healthcare scheme Medicare, brokering the Prices and Incomes Accord, creating APEC, floating the Australian dollar, deregulating the financial sector, introducing the Family Assistance Scheme, enacting the Sex Discrimination Act to prevent discrimination in the workplace, declaring \"Advance Australia Fair\" as the country's national anthem, initiating superannuation pension schemes for all workers, negotiating a ban on mining in Antarctica and overseeing passage of the Australia Act that removed all remaining jurisdiction by the United Kingdom from Australia.\nIn June 1991, Hawke faced a leadership challenge by the Treasurer, Paul Keating, but Hawke managed to retain power; however, Keating mounted a second challenge six months later, and won narrowly, replacing Hawke as prime minister. Hawke subsequently retired from parliament, pursuing both a business career and a number of charitable causes, until his death in 2019, aged 89. Hawke remains his party's longest-serving Prime Minister, and Australia's third-longest-serving prime minister behind Robert Menzies and John Howard. He is also the only prime minister to be born in South Australia and the only one raised and educated in Western Australia. Hawke holds the highest-ever approval rating for an Australian prime minister, reaching 75% approval in 1984. Hawke is frequently ranked within the upper tier of Australian prime ministers by historians.\nEarly life and family.\nBob Hawke was born on 9 December 1929 in Border Town, South Australia, the second child of Arthur \"Clem\" Hawke (1898\u20131989), a Congregationalist minister, and his wife Edith Emily (Lee) (1897\u20131979) (known as Ellie), a schoolteacher. His uncle, Bert, was the Labor premier of Western Australia between 1953 and 1959.\nHawke's brother Neil, who was seven years his senior, died at the age of seventeen after contracting meningitis, for which there was no cure at the time. Ellie Hawke subsequently developed an almost messianic belief in her son's destiny, and this contributed to Hawke's supreme self-confidence throughout his career. At the age of fifteen, he presciently boasted to friends that he would one day become the prime minister of Australia.\nAt the age of seventeen, Hawke had a serious crash while riding his Panther motorcycle that left him in a critical condition for several days. This near-death experience acted as his catalyst, driving him to make the most of his talents and not let his abilities go to waste. He joined the Labor Party in 1947 at the age of eighteen.\nEducation and early career.\nHawke was educated at West Leederville State School, Perth Modern School and the University of Western Australia, graduating in 1952 with Bachelor of Arts and Bachelor of Laws degrees. He was also president of the university's guild during the same year. The following year, Hawke won a Rhodes Scholarship to attend University College, Oxford, where he began a Bachelor of Arts course in philosophy, politics and economics (PPE). He soon found he was covering much the same ground as he had in his education at the University of Western Australia, and transferred to a Bachelor of Letters course. He wrote his thesis on wage-fixing in Australia and successfully presented it in January 1956.\nIn 1956, Hawke accepted a scholarship to undertake doctoral studies in the area of arbitration law in the law department at the Australian National University in Canberra. Soon after his arrival at ANU, he became the students' representative on the University Council. A year later, he was recommended to the President of the ACTU to become a research officer, replacing Harold Souter who had become ACTU Secretary. The recommendation was made by Hawke's mentor at ANU, H. P. Brown, who for a number of years had assisted the ACTU in national wage cases. Hawke decided to abandon his doctoral studies and accept the offer, moving to Melbourne with his wife Hazel.\nWorld record beer skol (scull).\nHawke is well known for a \"world record\" allegedly achieved at Oxford University for a beer skol (scull) of a yard of ale in 11 seconds. The record is widely regarded as having been important to his career and ocker chic image. A 2023 article in the \"Journal of Australian Studies\" by C. J. Coventry concluded that Hawke's achievement was \"possibly fabricated\" and \"cultural propaganda\" designed to make Hawke appealing to unionised workers and nationalistic middle-class voters. The article contends that \"its location and time remain uncertain; there are no known witnesses; the field of competition was exclusive and with no scientific accountability; the record was first published in a beer pamphlet; and Hawke's recollections were unreliable.\"\nAustralian Council of Trade Unions.\nNot long after Hawke began work at the ACTU, he became responsible for the presentation of its annual case for higher wages to the national wages tribunal, the Commonwealth Conciliation and Arbitration Commission. He was first appointed as an ACTU advocate in 1959. The 1958 case, under previous advocate R.L. Eggleston, had yielded only a five-shilling increase. The 1959 case found for a fifteen-shilling increase, and was regarded as a personal triumph for Hawke. He went on to attain such success and prominence in his role as an ACTU advocate that, in 1969, he was encouraged to run for the position of ACTU President, despite the fact that he had never held elected office in a trade union.\nHe was elected ACTU President in 1969 on a modernising platform by the narrow margin of 399 to 350, with the support of the left of the union movement, including some associated with the Communist Party of Australia. He later credited Ray Gietzelt, General Secretary of the FMWU, as the single most significant union figure in helping him achieve this outcome. Questioned after his election on his political stance, Hawke stated that \"socialist is not a word I would use to describe myself\", saying instead his approach to politics was pragmatic. His commitment to the cause of Jewish Refuseniks purportedly led to a planned assassination attempt on Hawke by the Popular Front for the Liberation of Palestine, and its Australian operative Munif Mohammed Abou Rish.\nIn 1971, Hawke along with other members of the ACTU requested that South Africa send a non-racially biased team for the rugby union tour, with the intention of unions agreeing not to serve the team in Australia. Prior to arrival, the Western Australian branch of the Transport Workers' Union, and the Barmaids' and Barmens' Union, announced that they would serve the team, which allowed the Springboks to land in Perth. The tour commenced on 26 June and riots occurred as anti-apartheid protesters disrupted games. Hawke and his family started to receive malicious mail and phone calls from people who thought that sport and politics should not mix. Hawke remained committed to the ban on apartheid teams and later that year, the South African cricket team was successfully denied and no apartheid team was to ever come to Australia again. It was this ongoing dedication to racial equality in South Africa that would later earn Hawke the respect and friendship of Nelson Mandela.\nIn industrial matters, Hawke continued to demonstrate a preference for, and considerable skill at, negotiation, and was generally liked and respected by employers as well as the unions he advocated for. As early as 1972, speculation began that he would seek to enter the Parliament of Australia and eventually run to become the Leader of the Australian Labor Party. But while his professional career continued successfully, his heavy drinking and womanising placed considerable strains on his family life.\nIn June 1973, Hawke was elected as the Federal President of the Labor Party. Two years later, when the Whitlam government was controversially dismissed by the Governor-General, Hawke showed an initial keenness to enter Parliament at the ensuing election. Harry Jenkins, the MP for Scullin, came under pressure to step down to allow Hawke to stand in his place, but he strongly resisted this push. Hawke eventually decided not to attempt to enter Parliament at that time, a decision he soon regretted. After Labor was defeated at the election, Whitlam initially offered the leadership to Hawke, although it was not within Whitlam's power to decide who would succeed him. Despite not taking on the offer, Hawke remained influential, playing a key role in averting national strike action.\nDuring the 1977 federal election, he emerged as a strident opponent of accepting Vietnamese boat people as refugees into Australia, stating that they should be subject to normal immigration requirements and should otherwise be deported. He further stated only refugees selected off-shore should be accepted.\nHawke resigned as President of the Labor Party in August 1978. Neil Batt was elected in his place. The strain of this period took its toll on Hawke and in 1979 he suffered a physical collapse. This shock led Hawke to publicly announce his alcoholism in a television interview, and that he would make a concerted\u2014and ultimately successful\u2014effort to overcome it. He was helped through this period by the relationship that he had established with writer Blanche d'Alpuget, who, in 1982, published a biography of Hawke. His popularity with the public was, if anything, enhanced by this period of rehabilitation, and opinion polling suggested that he was a more popular public figure than either Labor Leader Bill Hayden or Liberal Prime Minister Malcolm Fraser.\nInformer for the United States.\nDuring the period of 1973 to 1979, Hawke acted as an informant for the United States government. According to Coventry, Hawke as concurrent leader of the ACTU and ALP informed the US of details surrounding labour disputes, especially those relating to American companies and individuals, such as union disputes with Ford Motor Company and the black ban of Frank Sinatra. The major industrial action taken against Sinatra came about because Sinatra had made sexist comments against female journalists. The dispute was the subject of the 2003 film \"The Night We Called It a Day\".\nHawke was described by US diplomats as \"a bulwark against anti-American sentiment and resurgent communism during the economic turmoil of the 1970s\", and often disputed with the Whitlam government over issues of foreign policy and industrial relations. US diplomats played a major role in shaping Hawke's consensus politics and economics. Although Hawke was the most prolific Australian informer for the United States in the 1970s, there were other prominent people at that time who secretly gave information. Biographer Troy Bramston rejects the view that Hawke's prolonged, discreet involvement with known members of the Central Intelligence Agency within the US Embassy amounted to Hawke being a CIA \"spy\".\nMember of Parliament.\nHawke's first attempt to enter Parliament came during the 1963 federal election. He stood in the seat of Corio in Geelong and managed to achieve a 3.1% swing against the national trend, although he fell short of ousting longtime Liberal incumbent Hubert Opperman. Hawke rejected several opportunities to enter Parliament throughout the 1970s, something he later wrote that he \"regretted\". He eventually stood for election to the House of Representatives at the 1980 election for the safe Melbourne seat of Wills, winning it comfortably. Immediately upon his election to Parliament, Hawke was appointed to the Shadow Cabinet by Labor Leader Bill Hayden as Shadow Minister for Industrial Relations.\nHayden, after having led the Labor Party to narrowly lose the 1980 election, was increasingly subject to criticism from Labor MPs over his leadership style. To quell speculation over his position, Hayden called a leadership spill on 16 July 1982, believing that if he won he would be guaranteed to lead Labor through to the next election. Hawke decided to challenge Hayden in the spill, but Hayden defeated him by five votes; the margin of victory, however, was too slim to dispel doubts that he could lead the Labor Party to victory at an election. Despite his defeat, Hawke began to agitate more seriously behind the scenes for a change in leadership, with opinion polls continuing to show that Hawke was a far more popular public figure than both Hayden and Prime Minister Malcolm Fraser. Hayden was further weakened after Labor's unexpectedly poor performance at a by-election in December 1982 for the Victorian seat of Flinders, following the resignation of the sitting member, former deputy Liberal leader Phillip Lynch. Labor needed a swing of 5.5% to win the seat and had been predicted by the media to win, but could only achieve 3%.\nLabor Party power-brokers, such as Graham Richardson and Barrie Unsworth, now openly switched their allegiance from Hayden to Hawke. More significantly, Hayden's staunch friend and political ally, Labor's Senate Leader John Button, had become convinced that Hawke's chances of victory at an election were greater than Hayden's. Initially, Hayden believed that he could remain in his job, but Button's defection proved to be the final straw in convincing Hayden that he would have to resign as Labor Leader. Less than two months after the Flinders by-election result, Hayden announced his resignation as Leader of the Labor Party on 3 February 1983. Hawke was subsequently elected as Leader unopposed on 8 February, and became Leader of the Opposition in the process. Having learned that morning about the possible leadership change, on the same that Hawke assumed the leadership of the Labor Party, Malcolm Fraser called a snap election for 5 March 1983, unsuccessfully attempting to prevent Labor from making the leadership change. However, he was unable to have the Governor-General confirm the election before Labor announced the change.\nAt the 1983 election, Hawke led Labor to a landslide victory, achieving a 24-seat swing and ending seven years of Liberal Party rule.\nWith the election called at the same time that Hawke became Labor leader this meant that Hawke never sat in Parliament as Leader of the Opposition having spent the entirety of his short Opposition leadership in the election campaign which he won.\nPrime Minister of Australia (1983\u20131991).\nLeadership style.\nAfter Labor's landslide victory, Hawke was sworn in as the Prime Minister by the Governor-General Ninian Stephen on 11 March 1983. The style of the Hawke government was deliberately distinct from the Whitlam government, the Labor government that preceded it. Rather than immediately initiating multiple extensive reform programs as Whitlam had, Hawke announced that Malcolm Fraser's pre-election concealment of the budget deficit meant that many of Labor's election commitments would have to be deferred. As part of his internal reforms package, Hawke divided the government into two tiers, with only the most senior ministers sitting in the Cabinet of Australia. The Labor caucus was still given the authority to determine who would make up the Ministry, but this move gave Hawke unprecedented powers to empower individual ministers.\nAfter Australia won the America's Cup in 1983 Hawke said \"any boss who sacks anyone for not turning up today is a bum\", effectively declaring an impromptu national public holiday.\nIn particular, the political partnership that developed between Hawke and his Treasurer, Paul Keating, proved to be essential to Labor's success in government, with multiple Labor figures in years since citing the partnership as the party's greatest ever. The two men proved a study in contrasts: Hawke was a Rhodes Scholar; Keating left high school early. Hawke's enthusiasms were cigars, betting and most forms of sport; Keating preferred classical architecture, Mahler symphonies and collecting British Regency and French Empire antiques. Despite not knowing one another before Hawke assumed the leadership in 1983, the two formed a personal as well as political relationship which enabled the Government to pursue a significant number of reforms, although there were occasional points of tension between the two.\nThe Labor Caucus under Hawke also developed a more formalised system of parliamentary factions, which significantly altered the dynamics of caucus operations. Unlike many of his predecessor leaders, Hawke's authority within the Labor Party was absolute. This enabled him to persuade MPs to support a substantial set of policy changes which had not been considered achievable by Labor governments in the past. Individual accounts from ministers indicate that while Hawke was not often the driving force behind individual reforms, outside of broader economic changes, he took on the role of providing political guidance on what was electorally feasible and how best to sell it to the public, tasks at which he proved highly successful. Hawke took on a very public role as Prime Minister, campaigning frequently even outside of election periods, and for much of his time in office proved to be incredibly popular with the Australian electorate; to this date he still holds the highest ever AC Nielsen approval rating of 75%.\nEconomic policy.\nThe Hawke government oversaw significant economic reforms, and is often cited by economic historians as being a \"turning point\" from a protectionist, agricultural model to a more globalised and services-oriented economy. According to the journalist Paul Kelly, \"the most influential economic decisions of the 1980s were the floating of the Australian dollar and the deregulation of the financial system\". Although the Fraser government had played a part in the process of financial deregulation by commissioning the 1981 Campbell Report, opposition from Fraser himself had stalled this process. Shortly after its election in 1983, the Hawke government took the opportunity to implement a comprehensive program of economic reform, in the process \"transform(ing) economics and politics in Australia\".\nHawke and Keating together led the process for overseeing the economic changes by launching a \"National Economic Summit\" one month after their election in 1983, which brought together business and industrial leaders together with politicians and trade union leaders; the three-day summit led to a unanimous adoption of a national economic strategy, generating sufficient political capital for widespread reform to follow. Among other reforms, the Hawke government floated the Australian dollar, repealed rules that prohibited foreign-owned banks to operate in Australia, dismantled the protectionist tariff system, privatised several state sector industries, ended the subsidisation of loss-making industries, and sold off part of the state-owned Commonwealth Bank.\nThe taxation system was also significantly reformed, with income tax rates reduced and the introduction of a fringe benefits tax and a capital gains tax; the latter two reforms were strongly opposed by the Liberal Party at the time, but were never reversed by them when they eventually returned to office in 1996. Partially offsetting these imposts upon the business community\u2014the \"main loser\" from the 1985 Tax Summit according to Paul Kelly\u2014was the introduction of full dividend imputation, a reform insisted upon by Keating. Funding for schools was also considerably increased as part of this package, while financial assistance was provided for students to enable them to stay at school longer; the number of Australian children completing school rose from 3 in 10 at the beginning of the Hawke government to 7 in 10 by its conclusion in 1991. Considerable progress was also made in directing assistance \"to the most disadvantaged recipients over the whole range of welfare benefits.\"\nSocial and environmental policy.\nAlthough criticisms were leveled against the Hawke government that it did not achieve all it said it would do on social policy, it nevertheless enacting a series of reforms which remain in place to the present day. From 1983 to 1989, the Government oversaw the permanent establishment of universal health care in Australia with the creation of Medicare, doubled the number of subsidised childcare places, began the introduction of occupational superannuation, oversaw a significant increase in school retention rates, created subsidised homecare services, oversaw the elimination of poverty traps in the welfare system, increased the real value of the old-age pension, reintroduced the six-monthly indexation of single-person unemployment benefits, and established a wide-ranging programme for paid family support, known as the Family Income Supplement. During the 1980s, the proportion of total government outlays allocated to families, the sick, single parents, widows, the handicapped, and veterans was significantly higher than under the previous Fraser and Whitlam governments.\nIn 1984, the Hawke government enacted the landmark Sex Discrimination Act 1984, which eliminated discrimination on the grounds of sex within the workplace. In 1989, Hawke oversaw the gradual re-introduction of some tuition fees for university study, creating set up the Higher Education Contributions Scheme (HECS). Under the original HECS, a $1,800 fee was charged to all university students, and the Commonwealth paid the balance. A student could defer payment of this HECS amount and repay the debt through the tax system, when the student's income exceeds a threshold level. As part of the reforms, Colleges of Advanced Education entered the university sector by various means. by doing so, university places were able to be expanded. Further notable policy decisions taken during the Government's time in office included the public health campaign regarding HIV/AIDS, and Indigenous land rights reform, with an investigation of the idea of a treaty between Aborigines and the Government being launched, although the latter would be overtaken by events, notably the Mabo court decision.\nThe Hawke government also drew attention for a series of notable environmental decisions, particularly in its second and third terms. In 1983, Hawke personally vetoed the construction of the Franklin Dam in Tasmania, responding to a groundswell of protest around the issue. Hawke also secured the nomination of the Wet Tropics of Queensland as a UNESCO World Heritage Site in 1987, preventing the forests there from being logged. Hawke would later appoint Graham Richardson as Environment Minister, tasking him with winning the second-preference support from environmental parties, something which Richardson later claimed was the major factor in the government's narrow re-election at the 1990 election. In the Government's fourth term, Hawke personally led the Australian delegation to secure changes to the Protocol on Environmental Protection to the Antarctic Treaty, ultimately winning a guarantee that drilling for minerals within Antarctica would be totally prohibited until 2048 at the earliest. Hawke later claimed that the Antarctic drilling ban was his \"proudest achievement\".\nIndustrial relations policy.\nAs a former ACTU President, Hawke was well-placed to engage in reform of the industrial relations system in Australia, taking a lead on this policy area as in few others. Working closely with ministerial colleagues and the ACTU Secretary, Bill Kelty, Hawke negotiated with trade unions to establish the Prices and Incomes Accord in 1983, an agreement whereby unions agreed to restrict their demands for wage increases, and in turn the Government guaranteed to both minimise inflation and promote an increased social wage, including by establishing new social programmes such as Medicare.\nInflation had been a significant issue for the previous decade prior to the election of the Hawke government, regularly running into double-digits. The process of the Accord, by which the Government and trade unions would arbitrate and agree upon wage increases in many sectors, led to a decrease in both inflation and unemployment through to 1990. Criticisms of the Accord would come from both the right and the left of politics. Left-wing critics claimed that it kept real wages stagnant, and that the Accord was a policy of class collaboration and corporatism. By contrast, right-wing critics claimed that the Accord reduced the flexibility of the wages system. Supporters of the Accord, however, pointed to the improvements in the social security system that occurred, including the introduction of rental assistance for social security recipients, the creation of labour market schemes such as NewStart, and the introduction of the Family Income Supplement. In 1986, the Hawke government passed a bill to de-register the Builders Labourers Federation federally due to the union not following the Accord agreements.\nDespite a percentage fall in real money wages from 1983 to 1991, the social wage of Australian workers was argued by the Government to have improved drastically as a result of these reforms, and the ensuing decline in inflation. The Accord was revisited six further times during the Hawke government, each time in response to new economic developments. The seventh and final revisiting would ultimately lead to the establishment of the enterprise bargaining system, although this would be finalised shortly after Hawke left office in 1991.\nForeign policy.\nArguably the most significant foreign policy achievement of the Government took place in 1989, after Hawke proposed a south-east Asian region-wide forum for leaders and economic ministers to discuss issues of common concern. After winning the support of key countries in the region, this led to the creation of the Asia-Pacific Economic Cooperation (APEC). The first APEC meeting duly took place in Canberra in November 1989; the economic ministers of Australia, Brunei, Canada, Indonesia, Japan, South Korea, Malaysia, New Zealand, Philippines, Singapore, Thailand and the United States all attended. APEC would subsequently grow to become one of the most pre-eminent high-level international forums in the world, particularly after the later inclusions of China and Russia, and the Keating government's later establishment of the APEC Leaders' Forum.\nElsewhere in Asia, the Hawke government played a significant role in the build-up to the United Nations peace process for Cambodia, culminating in the Transitional Authority; Hawke's Foreign Minister Gareth Evans was nominated for the Nobel Peace Prize for his role in negotiations. Hawke also took a major public stand after the 1989 Tiananmen Square protests and massacre; despite having spent years trying to get closer relations with China, Hawke gave a tearful address on national television describing the massacre in graphic detail, and unilaterally offered asylum to over 42,000 Chinese students who were living in Australia at the time, many of whom had publicly supported the Tiananmen protesters. Hawke did so without even consulting his Cabinet, stating later that he felt he simply had to act.\nThe Hawke government pursued a close relationship with the United States, assisted by Hawke's close friendship with US Secretary of State George Shultz; this led to a degree of controversy when the Government supported the US's plans to test ballistic missiles off the coast of Tasmania in 1985, as well as seeking to overturn Australia's long-standing ban on uranium exports. Although the US ultimately withdrew the plans to test the missiles, the furore led to a fall in Hawke's approval ratings. Shortly after the 1990 election, Hawke would lead Australia into its first overseas military campaign since the Vietnam War, forming a close alliance with US President George H. W. Bush to join the coalition in the Gulf War. The Royal Australian Navy contributed several destroyers and frigates to the war effort, which successfully concluded in February 1991, with the expulsion of Iraqi forces from Kuwait. The success of the campaign, and the lack of any Australian casualties, led to a brief increase in the popularity of the Government.\nThrough his role on the Commonwealth Heads of Government Meeting, Hawke played a leading role in ensuring the Commonwealth initiated an international boycott on foreign investment into South Africa, building on work undertaken by his predecessor Malcolm Fraser, and in the process clashing publicly with Prime Minister of the United Kingdom Margaret Thatcher, who initially favoured a more cautious approach. The resulting boycott, led by the Commonwealth, was widely credited with helping bring about the collapse of apartheid, and resulted in a high-profile visit by Nelson Mandela in October 1990, months after the latter's release from a 27-year stint in prison. During the visit, Mandela publicly thanked the Hawke government for the role it played in the boycott.\nElection wins and leadership challenges.\nHawke benefited greatly from the disarray into which the Liberal Party fell after the resignation of Fraser following the 1983 election. The Liberals were torn between supporters of the more conservative John Howard and the more liberal Andrew Peacock, with the pair frequently contesting the leadership. Hawke and Keating were also able to use the concealment of the size of the budget deficit by Fraser before the 1983 election to great effect, damaging the Liberal Party's economic credibility as a result.\nHowever, Hawke's time as Prime Minister also saw friction develop between himself and the grassroots of the Labor Party, many of whom were unhappy at what they viewed as Hawke's iconoclasm and willingness to cooperate with business interests. Hawke regularly and publicly expressed his willingness to cull Labor's \"sacred cows\". The Labor Left faction, as well as prominent Labor backbencher Barry Jones, offered repeated criticisms of a number of government decisions. Hawke was also subject to challenges from some former colleagues in the trade union movement over his \"confrontationalist style\" in siding with the airline companies in the 1989 Australian pilots' strike.\nNevertheless, Hawke was able to comfortably maintain a lead as preferred prime minister in the vast majority of opinion polls carried out throughout his time in office. He recorded the highest popularity rating ever measured by an Australian opinion poll, reaching 75% approval in 1984. After leading Labor to a comfortable victory in the snap 1984 election, called to bring the mandate of the House of Representatives back in line with the Senate, Hawke was able to secure an unprecedented third consecutive term for Labor with a landslide victory in the double dissolution election of 1987. Hawke was subsequently able to lead the nation in the bicentennial celebrations of 1988, culminating with him welcoming Queen Elizabeth II to open the newly constructed Parliament House.\nThe late-1980s recession, and the accompanying high interest rates, saw the Government fall in opinion polls, with many doubting that Hawke could win a fourth election. Keating, who had long understood that he would eventually succeed Hawke as prime minister, began to plan a leadership change; at the end of 1988, Keating put pressure on Hawke to retire in the new year. Hawke rejected this suggestion but reached a secret agreement with Keating, the so-called \"Kirribilli Agreement\", stating that he would step down in Keating's favour at some point after the 1990 election. Hawke subsequently won that election, in the process leading Labor to a record fourth consecutive electoral victory, albeit by a slim margin. Hawke appointed Keating as deputy prime minister to replace the retiring Lionel Bowen.\nBy the end of 1990, frustrated by the lack of any indication from Hawke as to when he might retire, Keating made a provocative speech to the Federal Parliamentary Press Gallery. Hawke considered the speech disloyal, and told Keating he would renege on the Kirribilli Agreement as a result. After attempting to force a resolution privately, Keating finally resigned from the Government in June 1991 to challenge Hawke for the leadership. His resignation came soon after Hawke vetoed in Cabinet a proposal backed by Keating and other ministers for mining to take place at Coronation Hill in Kakadu National Park. Hawke won the leadership spill, and in a press conference after the result, Keating declared that he had fired his \"one shot\" on the leadership. Hawke appointed John Kerin to replace Keating as Treasurer.\nDespite his victory in the June spill, Hawke quickly began to be regarded by many of his colleagues as a \"wounded\" leader; he had now lost his long-term political partner, his ratings in opinion polls were beginning to fall significantly, and after nearly nine years as Prime Minister, there was speculation that it would soon be time for a new leader. Hawke's leadership was ultimately irrevocably damaged at the end of 1991; after Liberal Leader John Hewson released 'Fightback!', a detailed proposal for sweeping economic change, including the introduction of a goods and services tax, Hawke was forced to sack Kerin as Treasurer after the latter made a public gaffe attempting to attack the policy. Keating duly challenged for the leadership a second time on 19 December, arguing that he would better placed to defeat Hewson; this time, Keating succeeded, narrowly defeating Hawke by 56 votes to 51.\nIn a speech to the House of Representatives following the vote, Hawke declared that his nine years as prime minister had left Australia a better and wealthier country, and he was given a standing ovation by those present. He subsequently tendered his resignation to the Governor-General and pledged support to his successor. Hawke briefly returned to the backbench, before resigning from Parliament on 20 February 1992, sparking a by-election which was won by the independent candidate Phil Cleary from among a record field of 22 candidates. Keating would go on to lead Labor to a fifth victory at the 1993 election, although he was defeated by the Liberal Party at the 1996 election.\nHawke wrote that he had very few regrets over his time in office, although stated he wished he had been able to advance the cause of Indigenous land rights further. His bitterness towards Keating over the leadership challenges surfaced in his earlier memoirs, although by the 2000s Hawke stated he and Keating had buried their differences, and that they regularly dined together and considered each other friends. The publication of the book \"Hawke: The Prime Minister\", by Hawke's second wife, Blanche d'Alpuget, in 2010, reignited conflict between the two, with Keating accusing Hawke and d'Alpuget of spreading falsehoods about his role in the Hawke government. Despite this, the two campaigned together for Labor several times, including at the 2019 election, where they released their first joint article for nearly three decades; Craig Emerson, who worked for both men, said they had reconciled in later years after Hawke grew ill.\nRetirement and later life.\nAfter leaving Parliament, Hawke entered the business world, taking on a number of directorships and consultancy positions which enabled him to achieve considerable financial success. He avoided public involvement with the Labor Party during Keating's tenure as prime minister, not wanting to be seen as attempting to overshadow his successor. After Keating's defeat and the election of the Howard government at the 1996 election, he returned to public campaigning with Labor and regularly appearing at election launches. Despite his personal affection for Queen Elizabeth II, boasting that he had been her \"favourite Prime Minister\", Hawke was an enthusiastic republican and joined the campaign for a Yes vote in the 1999 republic referendum.\nIn 2002, Hawke was named to South Australia's Economic Development Board during the Rann government. In the lead up to the 2007 election, Hawke made a considerable personal effort to support Kevin Rudd, making speeches at a large number of campaign office openings across Australia, and appearing in multiple campaign advertisements. As well as campaigning against WorkChoices, Hawke also attacked John Howard's record as Treasurer, stating \"it was the judgement of every economist and international financial institution that it was the restructuring reforms undertaken by my government, with the full cooperation of the trade union movement, which created the strength of the Australian economy today\". In February 2008, after Rudd's victory, Hawke joined former Prime Ministers Gough Whitlam, Malcolm Fraser and Paul Keating in Parliament House to witness the long anticipated apology to the Stolen Generations.\nIn 2009, Hawke helped establish the Centre for Muslim and Non-Muslim Understanding at the University of South Australia. Interfaith dialogue was an important issue for Hawke, who told \"The Adelaide Review\" that he was \"convinced that one of the great potential dangers confronting the world is the lack of understanding in regard to the Muslim world. Fanatics have misrepresented what Islam is. They give a false impression of the essential nature of Islam.\"\nIn 2016, after taking part in Andrew Denton's Better Off Dead podcast, Hawke added his voice to calls for voluntary euthanasia to be legalised. Hawke labelled as 'absurd' the lack of political will to fix the problem. He revealed that he had such an arrangement with his wife Blanche should such a devastating medical situation occur. He also publicly advocated for nuclear power and the importation of international spent nuclear fuel to Australia for storage and disposal, stating that this could lead to considerable economic benefits for Australia.\nIn late December 2018, Hawke revealed that he was in \"terrible health\". While predicting a Labor win in the upcoming 2019 federal election, Hawke said he \"may not witness the party's success\". In May 2019, the month of the election, he issued a joint statement with Paul Keating endorsing Labor's economic plan and condemning the Liberal Party for \"completely [giving] up the economic reform agenda\". They stated that \"Shorten's Labor is the only party of government focused on the need to modernise the economy to deal with the major challenge of our time: human induced climate change\". It was the first joint press statement released by the two since 1991.\nIn March 2022, Troy Bramston, a journalist for \"The Australian\" and a political historian, wrote an unauthorised biography of Hawke titled \"Bob Hawke: Demons and Destiny\". Hawke gave Bramston full access to his previously unavailable personal papers and granted a series of interviews for the book. Bramston was the last person to interview Hawke before his death. The book, drawing on extensive Australian and international archives, and interviews with more than 100 people, is regarded as \"definitive\" and was shortlisted for the Australian Political Book of the Year Award.\nOn 16 May 2019, two days before the election, Hawke died at his home in Northbridge at the age of 89, following a short illness. His family held a private cremation on 27 May at Macquarie Park Cemetery and Crematorium where he was subsequently interred. A state memorial was held at the Sydney Opera House on 14 June; speakers included Craig Emerson as master of ceremonies and Kim Beazley reading the eulogy, as well as Paul Keating, Julia Gillard, Bill Kelty, Ross Garnaut, and incumbent Prime Minister Scott Morrison and Opposition Leader Anthony Albanese.\nPersonal life.\nHawke married Hazel Masterson in 1956 at Perth Trinity Church. They had three children: Susan (born 1957), Stephen (born 1959) and Roslyn (born 1961). Their fourth child, Robert Jr, died in early infancy in 1963. Hawke was named Victorian Father of the Year in 1971, an honour which his wife disputed due to his heavy drinking and womanising. The couple divorced in 1994, after he left her for the writer Blanche d'Alpuget, and the two lived together in Northbridge, a suburb on the North Shore of Sydney. The divorce estranged Hawke from some of his family for a period, although they had reconciled by the 2010s.\nThroughout his early life, Hawke was a heavy drinker, having set a world record for drinking during his years as a student. Hawke eventually suffered from alcohol poisoning following the death of his and Hazel's infant son in 1963. He publicly announced in 1980 that he would abstain from alcohol to seek election to Parliament, in a move which garnered significant public attention and support. Hawke began to drink again following his retirement from politics, although to a more manageable extent; on several occasions, in his later years, videos of Hawke downing beer at cricket matches would frequently go viral.\nOn the subject of religion, Hawke wrote, while attending the 1952 World Christian Youth Conference in India, that \"there were all these poverty stricken kids at the gate of this palatial place where we were feeding our face and I just (was) struck by this enormous sense of irrelevance of religion to the needs of people\". He subsequently abandoned his Christian beliefs. By the time he entered politics he was a self-described agnostic. Hawke told Andrew Denton in 2008 that his father's Christian faith had continued to influence his outlook, saying \"My father said if you believe in the fatherhood of God you must necessarily believe in the brotherhood of man, it follows necessarily, and even though I left the church and was not religious, that truth remained with me.\"\nHawke was a supporter of National Rugby League club the Canberra Raiders.\nLegacy.\nA biographical television film, \"Hawke\", premiered on the Ten Network in Australia on 18 July 2010, with Richard Roxburgh playing the title character. Rachael Blake and Felix Williamson portrayed Hazel Hawke and Paul Keating, respectively. Roxburgh reprised his role as Hawke in the 2020 episode \"Terra Nullius\" of the Netflix series \"The Crown\".\nThe Bob Hawke Gallery in Bordertown, which contains memorabilia from his life, was opened by Hawke in 2002. Hawke House, the house in Bordertown where Hawke spent his early childhood, was purchased by the Australian Government in 2021 and opened as an accommodation and function space in May 2024. A bronze bust of Hawke is located at the town's civic centre.\nIn December 2020, the Western Australian Government announced that it had purchased Hawke's childhood home in West Leederville and would maintain it as a state asset. The property will also be assessed for entry onto the State Register of Heritage Places.\nThe Australian Government pledged $5 million in July 2019 to establish a new annual scholarship\u2014the Bob Hawke John Monash Scholarship\u2014through the General Sir John Monash Foundation. Bob Hawke College, a high school in Subiaco, Western Australia named after Hawke, was opened in February 2020.\nIn March 2020, the Australian Electoral Commission announced that it would create a new Australian electoral division in the House of Representatives named in honour of Hawke. The Division of Hawke was first contested at the 2022 federal election, and is located in the state of Victoria, near the seat of Wills, which Hawke represented from 1980 to 1992.\nHonours.\nOrders\nForeign honours\nAwards.\nFellowships\nHonorary degrees"}
{"id": "4060", "revid": "41906662", "url": "https://en.wikipedia.org/wiki?curid=4060", "title": "Baldr", "text": "Baldr (Old Norse also Balder, Baldur) is a god in Germanic mythology. In Norse mythology, he is a son of the god Odin and the goddess Frigg, and has numerous brothers, such as Thor and V\u00e1li. In wider Germanic mythology, the god was known in Old English as ', and in Old High German as ', all ultimately stemming from the Proto-Germanic theonym ('hero' or 'prince').\nDuring the 12th century, Danish accounts by Saxo Grammaticus and other Danish Latin chroniclers recorded a euhemerized account of his story. Compiled in Iceland during the 13th century, but based on older Old Norse poetry, the \"Poetic Edda\" and the \"Prose Edda\" contain numerous references to the death of Baldr as both a great tragedy to the \u00c6sir and a harbinger of Ragnar\u00f6k.\nAccording to \"Gylfaginning\", a book of Snorri Sturluson's Prose Edda, Baldr's wife is Nanna and their son is Forseti. Baldr had the greatest ship ever built, \"Hringhorni\", and there is no place more beautiful than his hall, Breidablik.\nName.\nThe Old Norse theonym \"Baldr\" ('brave, defiant'; also 'lord, prince') and its various Germanic cognates \u2013 including Old English \"B\u00e6ld\u00e6g\" and Old High German \"Balder\" (or \"Palter\") \u2013 probably stems from Proto-Germanic \"*Bal\u00f0raz\" ('Hero, Prince'; cf. Old Norse \"mann-baldr\" 'great man', Old English \"bealdor\" 'prince, hero'), itself a derivative of \"*bal\u00feaz\", meaning 'brave' (cf. Old Norse \"ballr\" 'hard, stubborn', Gothic \"bal\u00fea*\" 'bold, frank', Old English \"beald\" 'bold, brave, confident', Old Saxon \"bald\" 'valiant, bold', Old High German \"bald\" 'brave, courageous').\nThis etymology was originally proposed by Jacob Grimm (1835), who also speculated on a comparison with the Lithuanian \"b\u00e1ltas\" ('white', also the name of a light-god) based on the semantic development from 'white' to 'shining' then 'strong'. According to linguist Vladimir Orel, this could be linguistically tenable. Philologist Rudolf Simek also argues that the Old English \"B\u00e6ld\u00e6g\" should be interpreted as meaning 'shining day', from a Proto-Germanic root *\"b\u0113l\"- (cf. Old English \"b\u00e6l\", Old Norse \"b\u00e1l\" 'fire') attached to \"d\u00e6g\" ('day').\nOld Norse also shows the usage of the word as an honorific in a few cases, as in \"baldur \u00ee brynju\" (S\u00e6m. 272b) and \"herbaldr\" (S\u00e6m. 218b), in general epithets of heroes. In continental Saxon and Anglo-Saxon tradition, the son of Woden is called not \"Bealdor\" but \"Baldag\" (Saxon) and \"B\u00e6ld\u00e6g, Beldeg\" (Anglo-Saxon), which shows association with \"day\", possibly with Day personified as a deity. This, as Grimm points out, would agree with the meaning \"shining one, white one, a god\" derived from the meaning of Baltic \"baltas\", further adducing Slavic \"Belobog\" and German \"Berhta\".\nAttestations.\nMerseburg Incantation.\nOne of the two Merseburg Incantations names \"Balder\" (in the genitive singular \"Balderes\"), but also mentions a figure named \"Phol\", considered to be a byname for Baldr (as in Scandinavian \"Falr\", \"Fjalarr\"; (in Saxo) \"Balderus\" : \"Fjallerus\"). The incantation relates of \"Phol ende Wotan\" riding to the woods, where the foot of Baldr's foal is sprained. Sinthgunt (the sister of the sun), Frigg and Odin sing to the foot in order for it to heal. The identification with Balder is not conclusive. Modern scholarship suggests that the god Freyr might be meant.\n\"Poetic Edda\".\nUnlike the Prose Edda, in the Poetic Edda the tale of Baldr's death is referred to rather than recounted at length. Baldr is mentioned in \"V\u00f6lusp\u00e1\", in \"Lokasenna\", and is the subject of the Eddic poem \"Baldr's Dreams\".\nAmong the visions which the V\u00f6lva sees and describes in V\u00f6lusp\u00e1 is Baldr's death. In stanza 32, the V\u00f6lva says she saw the fate of Baldr \"the bleeding god\":\nIn the next two stanzas, the V\u00f6lva refers to Baldr's killing, describes the birth of V\u00e1li for the slaying of H\u00f6\u00f0r and the weeping of Frigg:\nIn stanza 62 of V\u00f6lusp\u00e1, looking far into the future, the V\u00f6lva says that H\u00f6\u00f0r and Baldr will come back, with the union, according to Bellows, being a symbol of the new age of peace:\nBaldr is mentioned in two stanzas of Lokasenna, a poem which describes a flyting between the gods and the god Loki. In the first of the two stanzas, Frigg, Baldr's mother, tells Loki that if she had a son like Baldr, Loki would be killed:\nIn the next stanza, Loki responds to Frigg, and says that he is the reason Baldr \"will never ride home again\":\nThe Eddic poem \"Baldr's Dreams\" opens with the gods holding a council discussing why Baldr had had bad dreams:\nOdin then rides to Hel to a V\u00f6lva's grave and awakens her using magic. The V\u00f6lva asks Odin, who she does not recognize, who he is, and Odin answers that he is Vegtam (\"Wanderer\"). Odin asks the V\u00f6lva for whom are the benches covered in rings and the floor covered in gold. The V\u00f6lva tells him that in their location mead is brewed for Baldr, and that she spoke unwillingly, so she will speak no more:\nOdin asks the V\u00f6lva to not be silent and asks her who will kill Baldr. The V\u00f6lva replies and says that H\u00f6\u00f0r will kill Baldr, and again says that she spoke unwillingly, and that she will speak no more:\nOdin again asks the V\u00f6lva to not be silent and asks her who will avenge Baldr's death. The V\u00f6lva replies that V\u00e1li will, when he will be one night old. Once again, she says that she will speak no more:\nOdin again asks the V\u00f6lva to not be silent and says that he seeks to know who the women that will then weep be. The V\u00f6lva realizes that Vegtam is Odin in disguise. Odin says that the V\u00f6lva is not a V\u00f6lva, and that she is the mother of three giants. The V\u00f6lva tells Odin to ride back home proud, because she will speak to no more men until Loki escapes his bounds.\n\"Prose Edda\".\nIn \"Gylfaginning\", Baldr is described as follows:\nApart from this description, Baldr is known primarily for the story of his death, which is seen as the first in a chain of events that will ultimately lead to the destruction of the gods at Ragnar\u00f6k.\nBaldr had a dream of his own death and his mother, Frigg, had the same dream. Since dreams were usually prophetic, this depressed him, and so Frigg made every object on earth vow never to hurt Baldr. All objects made this vow, save for the mistletoe\u2014a detail which has traditionally been explained with the idea that it was too unimportant and nonthreatening to bother asking it to make the vow, but which Merrill Kaplan has instead argued echoes the fact that young people were not eligible to swear legal oaths, which could make them a threat later in life.\nWhen Loki, the mischief-maker, heard of this, he made a magical spear from this plant (in some later versions, an arrow). He hurried to the place where the gods were indulging in their new pastime of hurling objects at Baldr, which would bounce off without harming him. Loki gave the spear to Baldr's brother, the blind god H\u00f6\u00f0r, who then inadvertently killed his brother with it (other versions suggest that Loki guided the arrow himself). For this act, Odin and the \"\u00e1synja\" Rindr gave birth to V\u00e1li, who grew to adulthood within a day and slew H\u00f6\u00f0r.\nBaldr was ceremonially burnt upon his ship Hringhorni, the largest of all ships. On the pyre he was given the magical ring Draupnir. At first the gods were not able to push the ship out onto sea, and so they sent for Hyrrokin, a giantess, who came riding on a wolf and gave the ship such a push that fire flashed from the rollers and all the earth shook.\nAs he was carried to the ship, Odin whispered something in his ear. The import of this speech was held to be unknowable, and the question of what was said was thus used as an unanswerable riddle by Odin in other sources, namely against the giant Vafthrudnir in the Eddic poem \"Vafthrudnismal\" and in the riddles of Gestumblindi in \"Hervarar saga\".\nUpon seeing the corpse being carried to the ship, Nanna, his wife, died of grief. She was then placed on the funeral fire (perhaps a toned-down instance of Sati, also attested in the Arab traveller Ibn Fadlan's account of a funeral among the Rus'), after which it was set on fire. Baldr's horse with all its trappings was also laid on the pyre.\nAs the pyre was set on fire, Thor blessed it with his hammer Mj\u01ebllnir. As he did a small dwarf named Litr came running before his feet. Thor then kicked him into the pyre.\nUpon Frigg's entreaties, delivered through the messenger Hermod, Hel promised to release Baldr from the underworld if all objects alive and dead would weep for him. All did, except a giantess, \u00de\u00f6kk (often presumed to be the god Loki in disguise), who refused to mourn the slain god. Thus Baldr had to remain in the underworld, not to emerge until after Ragnar\u00f6k, when he and his brother H\u00f6\u00f0r would be reconciled and rule the new earth together with Thor's sons.\nBesides these descriptions of Baldr, the Prose Edda also explicitly links him to the Anglo-Saxon \"Beldeg\" in its prologue.\n\"Gesta Danorum\".\nWriting during the end of the 12th century, the Danish historian Saxo Grammaticus tells the story of Baldr (recorded as \"Balderus\") in a form that professes to be historical. According to him, Balderus and H\u00f8therus were rival suitors for the hand of Nanna, daughter of Gewar, King of Norway. Balderus was a demigod and common steel could not wound his sacred body. The two rivals encountered each other in a terrific battle. Though Odin and Thor and the other gods fought for Balderus, he was defeated and fled away, and H\u00f8therus married the princess.\nNevertheless, Balderus took heart of grace and again met H\u00f8therus in a stricken field. But he fared even worse than before. H\u00f8therus dealt him a deadly wound with a magic sword, named Mistletoe, which he had received from Mimir, the satyr of the woods; after lingering three days in pain Balderus died of his injury and was buried with royal honours in a barrow.\nUtrecht Inscription.\nA Latin votive inscription from Utrecht, from the 3rd or 4th century C.E., has been theorized as containing the dative form \"Baldruo\", pointing to a Latin nominative singular *\"Baldruus\", which some have identified with the Norse/Germanic god, although both the reading and this interpretation have been questioned.\n\"Anglo-Saxon Chronicle\".\nIn the Anglo-Saxon Chronicle Baldr is named as the ancestor of the monarchy of Kent, Bernicia, Deira, and Wessex through his supposed son Brond.\nToponyms.\nThere are a few old place names in Scandinavia that contain the name \"Baldr\". The most certain and notable one is the (former) parish name Balleshol in Hedmark county, Norway: \"a Balldrshole\" 1356 (where the last element is \"h\u00f3ll\" m \"mound; small hill\"). Others may be (in Norse forms) \"Baldrsberg\" in Vestfold county, \"Baldrsheimr\" in Hordaland county \"Baldrsnes\" in S\u00f8r-Tr\u00f8ndelag county\u2014and (very uncertain) the Balsfjorden fjord and Balsfjord Municipality in Troms county.\nIn Copenhagen, there is also a Baldersgade, or \"Balder's Street\". A street in downtown Reykjav\u00edk is called Baldursgata (Baldur's Street).\nIn Sweden there is a Baldersgatan (Balder's Street) in Stockholm. There is also Baldersn\u00e4s (Balder's isthmus), Baldersvik (Balder's bay), Balders udde (Balder's headland) and Baldersberg (Balder's mountain) at various places."}
{"id": "4061", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=4061", "title": "Breidablik", "text": "&lt;noinclude&gt;\nBrei\u00f0ablik (sometimes anglicised as Breithablik or Breidablik) is the home of Baldr in Nordic mythology.\nMeaning.\nThe word has been variously translated as 'broad sheen', 'Broad gleam', 'Broad-gleaming' or 'the far-shining one', \nAttestations.\nGr\u00edmism\u00e1l.\nThe Eddic poem Gr\u00edmnism\u00e1l describes Brei\u00f0ablik as the fair home of Baldr:\nGylfaginning.\nIn Snorri Sturluson's Gylfaginning, Brei\u00f0ablik is described in a list of places in heaven, identified by some scholars as Asgard:\nLater in the work, when Snorri describes Baldr, he gives another description, citing \"Gr\u00edmnism\u00e1l\", though he does not name the poem:\nInterpretation and discussion.\nThe name of Brei\u00f0ablik has been noted to link with Baldr's attributes of light and beauty.\nSimilarities have been drawn between the description of Brei\u00f0ablik in Gr\u00edmnism\u00e1l and Heorot in Beowulf, which are both free of 'baleful runes' ( and respectively). In Beowulf, the lack of refers to the absence of crimes being committed, and therefore both halls have been proposed to be sanctuaries."}
{"id": "4062", "revid": "1242588517", "url": "https://en.wikipedia.org/wiki?curid=4062", "title": "Bilskirnir", "text": "Bilskirnir (Old Norse \"lightning-crack\") is the hall of the god Thor in Norse mythology. Here he lives with his wife Sif and their children. According to \"Gr\u00edmnism\u00e1l\", the hall is the greatest of buildings and contains 540 rooms, located in Asgard, as are all the dwellings of the gods, in the kingdom of \u00der\u00fa\u00f0heimr (or \u00der\u00fa\u00f0vangar according to \"Gylfaginning\" and \"Ynglinga saga\")."}
{"id": "4063", "revid": "213139", "url": "https://en.wikipedia.org/wiki?curid=4063", "title": "Br\u00edsingamen", "text": "In Norse mythology, Br\u00edsingamen (or Br\u00edsinga men) is the torc or necklace of the goddess Freyja, of which little else is known for certain.\nEtymology.\nThe name is an Old Norse compound \"br\u00edsinga-men\" whose second element is \"men\" \"(ornamental) neck-ring (of precious metal), torc\".\nThe etymology of the first element is uncertain. It has been derived from Old Norse \"br\u00edsingr\", a poetic term for \"fire\" or \"amber\" mentioned in the anonymous versified word-lists (\"\u00feulur\") appended to many manuscripts of the Prose Edda, making Br\u00edsingamen \"gleaming torc\", \"sunny torc\", or the like. However, \"Br\u00edsingr\" can also be an ethnonym, in which case \"Br\u00edsinga men\" is \"torc of the Br\u00edsings\"; the Old English parallel in \"Beowulf\" supports this derivation, though who the Br\u00edsings (Old Norse \"Br\u00edsingar\") may have been remains unknown.\nAttestations.\n\"Beowulf\".\nBr\u00edsingamen is referred to in the Anglo-Saxon epic \"Beowulf\" as \"Brosinga mene\". The brief mention in \"Beowulf\" is as follows (trans. by Howell Chickering, 1977):\nThe \"Beowulf\" poet is clearly referring to the legends about Theoderic the Great. The \"\u00dei\u00f0rekssaga\" tells that the warrior Heime (\"H\u00e1ma\" in Old English) takes sides against Ermanaric (\"Eormanric\"), king of the Goths, and has to flee his kingdom after robbing him; later in life, Hama enters a monastery and gives them all his stolen treasure. However, this saga makes no mention of the great necklace.\n\"Poetic Edda\".\nIn the poem \"\u00derymskvi\u00f0a\" of the \"Poetic Edda\", \u00derymr, the king of the j\u01ebtnar, steals Thor's hammer, Mj\u00f6lnir. Freyja lends Loki her falcon cloak to search for it; but upon returning, Loki tells Freyja that \u00derymr has hidden the hammer and demanded to marry her in return. Freyja is so wrathful that all the \u00c6sir\u2019s halls beneath her are shaken and the necklace Br\u00edsingamen breaks off from her neck. Later Thor borrows Br\u00edsingamen when he dresses up as Freyja to go to the wedding at J\u01ebtunheimr.\n\"Prose Edda\".\n\"H\u00fasdr\u00e1pa\", a skaldic poem partially preserved in the \"Prose Edda\", relates the story of the theft of Br\u00edsingamen by Loki. One day when Freyja wakes up and finds Br\u00edsingamen missing, she enlists the help of Heimdallr to help her search for it. Eventually they find the thief, who turns out to be Loki who has transformed himself into a seal. Heimdallr turns into a seal as well and fights Loki (trans. Byock 2005):\nAfter a lengthy battle at Singasteinn, Heimdallr wins and returns Br\u00edsingamen to Freyja.\nSnorri Sturluson quoted this old poem in \"Sk\u00e1ldskaparm\u00e1l\", saying that because of this legend Heimdallr is called \"Seeker of Freyja's Necklace\" (\"Sk\u00e1ldskaparm\u00e1l\", section 8) and Loki is called \"Thief of Br\u00edsingamen\" (\"Sk\u00e1ldskaparm\u00e1l\", section 16). A similar story appears in the later \"S\u00f6rla \u00fe\u00e1ttr\", where Heimdallr does not appear.\n\"S\u00f6rla \u00fe\u00e1ttr\".\nS\u00f6rla \u00fe\u00e1ttr is a short story in the later and extended version of the \"Saga of Olaf Tryggvason\" in the manuscript of the \"Flateyjarb\u00f3k\", which was written and compiled by two Christian priests, Jon Thordson and Magnus Thorhalson, in the late 14th century. In the end of the story, the arrival of Christianity dissolves the old curse that traditionally was to endure until Ragnar\u00f6k.\nThe battle of H\u00f6gni and He\u00f0inn is recorded in several medieval sources, including the skaldic poem \"Ragnarsdr\u00e1pa\", \"Sk\u00e1ldskaparm\u00e1l\" (section 49), and \"Gesta Danorum\": king H\u00f6gni's daughter, Hildr, is kidnapped by king He\u00f0inn. When H\u00f6gni comes to fight He\u00f0inn on an island, Hildr comes to offer her father a necklace on behalf of He\u00f0inn for peace; but the two kings still battle, and Hildr resurrects the fallen to make them fight until Ragnar\u00f6k. None of these earlier sources mentions Freyja or king Olaf Tryggvason, the historical figure who Christianized Norway and Iceland in the 10th Century.\nArchaeological record.\nA V\u00f6lva was buried with considerable splendour in Hagebyh\u00f6ga in \u00d6sterg\u00f6tland, Sweden. In addition to being buried with her wand, she had received great riches which included horses, a wagon and an Arabian bronze pitcher. There was also a silver pendant, which represents a woman with a broad necklace around her neck. This kind of necklace was only worn by the most prominent women during the Iron Age and some have interpreted it as Freyja's necklace Br\u00edsingamen. The pendant may represent Freyja herself.\nModern influence.\nAlan Garner wrote a children's fantasy novel called \"The Weirdstone of Brisingamen\", published in 1960, about an enchanted teardrop bracelet.\nDiana Paxson's novel \"Brisingamen\" features Freyja and her necklace.\nBlack Phoenix Alchemy Lab has a perfumed oil scent named Brisingamen.\nFreyja's necklace Brisingamen features prominently in Betsy Tobin's novel \"Iceland\", where the necklace is seen to have significant protective powers.\nThe Brisingamen feature as a major item in Joel Rosenberg's Keepers of the Hidden Ways series of books. In it, there are seven jewels that were created for the necklace by the Dwarfs and given to the Norse goddess. She in turn eventually split them up into the seven separate jewels and hid them throughout the realm, as together they hold the power to shape the universe by its holder. The book's plot is about discovering one of them and deciding what to do with the power they allow while avoiding Loki and other Norse characters.\nIn Christopher Paolini's \"The Inheritance Cycle\", the word \"brisingr\" means fire. This is probably a distillation of the word \"brisinga\".\nUrsula Le Guin's short story \"Semley's Necklace\", the first part of her novel \"Rocannon's World\", is a retelling of the Brisingamen story on an alien planet.\nBrisingamen is represented as a card in the \"Yu-Gi-Oh!\" Trading Card Game, \"Nordic Relic Brisingamen\".\nBrisingamen was part of MMORPG \"Ragnarok Online\" lore, which is ranked as \"God item\". The game is heavily based from Norse mythology.\nIn the \"Firefly Online\" game, one of the planets of the Himinbj\u00f6rg system (which features planets named after figures from Germanic mythology) is named Brisingamen. It is third from the star, and has moons named Freya, Beowulf, and Alberich.\nThe Brisingamen is an item that can be found and equipped in the video game, \"\".\nIn the French comics \"Freaks' Squeele\", the character of Valkyrie accesses her costume change ability by touching a decorative torc necklace affixed to her forehead, named Brizingamen."}
{"id": "4064", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=4064", "title": "Borsuk\u2013Ulam theorem", "text": "In mathematics, the Borsuk\u2013Ulam theorem states that every continuous function from an \"n\"-sphere into Euclidean \"n\"-space maps some pair of antipodal points to the same point. Here, two points on a sphere are called antipodal if they are in exactly opposite directions from the sphere's center.\nFormally: if formula_1 is continuous then there exists an formula_2 such that: formula_3.\nThe case formula_4 can be illustrated by saying that there always exist a pair of opposite points on the Earth's equator with the same temperature. The same is true for any circle. This assumes the temperature varies continuously in space, which is, however, not always the case.\nThe case formula_5 is often illustrated by saying that at any moment, there is always a pair of antipodal points on the Earth's surface with equal temperatures and equal barometric pressures, assuming that both parameters vary continuously in space. \nThe Borsuk\u2013Ulam theorem has several equivalent statements in terms of odd functions. Recall that formula_6 is the \"n\"-sphere and formula_7 is the \"n\"-ball:\nHistory.\nAccording to , the first historical mention of the statement of the Borsuk\u2013Ulam theorem appears in . The first proof was given by , where the formulation of the problem was attributed to Stanis\u0142aw Ulam. Since then, many alternative proofs have been found by various authors, as collected by .\nEquivalent statements.\nThe following statements are equivalent to the Borsuk\u2013Ulam theorem.\nWith odd functions.\nA function formula_16 is called \"odd\" (aka \"antipodal\" or \"antipode-preserving\") if for every formula_17: formula_18.\nThe Borsuk\u2013Ulam theorem is equivalent to the following statement: A continuous odd function from an \"n\"-sphere into Euclidean \"n\"-space has a zero. PROOF: \nWith retractions.\nDefine a \"retraction\" as a function formula_25 The Borsuk\u2013Ulam theorem is equivalent to the following claim: there is no continuous odd retraction.\nProof: If the theorem is correct, then every continuous odd function from formula_6 must include 0 in its range. However, formula_27 so there cannot be a continuous odd function whose range is formula_12.\nConversely, if it is incorrect, then there is a continuous odd function formula_29 with no zeroes. Then we can construct another odd function formula_30 by:\nsince formula_16 has no zeroes, formula_33 is well-defined and continuous. Thus we have a continuous odd retraction.\nProofs.\n1-dimensional case.\nThe 1-dimensional case can easily be proved using the intermediate value theorem (IVT).\nLet formula_16 be the odd real-valued continuous function on a circle defined by formula_22. Pick an arbitrary formula_17. If formula_10 then we are done. Otherwise, without loss of generality, formula_38 But formula_39 Hence, by the IVT, there is a point formula_40 at which formula_41.\nGeneral case.\nAlgebraic topological proof.\nAssume that formula_30 is an odd continuous function with formula_43 (the case formula_44 is treated above, the case formula_45 can be handled using basic covering theory). By passing to orbits under the antipodal action, we then get an induced continuous function formula_46 between real projective spaces, which induces an isomorphism on fundamental groups. By the Hurewicz theorem, the induced ring homomorphism on cohomology with formula_47 coefficients [where formula_47 denotes the field with two elements], \nsends formula_50 to formula_51. But then we get that formula_52 is sent to formula_53, a contradiction.\nOne can also show the stronger statement that any odd map formula_54 has odd degree and then deduce the theorem from this result.\nCombinatorial proof.\nThe Borsuk\u2013Ulam theorem can be proved from Tucker's lemma.\nLet formula_8 be a continuous odd function. Because \"g\" is continuous on a compact domain, it is uniformly continuous. Therefore, for every formula_56, there is a formula_57 such that, for every two points of formula_58 which are within formula_59 of each other, their images under \"g\" are within formula_60 of each other.\nDefine a triangulation of formula_58 with edges of length at most formula_59. Label each vertex formula_63 of the triangulation with a label formula_64 in the following way:\nBecause \"g\" is odd, the labeling is also odd: formula_67. Hence, by Tucker's lemma, there are two adjacent vertices formula_68 with opposite labels. Assume w.l.o.g. that the labels are formula_69. By the definition of \"l\", this means that in both formula_70 and formula_71, coordinate #1 is the largest coordinate: in formula_70 this coordinate is positive while in formula_71 it is negative. By the construction of the triangulation, the distance between formula_70 and formula_71 is at most formula_60, so in particular formula_77 (since formula_78 and formula_79 have opposite signs) and so formula_80. But since the largest coordinate of formula_70 is coordinate #1, this means that formula_82 for each formula_83. So formula_84, where formula_85 is some constant depending on formula_86 and the norm formula_87 which you have chosen.\nThe above is true for every formula_56; since formula_58 is compact there must hence be a point \"u\" in which formula_90.\nEquivalent results.\nAbove we showed how to prove the Borsuk\u2013Ulam theorem from Tucker's lemma. The converse is also true: it is possible to prove Tucker's lemma from the Borsuk\u2013Ulam theorem. Therefore, these two theorems are equivalent."}
{"id": "4067", "revid": "308199", "url": "https://en.wikipedia.org/wiki?curid=4067", "title": "Bragi", "text": "Bragi (Old Norse) is the skaldic god of poetry in Norse mythology.\nEtymology.\nThe theonym Bragi probably stems from the masculine noun \"bragr\", which can be translated in Old Norse as 'poetry' (cf. Icelandic \"bragur\" 'poem, melody, wise') or as 'the first, noblest' (cf. poetic Old Norse \"bragnar\" 'chiefs, men', \"bragningr\" 'king'). It is unclear whether the theonym semantically derives from the first meaning or the second.\nA connection has been also suggested with the Old Norse \"bragarfull\", the cup drunk in solemn occasions with the taking of vows. The word is usually taken to semantically derive from the second meaning of \"bragr\" ('first one, noblest'). A relation with the Old English term \"brego\" ('lord, prince') remains uncertain.\n\"Bragi\" regularly appears as a personal name in Old Norse and Old Swedish sources, which according to linguist Jan de Vries might indicate the secondary character of the god's name.\nAttestations.\nSnorri Sturluson writes in the \"Gylfaginning\" after describing Odin, Thor, and Baldr:\nIn \"Sk\u00e1ldskaparm\u00e1l\" Snorri writes:\nThat Bragi is Odin's son is clearly mentioned only here and in some versions of a list of the sons of Odin (see Sons of Odin). But \"wish-son\" in stanza 16 of the \"Lokasenna\" could mean \"Odin's son\" and is translated by Hollander as \"Odin's kin\". Bragi's mother is possibly Frigg.\nIn that poem Bragi at first forbids Loki to enter the hall but is overruled by Odin. Loki then gives a greeting to all gods and goddesses who are in the hall save to Bragi. Bragi generously offers his sword, horse, and an arm ring as peace gift but Loki only responds by accusing Bragi of cowardice, of being the most afraid to fight of any of the \u00c6sir and Elves within the hall. Bragi responds that if they were outside the hall, he would have Loki's head, but Loki only repeats the accusation. When Bragi's wife I\u00f0unn attempts to calm Bragi, Loki accuses her of embracing her brother's slayer, a reference to matters that have not survived. It may be that Bragi had slain I\u00f0unn's brother.\nA passage in the \"Poetic Edda\" poem \"Sigrdr\u00edfum\u00e1l\" describes runes being graven on the sun, on the ear of one of the sun-horses and on the hoofs of the other, on Sleipnir's teeth, on bear's paw, on eagle's beak, on wolf's claw, and on several other things including on Bragi's tongue. Then the runes are shaved off and the shavings are mixed with mead and sent abroad so that \u00c6sir have some, Elves have some, Vanir have some, and Men have some, these being speech runes and birth runes, ale runes, and magic runes. The meaning of this is obscure.\nThe first part of Snorri Sturluson's \"Sk\u00e1ldskaparm\u00e1l\" is a dialogue between \u00c6gir and Bragi about the nature of poetry, particularly skaldic poetry. Bragi tells the origin of the mead of poetry from the blood of Kvasir and how Odin obtained this mead. He then goes on to discuss various poetic metaphors known as \"kennings\".\nSnorri Sturluson clearly distinguishes the god Bragi from the mortal skald Bragi Boddason, whom he often mentions separately. The appearance of Bragi in the \"Lokasenna\" indicates that if these two Bragis were originally the same, they have become separated for that author also, or that chronology has become very muddled and Bragi Boddason has been relocated to mythological time. Compare the appearance of the Welsh Taliesin in the second branch of the Mabinogi. Legendary chronology sometimes does become muddled. Whether Bragi the god originally arose as a deified version of Bragi Boddason was much debated in the 19th century, especially by the scholars Eugen Mogk and Sophus Bugge. The debate remains undecided.\nIn the poem \"Eir\u00edksm\u00e1l\" Odin, in Valhalla, hears the coming of the dead Norwegian king Eric Bloodaxe and his host, and bids the heroes Sigmund and Sinfj\u00f6tli rise to greet him. Bragi is then mentioned, questioning how Odin knows that it is Eric and why Odin has let such a king die. In the poem \"H\u00e1konarm\u00e1l\", H\u00e1kon the Good is taken to Valhalla by the valkyrie G\u00f6ndul and Odin sends Herm\u00f3\u00f0r and Bragi to greet him. In these poems Bragi could be either a god or a dead hero in Valhalla. Attempting to decide is further confused because \"Herm\u00f3\u00f0r\" also seems to be sometimes the name of a god and sometimes the name of a hero. That Bragi was also the first to speak to Loki in the \"Lokasenna\" as Loki attempted to enter the hall might be a parallel. It might have been useful and customary that a man of great eloquence and versed in poetry should greet those entering a hall. He is also depicted in tenth-century court poetry of helping to prepare Valhalla for new arrivals and welcoming the kings who have been slain in battle to the hall of Odin.\nSkalds named Bragi.\nBragi Boddason.\nIn the \"Prose Edda\" Snorri Sturluson quotes many stanzas attributed to Bragi Boddason the old (\"Bragi Boddason inn gamli\"), a Norwegian court poet who served several Swedish kings, Ragnar Lodbrok, \u00d6sten Beli and Bj\u00f6rn at Hauge who reigned in the first half of the 9th century. This Bragi was reckoned as the first skaldic poet, and was certainly the earliest skaldic poet then remembered by name whose verse survived in memory.\nSnorri especially quotes passages from Bragi's \"Ragnarsdr\u00e1pa\", a poem supposedly composed in honor of the famous legendary Viking Ragnar Lodbrok ('Hairy-breeches') describing the images on a decorated shield which Ragnar had given to Bragi. The images included Thor's fishing for J\u00f6rmungandr, Gefjun's ploughing of Zealand from the soil of Sweden, the attack of Hamdir and Sorli against King J\u00f6rmunrekk, and the never-ending battle between Hedin and H\u00f6gni.\nBragi son of H\u00e1lfdan the Old.\nBragi son of H\u00e1lfdan the Old is mentioned only in the \"Skj\u00e1ldskaparm\u00e1l\". This Bragi is the sixth of the second of two groups of nine sons fathered by King H\u00e1lfdan the Old on Alvig the Wise, daughter of King Eymund of H\u00f3lmgard. This second group of sons are all eponymous ancestors of legendary families of the north. Snorri says:\nBragi, from whom the Bragnings are sprung (that is the race of H\u00e1lfdan the Generous).\nOf the Bragnings as a race and of H\u00e1lfdan the Generous nothing else is known. However, \"Bragning\" is often, like some others of these dynastic names, used in poetry as a general word for 'king' or 'ruler'.\nBragi H\u00f6gnason.\nIn the eddic poem \"Helgakvi\u00f0a Hundingsbana II\", Bragi H\u00f6gnason, his brother Dag, and his sister Sigr\u00fan were children of H\u00f6gne, the king of East G\u00f6taland. The poem relates how Sigmund's son Helgi Hundingsbane agreed to take Sigr\u00fan daughter of H\u00f6gni as his wife against her unwilling betrothal to Hodbrodd son of Granmar the king of S\u00f6dermanland. In the subsequent battle of Frekastein (probably one of the 300 hill forts of S\u00f6dermanland, as \"stein\" meant \"hill fort\") against H\u00f6gni and Granmar, all the chieftains on Granmar's side are slain, including Bragi, except for Bragi's brother Dag.\nIn popular culture.\nIn the 2002 Ensemble Studios game \"Age of Mythology\", Bragi is one of nine minor gods Norse players can worship."}
{"id": "4068", "revid": "14703151", "url": "https://en.wikipedia.org/wiki?curid=4068", "title": "Blaise Pascal", "text": "Blaise Pascal (19June 162319August 1662) was a French mathematician, physicist, inventor, philosopher, and Catholic writer.\nPascal was a child prodigy who was educated by his father, a tax collector in Rouen. His earliest mathematical work was on projective geometry; he wrote a significant treatise on the subject of conic sections at the age of 16. He later corresponded with Pierre de Fermat on probability theory, strongly influencing the development of modern economics and social science. In 1642, he started some pioneering work on calculating machines (called Pascal's calculators and later Pascalines), establishing him as one of the first two inventors of the mechanical calculator.\nLike his contemporary Ren\u00e9 Descartes, Pascal was also a pioneer in the natural and applied sciences. Pascal wrote in defense of the scientific method and produced several controversial results. He made important contributions to the study of fluids, and clarified the concepts of pressure and vacuum by generalising the work of Evangelista Torricelli. Following Torricelli and Galileo Galilei, in 1647 he rebutted the likes of Aristotle and Descartes who insisted that nature abhors a vacuum.\nHe is also credited as the inventor of modern public transportation, having established the carrosses \u00e0 cinq sols, the first modern public transport service, shortly before his death in 1662.\nIn 1646, he and his sister Jacqueline identified with the religious movement within Catholicism known by its detractors as Jansenism. Following a religious experience in late 1654, he began writing influential works on philosophy and theology. His two most famous works date from this period: the and the \"Pens\u00e9es\", the former set in the conflict between Jansenists and Jesuits. The latter contains Pascal's wager, known in the original as the \"Discourse on the Machine\", a fideistic probabilistic argument for why one should believe in God. In that year, he also wrote an important treatise on the arithmetical triangle. Between 1658 and 1659, he wrote on the cycloid and its use in calculating the volume of solids. Following several years of illness, Pascal died in Paris at the age of 39.\nEarly life and education.\nPascal was born in Clermont-Ferrand, which is in France's Auvergne region, by the Massif Central. He lost his mother, Antoinette Begon, at the age of three. His father, \u00c9tienne Pascal, also an amateur mathematician, was a local judge and member of the \"Noblesse de Robe\". Pascal had two sisters, the younger Jacqueline and the elder Gilberte.\nMove to Paris.\nIn 1631, five years after the death of his wife, \u00c9tienne Pascal moved with his children to Paris. The newly arrived family soon hired Louise Delfault, a maid who eventually became a key member of the family. \u00c9tienne, who never remarried, decided that he alone would educate his children.\nThe young Pascal showed an extraordinary intellectual ability, with an amazing aptitude for mathematics and science. Etienne had tried to keep his son from learning mathematics; but by the age of 12, Pascal had rediscovered, on his own, using charcoal on a tile floor, Euclid\u2019s first thirty-two geometric propositions, and was thus given a copy of Euclid's \"Elements\".\n\"Essay on Conics\".\nParticularly of interest to Pascal was a work of Desargues on conic sections. Following Desargues' thinking, the 16-year-old Pascal produced, as a means of proof, a short treatise on what was called the \"Mystic Hexagram\", \"Essai pour les coniques\" (\"Essay on Conics\") and sent it \u2014 his first serious work of mathematics \u2014 to P\u00e8re Mersenne in Paris; it is known still today as Pascal's theorem. It states that if a hexagon is inscribed in a circle (or conic) then the three intersection points of opposite sides lie on a line (called the Pascal line).\nPascal's work was so precocious that Ren\u00e9 Descartes was convinced that Pascal's father had written it. When assured by Mersenne that it was, indeed, the product of the son and not the father, Descartes dismissed it with a sniff: \"I do not find it strange that he has offered demonstrations about conics more appropriate than those of the ancients,\" adding, \"but other matters related to this subject can be proposed that would scarcely occur to a 16-year-old child.\"\nLeaving Paris.\nIn France at that time offices and positions could be\u2014and were\u2014bought and sold. In 1631, \u00c9tienne sold his position as second president of the \"Cour des Aides\" for 65,665 livres. The money was invested in a government bond which provided, if not a lavish, then certainly a comfortable income which allowed the Pascal family to move to, and enjoy, Paris, but in 1638 Cardinal Richelieu, desperate for money to carry on the Thirty Years' War, defaulted on the government's bonds. Suddenly \u00c9tienne Pascal's worth had dropped from nearly 66,000 livres to less than 7,300.\nLike so many others, \u00c9tienne was eventually forced to flee Paris because of his opposition to the fiscal policies of Richelieu, leaving his three children in the care of his neighbour Madame Sainctot, a great beauty with an infamous past who kept one of the most glittering and intellectual salons in all France. It was only when Jacqueline performed well in a children's play with Richelieu in attendance that \u00c9tienne was pardoned. In time, \u00c9tienne was back in good graces with the Cardinal and in 1639 had been appointed the king's commissioner of taxes in the city of Rouen\u2014a city whose tax records, thanks to uprisings, were in utter chaos.\nPascaline.\nIn 1642, in an effort to ease his father's endless, exhausting calculations, and recalculations, of taxes owed and paid (into which work the young Pascal had been recruited), Pascal, not yet 19, constructed a mechanical calculator capable of addition and subtraction, called \"Pascal's calculator\" or the \"Pascaline\". Of the eight Pascalines known to have survived, four are held by the Mus\u00e9e des Arts et M\u00e9tiers in Paris and one more by the Zwinger museum in Dresden, Germany, exhibit two of his original mechanical calculators.\nAlthough these machines are pioneering forerunners to a further 400 years of development of mechanical methods of calculation, and in a sense to the later field of computer engineering, the calculator failed to be a great commercial success. Partly because it was still quite cumbersome to use in practice, but probably primarily because it was extraordinarily expensive, the Pascaline became little more than a toy, and a status symbol, for the very rich both in France and elsewhere in Europe. Pascal continued to make improvements to his design through the next decade, and he refers to some 50 machines that were built to his design. He built 20 finished machines over the following 10 years.\nMathematics.\nProbability.\nIn 1654, prompted by his friend the Chevalier de M\u00e9r\u00e9, Pascal corresponded with Pierre de Fermat on the subject of gambling problems, and from that collaboration was born the mathematical theory of probability. The specific problem was that of two players who want to finish a game early and, given the current circumstances of the game, want to divide the stakes fairly, based on the chance each has of winning the game from that point. From this discussion, the notion of expected value was introduced. John Ross writes, \"Probability theory and the discoveries following it changed the way we regard uncertainty, risk, decision-making, and an individual's and society's ability to influence the course of future events.\" Pascal, in the \"Pens\u00e9es\", used a probabilistic argument, Pascal's wager, to justify belief in God and a virtuous life. However, Pascal and Fermat, though doing important early work in probability theory, did not develop the field very far. Christiaan Huygens, learning of the subject from the correspondence of Pascal and Fermat, wrote the first book on the subject. Later figures who continued the development of the theory include Abraham de Moivre and Pierre-Simon Laplace. The work done by Fermat and Pascal into the calculus of probabilities laid important groundwork for Leibniz's formulation of the calculus.\n\"Treatise on the Arithmetical Triangle\".\nPascal's \"Trait\u00e9 du triangle arithm\u00e9tique\", written in 1654 but published posthumously in 1665, described a convenient tabular presentation for binomial coefficients which he called the arithmetical triangle, but is now called Pascal's triangle. The triangle can also be represented:\nHe defined the numbers in the triangle by recursion: Call the number in the (\"m\"\u00a0+\u00a01)th row and (\"n\"\u00a0+\u00a01)th column \"t\"\"mn\". Then \"t\"\"mn\"\u00a0=\u00a0\"t\"\"m\"\u20131,\"n\"\u00a0+\u00a0\"t\"\"m\",\"n\"\u20131, for \"m\"\u00a0=\u00a00,\u00a01,\u00a02,\u00a0... and \"n\"\u00a0=\u00a00,\u00a01,\u00a02,\u00a0... The boundary conditions are \"t\"\"m\",\u22121\u00a0=\u00a00, \"t\"\u22121,\"n\"\u00a0=\u00a00 for \"m\"\u00a0=\u00a01,\u00a02,\u00a03,\u00a0... and \"n\"\u00a0=\u00a01,\u00a02,\u00a03,\u00a0... The generator \"t\"00\u00a0=\u00a01. Pascal concluded with the proof,\nIn the same treatise, Pascal gave an explicit statement of the principle of mathematical induction. In 1654, he proved \"Pascal's identity\" relating the sums of the \"p\"-th powers of the first \"n\" positive integers for \"p\" = 0, 1, 2, ..., \"k\".\nThat same year, Pascal had a religious experience, and mostly gave up work in mathematics.\nCycloid.\nIn 1658, Pascal, while suffering from a toothache, began considering several problems concerning the cycloid. His toothache disappeared, and he took this as a heavenly sign to proceed with his research. Eight days later he had completed his essay and, to publicize the results, proposed a contest.\nPascal proposed three questions relating to the center of gravity, area and volume of the cycloid, with the winner or winners to receive prizes of 20 and 40 Spanish doubloons. Pascal, Gilles de Roberval and Pierre de Carcavi were the judges, and neither of the two submissions (by John Wallis and Antoine de Lalouv\u00e8re) were judged to be adequate. While the contest was ongoing, Christopher Wren sent Pascal a proposal for a proof of the rectification of the cycloid; Roberval claimed promptly that he had known of the proof for years. Wallis published Wren's proof (crediting Wren) in Wallis's \"Tractus Duo\", giving Wren priority for the first published proof.\nPhysics.\nPascal contributed to several fields in physics, most notably the fields of fluid mechanics and pressure. In honour of his scientific contributions, the name \"Pascal\" has been given to the SI unit of pressure and Pascal's law (an important principle of hydrostatics). He introduced a primitive form of roulette and the roulette wheel in his search for a perpetual motion machine.\nFluid dynamics.\nHis work in the fields of hydrodynamics and hydrostatics centered on the principles of hydraulic fluids. His inventions include the hydraulic press (using hydraulic pressure to multiply force) and the syringe. He proved that hydrostatic pressure depends not on the weight of the fluid but on the elevation difference. He demonstrated this principle by attaching a thin tube to a barrel full of water and filling the tube with water up to the level of the third floor of a building. This caused the barrel to leak, in what became known as Pascal's barrel experiment.\nVacuum.\nBy 1647, Pascal had learned of Evangelista Torricelli's experimentation with barometers. Having replicated an experiment that involved placing a tube filled with mercury upside down in a bowl of mercury, Pascal questioned what force kept some mercury in the tube and what filled the space above the mercury in the tube. At the time, most scientists including Descartes believed in a plenum, i. e. some invisible matter filled all of space, rather than a vacuum (\"Nature abhors a vacuum).\" This was based on the Aristotelian notion that everything in motion was a substance, moved by another substance. Furthermore, light passed through the glass tube, suggesting a substance such as aether rather than vacuum filled the space.\nFollowing more experimentation in this vein, in 1647 Pascal produced \"Experiences nouvelles touchant le vide\" (\"New experiments with the vacuum\"), which detailed basic rules describing to what degree various liquids could be supported by air pressure. It also provided reasons why it was indeed a vacuum above the column of liquid in a barometer tube. This work was followed by \"R\u00e9cit de la grande exp\u00e9rience de l'\u00e9quilibre des liqueurs\" (\"Account of the great experiment on equilibrium in liquids\") published in 1648.\nFirst atmospheric pressure vs. altitude experiment.\nThe Torricellian vacuum found that air pressure is equal to the weight of 30 inches of mercury. If air has a finite weight, Earth's atmosphere must have a maximum height. Pascal reasoned that if true, air pressure on a high mountain must be less than at a lower altitude. He lived near the Puy de D\u00f4me mountain, tall, but his health was poor so could not climb it. On 19 September 1648, after many months of Pascal's friendly but insistent prodding, Florin P\u00e9rier, husband of Pascal's elder sister Gilberte, was finally able to carry out the fact-finding mission vital to Pascal's theory. The account, written by P\u00e9rier, reads:\nPascal replicated the experiment in Paris by carrying a barometer up to the top of the bell tower at the church of Saint-Jacques-de-la-Boucherie, a height of about 50 metres. The mercury dropped two lines. He found with both experiments that an ascent of 7 fathoms lowers the mercury by half a line. Note: Pascal used \"pouce\" and \"ligne\" for \"inch\" and \"line\", and \"toise\" for \"fathom\".\nIn a reply to \u00c9tienne No\u00ebl, who believed in the plenum, Pascal wrote, echoing contemporary notions of science and falsifiability: \"In order to show that a hypothesis is evident, it does not suffice that all the phenomena follow from it; instead, if it leads to something contrary to a single one of the phenomena, that suffices to establish its falsity.\"\nBlaise Pascal Chairs are given to outstanding international scientists to conduct their research in the Ile de France region.\nAdult life: religion, literature, and philosophy.\nReligious conversion.\nIn the winter of 1646, Pascal's 58-year-old father broke his hip when he slipped and fell on an icy street of Rouen; given the man's age and the state of medicine in the 17th century, a broken hip could be a very serious condition, perhaps even fatal. Rouen was home to two of the finest doctors in France, Deslandes and de la Bouteillerie. The elder Pascal \"would not let anyone other than these men attend him...It was a good choice, for the old man survived and was able to walk again...\" However treatment and rehabilitation took three months, during which time La Bouteillerie and Deslandes had become regular visitors.\nBoth men were followers of Jean Guillebert, proponent of a splinter group from Catholic teaching known as Jansenism. This still fairly small sect was making surprising inroads into the French Catholic community at that time. It espoused rigorous Augustinism. Blaise spoke with the doctors frequently, and after their successful treatment of his father, borrowed from them works by Jansenist authors. In this period, Pascal experienced a sort of \"first conversion\" and began to write on theological subjects in the course of the following year.\nPascal fell away from this initial religious engagement and experienced a few years of what some biographers have called his \"worldly period\" (1648\u201354). His father died in 1651 and left his inheritance to Pascal and his sister Jacqueline, for whom Pascal acted as conservator. Jacqueline announced that she would soon become a postulant in the Jansenist convent of Port-Royal. Pascal was deeply affected and very sad, not because of her choice, but because of his chronic poor health; he needed her just as she had needed him.\nBy the end of October in 1651, a truce had been reached between brother and sister. In return for a healthy annual stipend, Jacqueline signed over her part of the inheritance to her brother. Gilberte had already been given her inheritance in the form of a dowry. In early January, Jacqueline left for Port-Royal. On that day, according to Gilberte concerning her brother, \"He retired very sadly to his rooms without seeing Jacqueline, who was waiting in the little parlor...\"\nIn early June 1653, after what must have seemed like endless badgering from Jacqueline,\nPascal formally signed over the whole of his sister's inheritance to Port-Royal, which, to him, \"had begun to smell like a cult.\" With two-thirds of his father's estate now gone, the 29-year-old Pascal was now consigned to genteel poverty.\nFor a while, Pascal pursued the life of a bachelor. During visits to his sister at Port-Royal in 1654, he displayed contempt for affairs of the world but was not drawn to God.\n\"Memorial\".\nOn the 23 of November, 1654, between 10:30 and 12:30 at night, Pascal had an intense religious experience and immediately wrote a brief note to himself which began: \"Fire. God of Abraham, God of Isaac, God of Jacob, not of the philosophers and the scholars...\" and concluded by quoting Psalm 119:16: \"I will not forget thy word. Amen.\" He seems to have carefully sewn this document into his coat and always transferred it when he changed clothes; a servant discovered it only by chance after his death. This piece is now known as the \"Memorial\". The story of a carriage accident as having led to the experience described in the \"Memorial\" is disputed by some scholars.\nHis belief and religious commitment revitalized, Pascal visited the older of two convents at Port-Royal for a two-week retreat in January 1655. For the next four years, he regularly travelled between Port-Royal and Paris. It was at this point immediately after his conversion when he began writing his first major literary work on religion, the \"Provincial Letters\".\nLiterature.\nIn literature, Pascal is regarded as one of the most important authors of the French Classical Period and is read today as one of the greatest masters of French prose. His use of satire and wit influenced later polemicists.\nThe \"Provincial Letters\".\nBeginning in 1656\u201357, Pascal published his memorable attack on casuistry, a popular ethical method used by Catholic thinkers in the early modern period (especially the Jesuits, and in particular Antonio Escobar). Pascal denounced casuistry as the mere use of complex reasoning to justify moral laxity and all sorts of sins. The 18-letter series was published between 1656 and 1657 under the pseudonym Louis de Montalte and incensed Louis XIV. The king ordered that the book be shredded and burnt in 1660. In 1661, in the midst of the formulary controversy, the Jansenist school at Port-Royal was condemned and closed down; those involved with the school had to sign a 1656 papal bull condemning the teachings of Jansen as heretical. The final letter from Pascal, in 1657, had defied Alexander VII himself. Even Pope Alexander, while publicly opposing them, nonetheless was persuaded by Pascal's arguments.\nAside from their religious influence, the \"Provincial Letters\" were popular as a literary work. Pascal's use of humor, mockery, and vicious satire in his arguments made the letters ripe for public consumption, and influenced the prose of later French writers like Voltaire and Jean-Jacques Rousseau.\nIt is in the \"Provincial Letters\" that Pascal made his oft-quoted apology for writing a long letter, as he had not had time to write a shorter one.\nFrom Letter XVI, as translated by Thomas M'Crie:\n'Reverend fathers, my letters were not wont either to be so prolix, or to follow so closely on\none another. Want of time must plead my excuse for both of these faults. The present letter is\na very long one, simply because I had no leisure to make it shorter.'\nCharles Perrault wrote of the \"Letters\": \"Everything is there\u2014purity of language, nobility of thought, solidity in reasoning, finesse in raillery, and throughout an \"agr\u00e9ment\" not to be found anywhere else.\"\nPhilosophy.\nPascal is arguably best known as a philosopher, considered by some the second greatest French mind behind Ren\u00e9 Descartes. He was a dualist following Descartes. However, he is also remembered for his opposition to both the rationalism of the likes of Descartes and simultaneous opposition to the main countervailing epistemology, empiricism, preferring fideism.\nIn terms of God, Descartes and Pascal disagreed. Pascal wrote that \"I cannot forgive Descartes. In all his philosophy he would have been quite willing to dispense with God, but he couldn't avoid letting him put the world in motion; afterwards he didn't need God anymore\". He opposed the rationalism of people like Descartes as applied to the existence of a God, preferring faith as \"reason can decide nothing here\". For Pascal the nature of God was such that such proofs cannot reveal God. Humans \"are in darkness and estranged from God\" because \"he has hidden Himself from their knowledge\".\nHe cared above all about the philosophy of religion. Pascalian theology has grown out of his perspective that humans are, according to Wood, \"born into a duplicitous world that shapes us into duplicitous subjects and so we find it easy to reject God continually and deceive ourselves about our own sinfulness\".\nPhilosophy of mathematics.\nPascal's major contribution to the philosophy of mathematics came with his \"De l'Esprit g\u00e9om\u00e9trique\" (\"Of the Geometrical Spirit\"), originally written as a preface to a geometry textbook for one of the famous Petites \u00e9coles de Port-Royal (\"Little Schools of Port-Royal\"). The work was unpublished until over a century after his death. Here, Pascal looked into the issue of discovering truths, arguing that the ideal of such a method would be to found all propositions on already established truths. At the same time, however, he claimed this was impossible because such established truths would require other truths to back them up\u2014first principles, therefore, cannot be reached. Based on this, Pascal argued that the procedure used in geometry was as perfect as possible, with certain principles assumed and other propositions developed from them. Nevertheless, there was no way to know the assumed principles to be true.\nPascal also used \"De l'Esprit g\u00e9om\u00e9trique\" to develop a theory of definition. He distinguished between definitions which are conventional labels defined by the writer and definitions which are within the language and understood by everyone because they naturally designate their referent. The second type would be characteristic of the philosophy of essentialism. Pascal claimed that only definitions of the first type were important to science and mathematics, arguing that those fields should adopt the philosophy of formalism as formulated by Descartes.\nIn \"De l'Art de persuader\" (\"On the Art of Persuasion\"), Pascal looked deeper into geometry's axiomatic method, specifically the question of how people come to be convinced of the axioms upon which later conclusions are based. Pascal agreed with Montaigne that achieving certainty in these axioms and conclusions through human methods is impossible. He asserted that these principles can be grasped only through intuition, and that this fact underscored the necessity for submission to God in searching out truths.\nPens\u00e9es.\nPascal's most influential theological work, referred to posthumously as the \"Pens\u00e9es\" (\"Thoughts\") is widely considered to be a masterpiece, and a landmark in \"French prose\". When commenting on one particular section (Thought #72), Sainte-Beuve praised it as the finest pages in the French language. Will Durant hailed the Pens\u00e9es as \"the most eloquent book in French prose\".\nThe \"Pens\u00e9es\" was not completed before his death. It was to have been a sustained and coherent examination and defense of the Christian faith, with the original title \"Apologie de la religion Chr\u00e9tienne\" (\"Defense of the Christian Religion\"). The first version of the numerous scraps of paper found after his death appeared in print as a book in 1669 titled \"Pens\u00e9es de M. Pascal sur la religion, et sur quelques autres sujets\" (\"Thoughts of M. Pascal on religion, and on some other subjects\") and soon thereafter became a classic.\nOne of the \"Apologie\"s main strategies was to use the contradictory philosophies of Pyrrhonism and Stoicism, personalized by Montaigne on one hand, and Epictetus on the other, in order to bring the unbeliever to such despair and confusion that he would embrace God.\nLast works and death.\nT. S. Eliot described him during this phase of his life as \"a man of the world among ascetics, and an ascetic among men of the world.\" Pascal's ascetic lifestyle derived from a belief that it was natural and necessary for a person to suffer. In 1659, Pascal fell seriously ill. During his last years, he frequently tried to reject the ministrations of his doctors, saying, \"Don't pity me, sickness is the natural state of Christians, because in it we are, as we should always be, in the suffering of evils, in the deprivation of all the goods and pleasures of the senses, free from all the passions that work throughout the course of life, without ambition, without avarice, in the continual expectation of death.\" Desiring to imitate Jesus\u2019 poverty of spirit, in his spirit of zeal and charity, Pascal said if God allowed him to recover from his illness, he would be resolved to \"have no other employment or occupation for the rest of my life than the service of the poor.\"\nLouis XIV suppressed the Jansenist movement at Port-Royal in 1661. In response, Pascal wrote one of his final works, \"\u00c9crit sur la signature du formulaire\" (\"Writ on the Signing of the Form\"), exhorting the Jansenists not to give in. Later that year, his sister Jacqueline died, which convinced Pascal to cease his polemics on Jansenism.\nInventor of public transportation.\nPascal's last major achievement, returning to his mechanical genius, was inaugurating one of the first land-based public transport services, the carrosses \u00e0 cinq sols, a network of horse-drawn multi-seat carriages that carried passengers on five fixed routes. Pascal also designated the operation principles which were later used to plan public transportation - the carriages had a fixed route, fixed price (five sols, hence the name), and left even if there were no passengers. The lines were not commercially successful, and the last one closed by 1675. Nonetheless, he has been described as the inventor of public transportation.\nIllness and death.\nIn 1662, Pascal's illness became more violent, and his emotional condition had severely worsened since his sister's death. Aware that his health was fading quickly, he sought a move to the hospital for incurable diseases, but his doctors declared that he was too unstable to be carried. In Paris on 18 August 1662, Pascal went into convulsions and received extreme unction. He died the next morning, his last words being \"May God never abandon me,\" and was buried in the cemetery of Saint-\u00c9tienne-du-Mont.\nAn autopsy performed after his death revealed grave problems with his stomach and other organs of his abdomen, along with damage to his brain. Despite the autopsy, the cause of his poor health was never precisely determined, though speculation focuses on tuberculosis, stomach cancer, or a combination of the two. The headaches which affected Pascal are generally attributed to his brain lesion.\nLegacy.\nOne of the Universities of Clermont-Ferrand, France \u2013 Universit\u00e9 Blaise Pascal \u2013 is named after him. \u00c9tablissement scolaire fran\u00e7ais Blaise-Pascal in Lubumbashi, Democratic Republic of the Congo is named after Pascal.\nThe 1969 Eric Rohmer film \"My Night at Maud's\" is based on the work of Pascal. Roberto Rossellini directed a filmed biopic, \"Blaise Pascal\", which originally aired on Italian television in 1971. Pascal was a subject of the first edition of the 1984 BBC Two documentary, \"Sea of Faith\", presented by Don Cupitt. The chameleon in the film \"Tangled\" is named for Pascal.\nA programming language is named for Pascal. In 2014, Nvidia announced its new Pascal microarchitecture, which is named for Pascal. The first graphics cards featuring Pascal were released in 2016.\nThe 2017 game \"\" has multiple characters named after famous philosophers; one of these is a sentient pacifistic machine named Pascal, who serves as a major supporting character. Pascal creates a village for machines to live peacefully with the androids they are at war with and acts as a parental figure for other machines trying to adapt to their newly-found individuality.\nThe otter in the \"Animal Crossing\" series is named for Pascal.\nThe minor planet 4500 Pascal is named in his honor.\nPope Paul VI, in encyclical \"Populorum progressio,\" issued in 1967, quotes Pascal's \"Pens\u00e9es\": \nIn 2023, Pope Francis released an apostolic letter, \"Sublimitas et miseria hominis\", dedicated to Blaise Pascal, in commemoration of the fourth centenary of his birth.\nPascal influenced French sociologist Pierre Bourdieu, who named his \"Pascalian Meditations\" (1997) after him, and French philosopher Louis Althusser."}
{"id": "4069", "revid": "43446872", "url": "https://en.wikipedia.org/wiki?curid=4069", "title": "Brittonic languages", "text": "The Brittonic languages (also Brythonic or British Celtic; ; ; and ) form one of the two branches of the Insular Celtic languages; the other is Goidelic. It comprises the extant languages Breton, Cornish, and Welsh. The name \"Brythonic\" was derived by Welsh Celticist John Rhys from the Welsh word , meaning Ancient Britons as opposed to an Anglo-Saxon or Gael.\nThe Brittonic languages derive from the Common Brittonic language, spoken throughout Great Britain during the Iron Age and Roman period. In the 5th and 6th centuries emigrating Britons also took Brittonic speech to the continent, most significantly in Brittany and Britonia. During the next few centuries, in much of Britain the language was replaced by Old English and Scottish Gaelic, with the remaining Common Brittonic language splitting into regional dialects, eventually evolving into Welsh, Cornish, Breton, Cumbric, and probably Pictish. Welsh and Breton continue to be spoken as native languages, while a revival in Cornish has led to an increase in speakers of that language. Cumbric and Pictish are extinct, having been replaced by Goidelic and Anglic speech. The Isle of Man and Orkney may also have originally spoken a Brittonic language, but this was later supplanted by Goidelic on the Isle of Man and Norse on Orkney. There is also a community of Brittonic language speakers in (the Welsh settlement in Patagonia).\nName.\nThe names \"Brittonic\" and \"Brythonic\" are scholarly conventions referring to the Celtic languages of Britain and to the ancestral language they originated from, designated Common Brittonic, in contrast to the Goidelic languages originating in Ireland. Both were created in the 19th century to avoid the ambiguity of earlier terms such as \"British\" and \"Cymric\". \"Brythonic\" was coined in 1879 by the Celticist John Rhys from the Welsh word . \"Brittonic\", derived from \"Briton\" and also earlier spelled \"Britonic\" and \"Britonnic\", emerged later in the 19th century. \"Brittonic\" became more prominent through the 20th century, and was used in Kenneth H. Jackson's highly influential 1953 work on the topic, \"Language and History in Early Britain\". Jackson noted by that time that \"Brythonic\" had become a dated term: \"of late there has been an increasing tendency to use Brittonic instead.\" Today, \"Brittonic\" often replaces \"Brythonic\" in the literature. Rudolf Thurneysen used \"Britannic\" in his influential \"A Grammar of Old Irish\", although this never became popular among subsequent scholars.\nComparable historical terms include the Medieval Latin and and the Welsh . Some writers use \"British\" for the language and its descendants, although, due to the risk of confusion, others avoid it or use it only in a restricted sense. Jackson, and later John T. Koch, use \"British\" only for the early phase of the Common Brittonic language.\nBefore Jackson's work, \"Brittonic\" and \"Brythonic\" were often used for all the P-Celtic languages, including not just the varieties in Britain but those Continental Celtic languages that similarly experienced the evolution of the Proto-Celtic language element to . However, subsequent writers have tended to follow Jackson's scheme, rendering this use obsolete.\nThe name \"Britain\" itself comes from , via Old French and Middle English , possibly influenced by Old English , probably also from Latin , ultimately an adaptation of the native word for the island, .\nAn early written reference to the British Isles may derive from the works of the Greek explorer Pytheas of Massalia; later Greek writers such as Diodorus of Sicily and Strabo who quote Pytheas' use of variants such as (), \"The Britannic [land, island]\", and (), \"Britannic islands\", with being a Celtic word that might mean 'painted ones' or 'tattooed folk', referring to body decoration.\nEvidence.\nKnowledge of the Brittonic languages comes from a variety of sources. The early language's information is obtained from coins, inscriptions, and comments by classical writers as well as place names and personal names recorded by them. For later languages, there is information from medieval writers and modern native speakers, together with place names. The names recorded in the Roman period are given in Rivet and Smith.\nCharacteristics.\nThe Brittonic branch is also referred to as \"P-Celtic\" because linguistic reconstruction of the Brittonic reflex of the Proto-Indo-European phoneme is \"p\" as opposed to Goidelic \"k\". Such nomenclature usually implies acceptance of the P-Celtic and Q-Celtic hypothesis rather than the Insular Celtic hypothesis because the term includes certain Continental Celtic languages as well.\nOther major characteristics include:\nInitial \"s-\":\nLenition:\nVoiceless spirants:\nNasal assimilation:\nClassification.\nThe family tree of the Brittonic languages is as follows:\nBrittonic languages in use today are Welsh, Cornish and Breton. Welsh and Breton have been spoken continuously since they formed. For all practical purposes Cornish died out during the 18th or 19th century, but a revival movement has more recently created small numbers of new speakers. Also notable are the extinct language Cumbric, and possibly the extinct Pictish. One view, advanced in the 1950s and based on apparently unintelligible ogham inscriptions, was that the Picts may have also used a non-Indo-European language. This view, while attracting broad popular appeal, has virtually no following in contemporary linguistic scholarship.\nHistory and origins.\nThe modern Brittonic languages are generally considered to all derive from a common ancestral language termed \"Brittonic\", \"British\", \"Common Brittonic\", \"Old Brittonic\" or \"Proto-Brittonic\", which is thought to have developed from Proto-Celtic or early Insular Celtic by the 6th century BC.\nA major archaeogenetics study uncovered a migration into southern Britain in the middle to late Bronze Age, during the 500-year period 1,300\u2013800 BC. The newcomers were genetically most similar to ancient individuals from Gaul. During 1,000\u2013875 BC, their genetic markers swiftly spread through southern Britain, but not northern Britain. The authors describe this as a \"plausible vector for the spread of early Celtic languages into Britain\". There was much less inward migration during the Iron Age, so it is likely that Celtic reached Britain before then. Barry Cunliffe suggests that a Goidelic branch of Celtic may already have been spoken in Britain, but that this middle Bronze Age migration would have introduced the Brittonic branch.\nBrittonic languages were probably spoken before the Roman invasion throughout most of Great Britain, though the Isle of Man later had a Goidelic language, Manx. During the period of the Roman occupation of what is now England and Wales (AD 43 to ), Common Brittonic borrowed a large stock of Latin words, both for concepts unfamiliar in the pre-urban society of Celtic Britain such as urbanization and new tactics of warfare, as well as for rather more mundane words which displaced native terms (most notably, the word for 'fish' in all the Brittonic languages derives from the Latin rather than the native \u2013 which may survive, however, in the Welsh name of the River Usk, ). Approximately 800 of these Latin loan-words have survived in the three modern Brittonic languages. Pictish may have resisted Latin influence to a greater extent than the other Brittonic languages.\nIt is probable that at the start of the Post-Roman period, Common Brittonic was differentiated into at least two major dialect groups \u2013 Southwestern and Western. (Additional dialects have also been posited, but have left little or no evidence, such as an Eastern Brittonic spoken in what is now the East of England.) Between the end of the Roman occupation and the mid-6th century, the two dialects began to diverge into recognizably separate varieties, the Western into Cumbric and Welsh, and the Southwestern into Cornish and its closely related sister language Breton, which was carried to continental Armorica. Jackson showed that a few of the dialect distinctions between West and Southwest Brittonic go back a long way. New divergencies began around AD 500 but other changes that were shared occurred in the 6th century. Other common changes occurred in the 7th century onward and are possibly due to inherent tendencies. Thus the concept of a Common Brittonic language ends by AD 600. Substantial numbers of Britons certainly remained in the expanding area controlled by Anglo-Saxons, but over the fifth and sixth centuries they mostly adopted the Old English language and culture.\nDecline.\nThe Brittonic languages spoken in what are now Scotland, the Isle of Man, and England began to be displaced in the 5th century through the settlement of Irish-speaking Gaels and Germanic peoples. Henry of Huntingdon wrote that Pictish was \"no longer spoken\".\nThe displacement of the languages of Brittonic descent was probably complete in all of Britain except Cornwall, Wales, and the English counties bordering these areas such as Devon, by the 11th century. Western Herefordshire continued to speak Welsh until the late nineteenth century, and isolated pockets of Shropshire speak Welsh today.\nSound changes.\nThe large array of Brittonic sound changes has been documented by Schrijver (1995), building upon Jackson (1953).\nChanges to long vowels and diphthongs.\nBrittonic has undergone an extensive remodeling of Proto-Celtic diphthongs and long vowels. All original Proto-Celtic diphthongs turned into monophthongs, albeit a number of these re-diphthongized at later stages.\nChanges to short vowels.\nThe distribution of Proto-Celtic short vowels were reshuffled by various processes in Brittonic, such as the two i-affections, a-affection, raisings, and contact with lenited consonants like \"*g\" &gt; and \"*s\" &gt; \"*h\".\nThe default outcomes of stressed short vowels in Brittonic are as follows:\nRaisings of \"*e\" and \"*o\".\nWelsh exhibits raisings of \"*e\" to \"*i\" &gt; ' &gt; ' and \"*o\" &gt; before a nasal followed by a stop.\nIt is difficult to determine whether the raising from \"*o\" to \"*u\" also affected Cornish and Breton, since both of those languages generally merge \"*o\" with \"*u\".\nThe raising of \"*e\" to \"*i\" occurred in all three major Brittonic languages:\nOther raising environments identified by Schrijver include:\nThis raising preceded a-affection, since a-affection reverses this raising whenever it applied.\nAll these raisings not only affected native vocabulary, but also affected Latin loanwords.\nInteractions of vowels followed by \"*g\".\nMultiple special interactions of vowels occurred when followed by \"*g\". \nAssimilation of \"*oRa\" to \"*aRa\".\nClosely paralleling the common Celtic change of \"*eRa\" &gt; \"*aRa\" (Joseph's rule) is the change of \"*oRa\" to \"*aRa\" in Brittonic, with \"R\" standing for any lone sonorant. Unlike Joseph's rule, \"*oRa\" to \"*aRa\" did not occur in Goidelic. Schrijver demonstrates this rule with the following examples:\nAssuming that Welsh \"manach\" (borrowed from Latin \"monachus\" \"monk\") also underwent this assimilation, Schrijver concludes that this change must predate the raising of vowels in \"*mVn-\" sequences, which in turn predates a-affection (an early fifth-century process).\n/je/ &gt; /ja/.\nIn Brittonic, Celtic \"*ye\" generally became /ja/. Some examples cited by Schrijver include:\n\"*wo\".\nThe sequence \"*wo\" was quite volatile in Brittonic. It originally manifested as \"*wo\" in unlenited position and \"*wa\" in lenited position. Word-initially, this allomorphy was gone in medieval times, leveled out in various ways. Whichever of \"*o\" or \"*a\" to be generalized in the reflexes of a word in a given Brittonic language is completely unpredictable, and occasionally both \"o\" and \"a\" reflexes have been attested within the same language. Southwest Brittonic languages like Breton and Cornish usually generalize the same variant of \"*wo\" in a given word while Welsh tends to have its own distribution of variants.\nThe distribution of \"*wo/wa\" is also complicated by an Old Breton development where \"*wo\" that had not turned to \"*gwa\" would split into \"go(u)-\" (Old Breton \"gu-\") in penultimate post-apocope syllables and \"go-\" in monosyllables.\nDevelopments of \"*ub\".\nThe sequence \"*ub\" &gt; \"*u\u03b2\" remained as such when followed by a consonant, for instance in Proto-Celtic \"*dubros\" \"water\" &gt; \"*du\u03b2r\" &gt; Welsh \"dwfr\", \"d\u0175r\" and Breton \"dour\".\nHowever, if no consonant exists after a \"*ub\" sequence, the \"*u\" merges with whatever Proto-Celtic \"*ou\" and \"*oi\" became, the result of which is written in the Brittonic languages. The lenited \"*b\" &gt; \"*\u03b2\" is lost word-finally after this happens.\nSchrijver dates this development between the 6th to 8th centuries, with subsequent loss of \"*\u03b2\" datable to the 9th century.\na-affection.\nIn Brittonic, final a-affection was triggered by final-syllable \"*\u0101\" or \"*a\", which was later apocopated. This process lowered \"*i\" and \"*u\" in the preceding syllable to \"*e\" and \"*o\", respectively.\nA-affection, by affecting feminine forms of adjectives and not their masculine counterparts, created root vowel alternations by gender such as \"*windos\", feminine \"*wind\u0101\" &gt; \"*gw\u026ann\", feminine \"*gwenn\" &gt; Welsh \"gwyn\", feminine \"gwen\".\ni-affection.\nThere were two separate processes of i-affection in Brittonic: final i-affection and internal i-affection. Both processes caused the fronting of vowels.\nSimplified summary of consonantal outcomes.\nThe regular consonantal sound changes from Proto-Celtic to Welsh, Cornish, and Breton are summarised in the following table. Where the graphemes have a different value from the corresponding IPA symbols, the IPA equivalent is indicated between slashes. V represents a vowel; C represents a consonant.\nRemnants in England, Scotland and Ireland.\nPlace names and river names.\nThe principal legacy left behind in those territories from which the Brittonic languages were displaced is that of toponyms (place names) and hydronyms (names of rivers and other bodies of water). There are many Brittonic place names in lowland Scotland and in the parts of England where it is agreed that substantial Brittonic speakers remained (Brittonic names, apart from those of the former Romano-British towns, are scarce over most of England). Names derived (sometimes indirectly) from Brittonic include London, Penicuik, Perth, Aberdeen, York, Dorchester, Dover, and Colchester. Brittonic elements found in England include and for 'hill', while some such as \"co[o]mb[e]\" (from ) for 'small deep valley' and \"tor\" for 'hill, rocky headland' are examples of Brittonic words that were borrowed into English. Others reflect the presence of Britons such as Dumbarton \u2013 from the Scottish Gaelic meaning 'Fort of the Britons', and Walton meaning (in Anglo-Saxon) a 'settlement' where the 'Britons' still lived.\nThe number of Celtic river names in England generally increases from east to west, a map showing these being given by Jackson. These include Avon, Chew, Frome, Axe, Brue and Exe, but also river names containing the elements \"der-/dar-/dur-\" and \"-went\" e.g. Derwent, Darwen, Deer, Adur, Dour, Darent, and Went. These names exhibit multiple different Celtic roots. One is * 'water' (Breton , Cumbric , Welsh ), also found in the place-name Dover (attested in the Roman period as ); this is the source of rivers named Dour. Another is 'oak' or 'true' (Bret. , Cumb. , W. ), coupled with two agent suffixes, and ; this is the origin of Derwent, Darent, and Darwen (attested in the Roman period as ). The final root to be examined is . In Roman Britain, there were three tribal capitals named (modern Winchester, Caerwent, and Caistor St Edmunds), whose meaning was 'place, town'.\nBrittonicisms in English.\nSome, including J. R. R. Tolkien, have argued that Celtic has acted as a substrate to English for both the lexicon and syntax. It is generally accepted that Brittonic effects on English are lexically few, aside from toponyms, consisting of a small number of domestic and geographical words, which \"may\" include \"bin\", \"brock\", \"carr\", \"comb\", \"crag\" and \"tor\". Another legacy may be the sheep-counting system \"yan tan tethera\" in the north, in the traditionally Celtic areas of England such as Cumbria. Several words of Cornish origin are still in use in English as mining-related terms, including costean, gunnies, and vug.\nThose who argue against the theory of a more significant Brittonic influence than is widely accepted point out that many toponyms have no semantic continuation from the Brittonic language. A notable example is \"Avon\" which comes from the Celtic term for river or the Welsh term for river, , but was used by the English as a personal name. Likewise the River Ouse, Yorkshire, contains the Celtic word which merely means 'water' and the name of the river Trent simply comes from the Welsh word for a 'trespasser' (figuratively suggesting 'overflowing river').\nScholars supporting a Brittonic substrate in English argue that the use of periphrastic constructions (using auxiliary verbs such as \"do\" and \"be\" in the continuous/progressive) of the English verb, which is more widespread than in the other Germanic languages, is traceable to Brittonic influence. Others, however, find this unlikely since many of these forms are only attested in the later Middle English period; these scholars claim a native English development rather than Celtic influence. Ian G. Roberts postulates Northern Germanic influence, despite such constructions not existing in Norse. Literary Welsh has the simple present = 'I love' and the present stative (al. continuous/progressive) = 'I am loving', where the Brittonic syntax is partly mirrored in English. (However, English \"I am loving\" comes from older \"I am a-loving\", from still older 'I am in the process of loving'). In the Germanic sister languages of English, there is only one form, for example in German, though in \"colloquial\" usage in some German dialects, a progressive aspect form has evolved which is formally similar to those found in Celtic languages, and somewhat less similar to the Modern English form, e.g. 'I am working' is , literally: 'I am on the working'. The same structure is also found in modern Dutch (), alongside other structures (e.g. , lit. 'I sit to working'). These parallel developments suggest that the English progressive is not necessarily due to Celtic influence; moreover, the native English development of the structure can be traced over 1000 years and more of English literature.\nSome researchers (Filppula, et al., 2001) argue that other elements of English syntax reflect Brittonic influences. For instance, in English tag questions, the form of the tag depends on the verb form in the main statement (\"aren't I?\", \"isn't he?\", \"won't we?\", etc.). The German and the French , by contrast, are fixed forms which can be used with almost any main statement. It has been claimed that the English system has been borrowed from Brittonic, since Welsh tag questions vary in almost exactly the same way.\nBrittonic effect on the Goidelic languages.\nFar more notable, but less well known, are Brittonic influences on Scottish Gaelic, though Scottish and Irish Gaelic, with their wider range of preposition-based periphrastic constructions, suggest that such constructions descend from their common Celtic heritage. Scottish Gaelic contains several P-Celtic loanwords, but, as there is a far greater overlap in terms of Celtic vocabulary than with English, it is not always possible to disentangle P- and Q-Celtic words. However, some common words such as = Welsh , Cumbric are particularly evident.\nThe Brittonic influence on Scots Gaelic is often indicated by considering Irish language usage, which is not likely to have been influenced so much by Brittonic. In particular, the word (anglicised as \"strath\") is a native Goidelic word, but its usage appears to have been modified by the Welsh cognate whose meaning is slightly different. The effect on Irish has been the loan from British of many Latin-derived words. This has been associated with the Christianisation of Ireland from Britain."}
{"id": "4071", "revid": "1272633671", "url": "https://en.wikipedia.org/wiki?curid=4071", "title": "Bronski Beat", "text": "Bronski Beat were a British synth-pop band formed in 1983 in London, England. The initial lineup, which recorded the majority of their hits, consisted of Scottish musicians Jimmy Somerville (vocals) and Steve Bronski (keyboards, percussion) and English musician Larry Steinbachek (keyboards, percussion). Simon Davolls contributed backing vocals to many songs. Throughout the band's career, Bronski was the only member to appear in every lineup. \nBronski Beat achieved success in the mid-1980s, particularly with the 1984 single \"Smalltown Boy\", from their debut album, \"The Age of Consent\". \"Smalltown Boy\" was their only US \"Billboard\" Hot 100 single. All members of the band were openly gay and their songs reflected this, often containing political commentary on gay issues. Somerville left Bronski Beat in 1985 and went on to have success as lead singer of the Communards and as a solo artist. He was replaced by vocalist John Foster, with whom the band continued to have hits in the UK and Europe through 1986. Foster left Bronski Beat after their second album, and the band were joined by Jonathan Hellyer before dissolving in 1995.\nSteve Bronski revived the band in 2016, recording new material with 1990s member Ian Donaldson. Steinbachek died later that year; Bronski died in 2021. As of 2025, Somerville is the last surviving member of the band's original lineup and one of six surviving members of the band. \nHistory.\n1983\u20131985: early years and \"The Age of Consent\".\nBronski Beat formed in 1983 when Jimmy Somerville, Steve Bronski (both from Glasgow) and Larry Steinbachek (from Southend, Essex) shared a three-bedroom flat at Lancaster House in Brixton, London. Steinbachek had heard Somerville singing during the making of \"\" and suggested they make some music. They first performed publicly at an arts festival, \"September in the Pink\". The trio were unhappy with the inoffensive nature of contemporary gay performers and sought to be more outspoken and political.\nBronski Beat signed a recording contract with London Records in 1984 after doing only nine live gigs. The band's debut single, \"Smalltown Boy\", about a gay teenager leaving his family and fleeing his home town, was a hit, peaking at No 3 in the UK Singles Chart, and topping charts in Belgium and the Netherlands. The single was accompanied by a promotional video directed by Bernard Rose, showing Somerville trying to befriend an attractive diver at a swimming pool, then being attacked by the diver's homophobic associates, being returned to his family by the police and having to leave home. (The police officer was played by Colin Bell, then the marketing manager of London Records.) \"Smalltown Boy\" reached 48 in the U.S. chart and peaked at 8 in Australia.\nThe follow-up single, \"Why?\", adopted a hi-NRG sound and was more lyrically focused on anti-gay prejudice. It also achieved Top 10 status in the UK, reaching 6, and was another Top 10 hit for the band in Australia, Switzerland, Germany, France and the Netherlands.\nAt the end of 1984, the trio released an album titled \"The Age of Consent\". The inner sleeve listed the varying ages of consent for consensual gay sex in different nations around the world. At the time, the age of consent for sexual acts between men in the UK was 21 compared with 16 for heterosexual acts, with several other countries having more liberal laws on gay sex. The album peaked at 4 in the UK Albums Chart, 36 in the U.S., and 12 in Australia.\nAround the same time, the band headlined \"Pits and Perverts\", a concert at the Electric Ballroom in London to raise funds for the Lesbians and Gays Support the Miners campaign. This event is featured in the film \"Pride\".\nThe third single, released before Christmas 1984, was a revival of \"It Ain't Necessarily So\", the George and Ira Gershwin classic (from \"Porgy and Bess\"). The song questions the accuracy of biblical tales. It also reached the UK Top 20.\nIn 1985, the trio joined up with Marc Almond to record a version of Donna Summer's \"I Feel Love\". The full version was actually a medley that also incorporated snippets of Summer's \"Love to Love You Baby\" and John Leyton's \"Johnny Remember Me\". It was a big success, reaching 3 in the UK and equalling the chart achievement of \"Smalltown Boy\". Although the original had been one of Marc Almond's all-time favourite songs, he had never read the lyrics and thus incorrectly sang \"What\u2019ll it be, what\u2019ll it be, you and me\" instead of \"Falling free, falling free, falling free\" on the finished record.\nThe band and their producer Mike Thorne had gone back into the studio in early 1985 to record a new single, \"Run from Love\", and PolyGram (London Records' parent company at that time) had pressed a number of promo singles and 12\" versions of the song and sent them to radio and record stores in the UK. However, the single was shelved as tensions in the band, both personal and political, resulted in Somerville leaving Bronski Beat in the summer of that year.\n\"Run from Love\" was subsequently released in remix form on the Bronski Beat album \"Hundreds &amp; Thousands\", a collection of mostly remixes (LP) and B-sides (as bonus tracks on the CD version) as well as the hit \"I Feel Love\". Somerville went on to form the Communards with Richard Coles while the remaining members of Bronski Beat searched for a new vocalist.\n1985\u20131995: Somerville's departure, John Foster and Jonathan Hellyer eras.\nBronski Beat recruited John Foster as Somerville's replacement (Foster is credited as \"Jon Jon\"). A single, \"Hit That Perfect Beat\", was released in November 1985, reaching 3 in the UK. It repeated this success on the Australian chart and was also featured in the film \"Letter to Brezhnev\". A second single, \"C'mon C'mon\", also charted in the UK Top 20 and an album, \"Truthdare Doubledare\", released in May 1986, peaked at 18. The film \"Parting Glances\" (1986) included Bronski Beat songs \"Love and Money\", \"Smalltown Boy\" and \"Why?\" During this period, the band teamed up with producer Mark Cunningham on the first-ever BBC Children In Need single, a cover of David Bowie's \"Heroes\", released in 1986 under the name of The County Line.\nFoster left the band in 1987. Following Foster's departure, Bronski Beat began work on their next album, \"Out and About\". The tracks were recorded at Berry Street studios in London with engineer Brian Pugsley. Some of the song titles were \"The Final Spin\" and \"Peace and Love\". The latter track featured Strawberry Switchblade vocalist Rose McDowall and appeared on several internet sites in 2006. One of the other songs from the project called \"European Boy\" was recorded in 1987 by disco group Splash. The lead singer of Splash was former Tight Fit singer Steve Grant. Steinbachek and Bronski toured extensively with the new material with positive reviews, however the project was abandoned as the group was dropped by London Records. Also in 1987, Bronski Beat and Somerville performed at a reunion concert for \"International AIDS Day\", supported by New Order, at the Brixton Academy, London.\nIn 1989, Jonathan Hellyer became lead singer, and the band extensively toured the U.S. and Europe with back-up vocalist Annie Conway. They achieved one minor hit with the song \"Cha Cha Heels\", a one-off collaboration sung by American actress and singer Eartha Kitt, which peaked at 32 in the UK. The song was originally written for movie and recording star Divine, who was unable to record the song before his death in 1988. 1990\u201391 saw Bronski Beat release three further singles on the Zomba record label, \"I'm Gonna Run Away\", \"One More Chance\" and \"What More Can I Say\". The singles were produced by Mike Thorne.\nFoster and Bronski Beat teamed up again in 1994, and released a techno \"Tell Me Why '94\" and an acoustic \"Smalltown Boy '94\" on the German record label, ZYX Music. The album \"Rainbow Nation\" was released the following year with Hellyer returning as lead vocalist, as Foster had dropped out of the project and Ian Donaldson was brought on board to do keyboards and programming. After a few years of touring, Bronski Beat then dissolved, with Steve Bronski going on to become a producer for other artists and Ian Donaldson becoming a successful DJ (Sordid Soundz). Larry Steinbachek became the musical director for Michael Laub's theatre company, 'Remote Control Productions'.\n2007\u20132016: Bronski solo activities and resurrection of Bronski Beat.\nIn 2007, Steve Bronski remixed the song \"Stranger to None\" by the UK alternative rock band, All Living Fear. Four different mixes were made, with one appearing on their retrospective album, \"Fifteen Years After\". Bronski also remixed the track \"Flowers in the Morning\" by Northern Irish electronic band Electrobronze in 2007, changing the style of the song from classical to Hi-NRG disco.\nIn 2015, Steve Bronski teamed up as a one-off with Jessica James (aka Barbara Bush) and said that she reminded him of Divine, because of her look and Eartha Kitt-like sound. The one-off project was to cover the track he made in 1989.\nIn 2016, Steve Bronski again teamed up with Ian Donaldson, with the aim of bringing Bronski Beat back, enlisting a new singer, Stephen Granville. In 2017, the new Bronski Beat released a reworked version of \"Age of Consent\" entitled \"Age of Reason\". \"Out &amp; About\", the unreleased Bronski Beat album from 1987, was released digitally via Steve Bronski's website. The album features the original tracks plus remixes by Bronski.\n2017\u2013present: deaths of Steinbachek and Bronski.\nOn 12 January 2017, it was revealed that Steinbachek had died the previous month after a short battle with cancer, with his family and friends at his bedside. He was 56. Bronski died on 7 December 2021, at the age of 61, in a Central London flat fire.\nAwards and nominations.\n! Year !! Awards !! Work !! Category !! Result !! Ref."}
{"id": "4074", "revid": "35358178", "url": "https://en.wikipedia.org/wiki?curid=4074", "title": "Barrel (disambiguation)", "text": "A barrel is a cylindrical container, traditionally made with wooden material.\nBarrel may also refer to:"}
{"id": "4077", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=4077", "title": "Binary prefix", "text": "A binary prefix is a unit prefix that indicates a multiple of a unit of measurement by an integer power of two. The most commonly used binary prefixes are kibi (symbol Ki, meaning ), mebi (), and gibi (). They are most often used in information technology as multipliers of bit and byte, when expressing the capacity of storage devices or the size of computer files.\nThe binary prefixes \"kibi\", \"mebi\", etc. were defined in 1999 by the International Electrotechnical Commission (IEC), in the IEC 60027-2 standard (Amendment 2). They were meant to replace the metric (SI) decimal power prefixes, such as \"kilo\" (), \"mega\" () and \"giga\" (), that were commonly used in the computer industry to indicate the nearest powers of two. For example, a memory module whose capacity was specified by the manufacturer as \"2\u00a0megabytes\" or \"2\u00a0MB\" would hold = , instead of = .\nOn the other hand, a hard disk whose capacity is specified by the manufacturer as \"10 gigabytes\" or \"10\u00a0GB\", holds = bytes, or a little more than that, but less than = and a file whose size is listed as \"2.3\u00a0GB\" may have a size closer to \u2248 or to = , depending on the program or operating system providing that measurement. This kind of ambiguity is often confusing to computer system users and has resulted in lawsuits. The IEC 60027-2 binary prefixes have been incorporated in the standard and are supported by other standards bodies, including the BIPM, which defines the SI system, the US NIST, and the European Union.\nPrior to the 1999 IEC standard, some industry organizations, such as the Joint Electron Device Engineering Council (JEDEC), attempted to redefine the terms \"kilobyte\", \"megabyte\", and \"gigabyte\", and the corresponding symbols \"KB\", \"MB\", and \"GB\" in the binary sense, for use in storage capacity measurements. However, other computer industry sectors (such as magnetic storage) continued using those same terms and symbols with the decimal meaning. Since then, the major standards organizations have expressly disapproved the use of SI prefixes to denote binary multiples, and recommended or mandated the use of the IEC prefixes for that purpose, but the use of SI prefixes in this sense has persisted in some fields.\nDefinitions.\nIn 2022, the International Bureau of Weights and Measures (BIPM) adopted the decimal prefixes \"ronna\" for 10009 and \"quetta\" for 100010. In analogy to the existing binary prefixes, a consultation paper of the International Committee for Weights and Measures' Consultative Committee for Units (CCU) suggested the prefixes \"robi\" () and \"quebi\" () for their binary counterparts, but , no corresponding binary prefixes have been adopted.\nComparison of binary and decimal prefixes.\nThe relative difference between the values in the binary and decimal interpretations increases, when using the SI prefixes as the base, from 2.4% for kilo to nearly 27% for the quetta prefix. Although the prefixes ronna and quetta have been defined, as of 2022 no names have been officially assigned to the corresponding binary prefixes.\nHistory.\nEarly prefixes.\nThe original metric system adopted by France in 1795 included two binary prefixes named \"double-\" (2\u00d7) and \"demi-\" (\u00d7). However, these were not retained when the SI prefixes were internationally adopted by the 11th CGPM conference in 1960.\nStorage capacity.\nMain memory.\nEarly computers used one of two addressing methods to access the system memory; binary (base 2) or decimal (base 10). For example, the IBM 701 (1952) used a binary methods and could address 2048 words of 36 bits each, while the IBM 702 (1953) used a decimal system, and could address ten thousand 7-bit words.\nBy the mid-1960s, binary addressing had become the standard architecture in most computer designs, and main memory sizes were most commonly powers of two. This is the most natural configuration for memory, as all combinations of states of their address lines map to a valid address, allowing easy aggregation into a larger block of memory with contiguous addresses.\nWhile early documentation specified those memory sizes as exact numbers such as 4096, 8192, or units (usually words, bytes, or bits), computer professionals also started using the long-established metric system prefixes \"kilo\", \"mega\", \"giga\", etc., defined to be powers of 10, to mean instead the nearest powers of two; namely, 210 = 1024, 220 = 10242, 230 = 10243, etc. The corresponding metric prefix symbols (\"k\", \"M\", \"G\", etc.) were used with the same binary meanings. The symbol for 210 = 1024 could be written either in lower case (\"k\") or in uppercase (\"K\"). The latter was often used intentionally to indicate the binary rather than decimal meaning. This convention, which could not be extended to higher powers, was widely used in the documentation of the IBM\u00a0360 (1964) and of the IBM System/370 (1972), of the CDC\u00a07600, of the DEC PDP-11/70 (1975) and of the DEC VAX-11/780 (1977).\nIn other documents, however, the metric prefixes and their symbols were used to denote powers of 10, but usually with the understanding that the values given were approximate, often truncated down. Thus, for example, a 1967 document by Control Data Corporation (CDC) abbreviated \"216 = = words\" as \"65K words\" (rather than \"64K\" or \"66K\"), while the documentation of the HP 21MX real-time computer (1974) denoted = = as \"196K\" and 220 = as \"1M\".\nThese three possible meanings of \"k\" and \"K\" (\"1024\", \"1000\", or \"approximately 1000\") were used loosely around the same time, sometimes by the same company. The HP 3000 business computer (1973) could have \"64K\", \"96K\", or \"128K\" bytes of memory. The use of SI prefixes, and the use of \"K\" instead of \"k\" remained popular in computer-related publications well into the 21st century, although the ambiguity persisted. The correct meaning was often clear from the context; for instance, in a binary-addressed computer, the true memory size had to be either a power of 2, or a small integer multiple thereof. Thus a \"512 megabyte\" RAM module was generally understood to have = bytes, rather than .\nHard disks.\nIn specifying disk drive capacities, manufacturers have always used conventional decimal SI prefixes representing powers of 10. Storage in a rotating disk drive is organized in platters and tracks whose sizes and counts are determined by mechanical engineering constraints so that the capacity of a disk drive has hardly ever been a simple multiple of a power of 2. For example, the first commercially sold disk drive, the IBM 350 (1956), had 50 physical disk platters containing a total of sectors of 100 characters each, for a total quoted capacity of 5 million characters.\nMoreover, since the 1960s, many disk drives used IBM's disk format, where each track was divided into blocks of user-specified size; and the block sizes were recorded on the disk, subtracting from the usable capacity. For example, the IBM 3336 disk pack was quoted to have a 200-megabyte capacity, achieved only with a single -byte block in each of its 808 \u00d7 19 tracks.\nDecimal megabytes were used for disk capacity by the CDC in 1974. The Seagate ST-412, one of several types installed in the IBM PC/XT, had a capacity of when formatted as 306 \u00d7 4 tracks and 32 256-byte sectors per track, which was quoted as \"10\u00a0MB\". Similarly, a \"300\u00a0GB\" hard drive can be expected to offer only slightly more than = , bytes, not (which would be about bytes or \"322\u00a0GB\"). The first terabyte (SI prefix, bytes) hard disk drive was introduced in 2007. Decimal prefixes were generally used by information processing publications when comparing hard disk capacities.\nSome programs and operating systems, such as Microsoft Windows, still use \"MB\" and \"GB\" to denote binary prefixes even when displaying disk drive capacities and file sizes, as did Classic Mac OS. Thus, for example, the capacity of a \"10 MB\" (decimal \"M\") disk drive could be reported as \"\", and that of a \"300\u00a0GB\" drive as \"279.4\u00a0GB\". Some operating systems, such as Mac OS X, Ubuntu, and Debian, have been updated to use \"MB\" and \"GB\" to denote decimal prefixes when displaying disk drive capacities and file sizes. Some manufacturers, such as Seagate Technology, have released recommendations stating that properly-written software and documentation should specify clearly whether prefixes such as \"K\", \"M\", or \"G\" mean binary or decimal multipliers.\nFloppy disks.\nFloppy disks used a variety of formats, and their capacities was usually specified with SI-like prefixes \"K\" and \"M\" with either decimal or binary meaning. The capacity of the disks was often specified without accounting for the internal formatting overhead, leading to more irregularities.\nThe early 8-inch diskette formats could contain less than a megabyte with the capacities of those devices specified in kilobytes, kilobits or megabits.\nThe 5.25-inch diskette sold with the IBM PC AT could hold = bytes, and thus was marketed as \"1200\u00a0KB\" with the binary sense of \"KB\". However, the capacity was also quoted \"1.2\u00a0MB\", which was a hybrid decimal and binary notation, since the \"M\" meant 1000 \u00d7 1024. The precise value was (decimal) or (binary).\nThe 5.25-inch Apple Disk II had 256 bytes per sector, 13 sectors per track, 35 tracks per side, or a total capacity of bytes. It was later upgraded to 16 sectors per track, giving a total of = bytes, which was described as \"140KB\" using the binary sense of \"K\".\nThe most recent version of the physical hardware, the \"3.5-inch diskette\" cartridge, had 720 512-byte blocks (single-sided). Since two blocks comprised 1024 bytes, the capacity was quoted \"360\u00a0KB\", with the binary sense of \"K\". On the other hand, the quoted capacity of \"1.44\u00a0MB\" of the High Density (\"HD\") version was again a hybrid decimal and binary notation, since it meant 1440 pairs of 512-byte sectors, or = . Some operating systems displayed the capacity of those disks using the binary sense of \"MB\", as \"1.4\u00a0MB\" (which would be \u2248 ). User complaints forced both Apple and Microsoft to issue support bulletins explaining the discrepancy.\nOptical disks.\nWhen specifying the capacities of optical compact discs, \"megabyte\" and \"MB\" usually meant 10242\u00a0bytes. Thus a \"700-MB\" (or \"80-minute\") CD has a nominal capacity of about , which is approximately (decimal).\nOn the other hand, capacities of other optical disc storage media like DVD, Blu-ray Disc, HD DVD and magneto-optical (MO) have been generally specified in decimal gigabytes (\"GB\"), that is, 10003 bytes. In particular, a typical \"\" DVD has a nominal capacity of about , which is about .\nTape drives and media.\nTape drive and media manufacturers have generally used SI decimal prefixes to specify the maximum capacity, although the actual capacity would depend on the block size used when recording.\nData and clock rates.\nComputer clock frequencies are always quoted using SI prefixes in their decimal sense. For example, the internal clock frequency of the original IBM PC was , that is .\nSimilarly, digital information transfer rates are quoted using decimal prefixe. The Parallel ATA \" disk interface can transfer bytes per second, and a \" modem transmits bits per second. Seagate specified the sustained transfer rate of some hard disk drive models with both decimal and IEC binary prefixes. \nThe standard sampling rate of music compact disks, quoted as , is indeed samples per second. A \"\" Ethernet interface can receive or transmit up to 109 bits per second, or bytes per second within each packet. A \"56k\" modem can encode or decode up to bits per second.\nDecimal SI prefixes are also generally used for processor-memory data transfer speeds. A PCI-X bus with clock and 64 bits wide can transfer 64-bit words per second, or = , which is usually quoted as . A PC3200 memory on a double data rate bus, transferring 8 bytes per cycle with a clock speed of has a bandwidth of = , which would be quoted as .\nAmbiguous standards.\nThe ambiguous usage of the prefixes \"kilo (\"K\" or \"k\"), \"mega\" (\"M\"), and \"giga\" (\"G\"), as meaning both powers of 1000 or (in computer contexts) of 1024, has been recorded in popular dictionaries, and even in some obsolete standards, such as ANSI/IEEE\u00a01084-1986 and ANSI/IEEE\u00a01212-1991, IEEE\u00a0610.10-1994, and IEEE\u00a0100-2000. Some of these standards specifically limited the binary meaning to multiples of \"byte\" (\"B\") or \"bit\" (\"b\").\nEarly binary prefix proposals.\nBefore the IEC standard, several alternative proposals existed for unique binary prefixes, starting in the late 1960s. In 1996, Markus Kuhn proposed the extra prefix \"di\" and the symbol suffix or subscript \"2\" to mean \"binary\"; so that, for example, \"one dikilobyte\" would mean \"1024 bytes\", denoted \"K2B\" or \"K2B\".\nIn 1968, Donald Morrison proposed to use the Greek letter kappa (\u03ba) to denote 1024, \u03ba2 to denote 10242, and so on. (At the time, memory size was small, and only K was in widespread use.) In the same year, Wallace Givens responded with a suggestion to use bK as an abbreviation for 1024 and bK2 or bK2 for 10242, though he noted that neither the Greek letter nor lowercase letter b would be easy to reproduce on computer printers of the day. Bruce Alan Martin of Brookhaven National Laboratory proposed that, instead of prefixes, binary powers of two were indicated by the letter B followed by the exponent, similar to E in decimal scientific notation. Thus one would write 3B20 for . This convention is still used on some calculators to present binary floating point-numbers today.\nIn 1969, Donald Knuth, who uses decimal notation like 1\u00a0MB = 1000\u00a0kB, proposed that the powers of 1024 be designated as \"large kilobytes\" and \"large megabytes\", with abbreviations KKB and MMB.\nConsumer confusion.\nThe ambiguous meanings of \"kilo\", \"mega\", \"giga\", etc., has caused significant consumer confusion, especially in the personal computer era. A common source of confusion was the discrepancy between the capacities of hard drives specified by manufacturers, using those prefixes in the decimal sense, and the numbers reported by operating systems and other software, that used them in the binary sense, such as the Apple Macintosh in 1984. For example, a hard drive marketed as \"1\u00a0TB\" could be reported as having only \"931\u00a0GB\". The confusion was compounded by fact that RAM manufacturers used the binary sense too.\nLegal disputes.\nThe different interpretations of disk size prefixes led to class action lawsuits against digital storage manufacturers. These cases involved both flash memory and hard disk drives.\nEarly cases.\nEarly cases (2004\u20132007) were settled prior to any court ruling with the manufacturers admitting no wrongdoing but agreeing to clarify the storage capacity of their products on the consumer packaging. Accordingly, many flash memory and hard disk manufacturers have disclosures on their packaging and web sites clarifying the formatted capacity of the devices or defining MB as 1\u00a0million bytes and 1\u00a0GB as 1\u00a0billion bytes.\n\"Willem Vroegh v. Eastman Kodak Company\".\nOn 20 February 2004, Willem Vroegh filed a lawsuit against Lexar Media, Dane\u2013Elec Memory, Fuji Photo Film USA, Eastman Kodak Company, Kingston Technology Company, Inc., Memorex Products, Inc.; PNY Technologies Inc., SanDisk Corporation, Verbatim Corporation, and Viking Interworks alleging that their descriptions of the capacity of their flash memory cards were false and misleading.\nVroegh claimed that a 256\u00a0MB Flash Memory Device had only 244\u00a0MB of accessible memory. \"Plaintiffs allege that Defendants marketed the memory capacity of their products by assuming that one megabyte equals one million bytes and one gigabyte equals one billion bytes.\" The plaintiffs wanted the defendants to use the customary values of 10242 for megabyte and 10243 for gigabyte. The plaintiffs acknowledged that the IEC and IEEE standards define a MB as one million bytes but stated that the industry has largely ignored the IEC standards.\nThe parties agreed that manufacturers could continue to use the decimal definition so long as the definition was added to the packaging and web sites. The consumers could apply for \"a discount of ten percent off a future online purchase from Defendants' Online Stores Flash Memory Device\".\n\"Orin Safier v. Western Digital Corporation\".\nOn 7 July 2005, an action entitled \"Orin Safier v. Western Digital Corporation, et al.\" was filed in the Superior Court for the City and County of San Francisco, Case No. CGC-05-442812. The case was subsequently moved to the Northern District of California, Case No. 05-03353 BZ.\nAlthough Western Digital maintained that their usage of units is consistent with \"the indisputably correct industry standard for measuring and describing storage capacity\", and that they \"cannot be expected to reform the software industry\", they agreed to settle in March 2006 with 14 June 2006 as the Final Approval hearing date.\nWestern Digital offered to compensate customers with a gratis download of backup and recovery software that they valued at US$30. They also paid in fees and expenses to San Francisco lawyers Adam Gutride and Seth Safier, who filed the suit. The settlement called for Western Digital to add a disclaimer to their later packaging and advertising.\nWestern Digital had this footnote in their settlement. \"Apparently, Plaintiff believes that he could sue an egg company for fraud for labeling a carton of 12 eggs a 'dozen', because some bakers would view a 'dozen' as including 13 items.\"\n\"Cho v. Seagate Technology (US) Holdings, Inc.\".\nA lawsuit (\"Cho v. Seagate Technology (US) Holdings, Inc.\", San Francisco Superior Court, Case No. CGC-06-453195) was filed against Seagate Technology, alleging that Seagate overrepresented the amount of usable storage by 7% on hard drives sold between 22 March 2001 and 26 September 2007. The case was settled without Seagate admitting wrongdoing, but agreeing to supply those purchasers with gratis backup software or a 5% refund on the cost of the drives.\n\"Dinan et al. v. SanDisk LLC\".\nOn 22 January 2020, the district court of the Northern District of California ruled in favor of the defendant, SanDisk, upholding its use of \"GB\" to mean .\nIEC 1999 Standard.\nIn 1995, the International Union of Pure and Applied Chemistry's (IUPAC) Interdivisional Committee on Nomenclature and Symbols (IDCNS) proposed the prefixes \"kibi\" (short for \"kilobinary\"), \"mebi\" (\"megabinary\"), \"gibi\" (\"gigabinary\") and \"tebi\" (\"terabinary\"), with respective symbols \"kb\", \"Mb\", \"Gb\" and \"Tb\", for binary multipliers. The proposal suggested that the SI prefixes should be used only for powers of 10; so that a disk drive capacity of \"500 gigabytes\", \"0.5 terabytes\", \"500\u00a0GB\", or \"0.5\u00a0TB\" should all mean , exactly or approximately, rather than (=\u00a0) or (=\u00a0).\nThe proposal was not accepted by IUPAC at the time, but was taken up in 1996 by the Institute of Electrical and Electronics Engineers (IEEE) in collaboration with the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC). The prefixes \"kibi\", \"mebi\", \"gibi\" and \"tebi\" were retained, but with the symbols \"Ki\" (with capital \"K\"), \"Mi\", \"Gi\" and \"Ti\" respectively.\nIn January 1999, the IEC published this proposal, with additional prefixes \"pebi\" (\"Pi\") and \"exbi\" (\"Ei\"), as an international standard (IEC 60027-2 Amendment 2) The standard reaffirmed the BIPM's position that the SI prefixes should always denote powers of 10. The third edition of the standard, published in 2005, added prefixes \"zebi\" and \"yobi\", thus matching all then-defined SI prefixes with binary counterparts.\nThe harmonized ISO/IEC IEC 80000-13:2008 standard cancels and replaces subclauses 3.8 and 3.9 of IEC 60027-2:2005 (those defining prefixes for binary multiples). The only significant change is the addition of explicit definitions for some quantities. In 2009, the prefixes kibi-, mebi-, etc. were defined by ISO 80000-1 in their own right, independently of the kibibyte, mebibyte, and so on.\nThe BIPM standard JCGM 200:2012 \"International vocabulary of metrology \u2013 Basic and general concepts and associated terms (VIM), 3rd edition\" lists the IEC binary prefixes and states \"SI prefixes refer strictly to powers of 10, and should not be used for powers of 2. For example, 1 kilobit should not be used to represent bits (210\u00a0bits), which is 1 kibibit.\"\nThe IEC 60027-2 standard recommended operating systems and other software were updated to use binary or decimal prefixes consistently, but incorrect usage of SI prefixes for binary multiples is still common. At the time, the IEEE decided that their standards would use the prefixes \"kilo\", etc. with their metric definitions, but allowed the binary definitions to be used in an interim period as long as such usage was explicitly pointed out on a case-by-case basis.\nOther standards bodies and organizations.\nThe IEC standard binary prefixes are supported by other standardization bodies and technical organizations.\nThe United States National Institute of Standards and Technology (NIST) supports the ISO/IEC standards for\n\"Prefixes for binary multiples\" and has a web page documenting them, describing and justifying their use. NIST suggests that in English, the first syllable of the name of the binary-multiple prefix should be pronounced in the same way as the first syllable of the name of the corresponding SI prefix, and that the second syllable should be pronounced as \"bee\". NIST has stated the SI prefixes \"refer strictly to powers of 10\" and that the binary definitions \"should not be used\" for them.\nAs of 2014, the microelectronics industry standards body JEDEC describes the IEC prefixes in its online dictionary, but acknowledges that the SI prefixes and the symbols \"K\", \"M\" and \"G\" are still commonly used with the binary sense for memory sizes.\nOn 19 March 2005, the IEEE standard IEEE 1541-2002 (\"Prefixes for Binary Multiples\") was elevated to a full-use standard by the IEEE Standards Association after a two-year trial period. , the IEEE Publications division does not require the use of IEC prefixes in its major magazines such as \"Spectrum\" or \"Computer\". \nThe International Bureau of Weights and Measures (BIPM), which maintains the International System of Units (SI), expressly prohibits the use of SI prefixes to denote binary multiples, and recommends the use of the IEC prefixes as an alternative since units of information are not included in the SI.\nThe Society of Automotive Engineers (SAE) prohibits the use of SI prefixes with anything but a power-of-1000 meaning, but does not cite the IEC binary prefixes.\nThe European Committee for Electrotechnical Standardization (CENELEC) adopted the IEC-recommended binary prefixes via the harmonization document HD\u00a060027-2:2003-03. The European Union (EU) has required the use of the IEC binary prefixes since 2007.\nCurrent practice.\nSome computer industry participants, such as Hewlett-Packard (HP), and IBM have adopted or recommended IEC binary prefixes as part of their general documentation policies.\nAs of 2023, the use of SI prefixes with the binary meanings is still prevalent for specifying the capacity of the main memory of computers, of RAM, ROM, EPROM, and EEPROM chips and memory modules, and of the cache of computer processors. For example, a \"512-megabyte\" or \"512\u00a0MB\" memory module holds 512\u00a0MiB; that is, 512\u00a0\u00d7\u00a0220 bytes, not 512\u00a0\u00d7\u00a0106 bytes.\nJEDEC continues to include the customary binary definitions of \"kilo\", \"mega\", and \"giga\" in the document \"Terms, Definitions, and Letter Symbols\", and, , still used those definitions in their memory standards.\nOn the other hand, the SI prefixes with powers of ten meanings are generally used for the capacity of external storage units, such as disk drives, solid state drives, and USB flash drives, except for some flash memory chips intended to be used as EEPROMs. However, some disk manufacturers have used the IEC prefixes to avoid confusion. The decimal meaning of SI prefixes is usually also intended in measurements of data transfer rates, and clock speeds.\nSome operating systems and other software use either the IEC binary multiplier symbols (\"Ki\", \"Mi\", etc.) or the SI multiplier symbols (\"k\", \"M\", \"G\", etc.) with decimal meaning. Some programs, such as the GNU ls command, let the user choose between binary or decimal multipliers. However, some continue to use the SI symbols with the binary meanings, even when reporting disk or file sizes. Some programs may also use \"K\" instead of \"k\", with either meaning.\nOther uses.\nWhile the binary prefixes are almost always used with the units of information, bits and bytes, they may be used with any other unit of measure, when convenient. For example, in signal processing one may need binary multiples of the frequency unit hertz (Hz), for example the kibihertz (KiHz), equal to ."}
{"id": "4078", "revid": "46834214", "url": "https://en.wikipedia.org/wiki?curid=4078", "title": "National Baseball Hall of Fame and Museum", "text": "The National Baseball Hall of Fame and Museum is a history museum and hall of fame in Cooperstown, New York, operated by a private foundation. It serves as the central point of the history of baseball in the United States and displays baseball-related artifacts and exhibits, honoring those who have excelled in playing, managing, and serving the sport. The Hall's motto is \"Preserving History, Honoring Excellence, Connecting Generations\". Cooperstown is often used as shorthand (or a metonym) for the National Baseball Hall of Fame and Museum. The museum also established and manages the process for honorees into the Hall of Fame.\nThe Hall of Fame was established in 1939 by Stephen Carlton Clark, an heir to the Singer Sewing Machine fortune. Clark sought to bring tourists to the village hurt by the Great Depression, which reduced the local tourist trade, and Prohibition, which devastated the local hops industry. Clark constructed the Hall of Fame's building, which was dedicated on June 12, 1939. (His granddaughter, Jane Forbes Clark, is the current chairman of the board of directors.) The erroneous claim that Civil War hero Abner Doubleday invented baseball in Cooperstown was instrumental in the early marketing of the Hall.\nAn expanded library and research facility opened in 1994. Dale Petroskey became the organization's president in 1999. In 2002, the Hall launched \"Baseball as America\", a traveling exhibit that toured ten American museums over six years. The Hall of Fame has since also sponsored educational programming on the Internet to bring the Hall of Fame to schoolchildren who might not visit. The Hall and Museum completed a series of renovations in spring 2005. The Hall of Fame also presents an annual exhibit at FanFest at the Major League Baseball All-Star Game.\nInductees.\nAmong baseball fans, \"Hall of Fame\" means not only the museum and facility in Cooperstown, New York, but the pantheon of players, managers, umpires, executives, and pioneers who have been inducted into the Hall. The first five men elected were Ty Cobb, Babe Ruth, Honus Wagner, Christy Mathewson and Walter Johnson, chosen in 1936; roughly 20 more were selected before the entire group was inducted at the Hall's 1939 opening. , 351 people had been elected to the Hall of Fame, including 278 former professional players, 23 managers, 10 umpires, and 40 pioneers, executives, and organizers. 119 members of the Hall of Fame have been inducted posthumously, including four who died after their selection was announced. Of the 39 members primarily recognized for their contributions to Negro league baseball, 31 were inducted posthumously, including all 26 selected since the 1990s. The Hall of Fame includes one female member, Effa Manley.\nThe newest members of the Hall of Fame as of January 21, 2025, are Dick Allen, Dave Parker, CC Sabathia, Ichiro Suzuki, and Billy Wagner.\nIn 2019, former Yankees closer Mariano Rivera became the first player to be elected unanimously on the writers' ballot. Derek Jeter, Marvin Miller, Ted Simmons, and Larry Walker were to be inducted in 2020, but their induction ceremony was delayed by the COVID-19 pandemic until September 8, 2021. The ceremony was open to the public, as COVID restrictions had been lifted.\nSelection process.\nPlayers are currently inducted into the Hall of Fame through election by either the Baseball Writers' Association of America (or BBWAA), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball. Five years after retirement, any player with 10 years of major league experience who passes a screening committee (which removes from consideration players of clearly lesser qualification) is eligible to be elected by BBWAA members with 10 years' membership or more who also have been actively covering MLB at any time in the 10 years preceding the election (the latter requirement was added for the 2016 election). From a final ballot typically including 25\u201340 candidates, each writer may vote for up to 10 players; until the late 1950s, voters were advised to cast votes for the maximum 10 candidates. Any player named on 75% or more of all ballots cast is elected. A player who is named on fewer than 5% of ballots is dropped from future elections. In some instances, the screening committee had restored their names to later ballots, but in the mid-1990s, dropped players were made permanently ineligible for Hall of Fame consideration, even by the Veterans Committee. A 2001 change in the election procedures restored the eligibility of these dropped players; while their names will not appear on future BBWAA ballots, they may be considered by the Veterans Committee. Players receiving 5% or more of the votes but fewer than 75% are reconsidered annually until a maximum of ten years of eligibility (lowered from fifteen years for the 2015 election).\nUnder special circumstances, certain players may be deemed eligible for induction even though they have not met all requirements. Addie Joss was elected in 1978, despite only playing nine seasons before he died of meningitis. Additionally, if an otherwise eligible player dies before his fifth year of retirement, then that player may be placed on the ballot at the first election at least six months after his death. Roberto Clemente set the precedent: the writers put him up for consideration after his death on New Year's Eve, 1972, and he was inducted in 1973.\nThe five-year waiting period was established in 1954 after an evolutionary process. In 1936 all players were eligible, including active ones. From the 1937 election until the 1945 election, there was no waiting period, so any retired player was eligible, but writers were discouraged from voting for current major leaguers. Since there was no formal rule preventing a writer from casting a ballot for an active player, the scribes did not always comply with the informal guideline; Joe DiMaggio received a vote in 1945, for example. From the 1946 election until the 1954 election, an official one-year waiting period was in effect. (DiMaggio, for example, retired after the 1951 season and was first eligible in the 1953 election.) The modern rule establishing a wait of five years was passed in 1954, although those who had already been eligible under the old rule were grandfathered into the ballot, thus permitting Joe DiMaggio to be elected within four years of his retirement.\nContrary to popular belief, no formal exception was made for Lou Gehrig (other than to hold a special one-man election for him): there was no waiting period at that time, and Gehrig met all other qualifications, so he would have been eligible for the next regular election after he retired during the 1939 season. However, the BBWAA decided to hold a special election at the 1939 Winter Meetings in Cincinnati, specifically to elect Gehrig (most likely because it was known that he was terminally ill, making it uncertain that he would live long enough to see another election). Nobody else was on that ballot, and the numerical results have never been made public. Since no elections were held in 1940 or 1941, the special election permitted Gehrig to enter the Hall while still alive.\nIf a player fails to be elected by the BBWAA within 10 years of his eligibility for election, he may be selected by the Veterans Committee. Following changes to the election process for that body made in 2010 and 2016, the Veterans Committee is now responsible for electing all otherwise eligible candidates who are not eligible for the BBWAA ballot \u2014 both long-retired players and non-playing personnel (managers, umpires, and executives). From 2011 to 2016, each candidate could be considered once every three years; now, the frequency depends on the era in which an individual made his greatest contributions. A more complete discussion of the new process is available below.\nFrom 2008 to 2010, following changes made by the Hall in July 2007, the main Veterans Committee, then made up of living Hall of Famers, voted only on players whose careers began in 1943 or later. These changes also established three separate committees to select other figures:\nPlayers of the Negro leagues have also been considered at various times, beginning in 1971. In 2005, the Hall completed a study on African American players between the late 19th century and the integration of the major leagues in 1947, and conducted a special election for such players in February 2006; seventeen figures from the Negro leagues were chosen in that election, in addition to the eighteen previously selected. Following the 2010 changes, Negro leagues figures were primarily considered for induction alongside other figures from the 1871\u20131946 era, called the \"Pre-Integration Era\" by the Hall; since 2016, Negro leagues figures are primarily considered alongside other figures from what the Hall calls the \"Early Baseball\" era (1871\u20131949).\nPredictably, the selection process catalyzes endless debate among baseball fans over the merits of various candidates. Even players elected years ago remain the subjects of discussions as to whether they deserved election. For example, Bill James' 1994 book \"Whatever Happened to the Hall of Fame?\" goes into detail about who he believes does and does not belong in the Hall of Fame.\nNon-induction of banned players.\nThe selection rules for the Baseball Hall of Fame were modified to prevent the induction of anyone on Baseball's \"permanently ineligible\" list. The most prominent former players to be affected are Pete Rose and \"Shoeless Joe\" Jackson\u2014many others have been barred from participation in MLB, but none have Hall of Fame qualifications on the level of Jackson or Rose. Jackson and Rose were both banned from MLB for life for actions related to gambling on games involving their own teams.\nJackson was determined to have cooperated with those who conspired to intentionally lose the 1919 World Series, and for accepting payment for losing, although his actual level of culpability is fiercely debated. The ensuing Black Sox Scandal led directly to baseball's Rule 21, prominently posted in every clubhouse locker room, which mandates permanent banishment from MLB for having a gambling interest of any sort on a game in which a player, manager or umpire is directly involved.\nRose voluntarily accepted a permanent spot on the ineligible list in return for MLB's promise to make no official finding in relation to alleged betting on the Cincinnati Reds when he was their manager in the 1980s. No credible evidence has ever emerged to support allegations that Rose bet against his team and/or that his betting influenced his managerial decisions, nevertheless, the betting constituted a clear violation of the aforementioned Rule 21. After years of denial, Rose admitted that he bet on the Reds in his 2004 autobiography.\nBaseball fans are deeply split on the issue of whether Rose and/or Jackson (now both deceased) should remain banned or have their punishments posthumously revoked. Writer Bill James, though he advocates Rose eventually making it into the Hall of Fame, compared the people who want to put Jackson in the Hall of Fame to \"those women who show up at murder trials wanting to marry the cute murderer\".\nChanges to Veterans Committee process.\nThe actions and composition of the Veterans Committee have been at times controversial, with occasional selections of contemporaries and teammates of the committee members over seemingly more worthy candidates.\nIn 2001, the Veterans Committee was reformed to comprise the living Hall of Fame members and other honorees. The revamped Committee held three elections, in 2003 and 2007, for both players and non-players, and in 2005 for players only. No individual was elected in that time, sparking criticism among some observers who expressed doubt whether the new Veterans Committee would ever elect a player. The Committee members, most of whom were Hall members, were accused of being reluctant to elect new candidates in the hope of heightening the value of their own selection. After no one was selected for the third consecutive election in 2007, Hall of Famer Mike Schmidt noted, \"The same thing happens every year. The current members want to preserve the prestige as much as possible, and are unwilling to open the doors.\" In 2007, the committee and its selection processes were again reorganized; the main committee then included all living members of the Hall, and voted on a reduced number of candidates from among players whose careers began in 1943 or later. Separate committees, including sportswriters and broadcasters, would select umpires, managers and executives, as well as players from earlier eras.\nIn the first election to be held under the 2007 revisions, two managers and three executives were elected in December 2007 as part of the 2008 election process. The next Veterans Committee elections for players were held in December 2008 as part of the 2009 election process; the main committee did not select a player, while the panel for pre\u2013World War II players elected Joe Gordon in its first and ultimately only vote. The main committee voted as part of the election process for inductions in odd-numbered years, while the pre-World War II panel would vote every five years, and the panel for umpires, managers, and executives voted as part of the election process for inductions in even-numbered years.\nFurther changes to the Veterans Committee process were announced by the Hall in July 2010, July 2016, and April 2022.\nCurrent structure.\nPer the latest changes, announced on April 22, 2022, the multiple eras previously utilized were collapsed to three, to be voted on in an annual rotation (one per year):\nA one-year waiting period beyond potential BBWAA eligibility (which had been abolished in 2016) was reintroduced, thus restricting the committee to considering players retired for at least 16 seasons.\nEligibility.\nThe eligibility criteria for Era Committee consideration differ between players, managers, and executives.\nPlayers and managers with multiple teams.\nWhile the text on a player's or manager's plaque lists all teams for which the inductee was a member in that specific role, inductees are usually depicted wearing the cap of a specific team, though in a few cases, like umpires, they wear caps without logos. (Executives are not depicted wearing caps.) Additionally, as of 2015, inductee biographies on the Hall's website for all players and managers, and executives who were associated with specific teams, list a \"primary team\", which does not necessarily match the cap logo. The Hall selects the logo \"based on where that player makes his most indelible mark.\"\nAlthough the Hall always made the final decision on which logo was shown, until 2001 the Hall deferred to the wishes of players or managers whose careers were linked with multiple teams. Some examples of inductees associated with multiple teams are the following:\nIn all of the above cases, the \"primary team\" is the team for which the inductee spent the largest portion of his career except for Ryan, whose primary team is listed as the Angels despite playing one fewer season for that team than for the Astros.\nIn 2001, the Hall of Fame decided to change the policy on cap logo selection, as a result of rumors that some teams were offering compensation, such as number retirement, money, or organizational jobs, in exchange for the cap designation. (For example, though Wade Boggs denied the claims, some media reports had said that his contract with the Tampa Bay Devil Rays required him to request depiction in the Hall of Fame as a Devil Ray.) The Hall decided that it would no longer defer to the inductee, though the player's wishes would be considered, when deciding on the logo to appear on the plaque. Newly elected members affected by the change include the following:\nThe museum.\nSam Crane (who had played a decade in 19th century baseball before becoming a manager and sportswriter) had first approached the idea of making a memorial to the great players of the past in what was believed to have been the birthplace of baseball: Cooperstown, New York, but the idea did not muster much momentum until after his death in 1925. In 1934, the idea for establishing a Baseball Hall of Fame and Museum was devised by several individuals, such as Ford C. Frick (president of the National League) and Alexander Cleland, a Scottish immigrant who decided to serve as the first executive secretary for the Museum for the next seven years that worked with the interests of the Village and Major League Baseball. Stephen Carlton Clark (a Cooperstown native) paid for the construction of the museum, which was planned to open in 1939 to mark the \"Centennial of Baseball\", which included renovations to Doubleday Field. William Beattie served as the first curator of the museum.\nAccording to the Hall of Fame, approximately 260,000 visitors enter the museum each year, and the running total has surpassed 17\u00a0million. These visitors see only a fraction of its 40,000 artifacts, 3\u00a0million library items (such as newspaper clippings and photos) and 140,000 baseball cards.\nThe Hall has seen a noticeable decrease in attendance since the mid-2010s. A 2013 story on ESPN.com about the village of Cooperstown and its relation to the game partially linked the reduced attendance with Cooperstown Dreams Park, a youth baseball complex about away in the town of Hartwick. The 22 fields at Dreams Park currently draw 17,000 players each summer for a week of intensive play; while the complex includes housing for the players, their parents and grandparents must stay elsewhere. According to the story,\nPrior to Dreams Park, a room might be filled for a week by several sets of tourists. Now, that room will be taken by just one family for the week, and that family may only go into Cooperstown and the Hall of Fame once. While there are other contributing factors (the recession and high gas prices among them), the Hall's attendance has tumbled since Dreams Park opened. The Hall drew 383,000 visitors in 1999. It drew 262,000 last year.\nNotable events.\n1982 unauthorized sales.\nA controversy erupted in 1982, when it emerged that some historic items given to the Hall had been sold on the collectibles market. The items had been lent to the Baseball Commissioner's office, gotten mixed up with other property owned by the Commissioner's office and employees of the office, and moved to the garage of Joe Reichler, an assistant to Commissioner Bowie Kuhn, who sold the items to resolve his personal financial difficulties. Under pressure from the New York Attorney General, the Commissioner's Office made reparations, but the negative publicity damaged the Hall of Fame's reputation, and made it more difficult for it to solicit donations.\n2014 commemorative coins.\nIn 2012, Congress passed and President Barack Obama signed a law ordering the United States Mint to produce and sell commemorative, non-circulating coins to benefit the private, non-profit Hall. The bill, , was introduced in the United States House of Representatives by Rep. Richard Hanna, a Republican from New York, and passed the House on October 26, 2011. The coins, which depict baseball gloves and balls, are the first concave designs produced by the Mint. The mintage included 50,000 gold coins, 400,000 silver coins, and 750,000 clad (nickel-copper) coins. The Mint released them on March 27, 2014, and the gold and silver editions quickly sold out. The Hall receives money from surcharges included in the sale price: a total of $9.5\u00a0million if all the coins are sold."}
{"id": "4079", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=4079", "title": "BPP (complexity)", "text": "In computational complexity theory, a branch of computer science, bounded-error probabilistic polynomial time (BPP) is the class of decision problems solvable by a probabilistic Turing machine in polynomial time with an error probability bounded by 1/3 for all instances.\nBPP is one of the largest \"practical\" classes of problems, meaning most problems of interest in BPP have efficient probabilistic algorithms that can be run quickly on real modern machines. BPP also contains P, the class of problems solvable in polynomial time with a deterministic machine, since a deterministic machine is a special case of a probabilistic machine.\nInformally, a problem is in BPP if there is an algorithm for it that has the following properties:\nDefinition.\nA language \"L\" is in BPP if and only if there exists a probabilistic Turing machine \"M\", such that\nUnlike the complexity class ZPP, the machine \"M\" is required to run for polynomial time on all inputs, regardless of the outcome of the random coin flips.\nAlternatively, BPP can be defined using only deterministic Turing machines. A language \"L\" is in BPP if and only if there exists a polynomial \"p\" and deterministic Turing machine \"M\", such that\nIn this definition, the string \"y\" corresponds to the output of the random coin flips that the probabilistic Turing machine would have made. For some applications this definition is preferable since it does not mention probabilistic Turing machines.\nIn practice, an error probability of 1/3 might not be acceptable; however, the choice of 1/3 in the definition is arbitrary. Modifying the definition to use any constant between 0 and 1/2 (exclusive) in place of 1/3 would not change the resulting set BPP. For example, if one defined the class with the restriction that the algorithm can be wrong with probability at most 1/2100, this would result in the same class of problems. The error probability does not even have to be constant: the same class of problems is defined by allowing error as high as 1/2 \u2212 \"n\"\u2212\"c\" on the one hand, or requiring error as small as 2\u2212\"nc\" on the other hand, where \"c\" is any positive constant, and \"n\" is the length of input. This flexibility in the choice of error probability is based on the idea of running an error-prone algorithm many times, and using the majority result of the runs to obtain a more accurate algorithm. The chance that the majority of the runs are wrong drops off exponentially as a consequence of the Chernoff bound.\nProblems.\nAll problems in P are obviously also in BPP. However, many problems have been known to be in BPP but not known to be in P. The number of such problems is decreasing, and it is conjectured that P = BPP.\nFor a long time, one of the most famous problems known to be in BPP but not known to be in P was the problem of determining whether a given number is prime. However, in the 2002 paper \"PRIMES is in P\", Manindra Agrawal and his students Neeraj Kayal and Nitin Saxena found a deterministic polynomial-time algorithm for this problem, thus showing that it is in P.\nAn important example of a problem in BPP (in fact in co-RP) still not known to be in P is polynomial identity testing, the problem of determining whether a polynomial is identically equal to the zero polynomial, when you have access to the value of the polynomial for any given input, but not to the coefficients. In other words, is there an assignment of values to the variables such that when a nonzero polynomial is evaluated on these values, the result is nonzero? It suffices to choose each variable's value uniformly at random from a finite subset of at least \"d\" values to achieve bounded error probability, where \"d\" is the total degree of the polynomial.\nRelated classes.\nIf the access to randomness is removed from the definition of BPP, we get the complexity class P. In the definition of the class, if we replace the ordinary Turing machine with a quantum computer, we get the class BQP.\nAdding postselection to BPP, or allowing computation paths to have different lengths, gives the class BPPpath. BPPpath is known to contain NP, and it is contained in its quantum counterpart PostBQP.\nA Monte Carlo algorithm is a randomized algorithm which is likely to be correct. Problems in the class BPP have Monte Carlo algorithms with polynomial bounded running time. This is compared to a Las Vegas algorithm which is a randomized algorithm which either outputs the correct answer, or outputs \"fail\" with low probability. Las Vegas algorithms with polynomial bound running times are used to define the class ZPP. Alternatively, ZPP contains probabilistic algorithms that are always correct and have expected polynomial running time. This is weaker than saying it is a polynomial time algorithm, since it may run for super-polynomial time, but with very low probability.\nComplexity-theoretic properties.\nIt is known that BPP is closed under complement; that is, BPP = co-BPP. BPP is low for itself, meaning that a BPP machine with the power to solve BPP problems instantly (a BPP oracle machine) is not any more powerful than the machine without this extra power. In symbols, BPPBPP = BPP.\nThe relationship between BPP and NP is unknown: it is not known whether BPP is a subset of NP, NP is a subset of BPP or neither. If NP is contained in BPP, which is considered unlikely since it would imply practical solutions for NP-complete problems, then NP = RP and PH \u2286 BPP.\nIt is known that RP is a subset of BPP, and BPP is a subset of PP. It is not known whether those two are strict subsets, since we don't even know if P is a strict subset of PSPACE. BPP is contained in the second level of the polynomial hierarchy and therefore it is contained in PH. More precisely, the Sipser\u2013Lautemann theorem states that formula_1. As a result, P = NP leads to P = BPP since PH collapses to P in this case. Thus either P = BPP or P \u2260 NP or both.\nAdleman's theorem states that membership in any language in BPP can be determined by a family of polynomial-size Boolean circuits, which means BPP is contained in P/poly. Indeed, as a consequence of the proof of this fact, every BPP algorithm operating on inputs of bounded length can be derandomized into a deterministic algorithm using a fixed string of random bits. Finding this string may be expensive, however. Some weak separation results for Monte Carlo time classes were proven by , see also .\nClosure properties.\nThe class BPP is closed under complementation, union and intersection.\nRelativization.\nRelative to oracles, we know that there exist oracles A and B, such that PA = BPPA and PB \u2260 BPPB. Moreover, relative to a random oracle with probability 1, P = BPP and BPP is strictly contained in NP and co-NP.\nThere is even an oracle in which (and hence ), which can be iteratively constructed as follows. For a fixed ENP (relativized) complete problem, the oracle will give correct answers with high probability if queried with the problem instance followed by a random string of length \"kn\" (\"n\" is instance length; \"k\" is an appropriate small constant). Start with \"n\"=1. For every instance of the problem of length \"n\" fix oracle answers (see lemma below) to fix the instance output. Next, provide the instance outputs for queries consisting of the instance followed by \"kn\"-length string, and then treat output for queries of length \u2264(\"k\"+1)\"n\" as fixed, and proceed with instances of length \"n\"+1.\nThe lemma ensures that (for a large enough \"k\"), it is possible to do the construction while leaving enough strings for the relativized answers. Also, we can ensure that for the relativized , linear time suffices, even for function problems (if given a function oracle and linear output size) and with exponentially small (with linear exponent) error probability. Also, this construction is effective in that given an arbitrary oracle A we can arrange the oracle B to have and . Also, for a oracle (and hence ), one would fix the answers in the relativized E computation to a special nonanswer, thus ensuring that no fake answers are given.\nDerandomization.\nThe existence of certain strong pseudorandom number generators is conjectured by most experts of the field. Such generators could replace true random numbers in any polynomial-time randomized algorithm, producing indistinguishable results. The conjecture that these generators exist implies that randomness does not give additional computational power to polynomial time computation, that is, P = RP = BPP. More strongly, the assumption that P = BPP is in some sense equivalent to the existence of strong pseudorandom number generators.\nL\u00e1szl\u00f3 Babai, Lance Fortnow, Noam Nisan, and Avi Wigderson showed that unless EXPTIME collapses to MA, BPP is contained in \nThe class i.o.-SUBEXP, which stands for infinitely often SUBEXP, contains problems which have sub-exponential time algorithms for infinitely many input sizes. They also showed that P = BPP if the exponential-time hierarchy, which is defined in terms of the polynomial hierarchy and E as EPH, collapses to E; however, note that the exponential-time hierarchy is usually conjectured \"not\" to collapse.\nRussell Impagliazzo and Avi Wigderson showed that if any problem in E, where \nhas circuit complexity 2\u03a9(\"n\") then P = BPP."}
{"id": "4080", "revid": "374440", "url": "https://en.wikipedia.org/wiki?curid=4080", "title": "BQP", "text": "In computational complexity theory, bounded-error quantum polynomial time (BQP) is the class of decision problems solvable by a quantum computer in polynomial time, with an error probability of at most 1/3 for all instances. It is the quantum analogue to the complexity class BPP.\nA decision problem is a member of BQP if there exists a quantum algorithm (an algorithm that runs on a quantum computer) that solves the decision problem with high probability and is guaranteed to run in polynomial time. A run of the algorithm will correctly solve the decision problem with a probability of at least 2/3.\nDefinition.\nBQP can be viewed as the languages associated with certain bounded-error uniform families of quantum circuits. A language \"L\" is in BQP if and only if there exists a polynomial-time uniform family of quantum circuits formula_1, such that\nAlternatively, one can define BQP in terms of quantum Turing machines. A language \"L\" is in BQP if and only if there exists a polynomial quantum Turing machine that accepts \"L\" with an error probability of at most 1/3 for all instances.\nSimilarly to other \"bounded error\" probabilistic classes, the choice of 1/3 in the definition is arbitrary. We can run the algorithm a constant number of times and take a majority vote to achieve any desired probability of correctness less than 1, using the Chernoff bound. The complexity class is unchanged by allowing error as high as 1/2 \u2212 \"n\"\u2212\"c\" on the one hand, or requiring error as small as 2\u2212\"nc\" on the other hand, where \"c\" is any positive constant, and \"n\" is the length of input.\nRelationship to other complexity classes.\nBQP is defined for quantum computers; the corresponding complexity class for classical computers (or more formally for probabilistic Turing machines) is BPP. Just like P and BPP, BQP is low for itself, which means . Informally, this is true because polynomial time algorithms are closed under composition. If a polynomial time algorithm calls polynomial time algorithms as subroutines, the resulting algorithm is still polynomial time.\nBQP contains P and BPP and is contained in AWPP, PP and PSPACE.\nIn fact, BQP is low for PP, meaning that a PP machine achieves no benefit from being able to solve BQP problems instantly, an indication of the possible difference in power between these similar classes. The known relationships with classic complexity classes are:\nAs the problem of has not yet been solved, the proof of inequality between BQP and classes mentioned above is supposed to be difficult. The relation between BQP and NP is not known. In May 2018, computer scientists Ran Raz of Princeton University and Avishay Tal of Stanford University published a paper which showed that, relative to an oracle, BQP was not contained in PH. It can be proven that there exists an oracle A such that formula_6. In an extremely informal sense, this can be thought of as giving PH and BQP an identical, but additional, capability and verifying that BQP with the oracle (BQPA) can do things PHA cannot. While an oracle separation has been proven, the fact that BQP is not contained in PH has not been proven. An oracle separation does not prove whether or not complexity classes are the same. The oracle separation gives intuition that BQP may not be contained in PH.\nIt has been suspected for many years that Fourier Sampling is a problem that exists within BQP, but not within the polynomial hierarchy. Recent conjectures have provided evidence that a similar problem, Fourier Checking, also exists in the class BQP without being contained in the polynomial hierarchy. This conjecture is especially notable because it suggests that problems existing in BQP could be classified as harder than NP-Complete problems. Paired with the fact that many practical BQP problems are suspected to exist outside of P (it is suspected and not verified because there is no proof that P \u2260 NP), this illustrates the potential power of quantum computing in relation to classical computing.\nAdding postselection to BQP results in the complexity class PostBQP which is equal to PP.\nA complete problem for Promise-BQP.\nPromise-BQP is the class of promise problems that can be solved by a uniform family of quantum circuits (i.e., within BQP). Completeness proofs focus on this version of BQP. Similar to the notion of NP-completeness and other complete problems, we can define a complete problem as a problem that is in Promise-BQP and that every other problem in Promise-BQP reduces to it in polynomial time. \nAPPROX-QCIRCUIT-PROB.\nThe APPROX-QCIRCUIT-PROB problem is complete for efficient quantum computation, and the version presented below is complete for the Promise-BQP complexity class (and not for the total BQP complexity class, for which no complete problems are known). APPROX-QCIRCUIT-PROB's completeness makes it useful for proofs showing the relationships between other complexity classes and BQP.\nGiven a description of a quantum circuit acting on qubits with gates, where is a polynomial in and each gate acts on one or two qubits, and two numbers formula_7, distinguish between the following two cases:\nHere, there is a promise on the inputs as the problem does not specify the behavior if an instance is not covered by these two cases.\nClaim. Any BQP problem reduces to APPROX-QCIRCUIT-PROB. \nProof. \nSuppose we have an algorithm that solves APPROX-QCIRCUIT-PROB, i.e., given a quantum circuit acting on qubits, and two numbers formula_7, distinguishes between the above two cases. We can solve any problem in BQP with this oracle, by setting formula_15.\nFor any formula_16, there exists family of quantum circuits formula_1 such that for all formula_2, a state formula_19 of formula_20 qubits, if formula_21; else if formula_22. Fix an input formula_19 of qubits, and the corresponding quantum circuit formula_24. We can first construct a circuit formula_25 such that formula_26. This can be done easily by hardwiring formula_19 and apply a sequence of CNOT gates to flip the qubits. Then we can combine two circuits to get formula_28, and now formula_29. And finally, necessarily the results of formula_24 is obtained by measuring several qubits and apply some (classical) logic gates to them. We can always defer the measurement and reroute the circuits so that by measuring the first qubit of formula_29, we get the output. This will be our circuit , and we decide the membership of formula_32 by running formula_33 with formula_15. By definition of BQP, we will either fall into the first case (acceptance), or the second case (rejection), so formula_16 reduces to APPROX-QCIRCUIT-PROB.\nBQP and EXP.\nWe begin with an easier containment. To show that formula_36, it suffices to show that APPROX-QCIRCUIT-PROB is in EXP since APPROX-QCIRCUIT-PROB is BQP-complete.\nNote that this algorithm also requires formula_37 space to store the vectors and the matrices. We will show in the following section that we can improve upon the space complexity.\nBQP and PSPACE.\nSum of histories is a technique introduced by physicist Richard Feynman for path integral formulation. APPROX-QCIRCUIT-PROB can be formulated in the sum of histories technique to show that formula_38.\nConsider a quantum circuit , which consists of gates, formula_39, where each formula_40 comes from a universal gate set and acts on at most two qubits.\nTo understand what the sum of histories is, we visualize the evolution of a quantum state given a quantum circuit as a tree. The root is the input formula_41, and each node in the tree has formula_42 children, each representing a state in formula_43. The weight on a tree edge from a node in -th level representing a state formula_44 to a node in formula_45-th level representing a state formula_46 is formula_47, the amplitude of formula_46 after applying formula_49 on formula_44. The transition amplitude of a root-to-leaf path is the product of all the weights on the edges along the path. To get the probability of the final state being formula_51, we sum up the amplitudes of all root-to-leave paths that ends at a node representing formula_51.\nMore formally, for the quantum circuit , its sum over histories tree is a tree of depth , with one level for each gate formula_53 in addition to the root, and with branching factor formula_42.\nNotice in the sum over histories algorithm to compute some amplitude formula_55, only one history is stored at any point in the computation. Hence, the sum over histories algorithm uses formula_56 space to compute formula_55 for any since formula_56 bits are needed to store the histories in addition to some workspace variables.\nTherefore, in polynomial space, we may compute formula_59 over all with the first qubit being , which is the probability that the first qubit is measured to be 1 by the end of the circuit.\nNotice that compared with the simulation given for the proof that formula_36, our algorithm here takes far less space but far more time instead. In fact it takes formula_61 time to calculate a single amplitude!\nBQP and PP.\nA similar sum-over-histories argument can be used to show that formula_62. \nP and BQP.\nWe know formula_63, since every classical circuit can be simulated by a quantum circuit. \nIt is conjectured that BQP solves hard problems outside of P, specifically, problems in NP. The claim is indefinite because we don't know if P=NP, so we don't know if those problems are actually in P. Below are some evidence of the conjecture:"}
{"id": "4086", "revid": "96647", "url": "https://en.wikipedia.org/wiki?curid=4086", "title": "Brainfuck", "text": "Brainfuck is an esoteric programming language created in 1993 by Swiss student Urban M\u00fcller. Designed to be extremely minimalistic, the language consists of only eight simple commands, a data pointer, and an instruction pointer.\nBrainfuck is an example of a so-called Turing tarpit: it can be used to write any program, but it is not practical to do so because it provides so little abstraction that the programs get very long or complicated. While Brainfuck is fully Turing-complete, it is not intended for practical use but to challenge and amuse programmers. Brainfuck requires one to break down commands into small and simple instructions.\nThe language takes its name from the slang term \"brainfuck\", which refers to things so complicated or unusual that they exceed the limits of one's understanding, as it was not meant or made for designing actual software but to challenge the boundaries of computer programming.\nBecause the language's name contains profanity, many substitutes are used, such as brainfsck, branflakes, brainoof, brainfrick, BrainF, and BF.\nHistory.\nM\u00fcller designed Brainfuck with the goal of implementing the smallest possible compiler, inspired by the 1024-byte compiler for the FALSE programming language. M\u00fcller's original compiler was implemented in Motorola 68000 assembly on the Amiga and compiled to a binary with a size of 296 bytes. He uploaded the first Brainfuck compiler to Aminet in 1993. The program came with a \"Readme\" file, which briefly described the language, and challenged the reader \"Who can program anything useful with it? :)\". M\u00fcller also included an interpreter and some examples. A second version of the compiler used only 240 bytes.\nLanguage design.\nThe language consists of eight commands. A brainfuck program is a sequence of these commands, possibly interspersed with other characters (which are ignored). The commands are executed sequentially, with some exceptions: an instruction pointer begins at the first command, and each command it points to is executed, after which it normally moves forward to the next command. The program terminates when the instruction pointer moves past the last command.\nThe brainfuck language uses a simple machine model consisting of the program and instruction pointer, as well as a one-dimensional array of at least 30,000 byte cells initialized to zero; a movable data pointer (initialized to point to the leftmost byte of the array); and two streams of bytes for input and output (most often connected to a keyboard and a monitor respectively, and using the ASCII character encoding).\nThe eight language commands each consist of a single character:\ncodice_1 and codice_2 match as parentheses usually do: each codice_1 matches exactly one codice_2 and vice versa, the codice_1 comes first, and there can be no unmatched codice_1 or codice_2 between the two.\nBrainfuck programs are usually difficult to comprehend. This is partly because any mildly complex task requires a long sequence of commands and partly because the program's text gives no direct indications of the program's state. These, as well as Brainfuck's inefficiency and its limited input/output capabilities, are some of the reasons it is not used for serious programming. Nonetheless, like any Turing-complete language, Brainfuck is theoretically capable of computing any computable function or simulating any other computational model if given access to an unlimited amount of memory and time. A variety of Brainfuck programs have been written. Although Brainfuck programs, especially complicated ones, are difficult to write, it is quite trivial to write an interpreter for Brainfuck in a more typical language such as C due to its simplicity. Brainfuck interpreters written in the Brainfuck language itself also exist.\nExamples.\nAdding two values.\nAs a first, simple example, the following code snippet will add the current cell's value to the next cell: Each time the loop is executed, the current cell is decremented, the data pointer moves to the right, that next cell is incremented, and the data pointer moves left again. This sequence is repeated until the starting cell is 0.\n[-&gt;+&lt;]\nThis can be incorporated into a simple addition program as follows:\n Cell c0 =.\n&gt; +++++ Cell c1 = 5\n[ Start your loops with your cell pointer on the loop counter (c1 in our case)\n&lt; + Add 1 to c0\n&gt; - Subtract 1 from c1\n] End your loops with the cell pointer on the loop counter\nAt this point our program has added 5 to 2 leaving 7 in c0 and 0 in c1\nbut we cannot output this value to the terminal since it is not ASCII encoded\nTo display the ASCII character \"7\" we must add 48 to the value 7\nWe use a loop to compute 48 = 6 * 8\n++ ++++ c1 = 8 and this will be our loop counter aga.\n&lt; +++ +++ Add 6 to c0\n&gt; - Subtract 1 from c1\n&lt; . Print out c0 which has the value 55 which translates to \"7\"!\nHello World!\nThe following program prints \"Hello World!\" and a newline to the screen:\n[ This program prints \"Hello World!\" and a newline to the screen; its\n length is 106 active command characters. [It is not the shortest.]\n This loop is an \"initial comment loop\", a simple way of adding a comment\n to a BF program such that you don't have to worry about any command\n characters. Any \".\", \",\", \"+\", \"-\", \"&lt;\" and \"&gt;\" characters are simply\n ignored, the \"[\" and \"]\" characters just have to be balanced. This\n loop and the commands it contains are ignored because the current cell\n defaults to a value of 0; the 0 value causes this loop to be skipped.\n++++++ Set Cell #0 to.\n &gt;++++ Add 4 to Cell #1; this will always set Cell #1 to 4\n [ as the cell will be cleared by the loop\n &gt;++ Add 2 to Cell #2\n &gt;+++ Add 3 to Cell #3\n &gt;+++ Add 3 to Cell #4\n &gt;+ Add 1 to Cell #5\n \u00ab\u00ab- Decrement the loop counter in Cell #1\n ] Loop until Cell #1 is zero; number of iterations is 4\n &gt;+ Add 1 to Cell #2\n &gt;+ Add 1 to Cell #3\n &gt;- Subtract 1 from Cell #4\n \u00bb+ Add 1 to Cell #6\n [&lt;] Move back to the first zero cell you find; this will\n be Cell #1 which was cleared by the previous loop\n &lt;- Decrement the loop Counter in Cell #0\n] Loop until Cell #0 is zero; number of iterations is 8\nThe result of this is:\nCell no : 0 1 2 3 4 5 6\nContents: 0 0 72 104 88 32 8\nPointer : ^\n\u00bb. Cell #2 has value 72 which is 'H'\n&gt;---. Subtract 3 from Cell #3 to get 101 which is 'e'\n+++++..+++. Likewise for 'llo' from Cell .\n\u00bb. Cell #5 is 32 for the space\n&lt;-. Subtract 1 from Cell #4 for 87 to give a 'W'\n&lt;. Cell #3 was set to 'o' from the end of 'Hello'\n+.------.--------. Cell #3 for 'rl' and '.\n\u00bb+. Add 1 to Cell #5 gives us an exclamation point\n&gt;++. And finally a newline from Cell #6\nFor readability, this code has been spread across many lines, and blanks and comments have been added. Brainfuck ignores all characters except the eight commands codice_8 so no special syntax for comments is needed (as long as the comments do not contain the command characters). The code could just as well have been written as:\nROT13.\nThis program enciphers its input with the ROT13 cipher. To do this, it must map characters A-M (ASCII 65\u201377) to N-Z (78\u201390), and vice versa. Also it must map a-m (97\u2013109) to n-z (110\u2013122) and vice versa. It must map all other characters to themselves; it reads characters one at a time and outputs their enciphered equivalents until it reads an EOF (here assumed to be represented as either -1 or \"no change\"), at which point the program terminates.\n-,+[ Read first character and start outer character reading loop\n -[ Skip forward if character is 0\n \u00bb++++[&gt;++++++++&lt;-] Set up divisor (32) for division loop\n (MEMORY LAYOUT: dividend copy remainder divisor quotient zero zero)\n &lt;+&lt;-[ Set up dividend (x minus 1) and enter division loop\n &gt;+&gt;+&gt;-[\u00bb&gt;] Increase copy and remainder / reduce divisor / Normal case: skip forward\n &lt;]&lt;[ Zero that flag unless quotient was 2 or 3; zero quotient; check flag\n ++++++++++++&lt;[ If flag then set up divisor (13) for second division loop\n (MEMORY LAYOUT: zero copy dividend divisor remainder quotient zero zero)\n &gt;-[&gt;+\u00bb] Reduce divisor; Normal case: increase remainder\n &gt;[+[&lt;+&gt;-]&gt;+\u00bb] Special case: increase remainder / move it back to divisor / increase quotient\n \u00ab\u00ab&lt;- Decrease dividend\n ] End division loop\n \u00bb[&lt;+&gt;-] Add remainder back to divisor to get a useful 13\n &gt;[ Skip forward if quotient was 0\n -[ Decrement quotient and skip forward if quotient was 1\n -\u00ab[-]\u00bb Zero quotient and divisor if quotient was 2\n ]\u00ab[\u00ab-\u00bb-]\u00bb Zero divisor and subtract 13 from copy if quotient was 1\n ]\u00ab[\u00ab+\u00bb-] Zero divisor and add 13 to copy if quotient was 0\n ] End outer skip loop (jump to here if ((character minus 1)/32) was not 2 or 3)\n &lt;[-] Clear remainder from first division if second division was skipped\n &lt;.[-] Output ROT13ed character from copy and clear it\n &lt;-,+ Read next character\n] End character reading loop\nSimulation of abiogenesis.\nIn 2024, a Google research project used a slightly modified 7-command version of Brainfuck as the basis of an artificial digital environment. In this environment, they found that replicators arose naturally and competed with each other for domination of the environment."}
{"id": "4090", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=4090", "title": "Binary and", "text": ""}
{"id": "4091", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=4091", "title": "Bartolomeo Ammannati", "text": "Bartolomeo Ammannati (18 June 1511 \u2013 13 April 1592) was an Italian architect and sculptor, born at Settignano, near Florence, Italy. He studied under Baccio Bandinelli and Jacopo Sansovino (assisting on the design of the Library of St. Mark's, the Biblioteca Marciana, Venice) and closely imitated the style of Michelangelo.\nHe was more distinguished in architecture than in sculpture. He worked in Rome in collaboration with Vignola and Vasari), including designs for the Villa Giulia, but also for works at Lucca. He labored during 1558\u20131570, in the refurbishment and enlargement of Pitti Palace, creating the courtyard consisting of three wings with rusticated facades, and one lower portico leading to the amphitheatre in the Boboli Gardens. His design mirrored the appearance of the main external fa\u00e7ade of Pitti. He was also named \"Consul\" of Accademia delle Arti del Disegno of Florence, which had been founded by the Duke Cosimo I in 1563.\nIn 1569, Ammannati was commissioned to build the Ponte Santa Trinita, a bridge over the Arno River. The three arches are elliptic, and though very light and elegant, has survived, when floods had damaged other Arno bridges at different times. Santa Trinita was destroyed in 1944, during World War II, and rebuilt in 1957.\nAmmannati designed what is considered a prototypic Mannerist sculptural ensemble in the Fountain of Neptune (\"Fontana del Nettuno\"), prominently located in the Piazza della Signoria in the center of Florence. The assignment was originally given to the aged Bartolommeo Bandinelli; however when Bandinelli died, Ammannati's design, bested the submissions of Benvenuto Cellini and Vincenzo Danti, to gain the commission. From 1563 and 1565, Ammannati and his assistants, among them Giambologna, sculpted the block of marble that had been chosen by Bandinelli. He took Grand Duke Cosimo I as model for Neptune's face. The statue was meant to highlight Cosimo's goal of establishing a Florentine Naval force. The ungainly sea god was placed at the corner of the Palazzo Vecchio within sight of Michelangelo's David statue, and the then 87-year-old sculptor is said to have scoffed at Ammannati\u2014 saying that he had ruined a beautiful piece of marble\u2014 with the ditty: \"Ammannati, Ammanato, che bel marmo hai rovinato!\" Ammannati continued work on this fountain for a decade, adding around the perimeter a cornucopia of demigod figures: bronze reclining river gods, laughing satyrs and marble sea horses emerging from the water.\nIn 1550 Ammannati married Laura Battiferri, an elegant poet and an accomplished woman. Later in his life he had a religious crisis, influenced by Counter-Reformation piety, which resulted in condemning his own works depicting nudity, and he left all his possessions to the Jesuits.\nHe died in Florence in 1592."}
{"id": "4092", "revid": "38912916", "url": "https://en.wikipedia.org/wiki?curid=4092", "title": "Bishop", "text": "A bishop is an ordained member of the clergy who is entrusted with a position of authority and oversight in a religious institution. In Christianity, bishops are normally responsible for the governance and administration of dioceses. The role or office of the bishop is called episcopacy. Organizationally, several Christian denominations utilize ecclesiastical structures that call for the position of bishops, while other denominations have dispensed with this office, seeing it as a symbol of power. Bishops have also exercised political authority within their dioceses.\nTraditionally, bishops claim apostolic succession, a direct historical lineage dating back to the original Twelve Apostles or Saint Paul. The bishops are by doctrine understood as those who possess the full priesthood given by Jesus Christ, and therefore may ordain other clergy, including other bishops. A person ordained as a deacon, priest (i.e. presbyter), and then bishop is understood to hold the fullness of the ministerial priesthood, given responsibility by Christ to govern, teach and sanctify the Body of Christ (the Church). Priests, deacons and lay ministers co-operate and assist their bishops in pastoral ministry. \nSome Pentecostal and other Protestant denominations have bishops who oversee congregations, though they do not necessarily claim apostolic succession.\nEtymology and terminology.\nThe English word \"bishop\" derives, via Latin , Old English , and Middle English , from the Greek word , meaning \"overseer\" or \"supervisor\". Greek was the language of the early Christian church, but the term did not originate in Christianity: it had been used in Greek for several centuries before the advent of Christianity.\nThe English words \"priest\" and \"presbyter\" both derive, via Latin, from the Greek word , meaning \"elder\" or \"senior\", and not originally referring to priesthood.\nIn the early Christian era the two terms were not always clearly distinguished, but is used in the sense of the order or office of bishop, distinct from that of , in the writings attributed to Ignatius of Antioch in the second century.\nHistory in Christianity.\nThe earliest organization of the Church in Jerusalem was, according to most scholars, similar to that of Jewish synagogues, but it had a council or college of ordained presbyters (). In Acts 11:30 and Acts 15:22, a collegiate system of government in Jerusalem is chaired by James the Just, according to tradition the first bishop of the city. In Acts 14:23, the Apostle Paul ordains presbyters in churches in Anatolia. The word \"presbyter\" was not yet distinguished from \"overseer\" (, later used exclusively to mean \"bishop\"), as in Acts 20:17, Titus 1:5\u20137 and 1 Peter 5:1. The earliest writings of the Apostolic Fathers, the Didache and the First Epistle of Clement, for example, show the church used two terms for local church offices\u2014presbyters (seen by many as an interchangeable term with or overseer) and deacon.\nIn the First epistle to Timothy and Epistle to Titus in the New Testament a more clearly defined episcopate can be seen. Both letters state that Paul had left Timothy in Ephesus and Titus in Crete to oversee the local church. Paul commands Titus to ordain presbyters/bishops and to exercise general oversight.\nEarly sources are unclear but various groups of Christian communities may have had the bishop surrounded by a group or college functioning as leaders of the local churches. Eventually the head or \"monarchic\" bishop came to rule more clearly, and all local churches would eventually follow the example of the other churches and structure themselves after the model of the others with the one bishop in clearer charge, though the role of the body of presbyters remained important.\nApostolic Fathers.\nAround the end of the 1st century, the church's organization became clearer in historical documents. In the works of the Apostolic Fathers, and Ignatius of Antioch in particular, the role of the episkopos, or bishop, became more important or, rather, already was very important and being clearly defined. While Ignatius of Antioch offers the earliest clear description of monarchial bishops (a single bishop over all house churches in a city) he is an advocate of monepiscopal structure rather than describing an accepted reality. To the bishops and house churches to which he writes, he offers strategies on how to pressure house churches who do not recognize the bishop into compliance. Other contemporary Christian writers do not describe monarchial bishops, either continuing to equate them with the presbyters or speaking of (bishops, plural) in a city.\nClement of Alexandria (end of the 2nd century) writes about the ordination of a certain Zach\u00e6us as bishop by the imposition of Simon Peter Bar-Jonah's hands. The words bishop and ordination are used in their technical meaning by the same Clement of Alexandria. The bishops in the 2nd century are defined also as the only clergy to whom the ordination to priesthood (presbyterate) and diaconate is entrusted: \"a priest (presbyter) lays on hands, but does not ordain.\" ().\nAt the beginning of the 3rd century, Hippolytus of Rome describes another feature of the ministry of a bishop, which is that of the : the primate of sacrificial priesthood and the power to forgive sins.\nChristian bishops and civil government.\nThe efficient organization of the Roman Empire became the template for the organisation of the church in the 4th century, particularly after Constantine's Edict of Milan. As the church moved from the shadows of privacy into the public forum it acquired land for churches, burials and clergy. In 391, Theodosius I decreed that any land that had been confiscated from the church by Roman authorities be returned.\nThe most usual term for the geographic area of a bishop's authority and ministry, the diocese, began as part of the structure of the Roman Empire under Diocletian. As Roman authority began to fail in the western portion of the empire, the church took over much of the civil administration. This can be clearly seen in the ministry of two popes: Pope Leo I in the 5th century, and Pope Gregory I in the 6th century. Both of these men were statesmen and public administrators in addition to their role as Christian pastors, teachers and leaders. In the Eastern churches, latifundia entailed to a bishop's see were much less common, the state power did not collapse the way it did in the West, and thus the tendency of bishops acquiring civil power was much weaker than in the West. However, the role of Western bishops as civil authorities, often called prince bishops, continued throughout much of the Middle Ages.\nBishops holding political office.\nAs well as being Archchancellors of the Holy Roman Empire after the 9th century, bishops generally served as chancellors to medieval monarchs, acting as head of the \"justiciary\" and chief chaplain. The Lord Chancellor of England was almost always a bishop up until the dismissal of Cardinal Thomas Wolsey by Henry VIII. Similarly, the position of Kanclerz in the Polish kingdom was always held by a bishop until the 16th century.\nIn modern times, the principality of Andorra is headed by Co-Princes of Andorra, one of whom is the Bishop of Urgell and the other, the sitting President of France, an arrangement that began with the Par\u00e9age of Andorra (1278), and was ratified in the 1993 constitution of Andorra.\nThe office of the Papacy is inherently held by the sitting Roman Catholic Bishop of Rome. Though not originally intended to hold temporal authority, since the Middle Ages the power of the Papacy gradually expanded deep into the secular realm and for centuries the sitting Bishop of Rome was the most powerful governmental office in Central Italy. In modern times, the Pope is also the sovereign Prince of Vatican City, an internationally recognized micro-state located entirely within the city of Rome.\nIn France, prior to the Revolution, representatives of the clergy \u2014 in practice, bishops and abbots of the largest monasteries \u2014 comprised the First Estate of the Estates-General. This role was abolished after separation of Church and State was implemented during the French Revolution.\nIn the 21st century, the more senior bishops of the Church of England continue to sit in the House of Lords of the Parliament of the United Kingdom, as representatives of the established church, and are known as Lords Spiritual. The Bishop of Sodor and Man, whose diocese lies outside the United Kingdom, is an \"ex officio\" member of the Legislative Council of the Isle of Man. In the past, the Bishop of Durham had extensive vice-regal powers within his northern diocese, which was a county palatine, the County Palatine of Durham, (previously, Liberty of Durham) of which he was \"ex officio\" the earl. In the 19th century, a gradual process of reform was enacted, with the majority of the bishop's historic powers vested in The Crown by 1858.\nEastern Orthodox bishops, along with all other members of the clergy, are canonically forbidden to hold political office. Occasional exceptions to this rule are tolerated when the alternative is political chaos. In the Ottoman Empire, the Patriarch of Constantinople, for example, had de facto administrative, cultural and legal jurisdiction, as well as spiritual authority, over all Eastern Orthodox Christians of the empire, as part of the Ottoman millet system. An Orthodox bishop headed the Prince-Bishopric of Montenegro from 1516 to 1852, assisted by a secular \"guvernadur\". More recently, Archbishop Makarios III of Cyprus, served as President of the Cyprus from 1960 to 1977, an extremely turbulent time period on the island.\nIn 2001, Peter Hollingworth, AC, OBE \u2013 then the Anglican Archbishop of Brisbane \u2013 was controversially appointed Governor-General of Australia. Although Hollingworth gave up his episcopal position to accept the appointment, it still attracted considerable opposition in a country which maintains a formal separation between Church and State.\nEpiscopacy during the English Civil War.\nDuring the period of the English Civil War, the role of bishops as wielders of political power and as upholders of the established church became a matter of heated political controversy. Presbyterianism was the polity of most Reformed Christianity in Europe, and had been favored by many in England since the English Reformation. Since in the primitive church the offices of \"presbyter\" and were not clearly distinguished, many Puritans held that this was the only form of government the church should have. The Anglican divine, Richard Hooker, objected to this claim in his famous work \"Of the Laws of Ecclesiastic Polity\" while, at the same time, defending Presbyterian ordination as valid (in particular Calvin's ordination of Beza). This was the official stance of the English Church until the Commonwealth, during which time, the views of Presbyterians and Independents (Congregationalists) were more freely expressed and practiced.\nChristian churches.\nCatholic, Eastern Orthodox, Oriental Orthodox, Lutheran and Anglican churches.\nBishops form the leadership in the Catholic Church, the Eastern Orthodox Church, the Oriental Orthodox Churches, certain Lutheran churches, the Anglican Communion, the Independent Catholic churches, the Independent Anglican churches, and certain other, smaller, denominations.\nThe traditional role of a bishop is as pastor of a diocese (also called a bishopric, synod, eparchy or see), and so to serve as a \"diocesan bishop\", or \"eparch\" as it is called in many Eastern Christian churches. Dioceses vary considerably in size, geographically and population-wise. Some dioceses around the Mediterranean Sea which were Christianised early are rather compact, whereas dioceses in areas of rapid modern growth in Christian commitment\u2014as in some parts of Sub-Saharan Africa, South America and the Far East\u2014are much larger and more populous.\nAs well as traditional diocesan bishops, many churches have a well-developed structure of church leadership that involves a number of layers of authority and responsibility.\nDuties.\nIn Catholicism, Eastern Orthodoxy, Oriental Orthodoxy, High Church Lutheranism, and Anglicanism, only a bishop can ordain other bishops, priests, and deacons.\nIn the Eastern liturgical tradition, a priest can celebrate the Divine Liturgy only with the blessing of a bishop. In Byzantine usage, an antimension signed by the bishop is kept on the altar partly as a reminder of whose altar it is and under whose omophorion the priest at a local parish is serving. In Syriac Church usage, a consecrated wooden block called a thabilitho is kept for the same reasons.\nThe bishop is the ordinary minister of the sacrament of confirmation in the Latin Church, and in the Old Catholic communion only a bishop may administer this sacrament. In the Lutheran and Anglican churches, the bishop normatively administers the rite of confirmation, although in those denominations that do not have an episcopal polity, confirmation is administered by the priest. However, in the Byzantine and other Eastern rites, whether Eastern or Oriental Orthodox or Eastern Catholic, chrismation is done immediately after baptism, and thus the priest is the one who confirms, using chrism blessed by a bishop.\nOrdination of Catholic, Eastern Orthodox, Oriental Orthodox, Lutheran and Anglican bishops.\nBishops in all of these communions are ordained by other bishops through the laying on of hands. \nOrdination of a bishop, and thus continuation of apostolic succession, takes place through a ritual centred on the imposition of hands and prayer.\nCatholic, Eastern Orthodox, Oriental Orthodox, Anglican, Old Catholic and some Lutheran bishops claim to be part of the continuous sequence of ordained bishops since the days of the apostles referred to as apostolic succession.\nIn Scandinavia and the Baltic region, Lutheran churches participating in the Porvoo Communion (those of Iceland, Norway, Sweden, Finland, Estonia, and Lithuania), as well as many non-Porvoo membership Lutheran churches (including those of Kenya, Latvia, and Russia), as well as the confessional Communion of Nordic Lutheran Dioceses, believe that they ordain their bishops in the apostolic succession in lines stemming from the original apostles. \"The New Westminster Dictionary of Church History\" states that \"In Sweden the apostolic succession was preserved because the Catholic bishops were allowed to stay in office, but they had to approve changes in the ceremonies.\"\nPeculiar to the Catholic Church.\nWhile traditional teaching maintains that any bishop with apostolic succession can validly perform the ordination of another bishop, some churches require two or three bishops participate, either to ensure sacramental validity or to conform with church law. Catholic doctrine holds that one bishop can validly ordain another (priest) as a bishop. Though a minimum of three bishops participating is desirable (there are usually several more) in order to demonstrate collegiality, canonically only one bishop is necessary. The practice of only one bishop ordaining was normal in countries where the church was persecuted under Communist rule.\nThe title of archbishop or metropolitan may be granted to a senior bishop, usually one who is in charge of a large ecclesiastical jurisdiction. He may, or may not, have provincial oversight of suffragan bishops and may possibly have auxiliary bishops assisting him.\nApart from the ordination, which is always done by other bishops, there are different methods as to the actual selection of a candidate for ordination as bishop. In the Catholic Church the Congregation for Bishops generally oversees the selection of new bishops with the approval of the pope. The papal nuncio usually solicits names from the bishops of a country, consults with priests and leading members of a laity, and then selects three to be forwarded to the Holy See. In Europe, some cathedral chapters have duties to elect bishops. The Eastern Catholic churches generally elect their own bishops. Most Eastern Orthodox churches allow varying amounts of formalised laity or lower clergy influence on the choice of bishops. This also applies in those Eastern churches which are in union with the pope, though it is required that he give assent.\nThe pope, in addition to being the Bishop of Rome and spiritual head of the Catholic Church, is also the Patriarch of the Latin Church. Each bishop within the Latin Church is answerable directly to the Pope and not any other bishop except to metropolitans in certain oversight instances. The pope previously used the title \"Patriarch of the West\", but this title was dropped from use in 2006, a move which caused some concern within the Eastern Orthodox Communion as, to them, it implied wider papal jurisdiction.\nRecognition of other churches' ordinations.\nThe Catholic Church does recognise as valid (though illicit) ordinations done by breakaway Catholic, Old Catholic or Oriental bishops, and groups descended from them; it also regards as both valid and licit those ordinations done by bishops of the Eastern churches, so long as those receiving the ordination conform to other canonical requirements (for example, is an adult male) and an eastern orthodox rite of episcopal ordination, expressing the proper functions and sacramental status of a bishop, is used; this has given rise to the phenomenon of (for example, clergy of the Independent Catholic groups which claim apostolic succession, though this claim is rejected by both Catholicism and Eastern Orthodoxy). With respect to Lutheranism, \"the Catholic Church has never officially expressed its judgement on the validity of orders as they have been handed down by episcopal succession in these two national Lutheran churches\" (the Evangelical Lutheran Church of Sweden and the Evangelical Lutheran Church of Finland) though it does \"question how the ecclesiastical break in the 16th century has affected the apostolicity of the churches of the Reformation and thus the apostolicity of their ministry\". \nSince Pope Leo XIII issued the bull in 1896, the Catholic Church has insisted that Anglican orders are invalid because of the Reformed changes in the Anglican ordination rites of the 16th century and divergence in understanding of the theology of priesthood, episcopacy and Eucharist. However, since the 1930s, Utrecht Old Catholic bishops (recognised by the Holy See as validly ordained) have sometimes taken part in the ordination of Anglican bishops. According to the writer Timothy Dufort, by 1969, all Church of England bishops had acquired Old Catholic lines of apostolic succession recognised by the Holy See. This development has been used to argue that the strain of apostolic succession has been re-introduced into Anglicanism, at least within the Church of England. However, other issues, such as the Anglican ordination of women, is at variance with Catholic understanding of Christian teaching, and have contributed to the reaffirmation of Catholic rejection of Anglican ordinations.\nThe Eastern Orthodox Churches do not accept the validity of any ordinations performed by the Independent Catholic groups, as Eastern Orthodoxy considers to be spurious any consecration outside the church as a whole. Eastern Orthodoxy considers apostolic succession to exist only within the Universal Church, and not through any authority held by individual bishops; thus, if a bishop ordains someone to serve outside the (Eastern Orthodox) Church, the ceremony is ineffectual, and no ordination has taken place regardless of the ritual used or the ordaining prelate's position within the Eastern Orthodox Churches.\nThe position of the Catholic Church is slightly different. Whilst it does recognise the validity of the orders of certain groups which separated from communion with Holy See (for instance, the ordinations of the Old Catholics in communion with Utrecht, as well as the Polish National Catholic Church - which received its orders directly from Utrecht, and was until recently part of that communion), Catholicism does not recognise the orders of any group whose teaching is at variance with what they consider the core tenets of Christianity; this is the case even though the clergy of the Independent Catholic groups may use the proper ordination ritual. There are also other reasons why the Holy See does not recognise the validity of the orders of the Independent clergy:\nWhilst members of the Independent Catholic movement take seriously the issue of valid orders, it is highly significant that the relevant Vatican Congregations tend not to respond to petitions from Independent Catholic bishops and clergy who seek to be received into communion with the Holy See, hoping to continue in some sacramental role. In those instances where the pope does grant reconciliation, those deemed to be clerics within the Independent Old Catholic movement are invariably admitted as laity and not priests or bishops.\nThere is a mutual recognition of the validity of orders amongst Catholic, Eastern Orthodox, Old Catholic, Oriental Orthodox and Assyrian Church of the East churches.\nSome provinces of the Anglican Communion have begun ordaining women as bishops in recent decades \u2013 for example, England, Ireland, Scotland, Wales, the United States, Australia, New Zealand, Canada and Cuba. The first woman to be consecrated a bishop within Anglicanism was Barbara Harris, who was ordained in the United States in 1989. In 2006, Katharine Jefferts Schori, the Episcopal Bishop of Nevada, became the first woman to become the presiding bishop of the Episcopal Church.\nIn the Evangelical Lutheran Church in America (ELCA) and the Evangelical Lutheran Church in Canada (ELCIC), the largest Lutheran Church bodies in the United States and Canada, respectively, and roughly based on the Nordic Lutheran national churches (similar to that of the Church of England), bishops are elected by Synod Assemblies, consisting of both lay members and clergy, for a term of six years, which can be renewed, depending upon the local synod's \"constitution\" (which is mirrored on either the ELCA or ELCIC's national constitution). Since the implementation of concordats between the ELCA and the Episcopal Church of the United States and the ELCIC and the Anglican Church of Canada, all bishops, including the presiding bishop (ELCA) or the national bishop (ELCIC), have been consecrated using the historic succession in line with bishops from the Evangelical Lutheran Church of Sweden, with at least one Anglican bishop serving as co-consecrator.\nSince going into ecumenical communion with their respective Anglican body, bishops in the ELCA or the ELCIC not only approve the \"rostering\" of all ordained pastors, diaconal ministers, and associates in ministry, but they serve as the principal celebrant of all pastoral ordination and installation ceremonies, diaconal consecration ceremonies, as well as serving as the \"chief pastor\" of the local synod, upholding the teachings of Martin Luther as well as the documentations of the Ninety-Five Theses and the Augsburg Confession. Unlike their counterparts in the United Methodist Church, ELCA and ELCIC synod bishops do not appoint pastors to local congregations (pastors, like their counterparts in the Episcopal Church, are called by local congregations). The presiding bishop of the ELCA and the national bishop of the ELCIC, the national bishops of their respective bodies, are elected for a single 6-year term and may be elected to an additional term.\nAlthough ELCA agreed with the Episcopal Church to limit ordination to the bishop \"ordinarily\", ELCA pastor-\"ordinators\" are given permission to perform the rites in \"extraordinary\" circumstance. In practice, \"extraordinary\" circumstance have included disagreeing with Episcopalian views of the episcopate, and as a result, ELCA pastors ordained by other pastors are not permitted to be deployed to Episcopal Churches (they can, however, serve in Presbyterian Church USA, United Methodist Church, Reformed Church in America, and Moravian Church congregations, as the ELCA is in full communion with these denominations). The Lutheran Church\u2013Missouri Synod (LCMS) and the Wisconsin Evangelical Lutheran Synod (WELS), the second and third largest Lutheran bodies in the United States and the two largest Confessional Lutheran bodies in North America, do not follow an episcopal form of governance, settling instead on a form of quasi-congregationalism patterned off what they believe to be the practice of the early church. The second largest of the three predecessor bodies of the ELCA, the American Lutheran Church, was a congregationalist body, with national and synod presidents before they were re-titled as bishops (borrowing from the Lutheran churches in Germany) in the 1980s. With regard to ecclesial discipline and oversight, national and synod presidents typically function similarly to bishops in episcopal bodies.\nMethodism.\nAfrican Methodist Episcopal Church.\nIn the African Methodist Episcopal Church, \"Bishops are the Chief Officers of the Connectional Organization. They are elected for life by a majority vote of the General Conference which meets every four years.\"\nChristian Methodist Episcopal Church.\nIn the Christian Methodist Episcopal Church in the United States, bishops are administrative superintendents of the church; they are elected by \"delegate\" votes for as many years deemed until the age of 74, then the bishop must retire. Among their duties, are responsibility for appointing clergy to serve local churches as pastor, for performing ordinations, and for safeguarding the doctrine and discipline of the church. The General Conference, a meeting every four years, has an equal number of clergy and lay delegates. In each Annual Conference, CME bishops serve for four-year terms. CME Church bishops may be male or female.\nUnited Methodist Church.\nIn the United Methodist Church (the largest branch of Methodism in the world) bishops serve as administrative and pastoral superintendents of the church. They are elected for life from among the ordained elders (presbyters) by vote of the delegates in regional (called jurisdictional) conferences, and are consecrated by the other bishops present at the conference through the laying on of hands. In the United Methodist Church bishops remain members of the \"Order of Elders\" while being consecrated to the \"Office of the Episcopacy\". Within the United Methodist Church only bishops are empowered to consecrate bishops and ordain clergy. Among their most critical duties is the ordination and appointment of clergy to serve local churches as pastor, presiding at sessions of the Annual, Jurisdictional, and General Conferences, providing pastoral ministry for the clergy under their charge, and safeguarding the doctrine and discipline of the church. Furthermore, individual bishops, or the Council of Bishops as a whole, often serve a prophetic role, making statements on important social issues and setting forth a vision for the denomination, though they have no legislative authority of their own. In all of these areas, bishops of the United Methodist Church function very much in the historic meaning of the term. According to the \"Book of Discipline of the United Methodist Church\", a bishop's responsibilities are:\nIn each Annual Conference, United Methodist bishops serve for four-year terms, and may serve up to three terms before either retirement or appointment to a new Conference. United Methodist bishops may be male or female, with Marjorie Matthews being the first woman to be consecrated a bishop in 1980.\nThe collegial expression of episcopal leadership in the United Methodist Church is known as the Council of Bishops. The Council of Bishops speaks to the church and through the church into the world and gives leadership in the quest for Christian unity and interreligious relationships. The Conference of Methodist Bishops includes the United Methodist \"Council of Bishops\" plus bishops from affiliated autonomous Methodist or United Churches.\nJohn Wesley consecrated Thomas Coke a \"General Superintendent\", and directed that Francis Asbury also be consecrated for the United States of America in 1784, where the Methodist Episcopal Church first became a separate denomination apart from the Church of England. Coke soon returned to England, but Asbury was the primary builder of the new church. At first he did not call himself bishop, but eventually submitted to the usage by the denomination.\nNotable bishops in United Methodist history include Coke, Asbury, Richard Whatcoat, Philip William Otterbein, Martin Boehm, Jacob Albright, John Seybert, Matthew Simpson, John S. Stamm, William Ragsdale Cannon, Marjorie Matthews, Leontine T. Kelly, William B. Oden, Ntambo Nkulu Ntanda, Joseph Sprague, William Henry Willimon, and Thomas Bickerton.\nThe Church of Jesus Christ of Latter-day Saints.\nIn The Church of Jesus Christ of Latter-day Saints, the Bishop is the leader of a local congregation, called a ward. As with most LDS priesthood holders, the bishop is a part-time lay minister and earns a living through other employment. As such, it is his duty to preside, call local leaders, and judge the worthiness of members for certain activities. The bishop does not deliver sermons at every service (generally asking members to do so), but is expected to be a spiritual guide for his congregation. It is therefore believed that he has both the right and ability to receive divine inspiration (through the Holy Spirit) for the ward under his direction. Because it is a part-time position, all able members are expected to assist in the management of the ward by holding delegated lay positions (for example, women's and youth leaders, teachers) referred to as callings. The bishop is especially responsible for leading the youth, in connection with the fact that a bishop is the president of the Aaronic priesthood in his ward (and is thus a form of Mormon Kohen). Although members are asked to confess serious sins to him, unlike the Catholic Church, he is not the instrument of divine forgiveness, but merely a guide through the repentance process (and a judge in case transgressions warrant excommunication or other official discipline). The bishop is also responsible for the physical welfare of the ward, and thus collects tithing and fast offerings and distributes financial assistance where needed.\nA literal descendant of Aaron has \"legal right\" to act as a bishop after being found worthy and ordained by the First Presidency. In the absence of a literal descendant of Aaron, a high priest in the Melchizedek priesthood is called to be a bishop. Each bishop is selected from resident members of the ward by the stake presidency with approval of the First Presidency, and chooses two \"counselors\" to form a \"bishopric\". An priesthood holder called as bishop must be ordained a high priest if he is not already one, unlike the similar function of branch president. In special circumstances (such as a ward consisting entirely of young university students), a bishop may be chosen from outside the ward. Traditionally, bishops are married, though this is not always the case. A bishop is typically released after about five years and a new bishop is called to the position. Although the former bishop is released from his duties, he continues to hold the Aaronic priesthood office of bishop. Church members frequently refer to a former bishop as \"Bishop\" as a sign of respect and affection.\nLatter-day Saint bishops do not wear any special clothing or insignia the way clergy in many other churches do, but are expected to dress and groom themselves neatly and conservatively per their local culture, especially when performing official duties. Bishops (as well as other members of the priesthood) can trace their line of authority back to Joseph Smith, who, according to church doctrine, was ordained to lead the church in modern times by the ancient apostles Peter, James, and John, who were ordained to lead the Church by Jesus Christ.\nAt the global level, the presiding bishop oversees the temporal affairs (buildings, properties, commercial corporations, and so on) of the worldwide church, including the church's massive global humanitarian aid and social welfare programs. The presiding bishop has two counselors; the three together form the presiding bishopric. As opposed to ward bishoprics, where the counselors do not hold the office of bishop, all three men in the presiding bishopric hold the office of bishop, and thus the counselors, as with the presiding bishop, are formally referred to as \"Bishop\".\nIrvingism.\nNew Apostolic Church.\nThe New Apostolic Church (NAC) knows three classes of ministries: Deacons, Priests and Apostles. The Apostles, who are all included in the apostolate with the Chief Apostle as head, are the highest ministries.\nOf the several kinds of priest...ministries, the bishop is the highest. Nearly all bishops are set in line directly from the chief apostle. They support and help their superior apostle.\nPentecostalism.\nChurch of God in Christ.\nIn the Church of God in Christ (COGIC), the ecclesiastical structure is composed of large dioceses that are called \"jurisdictions\" within COGIC, each under the authority of a bishop, sometimes called \"state bishops\". They can either be made up of large geographical regions of churches or churches that are grouped and organized together as their own separate jurisdictions because of similar affiliations, regardless of geographical location or dispersion. Each state in the U.S. has at least one jurisdiction while others may have several more, and each jurisdiction is usually composed of between 30 and 100 churches. Each jurisdiction is then broken down into several districts, which are smaller groups of churches (either grouped by geographical situation or by similar affiliations) which are each under the authority of District Superintendents who answer to the authority of their jurisdictional/state bishop. There are currently over 170 jurisdictions in the United States, and over 30 jurisdictions in other countries. The bishops of each jurisdiction, according to the COGIC Manual, are considered to be the modern day equivalent in the church of the early apostles and overseers of the New Testament church, and as the highest ranking clergymen in the COGIC, they are tasked with the responsibilities of being the head overseers of all religious, civil, and economic ministries and protocol for the church denomination. They also have the authority to appoint and ordain local pastors, elders, ministers, and reverends within the denomination. The bishops of the COGIC denomination are all collectively called \"The Board of Bishops\". From the Board of Bishops, and the General Assembly of the COGIC, the body of the church composed of clergy and lay delegates that are responsible for making and enforcing the bylaws of the denomination, every four years, twelve bishops from the COGIC are elected as \"The General Board\" of the church, who work alongside the delegates of the General Assembly and Board of Bishops to provide administration over the denomination as the church's head executive leaders. One of twelve bishops of the General Board is also elected the \"presiding bishop\" of the church, and two others are appointed by the presiding bishop himself, as his first and second assistant presiding bishops.\nBishops in the Church of God in Christ usually wear black clergy suits which consist of a black suit blazer, black pants, a purple or scarlet clergy shirt and a white clerical collar, which is usually referred to as \"Class B Civic attire\". Bishops in COGIC also typically wear the Anglican Choir Dress style vestments of a long purple or scarlet chimere, cuffs, and tippet worn over a long white rochet, and a gold pectoral cross worn around the neck with the tippet. This is usually referred to as \"Class A Ceremonial attire\". The bishops of COGIC alternate between Class A Ceremonial attire and Class B Civic attire depending on the protocol of the religious services and other events they have to attend.\nChurch of God (Cleveland, Tennessee).\nIn the polity of the Church of God (Cleveland, Tennessee), the international leader is the presiding bishop, and the members of the executive committee are executive bishops. Collectively, they supervise and appoint national and state leaders across the world. Leaders of individual states and regions are administrative bishops, who have jurisdiction over local churches in their respective states and are vested with appointment authority for local pastorates. All ministers are credentialed at one of three levels of licensure, the most senior of which is the rank of ordained bishop. To be eligible to serve in state, national, or international positions of authority, a minister must hold the rank of ordained bishop.\nPentecostal Church of God.\nIn 2002, the general convention of the Pentecostal Church of God came to a consensus to change the title of their overseer from general superintendent to bishop. The change was brought on because internationally, the term \"bishop\" is more commonly related to religious leaders than the previous title.\nThe title \"bishop\" is used for both the general (international leader) and the district (state) leaders. The title is sometimes used in conjunction with the previous, thus becoming general (district) superintendent/bishop.\nSeventh-day Adventists.\nAccording to the Seventh-day Adventist understanding of the doctrine of the church:\n\"The \"elders\" (Greek, ) or \"bishops\" () were the most important officers of the church. The term elder means older one, implying dignity and respect. His position was similar to that of the one who had supervision of the synagogue. The term bishop means \"overseer\". Paul used these terms interchangeably, equating elders with overseers or bishops (Acts 20:17,; Titus 1:5, 7).\n\"Those who held this position supervised the newly formed churches. Elder referred to the status or rank of the office, while bishop denoted the duty or responsibility of the office\u2014\"overseer\". Since the apostles also called themselves elders (1 Peter 5:1; 2 John 1; 3 John 1), it is apparent that there were both local elders and itinerant elders, or elders at large. But both kinds of elder functioned as shepherds of the congregations.\"\nThe above understanding is part of the basis of Adventist organizational structure. The world wide Seventh-day Adventist church is organized into local districts, conferences or missions, union conferences or union missions, divisions, and finally at the top is the general conference. At each level (with exception to the local districts), there is an elder who is elected president and a group of elders who serve on the executive committee with the elected president. Those who have been elected president would in effect be the \"bishop\" while never actually carrying the title or ordained as such because the term is usually associated with the episcopal style of church governance most often found in Catholic, Anglican, Methodist and some Pentecostal/Charismatic circles.\nOthers.\nSome Baptists also have begun taking on the title of \"bishop\".\nIn some smaller Protestant denominations and independent churches, the term \"bishop\" is used in the same way as \"pastor\", to refer to the leader of the local congregation, and may be male or female. This usage is especially common in African-American churches in the US.\nIn the Church of Scotland, which has a Presbyterian church structure, the word \"bishop\" refers to an ordained person, usually a normal parish minister, who has temporary oversight of a trainee minister. In the Presbyterian Church (USA), the term bishop is an expressive name for a Minister of Word and Sacrament who serves a congregation and exercises \"the oversight of the flock of Christ.\" The term is traceable to the 1789 Form of Government of the PC (USA) and the Presbyterian understanding of the pastoral office.\nWhile not considered orthodox Christian, the Ecclesia Gnostica Catholica uses roles and titles derived from Christianity for its clerical hierarchy, including bishops who have much the same authority and responsibilities as in Catholicism.\nThe Salvation Army does not have bishops but has appointed leaders of geographical areas, known as Divisional Commanders. Larger geographical areas, called Territories, are led by a Territorial Commander, who is the highest-ranking officer in that Territory.\nJehovah's Witnesses do not use the title 'Bishop' within their organizational structure, but appoint elders to be overseers (to fulfill the role of oversight) within their congregations.\nThe Batak Christian Protestant Church of Indonesia, the most prominent Protestant denomination in Indonesia, uses the term \"Ephorus\" instead of \"bishop\".\nIn the Vietnamese syncretist religion of Caodaism, bishops () comprise the fifth of nine hierarchical levels, and are responsible for spiritual and temporal education as well as record-keeping and ceremonies in their parishes. At any one time there are seventy-two bishops. Their authority is described in Section I of the text (revealed through seances in December 1926). Caodai bishops wear robes and headgear of embroidered silk depicting the Divine Eye and the Eight Trigrams. (The color varies according to branch.) This is the full ceremonial dress; the simple version consists of a seven-layered turban.\nDress and insignia in Christianity.\nTraditionally, a number of items are associated with the office of a bishop, most notably the mitre and the crosier. Other vestments and insignia vary between Eastern and Western Christianity.\nIn the Latin Rite of the Catholic Church, the choir dress of a bishop includes the purple cassock with amaranth trim, rochet, purple zucchetto (skull cap), purple biretta, and pectoral cross. The cappa magna may be worn, but only within the bishop's own diocese and on especially solemn occasions. The mitre, zucchetto, and stole are generally worn by bishops when presiding over liturgical functions. For liturgical functions other than the Mass the bishop typically wears the cope. Within his own diocese and when celebrating solemnly elsewhere with the consent of the local ordinary, he also uses the crosier. When celebrating Mass, a bishop, like a priest, wears the chasuble. The Caeremoniale Episcoporum recommends, but does not impose, that in solemn celebrations a bishop should also wear a dalmatic, which can always be white, beneath the chasuble, especially when administering the sacrament of holy orders, blessing an abbot or abbess, and dedicating a church or an altar. The Caeremoniale Episcoporum no longer makes mention of episcopal gloves, episcopal sandals, liturgical stockings (also known as buskins), or the accoutrements that it once prescribed for the bishop's horse. The coat of arms of a Latin Church Catholic bishop usually displays a galero with a cross and crosier behind the escutcheon; the specifics differ by location and ecclesiastical rank (see Ecclesiastical heraldry).\nAnglican bishops generally make use of the mitre, crosier, ecclesiastical ring, purple cassock, purple zucchetto, and pectoral cross. However, the traditional choir dress of Anglican bishops retains its late mediaeval form, and looks quite different from that of their Catholic counterparts; it consists of a long rochet which is worn with a chimere.\nIn the Eastern Churches (Eastern Orthodox, Eastern Rite Catholic) a bishop will wear the mandyas, panagia (and perhaps an enkolpion), sakkos, omophorion and an Eastern-style mitre. Eastern bishops do not normally wear an episcopal ring; the faithful kiss (or, alternatively, touch their forehead to) the bishop's hand. To seal official documents, he will usually use an inked stamp. An Eastern bishop's coat of arms will normally display an Eastern-style mitre, cross, eastern style crosier and a red and white (or red and gold) mantle. The arms of Oriental Orthodox bishops will display the episcopal insignia (mitre or turban) specific to their own liturgical traditions. Variations occur based upon jurisdiction and national customs.\nCathedra.\nIn Catholic, Eastern Orthodox, Oriental Orthodox, Lutheran and Anglican cathedrals there is a special chair set aside for the exclusive use of the bishop. This is the bishop's \"cathedra\" and is often called the throne. In some Christian denominations, for example, the Anglican Communion, parish churches may maintain a chair for the use of the bishop when he visits; this is to signify the parish's union with the bishop.\nThe term's use in non-Christian religions.\nBuddhism.\nThe leader of the Buddhist Churches of America (BCA) is their bishop, The Japanese title for the bishop of the BCA is , although the English title is favored over the Japanese. When it comes to many other Buddhist terms, the BCA chose to keep them in their original language (terms such as and ), but with some words (including ), they changed/translated these terms into English words.\nBetween 1899 and 1944, the BCA held the name Buddhist Mission of North America. The leader of the Buddhist Mission of North America was called (superintendent/director) between 1899 and 1918. In 1918 the was promoted to bishop (). However, according to George J. Tanabe, the title \"bishop\" was in practice already used by Hawaiian Shin Buddhists (in Honpa Hongwanji Mission of Hawaii) even when the official title was \"kantoku\".\nBishops are also present in other Japanese Buddhist organizations. Higashi Hongan-ji's North American District, Honpa Honganji Mission of Hawaii, Jodo Shinshu Buddhist Temples of Canada, a Jodo Shu temple in Los Angeles, the Shingon temple Koyasan Buddhist Temple, S\u014dt\u014d Mission in Hawai\u2018i (a Soto Zen Buddhist institution), and the S\u014dt\u014d Zen Buddhist Community of South America () all have or have had leaders with the title bishop. As for the S\u014dt\u014d Zen Buddhist Community of South America, the Japanese title is , but the leader is in practice referred to as \"bishop\".\nTenrikyo.\nTenrikyo is a Japanese New Religion with influences from both Shinto and Buddhism. The leader of the Tenrikyo North American Mission has the title of bishop."}
{"id": "4093", "revid": "9755426", "url": "https://en.wikipedia.org/wiki?curid=4093", "title": "Bertrand Andrieu", "text": "Bertrand Andrieu (24 November 1761\u00a0\u2013 6 December 1822) was a French engraver of medals. He was born in Bordeaux. In France, he was considered as the restorer of the art, which had declined after the time of Louis XIV. During the last twenty years of his life, the French government commissioned him to undertake every major work of importance."}
{"id": "4097", "revid": "29463730", "url": "https://en.wikipedia.org/wiki?curid=4097", "title": "Bordeaux", "text": "Bordeaux (, ; ; Gascon ; ) is a city on the river Garonne in the Gironde department, southwestern France. A port city, it is the capital of the Nouvelle-Aquitaine region, as well as the prefecture of the Gironde department. Its inhabitants are called \"\"Bordelais\" (masculine) or \"\"Bordelaises\" (feminine). The term \"Bordelais\" may also refer to the city and its surrounding region.\nThe city of Bordeaux proper had a population of 259,809 in 2020 within its small municipal territory of , but together with its suburbs and exurbs the Bordeaux metropolitan area had a population of 1,376,375 that same year (Jan. 2020 census), the sixth-most populated in France after Paris, Lyon, Marseille, Lille, and Toulouse.\nBordeaux and 27 suburban municipalities form the Bordeaux Metropolis, an indirectly elected metropolitan authority now in charge of wider metropolitan issues. The Bordeaux Metropolis, with a population of 819,604 at the January 2020 census, is the fifth most populated metropolitan council in France after those of Paris, Marseille, Lyon and Lille.\nBordeaux is a world capital of wine: many ch\u00e2teaux and vineyards stand on the hillsides of the Gironde, and the city is home to the world's main wine fair, Vinexpo. Bordeaux is also one of the centers of gastronomy and business tourism for the organization of international congresses. It is a central and strategic hub for the aeronautics, military and space sector, home to major companies such as Dassault Aviation, ArianeGroup, Safran and Thales. The link with aviation dates back to 1910, the year the first airplane flew over the city. A crossroads of knowledge through university research, it is home to one of the only two megajoule lasers in the world, as well as a university population of more than 130,000 students within the Bordeaux Metropolis.\nBordeaux is an international tourist destination for its architectural and cultural heritage with more than 362 historic , making it, after Paris, the city with the most listed or registered monuments in France. The \"Pearl of Aquitaine\" has been voted European Destination of the year in a 2015 online poll. The metropolis has also received awards and rankings by international organizations such as in 1957, Bordeaux was awarded the Europe Prize for its efforts in transmitting the European ideal. In June 2007, the Port of the Moon in historic Bordeaux was inscribed on the UNESCO World Heritage List, for its outstanding architecture and urban ensemble and in recognition of Bordeaux's international importance over the last 2000 years. Bordeaux is also ranked as a Sufficiency city by the Globalization and World Cities Research Network.\nHistory.\n5th century BC to 11th century AD.\nAround 300 BC, the region was the settlement of a Celtic tribe, the Bituriges Vivisci, who named the town Burdigala, probably of Aquitanian origin.\nIn 107 BC, the Battle of Burdigala was fought by the Romans who were defending the Allobroges, a Gallic tribe allied to Rome, and the Tigurini led by Divico. The Romans were defeated and their commander, the consul Lucius Cassius Longinus, was killed in battle.\nThe city came under Roman rule around 60 BC, and it became an important commercial centre for tin and lead. During this period were built the amphitheatre and the monument \"Les Piliers de Tutelle\".\nIn 276 AD, it was sacked by the Vandals. The Vandals attacked again in 409, followed by the Visigoths in 414, and the Franks in 498, and afterwards the city fell into a period of relative obscurity.\nIn the late 6th century AD the city re-emerged as the seat of a county and an archdiocese within the Merovingian kingdom of the Franks, but royal Frankish power was never strong. The city started to play a regional role as a major urban center on the fringes of the newly founded Frankish Duchy of Vasconia. Around 585 Gallactorius was made Count of Bordeaux and fought the Basques.\nIn 732, the city was plundered by the troops of Abd er Rahman who stormed the fortifications and overwhelmed the Aquitanian garrison. Duke Eudes mustered a force to engage the Umayyads, eventually engaging them in the Battle of the River Garonne somewhere near the river Dordogne. The battle had a high death toll, and although Eudes was defeated he had enough troops to engage in the Battle of Poitiers and so retain his grip on Aquitaine.\nIn 737, following his father Eudes's death, the Aquitanian duke Hunald led a rebellion to which Charles responded by launching an expedition that captured Bordeaux. However, it was not retained for long, during the following year the Frankish commander clashed in battle with the Aquitanians but then left to take on hostile Burgundian authorities and magnates. In 745 Aquitaine faced another expedition where Charles's sons Pepin and Carloman challenged Hunald's power and defeated him. Hunald's son Waifer replaced him and confirmed Bordeaux as the capital city (along with Bourges in the north).\nDuring the last stage of the war against Aquitaine (760\u2013768), it was one of Waifer's last important strongholds to fall to the troops of King Pepin the Short. Charlemagne built the fortress of Fronsac (\"Frontiacus\", \"Franciacus\") near Bordeaux on a hill across the border with the Basques (\"Wascones\"), where Basque commanders came and pledged their loyalty (769).\nIn 778, Seguin (or Sihimin) was appointed count of Bordeaux, probably undermining the power of the Duke Lupo, and possibly leading to the Battle of Roncevaux Pass. In 814, Seguin was made Duke of Vasconia, but was deposed in 816 for failing to suppress a Basque rebellion. Under the Carolingians, sometimes the Counts of Bordeaux held the title concomitantly with that of Duke of Vasconia. They were to keep the Basques in check and defend the mouth of the Garonne from the Vikings when they appeared in c. 844. In Autumn 845, the Vikings were raiding Bordeaux and Saintes, count Seguin II marched on them but was captured and executed.\nAlthough the port of Bordeaux was a buzzing trade center, the stability and success of the city was threatened by Viking and Norman incursions and political instability. The restoration of the Ramnulfid Dukes of Aquitaine under William IV and his successors (known as the House of Poitiers) brought continuity of government.\n12th century to 15th century, the English era.\nFrom the 12th to the 15th century, Bordeaux flourished once more following the marriage of El\u00e9onore, Duchess of Aquitaine and the last of the House of Poitiers, to Henry II Plantagen\u00eat, Count of Anjou and the grandson of Henry I of England, who succeeded to the English crown months after their wedding, bringing into being the vast Angevin Empire, which stretched from the Pyrenees to Ireland. After granting a tax-free trade status with England, Henry was adored by the locals as they could be even more profitable in the wine trade, their main source of income, and the city benefited from imports of cloth and wheat. The belfry (Grosse Cloche) and city cathedral St-Andr\u00e9 were built, the latter in 1227, incorporating the artisan quarter of Saint-Paul. Under the terms of the Treaty of Br\u00e9tigny it became briefly the capital of an independent state (1362\u20131372) under Edward, the Black Prince, but after the Battle of Castillon (1453) it was annexed by France.\n15th century to 17th century.\nIn 1462, Bordeaux created a local parliament.\nBordeaux adhered to the Fronde, being effectively annexed to the Kingdom of France only in 1653, when the army of Louis XIV entered the city.\n18th century, the golden era.\nThe 18th century saw another golden age of Bordeaux. The Port of the Moon supplied the majority of Europe with coffee, cocoa, sugar, cotton and indigo, becoming France's busiest port and the second busiest port in the world after London. Many downtown buildings (about 5,000), including those on the quays, are from this period.\nBordeaux was also a major trading centre for slaves. In total, the Bordeaux shipowners deported 150,000 Africans in some 500 expeditions.\nFrench Revolution: political disruption and loss of the most profitable colony.\nAt the beginning of the French Revolution (1789), many local revolutionaries were members of the Girondists. This Party represented the provincial bourgeoisie, favorable towards abolishing aristocracy privileges, but opposed to the Revolution's social dimension. The Gironde valley's economic value and significance was satiated by the city's commercial power which was in dire contrast to the emerging widespread poverty affecting its inhabitants. Trade and commerce were the driving factors in the region's economic prosperity, still this resulted in a significant number of locals struggling to survive on a daily basis due to lack of food and resources. This socioeconomic disparity served as fertile ground for discontent, sparking frequent episodes of mass unrest well before the tumultuous events of 1783. [1]\nIn 1793, the Montagnards led by Robespierre and Marat came to power. Fearing a bourgeois misappropriation of the Revolution, they executed a great number of Girondists. During the purge, the local Montagnard Section renamed the city of Bordeaux \"Commune-Franklin\" (Franklin-municipality) in homage to Benjamin Franklin.\nAt the same time, in 1791, a slave revolt broke out at Saint-Domingue (current Haiti), the most profitable of the French colonies. In the lively era of the 18th century, Bordeaux emerged as a center of economic activity, particularly known at first for its successful wine trade. The city's placement along the Gironde River was very strategic, helping to facilitate the transportation of produce to markets both internationally and domestically, which led to an increase in exports and Bordeaux's economic prosperity. There was a significant transformation to the economic landscape of Bordeaux in 1785, which was spurred by the attraction of large profits, traders and merchants in Bordeaux began to turn their attention to the slave trade. This was a very important moment in the city's economic history seeing as it diversified its commercial expansion, at a serious moral cost. This introduced a new layer of difficulty to Bordeaux's economic activities. Even though it brought along significant wealth to certain segments of society, it complicated the socio-economic inconsistencies within the region. The entry into the slave trade brought even more tension within Bordeaux society. The trade exacerbated the divide between an elite with growing wealth and those living in poverty. This economic divide laid out the foundation for the mass unrest that would break out in the French Revolution. [2]\nThree years later, the Montagnard Convention abolished slavery. In 1802, Napoleon revoked the manumission law but lost the war against the army of former slaves. In 1804, Haiti became independent. The loss of this \"Pearl\" of the West Indies generated the collapse of Bordeaux's port economy, which was dependent on the colonial trade and trade in slaves.\nTowards the end of the Peninsular War of 1814, the Duke of Wellington sent William Beresford with two divisions and seized Bordeaux, encountering little resistance. Bordeaux was largely anti-Bonapartist and the majority supported the Bourbons. The British troops were treated as liberators. Distinguished historian of the French revolution Suzanne Desan explains that \"examining intricate local dynamics\" is essential to studying the Revolution by region. [3]\n19th century, rebirth of the economy.\nFrom the Bourbon Restoration, the economy of Bordeaux was rebuilt by traders and shipowners. They engaged to construct the first bridge of Bordeaux, and customs warehouses. The shipping traffic grew through the new African colonies.\nGeorges-Eug\u00e8ne Haussmann, a longtime prefect of Bordeaux, used Bordeaux's 18th-century large-scale rebuilding as a model when he was asked by Emperor Napoleon III to transform the quasi-medieval Paris into a \"modern\" capital that would make France proud. Victor Hugo found the town so beautiful he said: \"Take Versailles, add Antwerp, and you have Bordeaux\".\nIn 1870, at the beginning of the Franco-Prussian war against Prussia, the French government temporarily relocated to Bordeaux from Paris. That recurred during World War I and again very briefly during World War II, when it became clear that Paris would fall into German hands.\n20th century.\nDuring World War II, Bordeaux fell under German occupation.\nIn May and June 1940, Bordeaux was the site of the life-saving actions of the Portuguese consul-general, Aristides de Sousa Mendes, who illegally granted thousands of Portuguese visas, which were needed to pass the Spanish border, to refugees fleeing the German occupation.\nFrom 1941 to 1943, the Italian Royal Navy established BETASOM, a submarine base at Bordeaux. Italian submarines participated in the Battle of the Atlantic from that base, which was also a major base for German U-boats as headquarters of 12th U-boat Flotilla. The massive, reinforced concrete U-boat pens have proved impractical to demolish and are now partly used as a cultural center for exhibitions.\n21st century, listed as World heritage.\nIn 2007, 40% of the city surface area, located around the Port of the Moon, was listed as World Heritage Site. UNESCO inscribed Bordeaux as \"an inhabited historic city, an outstanding urban and architectural ensemble, created in the age of the Enlightenment, whose values continued up to the first half of the 20th century, with more protected buildings than any other French city except Paris\".\nGeography.\nBordeaux is located close to the European Atlantic coast, in the southwest of France and in the north of the Aquitaine region. It is around southwest of Paris. The city is built on a bend of the river Garonne, and is divided into two parts: the right bank to the east and left bank in the west. Historically the left bank is more developed because when flowing outside the bend, the water makes a furrow of the required depth to allow the passing of merchant ships, which used to offload on this side of the river. But, today, the right bank is developing, including new urban projects. In Bordeaux, the Garonne River is accessible to ocean liners through the Gironde estuary. The right bank of the Garonne is a low-lying, often marshy plain.\nClimate.\nBordeaux's climate can be classified as oceanic (K\u00f6ppen climate classification \"Cfb\"), bordering on a humid subtropical climate (\"Cfa\"). However, the Trewartha climate classification system classifies the city as solely humid subtropical, due to a recent rise in temperatures related \u2013 to some degree or another \u2013 to climate change and the city's urban heat island.\nThe city enjoys cool to mild, wet winters, due to its relatively southerly latitude, and the prevalence of mild, westerly winds from the Atlantic. Its summers are warm and somewhat drier, although wet enough to avoid a Mediterranean classification. Frosts occur annually, but snowfall is quite infrequent, occurring for no more than 3\u20134 days a year. The summer of 2003 set a record with an average temperature of , while February 1956 was the coldest month on record with an average temperature of \u22122.00\u00a0\u00b0C at Bordeaux M\u00e9rignac-Airport.\nEconomy.\nBordeaux is a major centre for business in France as it has the sixth largest metropolitan population in France. It serves as a major regional center for trade, administration, services and industry.\nWine.\nThe vine was introduced to the Bordeaux region by the Romans, probably in the mid-first century, to provide wine for local consumption, and wine production has been continuous in the region since.\nBordeaux wine growing area has about of vineyards, 57 appellations, 10,000 wine-producing estates (ch\u00e2teaux) and 13,000 grape growers. With an annual production of approximately 960\u00a0million bottles, the Bordeaux area produces large quantities of everyday wine as well as some of the most expensive wines in the world. Included among the latter are the area's five \"premier cru\" (First Growth) red wines (four from M\u00e9doc and one, Ch\u00e2teau Haut-Brion, from Graves), established by the Bordeaux Wine Official Classification of 1855:\nBoth red and white wines are made in the Bordeaux region. Red Bordeaux wine is called claret in the United Kingdom. Red wines are generally made from a blend of grapes, and may be made from Cabernet Sauvignon, Merlot, Cabernet Franc, Petit verdot, Malbec, and, less commonly in recent years, Carm\u00e9n\u00e8re.\nWhite Bordeaux is made from Sauvignon blanc, S\u00e9millon, and Muscadelle. Sauternes is a sub-region of Graves known for its intensely sweet, white, dessert wines such as Ch\u00e2teau d'Yquem.\nBecause of a wine glut (wine lake) in the generic production, the price squeeze induced by an increasingly strong international competition, and vine pull schemes, the number of growers has recently dropped from 14,000 and the area under vine has also decreased significantly. In the meantime, the global demand for first growths and the most famous labels markedly increased and their prices skyrocketed.\nThe Cit\u00e9 du Vin, a museum as well as a place of exhibitions, shows, movie projections and academic seminars on the theme of wine opened its doors in June 2016.\nOthers.\nThe Laser M\u00e9gajoule will be one of the most powerful lasers in the world, allowing fundamental research and the development of the laser and plasma technologies.\nSome 15,000 people work for the aeronautic industry in Bordeaux. The city has some of the biggest companies including Dassault, EADS Sogerma, Snecma, Thales, SNPE, and others. The Dassault Falcon private jets are built there as well as the military aircraft Rafale and Mirage 2000, the Airbus A380 cockpit, the boosters of Ariane 5, and the M51 SLBM missile.\nTourism, especially wine tourism, is a major industry. Globelink.co.uk mentioned Bordeaux as the best tourist destination in Europe in 2015. Gourmet Touring is a tourism company operating in the Bordeaux wine region.\nAccess to the port from the Atlantic is via the Gironde estuary. Almost nine million tonnes of goods arrive and leave each year.\nMajor companies.\nThis list includes indigenous Bordeaux-based companies and companies that have major presence in Bordeaux, but are not necessarily headquartered there.\nPopulation.\nIn January 2020, there were 259,809 inhabitants in the city proper (commune) of Bordeaux. The commune (including Caud\u00e9ran which was annexed by Bordeaux in 1965) had its largest population of 284,494 at the 1954 census. The majority of the population is French, but there are sizable groups of Italians, Spaniards (Up to 20% of the Bordeaux population claim some degree of Spanish heritage), Portuguese, Turks, Germans.\nThe built-up area has grown for more than a century beyond the municipal borders of Bordeaux due to the small size of the commune () and urban sprawl. By January 2020 there were 1,376,375 people living in the overall metropolitan area (\"aire d'attraction\") of Bordeaux, only a fifth of whom lived in the city proper.\nPolitics.\nMunicipal administration.\nThe Mayor of the city is the environmentalist Pierre Hurmic.\nBordeaux is the capital of five cantons and the Prefecture of the Gironde and Aquitaine.\nThe town is divided into three districts, the first three of Gironde. The headquarters of Urban Community of Bordeaux M\u00e9riadeck is located in the neighbourhood and the city is at the head of the Chamber of Commerce and Industry that bears his name.\nThe number of inhabitants of Bordeaux is greater than 250,000 and less than 299,999 so the number of municipal councilors is 65. They are divided according to the following composition:\nMayors of Bordeaux.\nSince the Liberation (1944), there have been six mayors of Bordeaux:\nElections.\nPresidential elections of 2007.\nAt the 2007 presidential election, the Bordelais gave 31.37% of their votes to S\u00e9gol\u00e8ne Royal of the Socialist Party against 30.84% to Nicolas Sarkozy, president of the UMP. Then came Fran\u00e7ois Bayrou with 22.01%, followed by Jean-Marie Le Pen who recorded 5.42%. None of the other candidates exceeded the 5% mark. Nationally, Nicolas Sarkozy led with 31.18%, then S\u00e9gol\u00e8ne Royal with 25.87%, followed by Fran\u00e7ois Bayrou with 18.57%. After these came Jean-Marie Le Pen with 10.44%, none of the other candidates exceeded the 5% mark. In the second round, the city of Bordeaux gave S\u00e9gol\u00e8ne Royal 52.44% against 47.56% for Nicolas Sarkozy, the latter being elected President of the Republic with 53.06% against 46.94% for S\u00e9gol\u00e8ne Royal. The abstention rates for Bordeaux were 14.52% in the first round and 15.90% in the second round.\nParliamentary elections of 2007.\nIn the parliamentary elections of 2007, the left won eight constituencies against only three for the right. After the partial 2008 elections, the eighth district of Gironde switched to the left, bringing the count to nine. In Bordeaux, the left was for the first time in its history the majority as it held two of three constituencies following the elections. In the first division of the Gironde, the outgoing UMP MP Chantal Bourragu\u00e9 was well ahead with 44.81% against 25.39% for the Socialist candidate B\u00e9atrice Desaigues. In the second round, it was Chantal Bourragu\u00e9 who was re-elected with 54.45% against 45.55% for his socialist opponent. In the second district of Gironde the UMP mayor and all new Minister of Ecology, Energy, Sustainable Development and the Sea Alain Jupp\u00e9 confronted the General Counsel PS Mich\u00e8le Delaunay. In the first round, Alain Jupp\u00e9 was well ahead with 43.73% against 31.36% for Mich\u00e8le Delaunay. In the second round, it was finally Mich\u00e8le Delaunay who won the election with 50.93% of the votes against 49.07% for Alain Jupp\u00e9, the margin being only 670 votes. The defeat of the so-called constituency \"Mayor\" showed that Bordeaux was rocking increasingly left. Finally, in the third constituency of the Gironde, No\u00ebl Mam\u00e8re was well ahead with 39.82% against 28.42% for the UMP candidate Elizabeth Vine. In the second round, No\u00ebl Mam\u00e8re was re-elected with 62.82% against 37.18% for his right-wing rival.\nMunicipal elections of 2008.\nIn 2008 municipal elections saw the clash between mayor of Bordeaux, Alain Jupp\u00e9 and the President of the Regional Council of Aquitaine Socialist Alain Rousset. The PS had put up a Socialist heavyweight in the Gironde and had put great hopes in this election after the victory of S\u00e9gol\u00e8ne Royal and Mich\u00e8le Delaunay in 2007. However, after a rather exciting campaign it was Alain Jupp\u00e9 who was widely elected in the first round with 56.62 percent, far ahead of Alain Rousset who garnered 34.14 percent of the vote. At present, of the eight cantons that has Bordeaux, five are held by the PS and three by the UMP, the left eating a little each time into the right's numbers.\nEuropean elections of 2009.\nIn the European elections of 2009, Bordeaux voters largely voted for the UMP candidate Dominique Baudis, who won 31.54% against 15.00% for PS candidate Kader Arif. The candidate of Europe Ecology Jos\u00e9 Bov\u00e9 came second with 22.34%. None of the other candidates reached the 10% mark. The 2009 European elections were like the previous ones in eight constituencies. Bordeaux is located in the district \"Southwest\", here are the results:\nUMP candidate Dominique Baudis: 26.89%. His party gained four seats. PS candidate Kader Arif: 17.79%, gaining two seats in the European Parliament. Europe Ecology candidate Bove: 15.83%, obtaining two seats. MoDem candidate Robert Rochefort: 8.61%, winning a seat. Left Front candidate Jean-Luc M\u00e9lenchon: 8.16%, gaining the last seat. At regional elections in 2010, the Socialist incumbent president Alain Rousset won the first round by totaling 35.19% in Bordeaux, but this score was lower than the plan for Gironde and Aquitaine. Xavier Darcos, Minister of Labour followed with 28.40% of the votes, scoring above the regional and departmental average. Then came Monique De Marco, Green candidate with 13.40%, followed by the member of Pyrenees-Atlantiques and candidate of the MoDem Jean Lassalle who registered a low 6.78% while qualifying to the second round on the whole Aquitaine, closely followed by Jacques Colombier, candidate of the National Front, who gained 6.48%. Finally the candidate of the Left Front G\u00e9rard Boulanger with 5.64%, no other candidate above the 5% mark. In the second round, Alain Rousset had a tidal wave win as national totals rose to 55.83%. If Xavier Darcos largely lost the election, he nevertheless achieved a score above the regional and departmental average obtaining 33.40%. Jean Lassalle, who qualified for the second round, passed the 10% mark by totaling 10.77%. The ballot was marked by abstention amounting to 55.51% in the first round and 53.59% in the second round.\n2017 elections.\nBordeaux voted for Emmanuel Macron in the presidential election. In the 2017 parliamentary election, La R\u00e9publique En Marche! won most of the constituencies in Bordeaux.\n2019 European elections.\nBordeaux voted in the 2019 European Parliament election in France.\nMunicipal elections of 2020.\nAfter 73 years of right-of-centre rule, the ecologist Pierre Hurmic (EELV) came in ahead of Nicolas Florian (LR/LaREM).\nParliamentary representation.\nThe city area is represented by the following constituencies: Gironde's 1st, Gironde's 2nd, Gironde's 3rd, Gironde's 4th, Gironde's 5th, Gironde's 6th, Gironde's 7th.\nEducation.\nUniversity.\nDuring Antiquity, a first university had been created by the Romans in 286. The city was an important administrative centre and the new university had to train administrators. Only rhetoric and grammar were taught. Ausonius and Sulpicius Severus were two of the teachers.\nIn 1441, when Bordeaux was an English town, the Pope Eugene IV created a university by demand of the archbishop Pey Berland. In 1793, during the French Revolution, the National Convention abolished the university, and replace them with the \u00c9cole centrale in 1796. In Bordeaux, this one was located in the former buildings of the college of Guyenne. In 1808, the university reappeared with Napoleon. Bordeaux accommodates approximately 70,000 students on one of the largest campuses of Europe (235\u00a0ha).\nSchools.\nBordeaux has numerous public and private schools offering undergraduate and postgraduate programs.\nEngineering schools:\nBusiness and management schools:\nOther:\nWeekend education.\nThe , a part-time Japanese supplementary school, is held in the \"Salle de L'Ath\u00e9n\u00e9e Municipal\" in Bordeaux.\nAttractions and tourism.\nIn October 2021, Bordeaux was shortlisted for the European Commission's 2022 European Capital of Smart Tourism award along with Copenhagen, Dublin, Florence, Ljubljana, Palma de Mallorca, and Valencia.\nHeritage and architecture.\nBordeaux is classified \"City of Art and History\". The city is home to 362 \"monuments historiques\" (national heritage sites), with some buildings dating back to Roman times. Bordeaux, Port of the Moon, has been inscribed on UNESCO World Heritage List as \"an outstanding urban and architectural ensemble\".\nBordeaux is home to one of Europe's biggest 18th-century architectural urban areas, making it a sought-after destination for tourists and cinema production crews. It stands out as one of the first French cities, after Nancy, to have entered an era of urbanism and metropolitan big scale projects, with the team Gabriel father and son, architects for King Louis XV, under the supervision of two intendants (Governors), first Nicolas-Fran\u00e7ois Dupr\u00e9 de Saint-Maur then the Marquis de Tourny.\nSaint-Andr\u00e9 Cathedral, Saint-Michel Basilica and Saint-Seurin Basilica are part of the World Heritage Sites of the Routes of Santiago de Compostela in France. The organ in Saint-Louis-des-Chartrons is registered on the French monuments historiques.\nNotable historic buildings include:\nContemporary buildings in contemporary architectural style include:\nSlavery memorials.\nSlavery was part of a growing drive for the city. During the 18th and 19th centuries, Bordeaux was an important slave port, which saw some 500 slave expeditions that cause the deportation of 150,000 Africans by Bordeaux shipowners. Secondly, even though the \"Triangular trade\" represented only 5% of Bordeaux's wealth, the city's direct trade with the Caribbean, that accounted for the other 95%, concerns the colonial stuffs made by the slave (sugar, coffee, cocoa). And thirdly, in that same period, a major migratory movement by Aquitanians took place to the Caribbean colonies, with Saint-Domingue (now Haiti) being the most popular destination. 40% of the white population of the island came from Aquitaine. They prospered with plantations incomes, until the first slave revolts which concluded in 1848 in the final abolition of slavery in France.\nA statue of Modeste Testas, an Ethiopian woman who was enslaved by the Bordeaux-based Testas brothers was unveiled in 2019. She was trafficked by them from West Africa, to Philadelphia (where one of the brothers coerced her to have two children by him) and was ultimately freed and lived in Haiti. The bronze sculpture was created by the Haitian artists Woodly Caymitte.\nA number of traces and memorial sites are visible in the city. Moreover, in May 2009, the Museum of Aquitaine opened the spaces dedicated to \"Bordeaux in the 18th century, trans-Atlantic trading and slavery\". This work, richly illustrated with original documents, contributes to disseminate the state of knowledge on this question, presenting above all the facts and their chronology.\nThe region of Bordeaux was also the land of several prominent abolitionists, as Montesquieu, Laffon de Lad\u00e9bat and Elis\u00e9e Reclus. Others were members of the Society of the Friends of the Blacks as the revolutionaries Boyer-Fonfr\u00e8de, Gensonn\u00e9, Guadet and Ducos.\nPont Jacques Chaban-Delmas.\nEurope's longest-span vertical-lift bridge, the Pont Jacques Chaban-Delmas, was opened in 2013 in Bordeaux, spanning the River Garonne. The central lift span is , weighs 4,600 tons and can be lifted vertically up to to let tall ships pass underneath. The \u20ac160\u00a0million bridge was inaugurated by President Fran\u00e7ois Hollande and Mayor Alain Jupp\u00e9 on 16 March 2013. The bridge was named after the late Jacques Chaban-Delmas, who was a former Prime Minister and Mayor of Bordeaux.\nShopping.\nBordeaux has many shopping options. In the heart of Bordeaux is Rue Sainte-Catherine. This pedestrianised street has of shops, restaurants and caf\u00e9s; it is also one of the longest shopping streets in Europe. Rue Sainte-Catherine starts at Place de la Victoire and ends at Place de la Com\u00e9die by the Grand Th\u00e9\u00e2tre. The shops become progressively more upmarket as one moves towards Place de la Com\u00e9die and the nearby Cours de l'Intendance is where there are the more exclusive shops and boutiques.\nCulture.\nBordeaux is the first city in France to have created, in the 1980s, an architecture exhibition and research centre, \"Arc en r\u00eave\". \nThe city has a large number of cinemas, theatres, and is the home of the Op\u00e9ra national de Bordeaux. There are many music venues of varying capacity. The city also offers several festivals throughout the year. \nThe Bordeaux International Festival of Women in Cinema (Festival international du cin\u00e9ma au f\u00e9minin de Bordeaux) took place in Bordeaux from 2002 until 2005. The Festival international du film ind\u00e9pendant de Bordeaux (Fifib or FIFIB), or Bordeaux International Independent Film Festival, was established in 2012. \nTransport.\nRoad.\nBordeaux is an important road and motorway junction. The city is connected to Paris by the A10 motorway, with Lyon by the A89, with Toulouse by the A62, and with Spain by the A63. There is a ring road called the \"Rocade\" which is often very busy. Another ring road is under consideration.\nBordeaux has five road bridges that cross the Garonne, the Pont de pierre built in the 1820s and three modern bridges built after 1960: the Pont Saint Jean, just south of the Pont de pierre (both located downtown), the Pont d'Aquitaine, a suspension bridge downstream from downtown, and the Pont Fran\u00e7ois Mitterrand, located upstream of downtown. These two bridges are part of the ring-road around Bordeaux. A fifth bridge, the Pont Jacques-Chaban-Delmas, was constructed in 2009\u20132012 and opened to traffic in March 2013. Located halfway between the Pont de pierre and the Pont d'Aquitaine and serving downtown rather than highway traffic, it is a vertical-lift bridge with a height in closed position comparable to that of Pont de pierre, and to the Pont d'Aquitaine when open. All five road bridges, including the two highway bridges, are open to cyclists and pedestrians as well. Another bridge, the Pont Jean-Jacques Bosc, is to be built in 2018.\nLacking any steep hills, Bordeaux is relatively friendly to cyclists. Cycle paths (separate from the roadways) exist on the highway bridges, along the riverfront, on the university campuses, and incidentally elsewhere in the city. Cycle lanes and bus lanes that explicitly allow cyclists exist on many of the city's boulevards. A paid bicycle-sharing system with automated stations was established in 2010.\nRail.\nThe main railway station, Gare de Bordeaux Saint-Jean, near the center of the city, has 12\u00a0million passengers a year. It is served by the French national (SNCF) railway's high speed train, the TGV, that gets to Paris in two hours, with connections to major European centers such as Lille, Brussels, Amsterdam, Cologne, Geneva and London. The TGV also serves Toulouse and Irun (Spain) from Bordeaux. A regular train service is provided to Nantes, Nice, Marseille and Lyon. The Gare Saint-Jean is the major hub for regional trains (TER) operated by the SNCF to Arcachon, Limoges, Agen, P\u00e9rigueux, Langon, Pau, Le M\u00e9doc, Angoul\u00eame and Bayonne.\nHistorically the train line used to terminate at a station on the right bank of the river Garonne near the Pont de Pierre, and passengers crossed the bridge to get into the city. Subsequently, a double-track steel railway bridge was constructed in the 1850s, by Gustave Eiffel, to bring trains across the river direct into Gare de Bordeaux Saint-Jean. The old station was later converted and in 2010 comprised a cinema and restaurants.\nThe two-track Eiffel bridge with a speed limit of became a bottleneck and a new bridge was built, opening in 2009. The new bridge has four tracks and allows trains to pass at . During the planning there was much lobbying by the Eiffel family and other supporters to preserve the old bridge as a footbridge across the Garonne, with possibly a museum to document the history of the bridge and Gustave Eiffel's contribution. The decision was taken to save the bridge, but by early 2010 no plans had been announced as to its future use. The bridge remains intact, but unused and without any means of access.\nThe LGV Sud Europe Atlantique became fully operational in July 2017, shortening the journey time from Bordeaux city to Paris to 2hrs 4mins.\nAir.\nBordeaux is served by Bordeaux\u2013M\u00e9rignac Airport, located from the city centre in the suburban city of M\u00e9rignac.\nTrams, buses and boats.\nBordeaux has an important public transport system called Transports Bordeaux M\u00e9tropole (TBM). This company is run by the Keolis group. The network consists of:\nThis network is operated from 5\u00a0am to 2\u00a0am.\nThere had been several plans for a subway network to be set up, but they stalled for both geological and financial reasons. Work on the Tramway de Bordeaux system was started in the autumn of 2000, and services started in December 2003 connecting Bordeaux with its suburban areas. The tram system uses Alstom APS a form of ground-level power supply technology developed by French company Alstom and designed to preserve the aesthetic environment by eliminating overhead cables in the historic city. Conventional overhead cables are used outside the city. The system was controversial for its considerable cost of installation, maintenance and also for the numerous initial technical problems that paralysed the network. Many streets and squares along the tramway route became pedestrian areas, with limited access for cars.\nThe Bordeaux Tramway system reached the M\u00e9rignac airport on April 29th 2023 with the opening of a 5-km extension of Line A.\nTaxis.\nThere are more than 400 taxicabs in Bordeaux.\nPublic transportation statistics.\nThe average amount of time people spend commuting with public transit in Bordeaux, for example to and from work, on a weekday is 51 min. 12.% of public transit riders, ride for more than 2 hours every day. The average amount of time people wait at a stop or station for public transit is 13 min, while 15.5% of riders wait for over 20 minutes on average every day. The average distance people usually ride in a single trip with public transit is , while 8% travel for over in a single direction.\nSport.\nThe 41,458-capacity Nouveau Stade de Bordeaux is the largest stadium in Bordeaux. The stadium was opened in 2015 and replaced the Stade Chaban-Delmas, which was a venue for the FIFA World Cup in 1938 and 1998, as well as the 2007 Rugby World Cup. In the 1938 FIFA World Cup, it hosted a violent quarter-final known as the Battle of Bordeaux. The ground was formerly known as the \"Stade du Parc Lescure\" until 2001, when it was renamed in honour of the city's long-time mayor, Jacques Chaban-Delmas.\nThere are two major sport teams in Bordeaux, Girondins de Bordeaux is the football team who, following administrative relegation, currently play in Championnat National 2, the fourth tier of French football. They are one of the most successful clubs in France, with six Division 1/Ligue 1 titles. Union Bordeaux B\u00e8gles is a rugby team in the Top 14 in the Ligue Nationale de Rugby. Skateboarding, rollerblading, and BMX biking are activities enjoyed by many young inhabitants of the city. Bordeaux is home to a quay which runs along the Garonne river. On the quay there is a skate-park divided into three sections. One section is for Vert tricks, one for street style tricks, and one for little action sports athletes with easier features and softer materials. The skate-park is very well maintained by the municipality.\nOther sports clubs include top flight ice hockey team Boxers de Bordeaux and third-tier basketball team JSA Bordeaux Basket\nBordeaux is also the home to one of the strongest cricket teams in France and are champions of the South West League.\nThere is a wooden velodrome, V\u00e9lodrome du Lac, in Bordeaux which hosts international cycling competition in the form of UCI Track Cycling World Cup events.\nThe 2015 Trophee Eric Bompard was in Bordeaux. But the Free Skate was cancelled in all of the divisions due to the Paris and aftermath. The Short Program occurred hours before the bombing. French skaters Chafik Besseghier (68.36) in tenth place, Romain Ponsart (62.86) in 11th. Mae-Berenice-Meite (46.82) in 11th and Laurine Lecavelier (46.53) in 12th. Vanessa James/Morgan Cipres (65.75) in second.\nBetween 1951 and 1955, an annual Formula 1 motor race was held on a 2.5-kilometre circuit which looped around the Esplanade des Quinconces and along the waterfront, attracting drivers such as Juan Manuel Fangio, Stirling Moss, Jean Behra and Maurice Trintignant.\nInternational relationships.\nTwin towns \u2013 sister cities.\nBordeaux is twinned with:"}
{"id": "4098", "revid": "47015620", "url": "https://en.wikipedia.org/wiki?curid=4098", "title": "Puzzle Bobble", "text": " internationally known as Bust-A-Move, is a 1994 tile-matching puzzle arcade game developed and published by Taito. It is based on the 1986 arcade game \"Bubble Bobble\", featuring characters and themes from that game. Its characteristically cute Japanese animation and music, along with its play mechanics and level designs, made it successful as an arcade title and spawned several sequels and ports to home gaming systems.\nGameplay.\nAt the start of each round, the rectangular playing arena contains a prearranged pattern of colored \"bubbles\". At the bottom of the screen, the player controls a device called a \"pointer\", which aims and fires bubbles up the screen. The color of bubbles fired is randomly generated and chosen from the colors of bubbles still left on the screen.\nThe objective of the game is to clear all the bubbles from the arena without any bubble crossing the bottom line. Bubbles will fire automatically if the player remains idle. After clearing the arena, the next round begins with a new pattern of bubbles to clear. The arcade version of the game consists of 30 levels. The fired bubbles travel in straight lines (possibly bouncing off the sidewalls of the arena), stopping when they touch other bubbles or reach the top of the arena. If a bubble touches identically colored bubbles, forming a group of three or more, those bubbles\u2014as well as any bubbles hanging from them\u2014are removed from the field of play, and points are awarded. After every few shots, the \"ceiling\" of the playing arena drops downwards slightly, along with all the bubbles stuck to it. The number of shots between each drop of the ceiling is influenced by the number of bubble colors remaining. The closer the bubbles get to the bottom of the screen, the faster the music plays and if they cross the line at the bottom then the game is over.\nRelease.\nTwo different versions of the original game were released. \"Puzzle Bobble\" was originally released in Japan only in June 1994 by Taito, running on Taito B System hardware (with the preliminary title \"Bubble Buster\"). Then, six months later in December, the international Neo Geo version of \"Puzzle Bobble\" was released. It was almost identical aside from being in stereo and having some different sound effects and translated text.\nReception.\nIn Japan, \"Game Machine\" listed the Neo Geo version of \"Puzzle Bobble\" on their February 15, 1995 issue as being the second most-popular arcade game at the time. It went on to become Japan's second highest-grossing arcade printed circuit board (PCB) software of 1995, below \"Virtua Fighter 2\". In North America, \"RePlay\" reported the Neo Geo version of \"Puzzle Bobble\" to be the fourth most-popular arcade game in February 1995.\nReviewing the Super NES version, Mike Weigand of \"Electronic Gaming Monthly\" called it \"a thoroughly enjoyable and incredibly addicting puzzle game\". He considered the two player mode the highlight, but also said that the one player mode provides a solid challenge. \"GamePro\" gave it a generally negative review, saying it starts out fun but that ultimately lacks intricacy and longevity. They elaborated that in one player mode all the levels feel the same, and that two player matches are over too quickly to build up any excitement. They also criticized the lack of any 3D effects in the graphics. \"Next Generation\" reviewed the SNES version of the game and called it \"addictive as hell\".\nA reviewer for \"Next Generation\", while questioning the continued viability of the action puzzle genre, admitted that the game is \"very simple and \"very\" addictive\". He remarked that though the 3DO version makes no significant additions, none are called for by a game with such simple enjoyment. \"GamePro\"s brief review of the 3DO version commented that the game's controls are responsive, and they also praised visuals and music. \"Edge\" magazine ranked the game 73rd on their 100 Best Video Games in 2007. \"IGN\" rated the SNES version 54th in its Top 100 SNES Games.\nLegacy.\nThe simplicity of the concept has led to many clones, both commercial and otherwise. 1996's \"Snood\" replaced the bubbles with small creatures and has been successful in its own right. \"Worms Blast\" was Team 17's take on the concept. On September 24, 2000, British game publisher Empire Interactive released a similar game, \"Spin Jam\", for the original PlayStation console. Mobile clones include \"Bubble Witch Saga\" and \"Bubble Shooter\". \"Frozen Bubble\" is a free software clone. For \"Bubble Bobble\"s 35th anniversary, Taito launched \"Puzzle Bobble VR: Vacation Odyssey\" on the Oculus Quest and Oculus Quest 2, later coming to PlayStation 4 and PlayStation 5 as \"Puzzle Bobble 3D: Vacation Odyssey\" in 2021.\"\n\"Puzzle Bobble Everybubble!\".\n\"Puzzle Bobble Everybubble!\" was released on May 23, 2023, for Nintendo Switch. The game also comes with an extra mode called \"\"Puzzle Bobble\" vs. \"Space Invaders\"\", where up to four players can work together to erase bubble-encased invaders before they reach the player while only being able to aim straight up."}
{"id": "4099", "revid": "49032093", "url": "https://en.wikipedia.org/wiki?curid=4099", "title": "Bone", "text": "A bone is a rigid organ that constitutes part of the skeleton in most vertebrate animals. Bones protect the various other organs of the body, produce red and white blood cells, store minerals, provide structure and support for the body, and enable mobility. Bones come in a variety of shapes and sizes and have complex internal and external structures. They are lightweight yet strong and hard and serve multiple functions.\nBone tissue (osseous tissue), which is also called bone in the uncountable sense of that word, is hard tissue, a type of specialised connective tissue. It has a honeycomb-like matrix internally, which helps to give the bone rigidity. Bone tissue is made up of different types of bone cells. Osteoblasts and osteocytes are involved in the formation and mineralisation of bone; osteoclasts are involved in the resorption of bone tissue. Modified (flattened) osteoblasts become the lining cells that form a protective layer on the bone surface. The mineralised matrix of bone tissue has an organic component of mainly collagen called \"ossein\" and an inorganic component of bone mineral made up of various salts. Bone tissue is mineralized tissue of two types, cortical bone and cancellous bone. Other types of tissue found in bones include bone marrow, endosteum, periosteum, nerves, blood vessels and cartilage.\nIn the human body at birth, approximately 300 bones are present. Many of these fuse together during development, leaving a total of 206 separate bones in the adult, not counting numerous small sesamoid bones. The largest bone in the body is the femur or thigh-bone, and the smallest is the \"stapes\" in the middle ear.\nThe Greek word for bone is \u1f40\u03c3\u03c4\u03ad\u03bf\u03bd (\"osteon\"), hence the many terms that use it as a prefix\u2014such as osteopathy. In anatomical terminology, including the \"Terminologia Anatomica\" international standard, the word for a bone is \"os\" (for example, \"os breve\", \"os longum\", \"os sesamoideum\").\nStructure.\nBone is not uniformly solid, but consists of a flexible matrix (about 30%) and bound minerals (about 70%), which are intricately woven and continuously remodeled by a group of specialized bone cells. Their unique composition and design allows bones to be relatively hard and strong, while remaining lightweight.\nBone matrix is 90 to 95% composed of elastic collagen fibers, also known as ossein, and the remainder is ground substance. The elasticity of collagen improves fracture resistance. The matrix is hardened by the binding of inorganic mineral salt, calcium phosphate, in a chemical arrangement known as bone mineral, a form of calcium apatite. It is the mineralization that gives bones rigidity.\nBone is actively constructed and remodeled throughout life by specialized bone cells known as osteoblasts and osteoclasts. Within any single bone, the tissue is woven into two main patterns: cortical and cancellous bone, each with a distinct appearance and characteristics.\nCortex.\nThe hard outer layer of bones is composed of cortical bone, which is also called compact bone as it is much denser than cancellous bone. It forms the hard exterior (cortex) of bones. The cortical bone gives bone its smooth, white, and solid appearance, and accounts for 80% of the total bone mass of an adult human skeleton. It facilitates bone's main functions\u2014to support the whole body, to protect organs, to provide levers for movement, and to store and release chemical elements, mainly calcium. It consists of multiple microscopic columns, each called an osteon or Haversian system. Each column is multiple layers of osteoblasts and osteocytes around a central canal called the osteonic canal. Volkmann's canals at right angles connect the osteons together. The columns are metabolically active, and as bone is reabsorbed and created the nature and location of the cells within the osteon will change. Cortical bone is covered by a periosteum on its outer surface, and an endosteum on its inner surface. The endosteum is the boundary between the cortical bone and the cancellous bone. The primary anatomical and functional unit of cortical bone is the osteon.\nTrabeculae.\nCancellous bone or spongy bone, also known as trabecular bone, is the internal tissue of the skeletal bone and is an open cell porous network that follows the material properties of biofoams. Cancellous bone has a higher surface-area-to-volume ratio than cortical bone and it is less dense. This makes it weaker and more flexible. The greater surface area also makes it suitable for metabolic activities such as the exchange of calcium ions. Cancellous bone is typically found at the ends of long bones, near joints, and in the interior of vertebrae. Cancellous bone is highly vascular and often contains red bone marrow where hematopoiesis, the production of blood cells, occurs. The primary anatomical and functional unit of cancellous bone is the trabecula. The trabeculae are aligned towards the mechanical load distribution that a bone experiences within long bones such as the femur. As far as short bones are concerned, trabecular alignment has been studied in the vertebral pedicle. Thin formations of osteoblasts covered in endosteum create an irregular network of spaces, known as trabeculae. Within these spaces are bone marrow and hematopoietic stem cells that give rise to platelets, red blood cells and white blood cells. Trabecular marrow is composed of a network of rod- and plate-like elements that make the overall organ lighter and allow room for blood vessels and marrow. Trabecular bone accounts for the remaining 20% of total bone mass but has nearly ten times the surface area of compact bone.\nThe words \"cancellous\" and \"trabecular\" refer to the tiny lattice-shaped units (trabeculae) that form the tissue. It was first illustrated accurately in the engravings of Cris\u00f3stomo Martinez.\nMarrow.\nBone marrow, also known as myeloid tissue in red bone marrow, can be found in almost any bone that holds cancellous tissue. In newborns, all such bones are filled exclusively with red marrow or hematopoietic marrow, but as the child ages the hematopoietic fraction decreases in quantity and the fatty/ yellow fraction called marrow adipose tissue (MAT) increases in quantity. In adults, red marrow is mostly found in the bone marrow of the femur, the ribs, the vertebrae and pelvic bones.\nVascular supply.\nBone receives about 10% of cardiac output. Blood enters the endosteum, flows through the marrow, and exits through small vessels in the cortex. In humans, blood oxygen tension in bone marrow is about 6.6%, compared to about 12% in arterial blood, and 5% in venous and capillary blood.\nCells.\nBone is metabolically active tissue composed of several types of cells. These cells include osteoblasts, which are involved in the creation and mineralization of bone tissue, osteocytes, and osteoclasts, which are involved in the reabsorption of bone tissue. Osteoblasts and osteocytes are derived from osteoprogenitor cells, but osteoclasts are derived from the same cells that differentiate to form macrophages and monocytes. Within the marrow of the bone there are also hematopoietic stem cells. These cells give rise to other cells, including white blood cells, red blood cells, and platelets.\nOsteoblast.\nOsteoblasts are mononucleate bone-forming cells. They are located on the surface of osteon seams and make a protein mixture known as osteoid, which mineralizes to become bone. The osteoid seam is a narrow region of a newly formed organic matrix, not yet mineralized, located on the surface of a bone. Osteoid is primarily composed of Type I collagen. Osteoblasts also manufacture hormones, such as prostaglandins, to act on the bone itself. The osteoblast creates and repairs new bone by actually building around itself. First, the osteoblast puts up collagen fibers. These collagen fibers are used as a framework for the osteoblasts' work. The osteoblast then deposits calcium phosphate which is hardened by hydroxide and bicarbonate ions. The brand-new bone created by the osteoblast is called osteoid. Once the osteoblast is finished working it is actually trapped inside the bone once it hardens. When the osteoblast becomes trapped, it becomes known as an osteocyte. Other osteoblasts remain on the top of the new bone and are used to protect the underlying bone, these become known as bone lining cells.\nOsteocyte.\nOsteocytes are cells of mesenchymal origin and originate from osteoblasts that have migrated into and become trapped and surrounded by a bone matrix that they themselves produced. The spaces the cell body of osteocytes occupy within the mineralized collagen type I matrix are known as lacunae, while the osteocyte cell processes occupy channels called canaliculi. The many processes of osteocytes reach out to meet osteoblasts, osteoclasts, bone lining cells, and other osteocytes probably for the purposes of communication. Osteocytes remain in contact with other osteocytes in the bone through gap junctions\u2014coupled cell processes which pass through the canalicular channels.\nOsteoclast.\nOsteoclasts are very large multinucleate cells that are responsible for the breakdown of bones by the process of bone resorption. New bone is then formed by the osteoblasts. Bone is constantly remodeled by the resorption of osteoclasts and created by osteoblasts. Osteoclasts are large cells with multiple nuclei located on bone surfaces in what are called \"Howship's lacunae\" (or \"resorption pits\"). These lacunae are the result of surrounding bone tissue that has been reabsorbed. Because the osteoclasts are derived from a monocyte stem-cell lineage, they are equipped with phagocytic-like mechanisms similar to circulating macrophages. Osteoclasts mature and/or migrate to discrete bone surfaces. Upon arrival, active enzymes, such as tartrate-resistant acid phosphatase, are secreted against the mineral substrate. The reabsorption of bone by osteoclasts also plays a role in calcium homeostasis.\nComposition.\nBones consist of living cells (osteoblasts and osteocytes) embedded in a mineralized organic matrix. The primary inorganic component of human bone is hydroxyapatite, the dominant bone mineral, having the nominal composition of Ca10(PO4)6(OH)2. The organic components of this matrix consist mainly of type I collagen\u2014\"organic\" referring to materials produced as a result of the human body\u2014and inorganic components, which alongside the dominant hydroxyapatite phase, include other compounds of calcium and phosphate including salts. Approximately 30% of the acellular component of bone consists of organic matter, while roughly 70% by mass is attributed to the inorganic phase. The collagen fibers give bone its tensile strength, and the interspersed crystals of hydroxyapatite give bone its compressive strength. These effects are synergistic. The exact composition of the matrix may be subject to change over time due to nutrition and biomineralization, with the ratio of calcium to phosphate varying between 1.3 and 2.0 (per weight), and trace minerals such as magnesium, sodium, potassium and carbonate also be found.\nType I collagen composes 90\u201395% of the organic matrix, with the remainder of the matrix being a homogenous liquid called ground substance consisting of proteoglycans such as hyaluronic acid and chondroitin sulfate, as well as non-collagenous proteins such as osteocalcin, osteopontin or bone sialoprotein. Collagen consists of strands of repeating units, which give bone tensile strength, and are arranged in an overlapping fashion that prevents shear stress. The function of ground substance is not fully known. Two types of bone can be identified microscopically according to the arrangement of collagen: woven and lamellar.\nWoven bone is produced when osteoblasts produce osteoid rapidly, which occurs initially in all fetal bones, but is later replaced by more resilient lamellar bone. In adults, woven bone is created after fractures or in Paget's disease. Woven bone is weaker, with a smaller number of randomly oriented collagen fibers, but forms quickly; it is for this appearance of the fibrous matrix that the bone is termed \"woven\". It is soon replaced by lamellar bone, which is highly organized in concentric sheets with a much lower proportion of osteocytes to surrounding tissue. Lamellar bone, which makes its first appearance in humans in the fetus during the third trimester, is stronger and filled with many collagen fibers parallel to other fibers in the same layer (these parallel columns are called osteons). In cross-section, the fibers run in opposite directions in alternating layers, much like in plywood, assisting in the bone's ability to resist torsion forces. After a fracture, woven bone forms initially and is gradually replaced by lamellar bone during a process known as \"bony substitution\". Compared to woven bone, lamellar bone formation takes place more slowly. The orderly deposition of collagen fibers restricts the formation of osteoid to about 1 to 2\u00a0\u03bcm per day. Lamellar bone also requires a relatively flat surface to lay the collagen fibers in parallel or concentric layers.\nDeposition.\nThe extracellular matrix of bone is laid down by osteoblasts, which secrete both collagen and ground substance. These cells synthesise collagen alpha polypetpide chains and then secrete collagen molecules. The collagen molecules associate with their neighbors and crosslink via lysyl oxidase to form collagen fibrils. At this stage, they are not yet mineralized, and this zone of unmineralized collagen fibrils is called \"osteoid\". Around and inside collagen fibrils calcium and phosphate eventually precipitate within days to weeks becoming then fully mineralized bone with an overall carbonate substituted hydroxyapatite inorganic phase.\nIn order to mineralise the bone, the osteoblasts secrete alkaline phosphatase, some of which is carried by vesicles. This cleaves the inhibitory pyrophosphate and simultaneously generates free phosphate ions for mineralization, acting as the foci for calcium and phosphate deposition. Vesicles may initiate some of the early mineralization events by rupturing and acting as a centre for crystals to grow on. Bone mineral may be formed from globular and plate structures, and via initially amorphous phases.\nTypes.\nFive types of bones are found in the human body: long, short, flat, irregular, and sesamoid.\nTerminology.\nIn the study of anatomy, anatomists use a number of anatomical terms to describe the appearance, shape and function of bones. Other anatomical terms are also used to describe the location of bones. Like other anatomical terms, many of these derive from Latin and Greek. Some anatomists still use Latin to refer to bones. The term \"osseous\", and the prefix \"osteo-\", referring to things related to bone, are still used commonly today.\nSome examples of terms used to describe bones include the term \"foramen\" to describe a hole through which something passes, and a \"canal\" or \"meatus\" to describe a tunnel-like structure. A protrusion from a bone can be called a number of terms, including a \"condyle\", \"crest\", \"spine\", \"eminence\", \"tubercle\" or \"tuberosity\", depending on the protrusion's shape and location. In general, long bones are said to have a \"head\", \"neck\", and \"body\".\nWhen two bones join, they are said to \"articulate\". If the two bones have a fibrous connection and are relatively immobile, then the joint is called a \"suture\".\nDevelopment.\nThe formation of bone is called ossification. During the fetal stage of development this occurs by two processes: intramembranous ossification and endochondral ossification. Intramembranous ossification involves the formation of bone from connective tissue whereas endochondral ossification involves the formation of bone from cartilage.\nIntramembranous ossification mainly occurs during formation of the flat bones of the skull but also the mandible, maxilla, and clavicles; the bone is formed from connective tissue such as mesenchyme tissue rather than from cartilage. The process includes: the development of the ossification center, calcification, trabeculae formation and the development of the periosteum.\nEndochondral ossification occurs in long bones and most other bones in the body; it involves the development of bone from cartilage. This process includes the development of a cartilage model, its growth and development, development of the primary and secondary ossification centers, and the formation of articular cartilage and the epiphyseal plates.\nEndochondral ossification begins with points in the cartilage called \"primary ossification centers\". They mostly appear during fetal development, though a few short bones begin their primary ossification after birth. They are responsible for the formation of the diaphyses of long bones, short bones and certain parts of irregular bones. Secondary ossification occurs after birth and forms the epiphyses of long bones and the extremities of irregular and flat bones. The diaphysis and both epiphyses of a long bone are separated by a growing zone of cartilage (the epiphyseal plate). At skeletal maturity (18 to 25 years of age), all of the cartilage is replaced by bone, fusing the diaphysis and both epiphyses together (epiphyseal closure). In the upper limbs, only the diaphyses of the long bones and scapula are ossified. The epiphyses, carpal bones, coracoid process, medial border of the scapula, and acromion are still cartilaginous.\nThe following steps are followed in the conversion of cartilage to bone:\nBone development in youth is extremely important in preventing future complications of the skeletal system. Regular exercise during childhood and adolescence can help improve bone architecture, making bones more resilient and less prone to fractures in adulthood. Physical activity, specifically resistance training, stimulates growth of bones by increasing both bone density and strength. Studies have shown a positive correlation between the adaptations of resistance training and bone density. While nutritional and pharmacological approaches may also improve bone health, the strength and balance adaptations from resistance training are a substantial added benefit. Weight-bearing exercise may assist in osteoblast (bone-forming cells) formation and help to increase bone mineral content. High-impact sports, which involve quick changes in direction, jumping, and running, are particularly effective with stimulating bone growth in the youth. Sports such as soccer, basketball, and tennis have shown to have positive effects on bone mineral density as well as bone mineral content in teenagers. Engaging in physical activity during childhood years, particularly in these high-impact osteogenic sports, can help to positively influence bone mineral density in adulthood. Children and adolescents who participate in regular physical activity will place the groundwork for bone health later in life, reducing the risk of bone-related conditions such as osteoporosis. \nFunctions.\nBones have a variety of functions:\nMechanical.\nBones serve a variety of mechanical functions. Together the bones in the body form the skeleton. They provide a frame to keep the body supported, and an attachment point for skeletal muscles, tendons, ligaments and joints, which function together to generate and transfer forces so that individual body parts or the whole body can be manipulated in three-dimensional space (the interaction between bone and muscle is studied in biomechanics).\nBones protect internal organs, such as the skull protecting the brain or the ribs protecting the heart and lungs. Because of the way that bone is formed, bone has a high compressive strength of about , poor tensile strength of 104\u2013121 MPa, and a very low shear stress strength (51.6 MPa). This means that bone resists pushing (compressional) stress well, resist pulling (tensional) stress less well, but only poorly resists shear stress (such as due to torsional loads). While bone is essentially brittle, bone does have a significant degree of elasticity, contributed chiefly by collagen.\nMechanically, bones also have a special role in hearing. The ossicles are three small bones in the middle ear which are involved in sound transduction.\nSynthetic.\nThe cancellous part of bones contain bone marrow. Bone marrow produces blood cells in a process called hematopoiesis. Blood cells that are created in bone marrow include red blood cells, platelets and white blood cells. Progenitor cells such as the hematopoietic stem cell divide in a process called mitosis to produce precursor cells. These include precursors which eventually give rise to white blood cells, and erythroblasts which give rise to red blood cells. Unlike red and white blood cells, created by mitosis, platelets are shed from very large cells called megakaryocytes. This process of progressive differentiation occurs within the bone marrow. After the cells are matured, they enter the circulation. Every day, over 2.5 billion red blood cells and platelets, and 50\u2013100 billion granulocytes are produced in this way.\nAs well as creating cells, bone marrow is also one of the major sites where defective or aged red blood cells are destroyed.\nMetabolic.\nDetermined by the species, age, and the type of bone, bone cells make up to 15 percent of the bone. Growth factor storage\u2014mineralized bone matrix stores important growth factors such as insulin-like growth factors, transforming growth factor, bone morphogenetic proteins and others.\nCalcium.\nStrong bones during our youth is essential for preventing osteoporosis and bone fragility as we age. The importance of insuring factors that could influence increases in BMD while lowering our risks for further bone degradation is necessary during our childhood as these factors lead to a supportive and healthy lifestyle/bone health. Up till the age of 30, the bone stores that we have will ultimately start to decrease as we surpass this age. Influencing factors that can help us have larger stores and higher amounts of BMD will allow us to see less harmful results as we reach older adulthood.\nThe issue of having fragile bones during our childhood leads to an increase in certain disorders and conditions such as juvenile osteoporosis, though it is less common to see, the necessity for a healthy routine especially when it comes to bone development is essential in our youth. Children that naturally have lower bone mineral density have a lower quality of life and therefore lead a life that is less fulfilling and uncomfortable. Factors such as increases in Calcium intake has been shown to increase BMD stores. Studies have shown that increasing calcium stores whether that be through supplementation or intake via foods and beverages such as leafy greens and milk have pushed the notion that prepuberty or even early pubertal children will see increases in BMD with the addition of increase Calcium intake.\nAnother research study goes on to show that long-term calcium intake has been proven to significantly contribute to overall BMD in children without certain conditions or disorders. This data shows that ensuring adequate calcium intake in children reinforces the structure and rate at which bones will begin to densify. Further detailing how structuring a strong nutritional plan with adequate amounts of Calcium sources can lead to strong bones but also can be a worth-while strategy into preventing further damage or degradation of bone stores as we age.\nThe connection between Calcium intake &amp; BMD and its effects on youth as a whole is a very world-wide issue and has been shown to affect different ethnicities in a variety of differing ways. In a recent study, there was a strong correlation between calcium intake and BMD across a variety of diverse populations of children and adolescence ultimately coming to the conclusion that fundamentally, achieving optimal bone health is necessary for providing our youth with the ability to undergo hormonal changes as well. They found in a study of over 10,000 children ages 8-19 that in females, African Americans, and the 12-15 adolescent groups that at 2.6-2.8g/kg of body weight, they began to see a decrease in BMD. They elaborate on this by determining that this is strongly influenced by a lower baseline in calcium intake throughout puberty. Genetic factors have also been shown to influence lower acceptance of calcium stores.\nUltimately, the window that youth have for accruing and building resilient bones is very minimal. Being able to consistently meet calcium needs while also engaging in weight-bearing exercise is essential for building a strong initial bone foundation at which to build upon. Being able to reach our daily value of 1300mg for ages 9-18 is becoming more and more necessary and as we progress in health, the chance that osteoporosis and other factors such as bone fragility or potential for stunted growth can be greatly reduced through these resources, ultimately leading to a more fulfilling and healthier lifestyle. \nRemodeling.\nBone is constantly being created and replaced in a process known as remodeling. This ongoing turnover of bone is a process of resorption followed by replacement of bone with little change in shape. This is accomplished through osteoblasts and osteoclasts. Cells are stimulated by a variety of signals, and together referred to as a remodeling unit. Approximately 10% of the skeletal mass of an adult is remodelled each year. The purpose of remodeling is to regulate calcium homeostasis, repair microdamaged bones from everyday stress, and to shape the skeleton during growth. Repeated stress, such as weight-bearing exercise or bone healing, results in the bone thickening at the points of maximum stress (Wolff's law). It has been hypothesized that this is a result of bone's piezoelectric properties, which cause bone to generate small electrical potentials under stress.\nThe action of osteoblasts and osteoclasts are controlled by a number of chemical enzymes that either promote or inhibit the activity of the bone remodeling cells, controlling the rate at which bone is made, destroyed, or changed in shape. The cells also use paracrine signalling to control the activity of each other. For example, the rate at which osteoclasts resorb bone is inhibited by calcitonin and osteoprotegerin. Calcitonin is produced by parafollicular cells in the thyroid gland, and can bind to receptors on osteoclasts to directly inhibit osteoclast activity. Osteoprotegerin is secreted by osteoblasts and is able to bind RANK-L, inhibiting osteoclast stimulation.\nOsteoblasts can also be stimulated to increase bone mass through increased secretion of osteoid and by inhibiting the ability of osteoclasts to break down osseous tissue. Increased secretion of osteoid is stimulated by the secretion of growth hormone by the pituitary, thyroid hormone and the sex hormones (estrogens and androgens). These hormones also promote increased secretion of osteoprotegerin. Osteoblasts can also be induced to secrete a number of cytokines that promote reabsorption of bone by stimulating osteoclast activity and differentiation from progenitor cells. Vitamin D, parathyroid hormone and stimulation from osteocytes induce osteoblasts to increase secretion of RANK-ligand and interleukin 6, which cytokines then stimulate increased reabsorption of bone by osteoclasts. These same compounds also increase secretion of macrophage colony-stimulating factor by osteoblasts, which promotes the differentiation of progenitor cells into osteoclasts, and decrease secretion of osteoprotegerin.\nVolume.\nBone volume is determined by the rates of bone formation and bone resorption. Certain growth factors may work to locally alter bone formation by increasing osteoblast activity. Numerous bone-derived growth factors have been isolated and classified via bone cultures. These factors include insulin-like growth factors I and II, transforming growth factor-beta, fibroblast growth factor, platelet-derived growth factor, and bone morphogenetic proteins. Evidence suggests that bone cells produce growth factors for extracellular storage in the bone matrix. The release of these growth factors from the bone matrix could cause the proliferation of osteoblast precursors. Essentially, bone growth factors may act as potential determinants of local bone formation. Cancellous bone volume in postmenopausal osteoporosis may be determined by the relationship between the total bone forming surface and the percent of surface resorption.\nClinical significance.\nA number of diseases can affect bone, including arthritis, fractures, infections, osteoporosis and tumors. Conditions relating to bone can be managed by a variety of doctors, including rheumatologists for joints, and orthopedic surgeons, who may conduct surgery to fix broken bones. Other doctors, such as rehabilitation specialists may be involved in recovery, radiologists in interpreting the findings on imaging, and pathologists in investigating the cause of the disease, and family doctors may play a role in preventing complications of bone disease such as osteoporosis.\nWhen a doctor sees a patient, a history and exam will be taken. Bones are then often imaged, called radiography. This might include ultrasound X-ray, CT scan, MRI scan and other imaging such as a Bone scan, which may be used to investigate cancer. Other tests such as a blood test for autoimmune markers may be taken, or a synovial fluid aspirate may be taken.\nFractures.\nIn normal bone, fractures occur when there is significant force applied or repetitive trauma over a long time. Fractures can also occur when a bone is weakened, such as with osteoporosis, or when there is a structural problem, such as when the bone remodels excessively (such as Paget's disease) or is the site of the growth of cancer. Common fractures include wrist fractures and hip fractures, associated with osteoporosis, vertebral fractures associated with high-energy trauma and cancer, and fractures of long-bones. Not all fractures are painful. When serious, depending on the fractures type and location, complications may include flail chest, compartment syndromes or fat embolism.\nCompound fractures involve the bone's penetration through the skin. Some complex fractures can be treated by the use of bone grafting procedures that replace missing bone portions.\nFractures and their underlying causes can be investigated by X-rays, CT scans and MRIs. Fractures are described by their location and shape, and several classification systems exist, depending on the location of the fracture. A common long bone fracture in children is a Salter\u2013Harris fracture. When fractures are managed, pain relief is often given, and the fractured area is often immobilised. This is to promote bone healing. In addition, surgical measures such as internal fixation may be used. Because of the immobilisation, people with fractures are often advised to undergo rehabilitation.\nTumors.\nTumor that can affect bone in several ways. Examples of benign bone tumors include osteoma, osteoid osteoma, osteochondroma, osteoblastoma, enchondroma, giant-cell tumor of bone, and aneurysmal bone cyst.\nCancer.\nCancer can arise in bone tissue, and bones are also a common site for other cancers to spread (metastasise) to. Cancers that arise in bone are called \"primary\" cancers, although such cancers are rare. Metastases within bone are \"secondary\" cancers, with the most common being breast cancer, lung cancer, prostate cancer, thyroid cancer, and kidney cancer. Secondary cancers that affect bone can either destroy bone (called a \"lytic\" cancer) or create bone (a \"sclerotic\" cancer). Cancers of the bone marrow inside the bone can also affect bone tissue, examples including leukemia and multiple myeloma. Bone may also be affected by cancers in other parts of the body. Cancers in other parts of the body may release parathyroid hormone or parathyroid hormone-related peptide. This increases bone reabsorption, and can lead to bone fractures.\nBone tissue that is destroyed or altered as a result of cancers is distorted, weakened, and more prone to fracture. This may lead to compression of the spinal cord, destruction of the marrow resulting in bruising, bleeding and immunosuppression, and is one cause of bone pain. If the cancer is metastatic, then there might be other symptoms depending on the site of the original cancer. Some bone cancers can also be felt.\nCancers of the bone are managed according to their type, their stage, prognosis, and what symptoms they cause. Many primary cancers of bone are treated with radiotherapy. Cancers of bone marrow may be treated with chemotherapy, and other forms of targeted therapy such as immunotherapy may be used. Palliative care, which focuses on maximising a person's quality of life, may play a role in management, particularly if the likelihood of survival within five years is poor.\nDiabetes.\nType 1 diabetes is an autoimmune disease in which the body attacks the insulin-producing pancreas cells causing the body to not make enough insulin. In contrast type 2 diabetes in which the body creates enough Insulin, but becomes resistant to it over time.\nChildren makeup approximately 85% of Type 1 Diabetes cases and in America there was an average 22% rise in cases over the first 24 months of the Covid-19 Pandemic. With the increase of developing some form of diabetes across all ranges continually growing the health impacts on bone development and bone health in these populations are still being researched. Most evidence suggests that diabetes, either Type 1 and Type 2, inhibits osteoblastic activity and causes both lower BMD and BMC in both adults and children. The weakening of these developmental aspects is thought to lead to an increased risk of developing many diseases such as osteoarthritis, osteoporosis, osteopenia and fractures. Development of any of these diseases is thought to be correlated with a decrease in ability to perform in athletic environments and activities of daily living. \nFocusing on therapies that target molecules like osteocalcin or AGEs could provide new ways to improve bone health and help manage the complications of diabetes more effectively.\nOsteoporosis.\nOsteoporosis is a disease of bone where there is reduced bone mineral density, increasing the likelihood of fractures. Osteoporosis is defined in women by the World Health Organization as a bone mineral density of 2.5 standard deviations below peak bone mass, relative to the age and sex-matched average. This density is measured using dual energy X-ray absorptiometry (DEXA), with the term \"established osteoporosis\" including the presence of a fragility fracture. Osteoporosis is most common in women after menopause, when it is called \"postmenopausal osteoporosis\", but may develop in men and premenopausal women in the presence of particular hormonal disorders and other chronic diseases or as a result of smoking and medications, specifically glucocorticoids. Osteoporosis usually has no symptoms until a fracture occurs. For this reason, DEXA scans are often done in people with one or more risk factors, who have developed osteoporosis and are at risk of fracture.\nOne of the most important risk factors for osteoporosis is advanced age. Accumulation of oxidative DNA damage in osteoblastic and osteoclastic cells appears to be a key factor in age-related osteoporosis.\nOsteoporosis treatment includes advice to stop smoking, decrease alcohol consumption, exercise regularly, and have a healthy diet. Calcium and trace mineral supplements may also be advised, as may Vitamin D. When medication is used, it may include bisphosphonates, Strontium ranelate, and hormone replacement therapy.\nOsteopathic medicine.\nOsteopathic medicine is a school of medical thought that links the musculoskeletal system to overall health. , over 77,000 physicians in the United States are trained in osteopathic medical schools.\nBone health.\nBone health is vastly important all throughout life due to a number of reasons, some of those being, without strong healthy bones we are more at risk for different chronic diseases, and fractures as well as day to day function being more difficult with poor bone health. Developing strong bones as a child is one of the most important steps to having healthy bones all throughout life because this is when a strong foundation is built, which will make it much easier to maintain musculoskeletal health in later years. Adolescence offers a window to really develop bones in either a positive or negative way. It is estimated that diet and exercise during these years can impact peak bone mass as an adult nearly 20-40%. One study done on children with developmental coordination disorder found an increase in bone mass up to 4% and 5% in the cortical areas of the tibia alone from a 13 week training period, which is truly significant when considering how participants only participated in the multimodal workouts twice per week, and it would be reasonable to expect these increases to be greater if workouts were more frequent, especially in youth without developmental coordination disorder. Peak bone mass occurs between the second and third decade of most people's lives, and with this being the case if we can really stockpile as much bone mass and increase our BMD and BMC by living healthy active lives, and having good diets that consume adequate calcium and vitamin D then we will truly have a leg up in our later lives as well as actively decreasing risks of certain chronic diseases such as osteoporosis.\nOsteology.\nThe study of bones and teeth is referred to as osteology. It is frequently used in anthropology, archeology and forensic science for a variety of tasks. This can include determining the nutritional, health, age or injury status of the individual the bones were taken from. Preparing fleshed bones for these types of studies can involve the process of maceration.\nTypically anthropologists and archeologists study bone tools made by \"Homo sapiens\" and \"Homo neanderthalensis\". Bones can serve a number of uses such as projectile points or artistic pigments, and can also be made from external bones such as antlers.\nOther animals.\nBird skeletons are very lightweight. Their bones are smaller and thinner, to aid flight. Among mammals, bats come closest to birds in terms of bone density, suggesting that small dense bones are a flight adaptation. Many bird bones have little marrow due to them being hollow.\nA bird's beak is primarily made of bone as projections of the mandibles which are covered in keratin.\nSome bones, primarily formed separately in subcutaneous tissues, include headgears (such as bony core of horns, antlers, ossicones), osteoderm, and os penis/os clitoris. A deer's antlers are composed of bone which is an unusual example of bone being outside the skin of the animal once the velvet is shed.\nThe extinct predatory fish \"Dunkleosteus\" had sharp edges of hard exposed bone along its jaws.\nThe proportion of cortical bone that is 80% in the human skeleton may be much lower in other animals, especially in marine mammals and marine turtles, or in various Mesozoic marine reptiles, such as ichthyosaurs, among others. This proportion can vary quickly in evolution; it often increases in early stages of returns to an aquatic lifestyle, as seen in early whales and pinnipeds, among others. It subsequently decreases in pelagic taxa, which typically acquire spongy bone, but aquatic taxa that live in shallow water can retain very thick, pachyostotic, osteosclerotic, or pachyosteosclerotic bones, especially if they move slowly, like sea cows. In some cases, even marine taxa that had acquired spongy bone can revert to thicker, compact bones if they become adapted to live in shallow water, or in hypersaline (denser) water.\nMany animals, particularly herbivores, practice osteophagy\u2014the eating of bones. This is presumably carried out in order to replenish lacking phosphate.\nMany bone diseases that affect humans also affect other vertebrates\u2014an example of one disorder is skeletal fluorosis.\nSociety and culture.\nBones from slaughtered animals have a number of uses. In prehistoric times, they have been used for making bone tools. They have further been used in bone carving, already important in prehistoric art, and also in modern time as crafting materials for buttons, beads, handles, bobbins, calculation aids, head nuts, dice, poker chips, pick-up sticks, arrows, scrimshaw, ornaments, etc.\nBone glue can be made by prolonged boiling of ground or cracked bones, followed by filtering and evaporation to thicken the resulting fluid. Historically once important, bone glue and other animal glues today have only a few specialized uses, such as in antiques restoration. Essentially the same process, with further refinement, thickening and drying, is used to make gelatin.\nBroth is made by simmering several ingredients for a long time, traditionally including bones.\nBone char, a porous, black, granular material primarily used for filtration and also as a black pigment, is produced by charring mammal bones.\nOracle bone script was a writing system used in ancient China based on inscriptions in bones. Its name originates from oracle bones, which were mainly ox clavicle. The Ancient Chinese (mainly in the Shang dynasty), would write their questions on the oracle bone, and burn the bone, and where the bone cracked would be the answer for the questions.\nTo point the bone at someone is considered bad luck in some cultures, such as Australian aborigines, such as by the Kurdaitcha.\nThe wishbones of fowl have been used for divination, and are still customarily used in a tradition to determine which one of two people pulling on either prong of the bone may make a wish.\nVarious cultures throughout history have adopted the custom of shaping an infant's head by the practice of artificial cranial deformation. A widely practised custom in China was that of foot binding to limit the normal growth of the foot."}
{"id": "4100", "revid": "4051065", "url": "https://en.wikipedia.org/wiki?curid=4100", "title": "Bretwalda", "text": "Bretwalda (also brytenwalda and bretenanwealda, sometimes capitalised) is an Old English word. The first record comes from the late 9th-century \"Anglo-Saxon Chronicle\". It is given to some of the rulers of Anglo-Saxon kingdoms from the 5th century onwards who had achieved overlordship of some or all of the other Anglo-Saxon kingdoms. It is unclear whether the word dates back to the 5th century and was used by the kings themselves or whether it is a later, 9th-century, invention. The term \"bretwalda\" also appears in a 10th-century charter of \u00c6thelstan. The literal meaning of the word is disputed and may translate to either 'wide-ruler' or 'Britain-ruler'.\nThe rulers of Mercia were generally the most powerful of the Anglo-Saxon kings from the mid 7th century to the early 9th century but are not accorded the title of \"bretwalda\" by the \"Chronicle\", which had an anti-Mercian bias. The \"Annals of Wales\" continued to recognise the kings of Northumbria as \"Kings of the Saxons\" until the death of Osred I of Northumbria in 716.\nEtymology.\nThe first syllable of the term \"bretwalda\" may be related to \"Briton\" or \"Britain\". The second element is taken to mean 'ruler' or 'sovereign'. Thus, one interpretation might be 'sovereign of Britain'. Otherwise, the word may be a compound containing the Old English adjective \"brytten\" ('broad', from the verb \"breotan\" meaning 'to break' or 'to disperse'), an element also found in the terms \"bryten rice\" ('kingdom'), \"bryten-grund\" ('the wide expanse of the earth') and \"bryten cyning\" ('king whose authority was widely extended'). Though the origin is ambiguous, the draughtsman of the charter issued by \u00c6thelstan used the term in a way that can only mean 'wide-ruler'.\nThe latter etymology was first suggested by John Mitchell Kemble who alluded that \"of six manuscripts in which this passage occurs, one only reads \"Bretwalda\": of the remaining five, four have \"Bryten-walda\" or \"-wealda\", and one \"Breten-anweald\", which is precisely synonymous with Brytenwealda\"; that \u00c6thelstan was called \"brytenwealda ealles \u00f0yses ealondes\", which Kemble translates as 'ruler of all these islands'; and that \"bryten-\" is a common prefix to words meaning 'wide or general dispersion' and that the similarity to the word \"bretwealh\" ('Briton') is \"merely accidental\".\nContemporary use.\nThe first recorded use of the term \"Bretwalda\" comes from a West Saxon chronicle of the late 9th century that applied the term to Ecgberht, who ruled Wessex from 802 to 839. The chronicler also wrote down the names of seven kings that Bede listed in his \"Historia ecclesiastica gentis Anglorum\" in 731. All subsequent manuscripts of the \"Chronicle\" use the term \"Brytenwalda\", which may have represented the original term or derived from a common error.\nThere is no evidence that the term was a title that had any practical use, with implications of formal rights, powers and office, or even that it had any existence before the 9th-century. Bede wrote in Latin and never used the term and his list of kings holding \"imperium\" should be treated with caution, not least in that he overlooks kings such as Penda of Mercia, who clearly held some kind of dominance during his reign. Similarly, in his list of bretwaldas, the West Saxon chronicler ignored such Mercian kings as Offa.\nThe use of the term \"Bretwalda\" was the attempt by a West Saxon chronicler to make some claim of West Saxon kings to the whole of Great Britain. The concept of the overlordship of the whole of Britain was at least recognised in the period, whatever was meant by the term. Quite possibly it was a survival of a Roman concept of \"Britain\": it is significant that, while the hyperbolic inscriptions on coins and titles in charters often included the title \"rex Britanniae\", when England was unified the title used was \"rex Angulsaxonum\", ('king of the Anglo-Saxons'.)\nModern interpretation by historians.\nFor some time, the existence of the word \"bretwalda\" in the \"Anglo-Saxon Chronicle\", which was based in part on the list given by Bede in his \"Historia Ecclesiastica\", led historians to think that there was perhaps a \"title\" held by Anglo-Saxon overlords. This was particularly attractive as it would lay the foundations for the establishment of an English monarchy. The 20th-century historian Frank Stenton said of the Anglo-Saxon chronicler that \"his inaccuracy is more than compensated by his preservation of the English title applied to these outstanding kings\". He argued that the term \"bretwalda\" \"falls into line with the other evidence which points to the Germanic origin of the earliest English institutions\".\nOver the later 20th century, this assumption was increasingly challenged. Patrick Wormald interpreted it as \"less an objectively realized office than a subjectively perceived status\" and emphasised the partiality of its usage in favour of Southumbrian rulers. In 1991, Steven Fanning argued that \"it is unlikely that the term ever existed as a title or was in common usage in Anglo-Saxon England\". The fact that Bede never mentioned a special title for the kings in his list implies that he was unaware of one. In 1995, Simon Keynes observed that \"if Bede's concept of the Southumbrian overlord, and the chronicler's concept of the 'Bretwalda', are to be regarded as artificial constructs, which have no validity outside the context of the literary works in which they appear, we are released from the assumptions about political development which they seem to involve... we might ask whether kings in the eighth and ninth centuries were quite so obsessed with the establishment of a pan-Southumbrian state\".\nModern interpretations view the concept of \"bretwalda\" overlordship as complex and an important indicator of how a 9th-century chronicler interpreted history and attempted to insert the increasingly powerful Saxon kings into that history.\nOverlordship.\nA complex array of dominance and subservience existed during the Anglo-Saxon period. A king who used charters to grant land in another kingdom indicated such a relationship. If the other kingdom were fairly large, as when the Mercians dominated the East Anglians, the relationship would have been more equal than in the case of the Mercian dominance of the Hwicce, which was a comparatively small kingdom. Mercia was arguably the most powerful Anglo-Saxon kingdom for much of the late 7th though 8th centuries, though Mercian kings are missing from the two main \"lists\". For Bede, Mercia was a traditional enemy of his native Northumbria and he regarded powerful kings such as the pagan Penda as standing in the way of the Christian conversion of the Anglo-Saxons. Bede omits them from his list, even though it is evident that Penda held a considerable degree of power. Similarly powerful Mercia kings such as Offa are missed out of the West Saxon \"Anglo-Saxon Chronicle\", which sought to demonstrate the legitimacy of their kings to rule over other Anglo-Saxon peoples."}
{"id": "4101", "revid": "2660255", "url": "https://en.wikipedia.org/wiki?curid=4101", "title": "Brouwer fixed-point theorem", "text": "Brouwer's fixed-point theorem is a fixed-point theorem in topology, named after L. E. J. (Bertus) Brouwer. It states that for any continuous function formula_1 mapping a nonempty compact convex set to itself, there is a point formula_2 such that formula_3. The simplest forms of Brouwer's theorem are for continuous functions formula_1 from a closed interval formula_5 in the real numbers to itself or from a closed disk formula_6 to itself. A more general form than the latter is for continuous functions from a nonempty convex compact subset formula_7 of Euclidean space to itself.\nAmong hundreds of fixed-point theorems, Brouwer's is particularly well known, due in part to its use across numerous fields of mathematics. In its original field, this result is one of the key theorems characterizing the topology of Euclidean spaces, along with the Jordan curve theorem, the hairy ball theorem, the invariance of dimension and the Borsuk\u2013Ulam theorem. This gives it a place among the fundamental theorems of topology. The theorem is also used for proving deep results about differential equations and is covered in most introductory courses on differential geometry. It appears in unlikely fields such as game theory. In economics, Brouwer's fixed-point theorem and its extension, the Kakutani fixed-point theorem, play a central role in the proof of existence of general equilibrium in market economies as developed in the 1950s by economics Nobel prize winners Kenneth Arrow and G\u00e9rard Debreu.\nThe theorem was first studied in view of work on differential equations by the French mathematicians around Henri Poincar\u00e9 and Charles \u00c9mile Picard. Proving results such as the Poincar\u00e9\u2013Bendixson theorem requires the use of topological methods. This work at the end of the 19th century opened into several successive versions of the theorem. The case of differentiable mappings of the -dimensional closed ball was first proved in 1910 by Jacques Hadamard and the general case for continuous mappings by Brouwer in 1911.\nStatement.\nThe theorem has several formulations, depending on the context in which it is used and its degree of generalization. The simplest is sometimes given as follows:\nThis can be generalized to an arbitrary finite dimension:\nA slightly more general version is as follows:\nAn even more general form is better known under a different name:\nImportance of the pre-conditions.\nThe theorem holds only for functions that are \"endomorphisms\" (functions that have the same set as the domain and codomain) and for nonempty sets that are \"compact\" (thus, in particular, bounded and closed) and \"convex\" (or homeomorphic to convex). The following examples show why the pre-conditions are important.\nThe function \"f\" as an endomorphism.\nConsider the function\nwith domain [-1,1]. The range of the function is [0,2]. Thus, f is not an endomorphism.\nBoundedness.\nConsider the function\nwhich is a continuous function from formula_10 to itself. As it shifts every point to the right, it cannot have a fixed point. The space formula_10 is convex and closed, but not bounded.\nClosedness.\nConsider the function\nwhich is a continuous function from the open interval (\u22121,1) to itself. Since x = 1 is not part of the interval, there is not a fixed point of f(x) = x. The space (\u22121,1) is convex and bounded, but not closed. On the other hand, the function \"f\" have a fixed point for the closed interval [\u22121,1], namely \"f\"(1) = 1.\nConvexity.\nConvexity is not strictly necessary for Brouwer's fixed-point theorem. Because the properties involved (continuity, being a fixed point) are invariant under homeomorphisms, Brouwer's fixed-point theorem is equivalent to forms in which the domain is required to be a closed unit ball formula_13. For the same reason it holds for every set that is homeomorphic to a closed ball (and therefore also closed, bounded, connected, without holes, etc.).\nThe following example shows that Brouwer's fixed-point theorem does not work for domains with holes. Consider the function formula_14, which is a continuous function from the unit circle to itself. Since \"-x\u2260x\" holds for any point of the unit circle, \"f\" has no fixed point. The analogous example works for the \"n\"-dimensional sphere (or any symmetric domain that does not contain the origin). The unit circle is closed and bounded, but it has a hole (and so it is not convex) . The function \"f\" have a fixed point for the unit disc, since it takes the origin to itself.\nA formal generalization of Brouwer's fixed-point theorem for \"hole-free\" domains can be derived from the Lefschetz fixed-point theorem.\nNotes.\nThe continuous function in this theorem is not required to be bijective or surjective.\nIllustrations.\nThe theorem has several \"real world\" illustrations. Here are some examples.\nIntuitive approach.\nExplanations attributed to Brouwer.\nThe theorem is supposed to have originated from Brouwer's observation of a cup of gourmet coffee.\nIf one stirs to dissolve a lump of sugar, it appears there is always a point without motion.\nHe drew the conclusion that at any moment, there is a point on the surface that is not moving.\nThe fixed point is not necessarily the point that seems to be motionless, since the centre of the turbulence moves a little bit.\nThe result is not intuitive, since the original fixed point may become mobile when another fixed point appears.\nBrouwer is said to have added: \"I can formulate this splendid result different, I take a horizontal sheet, and another identical one which I crumple, flatten and place on the other. Then a point of the crumpled sheet is in the same place as on the other sheet.\"\nBrouwer \"flattens\" his sheet as with a flat iron, without removing the folds and wrinkles. Unlike the coffee cup example, the crumpled paper example also demonstrates that more than one fixed point may exist. This distinguishes Brouwer's result from other fixed-point theorems, such as Stefan Banach's, that guarantee uniqueness.\nOne-dimensional case.\nIn one dimension, the result is intuitive and easy to prove. The continuous function \"f\" is defined on a closed interval [\"a\",\u00a0\"b\"] and takes values in the same interval. Saying that this function has a fixed point amounts to saying that its graph (dark green in the figure on the right) intersects that of the function defined on the same interval [\"a\",\u00a0\"b\"] which maps \"x\" to \"x\" (light green).\nIntuitively, any continuous line from the left edge of the square to the right edge must necessarily intersect the green diagonal. To prove this, consider the function \"g\" which maps \"x\" to \"f\"(\"x\")\u00a0\u2212\u00a0\"x\". It is \u2265\u00a00 on \"a\" and \u2264\u00a00 on\u00a0\"b\". By the intermediate value theorem, \"g\" has a zero in [\"a\",\u00a0\"b\"]; this zero is a fixed point.\nBrouwer is said to have expressed this as follows: \"Instead of examining a surface, we will prove the theorem about a piece of string. Let us begin with the string in an unfolded state, then refold it. Let us flatten the refolded string. Again a point of the string has not changed its position with respect to its original position on the unfolded string.\"\nHistory.\nThe Brouwer fixed point theorem was one of the early achievements of algebraic topology, and is the basis of more general fixed point theorems which are important in functional analysis. The case \"n\" = 3 first was proved by Piers Bohl in 1904 (published in \"Journal f\u00fcr die reine und angewandte Mathematik\"). It was later proved by L. E. J. Brouwer in 1909. Jacques Hadamard proved the general case in 1910, and Brouwer found a different proof in the same year. Since these early proofs were all non-constructive indirect proofs, they ran contrary to Brouwer's intuitionist ideals. Although the existence of a fixed point is not constructive in the sense of constructivism in mathematics, methods to approximate fixed points guaranteed by Brouwer's theorem are now known.\nBefore discovery.\nAt the end of the 19th century, the old problem of the stability of the solar system returned into the focus of the mathematical community.\nIts solution required new methods. As noted by Henri Poincar\u00e9, who worked on the three-body problem, there is no hope to find an exact solution: \"Nothing is more proper to give us an idea of the hardness of the three-body problem, and generally of all problems of Dynamics where there is no uniform integral and the Bohlin series diverge.\"\nHe also noted that the search for an approximate solution is no more efficient: \"the more we seek to obtain precise approximations, the more the result will diverge towards an increasing imprecision\".\nHe studied a question analogous to that of the surface movement in a cup of coffee. What can we say, in general, about the trajectories on a surface animated by a constant flow? Poincar\u00e9 discovered that the answer can be found in what we now call the topological properties in the area containing the trajectory. If this area is compact, i.e. both closed and bounded, then the trajectory either becomes stationary, or it approaches a limit cycle. Poincar\u00e9 went further; if the area is of the same kind as a disk, as is the case for the cup of coffee, there must necessarily be a fixed point. This fixed point is invariant under all functions which associate to each point of the original surface its position after a short time interval\u00a0\"t\". If the area is a circular band, or if it is not closed, then this is not necessarily the case.\nTo understand differential equations better, a new branch of mathematics was born. Poincar\u00e9 called it \"analysis situs\". The French Encyclop\u00e6dia Universalis defines it as the branch which \"treats the properties of an object that are invariant if it is deformed in any continuous way, without tearing\". In 1886, Poincar\u00e9 proved a result that is equivalent to Brouwer's fixed-point theorem, although the connection with the subject of this article was not yet apparent. A little later, he developed one of the fundamental tools for better understanding the analysis situs, now known as the fundamental group or sometimes the Poincar\u00e9 group. This method can be used for a very compact proof of the theorem under discussion.\nPoincar\u00e9's method was analogous to that of \u00c9mile Picard, a contemporary mathematician who generalized the Cauchy\u2013Lipschitz theorem. Picard's approach is based on a result that would later be formalised by another fixed-point theorem, named after Banach. Instead of the topological properties of the domain, this theorem uses the fact that the function in question is a contraction.\nFirst proofs.\nAt the dawn of the 20th century, the interest in analysis situs did not stay unnoticed. However, the necessity of a theorem equivalent to the one discussed in this article was not yet evident. Piers Bohl, a Latvian mathematician, applied topological methods to the study of differential equations. In 1904 he proved the three-dimensional case of our theorem, but his publication was not noticed.\nIt was Brouwer, finally, who gave the theorem its first patent of nobility. His goals were different from those of Poincar\u00e9. This mathematician was inspired by the foundations of mathematics, especially mathematical logic and topology. His initial interest lay in an attempt to solve Hilbert's fifth problem. In 1909, during a voyage to Paris, he met Henri Poincar\u00e9, Jacques Hadamard, and \u00c9mile Borel. The ensuing discussions convinced Brouwer of the importance of a better understanding of Euclidean spaces, and were the origin of a fruitful exchange of letters with Hadamard. For the next four years, he concentrated on the proof of certain great theorems on this question. In 1912 he proved the hairy ball theorem for the two-dimensional sphere, as well as the fact that every continuous map from the two-dimensional ball to itself has a fixed point. These two results in themselves were not really new. As Hadamard observed, Poincar\u00e9 had shown a theorem equivalent to the hairy ball theorem. The revolutionary aspect of Brouwer's approach was his systematic use of recently developed tools such as homotopy, the underlying concept of the Poincar\u00e9 group. In the following year, Hadamard generalised the theorem under discussion to an arbitrary finite dimension, but he employed different methods. Hans Freudenthal comments on the respective roles as follows: \"Compared to Brouwer's revolutionary methods, those of Hadamard were very traditional, but Hadamard's participation in the birth of Brouwer's ideas resembles that of a midwife more than that of a mere spectator.\"\nBrouwer's approach yielded its fruits, and in 1910 he also found a proof that was valid for any finite dimension, as well as other key theorems such as the invariance of dimension. In the context of this work, Brouwer also generalized the Jordan curve theorem to arbitrary dimension and established the properties connected with the degree of a continuous mapping. This branch of mathematics, originally envisioned by Poincar\u00e9 and developed by Brouwer, changed its name. In the 1930s, analysis situs became algebraic topology.\nReception.\nThe theorem proved its worth in more than one way. During the 20th century numerous fixed-point theorems were developed, and even a branch of mathematics called fixed-point theory.\nBrouwer's theorem is probably the most important. It is also among the foundational theorems on the topology of topological manifolds and is often used to prove other important results such as the Jordan curve theorem.\nBesides the fixed-point theorems for more or less contracting functions, there are many that have emerged directly or indirectly from the result under discussion. A continuous map from a closed ball of Euclidean space to its boundary cannot be the identity on the boundary. Similarly, the Borsuk\u2013Ulam theorem says that a continuous map from the \"n\"-dimensional sphere to Rn has a pair of antipodal points that are mapped to the same point. In the finite-dimensional case, the Lefschetz fixed-point theorem provided from 1926 a method for counting fixed points. In 1930, Brouwer's fixed-point theorem was generalized to Banach spaces. This generalization is known as Schauder's fixed-point theorem, a result generalized further by S. Kakutani to set-valued functions. One also meets the theorem and its variants outside topology. It can be used to prove the Hartman-Grobman theorem, which describes the qualitative behaviour of certain differential equations near certain equilibria. Similarly, Brouwer's theorem is used for the proof of the Central Limit Theorem. The theorem can also be found in existence proofs for the solutions of certain partial differential equations.\nOther areas are also touched. In game theory, John Nash used the theorem to prove that in the game of Hex there is a winning strategy for white. In economics, P. Bich explains that certain generalizations of the theorem show that its use is helpful for certain classical problems in game theory and generally for equilibria (Hotelling's law), financial equilibria and incomplete markets.\nBrouwer's celebrity is not exclusively due to his topological work. The proofs of his great topological theorems are not constructive, and Brouwer's dissatisfaction with this is partly what led him to articulate the idea of constructivity. He became the originator and zealous defender of a way of formalising mathematics that is known as intuitionism, which at the time made a stand against set theory. Brouwer disavowed his original proof of the fixed-point theorem. \nProof outlines.\nA proof using degree.\nBrouwer's original 1911 proof relied on the notion of the degree of a continuous mapping, stemming from ideas in differential topology. Several modern accounts of the proof can be found in the literature, notably .\nLet formula_15 denote the closed unit ball in formula_16 centered at the origin. Suppose for simplicity that formula_17 is continuously differentiable. A regular value of formula_1 is a point formula_19 such that the Jacobian of formula_1 is non-singular at every point of the preimage of formula_21. In particular, by the inverse function theorem, every point of the preimage of formula_1 lies in formula_23 (the interior of formula_24). The degree of formula_1 at a regular value formula_19 is defined as the sum of the signs of the Jacobian determinant of formula_1 over the preimages of formula_21 under formula_1:\nThe degree is, roughly speaking, the number of \"sheets\" of the preimage \"f\" lying over a small open set around \"p\", with sheets counted oppositely if they are oppositely oriented. This is thus a generalization of winding number to higher dimensions.\nThe degree satisfies the property of \"homotopy invariance\": let formula_1 and formula_32 be two continuously differentiable functions, and formula_33 for formula_34. Suppose that the point formula_21 is a regular value of formula_36 for all \"t\". Then formula_37.\nIf there is no fixed point of the boundary of formula_24, then the function \nis well-defined, and\nformula_40\ndefines a homotopy from the identity function to it. The identity function has degree one at every point. In particular, the identity function has degree one at the origin, so formula_32 also has degree one at the origin. As a consequence, the preimage formula_42 is not empty. The elements of formula_42 are precisely the fixed points of the original function \"f\".\nThis requires some work to make fully general. The definition of degree must be extended to singular values of \"f\", and then to continuous functions. The more modern advent of homology theory simplifies the construction of the degree, and so has become a standard proof in the literature.\nA proof using the hairy ball theorem.\nThe hairy ball theorem states that on the unit sphere in an odd-dimensional Euclidean space, there is no nowhere-vanishing continuous tangent vector field on . (The tangency condition means that = 0 for every unit vector .) Sometimes the theorem is expressed by the statement that \"there is always a place on the globe with no wind\". An elementary proof of the hairy ball theorem can be found in . \nIn fact, suppose first that is \"continuously differentiable\". By scaling, it can be assumed that is a continuously differentiable unit tangent vector on . It can be extended radially to a small spherical shell of . For sufficiently small, a routine computation shows that the mapping () = + is a contraction mapping on and that the volume of its image is a polynomial in . On the other hand, as a contraction mapping, must restrict to a homeomorphism of onto (1 + ) and onto (1 + ) . This gives a contradiction, because, if the dimension of the Euclidean space is odd, (1 + )/2 is not a polynomial. \nIf is only a \"continuous\" unit tangent vector on , by the Weierstrass approximation theorem, it can be uniformly approximated by a polynomial map of into Euclidean space. The orthogonal projection on to the tangent space is given by () = () - () \u22c5 . Thus is polynomial and nowhere vanishing on ; by construction /|||| is a smooth unit tangent vector field on , a contradiction. \nThe continuous version of the hairy ball theorem can now be used to prove the Brouwer fixed point theorem. First suppose that is even. If there were a fixed-point-free continuous self-mapping of the closed unit ball of the -dimensional Euclidean space , set\nSince has no fixed points, it follows that, for in the interior of , the vector () is non-zero; and for in , the scalar product \u22c5 () = 1 \u2013 \u22c5 () is strictly positive. From the original -dimensional space Euclidean space , construct a new auxiliary ()-dimensional space = x R, with coordinates = (, ). Set\nBy construction is a continuous vector field on the unit sphere of , satisfying the tangency condition \u22c5 ()\u00a0=\u00a00. Moreover, () is nowhere vanishing (because, if has norm 1, then \u22c5 () is non-zero; while if has norm strictly less than 1, then and () are both non-zero). This contradiction proves the fixed point theorem when is even. For odd, one can apply the fixed point theorem to the closed unit ball in dimensions and the mapping (,) = ((),0).\nThe advantage of this proof is that it uses only elementary techniques; more general results like the Borsuk-Ulam theorem require tools from algebraic topology.\nA proof using homology or cohomology.\nThe proof uses the observation that the boundary of the \"n\"-disk \"D\"\"n\" is \"S\"\"n\"\u22121, the (\"n\" \u2212 1)-sphere.\nSuppose, for contradiction, that a continuous function has \"no\" fixed point. This means that, for every point x in \"D\"\"n\", the points \"x\" and \"f\"(\"x\") are distinct. Because they are distinct, for every point x in \"D\"\"n\", we can construct a unique ray from \"f\"(\"x\") to \"x\" and follow the ray until it intersects the boundary \"S\"\"n\"\u22121 (see illustration). By calling this intersection point \"F\"(\"x\"), we define a function \"F\"\u00a0:\u00a0\"D\"\"n\"\u00a0\u2192\u00a0\"S\"\"n\"\u22121 sending each point in the disk to its corresponding intersection point on the boundary. As a special case, whenever \"x\" itself is on the boundary, then the intersection point \"F\"(\"x\") must be \"x\".\nConsequently, \"F\" is a special type of continuous function known as a retraction: every point of the codomain (in this case \"S\"\"n\"\u22121) is a fixed point of \"F\".\nIntuitively it seems unlikely that there could be a retraction of \"D\"\"n\" onto \"S\"\"n\"\u22121, and in the case \"n\" = 1, the impossibility is more basic, because \"S\"0 (i.e., the endpoints of the closed interval \"D\"1) is not even connected. The case \"n\" = 2 is less obvious, but can be proven by using basic arguments involving the fundamental groups of the respective spaces: the retraction would induce a surjective group homomorphism from the fundamental group of \"D\"2 to that of \"S\"1, but the latter group is isomorphic to Z while the first group is trivial, so this is impossible. The case \"n\" = 2 can also be proven by contradiction based on a theorem about non-vanishing vector fields.\nFor \"n\" &gt; 2, however, proving the impossibility of the retraction is more difficult. One way is to make use of homology groups: the homology \"H\"\"n\"\u22121(\"D\"\"n\") is trivial, while \"H\"\"n\"\u22121(\"S\"\"n\"\u22121) is infinite cyclic. This shows that the retraction is impossible, because again the retraction would induce an injective group homomorphism from the latter to the former group.\nThe impossibility of a retraction can also be shown using the de Rham cohomology of open subsets of Euclidean space \"E\"\"n\". For \"n\" \u2265 2, the de Rham cohomology of \"U\" = \"E\"\"n\" \u2013 (0) is one-dimensional in degree 0 and \"n\" \u2013 1, and vanishes otherwise. If a retraction existed, then \"U\" would have to be contractible and its de Rham cohomology in degree \"n\" \u2013 1 would have to vanish, a contradiction.\nA proof using Stokes' theorem.\nAs in the proof of Brouwer's fixed-point theorem for continuous maps using homology, it is reduced to proving that there is no continuous retraction from the ball onto its boundary \u2202. In that case it can be assumed that is smooth, since it can be approximated using the Weierstrass approximation theorem or by convolving with non-negative smooth bump functions of sufficiently small support and integral one (i.e. mollifying). If is a volume form on the boundary then by Stokes' theorem,\ngiving a contradiction.\nMore generally, this shows that there is no smooth retraction from any non-empty smooth oriented compact manifold onto its boundary. The proof using Stokes' theorem is closely related to the proof using homology, because the form generates the de Rham cohomology group (\u2202) which is isomorphic to the homology group (\u2202) by de Rham's theorem.\nA combinatorial proof.\nThe BFPT can be proved using Sperner's lemma. We now give an outline of the proof for the special case in which \"f\" is a function from the standard \"n\"-simplex, formula_47 to itself, where\nFor every point formula_49 also formula_50 Hence the sum of their coordinates is equal:\nHence, by the pigeonhole principle, for every formula_49 there must be an index formula_53 such that the formula_54th coordinate of formula_55 is greater than or equal to the formula_54th coordinate of its image under \"f\":\nMoreover, if formula_55 lies on a \"k\"-dimensional sub-face of formula_47 then by the same argument, the index formula_54 can be selected from among the coordinates which are not zero on this sub-face.\nWe now use this fact to construct a Sperner coloring. For every triangulation of formula_47 the color of every vertex formula_55 is an index formula_54 such that formula_64\nBy construction, this is a Sperner coloring. Hence, by Sperner's lemma, there is an \"n\"-dimensional simplex whose vertices are colored with the entire set of available colors.\nBecause \"f\" is continuous, this simplex can be made arbitrarily small by choosing an arbitrarily fine triangulation. Hence, there must be a point formula_55 which satisfies the labeling condition in all coordinates: formula_66 for all formula_67\nBecause the sum of the coordinates of formula_55 and formula_69 must be equal, all these inequalities must actually be equalities. But this means that:\nThat is, formula_55 is a fixed point of formula_72\nA proof by Hirsch.\nThere is also a quick proof, by Morris Hirsch, based on the impossibility of a differentiable retraction. The indirect proof starts by noting that the map \"f\" can be approximated by a smooth map retaining the property of not fixing a point; this can be done by using the Weierstrass approximation theorem or by convolving with smooth bump functions. One then defines a retraction as above which must now be differentiable. Such a retraction must have a non-singular value, by Sard's theorem, which is also non-singular for the restriction to the boundary (which is just the identity). Thus the inverse image would be a 1-manifold with boundary. The boundary would have to contain at least two end points, both of which would have to lie on the boundary of the original ball\u2014which is impossible in a retraction.\nR. Bruce Kellogg, Tien-Yien Li, and James A. Yorke turned Hirsch's proof into a computable proof by observing that the retract is in fact defined everywhere except at the fixed points. For almost any point, \"q\", on the boundary, (assuming it is not a fixed point) the one manifold with boundary mentioned above does exist and the only possibility is that it leads from \"q\" to a fixed point. It is an easy numerical task to follow such a path from \"q\" to the fixed point so the method is essentially computable. gave a conceptually similar path-following version of the homotopy proof which extends to a wide variety of related problems.\nA proof using oriented area.\nA variation of the preceding proof does not employ the Sard's theorem, and goes as follows. If formula_73 is a smooth retraction, one considers the smooth deformation formula_74 and the smooth function\nDifferentiating under the sign of integral it is not difficult to check that \"\"(\"t\") = 0 for all \"t\", so \"\u03c6\" is a constant function, which is a contradiction because \"\u03c6\"(0) is the \"n\"-dimensional volume of the ball, while \"\u03c6\"(1) is zero. The geometric idea is that \"\u03c6\"(\"t\") is the oriented area of \"g\"\"t\"(\"B\") (that is, the Lebesgue measure of the image of the ball via \"g\"\"t\", taking into account multiplicity and orientation), and should remain constant (as it is very clear in the one-dimensional case). On the other hand, as the parameter \"t\" passes from 0 to 1 the map \"g\"\"t\" transforms continuously from the identity map of the ball, to the retraction \"r\", which is a contradiction since the oriented area of the identity coincides with the volume of the ball, while the oriented area of \"r\" is necessarily 0, as its image is the boundary of the ball, a set of null measure.\nA proof using the game Hex.\nA quite different proof given by David Gale is based on the game of Hex. The basic theorem regarding Hex, first proven by John Nash, is that no game of Hex can end in a draw; the first player always has a winning strategy (although this theorem is nonconstructive, and explicit strategies have not been fully developed for board sizes of dimensions 10 x 10 or greater). This turns out to be equivalent to the Brouwer fixed-point theorem for dimension 2. By considering \"n\"-dimensional versions of Hex, one can prove in general that Brouwer's theorem is equivalent to the determinacy theorem for Hex.\nA proof using the Lefschetz fixed-point theorem.\nThe Lefschetz fixed-point theorem says that if a continuous map \"f\" from a finite simplicial complex \"B\" to itself has only isolated fixed points, then the number of fixed points counted with multiplicities (which may be negative) is equal to the Lefschetz number\nand in particular if the Lefschetz number is nonzero then \"f\" must have a fixed point. If \"B\" is a ball (or more generally is contractible) then the Lefschetz number is one because the only non-zero simplicial homology group is: formula_77 and \"f\" acts as the identity on this group, so \"f\" has a fixed point.\nA proof in a weak logical system.\nIn reverse mathematics, Brouwer's theorem can be proved in the system WKL0, and conversely over the base system RCA0 Brouwer's theorem for a square implies the weak K\u0151nig's lemma, so this gives a precise description of the strength of Brouwer's theorem.\nGeneralizations.\nThe Brouwer fixed-point theorem forms the starting point of a number of more general fixed-point theorems.\nThe straightforward generalization to infinite dimensions, i.e. using the unit ball of an arbitrary Hilbert space instead of Euclidean space, is not true. The main problem here is that the unit balls of infinite-dimensional Hilbert spaces are not compact. For example, in the Hilbert space \u21132 of square-summable real (or complex) sequences, consider the map \"f\" : \u21132 \u2192 \u21132 which sends a sequence (\"x\"\"n\") from the closed unit ball of \u21132 to the sequence (\"y\"\"n\") defined by\nIt is not difficult to check that this map is continuous, has its image in the unit sphere of \u21132, but does not have a fixed point.\nThe generalizations of the Brouwer fixed-point theorem to infinite dimensional spaces therefore all include a compactness assumption of some sort, and also often an assumption of convexity. See fixed-point theorems in infinite-dimensional spaces for a discussion of these theorems.\nThere is also finite-dimensional generalization to a larger class of spaces: If formula_79 is a product of finitely many chainable continua, then every continuous function formula_80 has a fixed point, where a chainable continuum is a (usually but in this case not necessarily metric) compact Hausdorff space of which every open cover has a finite open refinement formula_81, such that formula_82 if and only if formula_83. Examples of chainable continua include compact connected linearly ordered spaces and in particular closed intervals of real numbers.\nThe Kakutani fixed point theorem generalizes the Brouwer fixed-point theorem in a different direction: it stays in R\"n\", but considers upper hemi-continuous set-valued functions (functions that assign to each point of the set a subset of the set). It also requires compactness and convexity of the set.\nThe Lefschetz fixed-point theorem applies to (almost) arbitrary compact topological spaces, and gives a condition in terms of singular homology that guarantees the existence of fixed points; this condition is trivially satisfied for any map in the case of \"D\"\"n\"."}
{"id": "4106", "revid": "5128741", "url": "https://en.wikipedia.org/wiki?curid=4106", "title": "Benzoic acid", "text": "Benzoic acid () is a white (or colorless) solid organic compound with the formula , whose structure consists of a benzene ring () with a carboxyl () substituent. The benzoyl group is often abbreviated \"Bz\" (not to be confused with \"Bn,\" which is used for benzyl), thus benzoic acid is also denoted as BzOH, since the benzoyl group has the formula \u2013. It is the simplest aromatic carboxylic acid. The name is derived from gum benzoin, which was for a long time its only source. \nBenzoic acid occurs naturally in many plants and serves as an intermediate in the biosynthesis of many secondary metabolites. Salts of benzoic acid are used as food preservatives. Benzoic acid is an important precursor for the industrial synthesis of many other organic substances. The salts and esters of benzoic acid are known as benzoates ().\nHistory.\nBenzoic acid was discovered in the sixteenth century. The dry distillation of gum benzoin was first described by Nostradamus (1556), and then by Alexius Pedemontanus (1560) and Blaise de Vigen\u00e8re (1596).\nJustus von Liebig and Friedrich W\u00f6hler determined the composition of benzoic acid. These latter also investigated how hippuric acid is related to benzoic acid.\nIn 1875 Salkowski discovered the antifungal properties of benzoic acid, which explains the preservation of benzoate-containing cloudberry fruits.\nProduction.\nIndustrial preparations.\nBenzoic acid is produced commercially by partial oxidation of toluene with oxygen. The process is catalyzed by cobalt or manganese naphthenates. The process uses abundant materials, and proceeds in high yield.\nThe first industrial process involved the reaction of benzotrichloride (trichloromethyl benzene) with calcium hydroxide in water, using iron or iron salts as catalyst. The resulting calcium benzoate is converted to benzoic acid with hydrochloric acid. The product contains significant amounts of chlorinated benzoic acid derivatives. For this reason, benzoic acid for human consumption was obtained by dry distillation of gum benzoin. Food-grade benzoic acid is now produced synthetically.\nLaboratory synthesis.\nBenzoic acid is cheap and readily available, so the laboratory synthesis of benzoic acid is mainly practiced for its pedagogical value. It is a common undergraduate preparation.\nBenzoic acid can be purified by recrystallization from water because of its high solubility in hot water and poor solubility in cold water. The avoidance of organic solvents for the recrystallization makes this experiment particularly safe. This process usually gives a yield of around 65%.\nBy hydrolysis.\nLike other nitriles and amides, benzonitrile and benzamide can be hydrolyzed to benzoic acid or its conjugate base in acid or basic conditions.\nFrom Grignard reagent.\nBromobenzene can be converted to benzoic acid by \"carboxylation\" of the intermediate phenylmagnesium bromide. This synthesis offers a convenient exercise for students to carry out a Grignard reaction, an important class of carbon\u2013carbon bond forming reaction in organic chemistry.\nOxidation of benzyl compounds.\nBenzyl alcohol and benzyl chloride and virtually all benzyl derivatives are readily oxidized to benzoic acid.\nUses.\nBenzoic acid is mainly consumed in the production of phenol by oxidative decarboxylation at 300\u2212400\u00a0\u00b0C:\nThe temperature required can be lowered to 200\u00a0\u00b0C by the addition of catalytic amounts of copper(II) salts. The phenol can be converted to cyclohexanol, which is a starting material for nylon synthesis.\nPrecursor to plasticizers.\nBenzoate plasticizers, such as the glycol-, diethyleneglycol-, and triethyleneglycol esters, are obtained by transesterification of methyl benzoate with the corresponding diol. These plasticizers, which are used similarly to those derived from terephthalic acid ester, represent alternatives to phthalates.\nPrecursor to sodium benzoate and related preservatives.\nBenzoic acid and its salts are used as food preservatives, represented by the E numbers E210, E211, E212, and E213. Benzoic acid inhibits the growth of mold, yeast and some bacteria. It is either added directly or created from reactions with its sodium, potassium, or calcium salt. The mechanism starts with the absorption of benzoic acid into the cell. If the intracellular pH changes to 5 or lower, the anaerobic fermentation of glucose through phosphofructokinase is decreased by 95%. The efficacy of benzoic acid and benzoate is thus dependent on the pH of the food. Benzoic acid, benzoates \nand their derivatives are used as preservatives for acidic foods and beverages such as citrus fruit juices (citric acid), sparkling drinks (carbon dioxide), soft drinks (phosphoric acid), pickles (vinegar) and other acidified foods.\nTypical concentrations of benzoic acid as a preservative in food are between 0.05 and 0.1%. Foods in which benzoic acid may be used and maximum levels for its application are controlled by local food laws.\nConcern has been expressed that benzoic acid and its salts may react with ascorbic acid (vitamin C) in some soft drinks, forming small quantities of carcinogenic benzene.\nMedicinal.\nBenzoic acid is a constituent of Whitfield's ointment which is used for the treatment of fungal skin diseases such as ringworm and athlete's foot. As the principal component of gum benzoin, benzoic acid is also a major ingredient in both tincture of benzoin and Friar's balsam. Such products have a long history of use as topical antiseptics and inhalant decongestants.\nBenzoic acid was used as an expectorant, analgesic, and antiseptic in the early 20th century.\nNiche and laboratory uses.\nIn teaching laboratories, benzoic acid is a common standard for calibrating a bomb calorimeter.\nBiology and health effects.\nBenzoic acid occurs naturally as do its esters in many plant and animal species. Appreciable amounts are found in most berries (around 0.05%). Ripe fruits of several \"Vaccinium\" species (e.g., cranberry, \"V. vitis macrocarpon\"; bilberry, \"V. myrtillus\") contain as much as 0.03\u20130.13% free benzoic acid. Benzoic acid is also formed in apples after infection with the fungus \"Nectria galligena\". Among animals, benzoic acid has been identified primarily in omnivorous or phytophageous species, e.g., in viscera and muscles of the rock ptarmigan (\"Lagopus muta\") as well as in gland secretions of male muskoxen (\"Ovibos moschatus\") or Asian bull elephants (\"Elephas maximus\"). Gum benzoin contains up to 20% of benzoic acid and 40% benzoic acid esters.\nIn terms of its biosynthesis, benzoate is produced in plants from cinnamic acid. A pathway has been identified from phenol via 4-hydroxybenzoate.\nReactions.\nReactions of benzoic acid can occur at either the aromatic ring or at the carboxyl group.\nAromatic ring.\nElectrophilic aromatic substitution reaction will take place mainly in 3-position due to the electron-withdrawing carboxylic group; i.e. benzoic acid is \"meta\" directing.\nCarboxyl group.\nReactions typical for carboxylic acids apply also to benzoic acid.\nSafety and mammalian metabolism.\nIt is excreted as hippuric acid. Benzoic acid is metabolized by butyrate-CoA ligase into an intermediate product, benzoyl-CoA, which is then metabolized by glycine \"N\"-acyltransferase into hippuric acid. Humans metabolize toluene which is also excreted as hippuric acid.\nFor humans, the World Health Organization's International Programme on Chemical Safety (IPCS) suggests a provisional tolerable intake would be 5\u00a0mg/kg body weight per day. Cats have a significantly lower tolerance against benzoic acid and its salts than rats and mice. Lethal dose for cats can be as low as 300\u00a0mg/kg body weight. The oral for rats is 3040\u00a0mg/kg, for mice it is 1940\u20132263\u00a0mg/kg.\nIn Taipei, Taiwan, a city health survey in 2010 found that 30% of dried and pickled food products had benzoic acid."}
{"id": "4107", "revid": "5229428", "url": "https://en.wikipedia.org/wiki?curid=4107", "title": "Boltzmann distribution", "text": "In statistical mechanics and mathematics, a Boltzmann distribution (also called Gibbs distribution) is a probability distribution or probability measure that gives the probability that a system will be in a certain state as a function of that state's energy and the temperature of the system. The distribution is expressed in the form:\nwhere is the probability of the system being in state , is the exponential function, is the energy of that state, and a constant of the distribution is the product of the Boltzmann constant and thermodynamic temperature . The symbol formula_2 denotes proportionality (see for the proportionality constant).\nThe term \"system\" here has a wide meaning; it can range from a collection of 'sufficient number' of atoms or a single atom to a macroscopic system such as a natural gas storage tank. Therefore, the Boltzmann distribution can be used to solve a wide variety of problems. The distribution shows that states with lower energy will always have a higher probability of being occupied.\nThe \"ratio\" of probabilities of two states is known as the Boltzmann factor and characteristically only depends on the states' energy difference:\nThe Boltzmann distribution is named after Ludwig Boltzmann who first formulated it in 1868 during his studies of the statistical mechanics of gases in thermal equilibrium. Boltzmann's statistical work is borne out in his paper \u201cOn the Relationship between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability Calculations Regarding the Conditions for Thermal Equilibrium\"\nThe distribution was later investigated extensively, in its modern generic form, by Josiah Willard Gibbs in 1902.\nThe Boltzmann distribution should not be confused with the Maxwell\u2013Boltzmann distribution or Maxwell-Boltzmann statistics. The Boltzmann distribution gives the probability that a system will be in a certain \"state\" as a function of that state's energy, while the Maxwell-Boltzmann distributions give the probabilities of particle \"speeds\" or \"energies\" in ideal gases. The distribution of energies in a one-dimensional gas however, does follow the Boltzmann distribution.\nThe distribution.\nThe Boltzmann distribution is a probability distribution that gives the probability of a certain state as a function of that state's energy and temperature of the system to which the distribution is applied. It is given as\nformula_4\nwhere:\nUsing Lagrange multipliers, one can prove that the Boltzmann distribution is the distribution that maximizes the entropy\nformula_6\nsubject to the normalization constraint that formula_7 and the constraint that formula_8 equals a particular mean energy value, except for two special cases. (These special cases occur when the mean value is either the minimum or maximum of the energies . In these cases, the entropy maximizing distribution is a limit of Boltzmann distributions where approaches zero from above or below, respectively.)\nThe partition function can be calculated if we know the energies of the states accessible to the system of interest. For atoms the partition function values can be found in the NIST Atomic Spectra Database.\nThe distribution shows that states with lower energy will always have a higher probability of being occupied than the states with higher energy. It can also give us the quantitative relationship between the probabilities of the two states being occupied. The ratio of probabilities for states and is given as\nformula_9\nwhere:\nThe corresponding ratio of populations of energy levels must also take their degeneracies into account.\nThe Boltzmann distribution is often used to describe the distribution of particles, such as atoms or molecules, over bound states accessible to them. If we have a system consisting of many particles, the probability of a particle being in state is practically the probability that, if we pick a random particle from that system and check what state it is in, we will find it is in state . This probability is equal to the number of particles in state divided by the total number of particles in the system, that is the fraction of particles that occupy state .\nwhere is the number of particles in state and is the total number of particles in the system. We may use the Boltzmann distribution to find this probability that is, as we have seen, equal to the fraction of particles that are in state i. So the equation that gives the fraction of particles in state as a function of the energy of that state is \nformula_11\nThis equation is of great importance to spectroscopy. In spectroscopy we observe a spectral line of atoms or molecules undergoing transitions from one state to another. In order for this to be possible, there must be some particles in the first state to undergo the transition. We may find that this condition is fulfilled by finding the fraction of particles in the first state. If it is negligible, the transition is very likely not observed at the temperature for which the calculation was done. In general, a larger fraction of molecules in the first state means a higher number of transitions to the second state. This gives a stronger spectral line. However, there are other factors that influence the intensity of a spectral line, such as whether it is caused by an allowed or a forbidden transition.\nThe softmax function commonly used in machine learning is related to the Boltzmann distribution:\nGeneralized Boltzmann distribution.\nDistribution of the form\nis called generalized Boltzmann distribution by some authors.\nThe Boltzmann distribution is a special case of the generalized Boltzmann distribution. The generalized Boltzmann distribution is used in statistical mechanics to describe canonical ensemble, grand canonical ensemble and isothermal\u2013isobaric ensemble. The generalized Boltzmann distribution is usually derived from the principle of maximum entropy, but there are other derivations.\nThe generalized Boltzmann distribution has the following properties:\nIn statistical mechanics.\nThe Boltzmann distribution appears in statistical mechanics when considering closed systems of fixed composition that are in thermal equilibrium (equilibrium with respect to energy exchange). The most general case is the probability distribution for the canonical ensemble. Some special cases (derivable from the canonical ensemble) show the Boltzmann distribution in different aspects:\nAlthough these cases have strong similarities, it is helpful to distinguish them as they generalize in different ways when the crucial assumptions are changed:\nIn economics.\nThe Boltzmann distribution can be introduced to allocate permits in emissions trading. The new allocation method using the Boltzmann distribution can describe the most probable, natural, and unbiased distribution of emissions permits among multiple countries.\nThe Boltzmann distribution has the same form as the multinomial logit model. As a discrete choice model, this is very well known in economics since Daniel McFadden made the connection to random utility maximization."}
{"id": "4109", "revid": "29278485", "url": "https://en.wikipedia.org/wiki?curid=4109", "title": "Leg theory", "text": "Leg theory is a bowling tactic in the sport of cricket. The term \"leg theory\" is somewhat archaic, but the basic tactic remains a play in modern cricket.\nSimply put, leg theory involves concentrating the bowling attack at or near the line of leg stump. This may or may not be accompanied by a concentration of fielders on the leg side. The line of attack aims to cramp the batsman, making him play the ball with the bat close to the body. This makes it difficult to hit the ball freely and score runs, especially on the off side. Since a leg theory attack means the batsman is more likely to hit the ball on the leg side, additional fielders on that side of the field can be effective in preventing runs and taking catches.\nStifling the batsman in this manner can lead to impatience and frustration, resulting in rash play by the batsman which in turn can lead to a quick dismissal. Concentrating attack on the leg stump is considered by many cricket fans and commentators to lead to boring play, as it stifles run scoring and encourages batsmen to play conservatively.\nLeg theory can be a moderately successful tactic when used with both fast bowling and spin bowling, particularly leg spin to right-handed batsmen or off spin to left-handed batsmen. However, because it relies on lack of concentration or discipline by the batsman, it can be risky against patient and skilled players, especially batsmen who are strong on the leg side. The English opening bowlers Sydney Barnes and Frank Foster used leg theory with some success in Australia in 1911\u201312. In England, at around the same time, Fred Root was one of the main proponents of the same tactic.\nFast leg theory.\nIn 1930, England captain Douglas Jardine, together with Nottinghamshire's captain Arthur Carr and his bowlers Harold Larwood and Bill Voce, developed a variant of leg theory in which the bowlers bowled fast, short-pitched balls that would rise into the batsman's body, together with a heavily stacked ring of close fielders on the leg side. The idea was that when the batsman defended against the ball, he would be likely to deflect the ball into the air for a catch.\nJardine called this modified form of the tactic \"fast leg theory\". On the 1932\u201333 English tour of Australia, Larwood and Voce bowled fast leg theory at the Australian batsmen. It turned out to be extremely dangerous, and most Australian players sustained injuries from being hit by the ball. Wicket-keeper Bert Oldfield's skull was fractured by a ball hitting his head (although the ball had first glanced off the bat and Larwood had an orthodox field), almost precipitating a riot by the Australian crowd.\nThe Australian press dubbed the tactic \"Bodyline\", and claimed it was a deliberate attempt by the English team to intimidate and injure the Australian players. Reports of the controversy reaching England at the time described the bowling as \"fast leg theory\", which sounded to many people to be a harmless and well-established tactic. This led to a serious misunderstanding amongst the English public and the Marylebone Cricket Club \u2013 the administrators of English cricket \u2013 of the dangers posed by Bodyline. The English press and cricket authorities declared the Australian protests to be a case of sore losing and \"squealing\".\nIt was only with the return of the English team and the subsequent use of Bodyline against English players in England by the touring West Indian cricket team in 1933 that demonstrated to the country the dangers it posed. The MCC subsequently revised the Laws of Cricket to prevent the use of \"fast leg theory\" tactics in future, also limiting the traditional tactic."}
{"id": "4110", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=4110", "title": "Blythe Danner", "text": "Blythe Katherine Danner (born February 3, 1943) is an American actress. Accolades she has received include two Primetime Emmy Awards for Best Supporting Actress in a Drama Series for her role as Izzy Huffstodt on \"Huff\" (2004\u20132006), and a Tony Award for Best Featured Actress for her performance in \"Butterflies Are Free\" on Broadway (1969\u20131972). Danner was twice nominated for the Primetime Emmy for Outstanding Guest Actress in a Comedy Series for portraying Marilyn Truman on \"Will &amp; Grace\" (2001\u201306; 2018\u201320), and the Primetime Emmy for Outstanding Lead Actress in a Miniseries or Movie for her roles in \"We Were the Mulvaneys\" (2002) and \"Back When We Were Grownups\" (2004). For the latter, she also received a Golden Globe Award nomination.\nDanner played Dina Byrnes in \"Meet the Parents\" (2000) and its sequels \"Meet the Fockers\" (2004) and \"Little Fockers\" (2010). She has collaborated on several occasions with Woody Allen, appearing in three of his films: \"Another Woman\" (1988), \"Alice\" (1990), and \"Husbands and Wives\" (1992). Her other notable film credits include \"1776\" (1972), \"Hearts of the West\" (1975), \"The Great Santini\" (1979), \"Mr. &amp; Mrs. Bridge\" (1990), \"The Prince of Tides\" (1991), \"To Wong Foo, Thanks for Everything! Julie Newmar\" (1995), \"The Myth of Fingerprints\" (1997), \"The X-Files\" (1998), \"Forces of Nature\" (1999), \"The Love Letter\" (1999), \"The Last Kiss\" (2006), \"Paul\" (2011), \"Hello I Must Be Going\" (2012), \"I'll See You in My Dreams\" (2015), and \"What They Had\" (2018).\nDanner is the sister of Harry Danner and the widow of Bruce Paltrow. She is the mother of actress Gwyneth Paltrow and director Jake Paltrow. She is the grandmother of media personality Apple Martin.\nEarly life.\nDanner was born in Philadelphia, Pennsylvania, the daughter of Katharine (n\u00e9e Kile) and Harry Earl Danner, a bank executive. She has a brother, opera singer and actor Harry Danner, a sister and a maternal half-brother. Danner has Pennsylvania Dutch, some English and Irish ancestry; her maternal grandmother was a German immigrant, and one of her paternal great-grandmothers was born in Barbados to a family of European descent.\nDanner graduated from George School, a Quaker high school located near Newtown, Bucks County, Pennsylvania, in 1960.\nCareer.\nA graduate of Bard College, Danner's first roles included the 1967 musical \"Mata Hari\" and the 1968 Off-Broadway production of \"Summertree\". Her early Broadway appearances included \"Cyrano de Bergerac\" (1968) and her Theatre World Award-winning performance in \"The Miser\" (1969). She won the Tony Award for Best Featured Actress in a Play for portraying a free-spirited divorc\u00e9e in \"Butterflies Are Free\" (1970).\nIn 1972, Danner portrayed Martha Jefferson in the film version of \"1776\". That same year, she played the unknowing wife of a husband who committed murder, opposite Peter Falk and John Cassavetes, in the \"Columbo\" episode \"\u00c9tude in Black\".\nHer earliest starring film role was opposite Alan Alda in \"To Kill a Clown\" (1972). Danner appeared in the episode of \"M*A*S*H\" entitled \"The More I See You\", playing the love interest of Alda's character Hawkeye Pierce. She played lawyer Amanda Bonner in television's \"Adam's Rib\", opposite Ken Howard as Adam Bonner. She played Zelda Fitzgerald in \"F. Scott Fitzgerald and 'The Last of the Belles\"' (1974). She was the eponymous heroine in the film \"Lovin' Molly\" (1974) (directed by Sidney Lumet). She appeared in \"Futureworld\", playing Tracy Ballard with co-star Peter Fonda (1976). In the 1982 TV movie \"Inside the Third Reich\", she played the wife of Albert Speer. In the film version of Neil Simon's semi-autobiographical play \"Brighton Beach Memoirs\" (1986), she portrayed a middle-aged Jewish mother. She has appeared in two films based on the novels of Pat Conroy, \"The Great Santini\" (1979) and \"The Prince of Tides\" (1991), as well as two television movies adapted from books by Anne Tyler, \"Saint Maybe\" and \"Back When We Were Grownups\", both for the Hallmark Hall of Fame.\nDanner appeared opposite Robert De Niro in the 2000 comedy hit \"Meet the Parents\", and its sequels, \"Meet the Fockers\" (2004) and \"Little Fockers\" (2010).\nFrom 2001 to 2006, she regularly appeared on NBC's sitcom \"Will &amp; Grace\" as Will Truman's mother Marilyn. From 2004 to 2006, she starred in the main cast of the comedy-drama series \"Huff\". In 2005, she was nominated for three Primetime Emmy Awards for her work on \"Will &amp; Grace\", \"Huff\", and the television film \"Back When We Were Grownups\", winning for her role in \"Huff\". The following year, she won a second consecutive Emmy Award for \"Huff\". For 25 years, she has been a regular performer at the Williamstown Summer Theater Festival, where she also serves on the board of directors.\nIn 2006, Danner was awarded an inaugural Katharine Hepburn Medal by Bryn Mawr College's Katharine Houghton Hepburn Center. In 2015, Danner was inducted into the American Theater Hall of Fame.\nEnvironmental activism.\nDanner has been involved in environmental issues such as recycling and conservation for over 30 years. She has been active with INFORM, Inc., is on the Board of Environmental Advocates of New York and the board of directors of the Environmental Media Association, and won the 2002 EMA Board of Directors Ongoing Commitment Award. In 2011, Danner joined Moms Clean Air Force, to help call on parents to join in the fight against toxic air pollution.\nHealth care activism.\nAfter the death of her husband Bruce Paltrow from oral cancer, she became involved with the nonprofit Oral Cancer Foundation. In 2005, she filmed a public service announcement to raise public awareness of the disease and the need for early detection. She has since appeared on morning talk shows and given interviews in such magazines as \"People\". The Bruce Paltrow Oral Cancer Fund, administered by the Oral Cancer Foundation, raises funding for oral cancer research and treatment, with a particular focus on those communities in which healthcare disparities exist.\nShe has also appeared in commercials for Prolia, a brand of denosumab used in the treatment of osteoporosis.\nPersonal life.\nDanner was married to producer and director Bruce Paltrow, who died of oral cancer in 2002. She and Paltrow had two children together, actress Gwyneth Paltrow and director Jake Paltrow.\nDanner's niece is the actress Katherine Moennig, the daughter of her maternal half-brother William.\nDanner co-starred with her daughter in the 1992 television film \"Cruel Doubt\" and again in the 2003 film \"Sylvia\", in which she portrayed Aurelia Plath, mother to Gwyneth's title role of Sylvia Plath.\nDanner is a practitioner of transcendental meditation, which she has described as \"very helpful and comforting\"."}
{"id": "4111", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=4111", "title": "Bioleaching", "text": "Bioleaching is the extraction or liberation of metals from their ores through the use of living organisms. Bioleaching is one of several applications within biohydrometallurgy and several methods are used to treat ores or concentrates containing copper, zinc, lead, arsenic, antimony, nickel, molybdenum, gold, silver, and cobalt.\nBioleaching falls into two broad categories. The first, is the use of microorganisms to oxidize refractory minerals to release valuable metals such and gold and silver. Most commonly the minerals that are the target of oxidization are pyrite and arsenopyrite.\nThe second category is leaching of sulphide minerals to release the associated metal, for example, leaching of pentlandite to release nickel, or the leaching of chalcocite, covellite or chalcopyrite to release copper.\nProcess.\nBioleaching can involve numerous ferrous iron and sulfur oxidizing bacteria, including \"Acidithiobacillus ferrooxidans\" (formerly known as \"Thiobacillus ferrooxidans\") and \"Acidithiobacillus thiooxidans \" (formerly known as \"Thiobacillus thiooxidans\"). As a general principle, in one proposed method of bacterial leaching known as Indirect Leaching, Fe3+ ions are used to oxidize the ore. This step is entirely independent of microbes. The role of the bacteria is further oxidation of the ore, but also the regeneration of the chemical oxidant Fe3+ from Fe2+. For example, bacteria catalyse the breakdown of the mineral pyrite (FeS2) by oxidising the sulfur and metal (in this case ferrous iron, (Fe2+)) using oxygen. This yields soluble products that can be further purified and refined to yield the desired metal.\nPyrite leaching (FeS2):\nIn the first step, disulfide is spontaneously oxidized to thiosulfate by ferric ion (Fe3+), which in turn is reduced to give ferrous ion (Fe2+):\nThe ferrous ion is then oxidized by bacteria using oxygen:\nThiosulfate is also oxidized by bacteria to give sulfate:\nThe ferric ion produced in reaction (2) oxidized more sulfide as in reaction (1), closing the cycle and given the net reaction:\nThe net products of the reaction are soluble ferrous sulfate and sulfuric acid.\nThe microbial oxidation process occurs at the cell membrane of the bacteria. The electrons pass into the cells and are used in biochemical processes to produce energy for the bacteria while reducing oxygen to water. The critical reaction is the oxidation of sulfide by ferric iron. The main role of the bacterial step is the regeneration of this reactant.\nThe process for copper is very similar, but the efficiency and kinetics depend on the copper mineralogy. The most efficient minerals are supergene minerals such as chalcocite, Cu2S and covellite, CuS. The main copper mineral chalcopyrite (CuFeS2) is not leached very efficiently, which is why the dominant copper-producing technology remains flotation, followed by smelting and refining. The leaching of CuFeS2 follows the two stages of being dissolved and then further oxidised, with Cu2+ ions being left in solution.\nChalcopyrite leaching:\nnet reaction:\nIn general, sulfides are first oxidized to elemental sulfur, whereas disulfides are oxidized to give thiosulfate, and the processes above can be applied to other sulfidic ores. Bioleaching of non-sulfidic ores such as pitchblende also uses ferric iron as an oxidant (e.g., UO2 + 2 Fe3+ ==&gt; UO22+ + 2 Fe2+). In this case, the sole purpose of the bacterial step is the regeneration of Fe3+. Sulfidic iron ores can be added to speed up the process and provide a source of iron. Bioleaching of non-sulfidic ores by layering of waste sulfides and elemental sulfur, colonized by \"Acidithiobacillus\" spp., has been accomplished, which provides a strategy for accelerated leaching of materials that do not contain sulfide minerals.\nFurther processing.\nThe dissolved copper (Cu2+) ions are removed from the solution by ligand exchange solvent extraction, which leaves other ions in the solution. The copper is removed by bonding to a ligand, which is a large molecule consisting of a number of smaller groups, each possessing a lone electron pair. The ligand-copper complex is extracted from the solution using an organic solvent such as kerosene:\nThe ligand donates electrons to the copper, producing a complex - a central metal atom (copper) bonded to the ligand. Because this complex has no charge, it is no longer attracted to polar water molecules and dissolves in the kerosene, which is then easily separated from the solution. Because the initial reaction is reversible, it is determined by pH. Adding concentrated acid reverses the equation, and the copper ions go back into an aqueous solution.\nThen the copper is passed through an electro-winning process to increase its purity: An electric current is passed through the resulting solution of copper ions. Because copper ions have a 2+ charge, they are attracted to the negative cathodes and collect there.\nThe copper can also be concentrated and separated by displacing the copper with Fe from scrap iron:\nThe electrons lost by the iron are taken up by the copper. Copper is the oxidising agent (it accepts electrons), and iron is the reducing agent (it loses electrons).\nTraces of precious metals such as gold may be left in the original solution. Treating the mixture with sodium cyanide in the presence of free oxygen dissolves the gold. The gold is removed from the solution by adsorbing (taking it up on the surface) to charcoal.\nWith fungi.\nSeveral species of fungi can be used for bioleaching. Fungi can be grown on many different substrates, such as electronic scrap, catalytic converters, and fly ash from municipal waste incineration. Experiments have shown that two fungal strains (\"Aspergillus niger, Penicillium simplicissimum\") were able to mobilize Cu and Sn by 65%, and Al, Ni, Pb, and Zn by more than 95%. \"Aspergillus niger\" can produce some organic acids such as citric acid. This form of leaching does not rely on microbial oxidation of metal but rather uses microbial metabolism as source of acids that directly dissolve the metal.\nFeasibility.\nEconomic feasibility.\nBioleaching is in general simpler and, therefore, cheaper to operate and maintain than traditional processes, since fewer specialists are needed to operate complex chemical plants. And low concentrations are not a problem for bacteria because they simply ignore the waste that surrounds the metals, attaining extraction yields of over 90% in some cases. These microorganisms actually gain energy by breaking down minerals into their constituent elements. The company simply collects the ions out of the solution after the bacteria have finished.\nBioleaching can be used to extract metals from low concentration ores such as gold that are too poor for other technologies. It can be used to partially replace the extensive crushing and grinding that translates to prohibitive cost and energy consumption in a conventional process. Because the lower cost of bacterial leaching outweighs the time it takes to extract the metal.\nHigh concentration ores, such as copper, are more economical to smelt rather bioleach due to the slow speed of the bacterial leaching process compared to smelting. The slow speed of bioleaching introduces a significant delay in cash flow for new mines. Nonetheless, at the largest copper mine of the world, Escondida in Chile the process seems to be favorable.\nEconomically it is also very expensive and many companies once started can not keep up with the demand and end up in debt.\nIn space.\nIn 2020 scientists showed, with an experiment with different gravity environments on the ISS, that microorganisms could be employed to mine useful elements from basaltic rocks via bioleaching in space.\nEnvironmental impact.\nThe process is more environmentally friendly than traditional extraction methods. For the company this can translate into profit, since the necessary limiting of sulfur dioxide emissions during smelting is expensive. Less landscape damage occurs, since the bacteria involved grow naturally, and the mine and surrounding area can be left relatively untouched. As the bacteria breed in the conditions of the mine, they are easily cultivated and recycled.\nToxic chemicals are sometimes produced in the process. Sulfuric acid and H+ ions that have been formed can leak into the ground and surface water turning it acidic, causing environmental damage. Heavy ions such as iron, zinc, and arsenic leak during acid mine drainage. When the pH of this solution rises, as a result of dilution by fresh water, these ions precipitate, forming \"Yellow Boy\" pollution. For these reasons, a setup of bioleaching must be carefully planned, since the process can lead to a biosafety failure. Unlike other methods, once started, bioheap leaching cannot be quickly stopped, because leaching would still continue with rainwater and natural bacteria. Projects like Finnish Talvivaara proved to be environmentally and economically disastrous."}
{"id": "4113", "revid": "16121786", "url": "https://en.wikipedia.org/wiki?curid=4113", "title": "Bouldering", "text": "Bouldering is a form of rock climbing that is performed on small rock formations or artificial rock walls without the use of ropes or harnesses. While bouldering can be done without any equipment, most climbers use climbing shoes to help secure footholds, chalk to keep their hands dry and to provide a firmer grip, and bouldering mats to prevent injuries from falls. Unlike free solo climbing, which is also performed without ropes, bouldering problems (the sequence of moves that a climber performs to complete the climb) are usually less than tall. Traverses, which are a form of boulder problem, require the climber to climb horizontally from one end to another. Artificial climbing walls allow boulderers to climb indoors in areas without natural boulders. In addition, bouldering competitions take place in both indoor and outdoor settings.\nThe sport was originally a method of training for roped climbs and mountaineering, so climbers could practice specific moves at a safe distance from the ground. Additionally, the sport served to build stamina and increase finger strength. Throughout the 20th century, bouldering evolved into a separate discipline. Individual problems are assigned ratings based on difficulty. Although there have been various rating systems used throughout the history of bouldering, modern problems usually use either the V-scale or the Fontainebleau scale.\nOutdoor bouldering.\nThe characteristics of boulder problems depend largely on the type of rock being climbed. For example, granite often features long cracks and slabs while sandstone rocks are known for their steep overhangs and frequent horizontal breaks. Limestone and volcanic rock are also used for bouldering.\nThere are many prominent bouldering areas throughout the United States, including Hueco Tanks in Texas, Mount Blue Sky in Colorado, The Appalachian Mountains in The Eastern United States, and The Buttermilks in Bishop, California. Squamish, British Columbia is one of the most popular bouldering areas in Canada. Europe is also home to a number of bouldering sites, such as Fontainebleau in France, Meschia in Italy, Albarrac\u00edn in Spain, and various mountains throughout Switzerland.\nIndoor bouldering.\nArtificial climbing walls are used to simulate boulder problems in an indoor environment, usually at climbing gyms. These walls are constructed with wooden panels, polymer cement panels, concrete shells, or precast molds of actual rock walls. Holds, usually made of plastic, are then bolted onto the wall to create problems. Some problems use steep overhanging surfaces which force the climber to support much of their weight using their upper body strength.\nClimbing gyms often feature multiple problems within the same section of wall. Historically, the most common method route-setters used to designate the intended problem was by placing colored tape next to each hold. For example, red tape would indicate one bouldering problem while green tape would be used to set a different problem in the same area. Indoor bouldering requires very little in terms of equipment: at minimum, climbing shoes; at maximum, a chalk bag, chalk, a brush, and climbing shoes.\nGrading.\nBouldering problems are assigned numerical difficulty ratings by route-setters and climbers. The two most widely used rating systems are the V-scale and the Fontainebleau system.\nThe V-scale, which originated in the United States, is an open-ended rating system with higher numbers indicating a higher degree of difficulty. The V1 rating indicates that a problem can be completed by a novice climber in good physical condition after several attempts. The scale begins at V0, and as of 2024, the highest V rating that has been assigned to a bouldering problem is V17. Some climbing gyms also use a VB grade to indicate beginner problems.\nThe Fontainebleau scale follows a similar system, with each numerical grade divided into three ratings with the letters \"a\", \"b\", and \"c\". For example, Fontainebleau 7A roughly corresponds with V6, while Fontainebleau 7C+ is equivalent to V10. In both systems, grades are further differentiated by appending \"+\" to indicate a small increase in difficulty. Despite this level of specificity, ratings of individual problems are often controversial, as ability level is not the only factor that affects how difficult a problem may be for a particular climber. Height, arm length, flexibility, and other body characteristics can also affect difficulty.\nHighball bouldering.\nHighball bouldering is \"a sub-discipline of bouldering in which climbers seek out tall, imposing lines to climb ropeless above crash pads.\" It may have begun in 1961 when John Gill, without top-rope rehearsal or bouldering pads (which did not exist), bouldered a steep face on a granite spire called \"The Thimble\". In 2002 Jason Kehl completed the first highball at double-digit V-difficulty, called Evilution, a boulder in the Buttermilks of California, earning the grade of V12.\nImportant milestone ascents in this style include:\nCompetition bouldering.\nThe International Federation of Sport Climbing (IFSC) employs an indoor format (although competitions can also take place in an outdoor setting) that breaks the competition into three rounds: qualifications, semi-finals, and finals. The rounds feature different sets of four to six boulder problems, and each competitor has a fixed amount of time to attempt each problem. At the end of each round, competitors are ranked by the number of completed problems with ties settled by the total number of attempts taken to solve the problems.\nSome competitions only permit climbers a fixed number of attempts at each problem with a timed rest period in between. In an open-format competition, all climbers compete simultaneously, and are given a fixed amount of time to complete as many problems as possible. More points are awarded for more difficult problems, while points are deducted for multiple attempts on the same problem.\nIn 2012, the IFSC submitted a proposal to the International Olympic Committee (IOC) to include lead climbing in the 2020 Summer Olympics. The proposal was later revised to an \"overall\" competition, which would feature bouldering, lead climbing, and speed climbing. In 2016, the International Olympic Committee (IOC) officially approved climbing, along with four other sports, as an Olympic sport, based on their \"impact on gender equality, the youth appeal of the sports and the legacy value of adding them to the Tokyo Games\".\nHistory.\nModern bouldering.\nModern recreational climbing began in the late 19th century in England, southeastern Germany, northern Italy, and France. Bouldering on the rocks of Fontainbleau outside of Paris began in the late 1800s, with the first guidebook written by Maurice Martin in 1945. Bouldering as training or a recreational past-time began also in the late 1800s in England and perhaps elsewhere. Oscar Eckenstein was an early proponent. In the late 1950s, John Gill, who was called \"the father of modern bouldering\", combined gymnastics with rock climbing, and felt that the best place to do that was on boulders. He developed a rating system that was closed-ended: B1 problems were as difficult as the most challenging roped routes of the time, B2 problems were more difficult, and B3 problems had been completed once. He also introduced chalk as a method of keeping the climber's hands dry, promoted a dynamic climbing style, and emphasized the importance of strength training to complement skill. As Gill improved in ability and influence, his ideas became the norm.\nIn the 1980s, two important training tools emerged. One important training tool was bouldering mats, also referred to as \"crash pads\", which protected against injuries from falling and enabled boulderers to climb in areas that would have been too dangerous otherwise. The second important tool was indoor climbing walls, which helped spread the sport to areas without outdoor climbing and allowed serious climbers to train year-round. As the sport grew in popularity, new bouldering areas were developed throughout Europe and the United States, and more athletes began participating in bouldering competitions. The visibility of the sport greatly increased in the early 2000s, as YouTube videos and climbing blogs helped boulderers around the world to quickly learn techniques, find hard problems, and announce newly completed projects.\nNotable ascents.\nNotable boulder climbs are chronicled by the climbing media to track progress in boulder climbing standards and levels of technical difficulty; in contrast, the hardest traditional climbing routes tend to be of lower technical difficulty due to the additional burden of having to place protection during the course of the climb, and due to the lack of any possibility of using natural protection on the most extreme climbs.\nAs of November 2022, the world's hardest bouldering routes are \"Burden of Dreams\" by Nalle Hukkataival and \"Return of the Sleepwalker\" by Daniel Woods, both at proposed grades of . There are a number of routes with a confirmed climbing grade of , the first of which was \"Gioia\" by Christian Core in 2008 (and confirmed by Adam Ondra in 2011). As of December 2021, female climbers Josune Bereziartu, Ashima Shiraishi, and Kaddi Lehmann have repeated boulder problems at the boulder grade. On July 28, 2023, Katie Lamb became the first female climber to climb an -rated boulder by repeating \"Box Therapy\" at Rocky Mountain National Park. However, after Brooke Raboutou repeated the climb In October 2023, the boulder was ultimately downgraded to .\nEquipment.\nUnlike other climbing sports, bouldering can be performed safely and effectively with very little equipment, an aspect which makes the discipline highly appealing, but opinions differ. While bouldering pioneer John Sherman asserted that \"The only gear really needed to go bouldering is boulders,\" others suggest the use of climbing shoes and a chalkbag \u2013 a small pouch where ground-up chalk is kept \u2013 as the bare minimum, and more experienced boulderers typically bring multiple pairs of climbing shoes, chalk, brushes, crash pads, and a skincare kit.\nClimbing shoes have the most direct impact on performance. Besides protecting the climber's feet from rough surfaces, climbing shoes are designed to help the climber secure footholds. Climbing shoes typically fit much tighter than other athletic footwear and often curl the toes downwards to enable precise footwork. They are manufactured in a variety of different styles to perform in different situations. Stiffer shoes excel at securing small edges, whereas softer shoes provide greater sensitivity. The front of the shoe, called the \"toe box\", can be asymmetric, which performs well on overhanging rocks, or symmetric, which is better suited for vertical problems and slabs.\nTo absorb sweat, most boulderers use gymnastics chalk on their hands, stored in a chalk bag, which can be tied around the waist (also called sport climbing chalk bags), allowing the climber to reapply chalk during the climb. There are also versions of floor chalk bags (also called bouldering chalk bags), which are usually bigger than sport climbing chalk bags and are meant to be kept on the floor while climbing; this is because boulders do not usually have so many movements as to require chalking up more than once. Different sizes of brushes are used to remove excess chalk and debris from boulders in between climbs; they are often attached to the end of a long straight object in order to reach higher holds. Crash pads, also referred to as bouldering mats, are foam cushions placed on the ground to protect climbers from injury after falling.\nBoulder problems are generally shorter than from ground to top. This makes the sport significantly safer than free solo climbing, which is also performed without ropes, but with no upper limit on the height of the climb. However, minor injuries are common in bouldering, particularly sprained ankles and wrists. To prevent injuries, boulderers position crash pads near the boulder to provide a softer landing, as well as one or more spotters to help redirect the climber towards the pads. Upon landing, boulderers employ falling techniques similar to those used in gymnastics: spreading the impact across the entire body to avoid bone fractures and positioning limbs to allow joints to move freely throughout the impact.\nTechniques.\nAlthough every type of rock climbing requires a high level of strength and technique, bouldering is the most dynamic form of the sport, requiring the highest level of power and placing considerable strain on the body. Training routines that strengthen fingers and forearms are useful in preventing injuries such as tendonitis and ruptured ligaments. However, as with other forms of climbing, bouldering technique begins with proper footwork. Leg muscles are significantly stronger than arm muscles; thus, proficient boulderers use their arms to maintain balance and body positioning as much as possible, relying on their legs to push them up the rock. Boulderers also keep their arms straight with their shoulders engaged whenever feasible, allowing their bones to support their body weight rather than their muscles.\nBouldering movements are described as either \"static\" or \"dynamic\". Static movements are those that are performed slowly, with the climber's position controlled by maintaining contact on the boulder with the other three limbs. Dynamic movements use the climber's momentum to reach holds that would be difficult or impossible to secure statically, with an increased risk of falling if the movement is not performed accurately.\nEnvironmental impact.\nBouldering can damage vegetation that grows on rocks, such as moss and lichens. This can occur as a result of the climber intentionally cleaning the boulder, or unintentionally from repeated use of handholds and footholds. Vegetation on the ground surrounding the boulder can also be damaged from overuse, particularly by climbers laying down crash pads. Soil erosion can occur when boulderers trample vegetation while hiking off of established trails, or when they unearth small rocks near the boulder in an effort to make the landing zone safer in case of a fall. Other environmental concerns include littering, improperly disposed feces, and graffiti. These issues have caused some land managers to prohibit bouldering, as was the case in Tea Garden, a popular bouldering area in Rocklands, South Africa."}
{"id": "4115", "revid": "1272989904", "url": "https://en.wikipedia.org/wiki?curid=4115", "title": "Boiling point", "text": " The boiling point of a substance is the temperature at which the vapor pressure of a liquid equals the pressure surrounding the liquid and the liquid changes into a vapor.\nThe boiling point of a liquid varies depending upon the surrounding environmental pressure. A liquid in a partial vacuum, i.e., under a lower pressure, has a lower boiling point than when that liquid is at atmospheric pressure. Because of this, water boils at 100\u00b0C (or with scientific precision: ) under standard pressure at sea level, but at at altitude. For a given pressure, different liquids will boil at different temperatures.\nThe normal boiling point (also called the atmospheric boiling point or the atmospheric pressure boiling point) of a liquid is the special case in which the vapor pressure of the liquid equals the defined atmospheric pressure at sea level, one atmosphere. At that temperature, the vapor pressure of the liquid becomes sufficient to overcome atmospheric pressure and allow bubbles of vapor to form inside the bulk of the liquid. The standard boiling point has been defined by IUPAC since 1982 as the temperature at which boiling occurs under a pressure of one bar.\nThe heat of vaporization is the energy required to transform a given quantity (a mol, kg, pound, etc.) of a substance from a liquid into a gas at a given pressure (often atmospheric pressure).\nLiquids may change to a vapor at temperatures below their boiling points through the process of evaporation. Evaporation is a surface phenomenon in which molecules located near the liquid's edge, not contained by enough liquid pressure on that side, escape into the surroundings as vapor. On the other hand, boiling is a process in which molecules anywhere in the liquid escape, resulting in the formation of vapor bubbles within the liquid.\nSaturation temperature and pressure.\nA \"saturated liquid\" contains as much thermal energy as it can without boiling (or conversely a \"saturated vapor\" contains as little thermal energy as it can without condensing).\nSaturation temperature means \"boiling point\". The saturation temperature is the temperature for a corresponding saturation pressure at which a liquid boils into its vapor phase. The liquid can be said to be saturated with thermal energy. Any addition of thermal energy results in a phase transition.\nIf the pressure in a system remains constant (isobaric), a vapor at saturation temperature will begin to condense into its liquid phase as thermal energy (heat) is removed. Similarly, a liquid at saturation temperature and pressure will boil into its vapor phase as additional thermal energy is applied.\nThe boiling point corresponds to the temperature at which the vapor pressure of the liquid equals the surrounding environmental pressure. Thus, the boiling point is dependent on the pressure. Boiling points may be published with respect to the NIST, USA standard pressure of 101.325\u00a0kPa (1\u00a0atm), or the IUPAC standard pressure of 100.000\u00a0kPa (1\u00a0bar). At higher elevations, where the atmospheric pressure is much lower, the boiling point is also lower. The boiling point increases with increased pressure up to the critical point, where the gas and liquid properties become identical. The boiling point cannot be increased beyond the critical point. Likewise, the boiling point decreases with decreasing pressure until the triple point is reached. The boiling point cannot be reduced below the triple point.\nIf the heat of vaporization and the vapor pressure of a liquid at a certain temperature are known, the boiling point can be calculated by using the Clausius\u2013Clapeyron equation, thus:\nwhere:\nSaturation pressure is the pressure for a corresponding saturation temperature at which a liquid boils into its vapor phase. Saturation pressure and saturation temperature have a direct relationship: as saturation pressure is increased, so is saturation temperature.\nIf the temperature in a system remains constant (an \"isothermal\" system), vapor at saturation pressure and temperature will begin to condense into its liquid phase as the system pressure is increased. Similarly, a liquid at saturation pressure and temperature will tend to flash into its vapor phase as system pressure is decreased.\nThere are two conventions regarding the \"standard boiling point of water\": The \"normal boiling point\" is commonly given as (actually following the thermodynamic definition of the Celsius scale based on the kelvin) at a pressure of 1\u00a0atm (101.325\u00a0kPa). The IUPAC-recommended \"standard boiling point of water\" at a standard pressure of 100\u00a0kPa (1\u00a0bar) is . For comparison, on top of Mount Everest, at elevation, the pressure is about and the boiling point of water is .\nThe Celsius temperature scale was defined until 1954 by two points: 0\u00a0\u00b0C being defined by the water freezing point and 100\u00a0\u00b0C being defined by the water boiling point at standard atmospheric pressure.\nRelation between the normal boiling point and the vapor pressure of liquids.\nThe higher the vapor pressure of a liquid at a given temperature, the lower the normal boiling point (i.e., the boiling point at atmospheric pressure) of the liquid.\nThe vapor pressure chart to the right has graphs of the vapor pressures versus temperatures for a variety of liquids. As can be seen in the chart, the liquids with the highest vapor pressures have the lowest normal boiling points.\nFor example, at any given temperature, methyl chloride has the highest vapor pressure of any of the liquids in the chart. It also has the lowest normal boiling point (\u221224.2\u00a0\u00b0C), which is where the vapor pressure curve of methyl chloride (the blue line) intersects the horizontal pressure line of one atmosphere (atm) of absolute vapor pressure.\nThe critical point of a liquid is the highest temperature (and pressure) it will actually boil at.\nSee also Vapour pressure of water.\nBoiling point of chemical elements.\nThe element with the lowest boiling point is helium. Both the boiling points of rhenium and tungsten exceed 5000 K at standard pressure; because it is difficult to measure extreme temperatures precisely without bias, both have been cited in the literature as having the higher boiling point.\nBoiling point as a reference property of a pure compound.\nAs can be seen from the above plot of the logarithm of the vapor pressure vs. the temperature for any given pure chemical compound, its normal boiling point can serve as an indication of that compound's overall volatility. A given pure compound has only one normal boiling point, if any, and a compound's normal boiling point and melting point can serve as characteristic physical properties for that compound, listed in reference books. The higher a compound's normal boiling point, the less volatile that compound is overall, and conversely, the lower a compound's normal boiling point, the more volatile that compound is overall. Some compounds decompose at higher temperatures before reaching their normal boiling point, or sometimes even their melting point. For a stable compound, the boiling point ranges from its triple point to its critical point, depending on the external pressure. Beyond its triple point, a compound's normal boiling point, if any, is higher than its melting point. Beyond the critical point, a compound's liquid and vapor phases merge into one phase, which may be called a superheated gas. At any given temperature, if a compound's normal boiling point is lower, then that compound will generally exist as a gas at atmospheric external pressure. If the compound's normal boiling point is higher, then that compound can exist as a liquid or solid at that given temperature at atmospheric external pressure, and will so exist in equilibrium with its vapor (if volatile) if its vapors are contained. If a compound's vapors are not contained, then some volatile compounds can eventually evaporate away in spite of their higher boiling points.\nIn general, compounds with ionic bonds have high normal boiling points, if they do not decompose before reaching such high temperatures. Many metals have high boiling points, but not all. Very generally\u2014with other factors being equal\u2014in compounds with covalently bonded molecules, as the size of the molecule (or molecular mass) increases, the normal boiling point increases. When the molecular size becomes that of a macromolecule, polymer, or otherwise very large, the compound often decomposes at high temperature before the boiling point is reached. Another factor that affects the normal boiling point of a compound is the polarity of its molecules. As the polarity of a compound's molecules increases, its normal boiling point increases, other factors being equal. Closely related is the ability of a molecule to form hydrogen bonds (in the liquid state), which makes it harder for molecules to leave the liquid state and thus increases the normal boiling point of the compound. Simple carboxylic acids dimerize by forming hydrogen bonds between molecules. A minor factor affecting boiling points is the shape of a molecule. Making the shape of a molecule more compact tends to lower the normal boiling point slightly compared to an equivalent molecule with more surface area.\nMost volatile compounds (anywhere near ambient temperatures) go through an intermediate liquid phase while warming up from a solid phase to eventually transform to a vapor phase. By comparison to boiling, a sublimation is a physical transformation in which a solid turns directly into vapor, which happens in a few select cases such as with carbon dioxide at atmospheric pressure. For such compounds, a sublimation point is a temperature at which a solid turning directly into vapor has a vapor pressure equal to the external pressure.\nImpurities and mixtures.\nIn the preceding section, boiling points of pure compounds were covered. Vapor pressures and boiling points of substances can be affected by the presence of dissolved impurities (solutes) or other miscible compounds, the degree of effect depending on the concentration of the impurities or other compounds. The presence of non-volatile impurities such as salts or compounds of a volatility far lower than the main component compound decreases its mole fraction and the solution's volatility, and thus raises the normal boiling point in proportion to the concentration of the solutes. This effect is called boiling point elevation. As a common example, salt water boils at a higher temperature than pure water.\nIn other mixtures of miscible compounds (components), there may be two or more components of varying volatility, each having its own pure component boiling point at any given pressure. The presence of other volatile components in a mixture affects the vapor pressures and thus boiling points and dew points of all the components in the mixture. The dew point is a temperature at which a vapor condenses into a liquid. Furthermore, at any given temperature, the composition of the vapor is different from the composition of the liquid in most such cases. In order to illustrate these effects between the volatile components in a mixture, a boiling point diagram is commonly used. Distillation is a process of boiling and [usually] condensation which takes advantage of these differences in composition between liquid and vapor phases.\nBoiling point of water with elevation.\nFollowing is a table of the change in the boiling point of water with elevation, at intervals of 500 meters over the range of human habitation [the Dead Sea at to La Rinconada, Peru at ], then of 1,000 meters over the additional range of uninhabited surface elevation [up to Mount Everest at ], along with a similar range in Imperial."}
{"id": "4116", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=4116", "title": "Big Bang", "text": " \nThe Big Bang is a physical theory that describes how the universe expanded from an initial state of high density and temperature. The concept of an expanding universe was scientifically originated by physicist Alexander Friedmann in 1922 with the mathematical derivation of the Friedmann equations. The earliest empirical observation of an expanding universe is known as Hubble's law, published in work by physicist Edwin Hubble in 1929, which discerned that galaxies are moving away from Earth at a rate that accelerates proportionally with distance. Independent of Friedmann's work, and independent of Hubble's observations, physicist Georges Lema\u00eetre proposed that the universe emerged from a \"primeval atom\" in 1931, introducing the modern notion of the Big Bang.\nVarious cosmological models of the Big Bang explain the evolution of the observable universe from the earliest known periods through its subsequent large-scale form. These models offer a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the cosmic microwave background (CMB) radiation, and large-scale structure. The uniformity of the universe, known as the flatness problem, is explained through cosmic inflation: a sudden and very rapid expansion of space during the earliest moments.\nExtrapolating this cosmic expansion backward in time using the known laws of physics, the models describe an increasingly concentrated cosmos preceded by a singularity in which space and time lose meaning (typically named \"the Big Bang singularity\"). Physics lacks a widely accepted theory of quantum gravity that can model the earliest conditions of the Big Bang. In 1964 the CMB was discovered, which convinced many cosmologists that the competing steady-state model of cosmic evolution was falsified, since the Big Bang models predict a uniform background radiation caused by high temperatures and densities in the distant past. A wide range of empirical evidence strongly favors the Big Bang event, which is now essentially universally accepted. Detailed measurements of the expansion rate of the universe place the Big Bang singularity at an estimated \u00a0billion years ago, which is considered the age of the universe.\nThere remain aspects of the observed universe that are not yet adequately explained by the Big Bang models. After its initial expansion, the universe cooled sufficiently to allow the formation of subatomic particles, and later atoms. The unequal abundances of matter and antimatter that allowed this to occur is an unexplained effect known as baryon asymmetry. These primordial elements\u2014mostly hydrogen, with some helium and lithium\u2014later coalesced through gravity, forming early stars and galaxies. Astronomers observe the gravitational effects of an unknown dark matter surrounding galaxies. Most of the gravitational potential in the universe seems to be in this form, and the Big Bang models and various observations indicate that this excess gravitational potential is not created by baryonic matter, such as normal atoms. Measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to an unexplained phenomenon known as dark energy.\nFeatures of the models.\nThe Big Bang models offer a comprehensive explanation for a broad range of observed phenomena, including the abundances of the light elements, the CMB, large-scale structure, and Hubble's law. The models depend on two major assumptions: the universality of physical laws and the cosmological principle. The universality of physical laws is one of the underlying principles of the theory of relativity. The cosmological principle states that on large scales the universe is homogeneous and isotropic\u2014appearing the same in all directions regardless of location.\nThese ideas were initially taken as postulates, but later efforts were made to test each of them. For example, the first assumption has been tested by observations showing that the largest possible deviation of the fine-structure constant over much of the age of the universe is of order 10\u22125. Also, general relativity has passed stringent tests on the scale of the Solar System and binary stars.\nThe large-scale universe appears isotropic as viewed from Earth. If it is indeed isotropic, the cosmological principle can be derived from the simpler Copernican principle, which states that there is no preferred (or special) observer or vantage point. To this end, the cosmological principle has been confirmed to a level of 10\u22125 via observations of the temperature of the CMB. At the scale of the CMB horizon, the universe has been measured to be homogeneous with an upper bound on the order of 10% inhomogeneity, as of 1995.\nHorizons.\nAn important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not yet had time to reach earth. This places a limit or a \"past horizon\" on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never \"catch up\" to very distant objects. This defines a \"future horizon\", which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker (FLRW) metric that describes the expansion of the universe.\nOur understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.\nThermalization.\nSome processes in the early universe occurred too slowly, compared to the expansion rate of the universe, to reach approximate thermodynamic equilibrium. Others were fast enough to reach thermalization. The parameter usually used to find out whether a process in the very early universe has reached thermal equilibrium is the ratio between the rate of the process (usually rate of collisions between particles) and the Hubble parameter. The larger the ratio, the more time particles had to thermalize before they were too far away from each other.\nTimeline.\nAccording to the Big Bang models, the universe at the beginning was very hot and very compact, and since then it has been expanding and cooling.\nSingularity.\nIn the absence of a perfect cosmological principle, extrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. This irregular behavior, known as the gravitational singularity, indicates that general relativity is not an adequate description of the laws of physics in this regime. Models based on general relativity alone cannot fully extrapolate toward the singularity. In some proposals, such as the emergent Universe models, the singularity is replaced by another cosmological epoch. A different approach identifies the initial singularity as a singularity predicted by some models of the Big Bang theory to have existed before the Big Bang event.\nThis primordial singularity is itself sometimes called \"the Big Bang\", but the term can also refer to a more generic early hot, dense phase of the universe. In either case, \"the Big Bang\" as an event is also colloquially referred to as the \"birth\" of our universe since it represents the point in history where the universe can be verified to have entered into a regime where the laws of physics as we understand them (specifically general relativity and the Standard Model of particle physics) work. Based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background, the time that has passed since that event\u2014known as the \"age of the universe\"\u2014is 13.8\u00a0billion years.\nDespite being extremely dense at this time\u2014far denser than is usually required to form a black hole\u2014the universe did not re-collapse into a singularity. Commonly used calculations and limits for explaining gravitational collapse are usually based upon objects of relatively constant size, such as stars, and do not apply to rapidly expanding space such as the Big Bang. Since the early universe did not immediately collapse into a multitude of black holes, matter at that time must have been very evenly distributed with a negligible density gradient.\nInflation and baryogenesis.\nThe earliest phases of the Big Bang are subject to much speculation, given the lack of available data. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures, and was very rapidly expanding and cooling. The period up to 10\u221243 seconds into the expansion, the Planck epoch, was a phase in which the four fundamental forces\u2014the electromagnetic force, the strong nuclear force, the weak nuclear force, and the gravitational force, were unified as one. In this stage, the characteristic scale length of the universe was the Planck length, , and consequently had a temperature of approximately 1032 degrees Celsius. Even the very concept of a particle breaks down in these conditions. A proper understanding of this period awaits the development of a theory of quantum gravity. The Planck epoch was succeeded by the grand unification epoch beginning at 10\u221243 seconds, where gravitation separated from the other forces as the universe's temperature fell.\nAt approximately 10\u221237 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially, unconstrained by the light speed invariance, and temperatures dropped by a factor of 100,000. This concept is motivated by the flatness problem, where the density of matter and energy is very close to the critical density needed to produce a flat universe. That is, the shape of the universe has no overall geometric curvature due to gravitational influence. Microscopic quantum fluctuations that occurred because of Heisenberg's uncertainty principle were \"frozen in\" by inflation, becoming amplified into the seeds that would later form the large-scale structure of the universe. At a time around 10\u221236 seconds, the electroweak epoch begins when the strong nuclear force separates from the other forces, with only the electromagnetic force and weak nuclear force remaining unified.\nInflation stopped locally at around 10\u221233 to 10\u221232 seconds, with the observable universe's volume having increased by a factor of at least 1078. Reheating followed as the inflaton field decayed, until the universe obtained the temperatures required for the production of a quark\u2013gluon plasma as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle\u2013antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons\u2014of the order of one part in 30\u00a0million. This resulted in the predominance of matter over antimatter in the present universe.\nCooling.\nThe universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry-breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form, with the electromagnetic force and weak nuclear force separating at about 10\u221212 seconds.\nAfter about 10\u221211 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. At about 10\u22126 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was no longer high enough to create either new proton\u2013antiproton or neutron\u2013antineutron pairs. A mass annihilation immediately followed, leaving just one in 108 of the original matter particles and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).\nA few minutes into the expansion, when the temperature was about a billion kelvin and the density of matter in the universe was comparable to the current density of Earth's atmosphere, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis (BBN). Most protons remained uncombined as hydrogen nuclei.\nAs the universe cooled, the rest energy density of matter came to gravitationally dominate that of the photon radiation. The recombination epoch began after about 379,000 years, when the electrons and nuclei combined into atoms (mostly hydrogen), which were able to emit radiation. This relic radiation, which continued through space largely unimpeded, is known as the cosmic microwave background.\nStructure formation.\nAfter the recombination epoch, the slightly denser regions of the uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter (CDM), warm dark matter, hot dark matter, and baryonic matter. The best measurements available, from the Wilkinson Microwave Anisotropy Probe (WMAP), show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold. (Warm dark matter is ruled out by early reionization.) This CDM is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%.\nIn an \"extended model\" which includes hot dark matter in the form of neutrinos, then the \"physical baryon density\" formula_1 is estimated at 0.023. (This is different from the 'baryon density' formula_2 expressed as a fraction of the total matter/energy density, which is about 0.046.) The corresponding cold dark matter density formula_3 is about 0.11, and the corresponding neutrino density formula_4 is estimated to be less than 0.0062.\nCosmic acceleration.\nIndependent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which appears to homogeneously permeate all of space. Observations suggest that 73% of the total energy density of the present day universe is in this form. When the universe was very young it was likely infused with dark energy, but with everything closer together, gravity predominated, braking the expansion. Eventually, after billions of years of expansion, the declining density of matter relative to the density of dark energy allowed the expansion of the universe to begin to accelerate.\nDark energy in its simplest formulation is modeled by a cosmological constant term in Einstein field equations of general relativity, but its composition and mechanism are unknown. More generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theory.\nAll of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the lambda-CDM model of cosmology, which uses the independent frameworks of quantum mechanics and general relativity. There are no easily testable models that would describe the situation prior to approximately 10\u221215 seconds. Understanding this earliest of eras in the history of the universe is one of the greatest unsolved problems in physics.\nConcept history.\nEtymology.\nEnglish astronomer Fred Hoyle is credited with coining the term \"Big Bang\" during a talk for a March 1949 BBC Radio broadcast, saying: \"These theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past.\" However, it did not catch on until the 1970s.\nIt is popularly reported that Hoyle, who favored an alternative \"steady-state\" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models. Helge Kragh writes that the evidence for the claim that it was meant as a pejorative is \"unconvincing\", and mentions a number of indications that it was not a pejorative.\nThe term itself has been argued to be a misnomer because it evokes an explosion. The argument is that whereas an explosion suggests expansion into a surrounding space, the Big Bang only describes the intrinsic expansion of the contents of the universe. Another issue pointed out by Santhosh Mathew is that bang implies sound, which is not an important feature of the model. An attempt to find a more suitable alternative was not successful.\nDevelopment.\nThe Big Bang models developed from observations of the structure of the universe and from theoretical considerations. In 1912, Vesto Slipher measured the first Doppler shift of a \"spiral nebula\" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were \"island universes\" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from the Einstein field equations, showing that the universe might be expanding in contrast to the static universe model advocated by Albert Einstein at that time.\nIn 1924, American astronomer Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Starting that same year, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929, Hubble discovered a correlation between distance and recessional velocity\u2014now known as Hubble's law.\nIndependently deriving Friedmann's equations in 1927, Georges Lema\u00eetre, a Belgian physicist and Roman Catholic priest, proposed that the recession of the nebulae was due to the expansion of the universe. He inferred the relation that Hubble would later observe, given the cosmological principle. In 1931, Lema\u00eetre went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a \"primeval atom\" where and when the fabric of time and space came into existence.\nIn the 1920s and 1930s, almost every major cosmologist preferred an eternal steady-state universe, and several complained that the beginning of time implied by the Big Bang imported religious concepts into physics; this objection was later repeated by supporters of the steady-state theory. This perception was enhanced by the fact that the originator of the Big Bang concept, Lema\u00eetre, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, \"viz\"., that matter is eternal. A beginning in time was \"repugnant\" to him. Lema\u00eetre, however, disagreed:\nDuring the 1930s, other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard C. Tolman) and Fritz Zwicky's tired light hypothesis.\nAfter World War II, two distinct possibilities emerged. One was Fred Hoyle's steady-state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lema\u00eetre's Big Bang theory, advocated and developed by George Gamow, who introduced BBN and whose associates, Ralph Alpher and Robert Herman, predicted the CMB. Ironically, it was Hoyle who coined the phrase that came to be applied to Lema\u00eetre's theory, referring to it as \"this \"big bang\" idea\" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over steady state. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe.\nIn 1968 and 1970, Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang models with the introduction of an epoch of rapid expansion in the early universe he called \"inflation\". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).\nIn the mid-1990s, observations of certain globular clusters appeared to indicate that they were about 15\u00a0billion years old, which conflicted with most then-current estimates of the age of the universe (and indeed with the age measured today). This issue was later resolved when new computer simulations, which included the effects of mass loss due to stellar winds, indicated a much younger age for globular clusters.\nSignificant progress in Big Bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as the Cosmic Background Explorer (COBE), the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.\nObservational evidence.\nThe earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis (BBN). More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures. These are sometimes called the \"four pillars\" of the Big Bang models.\nPrecise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently the subject of most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are unsolved problems in physics.\nHubble's law and the expansion of the universe.\nObservations of distant galaxies and quasars show that these objects are redshifted: the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:\nformula_5\nwhere\nHubble's law implies that the universe is uniformly expanding everywhere. This cosmic expansion was predicted from general relativity by Friedmann in 1922 and Lema\u00eetre in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang model as developed by Friedmann, Lema\u00eetre, Robertson, and Walker.\nThe theory requires the relation formula_9 to hold at all times, where formula_7 is the proper distance, formula_6 is the recessional velocity, and formula_6, formula_13, and formula_7 vary as the universe expands (hence we write formula_8 to denote the present-day Hubble \"constant\"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity formula_6. For distances comparable to the size of the observable universe, the attribution of the cosmological redshift becomes more ambiguous, although its interpretation as a kinematic Doppler shift remains the most natural one.\nAn unexplained discrepancy with the determination of the Hubble constant is known as Hubble tension. Techniques based on observation of the CMB suggest a lower value of this constant compared to the quantity derived from measurements based on the cosmic distance ladder.\nCosmic microwave background radiation.\nIn 1964, Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s, the radiation was found to be approximately consistent with a blackbody spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725\u00a0K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded the 1978 Nobel Prize in Physics.\nThe \"surface of last scattering\" corresponding to emission of the CMB occurs shortly after \"recombination\", the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around , the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.\nIn 1989, NASA launched COBE, which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 104, and measured a residual temperature of 2.726\u00a0K (more recent measurements have revised this figure down slightly to 2.7255\u00a0K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 105. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results.\nDuring the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000\u20132001, several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.\nIn early 2003, the first results of the Wilkinson Microwave Anisotropy Probe were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The \"Planck\" space probe was launched in May 2009. Other ground and balloon-based cosmic microwave background experiments are ongoing.\nAbundance of primordial elements.\nUsing Big Bang models, it is possible to calculate the expected concentration of the isotopes helium-4 (4He), helium-3 (3He), deuterium (2H), and lithium-7 (7Li) in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by abundance) are about 0.25 for 4He:H, about 10\u22123 for 2H:H, about 10\u22124 for 3He:H, and about 10\u22129 for 7Li:H.\nThe measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for 4He, and off by a factor of two for 7Li (this anomaly is known as the cosmological lithium problem); in the latter two cases, there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by BBN is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to \"tune\" the Big Bang to produce much more or less than 20\u201330% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products, should have more helium than deuterium or more deuterium than 3He, and in constant ratios, too.\nGalactic evolution and distribution.\nDetailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current Big Bang models. A combination of observations and theory suggest that the first quasars and galaxies formed within a billion years after the Big Bang, and since then, larger structures have been forming, such as galaxy clusters and superclusters.\nPopulations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.\nPrimordial gas clouds.\nIn 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. Despite being sensitive to carbon, oxygen, and silicon, these three elements were not detected in these two clouds. Since the clouds of gas have no detectable levels of heavy elements, they likely formed in the first few minutes after the Big Bang, during BBN.\nOther lines of evidence.\nThe age of the universe as estimated from the Hubble expansion and the CMB is now in agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars. It is also in agreement with age estimates based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background. The agreement of independent measurements of this age supports the Lambda-CDM (\u039bCDM) model, since the model is used to relate some of the measurements to an age estimate, and all estimates turn agree. Still, some observations of objects from the relatively early universe (in particular quasar APM 08279+5255) raise concern as to whether these objects had enough time to form so early in the \u039bCDM model.\nThe prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev\u2013Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.\nFuture observations.\nFuture gravitational-wave observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the Big Bang.\nProblems and related issues in physics.\nAs with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang models. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflation theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang concept still under intense investigation by cosmologists and astrophysicists.\nBaryon asymmetry.\nIt is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of normal matter, rather than antimatter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.\nDark energy.\nMeasurements of the redshift\u2013magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, general relativity requires that much of the energy in the universe consists of a component with large negative pressure, dubbed \"dark energy\".\nDark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the \"missing\" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure--baryon acoustic oscillations--as a cosmic ruler.\nNegative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.\nThe dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem, sometimes called the \"most embarrassing problem in physics\", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from Planck units.\nDark matter.\nDuring the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.\nIndirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.\nAdditionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.\nHorizon problem.\nThe horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit\u2014the particle horizon\u2014on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.\nA resolution to this apparent inconsistency is offered by inflation theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.\nHeisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to a cosmic scale. These fluctuations served as the seeds for all the current structures in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been confirmed by measurements of the CMB.\nA related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.\nMagnetic monopoles.\nThe magnetic monopole objection was raised in the late 1970s. Grand unified theories (GUTs) predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.\nFlatness problem.\nThe flatness problem (also known as the oldness problem) is an observational problem associated with a FLRW. The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to be \"flat\". Observations indicate the universe is consistent with being flat.\nThe problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10\u221243 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 1014 of its critical value, or it would not exist as it does today.\nMisconceptions.\nOne of the common misconceptions about the Big Bang model is that it fully explains the origin of the universe. However, the Big Bang model does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state. It is misleading to visualize the Big Bang by comparing its size to everyday objects. When the size of the universe at Big Bang is described, it refers to the size of the observable universe, and not the entire universe.\nAnother common misconception is that the Big Bang must be understood as the expansion of space and not in terms of the contents of space exploding apart. In fact, either description can be accurate. The expansion of space (implied by the FLRW metric) is only a mathematical convention, corresponding to a choice of coordinates on spacetime. There is no generally covariant sense in which space expands.\nThe recession speeds associated with Hubble's law are not velocities in a relativistic sense (for example, they are not related to the spatial components of 4-velocities). Therefore, it is not remarkable that according to Hubble's law, galaxies farther than the Hubble distance recede faster than the speed of light. Such recession speeds do not correspond to faster-than-light travel.\nMany popular accounts attribute the cosmological redshift to the expansion of space. This can be misleading because the expansion of space is only a coordinate choice. The most natural interpretation of the cosmological redshift is that it is a Doppler shift.\nImplications.\nGiven current understanding, scientific extrapolations about the future of the universe are only possible for finite durations, albeit for much longer periods than the current age of the universe. Anything beyond that becomes increasingly speculative. Likewise, at present, a proper understanding of the origin of the universe can only be subject to conjecture.\nPre\u2013Big Bang cosmology.\nThe Big Bang explains the evolution of the universe from a starting density and temperature that is well beyond humanity's capability to replicate, so extrapolations to the most extreme conditions and earliest times are necessarily more speculative. Lema\u00eetre called this initial state the \"\"primeval atom\" while Gamow called the material \"ylem\"\". How the initial state of the universe originated is still an open question, but the Big Bang model does constrain some of its characteristics. For example, if specific laws of nature were to come to existence in a random way, inflation models show, some combinations of these are far more probable, partly explaining why our Universe is rather stable. Another possible explanation for the stability of the Universe could be a hypothetical multiverse, which assumes every possible universe to exist, and thinking species could only emerge in those stable enough. A flat universe implies a balance between gravitational potential energy and other energy forms, requiring no additional energy to be created.\nThe Big Bang theory, built upon the equations of classical general relativity, indicates a singularity at the origin of cosmic time, and such an infinite energy density may be a physical impossibility. However, the physical theories of general relativity and quantum mechanics as currently realized are not applicable before the Planck epoch, and correcting this will require the development of a correct treatment of quantum gravity. Certain quantum gravity treatments, such as the Wheeler\u2013DeWitt equation, imply that time itself could be an emergent property. As such, physics may conclude that time did not exist before the Big Bang.\nWhile it is not known what could have preceded the hot dense state of the early universe or how and why it originated, or even whether such questions are sensible, speculation abounds on the subject of \"cosmogony\".\nSome speculative proposals in this regard, each of which entails untested hypotheses, are:\nProposals in the last two categories see the Big Bang as an event in either a much larger and older universe or in a multiverse.\nUltimate fate of the universe.\nBefore observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started\u2014a Big Crunch.\nAlternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. Collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would very gradually asymptotically approach absolute zero\u2014a Big Freeze. Moreover, if protons are unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.\nModern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The \u039bCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.\nReligious and philosophical interpretations.\nAs a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, while others argue that Big Bang cosmology makes the notion of a creator superfluous."}
{"id": "4119", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=4119", "title": "Bock", "text": "Bock () is a strong German beer, usually a dark lager.\nHistory.\nThe style now known as \"Bock\" was first brewed in the 14th century in the Hanseatic town of Einbeck in Lower Saxony.\nThe style was later adopted in Bavaria by Munich brewers in the 17th century. Due to their Bavarian accent, citizens of Munich pronounced \"Einbeck\" as \"ein Bock\" (\"a billy goat\"), and thus the beer became known as \"Bock\". A goat often appears on bottle labels.\nBock is historically associated with special occasions, often religious festivals such as Christmas, Easter, or Lent (\"\"). Bock has a long history of being brewed and consumed by Bavarian monks as a source of nutrition during times of fasting.\nStyles.\nSubstyles of Bock include:\nTraditionally Bock is a sweet, relatively strong (6.3\u20137.6% by volume), lightly hopped lager registering between 20 and 30 International Bitterness Units (IBUs). The beer should be clear, with colour ranging from light copper to brown, and a bountiful, persistent off-white head. The aroma should be malty and toasty, possibly with hints of alcohol, but no detectable hops or fruitiness. The mouthfeel is smooth, with low to moderate carbonation and no astringency. The taste is rich and toasty, sometimes with a bit of caramel. The low-to-undetectable presence of hops provides just enough bitterness so that the sweetness is not cloying and the aftertaste is muted.\nMaibock.\nThe Maibock style \u2013 also known as Heller Bock or Lente Bock in the Netherlandsis a strong pale lager, lighter in colour and with more hop presence.\nColour can range from deep gold to light amber with a large, creamy, persistent white head, and moderate to moderately high carbonation, while alcohol content ranges from 6.3% to 8.1% by volume. The flavour is typically less malty than a traditional Bock, and may be drier, hoppier, and more bitter, but still with a relatively low hop flavour, with a mild spicy or peppery quality from the hops, increased carbonation and alcohol content.\nDoppelbock.\n\"Doppelbock\" or \"Double Bock\" is a stronger version of traditional Bock that was first brewed in Munich by the Paulaner Friars, a Franciscan order founded by St. Francis of Paula.\nHistorically, Doppelbock was high in alcohol and sweetness. The story is told that it served as \"liquid bread\" for the Friars during times of fasting when solid food was not permitted. However, historian Mark Dredge, in his book \"A Brief History of Lager\", says that this story is myth and that the monks produced Doppelbock to supplement their order's vegetarian diet all year.\nToday, Doppelbock is still strongranging from 7% to 12% or more by volume. It is clear, with colour ranging from dark gold, for the paler version, to dark brown with ruby highlights for a darker version. It has a large, creamy, persistent head (although head retention may be impaired by alcohol in the stronger versions). The aroma is intensely malty, with some toasty notes, and possibly some alcohol presence as well; darker versions may have a chocolate-like or fruity aroma. The flavour is very rich and malty, with noticeable alcoholic strength, and little or no detectable hops (16\u201326 IBUs).\nPaler versions may have a drier finish. The monks who originally brewed Doppelbock named their beer \"Salvator\" (literally \"Savior\", but actually a malapropism for \"Sankt Vater\", \"St. Father\", originally brewed for the feast of St. Francis of Paola on 2 April which often falls in Lent), which today is trademarked by Paulaner.\nBrewers of modern Doppelbock often add \"-ator\" to their beer's name as a signpost of the style; there are 200 \"-ator\" Doppelbock names registered with the German patent office.\nThe following are representative examples of the style: Paulaner Salvator, Ayinger Celebrator, Weihenstephaner Korbinian, Andechser Doppelbock Dunkel, Spaten Optimator, Augustiner Brau Maximator, Tucher Bajuvator, Weltenburger Kloster Asam-Bock, Capital Autumnal Fire, EKU 28, Eggenberg Urbock 23\u00ba, Bell's Consecrator, Moretti La Rossa, Samuel Adams Double Bock, Tr\u00f6egs Tr\u00f6egenator Double Bock, Wasatch Brewery Devastator, Great Lakes Doppelrock, Abita Andygator, Wolverine State Brewing Company Predator, Burly Brewing's Burlynator, Monteith's Doppel Bock, and Christian Moerlein Emancipator Doppelbock.\nEisbock.\nEisbock is a traditional specialty beer of the Kulmbach district of Bavaria, made by partially freezing a Doppelbock and removing the water ice to concentrate the flavour and alcohol content, which ranges from 8.6% to 14.3% by volume.\nIt is clear, with a colour ranging from deep copper to dark brown in colour, often with ruby highlights. Although it can pour with a thin off-white head, head retention is frequently impaired by the higher alcohol content. The aroma is intense, with no hop presence, but frequently can contain fruity notes, especially of prunes, raisins, and plums. Mouthfeel is full and smooth, with significant alcohol, although this should not be hot or sharp. The flavour is rich and sweet, often with toasty notes, and sometimes hints of chocolate, always balanced by a significant alcohol presence.\nThe following are representative examples of the style: Colorado Team Brew \"Warning Sign\", Kulmbacher Reichelbr\u00e4u Eisbock, Eggenberg, Schneider Aventinus Eisbock, Urbock Dunkel Eisbock, Franconia Brewing Company Ice Bock 17%.\nThe strongest ice beer, Strength in Numbers, was a one-time collaboration in 2020 between Schorschbrau of Germany and BrewDog of Scotland, who had competed with each other in the early years of the 21st century to produce the world's strongest beer. \"Strength in Numbers\" was created using traditional ice distillation, reaching a final strength of 57.8% ABV.\nWeizenbock.\nWeizenbock is a style that replaces some of the barley in the grain bill with 40\u201360% wheat. It was first produced in Bavaria in 1907 by G.\u00a0Schneider &amp; Sohn and was named \"Aventinus\" after 16th-century Bavarian historian Johannes Aventinus. The style combines darker Munich malts and top-fermenting wheat beer yeast, brewed at the strength of a Doppelbock."}
{"id": "4122", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=4122", "title": "B roll", "text": ""}
{"id": "4124", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=4124", "title": "Bantu languages", "text": "The Bantu languages (English: , Proto-Bantu: *bant\u028a\u0300) are a language family of about 600 languages that are spoken by the Bantu peoples of Central, Southern, Eastern and Southeast Africa. They form the largest branch of the Southern Bantoid languages.\nThe total number of Bantu languages is estimated at between 440 and 680 distinct languages, depending on the definition of \"language\" versus \"dialect\". Many Bantu languages borrow words from each other, and some are mutually intelligible. Some of the languages are spoken by a very small number of people, for example the Kabwa language was estimated in 2007 to be spoken by only 8500 people but was assessed to be a distinct language.\nThe total number of Bantu speakers is estimated to be around 350 million in 2015 (roughly 30% of the population of Africa or 5% of the world population). Bantu languages are largely spoken southeast of Cameroon, and throughout Central, Southern, Eastern, and Southeast Africa. About one-sixth of Bantu speakers, and one-third of Bantu languages, are found in the Democratic Republic of the Congo.\nThe most widely spoken Bantu language by number of speakers is Swahili, with 16 million native speakers and 80 million L2 speakers (2015). Most native speakers of Swahili live in Tanzania, where it is a national language, while as a second language, it is taught as a mandatory subject in many schools in East Africa, and is a lingua franca of the East African Community.\nOther major Bantu languages include Lingala with more than 20 million speakers (Congo, DRC), followed by Zulu with 13.56 million speakers (South Africa), Xhosa at a distant third place with 8.2 million speakers (South Africa and Zimbabwe), and Shona with less than 10 million speakers (if Manyika and Ndau are included), while Sotho-Tswana languages (Sotho, Tswana and Pedi) have more than 15 million speakers (across Botswana, Lesotho, South Africa, and Zambia). Zimbabwe has Kalanga, Matebele, Nambiya, and Xhosa speakers. \"Ethnologue\" separates the largely mutually intelligible Kinyarwanda and Kirundi, which together have 20 million speakers.\nName.\nThe similarity among dispersed Bantu languages had been observed as early as the 17th century. The term \"Bantu\" as a name for the group was not coined but \"noticed\" or \"identified\" (as \"B\u00e2-ntu\") by Wilhelm Bleek as the first European in 1857 or 1858, and popularized in his \"Comparative Grammar\" of 1862. He noticed the term to represent the word for \"people\" in loosely reconstructed Proto-Bantu, from the plural noun class prefix \"*ba-\" categorizing \"people\", and the root \"*nt\u028a\u0300-\" \"some (entity), any\" (e.g. Xhosa \"umntu\" \"person\", \"abantu\" \"people\"; Zulu \"umuntu\" \"person\", \"abantu\" \"people\").\nThere is no native term for the people who speak Bantu languages because they are not an ethnic group. People speaking Bantu languages refer to their languages by ethnic endonyms, which did not have an indigenous concept prior to European contact for the larger ethnolinguistic phylum named by 19th-century European linguists. Bleek's identification was inspired by the anthropological observation of groups frequently self-identifying as \"people\" or \"the true people\" (as is the case, for example, with the term \"Khoikhoi\", but this is a \"kare\" \"praise address\" and not an ethnic name).\nThe term \"narrow Bantu\", excluding those languages classified as Bantoid by Malcolm Guthrie (1948), was introduced in the 1960s.\nThe prefix \"ba-\" specifically refers to people. Endonymically, the term for cultural objects, including language, is formed with the \"ki-\" noun class (Nguni \"\u00edsi-\"), as in KiSwahili (Swahili language and culture), IsiZulu (Zulu language and culture) and KiGanda (Ganda religion and culture).\nIn the 1980s, South African linguists suggested referring to these languages as \"KiNtu.\" The word \"kintu\" exists in some places, but it means \"thing\", with no relation to the concept of \"language\". In addition, delegates at the African Languages Association of Southern Africa conference in 1984 reported that, in some places, the term \"Kintu\" has a derogatory significance. This is because \"kintu\" refers to \"things\" and is used as a dehumanizing term for people who have lost their dignity.\nIn addition, \"Kintu\" is a figure in some mythologies.\nIn the 1990s, the term \"Kintu\" was still occasionally used by South African linguists. But in contemporary decolonial South African linguistics, the term \"Ntu languages\" is used.\nWithin the fierce debate among linguists about the word \"Bantu\", Seidensticker (2024) indicates that there has been a \"profound conceptual trend in which a \"purely technical [term] without any non-linguistic connotations was transformed into a designation referring indiscriminately to language, culture, society, and race\".\"\nOrigin.\nThe Bantu languages descend from a common Proto-Bantu language, which is believed to have been spoken in what is now Cameroon in Central Africa. An estimated 2,500\u20133,000 years ago (1000 BC to 500 BC), speakers of the Proto-Bantu language began a series of migrations eastward and southward, carrying agriculture with them. This Bantu expansion came to dominate Sub-Saharan Africa east of Cameroon, an area where Bantu peoples now constitute nearly the entire population. Some other sources estimate the Bantu Expansion started closer to 3000 BC.\nThe technical term Bantu, meaning \"human beings\" or simply \"people\", was first used by Wilhelm Bleek (1827\u20131875), as the concept is reflected in many of the languages of this group. A common characteristic of Bantu languages is that they use words such as \"muntu\" or \"mutu\" for \"human being\" or in simplistic terms \"person\", and the plural prefix for human nouns starting with \"mu-\" (class 1) in most languages is \"ba-\" (class 2), thus giving \"bantu\" for \"people\". Bleek, and later Carl Meinhof, pursued extensive studies comparing the grammatical structures of Bantu languages.\nClassification.\nThe most widely used classification is an alphanumeric coding system developed by Malcolm Guthrie in his 1948 classification of the Bantu languages. It is mainly geographic. The term \"narrow Bantu\" was coined by the \"Benue\u2013Congo Working Group\" to distinguish Bantu as recognized by Guthrie, from the Bantoid languages not recognized as Bantu by Guthrie.\nIn recent times, the distinctiveness of Narrow Bantu as opposed to the other Southern Bantoid languages has been called into doubt, but the term is still widely used.\nThere is no true genealogical classification of the (Narrow) Bantu languages. Until recently most attempted classifications only considered languages that happen to fall within traditional Narrow Bantu, but there seems to be a continuum with the related languages of South Bantoid.\nAt a broader level, the family is commonly split in two depending on the reflexes of proto-Bantu tone patterns: many Bantuists group together parts of zones A through D (the extent depending on the author) as \"Northwest Bantu\" or \"Forest Bantu\", and the remainder as \"Central Bantu\" or \"Savanna Bantu\". The two groups have been described as having mirror-image tone systems: where Northwest Bantu has a high tone in a cognate, Central Bantu languages generally have a low tone, and vice versa.\nNorthwest Bantu is more divergent internally than Central Bantu, and perhaps less conservative due to contact with non-Bantu Niger\u2013Congo languages; Central Bantu is likely the innovative line cladistically. Northwest Bantu is not a coherent family, but even for Central Bantu the evidence is lexical, with little evidence that it is a historically valid group.\nAnother attempt at a detailed genetic classification to replace the Guthrie system is the 1999 \"Tervuren\" proposal of Bastin, Coupez, and Mann. However, it relies on lexicostatistics, which, because of its reliance on overall similarity rather than shared innovations, may predict spurious groups of conservative languages that are not closely related. Meanwhile, \"Ethnologue\" has added languages to the Guthrie classification which Guthrie overlooked, while removing the Mbam languages (much of zone A), and shifting some languages between groups (much of zones D and E to a new zone J, for example, and part of zone L to K, and part of M to F) in an apparent effort at a semi-genetic, or at least semi-areal, classification. This has been criticized for sowing confusion in one of the few unambiguous ways to distinguish Bantu languages. Nurse &amp; Philippson (2006) evaluate many proposals for low-level groups of Bantu languages, but the result is not a complete portrayal of the family. \"Glottolog\" has incorporated many of these into their classification.\nThe languages that share Dahl's law may also form a valid group, Northeast Bantu. The infobox at right lists these together with various low-level groups that are fairly uncontroversial, though they continue to be revised. The development of a rigorous genealogical classification of many branches of Niger\u2013Congo, not just Bantu, is hampered by insufficient data.\nComputational phylogenetic classifications.\nSimplified phylogeny of northwestern branches of Bantu by Grollemund (2012):\nOther computational phylogenetic analyses of Bantu include Currie et al. (2013), Grollemund et al. (2015), Rexova et al. 2006, Holden et al., 2016, and Whiteley et al. 2018.\nGlottolog classification.\nGlottolog (2021) does not consider the older geographic classification by Guthrie relevant for its ongoing classification based on more recent linguistic studies, and divides Bantu into four main branches: Bantu A-B10-B20-B30, Central-Western Bantu, East Bantu and Mbam-Bube-Jarawan.\nLanguage structure.\nGuthrie reconstructed both the phonemic inventory and the vocabulary of Proto-Bantu.\nThe most prominent grammatical characteristic of Bantu languages is the extensive use of affixes (see Sotho grammar and Ganda noun classes for detailed discussions of these affixes). Each noun belongs to a class, and each language may have several numbered classes, somewhat like grammatical gender in European languages. The class is indicated by a prefix that is part of the noun, as well as agreement markers on verb and qualificative roots connected with the noun. Plurality is indicated by a change of class, with a resulting change of prefix. All Bantu languages are agglutinative.\nThe verb has a number of prefixes, though in the western languages these are often treated as independent words. In Swahili, for example, \"Kitoto kidogo kimekisoma\" (for comparison, \"Kamwana kadoko karikuverenga\" in Shona language) means 'The small child has read it [a book]'. \"kitoto\" 'child' governs the adjective prefix \"ki-\" (representing the diminutive form of the word) and the verb subject prefix \"a-\". Then comes perfect tense \"-me-\" and an object marker \"-ki-\" agreeing with implicit \"kitabu\" 'book' (from Arabic \"kitab\"). Pluralizing to 'children' gives \"Vitoto vidogo vimekisoma\" (\"Vana vadoko varikuverenga\" in Shona), and pluralizing to 'books' (\"vitabu\") gives \"vitoto vidogo vimevisoma\".\nBantu words are typically made up of open syllables of the type CV (consonant-vowel) with most languages having syllables exclusively of this type. The Bushong language recorded by Vansina, however, has final consonants, while slurring of the final syllable (though written) is reported as common among the Tonga of Malawi. The morphological shape of Bantu words is typically CV, VCV, CVCV, VCVCV, etc.; that is, any combination of CV (with possibly a V- syllable at the start). In other words, a strong claim for this language family is that almost all words end in a vowel, precisely because closed syllables (CVC) are not permissible in most of the documented languages, as far as is understood.\nThis tendency to avoid consonant clusters in some positions is important when words are imported from English or other non-Bantu languages. An example from Chewa: the word \"school\", borrowed from English, and then transformed to fit the sound patterns of this language, is \"sukulu\". That is, \"sk-\" has been broken up by inserting an epenthetic \"-u-\"; \"-u\" has also been added at the end of the word. Another example is \"buledi\" for \"bread\". Similar effects are seen in loanwords for other non-African CV languages like Japanese. However, a clustering of sounds at the beginning of a syllable can be readily observed in such languages as Shona, and the Makua languages.\nWith few exceptions, such as Kiswahili and Rutooro, Bantu languages are tonal and have two to four register tones.\nReduplication.\nReduplication is a common morphological phenomenon in Bantu languages and is usually used to indicate frequency or intensity of the action signalled by the (unreduplicated) verb stem.\nWell-known words and names that have reduplication include:\nRepetition emphasizes the repeated word in the context that it is used. For instance, \"Mwenda pole hajikwai,\" means \"He who goes slowly doesn't trip,\" while, \"Pole pole ndio mwendo,\" means \"A slow but steady pace wins the race.\" The latter repeats \"pole\" to emphasize the consistency of slowness of the pace.\nAs another example, \"Haraka haraka\" would mean \"hurrying just for the sake of hurrying\" (reckless hurry), as in \"Njoo! Haraka haraka\" [come here! Hurry, hurry].\nIn contrast, there are some words in some of the languages in which reduplication has the opposite meaning. It usually denotes short durations, or lower intensity of the action, and also means a few repetitions or a little bit more.\nNoun class.\nThe following is a list of nominal classes in Bantu languages:\nSyntax.\nVirtually all Bantu languages have a subject\u2013verb\u2013object word order, with some exceptions, such as \nthe Nen language, which has a subject\u2013object\u2013verb word order.\nBy country.\nFollowing is an incomplete list of the principal Bantu languages of each country. Included are those languages that constitute at least 1% of the population and have at least 10% the number of speakers of the largest Bantu language in the country.\nMost languages are referred to in English without the class prefix (\"Swahili\", \"Tswana\", \"Ndebele\"), but are sometimes seen with the (language-specific) prefix (\"Kiswahili\", \"Setswana\", \"Sindebele\"). In a few cases prefixes are used to distinguish languages with the same root in their name, such as Tshiluba and Kiluba (both \"Luba\"), Umbundu and Kimbundu (both \"Mbundu\"). The prefixless form typically does not occur in the language itself, but is the basis for other words based on the ethnicity. So, in the country of Botswana the people are the \"Batswana\", one person is a \"Motswana\", and the language is \"Setswana\"; and in Uganda, centred on the kingdom of \"Buganda\", the dominant ethnicity are the \"Baganda\" (singular \"Muganda\"), whose language is \"Luganda\".\nSouth Africa.\nAccording to the South African National Census of 2011:\nGeographic areas.\nMap 1 shows Bantu languages in Africa and map 2 a magnification of the Benin, Nigeria and Cameroon area, as of July 2017.\nBantu words popularised in western cultures.\nA case has been made out for borrowings of many place-names and even misremembered rhymes \u2013 chiefly from one of the Luba varieties \u2013 in the USA.\nSome words from various Bantu languages have been borrowed into western languages. These include: \nWriting systems.\nAlong with the Latin script and Arabic script orthographies, there are also some modern indigenous writing systems used for Bantu languages:"}
{"id": "4126", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=4126", "title": "Ballroom dancing", "text": ""}
{"id": "4127", "revid": "27335766", "url": "https://en.wikipedia.org/wiki?curid=4127", "title": "Bearing", "text": "Bearing(s) may refer to:"}
{"id": "4129", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=4129", "title": "BOMARC", "text": ""}
{"id": "4130", "revid": "7291611", "url": "https://en.wikipedia.org/wiki?curid=4130", "title": "CIM-10 Bomarc", "text": "The Boeing CIM-10 Bomarc (\"Boeing Michigan Aeronautical Research Center\") (IM-99 Weapon System prior to September 1962) was a supersonic ramjet powered long-range surface-to-air missile (SAM) used during the Cold War for the air defense of North America. In addition to being the first operational long-range SAM and the first operational pulse doppler aviation radar, it was the only SAM deployed by the United States Air Force.\nStored horizontally in a launcher shelter with a movable roof, the missile was erected, fired vertically using rocket boosters to high altitude, and then tipped over into a horizontal Mach 2.5 cruise powered by ramjet engines. This lofted trajectory allowed the missile to operate at a maximum range as great as . Controlled from the ground for most of its flight, when it reached the target area it was commanded to begin a dive, activating an onboard active radar homing seeker for terminal guidance. A radar proximity fuse detonated the warhead, either a large conventional explosive or the W40 nuclear warhead.\nThe Air Force originally planned for a total of 52 sites covering most of the major cities and industrial regions in the US. The United States Army was deploying their own systems at the same time, and the two services fought constantly both in political circles and in the press. Development dragged on, and by the time it was ready for deployment in the late 1950s, the nuclear threat had moved from manned bombers to the intercontinental ballistic missile (ICBM). By this time the Army had successfully deployed the much shorter range Nike Hercules that they claimed filled any possible need through the 1960s, in spite of Air Force claims to the contrary.\nAs testing continued, the Air Force reduced its plans to sixteen sites, and then again to eight with an additional two sites in Canada. The first US site was declared operational in 1959, but with only a single working missile. Bringing the rest of the missiles into service took years, by which time the system was obsolete. Deactivations began in 1969 and by 1972 all Bomarc sites had been shut down. A small number were used as target drones, and only a few remain on display today.\nDesign and development.\nInitial studies.\nDuring World War II, the US Army Air Force (USAAF) concluded that existing anti-aircraft guns, only marginally effective against existing generations of propeller-driven aircraft, would not be effective at all against the emerging jet-powered designs. Like the Germans and British before them, they concluded the only successful defence would be to use guided weapons.\nAs early as 1944 the United States Army started exploring anti-aircraft missiles, examining a variety of concepts. At the time, two basic concepts appeared possible; one would use a short-range rocket that flew directly at the target from below following a course close to the line-of-sight, and the other would fly up to the target's altitude and then tip over and fly horizontally towards the target like a fighter aircraft. As both concepts seemed promising, the Army Air Force was given the task of developing the airplane-like design, while the Army Ordnance Department was given the more ballistic collision-course concept. Official requirements were published in 1945.\nOfficial requirements were published in 1945; Bell Laboratories won the Ordnance contract for a short-range line-of-sight weapon under Project Nike, while a team of players led by Boeing won the contract for a long-range design known as Ground-to-Air Pilotless Aircraft, or GAPA. GAPA moved to the United States Air Force when that branch was formed in 1947. In 1946, the USAAF also started two early research projects into anti-missile systems in Project Thumper (MX-795) and Project Wizard (MX-794).\nBomarc A.\nFormally organized in 1946 under USAAF project MX-606, by 1950 Boeing had launched more than 100 test rockets in various configurations, all under the designator XSAM-A-1 GAPA. The tests were very promising, and Boeing received a USAF contract in 1949 to develop a production design under project MX-1599.\nThe MX-1599 missile was to be a ramjet-powered, nuclear-armed long-range surface-to-air missile to defend the Continental United States from high-flying bombers. The Michigan Aerospace Research Center (MARC) was added to the project soon afterward, and this gave the new missile its name Bomarc (for Boeing and MARC). In 1951, the USAF decided to emphasize its point of view that missiles were nothing else than pilotless aircraft by assigning aircraft designators to its missile projects, and anti-aircraft missiles received F-for-Fighter designations. The Bomarc became the F-99.\nBy this time, the Army's Nike project was progressing well and would enter operational service in 1953. This led the Air Force to begin a lengthy series of attacks on the Army in the press, a common occurrence at the time known as \"policy by press release\". When the Army released its first official information on Ajax to the press, the Air Force responded by leaking information on BOMARC to Aviation Week, and continued to denigrate Nike in the press over the next few years, in one case showing a graphic of Washington being destroyed by nuclear bombs that Ajax failed to stop.\nTests of the XF-99 test vehicles began in September 1952 and continued through early 1955. The XF-99 tested only the liquid-fueled booster rocket, which would accelerate the missile to ramjet ignition speed. In February 1955, tests of the XF-99A propulsion test vehicles began. These included live ramjets, but still had no guidance system or warhead. The designation YF-99A had been reserved for the operational test vehicles. In August 1955, the USAF discontinued the use of aircraft-like type designators for missiles, and the XF-99A and YF-99A became XIM-99A and YIM-99A, respectively. Originally the USAF had allocated the designation IM-69, but this was changed (possibly at Boeing's request to keep number 99) to IM-99 in October 1955.\nBy this time, Ajax was widely deployed around the United States and some overseas locations, and the Army was beginning to develop its much more powerful successor, Nike Hercules. Hercules was an existential threat to BOMARC, as its much greater range and nuclear warhead filled many of the roles that BOMARC was designed for. A new round of fighting in the press broke out, capped by an article in \"The New York Times\" entitled \"Air Force Calls Army Nike Unfit To Guard Nation\".\nIn October 1957, the first YIM-99A production-representative prototype flew with full guidance, and succeeded to pass the target within destructive range. In late 1957, Boeing received the production contract for the IM-99A Bomarc A, and in September 1959, the first IM-99A squadron became operational.\nThe IM-99A had an operational radius of and was designed to fly at Mach\u00a02.5\u20132.8 at a cruising altitude of . It was long and weighed . Its armament was either a conventional warhead or a W40 nuclear warhead (7\u201310 kiloton yield). A liquid-fuel rocket engine boosted the Bomarc to Mach 2, when its Marquardt RJ43-MA-3 ramjet engines, fueled by 80-octane gasoline, would take over for the remainder of the flight. This was the same model of engine used to power the Lockheed X-7, the Lockheed AQM-60 Kingfisher drone used to test air defenses, and the Lockheed D-21 launched from the back of an M-21, although the Bomarc and Kingfisher engines used different materials due to the longer duration of their flights.\nOperational units.\nThe operational IM-99A missiles were based horizontally in semi-hardened shelters, nicknamed \"coffins\". After the launch order, the shelter's roof would slide open, and the missile raised to the vertical. After the missile was supplied with fuel for the booster rocket, it would be launched by the Aerojet General LR59-AJ-13 booster. After sufficient speed was reached, the Marquardt RJ43-MA-3 ramjets would ignite and propel the missile to its cruise speed of Mach\u00a02.8 at an altitude of .\nWhen the Bomarc was within of the target, its own Westinghouse AN/DPN-34 radar guided the missile to the interception point. The maximum range of the IM-99A was , and it was fitted with either a conventional high-explosive or a 10\u00a0kiloton W-40 nuclear fission warhead.\nThe Bomarc relied on the Semi-Automatic Ground Environment (SAGE), an automated control system used by NORAD for detecting, tracking and intercepting enemy bomber aircraft. SAGE allowed for remote launching of the Bomarc missiles, which were housed in a constant combat-ready basis in individual launch shelters in remote areas. At the height of the program, there were 14 Bomarc sites located in the US and two in Canada.\nBomarc B.\nThe liquid-fuel booster of the Bomarc A had several drawbacks. It took two minutes to fuel before launch, which could be a long time in high-speed intercepts, and its hypergolic propellants (hydrazine and nitric acid) were very dangerous to handle, leading to several serious accidents.\nAs soon as high-thrust solid-fuel rockets became a reality in the mid-1950s, the USAF began to develop a new solid-fueled Bomarc variant, the IM-99B Bomarc\u00a0B. It used a Thiokol XM51 booster, and also had improved Marquardt RJ43-MA-7 (and finally the RJ43-MA-11) ramjets. The first IM-99B was launched in May 1959, but problems with the new propulsion system delayed the first fully successful flight until July 1960, when a supersonic MQM-15A Regulus II drone was intercepted. Because the new booster required less space in the missile, more ramjet fuel could be carried, thus increasing the range to . The terminal homing system was also improved, using the world's first pulse Doppler search radar, the Westinghouse AN/DPN-53. All Bomarc\u00a0Bs were equipped with the W-40 nuclear warhead. In June 1961, the first IM-99B squadron became operational, and Bomarc\u00a0B quickly replaced most Bomarc\u00a0A missiles. On 23 March 1961, a Bomarc\u00a0B successfully intercepted a Regulus\u00a0II cruise missile flying at , thus achieving the highest interception in the world up to that date.\nBoeing built 570 Bomarc missiles between 1957 and 1964, 269 CIM-10A, 301 CIM-10B.\nIn September 1958 Air Research &amp; Development Command decided to transfer the Bomarc program from its testing at Cape Canaveral Air Force Station to a new facility on Santa Rosa Island, south of Eglin AFB Hurlburt Field on the Gulf of Mexico. To operate the facility and to provide training and operational evaluation in the missile program, Air Defense Command established the 4751st Air Defense Wing (Missile) (4751st ADW) on 15 January 1958. The first launch from Santa Rosa took place on 15 January 1959.\nOperational history.\nIn 1955, to support a program which called for 40 squadrons of BOMARC (120 missiles to a squadron for a total of 4,800 missiles), ADC reached a decision on the location of these 40 squadrons and suggested operational dates for each. The sequence was as follows: ... l. McGuire 1/60 2. Suffolk 2/60 3. Otis 3/60 4. Dow 4/60 5. Niagara Falls 1/61 6. Plattsburgh 1/61 7. Kinross 2/61 8. K.I. Sawyer 2/61 9. Langley 2/61 10. Truax 3/61 11. Paine 3/61 12. Portland 3/61 ... At the end of 1958, ADC plans called for construction of the following BOMARC bases in the following order: l. McGuire 2. Suffolk 3. Otis 4. Dow 5. Langley 6. Truax 7. Kinross 8. Duluth 9. Ethan Allen 10. Niagara Falls 11. Paine 12. Adair 13. Travis 14. Vandenberg 15. San Diego 16. Malmstrom 17. Grand Forks 18. Minot 19. Youngstown 20. Seymour-Johnson 21. Bunker Hill 22. Sioux Falls 23. Charleston 24. McConnell 25. Holloman 26. McCoy 27. Amarillo 28. Barksdale 29. Williams.\nUnited States.\nThe first USAF operational Bomarc squadron was the 46th Air Defense Missile Squadron (ADMS), organized on 1 January 1959 and activated on 25 March. The 46th ADMS was assigned to the New York Air Defense Sector at McGuire Air Force Base, New Jersey. The training program, under the 4751st Air Defense Wing used technicians acting as instructors and was established for a four-month duration. Training included missile maintenance; SAGE operations and launch procedures, including the launch of an unarmed missile at Eglin. In September 1959 the squadron assembled at their permanent station, the Bomarc site near McGuire AFB, and trained for operational readiness. The first Bomarc-A were used at McGuire on 19 September 1959 with Kincheloe AFB getting the first operational IM-99Bs. While several of the squadrons replicated earlier fighter interceptor unit numbers, they were all new organizations with no previous historical counterpart.\nADC's initial plans called for some 52 Bomarc sites around the United States with 120 missiles each but as defense budgets decreased during the 1950s the number of sites dropped substantially. Ongoing development and reliability problems didn't help, nor did Congressional debate over the missile's usefulness and necessity. In June 1959, the Air Force authorized 16 Bomarc sites with 56 missiles each; the initial five would get the IM-99A with the remainder getting the IM-99B. However, in March 1960, HQ USAF cut deployment to eight sites in the United States and two in Canada.\nBomarc incident.\nWithin a year of operations, a Bomarc\u00a0A with a nuclear warhead caught fire at McGuire AFB on 7 June 1960 after its on-board helium tank exploded. While the missile's explosives did not detonate, the heat melted the warhead and released plutonium, which the fire crews spread. The Air Force and the Atomic Energy Commission cleaned up the site and covered it with concrete. This was the only major incident involving the weapon system. The site remained in operation for several years following the fire. Since its closure in 1972, the area has remained off limits, primarily due to low levels of plutonium contamination. Between 2002 and 2004, 21,998 cubic yards of contaminated debris and soils were shipped to what was then known as Envirocare, located in Utah.\nModification and deactivation.\nIn 1962, the US Air Force started using modified A-models as drones; following the October 1962 tri-service redesignation of aircraft and weapons systems they became CQM-10As. Otherwise the air defense missile squadrons maintained alert while making regular trips to Santa Rosa Island for training and firing practice. After the inactivation of the 4751st ADW(M) on 1 July 1962 and transfer of Hurlburt to Tactical Air Command for air commando operations the 4751st Air Defense Squadron (Missile) remained at Hurlburt and Santa Rosa Island for training purposes.\nIn 1964, the liquid-fueled Bomarc-A sites and squadrons began to be deactivated. The sites at Dow and Suffolk County closed first. The remainder continued to be operational for several more years while the government started dismantling the air defense missile network. Niagara Falls was the first BOMARC B installation to close, in December 1969; the others remained on alert through 1972. In April 1972, the last Bomarc B in U.S. Air Force service was retired at McGuire and the 46th ADMS inactivated and the base was deactivated.\nIn the era of the intercontinental ballistic missiles the Bomarc, designed to intercept relatively slow manned bombers, had become a useless asset. The remaining Bomarc missiles were used by all armed services as high-speed target drones for tests of other air-defense missiles. The Bomarc A and Bomarc B targets were designated as CQM-10A and CQM-10B, respectively.\nFollowing the accident, the McGuire complex has never been sold or converted to other uses and remains in Air Force ownership, making it the most intact site of the eight in the US. It has been nominated to the National Register of Historic Sites. Although a number of IM-99/CIM-10 Bomarcs have been placed on public display, because of concerns about the possible environmental hazards of the thoriated magnesium structure of the airframe several have been removed from public view.\nRuss Sneddon, director of the Air Force Armament Museum, Eglin Air Force Base, Florida provided information about missing CIM-10 exhibit airframe serial 59\u20132016, one of the museum's original artifacts from its founding in 1975 and donated by the 4751st Air Defense Squadron at Hurlburt Field, Eglin Auxiliary Field 9, Eglin AFB. As of December 2006, the suspect missile was stored in a secure compound behind the Armaments Museum. In December 2010, the airframe was still on premises, but partly dismantled.\nCanada.\nThe Bomarc Missile Program was highly controversial in Canada. The Progressive Conservative government of Prime Minister John Diefenbaker initially agreed to deploy the missiles, and shortly thereafter controversially scrapped the Avro Arrow, a supersonic manned interceptor aircraft, arguing that the missile program made the Arrow unnecessary.\nInitially, it was unclear whether the missiles would be equipped with nuclear warheads. By 1960 it became known that the missiles were to have a nuclear payload, and a debate ensued about whether Canada should accept nuclear weapons. Ultimately, the Diefenbaker government decided that the Bomarcs should not be equipped with nuclear warheads. The dispute split the Diefenbaker Cabinet, and led to the collapse of the government in 1963. The Official Opposition and Liberal Party leader Lester B. Pearson originally was against nuclear missiles, but reversed his personal position and argued in favour of accepting nuclear warheads. He won the 1963 election, largely on the basis of this issue, and his new Liberal government proceeded to accept nuclear-armed Bomarcs, with the first being deployed on 31 December 1963. When the nuclear warheads were deployed, Pearson's wife, Maryon, resigned her honorary membership in the anti-nuclear weapons group, Voice of Women.\nCanadian operational deployment of the Bomarc involved the formation of two specialized Surface/Air Missile squadrons. The first to begin operations was No. 446 SAM Squadron at RCAF Station North Bay, which was the command and control center for both squadrons. With construction of the compound and related facilities completed in 1961, the squadron received its Bomarcs in 1961, without nuclear warheads. The squadron became fully operational from 31 December 1963, when the nuclear warheads arrived, until disbanding on 31 March 1972. All the warheads were stored separately and under control of Detachment 1 of the USAF 425th Munitions Maintenance Squadron at Stewart Air Force Base. During operational service, the Bomarcs were maintained on stand-by, on a 24-hour basis, but were never fired, although the squadron test-fired the missiles at Eglin AFB, Florida on annual winter retreats.\nNo. 447 SAM Squadron operating out of RCAF Station La Macaza, Quebec, was activated on 15 September 1962 although warheads were not delivered until late 1963. The squadron followed the same operational procedures as No. 446, its sister squadron. With the passage of time the operational capability of the 1950s-era Bomarc system no longer met modern requirements; the Department of National Defence deemed that the Bomarc missile defense was no longer a viable system, and ordered both squadrons to be stood down in 1972. The bunkers and ancillary facilities remain at both former sites.\nOperators.\nLocations under construction but not activated. Each site was programmed for 28 IM-99B missiles:\nSurviving missiles.\nBelow is a list of museums or sites which have a Bomarc missile on display:\nImpact on popular music.\nThe Bomarc missile captured the imagination of the American and Canadian popular music industry, giving rise to a pop music group, the Bomarcs (composed mainly of servicemen stationed on a Florida radar site that tracked Bomarcs), a record label, Bomarc Records, and a moderately successful Canadian pop group, The Beau Marks."}
{"id": "4132", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=4132", "title": "Branco River", "text": "The Branco River (; Engl: \"White River\") is the principal affluent of the Rio Negro from the north.\nBasin.\nThe river drains the Guayanan Highlands moist forests ecoregion.\nIt is enriched by many streams from the Tepui highlands which separate Venezuela and Guyana from Brazil. Its two upper main tributaries are the Uraricoera and the Takutu. The latter almost links its sources with those of the Essequibo; during floods headwaters of the Branco and those of the Essequibo are connected, allowing a level of exchange in the aquatic fauna (such as fish) between the two systems.\nThe Branco flows nearly south, and finds its way into the Negro through several channels and a chain of lagoons similar to those of the latter river. It is long, up to its Uraricoera confluence. It has numerous islands, and, above its mouth, it is broken by a bad series of rapids.\nDischarge.\nAverage, minimum and maximum discharge of the Branco River at near mouth. Period from 1998 to 2022.\nWater chemistry.\nAs suggested by its name, the Branco (literally \"white\" in Portuguese) has whitish water that may appear almost milky due to the inorganic sediments it carries. It is traditionally considered a whitewater river, although the major seasonal fluctuations in its physico-chemical characteristics makes a classification difficult and some consider it clearwater. Especially the river's upper parts at the headwaters are clear and flow through rocky country, leading to the suggestion that sediments mainly originate from the lower parts. Furthermore, its chemistry and color may contradict each other compared to the traditional Amazonian river classifications. The Branco River has pH 6\u20137 and low levels of dissolved organic carbon.\nAlfred Russel Wallace mentioned the coloration in \"On the Rio Negro\", a paper read at the 13 June 1853 meeting of the Royal Geographical Society, in which he said: \"[The Rio Branco] is white to a remarkable degree, its waters being actually milky in appearance\". Alexander von Humboldt attributed the color to the presence of silicates in the water, principally mica and talc. There is a visible contrast with the waters of the Rio Negro at the confluence of the two rivers. The Rio Negro is a blackwater river with dark tea-colored acidic water (pH 3.5\u20134.5) that contains high levels of dissolved organic carbon.\nRiver capture.\nUntil approximately 20,000 years ago the headwaters of the Branco River flowed not into the Amazon, but via the Takutu Graben in the Rupununi area of Guyana towards the Caribbean. Currently in the rainy season much of the Rupununi area floods, with water draining both to the Amazon (via the Branco River) and the Essequibo River."}
