{"id": "6804", "revid": "1267307608", "url": "https://en.wikipedia.org/wiki?curid=6804", "title": "Charge-coupled device", "text": "A charge-coupled device (CCD) is an integrated circuit containing an array of linked, or coupled, capacitors. Under the control of an external circuit, each capacitor can transfer its electric charge to a neighboring capacitor. CCD sensors are a major technology used in digital imaging.\nOverview.\nIn a CCD image sensor, pixels are represented by p-doped metal\u2013oxide\u2013semiconductor (MOS) capacitors. These MOS capacitors, the basic building blocks of a CCD, are biased above the threshold for inversion when image acquisition begins, allowing the conversion of incoming photons into electron charges at the semiconductor-oxide interface; the CCD is then used to read out these charges.\nAlthough CCDs are not the only technology to allow for light detection, CCD image sensors are widely used in professional, medical, and scientific applications where high-quality image data are required.\nIn applications with less exacting quality demands, such as consumer and professional digital cameras, active pixel sensors, also known as CMOS sensors (complementary MOS sensors), are generally used.\nHowever, the large quality advantage CCDs enjoyed early on has narrowed over time and since the late 2010s CMOS sensors are the dominant technology, having largely if not completely replaced CCD image sensors.\nHistory.\nThe basis for the CCD is the metal\u2013oxide\u2013semiconductor (MOS) structure, with MOS capacitors being the basic building blocks of a CCD, and a depleted MOS structure used as the photodetector in early CCD devices.\nIn the late 1960s, Willard Boyle and George E. Smith at Bell Labs were researching MOS technology while working on semiconductor bubble memory. They realized that an electric charge was the analogy of the magnetic bubble and that it could be stored on a tiny MOS capacitor. As it was fairly straightforward to fabricate a series of MOS capacitors in a row, they connected a suitable voltage to them so that the charge could be stepped along from one to the next. This led to the invention of the charge-coupled device by Boyle and Smith in 1969. They conceived of the design of what they termed, in their notebook, \"Charge 'Bubble' Devices\".\nThe initial paper describing the concept in April 1970 listed possible uses as memory, a delay line, and an imaging device. The device could also be used as a shift register. The essence of the design was the ability to transfer charge along the surface of a semiconductor from one storage capacitor to the next. The concept was similar in principle to the bucket-brigade device (BBD), which was developed at Philips Research Labs during the late 1960s.\nThe first experimental device demonstrating the principle was a row of closely spaced metal squares on an oxidized silicon surface electrically accessed by wire bonds. It was demonstrated by Gil Amelio, Michael Francis Tompsett and George Smith in April 1970. This was the first experimental application of the CCD in image sensor technology, and used a depleted MOS structure as the photodetector. The first patent () on the application of CCDs to imaging was assigned to Tompsett, who filed the application in 1971.\nThe first working CCD made with integrated circuit technology was a simple 8-bit shift register, reported by Tompsett, Amelio and Smith in August 1970. This device had input and output circuits and was used to demonstrate its use as a shift register and as a crude eight pixel linear imaging device. Development of the device progressed at a rapid rate. By 1971, Bell researchers led by Michael Tompsett were able to capture images with simple linear devices.\nSeveral companies, including Fairchild Semiconductor, RCA and Texas Instruments, picked up on the invention and began development programs. Fairchild's effort, led by ex-Bell researcher Gil Amelio, was the first with commercial devices, and by 1974 had a linear 500-element device and a 2D 100 \u00d7 100 pixel device. Peter Dillon, a scientist at Kodak Research Labs, invented the first color CCD image sensor by overlaying a color filter array on this Fairchild 100 x 100 pixel Interline CCD starting in 1974. Steven Sasson, an electrical engineer working for the Kodak Apparatus Division, invented a digital still camera using this same Fairchild CCD in 1975.\nThe interline transfer (ILT) CCD device was proposed by L. Walsh and R. Dyck at Fairchild in 1973 to reduce smear and eliminate a mechanical shutter. To further reduce smear from bright light sources, the frame-interline-transfer (FIT) CCD architecture was developed by K. Horii, T. Kuroda and T. Kunii at Matsushita (now Panasonic) in 1981.\nThe first KH-11 KENNEN reconnaissance satellite equipped with charge-coupled device array ( pixels) technology for imaging was launched in December 1976. Under the leadership of Kazuo Iwama, Sony started a large development effort on CCDs involving a significant investment. Eventually, Sony managed to mass-produce CCDs for their camcorders. Before this happened, Iwama died in August 1982. Subsequently, a CCD chip was placed on his tombstone to acknowledge his contribution. The first mass-produced consumer CCD video camera, the CCD-G5, was released by Sony in 1983, based on a prototype developed by Yoshiaki Hagiwara in 1981.\nEarly CCD sensors suffered from shutter lag. This was largely resolved with the invention of the pinned photodiode (PPD). It was invented by Nobukazu Teranishi, Hiromitsu Shiraki and Yasuo Ishihara at NEC in 1980. They recognized that lag can be eliminated if the signal carriers could be transferred from the photodiode to the CCD. This led to their invention of the pinned photodiode, a photodetector structure with low lag, low noise, high quantum efficiency and low dark current. It was first publicly reported by Teranishi and Ishihara with A. Kohono, E. Oda and K. Arai in 1982, with the addition of an anti-blooming structure. The new photodetector structure invented at NEC was given the name \"pinned photodiode\" (PPD) by B.C. Burkey at Kodak in 1984. In 1987, the PPD began to be incorporated into most CCD devices, becoming a fixture in consumer electronic video cameras and then digital still cameras. Since then, the PPD has been used in nearly all CCD sensors and then CMOS sensors.\nIn January 2006, Boyle and Smith were awarded the National Academy of Engineering Charles Stark Draper Prize, and in 2009 they were awarded the Nobel Prize for Physics for their invention of the CCD concept. Michael Tompsett was awarded the 2010 National Medal of Technology and Innovation, for pioneering work and electronic technologies including the design and development of the first CCD imagers. He was also awarded the 2012 IEEE Edison Medal for \"pioneering contributions to imaging devices including CCD Imagers, cameras and thermal imagers\".\nBasics of operation.\nIn a CCD for capturing images, there is a photoactive region (an epitaxial layer of silicon), and a transmission region made out of a shift register (the CCD, properly speaking).\nAn image is projected through a lens onto the capacitor array (the photoactive region), causing each capacitor to accumulate an electric charge proportional to the light intensity at that location. A one-dimensional array, used in line-scan cameras, captures a single slice of the image, whereas a two-dimensional array, used in video and still cameras, captures a two-dimensional picture corresponding to the scene projected onto the focal plane of the sensor. Once the array has been exposed to the image, a control circuit causes each capacitor to transfer its contents to its neighbor (operating as a shift register). The last capacitor in the array dumps its charge into a charge amplifier, which converts the charge into a voltage. By repeating this process, the controlling circuit converts the entire contents of the array in the semiconductor to a sequence of voltages. In a digital device, these voltages are then sampled, digitized, and usually stored in memory; in an analog device (such as an analog video camera), they are processed into a continuous analog signal (e.g. by feeding the output of the charge amplifier into a low-pass filter), which is then processed and fed out to other circuits for transmission, recording, or other processing.\nDetailed physics of operation.\nCharge generation.\nBefore the MOS capacitors are exposed to light, they are biased into the depletion region; in n-channel CCDs, the silicon under the bias gate is slightly \"p\"-doped or intrinsic. The gate is then biased at a positive potential, above the threshold for strong inversion, which will eventually result in the creation of an \"n\" channel below the gate as in a MOSFET. However, it takes time to reach this thermal equilibrium: up to hours in high-end scientific cameras cooled at low temperature. Initially after biasing, the holes are pushed far into the substrate, and no mobile electrons are at or near the surface; the CCD thus operates in a non-equilibrium state called deep depletion.\nThen, when electron\u2013hole pairs are generated in the depletion region, they are separated by the electric field, the electrons move toward the surface, and the holes move toward the substrate. Four pair-generation processes can be identified:\nThe last three processes are known as dark-current generation, and add noise to the image; they can limit the total usable integration time. The accumulation of electrons at or near the surface can proceed either until image integration is over and charge begins to be transferred, or thermal equilibrium is reached. In this case, the well is said to be full. The maximum capacity of each well is known as the well depth, typically about 105 electrons per pixel. CCDs are normally susceptible to ionizing radiation and energetic particles which causes noise in the output of the CCD, and this must be taken into consideration in satellites using CCDs.\nDesign and manufacturing.\nThe photoactive region of a CCD is, generally, an epitaxial layer of silicon. It is lightly \"p\" doped (usually with boron) and is grown upon a substrate material, often p++. In buried-channel devices, the type of design utilized in most modern CCDs, certain areas of the surface of the silicon are ion implanted with phosphorus, giving them an n-doped designation. This region defines the channel in which the photogenerated charge packets will travel. Simon Sze details the advantages of a buried-channel device:\nThis thin layer (= 0.2\u20130.3 micron) is fully depleted and the accumulated photogenerated charge is kept away from the surface. This structure has the advantages of higher transfer efficiency and lower dark current, from reduced surface recombination. The penalty is smaller charge capacity, by a factor of 2\u20133 compared to the surface-channel CCD. The gate oxide, i.e. the capacitor dielectric, is grown on top of the epitaxial layer and substrate.\nLater in the process, polysilicon gates are deposited by chemical vapor deposition, patterned with photolithography, and etched in such a way that the separately phased gates lie perpendicular to the channels. The channels are further defined by utilization of the LOCOS process to produce the channel stop region.\nChannel stops are thermally grown oxides that serve to isolate the charge packets in one column from those in another. These channel stops are produced before the polysilicon gates are, as the LOCOS process utilizes a high-temperature step that would destroy the gate material. The channel stops are parallel to, and exclusive of, the channel, or \"charge carrying\", regions.\nChannel stops often have a p+ doped region underlying them, providing a further barrier to the electrons in the charge packets (this discussion of the physics of CCD devices assumes an electron transfer device, though hole transfer is possible).\nThe clocking of the gates, alternately high and low, will forward and reverse bias the diode that is provided by the buried channel (n-doped) and the epitaxial layer (p-doped). This will cause the CCD to deplete, near the p\u2013n junction and will collect and move the charge packets beneath the gates\u2014and within the channels\u2014of the device.\nCCD manufacturing and operation can be optimized for different uses. The above process describes a frame transfer CCD. While CCDs may be manufactured on a heavily doped p++ wafer it is also possible to manufacture a device inside p-wells that have been placed on an n-wafer. This second method, reportedly, reduces smear, dark current, and infrared and red response. This method of manufacture is used in the construction of interline-transfer devices.\nAnother version of CCD is called a peristaltic CCD. In a peristaltic charge-coupled device, the charge-packet transfer operation is analogous to the peristaltic contraction and dilation of the digestive system. The peristaltic CCD has an additional implant that keeps the charge away from the silicon/silicon dioxide interface and generates a large lateral electric field from one gate to the next. This provides an additional driving force to aid in transfer of the charge packets.\nArchitecture.\nThe CCD image sensors can be implemented in several different architectures. The most common are full-frame, frame-transfer, and interline. The distinguishing characteristic of each of these architectures is their approach to the problem of shuttering.\nIn a full-frame device, all of the image area is active, and there is no electronic shutter. A mechanical shutter must be added to this type of sensor or the image smears as the device is clocked or read out.\nWith a frame-transfer CCD, half of the silicon area is covered by an opaque mask (typically aluminum). The image can be quickly transferred from the image area to the opaque area or storage region with acceptable smear of a few percent. That image can then be read out slowly from the storage region while a new image is integrating or exposing in the active area. Frame-transfer devices typically do not require a mechanical shutter and were a common architecture for early solid-state broadcast cameras. The downside to the frame-transfer architecture is that it requires twice the silicon real estate of an equivalent full-frame device; hence, it costs roughly twice as much.\nThe interline architecture extends this concept one step further and masks every other column of the image sensor for storage. In this device, only one pixel shift has to occur to transfer from image area to storage area; thus, shutter times can be less than a microsecond and smear is essentially eliminated. The advantage is not free, however, as the imaging area is now covered by opaque strips dropping the fill factor to approximately 50 percent and the effective quantum efficiency by an equivalent amount. Modern designs have addressed this deleterious characteristic by adding microlenses on the surface of the device to direct light away from the opaque regions and on the active area. Microlenses can bring the fill factor back up to 90 percent or more depending on pixel size and the overall system's optical design.\nThe choice of architecture comes down to one of utility. If the application cannot tolerate an expensive, failure-prone, power-intensive mechanical shutter, an interline device is the right choice. Consumer snap-shot cameras have used interline devices. On the other hand, for those applications that require the best possible light collection and issues of money, power and time are less important, the full-frame device is the right choice. Astronomers tend to prefer full-frame devices. The frame-transfer falls in between and was a common choice before the fill-factor issue of interline devices was addressed. Today, frame-transfer is usually chosen when an interline architecture is not available, such as in a back-illuminated device.\nCCDs containing grids of pixels are used in digital cameras, optical scanners, and video cameras as light-sensing devices. They commonly respond to 70 percent of the incident light (meaning a quantum efficiency of about 70 percent) making them far more efficient than photographic film, which captures only about 2 percent of the incident light.\nMost common types of CCDs are sensitive to near-infrared light, which allows infrared photography, night-vision devices, and zero lux (or near zero lux) video-recording/photography. For normal silicon-based detectors, the sensitivity is limited to 1.1\u00a0\u03bcm. One other consequence of their sensitivity to infrared is that infrared from remote controls often appears on CCD-based digital cameras or camcorders if they do not have infrared blockers.\nCooling reduces the array's dark current, improving the sensitivity of the CCD to low light intensities, even for ultraviolet and visible wavelengths. Professional observatories often cool their detectors with liquid nitrogen to reduce the dark current, and therefore the thermal noise, to negligible levels.\nFrame transfer CCD.\nThe frame transfer CCD imager was the first imaging structure proposed for CCD Imaging by Michael Tompsett at Bell Laboratories. A frame transfer CCD is a specialized CCD, often used in astronomy and some professional video cameras, designed for high exposure efficiency and correctness.\nThe normal functioning of a CCD, astronomical or otherwise, can be divided into two phases: exposure and readout. During the first phase, the CCD passively collects incoming photons, storing electrons in its cells. After the exposure time is passed, the cells are read out one line at a time. During the readout phase, cells are shifted down the entire area of the CCD. While they are shifted, they continue to collect light. Thus, if the shifting is not fast enough, errors can result from light that falls on a cell holding charge during the transfer. These errors are referred to as rolling shutter effect, making fast moving objects appear distorted. In addition, the CCD cannot be used to collect light while it is being read out. A faster shifting requires a faster readout, and a faster readout can introduce errors in the cell charge measurement, leading to a higher noise level.\nA frame transfer CCD solves both problems: it has a shielded, not light sensitive, area containing as many cells as the area exposed to light. Typically, this area is covered by a reflective material such as aluminium. When the exposure time is up, the cells are transferred very rapidly to the hidden area. Here, safe from any incoming light, cells can be read out at any speed one deems necessary to correctly measure the cells' charge. At the same time, the exposed part of the CCD is collecting light again, so no delay occurs between successive exposures.\nThe disadvantage of such a CCD is the higher cost: the cell area is basically doubled, and more complex control electronics are needed.\nIntensified charge-coupled device.\nAn intensified charge-coupled device (ICCD) is a CCD that is optically connected to an image intensifier that is mounted in front of the CCD.\nAn image intensifier includes three functional elements: a photocathode, a micro-channel plate (MCP) and a phosphor screen. These three elements are mounted one close behind the other in the mentioned sequence. The photons which are coming from the light source fall onto the photocathode, thereby generating photoelectrons. The photoelectrons are accelerated towards the MCP by an electrical control voltage, applied between photocathode and MCP. The electrons are multiplied inside of the MCP and thereafter accelerated towards the phosphor screen. The phosphor screen finally converts the multiplied electrons back to photons which are guided to the CCD by a fiber optic or a lens.\nAn image intensifier inherently includes a shutter functionality: If the control voltage between the photocathode and the MCP is reversed, the emitted photoelectrons are not accelerated towards the MCP but return to the photocathode. Thus, no electrons are multiplied and emitted by the MCP, no electrons are going to the phosphor screen and no light is emitted from the image intensifier. In this case no light falls onto the CCD, which means that the shutter is closed. The process of reversing the control voltage at the photocathode is called \"gating\" and therefore ICCDs are also called gateable CCD cameras.\nBesides the extremely high sensitivity of ICCD cameras, which enable single photon detection, the gateability is one of the major advantages of the ICCD over the EMCCD cameras. The highest performing ICCD cameras enable shutter times as short as 200 picoseconds.\nICCD cameras are in general somewhat higher in price than EMCCD cameras because they need the expensive image intensifier. On the other hand, EMCCD cameras need a cooling system to cool the EMCCD chip down to temperatures around . This cooling system adds additional costs to the EMCCD camera and often yields heavy condensation problems in the application.\nICCDs are used in night vision devices and in various scientific applications.\nElectron-multiplying CCD.\nAn electron-multiplying CCD (EMCCD, also known as an L3Vision CCD, a product commercialized by e2v Ltd., GB, L3CCD or Impactron CCD, a now-discontinued product offered in the past by Texas Instruments) is a charge-coupled device in which a gain register is placed between the shift register and the output amplifier. The gain register is split up into a large number of stages. In each stage, the electrons are multiplied by impact ionization in a similar way to an avalanche diode. The gain probability at every stage of the register is small (\"P\" &lt; 2%), but as the number of elements is large (N &gt; 500), the overall gain can be very high (formula_1), with single input electrons giving many thousands of output electrons. Reading a signal from a CCD gives a noise background, typically a few electrons. In an EMCCD, this noise is superimposed on many thousands of electrons rather than a single electron; the devices' primary advantage is thus their negligible readout noise. The use of avalanche breakdown for amplification of photo charges had already been described in the in 1973 by George E. Smith/Bell Telephone Laboratories.\nEMCCDs show a similar sensitivity to intensified CCDs (ICCDs). However, as with ICCDs, the gain that is applied in the gain register is stochastic and the \"exact\" gain that has been applied to a pixel's charge is impossible to know. At high gains (&gt; 30), this uncertainty has the same effect on the signal-to-noise ratio (SNR) as halving the quantum efficiency (QE) with respect to operation with a gain of unity. This effect is referred to as the Excess Noise Factor (ENF). However, at very low light levels (where the quantum efficiency is most important), it can be assumed that a pixel either contains an electron\u2014or not. This removes the noise associated with the stochastic multiplication at the risk of counting multiple electrons in the same pixel as a single electron. To avoid multiple counts in one pixel due to coincident photons in this mode of operation, high frame rates are essential. The dispersion in the gain is shown in the graph on the right. For multiplication registers with many elements and large gains it is well modelled by the equation:\nformula_2\nwhere \"P\" is the probability of getting \"n\" output electrons given \"m\" input electrons and a total mean multiplication register gain of \"g\". For very large numbers of input electrons, this complex distribution function converges towards a Gaussian.\nBecause of the lower costs and better resolution, EMCCDs are capable of replacing ICCDs in many applications. ICCDs still have the advantage that they can be gated very fast and thus are useful in applications like range-gated imaging. EMCCD cameras indispensably need a cooling system\u2014using either thermoelectric cooling or liquid nitrogen\u2014to cool the chip down to temperatures in the range of . This cooling system adds additional costs to the EMCCD imaging system and may yield condensation problems in the application. However, high-end EMCCD cameras are equipped with a permanent hermetic vacuum system confining the chip to avoid condensation issues.\nThe low-light capabilities of EMCCDs find use in astronomy and biomedical research, among other fields. In particular, their low noise at high readout speeds makes them very useful for a variety of astronomical applications involving low light sources and transient events such as lucky imaging of faint stars, high speed photon counting photometry, Fabry-P\u00e9rot spectroscopy and high-resolution spectroscopy. More recently, these types of CCDs have broken into the field of biomedical research in low-light applications including small animal imaging, single-molecule imaging, Raman spectroscopy, super resolution microscopy as well as a wide variety of modern fluorescence microscopy techniques thanks to greater SNR in low-light conditions in comparison with traditional CCDs and ICCDs.\nIn terms of noise, commercial EMCCD cameras typically have clock-induced charge (CIC) and dark current (dependent on the extent of cooling) that together lead to an effective readout noise ranging from 0.01 to 1 electrons per pixel read. However, recent improvements in EMCCD technology have led to a new generation of cameras capable of producing significantly less CIC, higher charge transfer efficiency and an EM gain 5 times higher than what was previously available. These advances in low-light detection lead to an effective total background noise of 0.001 electrons per pixel read, a noise floor unmatched by any other low-light imaging device.\nUse in astronomy.\nDue to the high quantum efficiencies of charge-coupled device (CCD) (the ideal quantum efficiency is 100%, one generated electron per incident photon), linearity of their outputs, ease of use compared to photographic plates, and a variety of other reasons, CCDs were very rapidly adopted by astronomers for nearly all UV-to-infrared applications.\nThermal noise and cosmic rays may alter the pixels in the CCD array. To counter such effects, astronomers take several exposures with the CCD shutter closed and opened. The average of images taken with the shutter closed is necessary to lower the random noise. Once developed, the dark frame average image is then subtracted from the open-shutter image to remove the dark current and other systematic defects (dead pixels, hot pixels, etc.) in the CCD. Newer Skipper CCDs counter noise by collecting data with the same collected charge multiple times and has applications in precision light Dark Matter searches and neutrino measurements.\nThe Hubble Space Telescope, in particular, has a highly developed series of steps (\"data reduction pipeline\") to convert the raw CCD data to useful images.\nCCD cameras used in astrophotography often require sturdy mounts to cope with vibrations from wind and other sources, along with the tremendous weight of most imaging platforms. To take long exposures of galaxies and nebulae, many astronomers use a technique known as auto-guiding. Most autoguiders use a second CCD chip to monitor deviations during imaging. This chip can rapidly detect errors in tracking and command the mount motors to correct for them.\nAn unusual astronomical application of CCDs, called drift-scanning, uses a CCD to make a fixed telescope behave like a tracking telescope and follow the motion of the sky. The charges in the CCD are transferred and read in a direction parallel to the motion of the sky, and at the same speed. In this way, the telescope can image a larger region of the sky than its normal field of view. The Sloan Digital Sky Survey is the most famous example of this, using the technique to produce a survey of over a quarter of the sky. The Gaia space telescope is another instrument operating in this mode, rotating about its axis at a constant rate of 1 revolution in 6 hours and scanning a 360\u00b0 by 0.5\u00b0 strip on the sky during this time; a star traverses the entire focal plane in about 40 seconds (effective exposure time).\nIn addition to imagers, CCDs are also used in an array of analytical instrumentation including spectrometers and interferometers.\nColor cameras.\nDigital color cameras, including the digital color cameras in smartphones, generally use an integral color image sensor, which has a color filter array fabricated on top of the monochrome pixels of the CCD. The most popular CFA pattern is known as the Bayer filter, which is named for its inventor, Kodak scientist Bryce Bayer. In the Bayer pattern, each square of four pixels has one filtered red, one blue, and two green pixels (the human eye has greater acuity for luminance, which is more heavily weighted in green than in either red or blue). As a result, the luminance information is collected in each row and column using a checkerboard pattern, and the color resolution is lower than the luminance resolution.\nBetter color separation can be reached by three-CCD devices (3CCD) and a dichroic beam splitter prism, that splits the image into red, green and blue components. Each of the three CCDs is arranged to respond to a particular color. Many professional video camcorders, and some semi-professional camcorders, use this technique, although developments in competing CMOS technology have made CMOS sensors, both with beam-splitters and Bayer filters, increasingly popular in high-end video and digital cinema cameras. Another advantage of 3CCD over a Bayer mask device is higher quantum efficiency (higher light sensitivity), because most of the light from the lens enters one of the silicon sensors, while a Bayer mask absorbs a high proportion (more than 2/3) of the light falling on each pixel location.\nFor still scenes, for instance in microscopy, the resolution of a Bayer mask device can be enhanced by microscanning technology. During the process of color co-site sampling, several frames of the scene are produced. Between acquisitions, the sensor is moved in pixel dimensions, so that each point in the visual field is acquired consecutively by elements of the mask that are sensitive to the red, green, and blue components of its color. Eventually every pixel in the image has been scanned at least once in each color and the resolution of the three channels become equivalent (the resolutions of red and blue channels are quadrupled while the green channel is doubled).\nSensor sizes.\nSensors (CCD / CMOS) come in various sizes, or image sensor formats. These sizes are often referred to with an inch fraction designation such as 1/1.8\u2033 or 2/3\u2033 called the optical format. This measurement originates back in the 1950s and the time of Vidicon tubes.\nBlooming.\nWhen a CCD exposure is long enough, eventually the electrons that collect in the \"bins\" in the brightest part of the image will overflow the bin, resulting in blooming. The structure of the CCD allows the electrons to flow more easily in one direction than another, resulting in vertical streaking.\nSome anti-blooming features that can be built into a CCD reduce its sensitivity to light by using some of the pixel area for a drain structure.\nJames M. Early developed a vertical anti-blooming drain that would not detract from the light collection area, and so did not reduce light sensitivity."}
{"id": "6805", "revid": "30292728", "url": "https://en.wikipedia.org/wiki?curid=6805", "title": "Communist", "text": ""}
{"id": "6806", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=6806", "title": "Computer memory", "text": "Computer memory stores information, such as data and programs, for immediate use in the computer. The term \"memory\" is often synonymous with the terms \"RAM,\" \"main memory,\" or \"primary storage.\" Archaic synonyms for main memory include \"core\" (for magnetic core memory) and \"store\".\nMain memory operates at a high speed compared to mass storage which is slower but less expensive per bit and higher in capacity. Besides storing opened programs and data being actively processed, computer memory serves as a mass storage cache and write buffer to improve both reading and writing performance. Operating systems borrow RAM capacity for caching so long as it is not needed by running software. If needed, contents of the computer memory can be transferred to storage; a common way of doing this is through a memory management technique called \"virtual memory\".\nModern computer memory is implemented as semiconductor memory, where data is stored within memory cells built from MOS transistors and other components on an integrated circuit. There are two main kinds of semiconductor memory: volatile and non-volatile. Examples of non-volatile memory are flash memory and ROM, PROM, EPROM, and EEPROM memory. Examples of volatile memory are dynamic random-access memory (DRAM) used for primary storage and static random-access memory (SRAM) used mainly for CPU cache.\nMost semiconductor memory is organized into memory cells each storing one bit (0 or 1). Flash memory organization includes both one bit per memory cell and a multi-level cell capable of storing multiple bits per cell. The memory cells are grouped into words of fixed word length, for example, 1, 2, 4, 8, 16, 32, 64 or 128 bits. Each word can be accessed by a binary address of \"N\" bits, making it possible to store 2\"N\" words in the memory.\nHistory.\nIn the early 1940s, memory technology often permitted a capacity of a few bytes. The first electronic programmable digital computer, the ENIAC, using thousands of vacuum tubes, could perform simple calculations involving 20 numbers of ten decimal digits stored in the vacuum tubes.\nThe next significant advance in computer memory came with acoustic delay-line memory, developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through the mercury, with the quartz crystals acting as transducers to read and write bits. Delay-line memory was limited to a capacity of up to a few thousand bits.\nTwo alternatives to the delay line, the Williams tube and Selectron tube, originated in 1946, both using electron beams in glass tubes as means of storage. Using cathode-ray tubes, Fred Williams invented the Williams tube, which was the first random-access computer memory. The Williams tube was able to store more information than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and was less expensive. The Williams tube was nevertheless frustratingly sensitive to environmental disturbances.\nEfforts began in the late 1940s to find non-volatile memory. Magnetic-core memory allowed for memory recall after power loss. It was developed by Frederick W. Viehe and An Wang in the late 1940s, and improved by Jay Forrester and Jan A. Rajchman in the early 1950s, before being commercialized with the Whirlwind I computer in 1953. Magnetic-core memory was the dominant form of memory until the development of MOS semiconductor memory in the 1960s.\nThe first semiconductor memory was implemented as a flip-flop circuit in the early 1960s using bipolar transistors. Semiconductor memory made from discrete devices was first shipped by Texas Instruments to the United States Air Force in 1961. In the same year, the concept of solid-state memory on an integrated circuit (IC) chip was proposed by applications engineer Bob Norman at Fairchild Semiconductor. The first bipolar semiconductor memory IC chip was the SP95 introduced by IBM in 1965. While semiconductor memory offered improved performance over magnetic-core memory, it remained larger and more expensive and did not displace magnetic-core memory until the late 1960s.\nMOS memory.\nThe invention of the metal\u2013oxide\u2013semiconductor field-effect transistor (MOSFET) enabled the practical use of metal\u2013oxide\u2013semiconductor (MOS) transistors as memory cell storage elements. MOS memory was developed by John Schmidt at Fairchild Semiconductor in 1964. In addition to higher performance, MOS semiconductor memory was cheaper and consumed less power than magnetic core memory. In 1965, J. Wood and R. Ball of the Royal Radar Establishment proposed digital storage systems that use CMOS (complementary MOS) memory cells, in addition to MOSFET power devices for the power supply, switched cross-coupling, switches and delay-line storage. The development of silicon-gate MOS integrated circuit (MOS IC) technology by Federico Faggin at Fairchild in 1968 enabled the production of MOS memory chips. NMOS memory was commercialized by IBM in the early 1970s. MOS memory overtook magnetic core memory as the dominant memory technology in the early 1970s.\nThe two main types of volatile random-access memory (RAM) are static random-access memory (SRAM) and dynamic random-access memory (DRAM). Bipolar SRAM was invented by Robert Norman at Fairchild Semiconductor in 1963, followed by the development of MOS SRAM by John Schmidt at Fairchild in 1964. SRAM became an alternative to magnetic-core memory, but requires six transistors for each bit of data. Commercial use of SRAM began in 1965, when IBM introduced their SP95 SRAM chip for the System/360 Model 95.\nToshiba introduced bipolar DRAM memory cells for its Toscal BC-1411 electronic calculator in 1965. While it offered improved performance, bipolar DRAM could not compete with the lower price of the then dominant magnetic-core memory. MOS technology is the basis for modern DRAM. In 1966, Robert H. Dennard at the IBM Thomas J. Watson Research Center was working on MOS memory. While examining the characteristics of MOS technology, he found it was possible to build capacitors, and that storing a charge or no charge on the MOS capacitor could represent the 1 and 0 of a bit, while the MOS transistor could control writing the charge to the capacitor. This led to his development of a single-transistor DRAM memory cell. In 1967, Dennard filed a patent for a single-transistor DRAM memory cell based on MOS technology. This led to the first commercial DRAM IC chip, the Intel 1103 in October 1970. Synchronous dynamic random-access memory (SDRAM) later debuted with the Samsung KM48SL2000 chip in 1992.\nThe term \"memory\" is also often used to refer to non-volatile memory including read-only memory (ROM) through modern flash memory. Programmable read-only memory (PROM) was invented by Wen Tsing Chow in 1956, while working for the Arma Division of the American Bosch Arma Corporation. In 1967, Dawon Kahng and Simon Sze of Bell Labs proposed that the floating gate of a MOS semiconductor device could be used for the cell of a reprogrammable ROM, which led to Dov Frohman of Intel inventing EPROM (erasable PROM) in 1971. EEPROM (electrically erasable PROM) was developed by Yasuo Tarui, Yutaka Hayashi and Kiyoko Naga at the Electrotechnical Laboratory in 1972. Flash memory was invented by Fujio Masuoka at Toshiba in the early 1980s. Masuoka and colleagues presented the invention of NOR flash in 1984, and then NAND flash in 1987. Toshiba commercialized NAND flash memory in 1987.\nDevelopments in technology and economies of scale have made possible so-called (VLM) computers.\nVolatility categories.\nVolatile memory.\nVolatile memory is computer memory that requires power to maintain the stored information. Most modern semiconductor volatile memory is either static RAM (SRAM) or dynamic RAM (DRAM). DRAM dominates for desktop system memory. SRAM is used for CPU cache. SRAM is also found in small embedded systems requiring little memory.\nSRAM retains its contents as long as the power is connected and may use a simpler interface, but commonly uses six transistors per bit. Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs.\nNon-volatile memory.\nNon-volatile memory can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory, flash memory, most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks and magnetic tape), optical discs, and early computer storage methods such as magnetic drum, paper tape and punched cards.\nNon-volatile memory technologies under development include ferroelectric RAM, programmable metallization cell, Spin-transfer torque magnetic RAM, SONOS, resistive random-access memory, racetrack memory, Nano-RAM, 3D XPoint, and millipede memory.\nSemi-volatile memory.\nA third category of memory is \"semi-volatile\". The term is used to describe a memory that has some limited non-volatile duration after power is removed, but then data is ultimately lost. A typical goal when using a semi-volatile memory is to provide the high performance and durability associated with volatile memories while providing some benefits of non-volatile memory.\nFor example, some non-volatile memory types experience wear when written. A \"worn\" cell has increased volatility but otherwise continues to work. Data locations which are written frequently can thus be directed to use worn circuits. As long as the location is updated within some known retention time, the data stays valid. After a period of time without update, the value is copied to a less-worn circuit with longer retention. Writing first to the worn area allows a high write rate while avoiding wear on the not-worn circuits.\nAs a second example, an STT-RAM can be made non-volatile by building large cells, but doing so raises the cost per bit and power requirements and reduces the write speed. Using small cells improves cost, power, and speed, but leads to semi-volatile behavior. In some applications, the increased volatility can be managed to provide many benefits of a non-volatile memory, for example by removing power but forcing a wake-up before data is lost; or by caching read-only data and discarding the cached data if the power-off time exceeds the non-volatile threshold.\nThe term semi-volatile is also used to describe semi-volatile behavior constructed from other memory types, such as nvSRAM, which combines SRAM and a non-volatile memory on the same chip, where an external signal copies data from the volatile memory to the non-volatile memory, but if power is removed before the copy occurs, the data is lost. Another example is battery-backed RAM, which uses an external battery to power the memory device in case of external power loss. If power is off for an extended period of time, the battery may run out, resulting in data loss.\nManagement.\nProper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs or slow performance.\nBugs.\nImproper management of memory is a common cause of bugs and security vulnerabilities, including the following types:\nVirtual memory.\nVirtual memory is a system where physical memory is managed by the operating system typically with assistance from a memory management unit, which is part of many modern CPUs. It allows multiple types of memory to be used. For example, some data can be stored in RAM while other data is stored on a hard drive (e.g. in a swapfile), functioning as an extension of the cache hierarchy. This offers several advantages. Computer programmers no longer need to worry about where their data is physically stored or whether the user's computer will have enough memory. The operating system will place actively used data in RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving data from RAM to disk and back than it does accomplishing tasks; this is known as thrashing.\nProtected memory.\nProtected memory is a system where each program is given an area of memory to use and is prevented from going outside that range. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated (or otherwise restricted or redirected). This way, only the offending program crashes, and other programs are not affected by the misbehavior (whether accidental or intentional). Use of protected memory greatly enhances both the reliability and security of a computer system.\nWithout protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be rebooted. At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers. It may also be used benignly by desirable programs which are intended to modify other programs, debuggers, for example, to insert breakpoints or hooks."}
{"id": "6808", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6808", "title": "Citrate cycle", "text": ""}
{"id": "6809", "revid": "64853", "url": "https://en.wikipedia.org/wiki?curid=6809", "title": "CDC (disambiguation)", "text": "The Centers for Disease Control and Prevention is the national public health agency of the United States.\nCDC may also refer to:"}
{"id": "6810", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6810", "title": "Cipro", "text": ""}
{"id": "6811", "revid": "45589087", "url": "https://en.wikipedia.org/wiki?curid=6811", "title": "Centers for Disease Control and Prevention", "text": "The Centers for Disease Control and Prevention (CDC) is the national public health agency of the United States. It is a United States federal agency under the Department of Health and Human Services, and is headquartered in Atlanta, Georgia.\nThe agency's main goal is the protection of public health and safety through the control and prevention of disease, injury, and disability in the US and worldwide. The CDC focuses national attention on developing and applying disease control and prevention. It especially focuses its attention on infectious disease, food borne pathogens, environmental health, occupational safety and health, health promotion, injury prevention, and educational activities designed to improve the health of United States citizens. The CDC also conducts research and provides information on non-infectious diseases, such as obesity and diabetes, and is a founding member of the International Association of National Public Health Institutes.\nThe CDC's current acting director is Susan Monarez who assumed the role on January 23, 2025.\nHistory.\nEstablishment.\nThe Communicable Disease Center was founded July 1, 1946, as the successor to the World War II Malaria Control in War Areas program of the Office of National Defense Malaria Control Activities.\nPreceding its founding, organizations with global influence in malaria control were the Malaria Commission of the League of Nations and the Rockefeller Foundation. The Rockefeller Foundation greatly supported malaria control, sought to have the governments take over some of its efforts, and collaborated with the agency.\nThe new agency was a branch of the U.S. Public Health Service and Atlanta was chosen as the location because malaria was endemic in the Southern United States. The agency changed names (see infobox on top) before adopting the name \"Communicable Disease Center\" in 1946. Offices were located on the sixth floor of the Volunteer Building on Peachtree Street.\nWith a budget at the time of about $1million, 59 percent of its personnel were engaged in mosquito abatement and habitat control with the objective of control and eradication of malaria in the United States (see National Malaria Eradication Program).\nAmong its 369 employees, the main jobs at CDC were originally entomology and engineering. In CDC's initial years, more than six and a half million homes were sprayed, mostly with DDT. In 1946, there were only seven medical officers on duty and an early organization chart was drawn. Under Joseph Walter Mountin, the CDC continued to be an advocate for public health issues and pushed to extend its responsibilities to many other communicable diseases.\nIn 1947, the CDC made a token payment of $10 to Emory University for of land on Clifton Road in DeKalb County, still the home of CDC headquarters as of 2019. CDC employees collected the money to make the purchase. The benefactor behind the \"gift\" was Robert W. Woodruff, chairman of the board of the Coca-Cola Company. Woodruff had a long-time interest in malaria control, which had been a problem in areas where he went hunting. The same year, the PHS transferred its San Francisco based plague laboratory into the CDC as the Epidemiology Division, and a new Veterinary Diseases Division was established.\nGrowth.\nIn 1951, Chief Epidemiologist Alexander Langmuir's warnings of potential biological warfare during the Korean War spurred the creation of the Epidemic Intelligence Service (EIS) as a two-year postgraduate training program in epidemiology. The success of the EIS program led to the launch of Field Epidemiology Training Programs (FETP) in 1980, training more than 18,000 disease detectives in over 80 countries. In 2020, FETP celebrated the 40th anniversary of the CDC's support for Thailand's Field Epidemiology Training Program. Thailand was the first FETP site created outside of North America and is found in numerous countries, reflecting CDC's influence in promoting this model internationally. The Training Programs in Epidemiology and Public Health Interventions Network (TEPHINET) has graduated 950 students.\nThe mission of the CDC expanded beyond its original focus on malaria to include sexually transmitted diseases when the Venereal Disease Division of the U.S. Public Health Service (PHS) was transferred to the CDC in 1957. Shortly thereafter, Tuberculosis Control was transferred (in 1960) to the CDC from PHS, and then in 1963 the Immunization program was established.\nIt became the National Communicable Disease Center effective July 1, 1967, and the Center for Disease Control on June 24, 1970. At the end of the Public Health Service reorganizations of 1966\u20131973, it was promoted to being a principal operating agency of PHS.\nRecent history.\nIt was renamed to the plural Centers for Disease Control effective October 14, 1980, as the modern organization of having multiple constituent centers was established. By 1990, it had four centers formed in the 1980s: the Center for Infectious Diseases, Center for Chronic Disease Prevention and Health Promotion, the Center for Environmental Health and Injury Control, and the Center for Prevention Services; as well as two centers that had been absorbed by CDC from outside: the National Institute for Occupational Safety and Health in 1973, and the National Center for Health Statistics in 1987.\nAn act of the United States Congress appended the words \"and Prevention\" to the name effective October 27, 1992. However, Congress directed that the initialism CDC be retained because of its name recognition. Since the 1990s, the CDC focus has broadened to include chronic diseases, disabilities, injury control, workplace hazards, environmental health threats, and terrorism preparedness. CDC combats emerging diseases and other health risks, including birth defects, West Nile virus, obesity, avian, swine, and pandemic flu, E. coli, and bioterrorism, to name a few. The organization would also prove to be an important factor in preventing the abuse of penicillin. In May 1994 the CDC admitted having sent samples of communicable diseases to the Iraqi government from 1984 through 1989 which were subsequently repurposed for biological warfare, including Botulinum toxin, West Nile virus, \"Yersinia pestis\" and Dengue fever virus.\nOn April 21, 2005, then\u2013CDC director Julie Gerberding formally announced the reorganization of CDC to \"confront the challenges of 21st-century health threats\". She established four coordinating centers. In 2009 the Obama administration re-evaluated this change and ordered them cut as an unnecessary management layer.\nAs of 2013, the CDC's Biosafety Level 4 laboratories were among the few that exist in the world. They included one of only two official repositories of smallpox in the world, with the other one located at the State Research Center of Virology and Biotechnology VECTOR in the Russian Federation. In 2014, the CDC revealed they had discovered several misplaced smallpox samples while their lab workers were \"potentially infected\" with anthrax.\nThe city of Atlanta annexed the property of the CDC headquarters effective January 1, 2018, as a part of the city's largest annexation within a period of 65 years; the Atlanta City Council had voted to do so the prior December. The CDC and Emory University had requested that the Atlanta city government annex the area, paving the way for a MARTA expansion through the Emory campus, funded by city tax dollars. The headquarters were located in an unincorporated area, statistically in the Druid Hills census-designated place.\nOn August 17, 2022, Dr. Walensky said the CDC would make drastic changes in the wake of mistakes during the COVID-19 pandemic. She outlined an overhaul of how the CDC would analyze and share data and how they would communicate information to the general public. In her statement to all CDC employees, she said: \"For 75 years, CDC and public health have been preparing for COVID-19, and in our big moment, our performance did not reliably meet expectations.\" Based on the findings of an internal report, Walensky concluded that \"The CDC must refocus itself on public health needs, respond much faster to emergencies and outbreaks of disease, and provide information in a way that ordinary people and state and local health authorities can understand and put to use\" (as summarized by the New York Times).\nIn January 2025 it was reported that a CDC official had ordered all CDC staff to stop working with the World Health Organization. Around January 31, 2025, several CDC websites, pages, and data bases became unavailable for viewing as a response to Donald Trump's executive order to remove all material of \"diversity, equity, and inclusion\" and \"gender identity\".\nOrganization.\nThe CDC is organized into centers, institutes, and offices (CIOs), with each organizational unit implementing the agency's activities in a particular area of expertise while also providing intra-agency support and resource-sharing for cross-cutting issues and specific health threats.\nAs of the most recent reorganization in February 2023, the CIOs are:\nThe Office of Public Health Preparedness was created during the 2001 anthrax attacks shortly after the terrorist attacks of September 11, 2001. Its purpose was to coordinate among the government the response to a range of biological terrorism threats.\nLocations.\nMost CDC centers are located in Atlanta. Building 18, which opened in 2005 at the CDC's main Roybal campus (named in honor of the late Representative Edward R. Roybal), contains the premier BSL4 laboratory in the United States.\nA few of the centers are based in or operate other domestic locations:\nIn addition, CDC operates quarantine facilities in 20 cities in the U.S.\nBudget.\nThe CDC budget for fiscal year 2024 is $11.581billion.\nWorkforce.\n CDC staff numbered approximately 15,000 personnel (including 6,000 contractors and 840 United States Public Health Service Commissioned Corps officers) in 170 occupations. Eighty percent held bachelor's degrees or higher; almost half had advanced degrees (a master's degree or a doctorate such as a PhD, D.O., or M.D.).\nCommon CDC job titles include engineer, entomologist, epidemiologist, biologist, physician, veterinarian, behavioral scientist, nurse, medical technologist, economist, public health advisor, health communicator, toxicologist, chemist, computer scientist, and statistician. The CDC also operates a number of notable training and fellowship programs, including those indicated below.\nEpidemic Intelligence Service (EIS).\nThe Epidemic Intelligence Service (EIS) is composed of \"boots-on-the-ground disease detectives\" who investigate public health problems domestically and globally. When called upon by a governmental body, EIS officers may embark on short-term epidemiological assistance assignments, or \"Epi-Aids\", to provide technical expertise in containing and investigating disease outbreaks. The EIS program is a model for the international Field Epidemiology Training Program.\nPublic Health Associates Program.\nThe CDC also operates the Public Health Associate Program (PHAP), a two-year paid fellowship for recent college graduates to work in public health agencies all over the United States. PHAP was founded in 2007 and currently has 159 associates in 34 states.\nLeadership.\nThe Director of CDC is a position that currently requires Senate confirmation. The director serves at the pleasure of the President and may be fired at any time. The CDC Director concurrently serves as the Administrator of the Agency for Toxic Substances and Disease Registry.\nPrior to January 20, 2025, it was a Senior Executive Service position that could be filled either by a career employee, or as a political appointment that does not require Senate confirmation, with the latter method typically being used. The change to requiring Senate confirmation was due to a provision in the Consolidated Appropriations Act, 2023. \nTwenty directors have served the CDC or its predecessor agencies, including three who have served during the Trump administration (including Anne Schuchat who twice served as acting director) and three who have served during the Carter administration (including one acting director not shown here). Two served under Bill Clinton, but only one under the Nixon to Ford terms.\nPast Directors\nAreas of focus.\nCommunicable diseases.\nThe CDC's programs address more than 400 diseases, health threats, and conditions that are major causes of death, disease, and disability. The CDC's website has information on various infectious (and noninfectious) diseases, including smallpox, measles, and others.\nInfluenza.\nThe CDC targets the transmission of influenza, including the H1N1 swine flu, and launched websites to educate people about hygiene.\nDivision of Select Agents and Toxins.\nWithin the division are two programs: the Federal Select Agent Program (FSAP) and the Import Permit Program. The FSAP is run jointly with an office within the U.S. Department of Agriculture, regulating agents that can cause disease in humans, animals, and plants. The Import Permit Program regulates the importation of \"infectious biological materials.\"\nThe CDC runs a program that protects the public from rare and dangerous substances such as anthrax and the Ebola virus. The program, called the Federal Select Agent Program, calls for inspections of labs in the U.S. that work with dangerous pathogens.\nDuring the 2014 Ebola outbreak in West Africa, the CDC helped coordinate the return of two infected American aid workers for treatment at Emory University Hospital, the home of a special unit to handle highly infectious diseases.\nAs a response to the 2014 Ebola outbreak, Congress passed a Continuing Appropriations Resolution allocating $30,000,000 towards CDC's efforts to fight the virus.\nNon-communicable diseases.\nThe CDC also works on non-communicable diseases, including chronic diseases caused by obesity, physical inactivity and tobacco-use. The work of the Division for Cancer Prevention and Control, led from 2010 by Lisa C. Richardson, is also within this remit.\nAntibiotic resistance.\nThe CDC implemented their \"National Action Plan for Combating Antibiotic Resistant Bacteria\" as a measure against the spread of antibiotic resistance in the United States. This initiative has a budget of $161million and includes the development of the Antibiotic Resistance Lab Network.\nGlobal health.\nGlobally, the CDC works with other organizations to address global health challenges and contain disease threats at their source. They work with many international organizations such as the World Health Organization (WHO) as well as ministries of health and other groups on the front lines of outbreaks. The agency maintains staff in more than 60 countries, including some from the U.S. but more from the countries in which they operate. The agency's global divisions include the Division of Global HIV and TB (DGHT), the Division of Parasitic Diseases and Malaria (DPDM), the Division of Global Health Protection (DGHP), and the Global Immunization Division (GID).\nThe CDC has been working with the WHO to implement the \"International Health Regulations\" (IHR), an agreement between 196 countries to prevent, control, and report on the international spread of disease, through initiatives including the Global Disease Detection Program (GDD).\nThe CDC has also been involved in implementing the U.S. global health initiatives President's Emergency Plan for AIDS Relief (PEPFAR) and President's Malaria Initiative.\nTravelers' health.\nThe CDC collects and publishes health information for travelers in a comprehensive book, \"CDC Health Information for International Travel\", which is commonly known as the \"yellow book.\" The book is available online and in print as a new edition every other year and includes current travel health guidelines, vaccine recommendations, and information on specific travel destinations. The CDC also issues travel health notices on its website, consisting of three levels:\nVaccine safety.\nThe CDC uses a number of tools to monitor the safety of vaccines. The Vaccine Adverse Event Reporting System (VAERS), a national vaccine safety surveillance program run by CDC and the FDA. \"VAERS detects possible safety issues with U.S. vaccines by collecting information about adverse events (possible side effects or health problems) after vaccination.\" The CDC's Safety Information by Vaccine page provides a list of the latest safety information, side effects, and answers to common questions about CDC recommended vaccines.\nThe Vaccine Safety Datalink (VSD) works with a network of healthcare organizations to share data on vaccine safety and adverse events. The Clinical Immunization Safety Assessment (CISA) project is a network of vaccine experts and health centers that research and assist the CDC in the area of vaccine safety.\nCDC also runs a program called V-safe, a smartphone web application that allows COVID-19 vaccine recipients to be surveyed in detail about their health in response to getting the shot.\nCDC Foundation.\nThe CDC Foundation operates independently from CDC as a private, nonprofit 501(c)(3) organization incorporated in the State of Georgia. The creation of the Foundation was authorized by section 399F of the Public Health Service Act to support the mission of CDC in partnership with the private sector, including organizations, foundations, businesses, educational groups, and individuals. From 1995 to 2022, the foundation raised over $1.6 billion and launched more than 1,200 health programs. Bill Cosby formerly served as a member of the foundation's Board of Directors, continuing as an honorary member after completing his term.\nActivities.\nThe foundation engages in research projects and health programs in more than 160 countries every year, including in focus areas such as cardiovascular disease, cancer, emergency response, and infectious diseases, particularly HIV/AIDS, Ebola, rotavirus, and COVID-19.\nCriticism.\nIn 2015, \"BMJ\" associate editor Jeanne Lenzer raised concerns that the CDC's recommendations and publications may be influenced by donations received through the Foundation, which includes pharmaceutical companies.\nControversies.\nTuskegee study of untreated syphilis in Black men.\nFor 15 years, the CDC had direct oversight over the Tuskegee syphilis experiment. In the study, which lasted from 1932 to 1972, a group of Black men (nearly 400 of whom had syphilis) were studied to learn more about the disease. The disease was left untreated in the men, who had not given their informed consent to serve as research subjects. The Tuskegee Study was initiated in 1932 by the Public Health Service, with the CDC taking over the Tuskegee Health Benefit Program in 1995.\nGun control.\nAn area of partisan dispute related to CDC funding is studying firearms effectiveness. Although the CDC was one of the first government agencies to study gun related data, in 1996 the Dickey Amendment, passed with the support of the National Rifle Association of America, states \"none of the funds available for injury prevention and control at the Centers for Disease Control and Prevention may be used to advocate or promote gun control\". Advocates for gun control oppose the amendment and have tried to overturn it.\nLooking at the history of the passage of the Dickey Amendment, in 1992, Mark L. Rosenberg and five CDC colleagues founded the CDC's National Center for Injury Prevention and Control, with an annual budget of approximately $260,000. They focused on \"identifying causes of firearm deaths, and methods to prevent them\". Their first report, published in the \"New England Journal of Medicine\" in 1993 entitled \"Guns are a Risk Factor for Homicide in the Home\", reported \"mere presence of a gun in a home increased the risk of a firearm-related death by 2.7 percent, and suicide fivefolda \"huge\" increase.\" In response, the NRA launched a \"campaign to shut down the Injury Center.\" Two conservative pro-gun groups, Doctors for Responsible Gun Ownership and Doctors for Integrity and Policy Research joined the pro-gun effort, and, by 1995, politicians also supported the pro-gun initiative. In 1996, Jay Dickey (R) Arkansas introduced the Dickey Amendment statement stating \"none of the funds available for injury prevention and control at the Centers for Disease Control and Prevention may be used to advocate or promote gun control\" as a rider. in the 1996 appropriations bill.\" In 1997, \"Congress re-directed all of the money for gun research to the study of traumatic brain injury.\" David Satcher, CDC head 1993\u201398 advocated for firearms research. In 2016 over a dozen \"public health insiders, including current and former CDC senior leaders\" told \"The Trace\" interviewers that CDC senior leaders took a cautious stance in their interpretation of the Dickey Amendment and that they could do more but were afraid of political and personal retribution.\nIn 2013, the American Medical Association, the American Psychological Association, and the American Academy of Pediatrics sent a letter to the leaders of the Senate Appropriations Committee asking them \"to support at least $10million within the Centers for Disease Control and Prevention (CDC) in FY 2014 along with sufficient new taxes at the National Institutes of Health to support research into the causes and prevention of violence. Furthermore, we urge Members to oppose any efforts to reduce, eliminate, or condition CDC funding related to violence prevention research.\" Congress maintained the ban in subsequent budgets.\nEbola.\nIn October 2014, the CDC gave a nurse with a fever who was later diagnosed with Ebola permission to board a commercial flight to Cleveland.\nCOVID-19.\nThe CDC has been widely criticized for its handling of the COVID-19 pandemic. In 2022, CDC director Rochelle Walensky acknowledged \"some pretty dramatic, pretty public mistakes, from testing to data to communications\", based on the findings of an internal examination.\nThe first confirmed case of COVID-19 was discovered in the U.S. on January 20, 2020. However, widespread COVID-19 testing in the United States was effectively stalled until February 28, when federal officials revised a faulty CDC test, and days afterward, when the Food and Drug Administration began loosening rules that had restricted other labs from developing tests. In February 2020, as the CDC's early coronavirus test malfunctioned nationwide, CDC Director Robert R. Redfield reassured fellow officials on the White House Coronavirus Task Force that the problem would be quickly solved, according to White House officials. It took about three weeks to sort out the failed test kits, which may have been contaminated during their processing in a CDC lab. Later investigations by the FDA and the Department of Health and Human Services found that the CDC had violated its own protocols in developing its tests. In November 2020, \"NPR\" reported that an internal review document they obtained revealed that the CDC was aware that the first batch of tests which were issued in early January had a chance of being wrong 33 percent of the time, but they released them anyway.\nIn May 2020, \"The Atlantic\" reported that the CDC was conflating the results of two different types of coronavirus tests \u2013 tests that diagnose current coronavirus infections, and tests that measure whether someone has ever had the virus. The magazine said this distorted several important metrics, provided the country with an inaccurate picture of the state of the pandemic, and overstated the country's testing ability.\nIn July 2020, the Trump administration ordered hospitals to bypass the CDC and instead send all COVID-19 patient information to a database at the Department of Health and Human Services. Some health experts opposed the order and warned that the data might become politicized or withheld from the public. On July 15, the CDC alarmed health care groups by temporarily removing COVID-19 dashboards from its website. It restored the data a day later.\nIn August 2020, the CDC recommended that people showing no COVID-19 symptoms do not need testing. The new guidelines alarmed many public health experts. The guidelines were crafted by the White House Coronavirus Task Force without the sign-off of Anthony Fauci of the NIH. Objections by other experts at the CDC went unheard. Officials said that a CDC document in July arguing for \"the importance of reopening schools\" was also crafted outside the CDC. On August 16, the chief of staff, Kyle McGowan, and his deputy, Amanda Campbell, resigned from the agency. The testing guidelines were reversed on September 18, 2020, after public controversy.\nIn September 2020, the CDC drafted an order requiring masks on all public transportation in the United States, but the White House Coronavirus Task Force blocked the order, refusing to discuss it, according to two federal health officials.\nIn October 2020, it was disclosed that White House advisers had repeatedly altered the writings of CDC scientists about COVID-19, including recommendations on church choirs, social distancing in bars and restaurants, and summaries of public-health reports.\nIn the lead up to 2020 Thanksgiving, the CDC advised Americans not to travel for the holiday saying, \"It's not a requirement. It's a recommendation for the American public to consider.\" The White House coronavirus task force had its first public briefing in months on that date but travel was not mentioned.\nThe New York Times later concluded that the CDC's decisions to \"ben[d] to political pressure from the Trump White House to alter key public health guidance or withhold it from the public [...] cost it a measure of public trust that experts say it still has not recaptured\" as of 2022.\nIn May 2021, following criticism by scientists, the CDC updated its COVID-19 guidance to acknowledge airborne transmission of COVID-19, after having previously claimed that the majority of infections occurred via \"close contact, not airborne transmission\".\nIn December 2021, following a request from the CEO of Delta Air Lines, CDC shortened its recommended isolation period for asymptomatic individuals infected with COVID-19 from 10 days to five.\nUntil 2022, the CDC withheld critical data about COVID-19 vaccine boosters, hospitalizations and wastewater data.\nOn June 10, 2022, the Biden Administration ordered the CDC to remove the COVID-19 testing requirement for air travelers entering the United States.\nIn January 2022, it was revealed that the CDC had communicated with moderators at Facebook and Instagram over COVID-19 information and discussion on the platforms, including information that the CDC considered false or misleading and that might influence people not to get the COVID-19 vaccines.\nControversy over the Morbidity and Mortality Weekly Report.\nDuring the pandemic, the CDC Morbidity and Mortality Weekly Report (MMWR) came under pressure from political appointees at the Department of Health and Human Services (HHS) to modify its reporting so as not to conflict with what Trump was saying about the pandemic.\nStarting in June 2020, Michael Caputo, the HHS assistant secretary for public affairs, and his chief advisor Paul Alexander tried to delay, suppress, change, and retroactively edit MMR releases about the effectiveness of potential treatments for COVID-19, the transmissibility of the virus, and other issues where the president had taken a public stance. Alexander tried unsuccessfully to get personal approval of all issues of MMWR before they went out.\nCaputo claimed this oversight was necessary because MMWR reports were being tainted by \"political content\"; he demanded to know the political leanings of the scientists who reported that hydroxychloroquine had little benefit as a treatment while Trump was saying the opposite. In emails Alexander accused CDC scientists of attempting to \"hurt the president\" and writing \"hit pieces on the administration\".\nIn October 2020, emails obtained by \"Politico\" showed that Alexander requested multiple alterations in a report. The published alterations included a title being changed from \"Children, Adolescents, and Young Adults\" to \"Persons.\" One current and two former CDC officials who reviewed the email exchanges said they were troubled by the \"intervention to alter scientific reports viewed as untouchable prior to the Trump administration\" that \"appeared to minimize the risks of the coronavirus to children by making the report's focus on children less clear.\"\nEroding trust in the CDC as a result of COVID-19 controversies.\nA poll conducted in September 2020 found that nearly 8 in 10 Americans trusted the CDC, a decrease from 87 percent in April 2020. Another poll showed an even larger drop in trust with the results dropping 16 percentage points. By January 2022, according to an NBC News poll, only 44% of Americans trusted the CDC compared to 69% at the beginning of the pandemic. As the trustworthiness eroded, so too did the information it disseminates. The diminishing level of trust in the CDC and the information releases also incited \"vaccine hesitancy\" with the result that \"just 53 percent of Americans said they would be somewhat or extremely likely to get a vaccine.\"\nIn September 2020, amid the accusations and the faltering image of the CDC, the agency's leadership was called into question. Former acting director at the CDC, Richard Besser, said of Redfield that \"I find it concerning that the CDC director has not been outspoken when there have been instances of clear political interference in the interpretation of science.\" In addition, Mark Rosenberg, the first director of CDC's National Center for Injury Prevention and Control, also questioned Redfield's leadership and his lack of defense of the science.\nHistorically, the CDC has not been a political agency; however, the COVID-19 pandemic, and specifically the Trump administration's handling of the pandemic, resulted in a \"dangerous shift\" according to a previous CDC director and others. Four previous directors claim that the agency's voice was \"muted for political reasons.\" Politicization of the agency has continued into the Biden administration as COVID-19 guidance is contradicted by State guidance and the agency is criticized as \"CDC's credibility is eroding\".\nIn 2021, the CDC, then under the leadership of the Biden administration, received criticism for its mixed messaging surrounding COVID-19 vaccines, mask-wearing guidance, and the state of the pandemic.\nPopular culture.\nZombie Apocalypse campaign.\nOn May 16, 2011, the Centers for Disease Control and Prevention's blog what to do to prepare for a zombie invasion. While the article did not claim that such a scenario was possible, it did use the popular culture appeal as a means of urging citizens to prepare for all potential hazards, such as earthquakes, tornadoes, and floods.\nAccording to David Daigle, the associate director for communications, public health preparedness and response, the idea arose when his team was discussing their upcoming hurricane-information campaign and Daigle mused that \"we say pretty much the same things every year, in the same way, and I just wonder how many people are paying attention.\" A social-media employee mentioned that the subject of zombies had come up a lot on Twitter when she had been tweeting about the Fukushima Daiichi nuclear disaster and radiation. The team realized that a campaign like this would most likely reach a different audience from the one that normally pays attention to hurricane-preparedness warnings and went to work on the zombie campaign, launching it right before hurricane season began. \"The whole idea was, if you're prepared for a zombie apocalypse, you're prepared for pretty much anything,\" said Daigle.\nOnce the blog article was posted, the CDC announced an open contest for YouTube submissions of the most creative and effective videos covering preparedness for a zombie apocalypse (or apocalypse of any kind), to be judged by the \"CDC Zombie Task Force\". Submissions were open until October 11, 2011. They also released a zombie-themed graphic novella available on their website. Zombie-themed educational materials for teachers are available on the site."}
{"id": "6813", "revid": "12530063", "url": "https://en.wikipedia.org/wiki?curid=6813", "title": "Chandrasekhar limit", "text": "The Chandrasekhar limit () is the maximum mass of a stable white dwarf star. The currently accepted value of the Chandrasekhar limit is about (). The limit was named after Subrahmanyan Chandrasekhar.\nWhite dwarfs resist gravitational collapse primarily through electron degeneracy pressure, compared to main sequence stars, which resist collapse through thermal pressure. The Chandrasekhar limit is the mass above which electron degeneracy pressure in the star's core is insufficient to balance the star's own gravitational self-attraction.\nPhysics.\nNormal stars fuse gravitationally compressed hydrogen into helium, generating vast amounts of heat. As the hydrogen is consumed, the stars' core compresses further allowing the helium and heavier nuclei to fuse ultimately resulting in stable iron nuclei, a process called stellar evolution. The next step depends upon the mass of the star. Stars below the Chandrasekhar limit become stable white dwarf stars, remaining that way throughout the rest of the history of the universe (assuming the absence of external forces). Stars above the limit can become neutron stars or black holes.\nThe Chandrasekhar limit is a consequence of competition between gravity and electron degeneracy pressure.\nElectron degeneracy pressure is a quantum-mechanical effect arising from the Pauli exclusion principle. Since electrons are fermions, no two electrons can be in the same state, so not all electrons can be in the minimum-energy level. Rather, electrons must occupy a band of energy levels. Compression of the electron gas increases the number of electrons in a given volume and raises the maximum energy level in the occupied band. Therefore, the energy of the electrons increases on compression, so pressure must be exerted on the electron gas to compress it, producing electron degeneracy pressure. With sufficient compression, electrons are forced into nuclei in the process of electron capture, relieving the pressure.\nIn the nonrelativistic case, electron degeneracy pressure gives rise to an equation of state of the form , where is the pressure, is the mass density, and is a constant. Solving the hydrostatic equation leads to a model white dwarf that is a polytrope of index \u2013 and therefore has radius inversely proportional to the cube root of its mass, and volume inversely proportional to its mass.\nAs the mass of a model white dwarf increases, the typical energies to which degeneracy pressure forces the electrons are no longer negligible relative to their rest masses. The velocities of the electrons approach the speed of light, and special relativity must be taken into account. In the strongly relativistic limit, the equation of state takes the form . This yields a polytrope of index 3, which has a total mass, , depending only on .\nFor a fully relativistic treatment, the equation of state used interpolates between the equations for small and for large . When this is done, the model radius still decreases with mass, but becomes zero at . This is the Chandrasekhar limit. The curves of radius against mass for the non-relativistic and relativistic models are shown in the graph. They are colored blue and green, respectively. has been set equal to 2. Radius is measured in standard solar radii or kilometers, and mass in standard solar masses.\nCalculated values for the limit vary depending on the nuclear composition of the mass. Chandrasekhar gives the following expression, based on the equation of state for an ideal Fermi gas:\nformula_1\nwhere:\nAs is the Planck mass, the limit is of the order of\nformula_2\nThe limiting mass can be obtained formally from the Chandrasekhar's white dwarf equation by taking the limit of large central density.\nA more accurate value of the limit than that given by this simple model requires adjusting for various factors, including electrostatic interactions between the electrons and nuclei and effects caused by nonzero temperature. Lieb and Yau have given a rigorous derivation of the limit from a relativistic many-particle Schr\u00f6dinger equation.\nHistory.\nIn 1926, the British physicist Ralph H. Fowler observed that the relationship between the density, energy, and temperature of white dwarfs could be explained by viewing them as a gas of nonrelativistic, non-interacting electrons and nuclei that obey Fermi\u2013Dirac statistics. This Fermi gas model was then used by the British physicist Edmund Clifton Stoner in 1929 to calculate the relationship among the mass, radius, and density of white dwarfs, assuming they were homogeneous spheres. Wilhelm Anderson applied a relativistic correction to this model, giving rise to a maximum possible mass of approximately . In 1930, Stoner derived the internal energy\u2013density equation of state for a Fermi gas, and was then able to treat the mass\u2013radius relationship in a fully relativistic manner, giving a limiting mass of approximately (for ). Stoner went on to derive the pressure\u2013density equation of state, which he published in 1932. These equations of state were also previously published by the Soviet physicist Yakov Frenkel in 1928, together with some other remarks on the physics of degenerate matter. Frenkel's work, however, was ignored by the astronomical and astrophysical community.\nA series of papers published between 1931 and 1935 had its beginning on a trip from India to England in 1930, where the Indian physicist Subrahmanyan Chandrasekhar worked on the calculation of the statistics of a degenerate Fermi gas. In these papers, Chandrasekhar solved the hydrostatic equation together with the nonrelativistic Fermi gas equation of state, and also treated the case of a relativistic Fermi gas, giving rise to the value of the limit shown above. Chandrasekhar reviews this work in his Nobel Prize lecture.\nThe existence of a related limit, based on the conceptual breakthrough of combining relativity with Fermi degeneracy, was first established in separate papers published by Wilhelm Anderson and E. C. Stoner for a uniform density star in 1929. Eric G. Blackman wrote that the roles of Stoner and Anderson in the discovery of mass limits were overlooked when Freeman Dyson wrote a biography of Chandrasekhar. Michael Nauenberg claims that Stoner established the mass limit first.\nThe priority dispute has also been discussed at length by Virginia Trimble who writes that: \"Chandrasekhar famously, perhaps even notoriously did his critical calculation on board ship in 1930, and ... was not aware of either Stoner's or Anderson's work at the time. His work was therefore independent, but, more to the point, he adopted Eddington's polytropes for his models which could, therefore, be in hydrostatic equilibrium, which constant density stars cannot, and real ones must be.\" This value was also computed in 1932 by the Soviet physicist Lev Landau, who, however, did not apply it to white dwarfs and concluded that quantum laws might be invalid for stars heavier than 1.5 solar mass.\nChandrasekhar\u2013Eddington dispute.\nChandrasekhar's work on the limit aroused controversy, owing to the opposition of the British astrophysicist Arthur Eddington. Eddington was aware that the existence of black holes was theoretically possible, and also realized that the existence of the limit made their formation possible. However, he was unwilling to accept that this could happen. After a talk by Chandrasekhar on the limit in 1935, he replied:\nEddington's proposed solution to the perceived problem was to modify relativistic mechanics so as to make the law universally applicable, even for large . Although Niels Bohr, Fowler, Wolfgang Pauli, and other physicists agreed with Chandrasekhar's analysis, at the time, owing to Eddington's status, they were unwilling to publicly support Chandrasekhar. Through the rest of his life, Eddington held to his position in his writings, including his work on his fundamental theory. The drama associated with this disagreement is one of the main themes of \"Empire of the Stars\", Arthur I. Miller's biography of Chandrasekhar. In Miller's view:\nHowever, Chandrasekhar chose to move on, leaving the study of stellar structure to focus on stellar dynamics. In 1983 in recognition for his work, Chandrasekhar shared a Nobel prize \"for his theoretical studies of the physical processes of importance to the structure and evolution of the stars\" with William Alfred Fowler.\nApplications.\nThe core of a star is kept from collapsing by the heat generated by the fusion of nuclei of lighter elements into heavier ones. At various stages of stellar evolution, the nuclei required for this process are exhausted, and the core collapses, causing it to become denser and hotter. A critical situation arises when iron accumulates in the core, since iron nuclei are incapable of generating further energy through fusion. If the core becomes sufficiently dense, electron degeneracy pressure will play a significant part in stabilizing it against gravitational collapse.\nIf a main-sequence star is not too massive (less than approximately 8 solar masses), it eventually sheds enough mass to form a white dwarf having mass below the Chandrasekhar limit, which consists of the former core of the star. For more-massive stars, electron degeneracy pressure does not keep the iron core from collapsing to very great density, leading to formation of a neutron star, black hole, or, speculatively, a quark star. (For very massive, low-metallicity stars, it is also possible that instabilities destroy the star completely.) During the collapse, neutrons are formed by the capture of electrons by protons in the process of electron capture, leading to the emission of neutrinos. The decrease in gravitational potential energy of the collapsing core releases a large amount of energy on the order of (100\u00a0foes). Most of this energy is carried away by the emitted neutrinos and the kinetic energy of the expanding shell of gas; only about 1% is emitted as optical light. This process is believed responsible for supernovae of types Ib, Ic, and II.\nType Ia supernovae derive their energy from runaway fusion of the nuclei in the interior of a white dwarf. This fate may befall carbon\u2013oxygen white dwarfs that accrete matter from a companion giant star, leading to a steadily increasing mass. As the white dwarf's mass approaches the Chandrasekhar limit, its central density increases, and, as a result of compressional heating, its temperature also increases. This eventually ignites nuclear fusion reactions, leading to an immediate carbon detonation, which disrupts the star and causes the supernova.\nA strong indication of the reliability of Chandrasekhar's formula is that the absolute magnitudes of supernovae of Type Ia are all approximately the same; at maximum luminosity, is approximately \u221219.3, with a standard deviation of no more than 0.3. A 1-sigma interval therefore represents a factor of less than 2 in luminosity. This seems to indicate that all type Ia supernovae convert approximately the same amount of mass to energy.\nSuper-Chandrasekhar mass supernovas.\nIn April\u00a02003, the Supernova Legacy Survey observed a type\u00a0Ia supernova, designated SNLS-03D3bb, in a galaxy approximately 4\u00a0billion light years away. According to a group of astronomers at the University of Toronto and elsewhere, the observations of this supernova are best explained by assuming that it arose from a white dwarf that had grown to twice the mass of the Sun before exploding. They believe that the star, dubbed the \"Champagne Supernova\" may have been spinning so fast that a centrifugal tendency allowed it to exceed the limit. Alternatively, the supernova may have resulted from the merger of two white dwarfs, so that the limit was only violated momentarily. Nevertheless, they point out that this observation poses a challenge to the use of type\u00a0Ia supernovae as standard candles.\nSince the observation of the Champagne Supernova in 2003, several more type\u00a0Ia supernovae have been observed that are very bright, and thought to have originated from white dwarfs whose masses exceeded the Chandrasekhar limit. These include SN 2006gz, SN 2007if, and SN 2009dc. The super-Chandrasekhar mass white dwarfs that gave rise to these supernovae are believed to have had masses up to 2.4\u20132.8\u00a0solar masses. One way to potentially explain the problem of the Champagne Supernova was considering it the result of an aspherical explosion of a white dwarf. However, spectropolarimetric observations of SN 2009dc showed it had a polarization smaller than 0.3, making the large asphericity theory unlikely.\nTolman\u2013Oppenheimer\u2013Volkoff limit.\nStars sufficiently massive to pass the Chandrasekhar limit provided by electron degeneracy pressure do not become white dwarf stars. Instead they explode as supernovae. If the final mass is below the Tolman\u2013Oppenheimer\u2013Volkoff limit, then neutron degeneracy pressure contributes to the balance against gravity and the result will be a neutron star; but if the total mass is above the Tolman-Oppenheimer-Volkhoff limit, the result will be a black hole."}
{"id": "6814", "revid": "1242992925", "url": "https://en.wikipedia.org/wiki?curid=6814", "title": "Congregational polity", "text": "Congregational polity, or congregationalist polity, often known as congregationalism, is a system of ecclesiastical polity in which every local church (congregation) is independent, ecclesiastically sovereign, or \"autonomous\". Its first articulation in writing is the Cambridge Platform of 1648 in New England.\nMajor Protestant Christian traditions that employ congregationalism include Baptist churches, the Congregational Methodist Church, and Congregational churches known by the \"Congregationalist\" name and having descended from the Independent Reformed wing of the Anglo-American Puritan movement of the 17th century. More recent generations have witnessed a growing number of nondenominational churches, which are often congregationalist in their governance. Although autonomous, like minded congregations may enter into voluntary associations with other congregations, sometimes called conventions, denominations, or associations.\nCongregationalism is distinguished from episcopal polity which is governance by a hierarchy of bishops, and is also distinct from presbyterian polity in which higher assemblies of congregational representatives can exercise considerable authority over individual congregations.\nCongregationalism is not limited only to organization of Christian church congregations. The principles of congregationalism have been inherited by the Unitarian Universalist Association and the Canadian Unitarian Council.\nBasic form.\nThe term \"congregational polity\" describes a form of church governance that is based on the local congregation. Each local congregation is independent and self-supporting, governed by its own members. Some band into loose voluntary associations with other congregations that share similar beliefs (e.g., the Willow Creek Association and the Unitarian Universalist Association). Others join \"conventions\", such as the Southern Baptist Convention, the National Baptist Convention or the American Baptist Churches USA (formerly the Northern Baptist Convention). These conventions generally provide stronger ties between congregations, including some doctrinal direction and pooling of financial resources. Congregations that belong to associations and conventions are still independently governed. Most non-denominational churches are organized along congregationalist lines. Many do not see these voluntary associations as \"denominations\", because they \"believe that there is no church other than the local church, and denominations are in variance to Scripture.\"\nDenominational families.\nThese Christian traditions use forms of congregational polity.\nCongregational churches.\nCongregationalism is a Protestant tradition with roots in the Puritan and Independent movements. In congregational government, the covenanted congregation exists prior to its officers, and as such the members are equipped to call and dismiss their ministers without oversight from any higher ecclesiastical body. Their churches ordinarily have at least one pastor, but may also install ruling elders.\nStatements of polity in the congregational tradition called \"platforms\". These include the Savoy Confession's platform, the Cambridge Platform, and the Saybrook Platform. Denominations in the congregational tradition include the UCC, NACCC, CCCC, and EFCC. Denominations in the tradition support but do not govern their constituent members.\nBaptist churches.\nMost Baptists hold that no denominational or ecclesiastical organization has inherent authority over an individual Baptist church. Churches can properly relate to each other under this polity only through voluntary cooperation, never by any sort of coercion. Furthermore, this Baptist polity calls for freedom from governmental control. Exceptions to this local form of local governance include the Episcopal Baptists that have an episcopal system.\nIndependent Baptist churches have no formal organizational structure above the level of the local congregation. More generally among Baptists, a variety of parachurch agencies and evangelical educational institutions may be supported generously or not at all, depending entirely upon the local congregation's customs and predilections. Usually doctrinal conformity is held as a first consideration when a church makes a decision to grant or decline financial contributions to such agencies, which are legally external and separate from the congregations they serve. These practices also find currency among non-denominational fundamentalist or charismatic fellowships, many of which derive from Baptist origins, culturally if not theologically.\nMost Southern Baptist and National Baptist congregations, by contrast, generally relate more closely to external groups such as mission agencies and educational institutions than do those of independent persuasion. However, they adhere to a very similar ecclesiology, refusing to permit outside control or oversight of the affairs of the local church.\nChurches of Christ.\nEcclesiastical government is congregational rather than denominational. Churches of Christ purposefully have no central headquarters, councils, or other organizational structure above the local church level. Rather, the independent congregations are a network with each congregation participating at its own discretion in various means of service and fellowship with other congregations. Churches of Christ are linked by their shared commitment to restoration principles.\nCongregations are generally overseen by a plurality of elders (also known in some congregations as shepherds, bishops, or pastors) who are sometimes assisted in the administration of various works by deacons. Elders are generally seen as responsible for the spiritual welfare of the congregation, while deacons are seen as responsible for the non-spiritual needs of the church. Deacons serve under the supervision of the elders, and are often assigned to direct specific ministries. Successful service as a deacon is often seen as preparation for the eldership. Elders and deacons are chosen by the congregation based on the qualifications found in Timothy 3 and Titus 1. Congregations look for elders who have a mature enough understanding of scripture to enable them to supervise the minister and to teach, as well as to perform governance functions. In lieu of willing men who meet these qualifications, congregations are sometimes overseen by an unelected committee of the congregation's men.\nWhile the early Restoration Movement had a tradition of itinerant preachers rather than \"located Preachers\", during the 20th century a long-term, formally trained congregational minister became the norm among Churches of Christ. Ministers are understood to serve under the oversight of the elders. While the presence of a long-term professional minister has sometimes created \"significant \"de facto\" ministerial authority\" and led to conflict between the minister and the elders, the eldership has remained the \"ultimate locus of authority in the congregation\". There is a small group within the Churches of Christ which oppose a single preacher and, instead, rotate preaching duties among qualified elders (this group tends to overlap with groups which oppose Sunday School and also have only one cup to serve the Lord's Supper).\nChurches of Christ hold to the priesthood of all believers. No special titles are used for preachers or ministers that would identify them as clergy. Churches of Christ emphasize that there is no distinction between \"clergy\" and \"laity\" and that every member has a gift and a role to play in accomplishing the work of the church.\nCongregational Methodist Church.\nMethodists who disagreed with the episcopal polity of the Methodist Episcopal Church, South left their mother church to form the Congregational Methodist Church, which retains Wesleyan-Arminian theology but adopts congregationalist polity as a distinctive."}
{"id": "6816", "revid": "1310918", "url": "https://en.wikipedia.org/wiki?curid=6816", "title": "Cavalry", "text": "Historically, cavalry (from the French word \"cavalerie\", itself derived from \"cheval\" meaning \"horse\") are groups of soldiers or warriors who fight mounted on horseback. Until the 20th century, cavalry were the most mobile of the combat arms, operating as light cavalry in the roles of reconnaissance, screening, and skirmishing, or as heavy cavalry for decisive economy of force and shock attacks. An individual soldier in the cavalry is known by a number of designations depending on era and tactics, such as a cavalryman, horseman, trooper, cataphract, knight, drabant, hussar, uhlan, mamluk, cuirassier, lancer, dragoon, samurai or horse archer. The designation of \"cavalry\" was not usually given to any military forces that used other animals or platforms for mounts, such as chariots, camels or elephants. Infantry who moved on horseback, but dismounted to fight on foot, were known in the early 17th to the early 18th century as \"dragoons\", a class of mounted infantry which in most armies later evolved into standard cavalry while retaining their historic designation.\nCavalry had the advantage of improved mobility, and a soldier fighting from horseback also had the advantages of greater height, speed, and inertial mass over an opponent on foot. Another element of horse mounted warfare is the psychological impact a mounted soldier can inflict on an opponent.\nThe speed, mobility, and shock value of cavalry was greatly valued and exploited in warfare during the Ancient and Medieval eras. Some hosts were mostly cavalry, particularly in nomadic societies of Asia, notably the Huns of Attila and the later Mongol armies. In Europe, cavalry became increasingly armoured (heavy), and eventually evolving into the mounted knights of the medieval period. During the 17th century, cavalry in Europe discarded most of its armor, which was ineffective against the muskets and cannons that were coming into common use, and by the mid-18th century armor had mainly fallen into obsolescence, although some regiments retained a small thickened cuirass that offered protection against lances, sabres, and bayonets; including some protection against a shot from distance.\nIn the interwar period many cavalry units were converted into motorized infantry and mechanized infantry units, or reformed as tank troops. The cavalry tank or cruiser tank was one designed with a speed and purpose beyond that of infantry tanks and would subsequently develop into the main battle tank. Nonetheless, some cavalry still served during World War II (notably in the Red Army, the Mongolian People's Army, the Royal Italian Army, the Royal Hungarian Army, the Romanian Army, the Polish Land Forces, and German light reconnaissance units within the Waffen SS).\nMost cavalry units that are horse-mounted in modern armies serve in purely ceremonial roles, or as mounted infantry in difficult terrain such as mountains or heavily forested areas. Modern usage of the term generally refers to units performing the role of reconnaissance, surveillance, and target acquisition (analogous to historical light cavalry) or main battle tank units (analogous to historical heavy cavalry).\nRole.\nHistorically, cavalry was divided into light cavalry and heavy cavalry. The differences were their roles in combat, the size of their mounts, and how much armor was worn by the mount and rider.\nHeavy cavalry, such as Byzantine cataphracts and knights of the Early Middle Ages in Europe, were used as shock troops, charging the main body of the enemy at the height of a battle; in many cases their actions decided the outcome of the battle, hence the later term \"battle cavalry\". Light cavalry, such as horse archers, hussars, and Cossack cavalry, were assigned all the numerous roles that were ill-suited to more narrowly-focused heavy forces. This includes scouting, deterring enemy scouts, foraging, raiding, skirmishing, pursuit of retreating enemy forces, screening of retreating friendly forces, linking separated friendly forces, and countering enemy light forces in all these same roles.\nLight and heavy cavalry roles continued through early modern warfare, but armor was reduced, with light cavalry mostly unarmored. Yet many cavalry units still retained cuirasses and helmets for their protective value against sword and bayonet strikes, and the morale boost these provide to the wearers, despite the actual armour giving little protection from firearms. By this time the main difference between light and heavy cavalry was in their training and weight; the former was regarded as best suited for harassment and reconnaissance, while the latter was considered best for close-order charges. By the start of the 20th century, as total battlefield firepower increased, cavalry increasingly tended to become dragoons in practice, riding mounted between battles, but dismounting to fight as infantry, even though retaining unit names that reflected their older cavalry roles. Military conservatism was however strong in most continental cavalry during peacetime and in these dismounted action continued to be regarded as a secondary function until the outbreak of World War I in 1914.\nWith the development of armored warfare, the heavy cavalry role of decisive shock troops had been taken over by armored units employing medium and heavy tanks, and later main battle tanks. Despite horse-borne cavalry becoming obsolete, the term \"cavalry\" is still used, referring in modern times to units continuing to fulfill the traditional light cavalry roles, employing fast armored cars, light tanks, and infantry fighting vehicles instead of horses, while air cavalry employs helicopters.\nEarly history.\nOrigins.\nBefore the Iron Age, the role of cavalry on the battlefield was largely performed by light chariots. The chariot originated with the Sintashta-Petrovka culture in Central Asia and spread by nomadic or semi-nomadic Indo-Iranians. The chariot was quickly adopted by settled peoples both as a military technology and an object of ceremonial status, especially by the pharaohs of the New Kingdom of Egypt from 1550 BC as well as the Assyrian army and Babylonian royalty.\nThe power of mobility given by mounted units was recognized early on, but was offset by the difficulty of raising large forces and by the inability of horses (then mostly small) to carry heavy armor. Nonetheless, there are indications that, from the 15th century BC onwards, horseback riding was practiced amongst the military elites of the great states of the ancient Near East, most notably those in Egypt, Assyria, the Hittite Empire, and Mycenaean Greece.\nCavalry techniques, and the rise of true cavalry, were an innovation of equestrian nomads of the Eurasian Steppe and pastoralist tribes such as the Iranic Parthians and Sarmatians. Together with a core of armoured lancers, these were predominantly horse archers using the Parthian shot tactic. \nThe photograph straight above shows Assyrian cavalry from reliefs of 865\u2013860 BC. At this time, the men had no spurs, saddles, saddle cloths, or stirrups. Fighting from the back of a horse was much more difficult than mere riding. The cavalry acted in pairs; the reins of the mounted archer were controlled by his neighbour's hand. Even at this early time, cavalry used swords, shields, spears, and bows. The sculpture implies two types of cavalry, but this might be a simplification by the artist. Later images of Assyrian cavalry show saddle cloths as primitive saddles, allowing each archer to control his own horse.\nAs early as 490 BC a breed of large horses was bred in the Nisaean plain in Media to carry men with increasing amounts of armour (Herodotus 7,40 &amp; 9,20), but large horses were still very exceptional at this time. By the fourth century BC the Chinese during the Warring States period (403\u2013221 BC) began to use cavalry against rival states, and by 331 BC when Alexander the Great defeated the Persians the use of chariots in battle was obsolete in most nations; despite a few ineffective attempts to revive scythed chariots. The last recorded use of chariots as a shock force in continental Europe was during the Battle of Telamon in 225 BC. However, chariots remained in use for ceremonial purposes such as carrying the victorious general in a Roman triumph, or for racing.\nOutside of mainland Europe, the southern Britons met Julius Caesar with chariots in 55 and 54 BC, but by the time of the Roman conquest of Britain a century later chariots were obsolete, even in Britannia. The last mention of chariot use in Britain was by the Caledonians at the Mons Graupius, in 84 AD.\nAncient Greece: city-states, Thebes, Thessaly and Macedonia.\nDuring the classical Greek period cavalry were usually limited to those citizens who could afford expensive war-horses. Three types of cavalry became common: light cavalry, whose riders, armed with javelins, could harass and skirmish; heavy cavalry, whose troopers, using lances, had the ability to close in on their opponents; and finally those whose equipment allowed them to fight either on horseback or foot. The role of horsemen did however remain secondary to that of the hoplites or heavy infantry who comprised the main strength of the citizen levies of the various city states.\nCavalry played a relatively minor role in ancient Greek city-states, with conflicts decided by massed armored infantry. However, Thebes produced Pelopidas, their first great cavalry commander, whose tactics and skills were absorbed by Philip II of Macedon when Philip was a guest-hostage in Thebes. Thessaly was widely known for producing competent cavalrymen, and later experiences in wars both with and against the Persians taught the Greeks the value of cavalry in skirmishing and pursuit. The Athenian author and soldier Xenophon in particular advocated the creation of a small but well-trained cavalry force; to that end, he wrote several manuals on horsemanship and cavalry operations.\nThe Macedonian kingdom in the north, on the other hand, developed a strong cavalry force that culminated in the \"hetairoi\" (Companion cavalry) of Philip II of Macedon and Alexander the Great. In addition to these heavy cavalry, the Macedonian army also employed lighter horsemen called prodromoi for scouting and screening, as well as the Macedonian pike phalanx and various kinds of light infantry. There were also the \"Ippiko\" (or \"Horserider\"), Greek \"heavy\" cavalry, armed with kontos (or cavalry lance), and sword. These wore leather armour or mail plus a helmet. They were medium rather than heavy cavalry, meaning that they were better suited to be scouts, skirmishers, and pursuers rather than front line fighters. The effectiveness of this combination of cavalry and infantry helped to break enemy lines and was most dramatically demonstrated in Alexander's conquests of Persia, Bactria, and northwestern India.\nRoman Republic and early Empire.\nThe cavalry in the early Roman Republic remained the preserve of the wealthy landed class known as the \"equites\"\u2014men who could afford the expense of maintaining a horse in addition to arms and armor heavier than those of the common legions. Horses were provided by the Republic and could be withdrawn if neglected or misused, together with the status of being a cavalryman.\nAs the class grew to be more of a social elite instead of a functional property-based military grouping, the Romans began to employ Italian socii for filling the ranks of their cavalry. The weakness of Roman cavalry was demonstrated by Hannibal Barca during the Second Punic War where he used his superior mounted forces to win several battles. The most notable of these was the Battle of Cannae, where he inflicted a catastrophic defeat on the Romans. At about the same time the Romans began to recruit foreign auxiliary cavalry from among Gauls, Iberians, and Numidians, the last being highly valued as mounted skirmishers and scouts (see Numidian cavalry). Julius Caesar had a high opinion of his escort of Germanic mixed cavalry, giving rise to the \"Cohortes Equitatae\". Early emperors maintained an ala of Batavian cavalry as their personal bodyguards until the unit was dismissed by Galba after the Batavian Rebellion.\nFor the most part, Roman cavalry during the early Republic functioned as an adjunct to the legionary infantry and formed only one-fifth of the standing force comprising a consular army. Except in times of major mobilisation about 1,800 horsemen were maintained, with three hundred attached to each legion. \nThe relatively low ratio of horsemen to infantry does not mean that the utility of cavalry should be underestimated, as its strategic role in scouting, skirmishing, and outpost duties was crucial to the Romans' capability to conduct operations over long distances in hostile or unfamiliar territory. On some occasions Roman cavalry also proved its ability to strike a decisive tactical blow against a weakened or unprepared enemy, such as the final charge at the Battle of Aquilonia.\nAfter defeats such as the Battle of Carrhae, the Romans learned the importance of large cavalry formations from the Parthians. \nAt the same time heavy spears and shields modelled on those favoured by the horsemen of the Greek city-states were adopted to replace the lighter weaponry of early Rome. These improvements in tactics and equipment reflected those of a thousand years earlier when the first Iranians to reach the Iranian Plateau forced the Assyrians to undertake similar reform. Nonetheless, the Romans would continue to rely mainly on their heavy infantry supported by auxiliary cavalry.\nLate Roman Empire and the Migration Period.\nIn the army of the late Roman Empire, cavalry played an increasingly important role. The Spatha, the classical sword throughout most of the 1st millennium was adopted as the standard model for the Empire's cavalry forces. By the 6th century these had evolved into lengthy straight weapons influenced by Persian and other eastern patterns. Other specialist weapons during this period included javelins, long reaching lancers, axes and maces.\nThe most widespread employment of heavy cavalry at this time was found in the forces of the Iranian empires, the Parthians and their Persian Sasanian successors. Both, but especially the former, were famed for the cataphract (fully armored cavalry armed with lances) even though the majority of their forces consisted of lighter horse archers. The West first encountered this eastern heavy cavalry during the Hellenistic period with further intensive contacts during the eight centuries of the Roman\u2013Persian Wars. At first the Parthians' mobility greatly confounded the Romans, whose armoured close-order infantry proved unable to match the speed of the Parthians. However, later the Romans would successfully adapt such heavy armor and cavalry tactics by creating their own units of cataphracts and \"clibanarii\".\nThe decline of the Roman infrastructure made it more difficult to field large infantry forces, and during the 4th and 5th centuries cavalry began to take a more dominant role on the European battlefield, also in part made possible by the appearance of new, larger breeds of horses. The replacement of the Roman saddle by variants on the Scythian model, with pommel and cantle, was also a significant factor as was the adoption of stirrups and the concomitant increase in stability of the rider's seat. Armored cataphracts began to be deployed in Eastern Europe and the Near East, following the precedents established by Persian forces, as the main striking force of the armies in contrast to the earlier roles of cavalry as scouts, raiders, and outflankers.\nThe late-Roman cavalry tradition of organized units in a standing army differed fundamentally from the nobility of the Germanic invaders\u2014individual warriors who could afford to provide their own horses and equipment. While there was no direct linkage with these predecessors the early medieval knight also developed as a member of a social and martial elite, able to meet the considerable expenses required by his role from grants of land and other incomes.\nAsia.\nCentral Asia.\nXiongnu, Tujue, Avars, Kipchaks, Khitans, Mongols, Don Cossacks and the various Turkic peoples are also examples of the horse-mounted groups that managed to gain substantial successes in military conflicts with settled agrarian and urban societies, due to their strategic and tactical mobility. As European states began to assume the character of bureaucratic nation-states supporting professional standing armies, recruitment of these mounted warriors was undertaken in order to fill the strategic roles of scouts and raiders.\nThe best known instance of the continued employment of mounted tribal auxiliaries were the Cossack cavalry regiments of the Russian Empire. In Eastern Europe, and out onto the steppes, cavalry remained important much longer and dominated the scene of warfare until the early 17th century and even beyond, as the strategic mobility of cavalry was crucial for the semi-nomadic pastoralist lives that many steppe cultures led. Tibetans also had a tradition of cavalry warfare, in several military engagements with the Chinese Tang dynasty (618\u2013907 AD).\nEast Asia.\nChina.\nFurther east, the military history of China, specifically northern China, held a long tradition of intense military exchange between Han Chinese infantry forces of the settled dynastic empires and the mounted nomads or \"barbarians\" of the north. The naval history of China was centered more to the south, where mountains, rivers, and large lakes necessitated the employment of a large and well-kept navy.\nIn 307 BC, King Wuling of Zhao, the ruler of the former state of Jin, ordered his commanders and troops to adopt the trousers of the nomads as well as practice the nomads' form of mounted archery to hone their new cavalry skills.\nThe adoption of massed cavalry in China also broke the tradition of the chariot-riding Chinese aristocracy in battle, which had been in use since the ancient Shang dynasty (\u20131050 BC). By this time large Chinese infantry-based armies of 100,000 to 200,000 troops were now buttressed with several hundred thousand mounted cavalry in support or as an effective striking force. The handheld pistol-and-trigger crossbow was invented in China in the fourth century BC; it was written by the Song dynasty scholars Zeng Gongliang, Ding Du, and Yang Weide in their book \"Wujing Zongyao\" (1044 AD) that massed missile fire by crossbowmen was the most effective defense against enemy cavalry charges.\nOn many occasions the Chinese studied nomadic cavalry tactics and applied the lessons in creating their own potent cavalry forces, while in others they simply recruited the tribal horsemen wholesale into their armies; and in yet other cases nomadic empires proved eager to enlist Chinese infantry and engineering, as in the case of the Mongol Empire and its sinicized part, the Yuan dynasty (1279\u20131368). The Chinese recognized early on during the Han dynasty (202 BC \u2013 220 AD) that they were at a disadvantage in lacking the number of horses the northern nomadic peoples mustered in their armies. Emperor Wu of Han (r 141\u201387 BC) went to war with the Dayuan for this reason, since the Dayuan were hoarding a massive amount of tall, strong, Central Asian bred horses in the Hellenized\u2013Greek region of Fergana (established slightly earlier by Alexander the Great). Although experiencing some defeats early on in the campaign, Emperor Wu's war from 104 BC to 102 BC succeeded in gathering the prized tribute of horses from Fergana.\nCavalry tactics in China were enhanced by the invention of the saddle-attached stirrup by at least the 4th century, as the oldest reliable depiction of a rider with paired stirrups was found in a Jin dynasty tomb of the year 322 AD. The Chinese invention of the horse collar by the 5th century was also a great improvement from the breast harness, allowing the horse to haul greater weight without heavy burden on its skeletal structure.\nKorea.\nThe horse warfare of Korea was first started during the ancient Korean kingdom Gojoseon. Since at least the 3rd century BC, there was influence of northern nomadic peoples and Yemaek peoples on Korean warfare. By roughly the first century BC, the ancient kingdom of Buyeo also had mounted warriors. The cavalry of Goguryeo, one of the Three Kingdoms of Korea, were called \"Gaemamusa\" (\uac1c\ub9c8\ubb34\uc0ac, \u93a7\u99ac\u6b66\u58eb), and were renowned as a fearsome heavy cavalry force. King Gwanggaeto the Great often led expeditions into the Baekje, Gaya confederacy, Buyeo, Later Yan and against Japanese invaders with his cavalry.\nIn the 12th century, Jurchen tribes began to violate the Goryeo\u2013Jurchen borders, and eventually invaded Goryeo Korea. After experiencing invasion by the Jurchen, Korean general Yun Kwan realized that Goryeo lacked efficient cavalry units. He reorganized the Goryeo military into a professional army that would contain decent and well-trained cavalry units. In 1107, the Jurchen were ultimately defeated, and surrendered to Yun Kwan. To mark the victory, General Yun built nine fortresses to the northeast of the Goryeo\u2013Jurchen borders (\ub3d9\ubd81 9\uc131, \u6771\u5317 \u4e5d\u57ce).\nJapan.\nThe ancient Japanese of the Kofun period also adopted cavalry and equine culture by the 5th century AD. The emergence of the samurai aristocracy led to the development of armoured horse archers, themselves to develop into charging lancer cavalry as gunpowder weapons rendered bows obsolete. Japanese cavalry was largely made up of landowners who would be upon a horse to better survey the troops they were called upon to bring to an engagement, rather than traditional mounted warfare seen in other cultures with massed cavalry units.\nAn example is Yabusame (\u6d41\u93d1\u99ac), a type of mounted archery in traditional Japanese archery. An archer on a running horse shoots three special \"turnip-headed\" arrows successively at three wooden targets.\nThis style of archery has its origins at the beginning of the Kamakura period. Minamoto no Yoritomo became alarmed at the lack of archery skills his samurai had. He organized yabusame as a form of practice.\nCurrently, the best places to see yabusame performed are at the Tsurugaoka Hachiman-g\u016b in Kamakura and Shimogamo Shrine in Kyoto (during Aoi Matsuri in early May). It is also performed in Samukawa and on the beach at Zushi, as well as other locations.\nKasagake or Kasakake (\u7b20\u61f8, \u304b\u3055\u304c\u3051 lit. \"hat shooting\") is a type of Japanese mounted archery. In contrast to yabusame, the types of targets are various and the archer shoots without stopping the horse. While yabusame has been played as a part of formal ceremonies, kasagake has developed as a game or practice of martial arts, focusing on technical elements of horse archery.\nSouth Asia.\nIndian subcontinent.\nIn the Indian subcontinent, cavalry played a major role from the Gupta dynasty (320\u2013600) period onwards. India has also the oldest evidence for the introduction of toe-stirrups.\nIndian literature contains numerous references to the mounted warriors of the Central Asian horse nomads, notably the Sakas, Kambojas, Yavanas, Pahlavas and Paradas. Numerous Puranic texts refer to a conflict in ancient India (16th century BC) in which the horsemen of five nations, called the \"Five Hordes\" (\"pa\u00f1ca.ganan\") or K\u1e63atriya hordes (\"K\u1e63atriya ganah\"), attacked and captured the state of Ayudhya by dethroning its Vedic King Bahu\nThe Mahabharata, Ramayana, numerous Puranas and some foreign sources attest that the Kamboja cavalry frequently played role in ancient wars. V. R. Ramachandra Dikshitar writes: \"Both the Puranas and the epics agree that the horses of the Sindhu and Kamboja regions were of the finest breed, and that the services of the Kambojas as cavalry troopers were utilised in ancient wars\". J.A.O.S. writes: \"Most famous horses are said to come either from Sindhu or Kamboja; of the latter (i.e. the Kamboja), the Indian epic Mahabharata speaks among the finest horsemen\".\nThe Mahabharata speaks of the esteemed cavalry of the Kambojas, Sakas, Yavanas and Tusharas, all of whom had participated in the Kurukshetra war under the supreme command of Kamboja ruler Sudakshin Kamboj.\nMahabharata and Vishnudharmottara Purana pay especial attention to the Kambojas, Yavansa, Gandharas etc. being \"ashva.yuddha.kushalah\" (expert cavalrymen). In the Mahabharata war, the Kamboja cavalry along with that of the Sakas, Yavanas is reported to have been enlisted by the Kuru king Duryodhana of Hastinapura.\nHerodotus ( \u2013 ) attests that the Gandarian mercenaries (i.e. \"Gandharans/Kambojans\" of Gandari Strapy of Achaemenids) from the 20th strapy of the Achaemenids were recruited in the army of emperor Xerxes I (486\u2013465 BC), which he led against the Hellas. Similarly, the \"men of the Mountain Land \" from north of Kabul-River equivalent to medieval Kohistan (Pakistan), figure in the army of Darius III against Alexander at Arbela, providing a cavalry force and 15 elephants. This obviously refers to Kamboja cavalry south of Hindukush.\nThe Kambojas were famous for their horses, as well as cavalrymen (\"asva-yuddha-Kushalah\"). On account of their supreme position in horse (Ashva) culture, they were also popularly known as Ashvakas, i.e. the \"horsemen\" and their land was known as \"Home of Horses\". They are the Assakenoi and Aspasioi of the Classical writings, and the Ashvakayanas and Ashvayanas in P\u0101\u1e47ini's Ashtadhyayi. The Assakenoi had faced Alexander with 30,000 infantry, 20,000 cavalry and 30 war elephants. Scholars have identified the Assakenoi and Aspasioi clans of Kunar and Swat valleys as a section of the Kambojas. These hardy tribes had offered stubborn resistance to Alexander () during latter's campaign of the Kabul, Kunar and Swat valleys and had even extracted the praise of the Alexander's historians. These highlanders, designated as \"parvatiya Ayudhajivinah\" in P\u0101\u1e47ini's Astadhyayi, were rebellious, fiercely independent and freedom-loving cavalrymen who never easily yielded to any overlord.\nThe Sanskrit drama \"Mudra-rakashas\" by \"Visakha Dutta\" and the Jaina work \"Parishishtaparvan\" refer to Chandragupta's ( \u2013 ) alliance with Himalayan king \"Parvataka\". The Himalayan alliance gave Chandragupta a formidable composite army made up of the cavalry forces of the Shakas, Yavanas, Kambojas, Kiratas, Parasikas and Bahlikas as attested by Mudra-Rakashas (Mudra-Rakshasa 2). These hordes had helped Chandragupta Maurya defeat the ruler of Magadha and placed Chandragupta on the throne, thus laying the foundations of Mauryan dynasty in Northern India.\nThe cavalry of Hunas and the Kambojas is also attested in the Raghu Vamsa epic poem of Sanskrit poet Kalidasa. Raghu of Kalidasa is believed to be Chandragupta II (\"Vikaramaditya\") (375\u2013413/15 AD), of the well-known Gupta dynasty.\nAs late as the mediaeval era, the Kamboja cavalry had also formed part of the Gurjara-Pratihara armed forces from the eighth to the 10th centuries AD. They had come to Bengal with the Pratiharas when the latter conquered part of the province.\nAncient Kambojas organised military \"sanghas\" and shrenis (corporations) to manage their political and military affairs, as Arthashastra of Kautiliya as well as the Mahabharata record. They are described as \"Ayuddha-jivi\" or \"Shastr-opajivis\" (nations-in-arms), which also means that the Kamboja cavalry offered its military services to other nations as well. There are numerous references to Kambojas having been requisitioned as cavalry troopers in ancient wars by outside nations.\nMughal Empire.\nThe Mughal armies (\"lashkar\") were primarily a cavalry force. The elite corps were the \"ahadi\" who provided direct service to the Emperor and acted as guard cavalry. Supplementary cavalry or \"dakhilis\" were recruited, equipped and paid by the central state. This was in contrast to the \"tabinan\" horsemen who were the followers of individual noblemen. Their training and equipment varied widely but they made up the backbone of the Mughal cavalry. Finally there were tribal irregulars led by and loyal to tributary chiefs. These included Hindus, Afghans and Turks summoned for military service when their autonomous leaders were called on by the Imperial government.\nEuropean Middle Ages.\nAs the quality and availability of heavy infantry declined in Europe with the fall of the Roman Empire, heavy cavalry became more effective. Infantry that lack the cohesion and discipline of tight formations are more susceptible to being broken and scattered by shock combat\u2014the main role of heavy cavalry, which rose to become the dominant force on the European battlefield.\nAs heavy cavalry increased in importance, it became the main focus of military development. The arms and armour for heavy cavalry increased, the high-backed saddle developed, and stirrups and spurs were added, increasing the advantage of heavy cavalry even more.\nThis shift in military importance was reflected in an increasingly hierarchical society as well. From the late 10th century onwards heavily armed horsemen, \"milites\" or knights, emerged as an expensive elite taking centre stage both on and off the battlefield. This class of aristocratic warriors was considered the \"ultimate\" in heavy cavalry: well-equipped with the best weapons, state-of-the-art armour from head to foot, leading with the lance in battle in a full-gallop, close-formation \"knightly charge\" that might prove irresistible, winning the battle almost as soon as it began.\nBut knights remained the minority of total available combat forces; the expense of arms, armour, and horses was only affordable to a select few. While mounted men-at-arms focused on a narrow combat role of shock combat, medieval armies relied on a large variety of foot troops to fulfill all the rest (skirmishing, flank guards, scouting, holding ground, etc.). Medieval chroniclers tended to pay undue attention to the knights at the expense of the common soldiers, which led early students of military history to suppose that heavy cavalry was the only force that mattered on medieval European battlefields. But well-trained and disciplined infantry could defeat knights.\nMassed English longbowmen triumphed over French cavalry at Cr\u00e9cy, Poitiers and Agincourt, while at Gisors (1188), Bannockburn (1314), and Laupen (1339), foot-soldiers proved they could resist cavalry charges as long as they held their formation. Once the Swiss developed their pike squares for offensive as well as defensive use, infantry started to become the principal arm. This aggressive new doctrine gave the Swiss victory over a range of adversaries, and their enemies found that the only reliable way to defeat them was by the use of an even more comprehensive combined arms doctrine, as evidenced in the Battle of Marignano. The introduction of missile weapons that required less skill than the longbow, such as the crossbow and hand cannon, also helped remove the focus somewhat from cavalry elites to masses of cheap infantry equipped with easy-to-learn weapons. These missile weapons were very successfully used in the Hussite Wars, in combination with Wagenburg tactics.\nThis gradual rise in the dominance of infantry led to the adoption of dismounted tactics. From the earliest times knights and mounted men-at-arms had frequently dismounted to handle enemies they could not overcome on horseback, such as in the Battle of the Dyle (891) and the Battle of Bremule (1119), but after the 1350s this trend became more marked with the dismounted men-at-arms fighting as super-heavy infantry with two-handed swords and poleaxes. In any case, warfare in the Middle Ages tended to be dominated by raids and sieges rather than pitched battles, and mounted men-at-arms rarely had any choice other than dismounting when faced with the prospect of assaulting a fortified position.\nIslamic States.\nArabs.\nThe Islamic Prophet Muhammad made use of cavalry in many of his military campaigns including the Expedition of Dhu Qarad, and the expedition of Zaid ibn Haritha in al-Is which took place in September, 627 AD, fifth month of 6 AH of the Islamic calendar.\nEarly organized Arab mounted forces under the Rashidun caliphate comprised a light cavalry armed with lance and sword. Its main role was to attack the enemy flanks and rear. These relatively lightly armored horsemen formed the most effective element of the Muslim armies during the later stages of the Islamic conquest of the Levant. The best use of this lightly armed fast moving cavalry was revealed at the Battle of Yarmouk (636 AD) in which Khalid ibn Walid, knowing the skills of his horsemen, used them to turn the tables at every critical instance of the battle with their ability to engage, disengage, then turn back and attack again from the flank or rear. A strong cavalry regiment was formed by Khalid ibn Walid which included the veterans of the campaign of Iraq and Syria. Early Muslim historians have given it the name \"Tali'a mutaharrikah\"(\u0637\u0644\u064a\u0639\u0629 \u0645\u062a\u062d\u0631\u0643\u0629), or the Mobile guard. This was used as an advance guard and a strong striking force to route the opposing armies with its greater mobility that give it an upper hand when maneuvering against any Byzantine army. With this mobile striking force, the conquest of Syria was made easy.\nThe Battle of Talas in 751 AD was a conflict between the Arab Abbasid Caliphate and the Chinese Tang dynasty over the control of Central Asia. Chinese infantry were routed by Arab cavalry near the bank of the River Talas.\nUntil the 11th century the classic cavalry strategy of the Arab Middle East incorporated the \"razzia\" tactics of fast moving raids by mixed bodies of horsemen and infantry. Under the talented leadership of Saladin and other Islamic commanders the emphasis changed to Mamluk horse-archers backed by bodies of irregular light cavalry. Trained to rapidly disperse, harass and regroup these flexible mounted forces proved capable of withstanding the previously invincible heavy knights of the western crusaders at battles such as Hattin in 1187.\nMamluks.\nOriginating in the 9th century as Central Asian \"ghulams\" or captives utilised as mounted auxiliaries by Arab armies, Mamluks were subsequently trained as cavalry soldiers rather than solely mounted-archers, with increased priority being given to the use of lances and swords. \nMamluks were to follow the dictates of al-furusiyya, a code of conduct that included values like courage and generosity but also doctrine of cavalry tactics, horsemanship, archery and treatment of wounds.\nBy the late 13th century the Manluk armies had evolved into a professional elite of cavalry, backed by more numerous but less well-trained footmen.\nMaghreb.\nThe Islamic Berber states of North Africa employed elite horse mounted cavalry armed with spears and following the model of the original Arab occupiers of the region. Horse-harness and weapons were manufactured locally and the six-monthly stipends for horsemen were double those of their infantry counterparts. During the 8th century Islamic conquest of Iberia large numbers of horses and riders were shipped from North Africa, to specialise in raiding and the provision of support for the massed Berber footmen of the main armies.\nMaghrebi traditions of mounted warfare eventually influenced a number of sub-Saharan African polities in the medieval era. The Esos of Ikoyi, military aristocrats of the Yoruba peoples, were a notable manifestation of this phenomenon.\nIran.\nQizilbash, were a class of Safavid militant warriors in Iran during the 15th to 18th centuries, who often fought as elite cavalry.\nOttoman.\nDuring its period of greatest expansion, from the 14th to 17th centuries, cavalry formed the powerful core of the Ottoman armies. Registers dated 1475 record 22,000 \"Sipahi\" feudal cavalry levied in Europe, 17,000 \"Sipahis\" recruited from Anatolia, and 3,000 \"Kapikulu\" (regular body-guard cavalry). During the 18th century however the Ottoman mounted troops evolved into light cavalry serving in the thinly populated regions of the Middle East and North Africa. Such frontier horsemen were largely raised by local governors and were separate from the main field armies of the Ottoman Empire. At the beginning of the 19th century modernised \"Nizam-I Credit\" (\"New Army\") regiments appeared, including full-time cavalry units officered from the horse guards of the Sultan. \nRenaissance Europe.\nIronically, the rise of infantry in the early 16th century coincided with the \"golden age\" of heavy cavalry; a French or Spanish army at the beginning of the century could have up to half its numbers made up of various kinds of light and heavy cavalry, whereas in earlier medieval and later 17th-century armies the proportion of cavalry was seldom more than a quarter.\nKnighthood largely lost its military functions and became more closely tied to social and economic prestige in an increasingly capitalistic Western society. With the rise of drilled and trained infantry, the mounted men-at-arms, now sometimes called \"gendarmes\" and often part of the standing army themselves, adopted the same role as in the Hellenistic age, that of delivering a decisive blow once the battle was already engaged, either by charging the enemy in the flank or attacking their commander-in-chief.\nFrom the 1550s onwards, the use of gunpowder weapons solidified infantry's dominance of the battlefield and began to allow true mass armies to develop. This is closely related to the increase in the size of armies throughout the early modern period; heavily armored cavalrymen were expensive to raise and maintain and it took years to train a skilled horseman or a horse, while arquebusiers and later musketeers could be trained and kept in the field at much lower cost, and were much easier to recruit.\nThe Spanish tercio and later formations relegated cavalry to a supporting role. The pistol was specifically developed to try to bring cavalry back into the conflict, together with manoeuvres such as the caracole. The caracole was not particularly successful, however, and the charge (whether with lance, sword, or pistol) remained as the primary mode of employment for many types of European cavalry, although by this time it was delivered in much deeper formations and with greater discipline than before. The demi-lancers and the heavily armored sword-and-pistol reiters were among the types of cavalry whose heyday was in the 16th and 17th centuries. During this period the Polish Winged hussars were a dominating heavy cavalry force in Eastern Europe that initially achieved great success against Swedes, Russians, Turks and other, until repeatably beaten by either combined arms tactics, increase in firepower or beaten in melee with the Drabant cavalry of the Swedish Empire. From their last engagement in 1702 (at the Battle of Klisz\u00f3w) until 1776, the obsolete Winged hussars were demoted and largely assigned to ceremonial roles. The Polish Winged hussars military prowess peaked at the Siege of Vienna in 1683, when hussar banners participated in the largest cavalry charge in history and successfully repelled the Ottoman attack.\n18th-century Europe and Napoleonic Wars.\nCavalry retained an important role in this age of regularization and standardization across European armies. They remained the primary choice for confronting enemy cavalry. Attacking an unbroken infantry force head-on usually resulted in failure, but extended linear infantry formations were vulnerable to flank or rear attacks. Cavalry was important at Blenheim (1704), Rossbach (1757), Marengo (1800), Eylau and Friedland (1807), remaining significant throughout the Napoleonic Wars.\nEven with the increasing prominence of infantry, cavalry still had an irreplaceable role in armies, due to their greater mobility. Their non-battle duties often included patrolling the fringes of army encampments, with standing orders to intercept suspected shirkers and deserters, as well as, serving as outpost pickets in advance of the main body. During battle, lighter cavalry such as hussars and uhlans might skirmish with other cavalry, attack light infantry, or charge and either capture enemy artillery or render them useless by plugging the touchholes with iron spikes. Heavier cavalry such as cuirassiers, dragoons, and carabiniers usually charged towards infantry formations or opposing cavalry in order to rout them. Both light and heavy cavalry pursued retreating enemies, the point where most battle casualties occurred.\nThe greatest cavalry charge of modern history was at the 1807 Battle of Eylau, when the entire 11,000-strong French cavalry reserve, led by Joachim Murat, launched a huge charge on and through the Russian infantry lines. Cavalry's dominating and menacing presence on the battlefield was countered by the use of infantry squares. The most notable examples are at the Battle of Quatre Bras and later at the Battle of Waterloo, the latter which the repeated charges by up to 9,000 French cavalrymen ordered by Michel Ney failed to break the British-Allied army, who had formed into squares.\nMassed infantry, especially those formed in squares were deadly to cavalry, but offered an excellent target for artillery. Once a bombardment had disordered the infantry formation, cavalry were able to rout and pursue the scattered foot soldiers. It was not until individual firearms gained accuracy and improved rates of fire that cavalry was diminished in this role as well. Even then light cavalry remained an indispensable tool for scouting, screening the army's movements, and harassing the enemy's supply lines until military aircraft supplanted them in this role in the early stages of World War I.\n19th century.\nEurope.\nBy the beginning of the 19th century, European cavalry fell into four main categories:\nThere were cavalry variations for individual nations as well: France had the \"chasseurs \u00e0 cheval\"; Prussia had the \"J\u00e4ger zu Pferde\"; Bavaria, Saxony and Austria had the \"Chevaulegers\"; and Russia had Cossacks. Britain, from the mid-18th century, had Light Dragoons as light cavalry and Dragoons, Dragoon Guards and Household Cavalry as heavy cavalry. Only after the end of the Napoleonic wars were the Household Cavalry equipped with cuirasses, and some other regiments were converted to lancers. In the United States Army prior to 1862 the cavalry were almost always dragoons. The Imperial Japanese Army had its cavalry uniformed as hussars, but they fought as dragoons.\nIn the Crimean War, the Charge of the Light Brigade and the Thin Red Line at the Battle of Balaclava showed the vulnerability of cavalry, when deployed without effective support.\nFranco-Prussian War.\nDuring the Franco-Prussian War, at the Battle of Mars-la-Tour in 1870, a Prussian cavalry brigade decisively smashed the centre of the French battle line, after skilfully concealing their approach. This event became known as Von Bredow's Death Ride after the brigade commander Adalbert von Bredow; it would be used in the following decades to argue that massed cavalry charges still had a place on the modern battlefield.\nImperial expansion.\nCavalry found a new role in colonial campaigns (irregular warfare), where modern weapons were lacking and the slow moving infantry-artillery train or fixed fortifications were often ineffective against indigenous insurgents (unless the latter offered a fight on an equal footing, as at Tel-el-Kebir, Omdurman, etc.). Cavalry \"flying columns\" proved effective, or at least cost-effective, in many campaigns\u2014although an astute native commander (like Samori in western Africa, Shamil in the Caucasus, or any of the better Boer commanders) could turn the tables and use the greater mobility of their cavalry to offset their relative lack of firepower compared with European forces.\nIn 1903 the British Indian Army maintained forty regiments of cavalry, numbering about 25,000 Indian sowars (cavalrymen), with British and Indian officers.\nAmong the more famous regiments in the lineages of the modern Indian and Pakistani armies are:\nSeveral of these formations are still active, though they now are armoured formations, for example the Guides Cavalry of Pakistan.\nThe French Army maintained substantial cavalry forces in Algeria and Morocco from 1830 until the end of World War II. Much of the Mediterranean coastal terrain was suitable for mounted action and there was a long established culture of horsemanship amongst the Arab and Berber inhabitants. The French forces included Spahis, Chasseurs d' Afrique, Foreign Legion cavalry and mounted Goumiers. Both Spain and Italy raised cavalry regiments from amongst the indigenous horsemen of their North African territories (see regulares, Italian Spahis and savari respectively).\nImperial Germany employed mounted formations in South West Africa as part of the Schutztruppen (colonial army) garrisoning the territory.\nUnited States.\nIn the early American Civil War the regular United States Army mounted rifle, dragoon, and two existing cavalry regiments were reorganized and renamed cavalry regiments, of which there were six. Over a hundred other federal and state cavalry regiments were organized, but the infantry played a much larger role in many battles due to its larger numbers, lower cost per rifle fielded, and much easier recruitment. However, cavalry saw a role as part of screening forces and in foraging and scouting. The later phases of the war saw the Federal army developing a truly effective cavalry force fighting as scouts, raiders, and, with repeating rifles, as mounted infantry. The distinguished 1st Virginia Cavalry ranks as one of the most effectual and successful cavalry units on the Confederate side. Noted cavalry commanders included Confederate general J.E.B. Stuart, Nathan Bedford Forrest, and John Singleton Mosby (a.k.a. \"The Grey Ghost\") and on the Union side, Philip Sheridan and George Armstrong Custer.\nPost Civil War, as the volunteer armies disbanded, the regular army cavalry regiments increased in number from six to ten, among them Custer's U.S. 7th Cavalry Regiment of Little Bighorn fame, and the African-American U.S. 9th Cavalry Regiment and U.S. 10th Cavalry Regiment. The black units, along with others (both cavalry and infantry), collectively became known as the Buffalo Soldiers. According to Robert M. Utley: \nThese regiments, which rarely took the field as complete organizations, served throughout the American Indian Wars through the close of the frontier in the 1890s. Volunteer cavalry regiments like the Rough Riders consisted of horsemen such as cowboys, ranchers and other outdoorsmen, that served as a cavalry in the United States Military.\nDevelopments 1900\u20131914.\nAt the beginning of the 20th century, all armies still maintained substantial cavalry forces, although there was contention over whether their role should revert to that of mounted infantry (the historic dragoon function). With motorised vehicles and aircraft still under development, horse mounted troops remained the only fully mobile forces available for manoeuvre warfare until 1914.\nUnited Kingdom.\nFollowing the experience of the South African War of 1899\u20131902 (where mounted Boer citizen commandos fighting on foot from cover proved more effective than regular cavalry), the British Army withdrew lances for all but ceremonial purposes and placed a new emphasis on training for dismounted action in 1903. Lances were however readopted for active service in 1912.\nRussia.\nIn 1882, the Imperial Russian Army converted all its line hussar and lancer regiments to dragoons, with an emphasis on mounted infantry training. In 1910 these regiments reverted to their historic roles, designations and uniforms.\nGermany.\nBy 1909, official regulations dictating the role of the Imperial German cavalry had been revised to indicate an increasing realization of the realities of modern warfare. The massive cavalry charge in three waves which had previously marked the end of annual maneuvers was discontinued and a new emphasis was placed in training on scouting, raiding and pursuit; rather than main battle involvement. The perceived importance of cavalry was however still evident, with thirteen new regiments of mounted rifles (\"J\u00e4ger zu Pferde\") being raised shortly before the outbreak of war in 1914.\nFrance.\nIn spite of significant experience in mounted warfare in Morocco during 1908\u201314, the French cavalry remained a highly conservative institution. The traditional tactical distinctions between heavy, medium, and light cavalry branches were retained. French cuirassiers wore breastplates and plumed helmets unchanged from the Napoleonic period, during the early months of World War I. Dragoons were similarly equipped, though they did not wear cuirasses and did carry lances. Light cavalry were described as being \"a blaze of colour\". French cavalry of all branches were well mounted and were trained to change position and charge at full gallop. One weakness in training was that French cavalrymen seldom dismounted on the march and their horses suffered heavily from raw backs in August 1914.\nFirst World War.\nEurope 1914.\nIn August 1914, all combatant armies still retained substantial numbers of cavalry and the mobile nature of the opening battles on both Eastern and Western Fronts provided a number of instances of traditional cavalry actions, though on a smaller and more scattered scale than those of previous wars. The 110 regiments of Imperial German cavalry, while as colourful and traditional as any in peacetime appearance, had adopted a practice of falling back on infantry support when any substantial opposition was encountered. These cautious tactics aroused derision amongst their more conservative French and Russian opponents but proved appropriate to the new nature of warfare. A single attempt by the German army, on 12 August 1914, to use six regiments of massed cavalry to cut off the Belgian field army from Antwerp floundered when they were driven back in disorder by rifle fire. The two German cavalry brigades involved lost 492 men and 843 horses in repeated charges against dismounted Belgian lancers and infantry. One of the last recorded charges by French cavalry took place on the night of 9/10 September 1914 when a squadron of the 16th Dragoons overran a German airfield at Soissons, while suffering heavy losses. Once the front lines stabilised on the Western Front with the start of Trench Warfare, a combination of barbed wire, uneven muddy terrain, machine guns and rapid fire rifles proved deadly to horse mounted troops and by early 1915 most cavalry units were no longer seeing front line action.\nOn the Eastern Front, a more fluid form of warfare arose from flat open terrain favorable to mounted warfare. On the outbreak of war in 1914 the bulk of the Russian cavalry was deployed at full strength in frontier garrisons and, during the period that the main armies were mobilizing, scouting and raiding into East Prussia and Austrian Galicia was undertaken by mounted troops trained to fight with sabre and lance in the traditional style. On 21 August 1914 the 4th Austro-Hungarian \"Kavalleriedivison\" fought a major mounted engagement at Jaroslavic with the Russian 10th Cavalry Division, in what was arguably the final historic battle to involve thousands of horsemen on both sides. While this was the last massed cavalry encounter on the Eastern Front, the absence of good roads limited the use of mechanized transport and even the technologically advanced Imperial German Army continued to deploy up to twenty-four horse-mounted divisions in the East, as late as 1917.\nEurope 1915\u20131918.\nFor the remainder of the War on the Western Front, cavalry had virtually no role to play. The British and French armies dismounted many of their cavalry regiments and used them in infantry and other roles: the Life Guards for example spent the last months of the War as a machine gun corps; and the Australian Light Horse served as light infantry during the Gallipoli campaign. In September 1914 cavalry comprised 9.28% of the total manpower of the British Expeditionary Force in France\u2014by July 1918 this proportion had fallen to 1.65%. As early as the first winter of the war most French cavalry regiments had dismounted a squadron each, for service in the trenches. The French cavalry numbered 102,000 in May 1915 but had been reduced to 63,000 by October 1918. The German Army dismounted nearly all their cavalry in the West, maintaining only one mounted division on that front by January 1917.\nItaly entered the war in 1915 with thirty regiments of line cavalry, lancers and light horse. While employed effectively against their Austro-Hungarian counterparts during the initial offensives across the Isonzo River, the Italian mounted forces ceased to have a significant role as the front shifted into mountainous terrain. By 1916 most cavalry machine-gun sections and two complete cavalry divisions had been dismounted and seconded to the infantry.\nSome cavalry were retained as mounted troops in reserve behind the lines, in anticipation of a penetration of the opposing trenches that it seemed would never come. Tanks, introduced on the Western Front by the British in September 1916 during the Battle of the Somme, had the capacity to achieve such breakthroughs but did not have the reliable range to exploit them. In their first major use at the Battle of Cambrai (1917), the plan was for a cavalry division to follow behind the tanks, however they were not able to cross a canal because a tank had broken the only bridge. On a few other occasions, throughout the war, cavalry were readied in significant numbers for involvement in major offensives; such as in the Battle of Caporetto and the Battle of Moreuil Wood. However it was not until the German Army had been forced to retreat in the Hundred Days Offensive of 1918, that limited numbers of cavalry were again able to operate with any effectiveness in their intended role. There was a successful charge by the British 7th Dragoon Guards on the last day of the war.\nIn the wider spaces of the Eastern Front, a more fluid form of warfare continued and there was still a use for mounted troops. Some wide-ranging actions were fought, again mostly in the early months of the war. However, even here the value of cavalry was overrated and the maintenance of large mounted formations at the front by the Russian Army put a major strain on the railway system, to little strategic advantage. In February 1917, the Russian regular cavalry (exclusive of Cossacks) was reduced by nearly a third from its peak number of 200,000, as two squadrons of each regiment were dismounted and incorporated into additional infantry battalions. Their Austro-Hungarian opponents, plagued by a shortage of trained infantry, had been obliged to progressively convert most horse cavalry regiments to dismounted rifle units starting in late 1914.\nMiddle East.\nIn the Middle East, during the Sinai and Palestine Campaign mounted forces (British, Indian, Ottoman, Australian, Arab and New Zealand) retained an important strategic role both as mounted infantry and cavalry.\nIn Egypt, the mounted infantry formations like the New Zealand Mounted Rifles Brigade and Australian Light Horse of ANZAC Mounted Division, operating as mounted infantry, drove German and Ottoman forces back from Romani to Magdhaba and Rafa and out of the Egyptian Sinai Peninsula in 1916.\nAfter a stalemate on the Gaza\u2013Beersheba line between March and October 1917, Beersheba was captured by the Australian Mounted Division's 4th Light Horse Brigade. Their mounted charge succeeded after a coordinated attack by the British Infantry and Yeomanry cavalry and the Australian and New Zealand Light Horse and Mounted Rifles brigades. A series of coordinated attacks by these Egyptian Expeditionary Force infantry and mounted troops were also successful at the Battle of Mughar Ridge, during which the British infantry divisions and the Desert Mounted Corps drove two Ottoman armies back to the Jaffa\u2014Jerusalem line. The infantry with mainly dismounted cavalry and mounted infantry fought in the Judean Hills to eventually almost encircle Jerusalem which was occupied shortly after.\nDuring a pause in operations necessitated by the German spring offensive in 1918 on the Western Front, joint infantry and mounted infantry attacks towards Amman and Es Salt resulted in retreats back to the Jordan Valley which continued to be occupied by mounted divisions during the summer of 1918.\nThe Australian Mounted Division was armed with swords and in September, after the successful breaching of the Ottoman line on the Mediterranean coast by the British Empire infantry XXI Corps was followed by cavalry attacks by the 4th Cavalry Division, 5th Cavalry Division and Australian Mounted Divisions which almost encircled two Ottoman armies in the Judean Hills forcing their retreat. Meanwhile, Chaytor's Force of infantry and mounted infantry in ANZAC Mounted Division held the Jordan Valley, covering the right flank to later advance eastwards to capture Es Salt and Amman and half of a third Ottoman army. A subsequent pursuit by the 4th Cavalry Division and the Australian Mounted Division followed by the 5th Cavalry Division to Damascus. Armoured cars and 5th Cavalry Division lancers were continuing the pursuit of Ottoman units north of Aleppo when the Armistice of Mudros was signed by the Ottoman Empire.\nPost\u2013World War I.\nA combination of military conservatism in almost all armies and post-war financial constraints prevented the lessons of 1914\u20131918 being acted on immediately. There was a general reduction in the number of cavalry regiments in the British, French, Italian and other Western armies but it was still argued with conviction (for example in the 1922 edition of the \"Encyclop\u00e6dia Britannica\") that mounted troops had a major role to play in future warfare. The 1920s saw an interim period during which cavalry remained as a proud and conspicuous element of all major armies, though much less so than prior to 1914.\nCavalry was extensively used in the Russian Civil War and the Soviet-Polish War. The last major cavalry battle was the Battle of Komar\u00f3w in 1920, between Poland and the Russian Bolsheviks. Colonial warfare in Morocco, Syria, the Middle East and the North West Frontier of India provided some opportunities for mounted action against enemies lacking advanced weaponry.\nThe post-war German Army (Reichsheer) was permitted a large proportion of cavalry (18 regiments or 16.4% of total manpower) under the conditions of the Treaty of Versailles.\nThe British Army mechanised all cavalry regiments between 1929 and 1941, redefining their role from horse to armoured vehicles to form the Royal Armoured Corps together with the Royal Tank Regiment. The U.S. Cavalry abandoned its sabres in 1934 and commenced the conversion of its horsed regiments to mechanized cavalry, starting with the First Regiment of Cavalry in January 1933.\nDuring the Turkish War of Independence, Turkish cavalry under General Fahrettin Altay was instrumental in the Kemalist victory over the invading Greek Army in 1922 during the Battle of Dumlup\u0131nar. The 5th Cavalry Division was able to slip behind the main Greek army, cutting off all communication and supply lines as well as retreat options. This forced the surrender of the remaining Greek forces and may have been the last time in history that cavalry played a definitive role in the outcome of a battle. \nDuring the 1930s, the French Army experimented with integrating mounted and mechanised cavalry units into larger formations. Dragoon regiments were converted to motorised infantry (trucks and motor cycles), and cuirassiers to armoured units; while light cavalry (chasseurs a' cheval, hussars and spahis) remained as mounted sabre squadrons. The theory was that mixed forces comprising these diverse units could utilise the strengths of each according to circumstances. In practice mounted troops proved unable to keep up with fast moving mechanised units over any distance.\nThe 39 cavalry regiments of the British Indian Army were reduced to 21 as the result of a series of amalgamations immediately following World War I. The new establishment remained unchanged until 1936 when three regiments were redesignated as permanent training units, each with six, still mounted, regiments linked to them. In 1938, the process of mechanization began with the conversion of a full cavalry brigade (two Indian regiments and one British) to armoured car and tank units. By the end of 1940, all of the Indian cavalry had been mechanized, initially and in the majority of cases, to motorized infantry transported in 15cwt trucks. The last horsed regiment of the British Indian Army (other than the Viceroy's Bodyguard and some Indian States Forces regiments) was the 19th King George's Own Lancers which had its final mounted parade at Rawalpindi on 28 October 1939. This unit still exists in the Pakistan Army as an armored regiment.\nWorld War II.\nWhile most armies still maintained cavalry units at the outbreak of World War II in 1939, significant mounted action was largely restricted to the Polish, Balkan, and Soviet campaigns. Rather than charge their mounts into battle, cavalry units were either used as mounted infantry (using horses to move into position and then dismounting for combat) or as reconnaissance units (especially in areas not suited to tracked or wheeled vehicles).\nPolish.\nA popular myth is that Polish cavalry armed with lances charged German tanks during the September 1939 campaign. This arose from misreporting of a single clash on 1 September near Krojanty, when two squadrons of the Polish 18th Lancers armed with sabres scattered German infantry before being caught in the open by German armoured cars.\nTwo examples illustrate how the myth developed. First, because motorised vehicles were in short supply, the Poles used horses to pull anti-tank weapons into position. Second, there were a few incidents when Polish cavalry was trapped by German tanks, and attempted to fight free. However, this did not mean that the Polish army chose to attack tanks with horse cavalry. Later, on the Eastern Front, the Red Army did deploy cavalry units effectively against the Germans.\nA more correct term would be \"mounted infantry\" instead of \"cavalry\", as horses were primarily used as a means of transportation, for which they were very suitable in view of the very poor road conditions in pre-war Poland. Another myth describes Polish cavalry as being armed with both sabres and lances; lances were used for peacetime ceremonial purposes only and the primary weapon of the Polish cavalryman in 1939 was a rifle. Individual equipment did include a sabre, probably because of well-established tradition, and in the case of a melee combat this secondary weapon would probably be more effective than a rifle and bayonet. Moreover, the Polish cavalry brigade order of battle in 1939 included, apart from the mounted soldiers themselves, light and heavy machine guns (wheeled), the Anti-tank rifle, model 35, anti-aircraft weapons, anti tank artillery such as the Bofors 37 mm, also light and scout tanks, etc. The last cavalry vs. cavalry mutual charge in Europe took place in Poland during the Battle of Krasnobr\u00f3d, when Polish and German cavalry units clashed with each other.\nThe last classical cavalry charge of the war took place on March 1, 1945, during the Battle of Schoenfeld by the 1st \"Warsaw\" Independent Cavalry Brigade. Infantry and tanks had been employed to little effect against the German position, both of which floundered in the open wetlands only to be dominated by infantry and antitank fire from the German fortifications on the forward slope of Hill 157, overlooking the wetlands. The Germans had not taken cavalry into consideration when fortifying their position which, combined with the \"Warsaw\"s swift assault, overran the German anti-tank guns and consolidated into an attack into the village itself, now supported by infantry and tanks.\nGreek.\nThe Italian invasion of Greece in October 1940 saw mounted cavalry used effectively by the Greek defenders along the mountainous frontier with Albania. Three Greek cavalry regiments (two mounted and one partially mechanized) played an important role in the Italian defeat in this difficult terrain.\nSoviet.\nThe contribution of Soviet cavalry to the development of modern military operational doctrine and its importance in defeating Nazi Germany has been eclipsed by the higher profile of tanks and airplanes. Soviet cavalry contributed significantly to the defeat of the Axis armies. They were able to provide the most mobile troops available in the early stages, when trucks and other equipment were low in quality; as well as providing cover for retreating forces.\nConsidering their relatively limited numbers, the Soviet cavalry played a significant role in giving Germany its first real defeats in the early stages of the war. The continuing potential of mounted troops was demonstrated during the Battle of Moscow, against Guderian and the powerful central German 9th Army. Pavel Belov was given by Stavka a mobile group including the elite 9th tank brigade, ski battalions, Katyusha rocket launcher battalion among others, the unit additionally received new weapons. This newly created group became the first to carry the Soviet counter-offensive in late November, when the general offensive began on 5 December. These mobile units often played major roles in both defensive and offensive operations.\nCavalry were amongst the first Soviet units to complete the encirclement in the Battle of Stalingrad, thus sealing the fate of the German 6th Army. Mounted Soviet forces also played a role in the encirclement of Berlin, with some Cossack cavalry units reaching the Reichstag in April 1945. Throughout the war they performed important tasks such as the capture of bridgeheads which is considered one of the hardest jobs in battle, often doing so with inferior numbers. For instance the 8th Guards Cavalry Regiment of the 2nd Guards Cavalry Division (Soviet Union), 1st Guards Cavalry Corps often fought outnumbered against elite German units.\nBy the final stages of the war only the Soviet Union was still fielding mounted units in substantial numbers, some in combined mechanized and horse units. The main advantage of this tactical approach was in enabling mounted infantry to keep pace with advancing tanks. Other factors favoring the retention of mounted forces included the high quality of Russian Cossacks, which provided about half of all mounted Soviet cavalry throughout the war. They excelled in warfare manoeuvers, since the lack of roads limited the effectiveness of wheeled vehicles in many parts of the Eastern Front. Another consideration was that sufficient logistic capacity was often not available to support very large motorized forces, whereas cavalry was relatively easy to maintain when detached from the main army and acting on its own initiative. The main usage of the Soviet cavalry involved infiltration through front lines with subsequent deep raids, which disorganized German supply lines. Another role was the pursuit of retreating enemy forces during major front-line operations and breakthroughs.\nHungarian.\nDuring World War II, the Royal Hungarian Army's hussars were typically only used to undertake reconnaissance tasks against Soviet forces, and then only in detachments of section or squadron strength.\nThe last documented hussar attack was conducted by Lieutenant Colonel K\u00e1lm\u00e1n Mikecz on August 16, 1941, at Nikolaev. The hussars arriving as reinforcements, were employed to break through Russian positions ahead of German troops. The hussars equipped with swords and submachine guns broke through the Russian lines in a single attack.\nAn eyewitness account of the last hussar attack by Erich Kern, a German officer, was written in his memoir in 1948:\n\u2026 We were again in a tough fight with the desperately defensive enemy who dug himself along a high railway embankment. We've been attacked four times already, and we've been kicked back all four times. The battalion commander swore, but the company commanders were helpless. Then, instead of the artillery support we asked for countless times, a Hungarian hussar regiment appeared on the scene. We laughed. What the hell do they want here with their graceful, elegant horses? We froze at once: these Hungarians went crazy. Cavalry Squadron approached after a cavalry squadron. The command word rang. The bronze-brown, slender riders almost grew to their saddle.\nTheir shining colonel of golden parolis jerked his sword. Four or five armored cars cut out of the wings, and the regiment slashed across the wide plain with flashing swords in the afternoon sun. Seydlitz attacked like this once before. Forgetting all caution, we climbed out of our covers. It was all like a great equestrian movie. The first shots rumbled, then became less frequent. With astonished eyes, in disbelief, we watched as the Soviet regiment, which had so far repulsed our attacks with desperate determination, now turned around and left its positions in panic. And the triumphant Hungarians chased the Russian in front of them and shredded them with their glittering sabers. The hussar sword, it seems, was a bit much for the nerves of Russians. Now, for once, the ancient weapon has triumphed over modern equipment ...\nItalian.\nThe last mounted sabre charge by Italian cavalry occurred on August 24, 1942, at Isbuscenski (Russia), when a squadron of the Savoia Cavalry Regiment charged the 812th Siberian Infantry Regiment. The remainder of the regiment, together with the Novara Lancers made a dismounted attack in an action that ended with the retreat of the Russians after heavy losses on both sides. The final Italian cavalry action occurred on October 17, 1942, in Poloj (now Croatia) by a squadron of the Alexandria Cavalry Regiment against a large group of Yugoslav partisans.\nOther Axis Powers.\nRomanian, Hungarian and Italian cavalry were dispersed or disbanded following the retreat of the Axis forces from Russia. Germany still maintained some mounted (mixed with bicycles) SS and Cossack units until the last days of the War.\nFinnish.\nFinland used mounted troops against Russian forces effectively in forested terrain during the Continuation War. The last Finnish cavalry unit was not disbanded until 1947.\nAmerican.\nThe U.S. Army's last horse cavalry actions were fought during World War II: a) by the 26th Cavalry Regiment\u2014a small mounted regiment of Philippine Scouts which fought the Japanese during the retreat down the Bataan peninsula, until it was effectively destroyed by January 1942; and b) on captured German horses by the mounted reconnaissance section of the U.S. 10th Mountain Division in a spearhead pursuit of the German Army across the Po Valley in Italy in April 1945. The last horsed U.S. Cavalry (the Second Cavalry Division) were dismounted in March 1944.\nBritish.\nAll British Army cavalry regiments had been mechanised since 1 March 1942 when the Queen's Own Yorkshire Dragoons (Yeomanry) was converted to a motorised role, following mounted service against the Vichy French in Syria the previous year. The final cavalry charge by British Empire forces occurred on 21 March 1942 when a 60 strong patrol of the Burma Frontier Force encountered Japanese infantry near Toungoo airfield in central Myanmar. The Sikh sowars of the Frontier Force cavalry, led by Captain Arthur Sandeman of The Central India Horse (21st King George V's Own Horse), charged in the old style with sabres and most were killed.\nMongolian.\nIn the early stages of World War II, mounted units of the Mongolian People's Army were involved in the Battle of Khalkhin Gol against invading Japanese forces. Soviet forces under the command of Georgy Zhukov, together with Mongolian forces, defeated the Japanese Sixth army and effectively ended the Soviet\u2013Japanese Border Wars. After the Soviet\u2013Japanese Neutrality Pact of 1941, Mongolia remained neutral throughout most of the war, but its geographical situation meant that the country served as a buffer between Japanese forces and the Soviet Union. In addition to keeping around 10% of the population under arms, Mongolia provided half a million trained horses for use by the Soviet Army. In 1945 a partially mounted Soviet-Mongolian Cavalry Mechanized Group played a supporting role on the western flank of the Soviet invasion of Manchuria. The last active service seen by cavalry units of the Mongolian Army occurred in 1946\u20131948, during border clashes between Mongolia and the Republic of China.\nPost\u2013World War II to the present day.\nWhile most modern \"cavalry\" units have some historic connection with formerly mounted troops this is not always the case. The modern Irish Defence Forces (DF) includes a \"Cavalry Corps\" equipped with armoured cars and Scorpion tracked combat reconnaissance vehicles. The DF has never included horse cavalry since its establishment in 1922 (other than a small mounted escort of Blue Hussars drawn from the Artillery Corps when required for ceremonial occasions). However, the mystique of the cavalry is such that the name has been introduced for what was always a mechanised force.\nSome engagements in late 20th and early 21st century guerrilla wars involved mounted troops, particularly against partisan or guerrilla fighters in areas with poor transport infrastructure. Such units were not used as cavalry but rather as mounted infantry. Examples occurred in Afghanistan, Portuguese Africa and Rhodesia. The French Army used existing mounted squadrons of Spahis to a limited extent for patrol work during the Algerian War (1954\u20131962). The last mounted charge by French cavalry was carried out on 14 May 1957 by a detachment of Spahis at Magoura during the Algerian War. \nThe Swiss Army maintained a mounted dragoon regiment for combat purposes until 1973. The Portuguese Army used horse mounted cavalry with some success in the wars of independence in Angola and Mozambique in the 1960s and 1970s. During the 1964\u20131979 Rhodesian Bush War the Rhodesian Army created an elite mounted infantry unit called Grey's Scouts to fight unconventional actions against the rebel forces of Robert Mugabe and Joshua Nkomo. The horse mounted infantry of the Scouts were effective and reportedly feared by their opponents in the rebel African forces. In the 1978 to present Afghan Civil War period there have been several instances of horse mounted combat.\nCentral and South American armies maintained mounted cavalry for longer than those of Asia, Europe, or North America. The Mexican Army included a number of horse mounted cavalry regiments as late as the mid-1990s and the Chilean Army had five such regiments in 1983 as mounted mountain troops.\nAfter the end of World War II, the remaining 26 Soviet cavalry divisions were mostly converted into mechanized and tank units or disbanded. Meanwhile the overall Red Army became the Soviet Ground Forces in 1945. The last cavalry divisions were not disbanded until the early 1950s, with the last cavalry division, the 4th Guards Cavalry Division (II Formation, previously reduced in status from 4th Guards Cavalry Corps), being disbanded in April 1955.\nOperational horse cavalry.\nToday the Indian Army's 61st Cavalry is reported to be the largest existing horse-mounted cavalry unit still having operational potential. It was raised in 1951 from the amalgamated state cavalry squadrons of Gwalior, Jodhpur, and Mysore. While primarily utilised for ceremonial purposes, the regiment can be deployed for internal security or police roles if required. The 61st Cavalry and the President's Body Guard parade in full dress uniform in New Delhi each year in what is probably the largest assembly of traditional cavalry still to be seen in the world. Both the Indian and the Pakistani armies maintain armoured regiments with the titles of Lancers or Horse, dating back to the 19th century. \nAs of 2007, the Chinese People's Liberation Army employed two battalions of horse-mounted border guards in Xinjiang for border patrol purposes. PLA mounted units last saw action during border clashes with Vietnam in the 1970s and 1980s, after which most cavalry units were disbanded as part of major military downsizing in the 1980s. In the wake of the 2008 Sichuan earthquake, there were calls to rebuild the army horse inventory for disaster relief in difficult terrain. Subsequent Chinese media reports confirm that the PLA maintains operational horse cavalry at squadron strength in Xinjiang and Inner Mongolia for scouting, logistical, and border security purposes, and one at company strength in Qinghai.\nThe Chilean Army still maintains a mixed armoured cavalry regiment, with elements of it acting as mounted mountain exploration troops, based in the city of Angol, being part of the III Mountain Division, and another independent exploration cavalry detachment in the town of Chait\u00e9n. The rugged mountain terrain calls for the use of special horses suited for that use.\nThe Argentine Army has two mounted cavalry units: the Regiment of Horse Grenadiers, which performs mostly ceremonial duties but at the same time is responsible for the president's security (in this case, acting as infantry), and the 4th Mountain Cavalry Regiment (which comprises both horse and light armoured squadrons), stationed in San Mart\u00edn de los Andes, where it has an exploration role as part the 6th Mountain Brigade. Most armoured cavalry units of the Army are considered successors to the old cavalry regiments from the Independence Wars, and keep their traditional names, such as Hussars, Cuirassiers, Lancers, etc., and uniforms. Equestrian training remains an important part of their tradition, especially among officers.\nCeremonial horse cavalry and armored cavalry retaining traditional titles.\nCavalry or mounted gendarmerie units continue to be maintained for purely or primarily ceremonial purposes by the Algerian, Argentine, Bolivian, Brazilian, British, Bulgarian, Canadian, Chilean, Colombian, Danish, Dutch, Finnish, French, Hungarian, Indian, Italian, Jordanian, Malaysian, Moroccan, Nepalese, Nigerian, North Korean, Omani, Pakistani, Panamanian, Paraguayan, Peruvian, Polish, Portuguese, Russian, Senegalese, Spanish, Swedish, Thai, Tunisian, Turkmenistan, United States, Uruguayan and Venezuelan armed forces.\nA number of armoured regiments in the British Army retain the historic designations of Hussars, Dragoons, Light Dragoons, Dragoon Guards, Lancers and Yeomanry. Only the Household Cavalry (consisting of the Life Guards' mounted squadron, The Blues and Royals' mounted squadron, the State Trumpeters of The Household Cavalry and the Household Cavalry Mounted Band) are maintained for mounted (and dismounted) ceremonial duties in London.\nThe French Army still has regiments with the historic designations of Cuirassiers, Hussars, Chasseurs, Dragoons and Spahis. Only the cavalry of the Republican Guard and a ceremonial \"fanfare\" detachment of trumpeters for the cavalry/armoured branch as a whole are now mounted.\nIn the Canadian Army, a number of regular and reserve units have cavalry roots, including The Royal Canadian Hussars (Montreal), the Governor General's Horse Guards, Lord Strathcona's Horse, The British Columbia Dragoons , The Royal Canadian Dragoons, and the South Alberta Light Horse. Of these, only Lord Strathcona's Horse and the Governor General's Horse Guards maintain an official ceremonial horse-mounted cavalry troop or squadron.\nThe modern Pakistan army maintains about 40 armoured regiments with the historic titles of Lancers, Cavalry or Horse. Six of these date back to the 19th century, although only the President's Body Guard remains horse-mounted.\nIn 2002, the Army of the Russian Federation reintroduced a ceremonial mounted squadron wearing historic uniforms.\nBoth the Australian and New Zealand armies follow the British practice of maintaining traditional titles (Light Horse or Mounted Rifles) for modern mechanised units. However, neither country retains a horse-mounted unit.\nSeveral armored units of the modern United States Army retain the designation of \"armored cavalry\". The United States also has \"air cavalry\" units equipped with helicopters. The Horse Cavalry Detachment of the U.S. Army's 1st Cavalry Division, made up of active duty soldiers, still functions as an active unit, trained to approximate the weapons, tools, equipment and techniques used by the United States Cavalry in the 1880s.\nThe Turkish Armed Forces retain a ceremonial cavalry regiment, which also participates in equestrianism, following the disbandment of the operational mounted brigades during the 1960s. \nNon-combat support roles.\nThe First Troop Philadelphia City Cavalry is a volunteer unit within the Pennsylvania Army National Guard which serves as a combat force when in federal service but acts in a mounted disaster relief role when in state service. In addition, the Parsons' Mounted Cavalry is a Reserve Officer Training Corps unit which forms part of the Corps of Cadets at Texas A&amp;M University. Valley Forge Military Academy and College also has a Mounted Company, known as D-Troop .\nSome individual U.S. states maintain cavalry units as a part of their respective state defense forces. The Maryland Defense Force includes a cavalry unit, Cavalry Troop A, which serves primarily as a ceremonial unit. The unit training includes a saber qualification course based upon the 1926 U.S. Army course. Cavalry Troop A also assists other Maryland agencies as a rural search and rescue asset. In Massachusetts, The National Lancers trace their lineage to a volunteer cavalry militia unit established in 1836 and are currently organized as an official part of the Massachusetts Organized Militia. The National Lancers maintain three units, Troops A, B, and C, which serve in a ceremonial role and assist in search and rescue missions. In July 2004, the National Lancers were ordered into active state service to guard Camp Curtis Guild during the 2004 Democratic National Convention. The Governor's Horse Guard of Connecticut maintains two companies which are trained in urban crowd control. In 2020, the California State Guard stood up the 26th Mounted Operations Detachment, a search-and-rescue cavalry unit.\nSocial status.\nFrom the beginning of civilization to the 20th century, ownership of heavy cavalry horses has been a mark of wealth amongst settled peoples. A cavalry horse involves considerable expense in breeding, training, feeding, and equipment, and has very little productive use except as a mode of transport.\nFor this reason, and because of their often decisive military role, the cavalry has typically been associated with high social status. This was most clearly seen in the feudal system, where a lord was expected to enter combat armored and on horseback and bring with him an entourage of lightly armed peasants on foot. If landlords and peasant levies came into conflict, the poorly trained footmen would be ill-equipped to defeat armored knights.\nIn later national armies, service as an officer in the cavalry was generally a badge of high social status. For instance prior to 1914 most officers of British cavalry regiments came from a socially privileged background and the considerable expenses associated with their role generally required private means, even after it became possible for officers of the line infantry regiments to live on their pay. Options open to poorer cavalry officers in the various European armies included service with less fashionable (though often highly professional) frontier or colonial units. These included the British Indian cavalry, the Russian Cossacks or the French Chasseurs d'Afrique.\nDuring the 19th and early 20th centuries most monarchies maintained a mounted cavalry element in their royal or imperial guards. These ranged from small units providing ceremonial escorts and palace guards, through to large formations intended for active service. The mounted escort of the Spanish Royal Household provided an example of the former and the twelve cavalry regiments of the Prussian Imperial Guard an example of the latter. In either case the officers of such units were likely to be drawn from the aristocracies of their respective societies.\nOn film.\nSome sense of the noise and power of a cavalry charge can be gained from the 1970 film \"Waterloo\", which featured some 2,000 cavalrymen, some of them Cossacks. It included detailed displays of the horsemanship required to manage animal and weapons in large numbers at the gallop (unlike the real battle of Waterloo, where deep mud significantly slowed the horses). The Gary Cooper movie \"They Came to Cordura\" contains a scene of a cavalry regiment deploying from march to battle line formation. A smaller-scale cavalry charge can be seen in \"\" (2003); although the finished scene has substantial computer-generated imagery, raw footage and reactions of the riders are shown in the Extended Version DVD Appendices.\nOther films that show cavalry actions include:"}
{"id": "6817", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=6817", "title": "Canonization of Saints", "text": ""}
{"id": "6818", "revid": "5320876", "url": "https://en.wikipedia.org/wiki?curid=6818", "title": "Citric acid cycle", "text": "The citric acid cycle\u2014also known as the Krebs cycle, Szent\u2013Gy\u00f6rgyi\u2013Krebs cycle, or TCA cycle (tricarboxylic acid cycle)\u2014is a series of biochemical reactions to release the energy stored in nutrients through the oxidation of acetyl-CoA derived from carbohydrates, fats, proteins, and alcohol. The chemical energy released is available in the form of ATP. The Krebs cycle is used by organisms that respire (as opposed to organisms that ferment) to generate energy, either by anaerobic respiration or aerobic respiration. In addition, the cycle provides precursors of certain amino acids, as well as the reducing agent NADH, that are used in numerous other reactions. Its central importance to many biochemical pathways suggests that it was one of the earliest components of metabolism. Even though it is branded as a \"cycle\", it is not necessary for metabolites to follow only one specific route; at least three alternative segments of the citric acid cycle have been recognized.\nThe name of this metabolic pathway is derived from the citric acid (a tricarboxylic acid, often called citrate, as the ionized form predominates at biological pH) that is consumed and then regenerated by this sequence of reactions to complete the cycle. The cycle consumes acetate (in the form of acetyl-CoA) and water, reduces NAD+ to NADH, releasing carbon dioxide. The NADH generated by the citric acid cycle is fed into the oxidative phosphorylation (electron transport) pathway. The net result of these two closely linked pathways is the oxidation of nutrients to produce usable chemical energy in the form of ATP.\nIn eukaryotic cells, the citric acid cycle occurs in the matrix of the mitochondrion. In prokaryotic cells, such as bacteria, which lack mitochondria, the citric acid cycle reaction sequence is performed in the cytosol with the proton gradient for ATP production being across the cell's surface (plasma membrane) rather than the inner membrane of the mitochondrion.\nFor each pyruvate molecule (from glycolysis), the overall yield of energy-containing compounds from the citric acid cycle is three NADH, one FADH2, and one GTP.\nDiscovery.\nSeveral of the components and reactions of the citric acid cycle were established in the 1930s by the research of Albert Szent-Gy\u00f6rgyi, who received the Nobel Prize in Physiology or Medicine in 1937 specifically for his discoveries pertaining to fumaric acid, a component of the cycle. He made this discovery by studying pigeon breast muscle. Because this tissue maintains its oxidative capacity well after breaking down in the Latapie mincer and releasing in aqueous solutions, breast muscle of the pigeon was very well qualified for the study of oxidative reactions. The citric acid cycle itself was finally identified in 1937 by Hans Adolf Krebs and William Arthur Johnson while at the University of Sheffield, for which the former received the Nobel Prize for Physiology or Medicine in 1953, and for whom the cycle is sometimes named the \"Krebs cycle\".\nOverview.\nThe citric acid cycle is a metabolic pathway that connects carbohydrate, fat, and protein metabolism. The reactions of the cycle are carried out by eight enzymes that completely oxidize acetate (a two carbon molecule), in the form of acetyl-CoA, into two molecules each of carbon dioxide and water. Through catabolism of sugars, fats, and proteins, the two-carbon organic product acetyl-CoA is produced which enters the citric acid cycle. The reactions of the cycle also convert three equivalents of nicotinamide adenine dinucleotide (NAD+) into three equivalents of reduced NAD (NADH), one equivalent of flavin adenine dinucleotide (FAD) into one equivalent of FADH2, and one equivalent each of guanosine diphosphate (GDP) and inorganic phosphate (Pi) into one equivalent of guanosine triphosphate (GTP). The NADH and FADH2 generated by the citric acid cycle are, in turn, used by the oxidative phosphorylation pathway to generate energy-rich ATP.\nOne of the primary sources of acetyl-CoA is from the breakdown of sugars by glycolysis which yield pyruvate that in turn is decarboxylated by the pyruvate dehydrogenase complex generating acetyl-CoA according to the following reaction scheme:\nThe product of this reaction, acetyl-CoA, is the starting point for the citric acid cycle. Acetyl-CoA may also be obtained from the oxidation of fatty acids. Below is a schematic outline of the cycle:\nSteps.\nThere are ten basic steps in the citric acid cycle, as outlined below. The cycle is continuously supplied with new carbon in the form of acetyl-CoA, entering at step 0 in the table.\nTwo carbon atoms are oxidized to CO2, the energy from these reactions is transferred to other metabolic processes through GTP (or ATP), and as electrons in NADH and QH2. The NADH generated in the citric acid cycle may later be oxidized (donate its electrons) to drive ATP synthesis in a type of process called oxidative phosphorylation. FADH2 is covalently attached to succinate dehydrogenase, an enzyme which functions both in the citric acid cycle and the mitochondrial electron transport chain in oxidative phosphorylation. FADH2, therefore, facilitates transfer of electrons to coenzyme Q, which is the final electron acceptor of the reaction catalyzed by the succinate:ubiquinone oxidoreductase complex, also acting as an intermediate in the electron transport chain.\nMitochondria in animals, including humans, possess two succinyl-CoA synthetases: one that produces GTP from GDP, and another that produces ATP from ADP. Plants have the type that produces ATP (ADP-forming succinyl-CoA synthetase). Several of the enzymes in the cycle may be loosely associated in a multienzyme protein complex within the mitochondrial matrix.\nThe GTP that is formed by GDP-forming succinyl-CoA synthetase may be utilized by nucleoside-diphosphate kinase to form ATP (the catalyzed reaction is GTP + ADP \u2192 GDP + ATP).\nProducts.\nProducts of the first turn of the cycle are one GTP (or ATP), three NADH, one FADH2 and two CO2.\nBecause two acetyl-CoA molecules are produced from each glucose molecule, two cycles are required per glucose molecule. Therefore, at the end of two cycles, the products are: two GTP, six NADH, two FADH2, and four CO2.\nThe above reactions are balanced if Pi represents the H2PO4\u2212 ion, ADP and GDP the ADP2\u2212 and GDP2\u2212 ions, respectively, and ATP and GTP the ATP3\u2212 and GTP3\u2212 ions, respectively.\nThe total number of ATP molecules obtained after complete oxidation of one glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is estimated to be between 30 and 38.\nEfficiency.\nThe theoretical maximum yield of ATP through oxidation of one molecule of glucose in glycolysis, citric acid cycle, and oxidative phosphorylation is 38 (assuming 3 molar equivalents of ATP per equivalent NADH and 2 ATP per FADH2). In eukaryotes, two equivalents of NADH and two equivalents of ATP are generated in glycolysis, which takes place in the cytoplasm. If transported using the glycerol phosphate shuttle rather than the malate\u2013aspartate shuttle, transport of two of these equivalents of NADH into the mitochondria effectively consumes two equivalents of ATP, thus reducing the net production of ATP to 36. Furthermore, inefficiencies in oxidative phosphorylation due to leakage of protons across the mitochondrial membrane and slippage of the ATP synthase/proton pump commonly reduces the ATP yield from NADH and FADH2 to less than the theoretical maximum yield. The observed yields are, therefore, closer to ~2.5 ATP per NADH and ~1.5 ATP per FADH2, further reducing the total net production of ATP to approximately 30. An assessment of the total ATP yield with newly revised proton-to-ATP ratios provides an estimate of 29.85 ATP per glucose molecule.\nVariation.\nWhile the citric acid cycle is in general highly conserved, there is significant variability in the enzymes found in different taxa (note that the diagrams on this page are specific to the mammalian pathway variant).\nSome differences exist between eukaryotes and prokaryotes. The conversion of D-\"threo\"-isocitrate to 2-oxoglutarate is catalyzed in eukaryotes by the NAD+-dependent EC 1.1.1.41, while prokaryotes employ the NADP+-dependent EC 1.1.1.42. Similarly, the conversion of (\"S\")-malate to oxaloacetate is catalyzed in eukaryotes by the NAD+-dependent EC 1.1.1.37, while most prokaryotes utilize a quinone-dependent enzyme, EC 1.1.5.4.\nA step with significant variability is the conversion of succinyl-CoA to succinate. Most organisms utilize EC 6.2.1.5, succinate\u2013CoA ligase (ADP-forming) (despite its name, the enzyme operates in the pathway in the direction of ATP formation). In mammals a GTP-forming enzyme, succinate\u2013CoA ligase (GDP-forming) (EC 6.2.1.4) also operates. The level of utilization of each isoform is tissue dependent. In some acetate-producing bacteria, such as \"Acetobacter aceti\", an entirely different enzyme catalyzes this conversion\u00a0\u2013 EC 2.8.3.18, succinyl-CoA:acetate CoA-transferase. This specialized enzyme links the TCA cycle with acetate metabolism in these organisms. Some bacteria, such as \"Helicobacter pylori\", employ yet another enzyme for this conversion\u00a0\u2013 succinyl-CoA:acetoacetate CoA-transferase (EC 2.8.3.5).\nSome variability also exists at the previous step\u00a0\u2013 the conversion of 2-oxoglutarate to succinyl-CoA. While most organisms utilize the ubiquitous NAD+-dependent 2-oxoglutarate dehydrogenase, some bacteria utilize a ferredoxin-dependent 2-oxoglutarate synthase (EC 1.2.7.3).\nOther organisms, including obligately autotrophic and methanotrophic bacteria and archaea, bypass succinyl-CoA entirely, and convert 2-oxoglutarate to succinate via succinate semialdehyde, using EC 4.1.1.71, 2-oxoglutarate decarboxylase, and EC 1.2.1.79, succinate-semialdehyde dehydrogenase.\nIn cancer, there are substantial metabolic derangements that occur to ensure the proliferation of tumor cells, and consequently metabolites can accumulate which serve to facilitate tumorigenesis, dubbed oncometabolites. Among the best characterized oncometabolites is 2-hydroxyglutarate which is produced through a heterozygous gain-of-function mutation (specifically a neomorphic one) in isocitrate dehydrogenase (IDH) (which under normal circumstances catalyzes the oxidation of isocitrate to oxalosuccinate, which then spontaneously decarboxylates to alpha-ketoglutarate, as discussed above; in this case an additional reduction step occurs after the formation of alpha-ketoglutarate via NADPH to yield 2-hydroxyglutarate), and hence IDH is considered an oncogene. Under physiological conditions, 2-hydroxyglutarate is a minor product of several metabolic pathways as an error but readily converted to alpha-ketoglutarate via hydroxyglutarate dehydrogenase enzymes (L2HGDH and D2HGDH) but does not have a known physiologic role in mammalian cells; of note, in cancer, 2-hydroxyglutarate is likely a terminal metabolite as isotope labelling experiments of colorectal cancer cell lines show that its conversion back to alpha-ketoglutarate is too low to measure. In cancer, 2-hydroxyglutarate serves as a competitive inhibitor for a number of enzymes that facilitate reactions via alpha-ketoglutarate in alpha-ketoglutarate-dependent dioxygenases. This mutation results in several important changes to the metabolism of the cell. For one thing, because there is an extra NADPH-catalyzed reduction, this can contribute to depletion of cellular stores of NADPH and also reduce levels of alpha-ketoglutarate available to the cell. In particular, the depletion of NADPH is problematic because NADPH is highly compartmentalized and cannot freely diffuse between the organelles in the cell. It is produced largely via the pentose phosphate pathway in the cytoplasm. The depletion of NADPH results in increased oxidative stress within the cell as it is a required cofactor in the production of GSH, and this oxidative stress can result in DNA damage. There are also changes on the genetic and epigenetic level through the function of histone lysine demethylases (KDMs) and ten-eleven translocation (TET) enzymes; ordinarily TETs hydroxylate 5-methylcytosines to prime them for demethylation. However, in the absence of alpha-ketoglutarate this cannot be done and there is hence hypermethylation of the cell's DNA, serving to promote epithelial-mesenchymal transition (EMT) and inhibit cellular differentiation. A similar phenomenon is observed for the Jumonji C family of KDMs which require a hydroxylation to perform demethylation at the epsilon-amino methyl group. Additionally, the inability of prolyl hydroxylases to catalyze reactions results in stabilization of hypoxia-inducible factor alpha, which is necessary to promote degradation of the latter (as under conditions of low oxygen there will not be adequate substrate for hydroxylation). This results in a pseudohypoxic phenotype in the cancer cell that promotes angiogenesis, metabolic reprogramming, cell growth, and migration.\nRegulation.\nAllosteric regulation by metabolites. The regulation of the citric acid cycle is largely determined by product inhibition and substrate availability. If the cycle were permitted to run unchecked, large amounts of metabolic energy could be wasted in overproduction of reduced coenzyme such as NADH and ATP. The major eventual substrate of the cycle is ADP which gets converted to ATP. A reduced amount of ADP causes accumulation of precursor NADH which in turn can inhibit a number of enzymes. NADH, a product of all dehydrogenases in the citric acid cycle with the exception of succinate dehydrogenase, inhibits pyruvate dehydrogenase, isocitrate dehydrogenase, \u03b1-ketoglutarate dehydrogenase, and also citrate synthase. Acetyl-coA inhibits pyruvate dehydrogenase, while succinyl-CoA inhibits alpha-ketoglutarate dehydrogenase and citrate synthase. When tested in vitro with TCA enzymes, ATP inhibits citrate synthase and \u03b1-ketoglutarate dehydrogenase; however, ATP levels do not change more than 10% in vivo between rest and vigorous exercise. There is no known allosteric mechanism that can account for large changes in reaction rate from an allosteric effector whose concentration changes less than 10%.\nCitrate is used for feedback inhibition, as it inhibits phosphofructokinase, an enzyme involved in glycolysis that catalyses formation of fructose 1,6-bisphosphate, a precursor of pyruvate. This prevents a constant high rate of flux when there is an accumulation of citrate and a decrease in substrate for the enzyme.\nRegulation by calcium. Calcium is also used as a regulator in the citric acid cycle. Calcium levels in the mitochondrial matrix can reach up to the tens of micromolar levels during cellular activation. It activates pyruvate dehydrogenase phosphatase which in turn activates the pyruvate dehydrogenase complex. Calcium also activates isocitrate dehydrogenase and \u03b1-ketoglutarate dehydrogenase. This increases the reaction rate of many of the steps in the cycle, and therefore increases flux throughout the pathway.\nTranscriptional regulation. There is a link between intermediates of the citric acid cycle and the regulation of hypoxia-inducible factors (HIF). HIF plays a role in the regulation of oxygen homeostasis, and is a transcription factor that targets angiogenesis, vascular remodeling, glucose utilization, iron transport and apoptosis. HIF is synthesized constitutively, and hydroxylation of at least one of two critical proline residues mediates their interaction with the von Hippel Lindau E3 ubiquitin ligase complex, which targets them for rapid degradation. This reaction is catalysed by prolyl 4-hydroxylases. Fumarate and succinate have been identified as potent inhibitors of prolyl hydroxylases, thus leading to the stabilisation of HIF.\nMajor metabolic pathways converging on the citric acid cycle.\nSeveral catabolic pathways converge on the citric acid cycle. Most of these reactions add intermediates to the citric acid cycle, and are therefore known as anaplerotic reactions, from the Greek meaning to \"fill up\". These increase the amount of acetyl CoA that the cycle is able to carry, increasing the mitochondrion's capability to carry out respiration if this is otherwise a limiting factor. Processes that remove intermediates from the cycle are termed \"cataplerotic\" reactions.\nIn this section and in the next, the citric acid cycle intermediates are indicated in \"italics\" to distinguish them from other substrates and end-products.\nPyruvate molecules produced by glycolysis are actively transported across the inner mitochondrial membrane, and into the matrix. Here they can be oxidized and combined with coenzyme A to form CO2, \"acetyl-CoA\", and NADH, as in the normal cycle.\nHowever, it is also possible for pyruvate to be carboxylated by pyruvate carboxylase to form \"oxaloacetate\". This latter reaction \"fills up\" the amount of \"oxaloacetate\" in the citric acid cycle, and is therefore an anaplerotic reaction, increasing the cycle's capacity to metabolize \"acetyl-CoA\" when the tissue's energy needs (e.g. in muscle) are suddenly increased by activity.\nIn the citric acid cycle all the intermediates (e.g. \"citrate\", \"iso-citrate\", \"alpha-ketoglutarate\", \"succinate\", \"fumarate\", \"malate\", and \"oxaloacetate\") are regenerated during each turn of the cycle. Adding more of any of these intermediates to the mitochondrion therefore means that that additional amount is retained within the cycle, increasing all the other intermediates as one is converted into the other. Hence the addition of any one of them to the cycle has an anaplerotic effect, and its removal has a cataplerotic effect. These anaplerotic and cataplerotic reactions will, during the course of the cycle, increase or decrease the amount of \"oxaloacetate\" available to combine with \"acetyl-CoA\" to form \"citric acid\". This in turn increases or decreases the rate of ATP production by the mitochondrion, and thus the availability of ATP to the cell.\n\"Acetyl-CoA\", on the other hand, derived from pyruvate oxidation, or from the beta-oxidation of fatty acids, is the only fuel to enter the citric acid cycle. With each turn of the cycle one molecule of \"acetyl-CoA\" is consumed for every molecule of \"oxaloacetate\" present in the mitochondrial matrix, and is never regenerated. It is the oxidation of the acetate portion of \"acetyl-CoA\" that produces CO2 and water, with the energy thus released captured in the form of ATP. The three steps of beta-oxidation resemble the steps that occur in the production of oxaloacetate from succinate in the TCA cycle. Acyl-CoA is oxidized to trans-Enoyl-CoA while FAD is reduced to FADH2, which is similar to the oxidation of succinate to fumarate. Following, trans-enoyl-CoA is hydrated across the double bond to beta-hydroxyacyl-CoA, just like fumarate is hydrated to malate. Lastly, beta-hydroxyacyl-CoA is oxidized to beta-ketoacyl-CoA while NAD+ is reduced to NADH, which follows the same process as the oxidation of malate to oxaloacetate.\nIn the liver, the carboxylation of cytosolic pyruvate into intra-mitochondrial \"oxaloacetate\" is an early step in the gluconeogenic pathway which converts lactate and de-aminated alanine into glucose, under the influence of high levels of glucagon and/or epinephrine in the blood. Here the addition of \"oxaloacetate\" to the mitochondrion does not have a net anaplerotic effect, as another citric acid cycle intermediate (\"malate\") is immediately removed from the mitochondrion to be converted into cytosolic oxaloacetate, which is ultimately converted into glucose, in a process that is almost the reverse of glycolysis.\nIn protein catabolism, proteins are broken down by proteases into their constituent amino acids. Their carbon skeletons (i.e. the de-aminated amino acids) may either enter the citric acid cycle as intermediates (e.g. \"alpha-ketoglutarate\" derived from glutamate or glutamine), having an anaplerotic effect on the cycle, or, in the case of leucine, isoleucine, lysine, phenylalanine, tryptophan, and tyrosine, they are converted into \"acetyl-CoA\" which can be burned to CO2 and water, or used to form ketone bodies, which too can only be burned in tissues other than the liver where they are formed, or excreted via the urine or breath. These latter amino acids are therefore termed \"ketogenic\" amino acids, whereas those that enter the citric acid cycle as intermediates can only be cataplerotically removed by entering the gluconeogenic pathway via \"malate\" which is transported out of the mitochondrion to be converted into cytosolic oxaloacetate and ultimately into glucose. These are the so-called \"glucogenic\" amino acids. De-aminated alanine, cysteine, glycine, serine, and threonine are converted to pyruvate and can consequently either enter the citric acid cycle as \"oxaloacetate\" (an anaplerotic reaction) or as \"acetyl-CoA\" to be disposed of as CO2 and water.\nIn fat catabolism, triglycerides are hydrolyzed to break them into fatty acids and glycerol. In the liver the glycerol can be converted into glucose via dihydroxyacetone phosphate and glyceraldehyde-3-phosphate by way of gluconeogenesis. In skeletal muscle, glycerol is used in glycolysis by converting glycerol into glycerol-3-phosphate, then into dihydroxyacetone phosphate (DHAP), then into glyceraldehyde-3-phosphate.\nIn many tissues, especially heart and skeletal muscle tissue, fatty acids are broken down through a process known as beta oxidation, which results in the production of mitochondrial \"acetyl-CoA\", which can be used in the citric acid cycle. Beta oxidation of fatty acids with an odd number of methylene bridges produces propionyl-CoA, which is then converted into \"succinyl-CoA\" and fed into the citric acid cycle as an anaplerotic intermediate.\nThe total energy gained from the complete breakdown of one (six-carbon) molecule of glucose by glycolysis, the formation of 2 \"acetyl-CoA\" molecules, their catabolism in the citric acid cycle, and oxidative phosphorylation equals about 30 ATP molecules, in eukaryotes. The number of ATP molecules derived from the beta oxidation of a 6 carbon segment of a fatty acid chain, and the subsequent oxidation of the resulting 3 molecules of \"acetyl-CoA\" is 40.\nCitric acid cycle intermediates serve as substrates for biosynthetic processes.\nIn this subheading, as in the previous one, the TCA intermediates are identified by \"italics\".\nSeveral of the citric acid cycle intermediates are used for the synthesis of important compounds, which will have significant cataplerotic effects on the cycle.\n\"Acetyl-CoA\" cannot be transported out of the mitochondrion. To obtain cytosolic acetyl-CoA, \"citrate\" is removed from the citric acid cycle and carried across the inner mitochondrial membrane into the cytosol. There it is cleaved by ATP citrate lyase into acetyl-CoA and oxaloacetate. The oxaloacetate is returned to mitochondrion as \"malate\" (and then converted back into \"oxaloacetate\" to transfer more \"acetyl-CoA\" out of the mitochondrion). The cytosolic acetyl-CoA is used for fatty acid synthesis and the production of cholesterol. Cholesterol can, in turn, be used to synthesize the steroid hormones, bile salts, and vitamin D.\nThe carbon skeletons of many non-essential amino acids are made from citric acid cycle intermediates. To turn them into amino acids the alpha keto-acids formed from the citric acid cycle intermediates have to acquire their amino groups from glutamate in a transamination reaction, in which pyridoxal phosphate is a cofactor. In this reaction the glutamate is converted into \"alpha-ketoglutarate\", which is a citric acid cycle intermediate. The intermediates that can provide the carbon skeletons for amino acid synthesis are \"oxaloacetate\" which forms aspartate and asparagine; and \"alpha-ketoglutarate\" which forms glutamine, proline, and arginine.\nOf these amino acids, aspartate and glutamine are used, together with carbon and nitrogen atoms from other sources, to form the purines that are used as the bases in DNA and RNA, as well as in ATP, AMP, GTP, NAD, FAD and CoA.\nThe pyrimidines are partly assembled from aspartate (derived from \"oxaloacetate\"). The pyrimidines, thymine, cytosine and uracil, form the complementary bases to the purine bases in DNA and RNA, and are also components of CTP, UMP, UDP and UTP.\nThe majority of the carbon atoms in the porphyrins come from the citric acid cycle intermediate, \"succinyl-CoA\". These molecules are an important component of the hemoproteins, such as hemoglobin, myoglobin and various cytochromes.\nDuring gluconeogenesis mitochondrial \"oxaloacetate\" is reduced to \"malate\" which is then transported out of the mitochondrion, to be oxidized back to oxaloacetate in the cytosol. Cytosolic oxaloacetate is then decarboxylated to phosphoenolpyruvate by phosphoenolpyruvate carboxykinase, which is the rate limiting step in the conversion of nearly all the gluconeogenic precursors (such as the glucogenic amino acids and lactate) into glucose by the liver and kidney.\nBecause the citric acid cycle is involved in both catabolic and anabolic processes, it is known as an amphibolic pathway.\nEvan M.W.Duo\nGlucose feeds the TCA cycle via circulating lactate.\nThe metabolic role of lactate is well recognized as a fuel for tissues, mitochondrial cytopathies such as DPH Cytopathy, and the scientific field of oncology (tumors). In the classical Cori cycle, muscles produce lactate which is then taken up by the liver for gluconeogenesis. New studies suggest that lactate can be used as a source of carbon for the TCA cycle.\nEvolution.\nIt is believed that components of the citric acid cycle were derived from anaerobic bacteria, and that the TCA cycle itself may have evolved more than once. It may even predate biosis: the substrates appear to undergo most of the reactions spontaneously in the presence of persulfate radicals. Theoretically, several alternatives to the TCA cycle exist; however, the TCA cycle appears to be the most efficient. If several TCA alternatives had evolved independently, they all appear to have converged to the TCA cycle."}
{"id": "6821", "revid": "1266603195", "url": "https://en.wikipedia.org/wiki?curid=6821", "title": "Military engineering vehicle", "text": "A military engineering vehicle is a vehicle built for construction work or for the transportation of combat engineers on the battlefield. These vehicles may be modified civilian equipment (such as the armoured bulldozers that many nations field) or purpose-built military vehicles (such as the AVRE). The first appearance of such vehicles coincided with the appearance of the first tanks, these vehicles were modified Mark V tanks for bridging and mine clearance. Modern \"military engineering vehicles\" are expected to fulfill numerous roles such as; bulldozer, crane, grader, excavator, dump truck, breaching vehicle, bridging vehicle, military ferry, amphibious crossing vehicle, and combat engineer section carrier.\nHistory.\nWorld War One.\nA Heavy RE tank was developed shortly after World War I by Major Giffard LeQuesne Martel RE. This vehicle was a modified Mark V tank. Two support functions for these Engineer Tanks were developed: bridging and mine clearance. The bridging component involved an assault bridge, designed by Major Charles Inglis RE, called the Canal Lock Bridge, which had sufficient length to span a canal lock. Major Martel mated the bridge with the tank and used hydraulic power generated by the tank's engine to maneuver the bridge into place. For mine clearance the tanks were equipped with 2 ton rollers.\n1918-1939.\nBetween the wars various experimental bridging tanks were used to test a series of methods for bridging obstacles and developed by the Experimental Bridging Establishment (EBE). Captain SG Galpin RE conceived a prototype Light Tank Mk V to test the Scissors Assault Bridge. This concept was realised by Captain SA Stewart RE with significant input from a Mr DM Delany, a scientific civil servant in the employ of the EBE. MB Wild &amp; Co, Birmingham, also developed a bridge that could span gaps of 26 feet using a complex system of steel wire ropes and a traveling jib, where the front section was projected and then attached to the rear section prior to launching the bridge. This system had to be abandoned due to lack of success in getting it to work, however the idea was later used successfully on the Beaver Bridge Laying Tank.\nEarly World War Two.\nOnce World War Two had begun, the development of armoured vehicles for use by engineers in the field was accelerated under Delaney's direction. The EBE rapidly developed an assault bridge carried on a modified Covenanter tank capable of deploying a 24-ton tracked load capacity bridge (Class 24) that could span gaps of 30 feet. However, it did not see service in the British armed forces, and all vehicles were passed onto Allied forces such as Australia and Czechoslovakia.\nA Class 30 design superseded the Class 24 with no real re-design, simply the substitution of the Covenanter tank with a suitably modified Valentine.\nAs tanks in the war got heavier, a new bridge capable of supporting them was developed. A heavily modified Churchill used a single-piece bridge mounted on a turret-less tank and was able to lay the bridge in 90 seconds; this bridge was able to carry a 60-ton tracked or 40-ton wheeled load.\nLate World War 2: Hobart's 'Funnies' and D-Day.\nHobart's Funnies were a number of unusually modified tanks operated during the Second World War by the 79th Armoured Division of the British Army or by specialists from the Royal Engineers. They were designed in light of problems that more standard tanks experienced during the amphibious Dieppe Raid, so that the new models would be able to overcome the problems of the planned Invasion of Normandy. These tanks played a major part on the Commonwealth beaches during the landings. They were forerunners of the modern combat engineering vehicle and were named after their commander, Major General Percy Hobart.\nHobart's unusual, specialized tanks, nicknamed \"funnies\", included:\nIn U.S. Forces, Sherman tanks were also fitted with dozer blades, and anti-mine roller devices were developed, enabling engineering operations and providing similar capabilities.\nPost war.\nPost war, the value of the combat engineering vehicles had been proven, and armoured multi-role engineering vehicles have been added to the majority of armoured forces.\nTypes.\nCivilian and militarized heavy equipment.\nMilitary engineering can employ a wide variety of heavy equipment in the same ways to how this equipment is used outside the military. Bulldozers, cranes, graders, excavators, dump trucks, loaders, and backhoes all see extensive use by military engineers.\nMilitary engineers may also use civilian heavy equipment which was modified for military applications. Typically, this involves adding armour for protection from battlefield hazards such as artillery, unexploded ordnance, mines, and small arms fire. Often this protection is provided by armour plates and steel jackets. Some examples of armoured civilian heavy equipment are the IDF Caterpillar D9, American D7 TPK, Canadian D6 armoured bulldozer, cranes, graders, excavators, and M35 2-1/2 ton cargo truck.\nMilitarized heavy equipment may also take on the form of traditional civilian equipment designed and built to unique military specifications. These vehicles typically sacrifice some depth of capability from civilian models in order to gain greater speed and independence from prime movers. Examples of this type of vehicle include high speed backhoes such as the Australian Army's High Mobility Engineering Vehicle (HMEV) from Thales or the Canadian Army's Multi-Purpose Engineer Vehicle (MPEV) from Arva.\n\"The main article for civilian heavy equipment is:\" Heavy equipment (construction)\nArmoured engineering vehicle.\nTypically based on the platform of a main battle tank, these vehicles go by different names depending upon the country of use or manufacture. In the US the term \"combat engineer vehicle (CEV)\" is used, in the UK the terms \"Armoured Vehicle Royal Engineers (AVRE)\" or Armoured Repair and Recovery Vehicle (ARRV) are used, while in Canada and other commonwealth nations the term \"armoured engineer vehicle (AEV)\" is used. There is no set template for what such a vehicle will look like, yet likely features include a large dozer blade or mine ploughs, a large caliber demolition cannon, augers, winches, excavator arms and cranes or lifting booms.\nThese vehicles are designed to directly conduct obstacle breaching operations and to conduct other earth-moving and engineering work on the battlefield. Good examples of this type of vehicle include the UK Trojan AVRE, the Russian IMR, and the US M728 Combat Engineer Vehicle. Although the term \"armoured engineer vehicle\" is used specifically to describe these multi-purpose tank based engineering vehicles, that term is also used more generically in British and Commonwealth militaries to describe all heavy tank based engineering vehicles used in the support of mechanized forces. Thus, \"armoured engineer vehicle\" used generically would refer to AEV, AVLB, Assault Breachers, and so on.\nArmoured earth mover.\nLighter and less multi-functional than the CEVs or AEVs described above, these vehicles are designed to conduct earth-moving work on the battlefield and generally be anti-tank explosive proof. These vehicles have greater high speed mobility than traditional heavy equipment and are protected against the effects of blast and fragmentation. Good examples are the American M9 ACE and the UK FV180 Combat Engineer Tractor.\nBreaching vehicle.\nThese vehicles are equipped with mechanical or other means for the breaching of man-made obstacles. Common types of breaching vehicles include mechanical flails, mine plough vehicles, and mine roller vehicles. In some cases, these vehicles will also mount mine-clearing line charges. Breaching vehicles may be either converted armoured fighting vehicles or purpose built vehicles. In larger militaries, converted AFV are likely to be used as \"assault breachers\" while the breached obstacle is still covered by enemy observation and fire, and then purpose built breaching vehicles will create additional lanes for following forces.\nGood examples of breaching vehicles include the US M1150 assault breacher vehicle, the UK Aardvark JSFU, and the Singaporean Trailblazer.\nBridging vehicles.\nSeveral types of military bridging vehicles have been developed. An armoured vehicle-launched bridge (AVLB) is typically a modified tank hull converted to carry a bridge into battle in order to support crossing ditches, small waterways, or other gap obstacles.\nAnother type of bridging vehicle is the truck launched bridge. The Soviet TMM bridging truck could carry and launch a 10-meter bridge that could be daisy-chained with other TMM bridges to cross larger obstacles. More recent developments have seen the conversion of AVLB and truck launched bridge with launching systems that can be mounted on either tank or truck for bridges that are capable of supporting heavy main battle tanks.\nEarlier examples of bridging vehicles include a type in which a converted tank hull is the bridge. On these vehicles, the hull deck comprises the main portion of the tread way while ramps extend from the front and rear of the vehicle to allow other vehicles to climb over the bridging vehicle and cross obstacles. An example of this type of armoured bridging vehicle was the Churchill Ark used in the Second World War.\nCombat engineer section carriers.\nAnother type of CELLs are armoured fighting vehicles which are used to transport sappers (combat engineers) and can be fitted with a bulldozer's blade and other mine-breaching devices. They are often used as APCs because of their carrying ability and heavy protection. They are usually armed with machine guns and grenade launchers and usually tracked to provide enough tractive force to push blades and rakes. Some examples are the U.S. M113 APC, IDF Puma, Nagmachon, Husky, and U.S. M1132 ESV (a Stryker variant).\nMilitary ferries and amphibious crossing vehicles.\nOne of the major tasks of military engineering is crossing major rivers. Several military engineering vehicles have been developed in various nations to achieve this task. One of the more common types is the amphibious ferry such as the M3 Amphibious Rig. These vehicles are self-propelled on land, they can transform into raft type ferries when in the water, and often multiple vehicles can connect to form larger rafts or floating bridges. Other types of military ferries, such as the Soviet \"Plavayushij Transportyor - Srednyj\", are able to load while still on land and transport other vehicles cross country and over water.\nIn addition to amphibious crossing vehicles, military engineers may also employ several types of boats. Military assault boats are small boats propelled by oars or an outboard motor and used to ferry dismounted infantry across water.\nTank-based combat engineering vehicles.\nMost CEVs are armoured fighting vehicles that may be based on a tank chassis and have special attachments in order to breach obstacles. Such attachments may include dozer blades, mine rollers, cranes etc. An example of an engineering vehicle of this kind is a bridgelaying tank, which replaces the turret with a segmented hydraulic bridge. The Hobart's Funnies of the Second World War were a wide variety of armoured vehicles for combat engineering tasks. They were allocated to the initial beachhead assaults by the British and Commonwealth forces in the D-Day landings.\nChurchill tank.\nThe British Churchill tank because of its good cross-country performance and capacious interior with side hatches became the most adapted with modifications, the base unit being the AVRE carrying a large demolition gun."}
{"id": "6822", "revid": "10412272", "url": "https://en.wikipedia.org/wiki?curid=6822", "title": "Catalonia", "text": "Catalonia is an autonomous community of Spain, designated as a \"nationality\" by its Statute of Autonomy. Most of its territory (except the Val d'Aran) is situated on the northeast of the Iberian Peninsula, to the south of the Pyrenees mountain range. Catalonia is administratively divided into four provinces or eight \"vegueries\" (regions), which are in turn divided into 43 \"comarques\". The capital and largest city, Barcelona, is the second-most populous municipality in Spain and the fifth-most populous urban area in the European Union.\nModern-day Catalonia comprises most of the medieval and early modern Principality of Catalonia, with the remainder northern area now part of France's Pyr\u00e9n\u00e9es-Orientales. It is bordered by France (Occitanie) and Andorra to the north, the Mediterranean Sea to the east, and the Spanish autonomous communities of Aragon to the west and Valencia to the south. In addition to about 580\u00a0km of coastline, Catalonia also has major high landforms such as the Pyrenees and the Pre-Pyrenees, the Transversal Range (Serralada Transversal) or the Central Depression. The official languages are Catalan, Spanish and the Aranese dialect of Occitan.\nIn the 10th century, the County of Barcelona and the other neighboring counties became independent from West Francia. In 1137, Barcelona and the Kingdom of Aragon were united by marriage, resulting in a composite monarchy, the Crown of Aragon. Within the Crown, the Catalan counties merged in to a state, the Principality of Catalonia, with its own distinct institutional system, such as Courts, Generalitat and constitutions, being the base and promoter for the Crown's Mediterranean trade and expansionism. In the later Middle Ages, Catalan literature flourished. In 1516, Charles V became monarch of both the crowns of Aragon and Castile, retaining their previous distinct institutions and legislation. Growing tensions led to the revolt of the Principality of Catalonia (1640\u20131652), briefly becoming a republic under French protection. By the Treaty of the Pyrenees (1659), the northern parts of Catalonia were ceded to France. During the War of the Spanish Succession (1701\u20131714), the states of the Crown of Aragon sided against the Bourbon Philip V of Spain, but following Catalan capitulation on 11 September 1714 he imposed a unifying administration across Spain, enacting the Nueva Planta decrees which ended Catalonia's separate status, supressing its institutions and legal system. Catalan as a language of government and literature was eclipsed by Spanish.\nIn the 19th century, Napoleonic and Carlist Wars affected Catalonia. In the second third of the century, it experienced industrialisation, while saw a cultural renaissance coupled with incipient nationalism and several workers' movements. The Second Spanish Republic (1931\u20131939) granted self-governance to Catalonia, being restored the Generalitat as its government. After the Spanish Civil War (1936-1939), the Francoist dictatorship enacted repressive measures, abolishing self-government and banning again the official use of the Catalan language. After a harsh autarky, from the late 1950s Catalonia saw rapid economic growth, drawing many workers from across Spain and making it one of Europe's largest industrial and touristic areas. During the Spanish transition to democracy (1975\u20131982), the Generalitat and Catalonia's self-government were reestablished, remaining one of the most economically dynamic communities in Spain.\nIn the 2010s, there was growing support for Catalan independence. On 27 October 2017, the Catalan Parliament unilaterally declared independence following a referendum that was deemed unconstitutional by the Spanish state. The Spanish Senate voted in favour of enforcing direct rule by removing the Catalan government and calling a snap regional election. The Spanish Supreme Court imprisoned seven former ministers of the Catalan government on charges of rebellion and misuse of public funds, while several others\u2014including then-President Carles Puigdemont\u2014fled to other European countries. Those in prison were pardoned by the Spanish government in 2021.\nEtymology and pronunciation.\nThe name \"Catalonia\" (), spelled \"Cathalonia\", began to be used for the homeland of the Catalans (\"Cathalanenses\") in the late 11th century and was probably used before as a territorial reference to the group of counties that comprised part of the March of Gothia and the March of Hispania under the control of the Count of Barcelona and his relatives. The origin of the name \"Catalunya\" is subject to diverse interpretations because of a lack of evidence.\nOne theory suggests that \"Catalunya\" derives from the name \"Gothia\" (or \"Gauthia\") \"Launia\" (\"Land of the Goths\"), since the origins of the Catalan counts, lords and people were found in the March of Gothia, known as \"Gothia\", whence \"Gothland\" &gt; &gt; &gt; &gt; \"Catalonia\" theoretically derived. During the Middle Ages, Byzantine chroniclers claimed that \"Catalania\" derives from the local medley of Goths with Alans, initially constituting a \"Goth-Alania\".\nOther theories suggest:\nIn English, \"Catalonia\" is pronounced . The native name, \"Catalunya\", is pronounced in Central Catalan, the most widely spoken variety, and in North-Western Catalan. The Spanish name is \"Catalu\u00f1a\" (), and the Aranese name is \"Catalonha\" ().\nHistory.\nPrehistory.\nThe first known human settlements in what is now Catalonia were at the beginning of the Middle Paleolithic. The oldest known trace of human occupation is a mandible found in Banyoles, described as pre-Neanderthal, that is, some 200,000 years old; other sources suggest it to be only about one third that old. From the Epipalaeolithic or Mesolithic, important remains dated between 8000 and 5000BC, such as those of Sant Gregori (Falset) and el Filador (Margalef de Montsant). The most important sites from these eras, all excavated in the region of Moian\u00e8s, are the Balma del Gai (Epipaleolithic) and the Balma de l'Espluga. The Neolithic era began in Catalonia around 5000BC, although the population was slower to develop fixed settlements thanks to the abundance of woods, which allowed the continuation of a fundamentally hunter-gatherer culture. An example of such settlements would be La Draga at Banyoles, an \"early Neolithic village which dates from the end of the 6th millenniumBC.\"\nThe Bronze Age occurred between 1800 and 700BC. There were some known settlements in the low Segre zone. The Bronze Age coincided with the arrival of the Indo-Europeans through the Urnfield Culture, whose successive waves of migration began around 1200BC, and they were responsible for the creation of the first proto-urban settlements. Around the middle of the 7th centuryBC, the Iron Age arrived in Catalonia.\nPre-Roman and Roman period.\nIn pre-Roman times, the area that is now Catalonia was populated by the Iberians. The Iberians tribes \u2013 the Ilergetes, Indigetes and Lacetani (Cerretains) \u2013 also maintained relations with the peoples of the Mediterranean. Some urban agglomerations became relevant, including Ilerda (Lleida) inland, Hibera (perhaps Amposta or Tortosa) or Indika (Ullastret). Coastal trading colonies were established by the ancient Greeks, who settled around the Gulf of Roses, in Emporion (Emp\u00faries) and Roses in the 8th century BC.\nAfter the Carthaginian defeat by the Roman Republic, the north-east of Iberia became the first to come under Roman rule and became part of Hispania, the westernmost part of the Roman Empire. Tarraco (modern Tarragona) was one of the most important Roman cities in Hispania and the capital of the province of Tarraconensis. Other important cities of the Roman period are Ilerda (Lleida), Dertosa (Tortosa), Gerunda (Girona) as well as the ports of Empuri\u00e6 (former Emporion) and Barcino (Barcelona). As for the rest of Hispania, Latin law was granted to all cities under the reign of Vespasian (69\u201379AD), while Roman citizenship was granted to all free men of the empire by the Edict of Caracalla in 212AD (Tarraco, the capital, was already a colony of Roman law since 45BC). It was a rich agricultural province (olive oil, wine, wheat), and the first centuries of the Empire saw the construction of roads (the most important being the Via Augusta, parallel to Mediterranean coastline) and infrastructure like aqueducts.\nConversion to Christianity, attested in the 3rdcentury, was completed in urban areas in the 4thcentury. Although Hispania remained under Roman rule and did not fall under the rule of Vandals, Suebi and Alans in the 5thcentury, the main cities suffered frequent sacking and some deurbanization.\nMiddle Ages.\nAfter the fall of the Western Roman Empire, the area was conquered by the Visigoths and was ruled as part of the Visigothic Kingdom for almost two and a half centuries. In 718, it came under Muslim control and became part of Al-Andalus, a province of the Umayyad Caliphate. From the conquest of Roussillon in 760, to the conquest of Barcelona in 801, the Frankish empire took control of the area between Septimania and the Llobregat river from the Muslims and created heavily militarised, self-governing counties. These counties formed part of the historiographically known as the Gothic and Hispanic Marches, a buffer zone in the south of the Frankish Empire in the northeast of the Iberian Peninsula, to act as a defensive barrier against further invasions from Al-Andalus. These counties came under the rule of the counts of Barcelona, who were Frankish vassals nominated by the emperor of the Franks, to whom they were feudatories (801\u2013988). At the end of the 9thcentury, the Count of Barcelona Wilfred the Hairy (878\u2013897) made his titles hereditaries and thus founded the dynasty of the House of Barcelona, which reigned in Catalonia until 1410.\nIn 988 Borrell II, Count of Barcelona, did not recognise the new French king Hugh Capet as his king, evidencing the loss of dependency from Frankish rule and confirming his successors (from Ramon Borrell I onwards) as independent of the Capetian crown. At the beginning of eleventh century the Catalan counties experienced an important process of feudalisation, however, the efforts of church's sponsored Peace and Truce Assemblies and the intervention of Ramon Berenguer I, count of Barcelona (1035\u20131076) in the negotiations with the rebel nobility resulted in the partial restoration of the comital authority under the new feudal order. To fulfill that purpose, Ramon Berenguer began the modification of the legislation in the written Usages of Barcelona, being one of the first European compilations of feudal law. The earliest known use of the name \"Catalonia\" for these counties dates to 1117.\nIn 1137, Ramon Berenguer IV, Count of Barcelona decided to accept King Ramiro II of Aragon's proposal to receive the Kingdom of Aragon and to marry his daughter Petronila, establishing the dynastic union of the County of Barcelona with Aragon, creating a composite monarchy later known as the Crown of Aragon and making the Catalan counties that were vassalized or merged with the County of Barcelona into a principality of the Aragonese Crown. During the reign of his son Alphons, in 1173, Catalonia was regarded as a legal entity for the first time, while the Usages of Barcelona were compiled in the process to turn them into the law and custom of Catalonia (\"Consuetudinem Cathalonie\"), being considered one of the \"milestones of Catalan political identity\". In 1258, by means of the Treaty of Corbeil James I of Aragon renounced his family rights and dominions in Occitania, while the king of France, Louis IX, formally relinquished to any historical claim of feudal lordship he might have over the Catalan counties. This treaty confirmed, from French point of view, the independence of the Catalan counties already established the previous three centuries.\nAs a coastal land, Catalonia became the base of the Aragonese Crown's maritime forces, which spread the power of the Crown in the Mediterranean, turning Barcelona into a powerful and wealthy city. In the period of 1164\u20131410, new territories, the Kingdom of Valencia, the Kingdom of Majorca, the Kingdom of Sardinia, the Kingdom of Sicily, and, briefly, the Duchies of Athens and Neopatras, were incorporated into the dynastic domains of the House of Aragon. The expansion was accompanied by a great development of the Catalan trade, creating an extensive trade network across the Mediterranean which competed with those of the maritime republics of Genoa and Venice.\nAt the same time, the Principality of Catalonia developed a complex institutional and political system based in the concept of a pact between the estates of the realm and the king. The legislation had to be passed by the Catalan Courts (\"Corts Catalanes\"), one of the first parliamentary bodies of Europe that, after 1283, officially obtained the power to pass legislation with the monarch. The Courts were composed of the three estates organized into \"arms\" (\"bra\u00e7os\"), were presided over by the monarch, and approved the Catalan constitutions, which established a compilation of rights for the inhabitants of the Principality. In order to collect general taxes, the Catalan Courts of 1359 established a permanent representative body, known as the Generalitat, which gained considerable political power over the next centuries.\nThe domains of the Aragonese Crown were severely affected by the Black Death pandemic and by later outbreaks of the plague. Between 1347 and 1497 Catalonia lost 37percent of its population. In 1410, the last reigning monarch of the House of Barcelona, King Martin I died without surviving descendants. Under the Compromise of Caspe (1412), the representatives of the kingdoms of Aragon, Valencia and the Principality of Catalonia appointed Ferdinand from the Castilian House of Trast\u00e1mara as King of the Crown of Aragon. During the reign of his son, John II, the persistent economic crisis and social and political tensions in the Principality led to the Catalan Civil War (1462\u20131472) and the War of the Remences (1462\u20131486) that left Catalonia exhausted. The Sentencia Arbitral de Guadalupe (1486) liberated the remen\u00e7a peasants from the feudal evil customs.\nIn the later Middle Ages, Catalan literature flourished in Catalonia proper and in the kingdoms of Majorca and Valencia, with such remarkable authors as the philosopher Ramon Llull, the Valencian poet Ausi\u00e0s March, and Joanot Martorell, author of the novel \"Tirant lo Blanch\", published in 1490.\nModern era.\nFerdinand II of Aragon, the grandson of Ferdinand I, and Queen Isabella I of Castile were married in 1469, later taking the title the Catholic Monarchs; subsequently, this event was seen by historiographers as the dawn of a unified Spain. At this time, though united by marriage, the Crowns of Castile and Aragon maintained distinct territories, each keeping its own traditional institutions, parliaments, laws and currency. Castile commissioned expeditions to the Americas and benefited from the riches acquired in the Spanish colonisation of the Americas, but, in time, also carried the main burden of military expenses of the united Spanish kingdoms. After Isabella's death, Ferdinand II personally ruled both crowns. By virtue of descent from his maternal grandparents, Ferdinand and Isabella, in 1516 Charles I of Spain became the first king to rule the Crowns of Castile and Aragon simultaneously by his own right. Following the death of his paternal (House of Habsburg) grandfather, Maximilian I, Holy Roman Emperor, he was also elected Charles V, Holy Roman Emperor, in 1519.\nOver the next few centuries, the Principality of Catalonia was generally on the losing side of a series of wars that led steadily to an increased centralization of power in Spain. However, between the 16th and 18th centuries, the participation of the political community in the local and the general Catalan government grew (thus consolidating its constitutional system), while the kings remained absent, represented by a viceroy. Tensions between Catalan institutions and the monarchy began to arise. The large and burdensome presence of the Spanish royal army in the Principality due to the Franco-Spanish War led to an uprising of peasants, provoking the Reapers' War (1640\u20131652), which saw Catalonia rebel (briefly as a republic led by the president of the Generalitat, Pau Claris) with French help against the Spanish Crown for overstepping Catalonia's rights during the Thirty Years' War. Within a brief period France took full control of Catalonia. Most of Catalonia was reconquered by the Spanish monarchy but Catalan rights were mostly recognised. Roussillon and half of Cerdanya was lost to France by the Treaty of the Pyrenees (1659).\nThe most significant conflict concerning the governing monarchy was the War of the Spanish Succession (1701\u20131715), which began when the childless Charles II of Spain, the last Spanish Habsburg, died without an heir in 1700. Charles II had chosen Philip V of Spain from the French House of Bourbon. Catalonia, like other territories that formed the Crown of Aragon, rose up in support of the Austrian Habsburg pretender Charles VI, Holy Roman Emperor, in his claim for the Spanish throne as Charles III of Spain. The fight between the houses of Bourbon and Habsburg for the Spanish Crown split Spain and Europe.\nThe fall of Barcelona on 11 September 1714 to the Bourbon king Philip V militarily ended the Habsburg claim to the Spanish Crown, which became legal fact in the Treaty of Utrecht (1713). Philip felt that he had been betrayed by the Catalan Courts, as it had initially sworn its loyalty to him when he had presided over it in 1701. In retaliation for the betrayal, and inspired by the French model, the first Bourbon king enacted the Nueva Planta decrees (1707, 1715 and 1716), incorporating the realms of the Crown of Aragon, including the Principality of Catalonia in 1716, as provinces of the Crown of Castile, terminating their status as separate states along with their parliaments, institutions and public laws, as well as their politics, within a French-style centralized and absolutist kingdom of Spain. After the War of the Spanish Succession, the assimilation of the Crown of Aragon in the Castilian Crown through the Nueva Planta Decrees was the first step in the creation of the Spanish nation state. These nationalist policies, sometimes aggressive, and still in force, have been and are the seed of repeated territorial conflicts within the state. In the second half of the 17th century and the 18th century (excluding the parentesis of the Succession War and the post-war inestability) Catalonia carried out a successful process of economic growth and proto-industrialization, reinforced in the late quarter of the century when Castile's trade monopoly with American colonies ended.\nLate modern history.\nAt the beginning of the nineteenth century, Catalonia was severely affected by the Napoleonic Wars. In 1808, it was occupied by French troops; the resistance against the occupation eventually developed into the Peninsular War. The rejection of French dominion was institutionalized with the creation of \"juntas\" (councils) who, remaining loyal to the Bourbons, exercised the sovereignty and representation of the territory due to the disappearance of the old institutions. In 1810, Napoleon took direct control of Catalonia, creating the Government of Catalonia under the rule of Marshall Augereau, and making Catalan briefly an official language again. Between 1812 and 1814, Catalonia was annexed to France. The French troops evacuated Catalan territory at the end of 1814. After the Bourbon restoration in Spain and the death of the absolutist king Ferdinand VII (1833), Carlist Wars erupted against the newly established liberal state of Isabella II. Catalonia was divided, with the coastal and most industrialized areas supporting liberalism, while most of the countryside were in the hands of the Carlist faction; the latter proposed to reestablish the institutional systems suppressed by the Nueva Planta decrees in the ancient realms of the Crown of Aragon. The consolidation of the liberal state saw a new provincial division of Spain, including Catalonia, which was divided into four provinces (Barcelona, Girona, Lleida and Tarragona).\nIn the second third of the 19thcentury, Catalonia became an important industrial center, particularly focused on textiles. This process was a consequence of the conditions of proto-industrialisation of textile production in the prior two centuries, growing capital from wine and brandy export,\nand was later boosted by the government support for domestic manufacturing. In 1832, the Bonaplata Factory in Barcelona became the first factory in the country to make use of the steam engine.\nThe first railway on the Iberian Peninsula was built between Barcelona and Matar\u00f3 in 1848. A policy to encourage company towns also saw the textile industry flourish in the countryside in the 1860s and 1870s. Although the policy of Spanish governments oscillated between free trade and protectionism, become more common. To this day Catalonia remains one of the most industrialised areas of Spain. In the same period, Barcelona was the focus of industrial conflict and revolutionary uprisings known as \"bullangues\". In Catalonia, a republican current began to develop among the progressives, attrackting many Catalans who favored the federalisation of Spain. Meanwhile, the Catalan language saw a Romantic cultural renaissance from the second third of the century onwards, the \"Renaixen\u00e7a\", among both the working class and the bourgeoisie. Right after the fall of the First Spanish Republic (1873\u20131874) and the subsequent restoration of the Bourbon dynasty (1874), Catalan nationalism began to be organized politically under the leadership of the republican federalist Valent\u00ed Almirall.\nThe anarchist movement had been active throughout the last quarter of the 19th century and the early 20th century, founding the CNT trade union in 1910 and achieving one of the first eight-hour workdays in Europe in 1919. Growing resentment of conscription and of the military culminated in the Tragic Week (Catalan: \"Setmana Tr\u00e0gica\") in Barcelona in 1909. Under the hegemony of the Regionalist League, Catalonia gained a degree of administrative unity for the first time in the Modern era. In 1914, the four Catalan provinces were authorized to create a commonwealth (Catalan: \"Mancomunitat\"), lacking legislative power or political autonomy, which carried out an ambitious program of modernization, but it was disbanded in 1925 by the dictatorship of Primo de Rivera (1923\u20131930). During the final stage of the Dictatorship, with Spain beginning to suffer an economic crisis, Barcelona hosted the 1929 International Exposition.\nAfter the fall of the dictatorship and a brief proclamation of the Catalan Republic, during the events of the proclamation of the Second Spanish Republic (14\u201317April1931), Catalonia received, in 1932, its first Statute of Autonomy from the Spanish Republic's Parliament, granting it a considerable degree of self-governance, establishing an autonomous body, the Generalitat of Catalonia, which included a parliament. The left-wing pro-independence leader Francesc Maci\u00e0 was appointed its first president. Under the Statute, Catalan became an official language. The governments of the Republican Generalitat, led by the Republican Left of Catalonia (ERC) leaders Francesc Maci\u00e0 (1931\u20131933) and Llu\u00eds Companys (1933\u20131940), sought to implement a modernizing and progressive social agenda, despite the internal difficulties. This period was marked by political unrest, the effects of the economic crisis and their social repercussions. The Statute of Autonomy was suspended in 1934, due to the Events of 6 October in Barcelona, after the accession of right-wing Spanish nationalist party CEDA to the government of the Republic, considered close to fascism. After the electoral victory of the left wing Popular Front in February 1936, the Government of Catalonia was pardoned and the self-government was restored.\nSpanish Civil War (1936\u20131939) and Franco's rule (1939\u20131975).\nThe defeat of the military rebellion against the Republican government in Barcelona placed Catalonia firmly in the Republican side of the Spanish Civil War. During the war, there were two rival powers in Catalonia: the de jure power of the Generalitat and the de facto power of the armed popular militias. Violent confrontations between the workers' parties (CNT-FAI and POUM against the PSUC) culminated in the defeat of the first ones in 1937. The situation resolved itself progressively in favor of the Generalitat, but at the same time the Generalitat lost most of its autonomous powers within Republican Spain. In 1938 Franco's troops broke the Republican territory in two, isolating Catalonia from the rest of the Republican territory. The defeat of the Republican army in the Battle of the Ebro led in 1938 and 1939 to the occupation of Catalonia by Franco's forces.\nThe defeat of the Spanish Republic in the Spanish Civil War brought to power the dictatorship of Francisco Franco, whose first ten-year rule was particularly violent, autocratic, and repressive both in a political, cultural, social, and economical sense. In Catalonia, any kind of public activities associated with Catalan nationalism, republicanism, anarchism, socialism, liberalism, democracy or communism, including the publication of books on those subjects or simply discussion of them in open meetings, was banned. Franco's regime banned the use of Catalan in government-run institutions and during public events, and the Catalan institutions of self-government were abolished. The president of Catalonia, Llu\u00eds Companys, was taken to Spain from his exile in the German-occupied France and was tortured and executed in the Montju\u00efc Castle of Barcelona for the crime of 'military rebellion'.\nDuring later stages of Francoist Spain, certain folkloric and religious celebrations in Catalan resumed and were tolerated. Use of Catalan in the mass media had been forbidden but was permitted from the early 1950s in the theatre. Despite the ban during the first years and the difficulties of the next period, publishing in Catalan continued throughout his rule.\nThe years after the war were extremely hard. Catalonia, like many other parts of Spain, had been devastated by the war. Recovery from the war damage was slow and made more difficult by the international trade embargo and the autarkic politics of Franco's regime. By the late 1950s, the region had recovered its pre-war economic levels and in the 1960s was the second-fastest growing economy in the world in what became known as the Spanish miracle. During this period there was a spectacular growth of industry and tourism in Catalonia that drew large numbers of workers to the region from across Spain and made the area around Barcelona one of Europe's largest industrial metropolitan areas.\nTransition and democratic period (1975\u2013\"present\").\nAfter Franco's death in 1975, Catalonia voted for the adoption of a democratic Spanish Constitution in 1978, in which Catalonia recovered political and cultural autonomy, restoring the Generalitat (exiled since the end of the Civil War in 1939) in 1977 and adopting a new Statute of Autonomy in 1979, which defined Catalonia as a \"nationality\". The first elections to the Parliament of Catalonia under this Statute gave the Catalan presidency to Jordi Pujol, leader of Converg\u00e8ncia i Uni\u00f3 (CiU), a center-right Catalan nationalist electoral coalition, with Pujol re-elected until 2003. Throughout the 1980s and 1990s, the institutions of Catalan autonomy were deployed, among them an autonomous police force, the Mossos d'Esquadra, in 1983, and the broadcasting network Televisi\u00f3 de Catalunya and its first channel TV3, created in 1983. An extensive program of normalization of Catalan language was carried out. Today, Catalonia remains one of the most economically dynamic communities of Spain. The Catalan capital and largest city, Barcelona, is a major international cultural centre and a major tourist destination. In 1992, Barcelona hosted the Summer Olympic Games.\nIndependence movement.\nIn November 2003, elections to the Parliament of Catalonia gave the government to a left-wing Catalanist coalition formed by the Socialists' Party of Catalonia (PSC-PSOE), Republican Left of Catalonia (ERC) and Initiative for Catalonia Greens (ICV), and the socialist Pasqual Maragall was appointed president. The new government prepared a bill for a new Statute of Autonomy, with the aim of consolidate and expand self-government.\nThe new Statute of Autonomy of Catalonia, approved after a referendum in 2006, was contested by important sectors of the Spanish society, especially by the conservative People's Party, which sent the law to the Constitutional Court of Spain. In 2010, the Court declared non-valid some of the articles that established an autonomous Catalan system of Justice, improved financing, a new territorial division, the status of Catalan language or the symbolical declaration of Catalonia as a nation. This decision was severely contested by large sectors of Catalan society, which increased the demands of independence.\nA controversial independence referendum was held in Catalonia on 1 October 2017, using a disputed voting process. It was declared illegal and suspended by the Constitutional Court of Spain, because it breached the 1978 Constitution. Subsequent developments saw, on 27 October 2017, a symbolic declaration of independence by the Parliament of Catalonia, the enforcement of direct rule by the Spanish government through the use of Article 155 of the Constitution, the dismissal of the Executive Council and the dissolution of the Parliament, with a snap regional election called for 21 December 2017, which ended with a victory of pro-independence parties. Former President Carles Puigdemont and five former cabinet ministers fled Spain and took refuge in other European countries (such as Belgium, in Puigdemont's case), whereas nine other cabinet members, including vice-president Oriol Junqueras, were sentenced to prison under various charges of rebellion, sedition, and misuse of public funds. Quim Torra became the 131st President of the Government of Catalonia on 17 May 2018, after the Spanish courts blocked three other candidates.\nIn 2018, the Assemblea Nacional Catalana joined the Unrepresented Nations and Peoples Organization (UNPO) on behalf of Catalonia.\nOn 14 October 2019, the Spanish Supreme court sentenced several Catalan political leaders, involved in organizing a referendum on Catalonia's independence from Spain, and convicted them on charges ranging from sedition to misuse of public funds, with sentences ranging from 9 to 13 years in prison. This decision sparked demonstrations around Catalonia. They were later pardoned by the Spanish government and left prison in June 2021.\nIn the early-to-mid 2020s support for independence declined.\nGeography.\nClimate.\nThe climate of Catalonia is diverse. The populated areas lying by the coast in Tarragona, Barcelona and Girona provinces feature a Hot-summer Mediterranean climate (K\u00f6ppen \"Csa\"). The inland part (including the Lleida province and the inner part of Barcelona province) show a mostly Mediterranean climate (K\u00f6ppen \"Csa\"). The Pyrenean peaks have a continental (K\u00f6ppen \"D\") or even Alpine climate (K\u00f6ppen \"ET\") at the highest summits, while the valleys have a maritime or oceanic climate sub-type (K\u00f6ppen \"Cfb\").\nIn the Mediterranean area, summers are dry and hot with sea breezes, and the maximum temperature is around . Winter is cool or slightly cold depending on the location. It snows frequently in the Pyrenees, and it occasionally snows at lower altitudes, even by the coastline. Spring and autumn are typically the rainiest seasons, except for the Pyrenean valleys, where summer is typically stormy.\nThe inland part of Catalonia is hotter and drier in summer. Temperature may reach , some days even . Nights are cooler there than at the coast, with the temperature of around . Fog is not uncommon in valleys and plains; it can be especially persistent, with freezing drizzle episodes and subzero temperatures during winter, mainly along the Ebro and Segre valleys and in Plain of Vic.\nTopography.\nCatalonia has a marked geographical diversity, considering the relatively small size of its territory. The geography is conditioned by the Mediterranean coast, with of coastline, and the towering Pyrenees along the long northern border. Catalonia is divided into three main geomorphological units:\nThe Catalan Pyrenees represent almost half in length of the Pyrenees, as it extends more than . Traditionally differentiated the Axial Pyrenees (the main part) and the Pre-Pyrenees (southern from the Axial) which are mountainous formations parallel to the main mountain ranges but with lower altitudes, less steep and a different geological formation. The highest mountain of Catalonia, located north of the comarca of Pallars Sobir\u00e0 is the Pica d'Estats (3,143m), followed by the Puigpedr\u00f3s (2,914m). The Serra del Cad\u00ed comprises the highest peaks in the Pre-Pyrenees and forms the southern boundary of the Cerdanya valley.\nThe Central Catalan Depression is a plain located between the Pyrenees and Pre-Coastal Mountains. Elevation ranges from . The plains and the water that descend from the Pyrenees have made it fertile territory for agriculture and numerous irrigation canals have been built. Another major plain is the Empord\u00e0, located in the northeast.\nThe Catalan Mediterranean system is based on two ranges running roughly parallel to the coast (southwest\u2013northeast), called the Coastal and the Pre-Coastal Ranges. The Coastal Range is both the shorter and the lower of the two, while the Pre-Coastal is greater in both length and elevation. Areas within the Pre-Coastal Range include Montserrat, Montseny and the Ports de Tortosa-Beseit. Lowlands alternate with the Coastal and Pre-Coastal Ranges. The Coastal Lowland is located to the East of the Coastal Range between it and the coast, while the Pre-Coastal Lowlands are located inland, between the Coastal and Pre-Coastal Ranges, and includes the Vall\u00e8s and Pened\u00e8s plains.\nFlora and fauna.\nCatalonia is a showcase of European landscapes on a small scale. Just over hosting a variety of substrates, soils, climates, directions, altitudes and distances to the sea. The area is of great ecological diversity and a remarkable wealth of landscapes, habitats and species.\nThe fauna of Catalonia comprises a minority of animals endemic to the region and a majority of non-endemic animals. Much of Catalonia enjoys a Mediterranean climate (except mountain areas), which makes many of the animals that live there adapted to Mediterranean ecosystems. Of mammals, there are plentiful wild boar, red foxes, as well as roe deer and in the Pyrenees, the Pyrenean chamois. Other large species such as the bear have been recently reintroduced.\nThe waters of the Balearic Sea are rich in biodiversity, and even the megafaunas of the oceans; various types of whales (such as fin, sperm, and pilot) and dolphins can be found in the area.\nHydrography.\nMost of Catalonia belongs to the Mediterranean Basin. The Catalan hydrographic network consists of two important basins, the one of the Ebro and the one that comprises the internal basins of Catalonia (respectively covering 46.84% and 51.43% of the territory), all of them flow to the Mediterranean. Furthermore, there is the Garona river basin that flows to the Atlantic Ocean, but it only covers 1.73% of the Catalan territory.\nThe hydrographic network can be divided in two sectors, an occidental slope or Ebro river slope and one oriental slope constituted by minor rivers that flow to the Mediterranean along the Catalan coast. The first slope provides an average of per year, while the second only provides an average of /year. The difference is due to the big contribution of the Ebro river, from which the Segre is an important tributary. Moreover, in Catalonia there is a relative wealth of groundwaters, although there is inequality between \"comarques\", given the complex geological structure of the territory. In the Pyrenees there are many small lakes, remnants of the ice age. The biggest are the lake of Banyoles and the recently recovered lake of Ivars.\nThe Catalan coast is almost rectilinear, with a length of and few landforms\u2014the most relevant are the Cap de Creus and the Gulf of Roses to the north and the Ebro Delta to the south. The Catalan Coastal Range hugs the coastline, and it is split into two segments, one between L'Estartit and the town of Blanes (the Costa Brava), and the other at the south, at the Costes del Garraf.\nThe principal rivers in Catalonia are the Ter, Llobregat, and the Ebro (Catalan: ), all of which run into the Mediterranean.\nAnthropic pressure and protection of nature.\nThe majority of Catalan population is concentrated in 30% of the territory, mainly in the coastal plains. Intensive agriculture, livestock farming and industrial activities have been accompanied by a massive tourist influx (more than 20million annual visitors), a rate of urbanization and even of major metropolisation which has led to a strong urban sprawl: two thirds of Catalans live in the urban area of Barcelona, while the proportion of urban land increased from 4.2% in 1993 to 6.2% in 2009, a growth of 48.6% in sixteen years, complemented with a dense network of transport infrastructure. This is accompanied by a certain agricultural abandonment (decrease of 15% of all areas cultivated in Catalonia between 1993 and 2009) and a global threat to natural environment. Human activities have also put some animal species at risk, or even led to their disappearance from the territory, like the gray wolf and probably the brown bear of the Pyrenees. The pressure created by this model of life means that the country's ecological footprint exceeds its administrative area.\nFaced with these problems, Catalan authorities initiated several measures whose purpose is to protect natural ecosystems. Thus, in 1990, the Catalan government created the Nature Conservation Council (Catalan: ), an advisory body with the aim to study, protect and manage the natural environments and landscapes of Catalonia. In addition, the Generalitat has carried out the Plan of Spaces of Natural Interest ( or PEIN) in 1992 while eighteen Natural Spaces of Special Protection ( or ENPE) have been instituted.\nThere's a National Park, Aig\u00fcestortes i Estany de Sant Maurici; fourteen Natural Parks, Alt Pirineu, Aiguamolls de l'Empord\u00e0, Cad\u00ed-Moixer\u00f3, Cap de Creus, Sources of Ter and Freser, Collserola, Ebro Delta, Ports, Montgr\u00ed, Medes Islands and Baix Ter, Montseny, Montserrat, Sant Lloren\u00e7 del Munt and l'Obac, Serra de Montsant, and the Garrotxa Volcanic Zone; as well as three Natural Places of National Interest ( or PNIN), the Pedraforca, the Poblet Forest and the Alb\u00e8res.\nPolitics.\nAfter Franco's death in 1975 and the adoption of a democratic constitution in Spain in 1978, Catalonia recovered and extended the powers that it had gained in the Statute of Autonomy of 1932 but lost with the fall of the Second Spanish Republic at the end of the Spanish Civil War in 1939.\nThis autonomous community has gradually achieved more autonomy since the approval of the Spanish Constitution of 1978. The Generalitat holds exclusive jurisdiction in education, health, culture, environment, communications, transportation, commerce, public safety and local government, and only shares jurisdiction with the Spanish government in justice. In all, some analysts argue that formally the current system grants Catalonia with \"more self-government than almost any other corner in Europe\".\nThe support for Catalan nationalism ranges from a demand for further autonomy and the federalisation of Spain to the desire for independence from the rest of Spain, expressed by Catalan independentists. The first survey following the Constitutional Court ruling that cut back elements of the 2006 Statute of Autonomy, published by \"La Vanguardia\" on 18July2010, found that 46% of the voters would support independence in a referendum. In February of the same year, a poll by the Open University of Catalonia gave more or less the same results. Other polls have shown lower support for independence, ranging from 40 to 49%. Although it is established in the whole of the territory, support for independence is significantly higher in the hinterland and the northeast, away from the more populous coastal areas such as Barcelona.\nSince 2011 when the question started to be regularly surveyed by the governmental Center for Public Opinion Studies (CEO), support for Catalan independence has been on the rise. According to the CEO opinion poll from July2016, 47.7% of Catalans would vote for independence and 42.4% against it while, about the question of preferences, according to the CEO opinion poll from March 2016, a 57.2 claim to be \"absolutely\" or \"fairly\" in favour of independence. Other polls have shown lower support for independence, ranging from 40 to 49%. Other polls show more variable results, according with the Spanish CIS, as of December2016, 47% of Catalans rejected independence and 45% supported it.\nIn hundreds of non-binding local referendums on independence, organised across Catalonia from 13September2009, a large majority voted for independence, although critics argued that the polls were mostly held in pro-independence areas. In December2009, 94% of those voting backed independence from Spain, on a turn-out of 25%. The final local referendum was held in Barcelona, in April2011. On 11September2012, a pro-independence march pulled in a crowd of between 600,000 (according to the Spanish Government), 1.5million (according to the Gu\u00e0rdia Urbana de Barcelona), and 2million (according to its promoters); whereas poll results revealed that half the population of Catalonia supported secession from Spain.\nTwo major factors were Spain's Constitutional Court's 2010 decision to declare part of the 2006 Statute of Autonomy of Catalonia unconstitutional, as well as the fact that Catalonia contributes 19.49% of the central government's tax revenue, but only receives 14.03% of central government's spending.\nParties that consider themselves either Catalan nationalist or independentist have been present in all Catalan governments since 1980. The largest Catalan nationalist party, Convergence and Union, ruled Catalonia from 1980 to 2003, and returned to power in the 2010 election. Between 2003 and 2010, a leftist coalition, composed by the Catalan Socialists' Party, the pro-independence Republican Left of Catalonia and the leftist-environmentalist Initiative for Catalonia-Greens, implemented policies that widened Catalan autonomy.\nIn the 25 November 2012 Catalan parliamentary election, sovereigntist parties supporting a secession referendum gathered 59.01% of the votes and held 87 of the 135seats in the Catalan Parliament. Parties supporting independence from the rest of Spain obtained 49.12% of the votes and a majority of 74seats.\nArtur Mas, then the president of Catalonia, organised early elections that took place on 27September2015. In these elections, Converg\u00e8ncia and Esquerra Republicana decided to join, and they presented themselves under the coalition named Junts pel S\u00ed (in Catalan, Together for Yes). Junts pel S\u00ed won 62seats and was the most voted party, and CUP (Candidatura d'Unitat Popular, a far-left and independentist party) won another 10, so the sum of all the independentist forces/parties was 72seats, reaching an absolute majority, but not in number of individual votes, comprising 47,74% of the total.\nStatute of Autonomy.\nThe Statute of Autonomy of Catalonia is the fundamental organic law, second only to the Spanish Constitution from which the Statute originates.\nIn the Spanish Constitution of 1978 Catalonia, along with the Basque Country and Galicia, was defined as a \"nationality\". The same constitution gave Catalonia the automatic right to autonomy, which resulted in the Statute of Autonomy of Catalonia of 1979.\nBoth the 1979 Statute of Autonomy and the current one, approved in 2006, state that \"Catalonia, as a nationality, exercises its self-government constituted as an Autonomous Community in accordance with the Constitution and with the Statute of Autonomy of Catalonia, which is its basic institutional law, always under the law in Spain\".\nThe Preamble of the 2006 Statute of Autonomy of Catalonia states that the Parliament of Catalonia has defined Catalonia as a nation, but that \"the Spanish Constitution recognizes Catalonia's national reality as a nationality\". While the Statute was approved by and sanctioned by both the Catalan and Spanish parliaments, and later by referendum in Catalonia, it has been subject to a legal challenge by the surrounding autonomous communities of Aragon, Balearic Islands and Valencia, as well as by the conservative People's Party. The objections are based on various issues such as disputed cultural heritage but, especially, on the Statute's alleged breaches of the principle of \"solidarity between regions\" in fiscal and educational matters enshrined by the Constitution.\nSpain's Constitutional Court assessed the disputed articles and on 28 June 2010, issued its judgment on the principal allegation of unconstitutionality presented by the People's Party in 2006. The judgment granted clear passage to 182 articles of the 223 that make up the fundamental text. The court approved 73 of the 114 articles that the People's Party had contested, while declaring 14 articles unconstitutional in whole or in part and imposing a restrictive interpretation on 27 others. The court accepted the specific provision that described Catalonia as a \"nation\", however ruled that it was a historical and cultural term with no legal weight, and that Spain remained the only nation recognised by the constitution.\nGovernment and law.\nThe Catalan Statute of Autonomy establishes that Catalonia, as an autonomous community, is organised politically through the Generalitat of Catalonia (Catalan: ), confirmed by the Parliament, the Presidency of the Generalitat, the Government or Executive Council and the other institutions established by the Parliament, among them the Ombudsman (), the Office of Auditors () the Council for Statutory Guarantees () or the Audiovisual Council of Catalonia ().\nThe Parliament of Catalonia (Catalan: ) is the unicameral legislative body of the Generalitat and represents the people of Catalonia. Its 135members (\"diputats\") are elected by universal suffrage to serve for a four-year period. According to the Statute of Autonomy, it has powers to legislate over devolved matters such as education, health, culture, internal institutional and territorial organization, nomination of the President of the Generalitat and control the Government, budget and other affairs. The last Catalan election was held on 12 May 2024, and its current speaker (president) is Josep Rull, incumbent since 10June2024.\nThe President of the Generalitat of Catalonia (Catalan: ) is the highest representative of Catalonia, and is also responsible of leading the government's action, presiding the Executive Council. Since the restoration of the Generalitat on the return of democracy in Spain, the Presidents of Catalonia have been Josep Tarradellas (1977\u20131980, president in exile since 1954), Jordi Pujol (1980\u20132003), Pasqual Maragall (2003\u20132006), Jos\u00e9 Montilla (2006\u20132010), Artur Mas (2010\u20132016), Carles Puigdemont (2016\u20132017) and, after the imposition of direct rule from Madrid, Quim Torra (2018\u20132020), Pere Aragon\u00e8s (2021\u20132024) and Salvador Illa (2024\u2013).\nThe Executive Council (Catalan: ) or Government (), is the body responsible of the government of the Generalitat, it holds executive and regulatory power, being accountable to the Catalan Parliament. It comprises the President of the Generalitat, the First Minister () or the Vice President, and the ministers () appointed by the president. Its seat is the Palau de la Generalitat, Barcelona. In 2021 the government was a coalition of two parties, the Republican Left of Catalonia (ERC) and Together for Catalonia (Junts) and is made up of 14 ministers, including the vice President, alongside to the president and a secretary of government, but in October2022 Together for Catalonia (Junts) left the coalition and the government.\nSecurity forces and Justice.\nCatalonia has its own police force, the (officially called ), whose origins date back to the 18thcentury. Since 1980 they have been under the command of the Generalitat, and since 1994 they have expanded in number in order to replace the national Civil Guard and National Police Corps, which report directly to the Homeland Department of Spain. The national bodies retain personnel within Catalonia to exercise functions of national scope such as overseeing ports, airports, coasts, international borders, custom offices, the identification of documents and arms control, immigration control, terrorism prevention, arms trafficking prevention, amongst others.\nMost of the justice system is administered by national judicial institutions, the highest body and last judicial instance in the Catalan jurisdiction, integrating the Spanish judiciary, is the High Court of Justice of Catalonia. The criminal justice system is uniform throughout Spain, while civil law is administered separately within Catalonia. The civil laws that are subject to autonomous legislation have been codified in the Civil Code of Catalonia () since 2002.\nCatalonia, together with Navarre and the Basque Country, are the Spanish communities with the highest degree of autonomy in terms of law enforcement.\nAdministrative divisions.\nCatalonia is organised territorially into provinces or regions, further subdivided into comarques and municipalities. The 2006Statute of Autonomy of Catalonia establishes the administrative organisation of the later three.\nProvinces.\nMuch like the rest of Spain, Catalonia is divided administratively into four provinces, the governing body of which is the Provincial Deputation (, , ). As of 2010, the four provinces and their populations were:\nUnlike vegueries, provinces do not follow the limitations of the subdivisional counties, notably Baixa Cerdanya, which is split in half between the demarcations of Lleida and Girona. This situation has led some isolated municipalities to request province changes from the Spanish government.\nVegueries.\nBesides provinces, Catalonia is internally divided into eight regions or vegueries, based on the feudal administrative territorial jurisdiction of the Principality of Catalonia. Established in 2006, vegueries are used by the Generalitat de Catalunya with the aim to more effectively divide Catalonia administratively. In addition, vegueries are intended to become Catalonia's first-level administrative division and a full replacement for the four deputations of the Catalan provinces, creating a council for each vegueria, but this has not been realised as changes to the statewide provinces system are unconstitutional without a constitutional amendment.\nThe territorial plan of Catalonia () provided six general functional areas, but was amended by Law24/2001, of 31December, recognizing \"Alt Pirineu and Aran\" as a new functional area differentiated of Ponent. After some opposition from some territories, it was made possible for the Aran Valley to retain its government (the vegueria is renamed to \"Alt Pirineu\", although the name \"Alt Pirineu and Aran\" is still used by the regional plan) and in 2016, the Catalan Parliament approved the eighth vegueria, Pened\u00e8s, split from the Barcelona region.\nAs of 2022, the eight regions and their populations were:\nComarques.\nComarques (often known as \"counties\" in English, but different from the historical Catalan counties) are entities composed of municipalities to internally manage their responsibilities and services. The current regional division has its roots in a decree of the Generalitat de Catalunya of 1936, in effect until 1939, when it was suppressed by Franco. In 1987 the Catalan Government reestablished the comarcal division and in 1988 three new comarques were added (Alta Ribagor\u00e7a, Pla d'Urgell and Pla de l'Estany). Some further revisions have been realised since then, such as the additions of Moian\u00e8s and Llu\u00e7an\u00e8s counties, in 2015 and 2023 respectively. Except for Barcelon\u00e8s, every comarca is administered by a comarcal council ().\nAs of 2024, Catalonia is divided in 42 counties plus the Aran Valley. The latter, although previously (and still informally) considered a comarca, obtained in 1990 a particular status within Catalonia due to its differences in culture and language, being administered by a body known as the (General Council of Aran), and in 2015 it was defined as a \"unique territorial entity\" instead of a county.\nMunicipalities.\nThere are at present 947municipalities () in Catalonia. Each municipality is run by a council () elected every four years by the residents in local elections. The council consists of a number of members () depending on population, who elect the mayor ( or ). Its seat is the town hall (, or ).\nEconomy.\nA highly industrialized region, the nominal GDP of Catalonia in 2018 was \u20ac228billion (second after the community of Madrid, \u20ac230billion) and the per capitaGDP was \u20ac30,426 ($32,888), behind Madrid (\u20ac35,041), the Basque Country (\u20ac33,223), and Navarre (\u20ac31,389). That year, the GDP growth was 2.3%.\nCatalonia's long-term credit rating is BB(Non-Investment Grade) according to Standard &amp; Poor's, Ba2(Non-Investment Grade) according to Moody's, and BBB-(Low Investment Grade) according to Fitch Ratings. Catalonia's rating is tied for worst with between 1 and 5 other autonomous communities of Spain, depending on the rating agency.\nThe city of Barcelona occupies the eighth position as one of the world's best cities to live, work, research and visit in 2021, according to the report \"The World's Best Cities 2021\", prepared by Resonance Consultancy.\nAccording to a 2020 study by Eu-Starts-Up, the Catalan capital is one of the European bases of \"reference for start-ups\" and the fifth city in the world to establish one of these companies, behind London, Berlin, Paris and Amsterdam. Barcelona is behind London, New York, Paris, Moscow, Tokyo, Dubai and Singapore and ahead of Los Angeles and Madrid.\nIn the context of the financial crisis of 2007\u20132008, Catalonia was expected to suffer a recession amounting to almost a 2% contraction of its regional GDP in 2009. Catalonia's debt in 2012 was the highest of all Spain's autonomous communities, reaching \u20ac13,476million, i.e. 38% of the total debt of the 17autonomous communities, but in recent years its economy recovered a positive evolution and the GDP grew a 3.3% in 2015.\nCatalonia is amongst the List of country subdivisions by GDP over 100 billion US dollars and is a member of the Four Motors for Europe organisation.\nThe distribution of sectors is as follows:\nThe main tourist destinations in Catalonia are the city of Barcelona, the beaches of the Costa Brava in Girona, the beaches of the Costa del Maresme and Costa del Garraf from Malgrat de Mar to Vilanova i la Geltr\u00fa and the Costa Daurada in Tarragona. In the High Pyrenees there are several ski resorts, near Lleida. On 1November2012, Catalonia started charging a tourist tax. The revenue is used to promote tourism, and to maintain and upgrade tourism-related infrastructure.\nMany of Spain's leading savings banks were based in Catalonia before the independence referendum of 2017. However, in the aftermath of the referendum, many of them moved their registered office to other parts of Spain. That includes the two biggest Catalan banks at that moment, La Caixa, which moved its office to Palma de Mallorca, and Banc Sabadell, ranked fourth among all Spanish private banks and which moved its office to Alicante. That happened after the Spanish government passed a law allowing companies to move their registered office without requiring the approval of the company's general meeting of shareholders. Overall, there was a negative net relocation rate of companies based in Catalonia moving to other autonomous communities of Spain. From the 2017 independence referendum until the end of 2018, for example, Catalonia lost 5454companies to other parts of Spain (mainly Madrid), 2359 only in 2018, gaining 467 new ones from the rest of the country during 2018. It has been reported that the Spanish government and the Spanish King Felipe VI pressured some of the big Catalan companies to move their headquarters outside of the region.\nThe stock market of Barcelona, which in 2016 had a volume of around \u20ac152billion, is the second largest of Spain after Madrid, and Fira de Barcelona organizes international exhibitions and congresses to do with different sectors of the economy.\nThe main economic cost for Catalan families is the purchase of a home. According to data from the Society of Appraisal on 31December2005 Catalonia is, after Madrid, the second most expensive region in Spain for housing: 3,397\u20ac/m2 on average (see Spanish property bubble).\nUnemployment.\nThe unemployment rate stood at 10.5% in 2019 and was lower than the national average.\nTransport.\nAirports.\nAirports in Catalonia are owned and operated by Aena (a Spanish Government entity) except two airports in Lleida which are operated by Aeroports de Catalunya (an entity belonging to the Government of Catalonia).\nPorts.\nSince the Middle Ages, Catalonia has been well integrated into international maritime networks. The port of Barcelona (owned and operated by , a Spanish Government entity) is an industrial, commercial and tourist port of worldwide importance. With 1,950,000TEUs in 2015, it is the first container port in Catalonia, the third in Spain after Valencia and Algeciras in Andalusia, the 9thin the Mediterranean Sea, the 14thin Europe and the 68thin the world. It is sixth largest cruise port in the world, the first in Europe and the Mediterranean with 2,364,292passengers in 2014. The ports of Tarragona (owned and operated by Puertos del Estado) in the southwest and Palam\u00f3s near Girona at northeast are much more modest. The port of Palam\u00f3s and the other ports in Catalonia(26) are operated and administered by , a Catalan Government entity.\nThe development of these infrastructures, resulting from the topography and history of the Catalan territory, responds strongly to the administrative and political organization of this autonomous community.\nRoads.\nThere are of roads throughout Catalonia.\nThe principal highways are AP-7 () and A-7 (). They follow the coast from the French border to Valencia, Murcia and Andalusia. The main roads generally radiate from Barcelona. The AP-2 () and A-2 () connect inland and onward to Madrid.\nOther major roads are:\nPublic-own roads in Catalonia are either managed by the autonomous government of Catalonia (e.g., C- roads) or the Spanish government (e.g., AP-, A-, N- roads).\nRailways.\nCatalonia saw the first railway construction in the Iberian Peninsula in 1848, linking Barcelona with Matar\u00f3. Given the topography, most lines radiate from Barcelona. The city has both suburban and inter-city services. The main east coast line runs through the province connecting with the SNCF (French Railways) at Portbou on the coast.\nThere are two publicly owned railway companies operating in Catalonia: the Catalan FGC that operates commuter and regional services, and the Spanish national Renfe that operates long-distance and high-speed rail services (AVE and Avant) and the main commuter and regional service , administered by the Catalan government since 2010.\nHigh-speed rail (AVE) services from Madrid currently reach Barcelona, via Lleida and Tarragona. The official opening between Barcelona and Madrid took place 20February2008. The journey between Barcelona and Madrid now takes about two-and-a-half hours. A connection to the French high-speed TGV network has been completed (called the Perpignan\u2013Barcelona high-speed rail line) and the Spanish AVE service began commercial services on the line 9January2013, later offering services to Marseille on their high speed network. This was shortly followed by the commencement of commercial service by the French TGV on 17January2013, leading to an average travel time on the Paris-Barcelona TGV route of 7h42m. This new line passes through Girona and Figueres with a tunnel through the Pyrenees.\nDemographics.\nAs of 2024, the official population of Catalonia was 8.067.454. 1,194,947residents did not have Spanish citizenship, accounting for about 16% of the population.\nThe Urban Region of Barcelona includes 5,217,864people and covers an area of . The metropolitan area of the Urban Region includes cities such as L'Hospitalet de Llobregat, Sabadell, Terrassa, Badalona, Santa Coloma de Gramenet and Cornell\u00e0 de Llobregat.\nIn 1900, the population of Catalonia was 1,966,382people and in 1970 it was 5,122,567. The sizeable increase of the population was due to the demographic boom in Spain during the 1960s and early 1970s as well as in consequence of large-scale internal migration from the rural economically weak regions to its more prospering industrial cities. In Catalonia, that wave of internal migration arrived from several regions of Spain, especially from Andalusia, Murcia and Extremadura. As of 1999, it was estimated that over 60% of Catalans descended from 20thcentury migrations from other parts of Spain.\nImmigrants from other countries settled in Catalonia since the 1990s; a large percentage comes from Africa, Latin America and Eastern Europe, and smaller numbers from Asia and Southern Europe, often settling in urban centers such as Barcelona and industrial areas. In 2017, Catalonia had 940,497foreign residents (11.9%of the total population) with non-Spanish ID cards, without including those who acquired Spanish citizenship.\nReligion.\nHistorically, all the Catalan population was Christian, specifically Catholic, but since the 1980s there has been a trend of decline of Christianity. Nevertheless, according to the most recent study sponsored by the Government of Catalonia, as of 2020, 62.3% of the Catalans identify as Christians (up from 61.9% in 2016 and 56.5% in 2014) of whom 53.0%Catholics, 7.0%Protestants and Evangelicals, 1.3%Orthodox Christians and 1.0%Jehovah's Witnesses. At the same time, 18.6% of the population identify as atheists, 8.8%as agnostics, 4.3%as Muslims, and a further 3.4% as being of other religions.\nLanguages.\nAccording to the linguistic census held by the Government of Catalonia in 2013, Spanish is the most spoken language in Catalonia (46.53%claim Spanish as \"their own language\"), followed by Catalan (37.26%claim Catalan as \"their own language\"). In everyday use, 11.95%of the population claim to use both languages equally, whereas 45.92%mainly use Spanish and 35.54%mainly use Catalan. There is a significant difference between the Barcelona metropolitan area (and, to a lesser extent, the Tarragona area), where Spanish is more spoken than Catalan, and the more rural and small town areas, where Catalan clearly prevails over Spanish.\nOriginating in the historic territory of Catalonia, Catalan has enjoyed special status since the approval of the Statute of Autonomy of 1979 which declares it to be \"Catalonia's own language\", a term which signifies a language given special legal status within a Spanish territory, or which is historically spoken within a given region. The other languages with official status in Catalonia are Spanish, which has official status throughout Spain, and Aranese Occitan, which is spoken in Val d'Aran.\nSince the Statute of Autonomy of 1979, Aranese (a Gascon dialect of Occitan) has also been official and subject to special protection in Val d'Aran. This small area of 7,000inhabitants was the only place where a dialect of Occitan had received full official status. Then, on 9August2006, when the new Statute came into force, Occitan became official throughout Catalonia. Occitan is the mother tongue of 22.4% of the population of Val d'Aran, which has attracted heavy immigration from other Spanish regions to work in the service industry. Catalan Sign Language is also officially recognised.\nAlthough not considered an \"official language\" in the same way as Catalan, Spanish, and Occitan, the Catalan Sign Language, with about 18,000 users in Catalonia, is granted official recognition and support: \"The public authorities shall guarantee the use of Catalan sign language and conditions of equality for deaf people who choose to use this language, which shall be the subject of education, protection and respect.\"\nAs was the case since the ascent of the Bourbon dynasty to the throne of Spain after the War of the Spanish Succession, and with the exception of the short period of the Second Spanish Republic, under Francoist Spain Catalan was banned from schools and all other official use, so that for example families were not allowed to officially register children with Catalan names. Although never completely banned, Catalan language publishing was severely restricted during the early 1940s, with only religious texts and small-run self-published texts being released. Some books were published clandestinely or circumvented the restrictions by showing publishing dates prior to 1936. This policy was changed in 1946, when restricted publishing in Catalan resumed.\nRural\u2013urban migration originating in other parts of Spain also reduced the social use of Catalan in urban areas and increased the use of Spanish. Lately, a similar sociolinguistic phenomenon has occurred with foreign immigration. Catalan cultural activity increased in the 1960s and the teaching of Catalan began thanks to the initiative of associations such as \u00d2mnium Cultural.\nAfter the end of Francoist Spain, the newly established self-governing democratic institutions in Catalonia embarked on a long-term language policy to recover the use of Catalan and has, since 1983, enforced laws which attempt to protect and extend the use of Catalan. This policy, known as the \"linguistic normalisation\" ( in Catalan, in Spanish) has been supported by the vast majority of Catalan political parties through the last thirty years. Some groups consider these efforts a way to discourage the use of Spanish, whereas some others, including the Catalan government and the European Union consider the policies respectful, or even as an example which \"should be disseminated throughout the Union\".\nToday, Catalan is the main language of the Catalan autonomous government and the other public institutions that fall under its jurisdiction. Basic public education is given mainly in Catalan, but also there are some hours per week of Spanish medium instruction. Although businesses are required by law to display all information (e.g. menus, posters) at least in Catalan, this not systematically enforced. There is no obligation to display this information in either Occitan or Spanish, although there is no restriction on doing so in these or other languages. The use of fines was introduced in a 1997 linguistic law that aims to increase the public use of Catalan and defend the rights of Catalan speakers. On the other hand, the Spanish Constitution does not recognize equal language rights for national minorities since it enshrined Spanish as the only official language of the state, the knowledge of which being compulsory. Numerous laws regarding for instance the labelling of pharmaceutical products, make in effect Spanish the only language of compulsory use.\nThe law ensures that both Catalan and Spanish \u2013 being official languages \u2013 can be used by the citizens without prejudice in all public and private activities. The Generalitat uses Catalan in its communications and notifications addressed to the general population, but citizens can also receive information from the Generalitat in Spanish if they so wish. Debates in the Catalan Parliament take place almost exclusively in Catalan and the Catalan public television broadcasts programs mainly in Catalan.\nDue to the intense immigration which Spain in general and Catalonia in particular experienced in the first decade of the 21st century, many foreign languages are spoken in various cultural communities in Catalonia, of which Rif-Berber, Moroccan Arabic, Romanian and Urdu are the most common ones.\nIn Catalonia, there is a high social and political consensus on the language policies favoring Catalan, also among Spanish speakers and speakers of other languages. However, some of these policies have been criticised for trying to promote Catalan by imposing fines on businesses. For example, following the passage of the law on Catalan cinema in March 2010, which established that half of the movies shown in Catalan cinemas had to be in Catalan, a general strike of 75% of the cinemas took place. The Catalan government gave in and dropped the clause that forced 50% of the movies to be dubbed or subtitled in Catalan before the law came to effect. On the other hand, organisations such as Plataforma per la Llengua reported different violations of the linguistic rights of the Catalan speakers in Catalonia and the other Catalan-speaking territories in Spain, most of them caused by the institutions of the Spanish government in these territories.\nThe Catalan language policy has been challenged by some political parties in the Catalan Parliament. Citizens, currently the main opposition party, has been one of the most consistent critics of the Catalan language policy within Catalonia. The Catalan branch of the People's Party has a more ambiguous position on the issue: on one hand, it demands a bilingual Catalan\u2013Spanish education and a more balanced language policy that would defend Catalan without favoring it over Spanish, whereas on the other hand, a few local PP politicians have supported in their municipalities measures privileging Catalan over Spanish and it has defended some aspects of the official language policies, sometimes against the positions of its colleagues from other parts of Spain.\nCulture.\nArt and architecture.\nCatalonia has given to the world many important figures in the area of the art. Catalan painters internationally known are, among others, Salvador Dal\u00ed, Joan Mir\u00f3 and Antoni T\u00e0pies. Closely linked with the Catalan pictorial atmosphere, Pablo Picasso lived in Barcelona during his youth, training them as an artist and creating the movement of cubism. Other important artists are Claudi Lorenzale for the medieval Romanticism that marked the artistic Renaixen\u00e7a, Mari\u00e0 Fortuny for the Romanticism and Catalan Orientalism of the nineteenth century, Ramon Casas or Santiago Rusi\u00f1ol, main representatives of the pictorial current of Catalan modernism from the end of the nineteenth century to the beginning of the twentieth century, Josep Maria Sert for early 20th-century Noucentisme, or Josep Maria Subirachs for expressionist or abstract sculpture and painting of the late twentieth century.\nThe most important painting museums of Catalonia are the Teatre-Museu Dal\u00ed in Figueres, the National Art Museum of Catalonia (MNAC), Picasso Museum, Fundaci\u00f3 Antoni T\u00e0pies, Joan Mir\u00f3 Foundation, the Barcelona Museum of Contemporary Art (MACBA), the Centre of Contemporary Culture of Barcelona (CCCB), and the CaixaForum.\nIn the field of architecture were developed and adapted to Catalonia different artistic styles prevalent in Europe, leaving footprints in many churches, monasteries and cathedrals, of Romanesque (the best examples of which are located in the northern half of the territory) and Gothic styles. The Gothic developed in Barcelona and its area of influence is known as Catalan Gothic, with some particular characteristics. The church of Santa Maria del Mar is an example of this kind of style. During the Middle Ages, many fortified castles were built by feudal nobles to mark their powers.\nThere are some examples of Renaissance (such as the Palau de la Generalitat), Baroque and Neoclassical architectures. In the late nineteenth century Modernism (Art Nouveau) appeared as the national art. The world-renowned Catalan architects of this style are Antoni Gaud\u00ed, Llu\u00eds Dom\u00e8nech i Montaner and Josep Puig i Cadafalch. Thanks to the urban expansion of Barcelona during the last decades of the century and the first ones of the next, many buildings of the Eixample are modernists. In the field of architectural rationalism, which turned especially relevant in Catalonia during the Republican era (1931\u20131939) highlighting Josep Llu\u00eds Sert and Josep Torres i Clav\u00e9, members of the GATCPAC and, in contemporany architecture, Ricardo Bofill and Enric Miralles.\nMonuments and World Heritage Sites.\nThere are several UNESCO World Heritage Sites in Catalonia:\nLiterature.\nThe oldest surviving literary use of the Catalan language is considered to be the religious text known as Homilies d'Organy\u00e0, written either in late 11th or early 12thcentury.\nThere are two historical moments of splendor of Catalan literature. The first begins with the historiographic chronicles of the 13thcentury (chronicles written between the thirteenth and fourteenth centuries narrating the deeds of the monarchs and leading figures of the Crown of Aragon) and the subsequent Golden Age of the 14th and 15thcenturies. After that period, between the 16th and 19thcenturies the Romantic historiography defined this era as the , considered as the \"decadent\" period in Catalan literature because of a general falling into disuse of the vernacular language in cultural contexts and lack of patronage among the nobility.\nThe second moment of splendor began in the 19thcentury with the cultural and political (Renaissance) represented by writers and poets such as Jacint Verdaguer, V\u00edctor Catal\u00e0 (pseudonym of Caterina Albert i Parad\u00eds), Narc\u00eds Oller, Joan Maragall and \u00c0ngel Guimer\u00e0. During the 20thcentury, avant-garde movements developed, initiated by the Generation of '14 (called Noucentisme in Catalonia), represented by Eugenio d'Ors, Joan Salvat-Papasseit, Josep Carner, Carles Riba, J.V. Foix and others. During the dictatorship of Primo de Rivera, the Civil War (Generation of '36) and the Francoist period, Catalan literature was maintained despite the repression against the Catalan language, being often produced in exile.\nThe most outstanding authors of this period are Salvador Espriu, Josep Pla, Josep Maria de Sagarra (who are considered mainly responsible for the renewal of Catalan prose), Merc\u00e8 Rodoreda, Joan Oliver Sallar\u00e8s or \"Pere Quart\", Pere Calders, Gabriel Ferrater, Manuel de Pedrolo, Agust\u00ed Bartra or Miquel Mart\u00ed i Pol. In addition, several foreign writers who fought in the International Brigades, or other military units, have since recounted their experiences of fighting in their works, historical or fictional, with for example, George Orwell, in \"Homage to Catalonia\" (1938) or Claude Simon's \"Le Palace\" (1962) and \"Les G\u00e9orgiques\" (1981).\nAfter the transition to democracy (1975\u20131978) and the restoration of the Generalitat (1977), literary life and the editorial market have returned to normality and literary production in Catalan is being bolstered with a number of language policies intended to protect Catalan culture. Besides the aforementioned authors, other relevant 20th-century writers of the Francoist and democracy periods include Joan Brossa, Agust\u00ed Bartra, Manuel de Pedrolo, Pere Calders or Quim Monz\u00f3.\nAna Mar\u00eda Matute, Jaime Gil de Biedma, Manuel V\u00e1zquez Montalb\u00e1n and Juan Goytisolo are among the most prominent Catalan writers in the Spanish language since the democratic restoration in Spain.\nFestivals and public holidays.\nCastells are one of the main manifestations of Catalan popular culture. The activity consists in constructing human towers by competing (teams). This practice originated in Valls, on the region of the Camp de Tarragona, during the 18th century, and later it was extended to the rest of the territory, especially in the late 20th century. The tradition of els Castells i els Castellers was declared Masterpiece of the Oral and Intangible Heritage of Humanity by UNESCO in 2010.\nIn main celebrations, other elements of the Catalan popular culture are also usually present: parades with (giants), bigheads, stick-dancers and musicians, and the , where devils and monsters dance and spray showers of sparks using firecrackers. Another traditional celebration in Catalonia is , declared a Masterpiece of the Oral and Intangible Heritage of Humanity by the UNESCO on 25 November 2005.\nChristmas in Catalonia lasts two days, plus Christmas Eve. On the 25th, Christmas is celebrated, followed by a similar feast on the 26, called Sant Esteve (Saint Steve's Day). This allows families to visit and dine with different sectors of the extended family or get together with friends on the second day.\nOne of the most deeply rooted and curious Christmas traditions is the popular figure of the , consisting of an (often hollow) log with a face painted on it and often two little front legs appended, usually wearing a Catalan hat and scarf. The word has nothing to do with the Spanish word \"t\u00edo\", meaning uncle. \"Ti\u00f3\" means log in Catalan. The log is sometimes \"found in the woods\" (in an event staged for children) and then adopted and taken home, where it is fed and cared for during a month or so. On Christmas Day or on Christmas Eve, a game is played where children march around the house singing a song requesting the log to poop, then they hit the log with a stick, to make it poop, and lo and behold, as if through magic, it poops candy, and sometimes other small gifts. Usually, the larger or main gifts are brought by the Three Kings on 6 January, and the ti\u00f3 only brings small things.\nAnother custom is to make a (nativity scene) in the home or in shop windows, the latter sometimes competing in originality or sheer size and detail. Churches often host exhibits of numerous dioramas by nativity scene makers, or a single nativity scene they put out, and town halls generally put out a nativity scene in the central square. In Barcelona, every year, the main nativity scene is designed by different artists, and often ends up being an interesting, post-modern or conceptual and strange creation. In the home, the nativity scene often consists of strips of cork bark to represent cliffs or mountains in the background, moss as grass in the foreground, some wood chips or other as dirt, and aluminum foil for rivers and lakes. The traditional figurines often included are the three wise men on camels or horses, which are moved every day or so to go closer to the manger, a star with a long tail in the background to lead people to the spot, the annunciation with shepherds having a meal and an angel appearing (hanging from something), a washer lady washing clothes in the pond, sheep, ducks, people carrying packages on their backs, a donkey driver with a load of twigs, and atrezzo such as a starry sky, miniature towns placed in the distance, either Oriental-styled or local-looking, a bridge over the river, trees, etc.\nOne of the most astonishing and sui-generis figurines traditionally placed in the nativity scene, to the great glee of children, is the , a person depicted in the act of defecating. This figurine is hidden in some corner of the nativity scene and the game is to detect it. Of course, churches forgo this figurine, and the main nativity scene of Barcelona, for instance, likewise does not feature it. The caganer is so popular it has, together with the ti\u00f3, long been a major part of the Christmas markets, where they come in the guise of your favorite politicians or other famous people, as well as the traditional figures of a Catalan farmer. People often buy a figurine of a caganer in the guise of a famous person they are actually fond of, contrary to what one would imagine, though sometimes people buy a caganer in the guise of someone they dislike, although this means they have to look at them in the home.\nAnother (extended) Christmas tradition is the celebration of the Epiphany on 6 January, which is called \"Reis\", meaning Three Kings Day. This is every important in Catalonia and the Catalan-speaking areas, and families go to watch major parades on the eve of the Epiphany, where they can greet the kings and watch them pass by in pomp and circumstance, on floats and preceded and followed by pages, musicians, dancers, etc. They often give the kings letters with their gift requests, which are collected by the pages. On the next day, the children find the gifts the three kings brought for them.\nIn addition to traditional local Catalan culture, traditions from other parts of Spain can be found as a result of migration from other regions, for instance the celebration of the Andalusian in Catalonia.\nOn 28 July 2010, second only after the Canary Islands, Catalonia became another Spanish territory to forbid bullfighting. The ban, which went into effect on 1 January 2012, had originated in a popular petition supported by over 180,000 signatures.\nMusic and dance.\nThe sardana is considered to be the most characteristic Catalan folk dance, interpreted to the rhythm of tambor\u00ed, tible and tenora (from the oboe family), trumpet, tromb\u00f3 (trombone), fiscorn (family of bugles) and contrabaix with three strings played by a cobla, and are danced in a circle dance. Other tunes and dances of the traditional music are the contrap\u00e0s (obsolete today), ball de bastons (the \"dance of sticks\"), the moixiganga, the goigs (popular songs), the galops or the jota in the southern part. The are characteristic in some marine localities of the Costa Brava, especially during the summer months when these songs are sung outdoors accompanied by a of burned rum.\nArt music was first developed, up to the nineteenth century and, as in much of Europe, in a liturgical setting, particularly marked by the Escolania de Montserrat. The main Western musical trends have marked these productions, medieval monodies or polyphonies, with the work of Abbot Oliba in the eleventh century or the compilation Llibre Vermell de Montserrat (\"Red Book of Montserrat\") from the fourteenth century. Through the Renaissance there were authors such as Pere Albert Vila, Joan Brudieu or the two Mateu Fletxa (\"The Old\" and \"The Young\"). Baroque had composers like Joan Cererols. The Romantic music was represented by composers such as Fernando Sor, Josep Anselm Clav\u00e9 (father of choir movement in Catalonia and responsible of the music folk reviving) or Felip Pedrell.\nModernisme also expressed in musical terms from the end of the 19th century onwards, mixing folkloric and post-romantic influences, through the works of Isaac Alb\u00e9niz and Enric Granados. The avant-garde spirit initiated by the modernists is prolonged throughout the twentieth century, thanks to the activities of the Orfe\u00f3 Catal\u00e0, a choral society founded in 1891, with its monumental concert hall, the Palau de la M\u00fasica Catalana in Catalan, built by Llu\u00eds Dom\u00e8nech i Montaner from 1905 to 1908, the Barcelona Symphony Orchestra created in 1944 and composers, conductors and musicians engaged against the Francoism like Robert Gerhard, Eduard Toldr\u00e0 and Pau Casals.\nPerformances of opera, mostly imported from Italy, began in the 18th century, but some native operas were written as well, including the ones by Dom\u00e8nec Terradellas, Carles Baguer, Ramon Carles, Isaac Alb\u00e9niz and Enric Granados. The Barcelona main opera house, Gran Teatre del Liceu (opened in 1847), remains one of the most important in Spain, hosting one of the most prestigious music schools in Barcelona, the Conservatori Superior de M\u00fasica del Liceu. Several lyrical artists trained by this institution gained international renown during the 20th century, such as Victoria de los \u00c1ngeles, Montserrat Caball\u00e9, Giacomo Aragall and Josep Carreras.\nCellist Pau Casals is admired as an outstanding player. Other popular musical styles were born in the second half of the 20th century such as Nova Can\u00e7\u00f3 from the 1960s with Llu\u00eds Llach and the group Els Setze Jutges, the Catalan rumba in the 1960s with Peret, Catalan Rock from the late 1970s with La Banda Trapera del R\u00edo and Decibelios for Punk Rock, Sau, Els Pets, Sopa de Cabra or Lax'n'Busto for pop rock or Sangtra\u00eft for hard rock, electropop since the 1990s with OBK and indie pop from the 1990s.\nMedia and cinema.\nCatalonia is the autonomous community, along with Madrid, that has the most media (TV, magazines, newspapers etc.). In Catalonia there is a wide variety of local and comarcal media. With the restoration of democracy, many newspapers and magazines, until then in the hands of the Franco government, were recovered in order to convert them into free and democratic media, while local radio and television began broadcasting.\nTelevisi\u00f3 de Catalunya, which broadcasts entirely in the Catalan language, is the main Catalan public network. It has five channels: TV3, El 33, Super3, 3/24, Esport3 and TV3CAT. In 2018, TV3 became the first television channel to be the most viewed for nine consecutive years in Catalonia. State television that broadcasts in Catalonia in the Spanish language include (with few emissions in Catalan), Antena 3, Cuatro, Telecinco, and La Sexta. Other smaller Catalan television channels include local television channels, notably betev\u00e9, owned by the City Council of Barcelona, and broadcast in Catalan.\nThe two main Catalan newspapers of general information are \"El Peri\u00f3dico de Catalunya\" and \"La Vanguardia\", both with editions in Catalan and Spanish. Catalan only published newspapers include \"Ara\" and \"El Punt Avui\" (from the fusion of \"El Punt\" and \"Avui\" in 2011), as well as most part of the local press. The Spanish newspapers, such as \"El Pa\u00eds\", \"El Mundo\" or \"La Raz\u00f3n\", can be also acquired.\nCatalonia has a long tradition of use of radio, the first regular radio broadcast in the country was from R\u00e0dio Barcelona in 1924. Today, the public Catalunya R\u00e0dio (owned by Catalan Media Corporation) and the private RAC 1 (belonging to Grup God\u00f3) are the two main radio stations of Catalonia, both in Catalan.\nRegarding the cinema, after the democratic transition, three styles have dominated since then. First, auteur cinema, in the continuity of the Barcelona School, emphasizes experimentation and form, while focusing on developing social and political themes. Worn first by Josep Maria Forn or Bigas Luna, then by Marc Recha, Jaime Rosales and Albert Serra, this genre has achieved some international recognition. Then, the documentary became another genre particularly representative of contemporary Catalan cinema, boosted by Joaquim Jord\u00e0 i Catal\u00e0 and Jos\u00e9 Luis Guer\u00edn. Later, horror films and thrillers have also emerged as a specialty of the Catalan film industry, thanks in particular to the vitality of the Sitges Film Festival, created in 1968. Several directors have gained worldwide renown thanks to this genre, starting with Jaume Balaguer\u00f3 and his series \"REC\" (co-directed with Valencian Paco Plaza), Juan Antonio Bayona and \"El Orfanato\" or Jaume Collet-Serra with \"Orphan\", \"Unknown\" and \"Non-Stop\".\nCatalan actors have shot for Spanish and international productions, such as Sergi L\u00f3pez.\nThe Museum of Cinema - Tom\u00e0s Mallol Collection (Museu del Cinema \u2013 Col.lecci\u00f3 Tom\u00e0s Mallol in Catalan) of Girona is home of important permanent exhibitions of cinema and pre-cinema objects. Other important institutions for the promotion of cinema are the Gaud\u00ed Awards (Premis Gaud\u00ed in Catalan, which replaced from 2009 Barcelona Film Awards themselves created in 2002), serving as equivalent for Catalonia to the Spanish Goya or French C\u00e9sar.\nPhilosophy.\n is a form of ancestral Catalan wisdom or sensibleness. It involves well-pondered perception of situations, level-headedness, awareness, integrity, and right action. Many Catalans consider seny something unique to their culture, is based on a set of ancestral local customs stemming from the scale of values and social norms of their society.\nSport.\nSport has had a distinct importance in Catalan life and culture since the beginning of the 20th century; consequently, the region has a well-developed sports infrastructure. The main sports are football, basketball, handball, rink hockey, tennis and motorsport.\nWhile the most popular sports are represented at international level by the Spanish national teams, Catalonia plays as itself in some minor ones, such as korfball, futsal or rugby league. Various Catalan Sports Federations have a long tradition and some of them participated in the foundation of international sports federations, as the Catalan Federation of Rugby, that was one of the founder members of the F\u00e9d\u00e9ration Internationale de Rugby Amateur (FIRA) in 1934. The majority of Catalan sport federations are part of the Sports Federation Union of Catalonia (Catalan: ), founded in 1933. The presence of separate Catalan teams has caused disputes with Spanish sports institutions, as happened to roller hockey in the controversial Fresno Case (2004).\nThe Catalan Football Federation also periodically fields a national team against international opposition, organizing friendly matches. In the recent years they have played with Bulgaria, Argentina, Brazil, Basque Country, Colombia, Nigeria, Cape Verde and Tunisia. The biggest football clubs are Barcelona (also known as \"Bar\u00e7a\"), who have won five European Cups (UEFA Champions League), and Espanyol, who have twice been runner-up of the UEFA Cup (now UEFA Europa League). As of December 2024, Bar\u00e7a, Espanyol and Girona FC play in the top Spanish League (La Liga).\nThe Catalan waterpolo is one of the main powers of the Iberian Peninsula. The Catalans won triumphs in waterpolo competitions at European and world level by club (the Barcelona was champion of Europe in 1981/82 and the Catalonia in 1994/95) and national team (one gold and one silver in Olympic Games and World Championships). It also has many international synchronized swimming champions.\nMotorsport has a long tradition in Catalonia, which involving many people, with some world champions and several competitions organized since the beginning of the 20th century. The Circuit de Catalunya, built in 1991, is one of the main motorsport venues, holding the Catalan motorcycle Grand Prix, the Spanish F1 Grand Prix, a DTM race, and several other races.\nCatalonia hosted many relevant international sport events, such as the 1992 Summer Olympics in Barcelona, as well as the 1955 Mediterranean Games, the 2013 World Aquatics Championships or the 2018 Mediterranean Games. It held annually the fourth-oldest still-existing cycling stage race in the world, the Volta a Catalunya (Tour of Catalonia).\nSymbols.\nCatalonia has its own representative and distinctive national symbols such as:\nCuisine.\nCatalan gastronomy has a long culinary tradition. Various local food recipes have been described in documents dating from the fifteenth century. As with all the cuisines of the Mediterranean, Catatonian dishes make abundant use of fish, seafood, olive oil, bread and vegetables. Regional specialties include the (bread with tomato), which consists of bread (sometimes toasted), and tomato seasoned with olive oil and salt. Often the dish is accompanied with any number of sausages (cured botifarres, fuet, iberic ham, etc.), ham or cheeses. Others dishes include the , , (fish stew), and a dessert, Catalan cream.\nCatalan vineyards also have several wines, such as: Priorat, Montsant, Pened\u00e8s and Empord\u00e0. There is also a sparkling wine, the cava.\nCatalonia is internationally recognized for its fine dining. Three of the World's 50 Best Restaurants are in Catalonia, and four restaurants have three Michelin stars, including restaurants like El Bulli or El Celler de Can Roca, both of which regularly dominate international rankings of restaurants. The region has been awarded the European Region of Gastronomy title for the year 2016."}
{"id": "6823", "revid": "35585582", "url": "https://en.wikipedia.org/wiki?curid=6823", "title": "Konstantinos Kanaris", "text": "Konstantinos Kanaris (, ; c.\u00a017902 September 1877), also anglicised as Constantine Kanaris or Canaris, was a Greek statesman, admiral, and a hero of the Greek War of Independence.\nBiography.\nEarly life.\nKonstantinos Kanaris was born and grew up on the island of Psara, close to the island of Chios, in the Aegean. The exact year of his birth is unknown. Official records of the Hellenic Navy indicate 1795, however, modern Greek historians consider 1790 or 1793 to be more probable.\nHe was left an orphan at a young age. Having to support himself, he chose to become a seaman like most members of his family since the beginning of the 18th century. He was subsequently hired as a boy on the brig of his uncle Dimitris Bourekas.\nMilitary career.\nKanaris gained his fame during the Greek War of Independence (1821\u20131829). Unlike most other prominent figures of the War, he had never been initiated into the \"Filiki Eteria\" (Society of Friends), which played a significant role in the uprising against the Ottoman Empire, primarily by secret recruitment of supporters against the Turkish rule.\nBy early 1821, the movement had gained enough support to launch a revolution. This seems to have inspired Kanaris, who was in Odessa at the time. He returned to the island of Psara in haste and was present when it joined the uprising on 10 April 1821.\nThe island formed its own fleet and the famed seamen of Psara, already known for their well-equipped ships and successful battles against sea pirates, proved to be highly effective in naval warfare. Kanaris soon distinguished himself as a fire ship captain.\nAt Chios, on the moonless night of 6\u20137 June 1822, forces under his command destroyed the flagship of Nasuhzade Ali Pasha, Kapudan Pasha (Grand Admiral) of the Ottoman fleet, in revenge for the Chios massacre. The admiral was holding a \"Bayram\" celebration, allowing Kanaris and his men to position their fire ship without being noticed. When the flagship's powder store caught fire, all men aboard were instantly killed. The Turkish casualties comprised men, both naval officers and common sailors, as well as Nasuhzade Ali Pasha himself.\nKanaris led another successful attack against the Ottoman fleet at Tenedos in November 1822. He was famously said to have encouraged himself by murmuring \"Konstant\u00ed, you are going to die\" every time he was approaching a Turkish warship on the fire boat he was about to detonate.\nThe Ottoman fleet captured Psara on 21 June 1824. A part of the population, including Kanaris, managed to flee the island, but those who didn't were either sold into slavery or slaughtered. After the destruction of his home island, he continued to lead attacks against Turkish forces. In August 1824, he engaged in naval combats in the Dodecanese.\nThe following year, Kanaris led the Greek raid on Alexandria, a daring attempt to destroy the Egyptian fleet with fire ships that might have been successful if the wind had not failed just after the Greek ships entered Alexandria harbour.\nAfter the end of the War and the independence of Greece, Kanaris became an officer of the new Hellenic Navy, reaching the rank of admiral, and a prominent politician.\nPolitical career.\nKonstantinos Kanaris was one of the few with the personal confidence of Ioannis Kapodistrias, the first Head of State of independent Greece. After the assassination of Kapodistrias on 9 October 1831, he retired to the island of Syros.\nDuring the reign of King Otto I, Kanaris served as Minister in various governments and then as Prime Minister in the provisional government (16 February30 March 1844). He served a second term (15 October 184812 December 1849), and as Navy Minister in the 1854 cabinet of Alexandros Mavrokordatos.\nIn 1862, he was among the rare War of Independence veterans who took part in the bloodless insurrection that deposed the increasingly unpopular King Otto I and led to the election of Prince William of Denmark as King George I of Greece. During his reign, Kanaris served as a Prime Minister for a third term (6 March16 April 1864), fourth term (26 July 186426 February 1865), and fifth and last term (7 June2 September 1877).\nKanaris died on 2 September 1877 whilst still serving in office as Prime Minister. Following his death his government remained in power until 14 September 1877 without agreeing on a replacement at its head. He was buried in the First Cemetery of Athens and his heart was placed in a silver urn.\nLegacy.\nKonstantinos Kanaris is considered a national hero in Greece and ranks amongst the most notable participants of the War of Independence. Many statues and busts have been erected in his honour, such as \"Kanaris at Chios\" by Benedetto Civiletti in Palermo, a statue by Lazaros Fytalis in Athens, and a bust by David d'Angers. He was also featured on a Greek \u20af1 coin and a \u20af100 banknote issued by the Bank of Greece.\nTo honour Kanaris, the following ships of the Hellenic Navy have been named after him:\n\"Te Korowhakaunu / Kan\u00e1ris Sound\", a section of Taiari / Chalky Inlet in New Zealand's Fiordland National Park, was named after Konstantinos Kanaris by French navigator and explorer Jules de Blosseville (1802\u20131833).\nFamily.\nIn 1817, Konstantinos Kanaris married Despoina Maniatis, from a historical family of Psara.\nThey had seven children:\nWilhelm Canaris, a German Admiral, speculated that he might be a descendant of Konstantinos Kanaris. An official genealogical family history that was researched in 1938 showed however, that he was of Italian descent and not related to the Kanaris family from Greece."}
{"id": "6824", "revid": "12472594", "url": "https://en.wikipedia.org/wiki?curid=6824", "title": "Carl Sagan", "text": "Carl Edward Sagan (; ; November 9, 1934December 20, 1996) was an American astronomer, planetary scientist and science communicator. His best known scientific contribution is his research on the possibility of extraterrestrial life, including experimental demonstration of the production of amino acids from basic chemicals by exposure to light. He assembled the first physical messages sent into space, the Pioneer plaque and the Voyager Golden Record, which were universal messages that could potentially be understood by any extraterrestrial intelligence that might find them. He argued in favor of the hypothesis, which has since been accepted, that the high surface temperatures of Venus are the result of the greenhouse effect.\nInitially an assistant professor at Harvard, Sagan later moved to Cornell University, where he spent most of his career. He published more than 600 scientific papers and articles and was author, co-author or editor of more than 20 books. He wrote many popular science books, such as \"The Dragons of Eden\", \"Broca's Brain\", \"Pale Blue Dot\" and \"The Demon-Haunted World\". He also co-wrote and narrated the award-winning 1980 television series \"\", which became the most widely watched series in the history of American public television: \"Cosmos\" has been seen by at least 500 million people in 60 countries. A book, also called \"Cosmos\", was published to accompany the series. Sagan also wrote a science-fiction novel, published in 1985, called \"Contact\", which became the basis for the 1997 film \"Contact\". His papers, comprising 595,000 items, are archived in the Library of Congress.\nSagan was a popular public advocate of skeptical scientific inquiry and the scientific method; he pioneered the field of exobiology and promoted the search for extraterrestrial intelligent life (SETI). He spent most of his career as a professor of astronomy at Cornell University, where he directed the Laboratory for Planetary Studies. Sagan and his works received numerous awards and honors, including the NASA Distinguished Public Service Medal, the National Academy of Sciences Public Welfare Medal, the Pulitzer Prize for General Nonfiction (for his book \"The Dragons of Eden\"), and (for \"Cosmos: A Personal Voyage\") two Emmy Awards, the Peabody Award, and the Hugo Award. He married three times and had five children. After developing myelodysplasia, Sagan died of pneumonia at the age of 62 on December 20, 1996.\nEarly life.\nChildhood.\nCarl Edward Sagan was born on November 9, 1934, in the Bensonhurst neighborhood of New York City's Brooklyn borough. His mother, Rachel Molly Gruber (1906\u20131982), was a housewife from New York City; his father, Samuel Sagan (1905\u20131979), was a Ukrainian-born garment worker who had emigrated from Kamianets-Podilskyi (then in the Russian Empire). Sagan was named in honor of his maternal grandmother, Chaiya Clara, who had died while giving birth to her second child; she was, in Sagan's words, \"the mother she [Rachel] never knew.\" Sagan's maternal grandfather later married a woman named Rose, who Sagan's sister, Carol, would later say, was \"never accepted\" as Rachel's mother because Rachel \"knew she [Rose] wasn't her birth mother.\" Sagan's family lived in a modest apartment in Bensonhurst. He later described his family as Reform Jews, one of the more liberal of Judaism's four main branches. He and his sister agreed that their father was not especially religious, but that their mother \"definitely believed in God, and was active in the temple [...] and served only kosher meat.\" During the worst years of the Depression, his father worked as a movie theater usher.\nAccording to biographer Keay Davidson, Sagan experienced a kind of \"inner war\" as a result of his close relationship with both his parents, who were in many ways \"opposites.\" He traced his analytical inclinations to his mother, who had been extremely poor as a child in New York City during World War I and the 1920s, and whose later intellectual ambitions were sabotaged by her poverty, status as a woman and wife, and Jewish ethnicity. Davidson suggested she \"worshipped her only son, Carl\" because \"he would fulfill her unfulfilled dreams.\" Sagan believed that he had inherited his sense of wonder from his father, who spent his free time giving apples to the poor or helping soothe tensions between workers and management within New York City's garment industry. Although awed by his son's intellectual abilities, Sagan's father also took his inquisitiveness in stride, viewing it as part of growing up. Later, during his career, Sagan would draw on his childhood memories to illustrate scientific points, as he did in his book \"Shadows of Forgotten Ancestors\".\nDescribing his parents' influence on his later thinking, Sagan said: \"My parents were not scientists. They knew almost nothing about science. But in introducing me simultaneously to skepticism and to wonder, they taught me the two uneasily cohabiting modes of thought that are central to the scientific method.\" He recalled that a defining moment in his development came when his parents took him, at age four, to the 1939 New York World's Fair. He later described his vivid memories of several exhibits there. One, titled \"America of Tomorrow\", included a moving map, which, as he recalled, \"showed beautiful highways and cloverleaves and little General Motors cars all carrying people to skyscrapers, buildings with lovely spires, flying buttresses\u2014and it looked great!\" Another involved a flashlight shining on a photoelectric cell, which created a crackling sound, and another showed how the sound from a tuning fork became a wave on an oscilloscope. He also saw an exhibit of the then-nascent medium known as television. Remembering it, he later wrote: \"Plainly, the world held wonders of a kind I had never guessed. How could a tone become a picture and light become a noise?\"\nSagan also saw one of the fair's most publicized events: the burial at Flushing Meadows of a time capsule, which contained mementos from the 1930s to be recovered by Earth's descendants in a future millennium. Davidson wrote that this \"thrilled Carl.\" As an adult, inspired by his memories of the World's Fair, Sagan and his colleagues would create similar time capsules to be sent out into the galaxy: the Pioneer plaque and the \"Voyager Golden Record\" pr\u00e9cis.\nDuring World War\u00a0II, Sagan's parents worried about the fate of their European relatives, but he was generally unaware of the details of the ongoing war. He wrote, \"Sure, we had relatives who were caught up in the Holocaust. Hitler was not a popular fellow in our household... but on the other hand, I was fairly insulated from the horrors of the war.\" His sister, Carol, said that their mother \"above all wanted to protect Carl... she had an extraordinarily difficult time dealing with World War\u00a0II and the Holocaust.\" Sagan's book \"The Demon-Haunted World\" (1996) included his memories of this conflicted period, when his family dealt with the realities of the war in Europe, but tried to prevent it from undermining his optimistic spirit.\nSoon after entering elementary school, Sagan began to express his strong inquisitiveness about nature. He recalled taking his first trips to the public library alone, at age five, when his mother got him a library card. He wanted to learn what stars were, since none of his friends or their parents could give him a clear answer: \"I went to the librarian and asked for a book about stars [...] and the answer was stunning. It was that the Sun was a star, but really close. The stars were suns, but so far away they were just little points of light. The scale of the universe suddenly opened up to me. It was a kind of religious experience. There was a magnificence to it, a grandeur, a scale which has never left me. Never ever left me.\" When he was about six or seven, he and a close friend took trips to the American Museum of Natural History, in Manhattan. While there, they visited the Hayden Planetarium and walked around exhibits of space objects, such as meteorites, as well as displays of dinosaur skeletons and naturalistic scenes with animals. As Sagan later wrote, \"I was transfixed by the dioramas\u2014lifelike representations of animals and their habitats all over the world. Penguins on the dimly lit Antarctic ice [...] a family of gorillas, the male beating his chest [...] an American grizzly bear standing on his hind legs, ten or twelve feet tall, and staring me right in the eye.\"\nSagan's parents nurtured his growing interest in science, buying him chemistry sets and reading matter. But his fascination with outer space emerged as his primary focus, especially after he had read science fiction by such writers as H. G. Wells and Edgar Rice Burroughs, stirring his curiosity about the possibility of life on Mars and other planets. According to biographer Ray Spangenburg, Sagan's efforts in his early years to understand the mysteries of the planets became a \"driving force in his life, a continual spark to his intellect, and a quest that would never be forgotten.\" In 1947, Sagan discovered the magazine \"Astounding Science Fiction\", which introduced him to more hard science fiction speculations than those in the Burroughs novels. That same year, mass hysteria developed about the possibility that extraterrestrial visitors had arrived in flying saucers, and the young Sagan joined in the speculation that the flying \"discs\" people reported seeing in the sky might be alien spaceships.\nEducation.\nSagan attended David A. Boody Junior High School in his native Bensonhurst and had his bar mitzvah when he turned 13. In 1948, when he was 14, his father's work took the family to the older semi-industrial town of Rahway, New Jersey, where he attended Rahway High School. He was a straight-A student but was bored because his classes did not challenge him and his teachers did not inspire him. His teachers realized this and tried to convince his parents to send him to a private school, with an administrator telling them, \"This kid ought to go to a school for gifted children, he has something really remarkable.\" However, his parents could not afford to do so. Sagan became president of the school's chemistry club, and set up his own laboratory at home. He taught himself about molecules by making cardboard cutouts to help him visualize how they were formed: \"I found that about as interesting as doing [chemical] experiments.\" He was mostly interested in astronomy, learning about it in his spare time. In his junior year of high school, he discovered that professional astronomers were paid for doing something he always enjoyed, and decided on astronomy as a career goal: \"That was a splendid day\u2014when I began to suspect that if I tried hard I could do astronomy full-time, not just part-time.\" Sagan graduated from Rahway High School in 1951.\nBefore the end of high school, Sagan entered an essay writing contest in which he explored the idea that human contact with advanced life forms from another planet might be as disastrous for people on Earth as Native Americans' first contact with Europeans had been for Native Americans. The subject was considered controversial, but his rhetorical skill won over the judges and they awarded him first prize. When he was about to graduate from high school, his classmates voted him \"most likely to succeed\" and put him in line to be valedictorian. He attended the University of Chicago because, despite his excellent high school grades, it was one of the very few colleges he had applied to that would consider accepting a 16-year-old. Its chancellor, Robert Maynard Hutchins, had recently retooled the undergraduate College of the University of Chicago into an \"ideal meritocracy\" built on Great Books, Socratic dialogue, comprehensive examinations, and early entrance to college with no age requirement.\nAs an honors-program undergraduate, Sagan worked in the laboratory of geneticist H. J. Muller and wrote a thesis on the origins of life with physical chemist Harold Urey. He also joined the Ryerson Astronomical Society. In 1954, he was awarded a Bachelor of Liberal Arts with general and special honors in what he quipped was \"nothing.\" In 1955, he earned a Bachelor of Science in physics. He went on to do graduate work at the University of Chicago, earning a Master of Science in physics in 1956 and a Doctor of Philosophy in astronomy and astrophysics in 1960. His doctoral thesis, submitted to the Department of Astronomy and Astrophysics, was entitled \"Physical Studies of the Planets\". During his graduate studies, he used the summer months to work with planetary scientist Gerard Kuiper, who was his dissertation director, as well as physicist George Gamow and chemist Melvin Calvin. The title of Sagan's dissertation reflected interests he had in common with Kuiper, who had been president of the International Astronomical Union's commission on \"Physical Studies of Planets and Satellites\" throughout the 1950s.\nIn 1958, Sagan and Kuiper worked on the classified military Project A119, a secret United States Air Force plan to detonate a nuclear warhead on the Moon and document its effects. Sagan had a Top Secret clearance at the Air Force and a Secret clearance with NASA. In 1999, an article published in the journal \"Nature\" revealed that Sagan had included the classified titles of two Project A119 papers in his 1959 application for a scholarship to University of California, Berkeley. A follow-up letter to the journal by project leader Leonard Reiffel confirmed Sagan's security leak.\nCareer and research.\nFrom 1960 to 1962, Sagan was a Miller Fellow at the University of California, Berkeley. Meanwhile, he published an article in 1961 in the journal \"Science\" on the atmosphere of Venus, while also working with NASA's Mariner 2 team, and served as a \"Planetary Sciences Consultant\" to the RAND Corporation.\nAfter the publication of Sagan's \"Science\" article, in 1961, Harvard University astronomers Fred Whipple and Donald Menzel offered Sagan the opportunity to give a colloquium at Harvard and subsequently offered him a lecturer position at the institution. Sagan instead asked to be made an assistant professor, and eventually Whipple and Menzel were able to convince Harvard to offer Sagan the assistant professor position he requested. Sagan lectured, performed research, and advised graduate students at the institution from 1963 until 1968, as well as working at the Smithsonian Astrophysical Observatory, also located in Cambridge, Massachusetts.\nIn 1968, Sagan was denied academic tenure at Harvard. He later indicated that the decision was very unexpected. The denial has been blamed on several factors, including that he focused his interests too broadly across a number of areas (while the norm in academia is to become a renowned expert in a narrow specialty), and perhaps because of his well-publicized scientific advocacy, which some scientists perceived as borrowing the ideas of others for little more than self-promotion. An advisor from his years as an undergraduate student, Harold Urey, wrote a letter to the tenure committee recommending strongly against tenure for Sagan."}
{"id": "6826", "revid": "10049", "url": "https://en.wikipedia.org/wiki?curid=6826", "title": "Cases of anthrax", "text": ""}
{"id": "6827", "revid": "3236687", "url": "https://en.wikipedia.org/wiki?curid=6827", "title": "Cuban Missile Crisis", "text": "The Cuban Missile Crisis, also known as the October Crisis () in Cuba, or the Caribbean Crisis (), was a 13-day confrontation between the governments of the United States and the Soviet Union, when American deployments of nuclear missiles in Italy and Turkey were matched by Soviet deployments of nuclear missiles in Cuba. The crisis lasted from 16to28 October 1962. The confrontation is widely considered the closest the Cold War came to escalating into full-scale nuclear war.\nIn 1961 the US government put Jupiter nuclear missiles in Italy and Turkey. It had trained a paramilitary force of expatriate Cubans, which the CIA led in an attempt to invade Cuba and overthrow its government. Starting in November of that year, the US government engaged in a violent campaign of terrorism and sabotage in Cuba, referred to as the Cuban Project, which continued throughout the first half of the 1960s. The Soviet administration was concerned about a Cuban drift towards China, with which the Soviets had an increasingly fractious relationship. In response to these factors the Soviet and Cuban governments agreed, at a meeting between leaders Nikita Khrushchev and Fidel Castro in July 1962, to place nuclear missiles on Cuba to deter a future US invasion. Construction of launch facilities started shortly thereafter.\nA U-2 spy plane captured photographic evidence of medium- and long-range launch facilities in October. US President John F. Kennedy convened a meeting of the National Security Council and other key advisers, forming the Executive Committee of the National Security Council (EXCOMM). Kennedy was advised to carry out an air strike on Cuban soil in order to compromise Soviet missile supplies, followed by an invasion of the Cuban mainland. He chose a less aggressive course in order to avoid a declaration of war. On 22 October Kennedy ordered a naval blockade to prevent further missiles from reaching Cuba. He referred to the blockade as a \"quarantine\", not as a blockade, so the US could avoid the formal implications of a state of war.\nAn agreement was eventually reached between Kennedy and Khrushchev. The Soviets would dismantle their offensive weapons in Cuba, subject to United Nations verification, in exchange for a US public declaration and agreement not to invade Cuba again. The United States secretly agreed to dismantle all of the offensive weapons it had deployed to Turkey. There has been debate on whether Italy was also included in the agreement. While the Soviets dismantled their missiles, some Soviet bombers remained in Cuba, and the United States kept the naval quarantine in place until 20 November 1962. The blockade was formally ended on 20 November after all offensive missiles and bombers had been withdrawn from Cuba. The evident necessity of a quick and direct communication line between the two powers resulted in the Moscow\u2013Washington hotline. A series of agreements later reduced US\u2013Soviet tensions for several years.\nThe compromise embarrassed Khrushchev and the Soviet Union because the withdrawal of US missiles from Italy and Turkey was a secret deal between Kennedy and Khrushchev, and the Soviets were seen as retreating from a situation that they had started. Khrushchev's fall from power two years later was in part because of the Soviet Politburo's embarrassment at both Khrushchev's eventual concessions to the US and his ineptitude in precipitating the crisis. According to the Soviet Ambassador to the United States, Anatoly Dobrynin, the top Soviet leadership took the Cuban outcome as \"a blow to its prestige bordering on humiliation\".\nBackground.\nCuba\u2013Soviet relations.\nIn late 1961, Fidel Castro asked for more SA-2 anti-aircraft missiles from the Soviet Union. The request was not acted upon by the Soviet leadership. In the interval, Castro began criticizing the Soviets for lack of \"revolutionary boldness\", and began talking to China about agreements for economic assistance. In March 1962, Castro ordered the ousting of Anibal Escalante and his pro-Moscow comrades from Cuba's Integrated Revolutionary Organizations. This affair alarmed the Soviet leadership and raised fears of a possible US invasion. As a result, the Soviet Union sent more SA-2 anti-aircraft missiles in April, as well as a regiment of regular Soviet troops.\nHistorian Timothy Naftali has contended that Escalante's dismissal was a motivating factor behind the Soviet decision to place nuclear missiles in Cuba in 1962. According to Naftali, Soviet foreign policy planners were concerned that Castro's break with Escalante foreshadowed a Cuban drift toward China, and they sought to solidify the Soviet-Cuban relationship through the missile basing program.\nCuba\u2013US relations.\nThe Cuban government regarded US imperialism as the primary explanation for the island's structural weaknesses. The US government provided weapons, money, and its authority to the military dictatorship of Fulgencio Batista that ruled Cuba until 1958. The majority of the Cuban population had tired of the severe socioeconomic problems associated with the US domination of the country. The Cuban government was thus aware of the necessity of ending the turmoil and incongruities of US-dominated prerevolution Cuban society. It determined that the US government's demands, part of the hostile US reaction to Cuban government policy, were unacceptable.\nWith the ending of World War II and the start of the Cold War, the US government had grown concerned about the expansion of communism and sought to promote private enterprise as an instrument for advancing US strategic interests in the developing world. \nIn December 1959, under the Eisenhower administration and less than twelve months after the Cuban Revolution, the Central Intelligence Agency (CIA) developed a plan for paramilitary action against Cuba. The CIA recruited operatives on the island to carry out terrorism and sabotage, kill civilians, and cause economic damage. At the initiative of the CIA Deputy Director for Plans, Richard Bissell, and approved by the new President John F. Kennedy, the US launched the attempted Bay of Pigs Invasion in April 1961 using CIA-trained forces of Cuban expatriates. The complete failure of the invasion, and the exposure of the US government's role before the operation began, was a source of diplomatic embarrassment for the Kennedy administration. Former President Eisenhower told Kennedy that \"the failure of the Bay of Pigs will embolden the Soviets to do something that they would otherwise not do.\"\nFollowing the failed invasion, the US escalated its sponsorship of terrorism against Cuba. Starting in late 1961, using the military and the CIA, the US government engaged in an extensive campaign of state-sponsored terrorism against civilian and military targets on the island. The terrorist attacks killed significant numbers of civilians. The US armed, trained, funded and directed the terrorists, most of whom were Cuban expatriates. Terrorist attacks were planned at the direction and with the participation of US government employees and launched from US territory. In January 1962, US Air Force General Edward Lansdale described the plans to overthrow the Cuban government in a top-secret report, addressed to Kennedy and officials involved with Operation Mongoose. CIA agents or \"pathfinders\" from the Special Activities Division were to be infiltrated into Cuba to carry out sabotage and organization, including radio broadcasts. In February 1962, the US launched an embargo against Cuba, and Lansdale presented a 26-page, top-secret timetable for implementation of the overthrow of the Cuban government, mandating guerrilla operations to begin in August and September. \"Open revolt and overthrow of the Communist regime\" was hoped by the planners to occur in the first two weeks of October.\nThe terrorism campaign and the threat of invasion were crucial factors in the Soviet decision to place nuclear missiles on Cuba, and in the Cuban government's decision to accept. The US government was aware at the time, as reported to the president in a National Intelligence Estimate, that the invasion threat was a key reason for Cuban acceptance of the missiles.\nUS\u2013Soviet relations.\nWhen Kennedy ran for president in 1960, one of his key election issues was an alleged \"missile gap\" with the Soviets. In fact the US at that time was ahead of the Soviets and by an increasingly wide margin. In 1961 the Soviets had four R-7 Semyorka intercontinental ballistic missiles (ICBMs); by October 1962, some intelligence estimates indicated a figure of 75.\nThe US had 170 ICBMs and was quickly building more. It also had eight - and ballistic missile submarines, with the capability to launch 16 Polaris missiles, each with a range of . The Soviet First Secretary, Nikita Khrushchev, increased the perception of a 'missile gap' when he boasted to the world that the Soviets were building missiles \"like sausages\", but Soviet missile numbers and capabilities were nowhere close to his assertions. The Soviet Union had medium-range ballistic missiles in quantity, about 700, but they were unreliable and inaccurate. The US had a considerable advantage in total number of nuclear warheads (27,000 against 3,600) and in the technology required for accurate delivery. The US also led in missile defensive capabilities, naval and air power. The Soviets had a two-to-one advantage in conventional ground forces, particularly in field guns and tanks in the European theatre.\nKhrushchev also thought Kennedy was weak. This impression was confirmed by the President's response during the Berlin Crisis of 1961, particularly to the building of the Berlin Wall by East Germany to prevent its citizens from emigrating to the West. The half-hearted nature of the Bay of Pigs invasion reinforced his impression that Kennedy was indecisive and, as one Soviet aide wrote, \"too young, intellectual, not prepared well for decision making in crisis situations... too intelligent and too weak\". Speaking to Soviet officials in the aftermath of the crisis, Khrushchev said, \"I know for certain that Kennedy doesn't have a strong background, nor, generally speaking, does he have the courage to stand up to a serious challenge.\" He told his son Sergei that on Cuba, Kennedy \"would make a fuss, make more of a fuss, and then agree\".\nPrelude.\nConception.\nIn May 1962, Soviet First Secretary Nikita Khrushchev decided to counter the growing lead of the US in developing and deploying strategic missiles by placing Soviet intermediate-range nuclear missiles in Cuba, despite the misgivings of the Soviet Ambassador in Havana, Alexandr Ivanovich Alexeyev, who argued that Castro would not accept them. Khrushchev faced a strategic situation in which the US was perceived to have a \"splendid first strike\" capability that put the Soviet Union at a disadvantage. In 1962, the Soviets had only 20 ICBMs capable of delivering nuclear warheads to the US from inside the Soviet Union. Their poor accuracy and reliability raised serious doubts about their effectiveness. A newer, more reliable generation of Soviet ICBMs only became operational after 1965.\nSoviet nuclear capability in 1962 placed less emphasis on ICBMs than on medium and intermediate-range ballistic missiles (MRBMs and IRBMs) which could strike American allies and most of Alaska from Soviet territory, but not the contiguous United States. As Graham Allison, the director of Harvard University's Belfer Center for Science and International Affairs, pointed out, \"The Soviet Union could not right the nuclear imbalance by deploying new ICBMs on its own soil. In order to meet the threat it faced in 1962, 1963, and 1964, it had very few options. Moving existing nuclear weapons to locations from which they could reach American targets was one.\"\nA second reason that Soviet missiles were deployed to Cuba was that Khrushchev wanted to bring West Berlin, which was controlled by the American, British and French within Communist East Germany, into the Soviet orbit. The East Germans and Soviets considered western control over a portion of Berlin to be a threat to East Germany. Khrushchev made West Berlin the central battlefield of the Cold War. He believed that if the US did nothing over the deployments of missiles in Cuba, he could force the West out of Berlin by using the missiles as a deterrent to western countermeasures in Berlin. If the US tried to bargain with the Soviets after it became aware of them, Khrushchev could demand a trade of the missiles for West Berlin. Since Berlin was strategically more important than Cuba, the trade would be a win for Khrushchev, as Kennedy recognized: \"The advantage is, from Khrushchev's point of view, he takes a great chance but there are quite some rewards to it.\"\nThirdly, it seemed from the perspective both of the Soviet Union and of Cuba that the United States wanted to invade Cuba or increase its presence there. In view of actions which included an attempt to expel Cuba from the Organization of American States, a campaign of violent terrorist attacks on civilians which the US was carrying out on Cuba, economic sanctions against the country and an earlier attempt to invade the island, Cuban officials understood that America was trying to overrun their country. The USSR would respond by placing missiles on Cuba, which would secure the country against attack and keep it in the Socialist Bloc.\nAmerican missiles could have been launched from Turkey to attack the USSR before the Soviets had a chance to react. Placing nuclear missiles on Cuba would have created a balance of mutual assured destruction. If the United States launched a nuclear strike against the Soviet Union, the Soviets would have been able to react by launching a retaliatory nuclear strike against the US.\nPlacing nuclear missiles on Cuba was also a way for the USSR to show support for Cuba and the Cuban people who viewed the United States as a threat. The USSR had become Cuba's ally after the Cuban Revolution of 1959. According to Khrushchev, the Soviet Union's motives were \"aimed at allowing Cuba to live peacefully and develop as its people desire\".\nArthur M. Schlesinger Jr., a historian and adviser to Kennedy, told National Public Radio in an interview on 16 October 2002 that Castro did not want the missiles, but Khrushchev pressured him to accept them. Castro was not completely happy with the idea, but the Cuban National Directorate of the Revolution accepted them, both to protect Cuba against US attack and to aid the Soviet Union.\nSoviet military deployments.\nIn early 1962, a group of Soviet military and missile construction specialists accompanied an agricultural delegation to Havana and met Cuban prime minister Fidel Castro. According to one report, the Cuban leadership expected that the US would invade Cuba again and enthusiastically approved the idea of installing nuclear missiles on Cuba. According to another source, Castro objected to being made to look like a Soviet puppet, but was persuaded that missiles in Cuba would be an irritant to the US and would help the interests of the entire socialist camp. The deployment would include short-range tactical weapons with a range of 40km, usable only against naval vessels, that would provide a \"nuclear umbrella\" for attacks upon the island.\nBy May, Khrushchev and Castro agreed to place strategic nuclear missiles secretly in Cuba. Like Castro, Khrushchev felt that a US invasion of Cuba was imminent and that to lose Cuba would do great harm to the communists, especially in Latin America. He said he wanted to confront the Americans \"with more than words... the logical answer was missiles\". The Soviets maintained their tight secrecy, writing their plans in longhand, which were approved by Marshal of the Soviet Union Rodion Malinovsky on 4 July and Khrushchev on 7 July.\nThe Soviets' operation entailed elaborate denial and deception, known as \"maskirovka\". All the planning and preparation for transporting and deploying the missiles was carried out in the utmost secrecy, with only a very few knowing the exact nature of the mission. Even the troops detailed for the mission were given misdirection by being told that they were headed for a cold region and were outfitted with ski boots, fleece-lined parkas, and other winter equipment. The Soviet code-name was Operation Anadyr. The Anadyr River flows into the Bering Sea, and Anadyr is also the capital of Chukotsky District and a bomber base in the far eastern region. All these measures were intended to conceal the program.\nSpecialists in missile construction, under the guise of machine operators and agricultural specialists, arrived in July. A total of 43,000 foreign troops would ultimately be brought in. Chief Marshal of Artillery Sergei Biryuzov, Head of the Soviet Rocket Forces, led a survey team that visited Cuba. He told Khrushchev that the missiles would be concealed and camouflaged by palm trees. The Soviet troops would arrive in Cuba heavily underprepared. They did not know that the tropical climate would render ineffective many of their weapons and much of their equipment. In the first few days of setting up the missiles, troops complained of fuse failures, excessive corrosion, overconsumption of oil, and generator blackouts.\nAs early as August 1962, the US suspected that the Soviets were building missile facilities in Cuba. During that month, its intelligence services gathered information of sightings by ground observers of Soviet-built MiG-21 fighters and Il-28 light bombers. U-2 spy planes found S-75 Dvina (NATO designation \"SA-2\") surface-to-air missile sites at eight different locations. CIA director John A. McCone was suspicious. Sending antiaircraft missiles into Cuba, he reasoned, \"made sense only if Moscow intended to use them to shield a base for ballistic missiles aimed at the United States\". On 10 August, he wrote a memo to Kennedy in which he guessed that the Soviets were preparing to introduce ballistic missiles into Cuba. Che Guevara himself traveled to the Soviet Union on 30 August 1962, to sign the final agreement regarding the deployment of missiles in Cuba. The visit was heavily monitored by the CIA as Guevara was being watched closely by American intelligence. While in the Soviet Union, Guevara argued with Khrushchev that the missile deal should be made public but Khrushchev insisted on total secrecy, and promised the Soviet Union's support if the Americans discovered the missiles. By the time Guevara arrived in Cuba, U-2 spy planes had already discovered the Soviet troops in Cuba.\nWith important Congressional elections scheduled for November, the crisis became enmeshed in American politics. On 31 August, Senator Kenneth Keating (R-New York) warned on the Senate floor that the Soviet Union was \"in all probability\" constructing a missile base in Cuba. He charged the Kennedy administration with covering up a major threat to the US, thereby starting the crisis. He may have received this initial \"remarkably accurate\" information from his friend, former congresswoman and ambassador Clare Boothe Luce, who in turn received it from Cuban exiles. A later confirming source for Keating's information may have been the West German ambassador to Cuba, who had received information from dissidents inside Cuba that Soviet troops had arrived in Cuba in early August and were seen working \"in all probability on or near a missile base\". The ambassador passed this information to Keating on a trip to Washington in early October. Air Force General Curtis LeMay presented a pre-invasion bombing plan to Kennedy in September, and spy flights and minor military harassment from US forces at Guantanamo Bay Naval Base were the subject of continual Cuban diplomatic complaints to the US government.\nThe first consignment of Soviet R-12 missiles arrived on the night of 8 September, followed by a second on 16 September. The R-12 was a medium-range ballistic missile capable of carrying a thermonuclear warhead. It was a single-stage, road-transportable, surface-launched, storable liquid propellant-fuelled missile that could deliver a megaton-class nuclear weapon. The Soviets were building nine sites, six for R-12 medium-range missiles (NATO designation \"SS-4 Sandal\") with an effective range of and three for R-14 intermediate-range ballistic missiles (NATO designation \"SS-5 Skean\") with a maximum range of .\nOn 7 October, Cuban President Osvaldo Dortic\u00f3s Torrado spoke at the UN General Assembly: \"If... we are attacked, we will defend ourselves. I repeat, we have sufficient means with which to defend ourselves; we have indeed our inevitable weapons, the weapons, which we would have preferred not to acquire, and which we do not wish to employ.\" On 11 October in another Senate speech, Sen Keating reaffirmed his earlier warning of 31 August and stated that, \"Construction has begun on at least a half dozen launching sites for intermediate range tactical missiles.\"\nThe Cuban leadership was further upset when on 20 September, the US Senate approved Joint Resolution 230, which stated that the US was determined \"to prevent in Cuba the creation or use of an externally-supported military capability endangering the security of the United States\". On the same day, the US announced a major military exercise in the Caribbean, PHIBRIGLEX-62, which Cuba denounced as a deliberate provocation and proof that the US planned to invade Cuba.\nThe Soviet leadership believed, based on its perception of Kennedy's lack of confidence during the Bay of Pigs Invasion, that he would avoid confrontation and would accept the missiles as a . On 11 September, the Soviet Union publicly warned that a US attack on Cuba or on Soviet ships that were carrying supplies to the island would mean war. The Soviets continued the \"Maskirovka\" program to conceal their actions in Cuba. They repeatedly denied that the weapons being brought into Cuba were offensive in nature. On 7 September, Soviet Ambassador to the United States Anatoly Dobrynin assured United States Ambassador to the United Nations Adlai Stevenson that the Soviet Union was supplying only defensive weapons to Cuba. On 11 September, the Telegraph Agency of the Soviet Union (TASS: \"Telegrafnoe Agentstvo Sovetskogo Soyuza\") announced that the Soviet Union had no need or intention to introduce offensive nuclear missiles into Cuba. On 13 October, Dobrynin was questioned by former Undersecretary of State Chester Bowles about whether the Soviets planned to put offensive weapons in Cuba. He denied any such plans. On 17 October, Soviet embassy official Georgy Bolshakov brought President Kennedy a personal message from Khrushchev reassuring him that \"under no circumstances would surface-to-surface missiles be sent to Cuba.\"\nMissiles reported.\nMissiles placed in Cuba would enable the Soviets to target most of the Continental US. The planned arsenal consisted of forty launchers. The Cuban populace observed the arrival and deployment of the missiles and hundreds of reports reached Miami. US intelligence received countless reports, many of dubious quality or even laughable, most of which could be dismissed as describing defensive missiles.\nOnly five reports bothered the analysts. They described large trucks passing through towns at night that were carrying very long canvas-covered cylindrical objects and could not make turns through towns without backing up and maneuvering. Defensive missile transporters, it was believed, could make such turns without undue difficulty. The reports could not be satisfactorily dismissed.\nAerial confirmation.\nThe United States had been sending U-2 surveillance flights over Cuba since the failed Bay of Pigs Invasion. A pause in reconnaissance flights occurred on 30 August 1962 when a U-2 operated by the US Air Force's Strategic Air Command flew over Sakhalin Island in the Soviet Far East by mistake. The Soviets lodged a protest and the US apologized. Nine days later, a Taiwanese-operated U-2 was lost over western China to an SA-2 surface-to-air missile (SAM). US officials were worried that one of the Cuban or Soviet SAMs in Cuba might shoot down a CIA U-2, causing another international incident. In a meeting with members of the Committee on Overhead Reconnaissance (COMOR) on 10 September 1962, Secretary of State Dean Rusk and National Security Advisor McGeorge Bundy restricted further U-2 flights over Cuban airspace. The resulting lack of coverage over the island for the next five weeks became known to historians as the \"Photo Gap\". No significant U-2 coverage was achieved over the interior of the island during this time. US officials attempted to use a Corona photo-reconnaissance satellite to photograph reported Soviet military deployments, but the imagery acquired over western Cuba by a Corona KH-4 mission on October 1 1962 was obscured by clouds and haze and did not provide usable intelligence. At the end of September, Navy reconnaissance aircraft photographed the Soviet ship \"Kasimov\" with large crates on its deck the size and shape of Il-28 jet bomber fuselages.\nIn September 1962, analysts from the Defense Intelligence Agency (DIA) noticed that Cuban surface-to-air missile sites were arranged in a pattern similar to those used by the Soviet Union to protect ICBM bases, and the DIA lobbied for resumption of U-2 flights over the island. In the past the flights had been conducted by the CIA, but pressure from the Defense Department led to that authority being transferred to the Air Force. After the loss of a CIA U-2 over the Soviet Union in May 1960, it was thought that if another U-2 were shot down, an Air Force aircraft apparently being used for a legitimate military purpose would be easier to explain than a CIA flight.\nWhen reconnaissance missions were permitted again, on 9 October 1962, poor weather kept the planes from flying. The US first obtained U-2 photographic evidence of the Soviet missiles on 14 October 1962, when a U-2 flight piloted by Major Richard Heyser took 928 pictures on a path selected by DIA analysts, capturing images of what turned out to be an SS-4 construction site at San Crist\u00f3bal, Pinar del R\u00edo Province (now in Artemisa Province), in western Cuba.\nPresident notified.\nOn 15 October 1962, the CIA's National Photographic Interpretation Center (NPIC) reviewed the U-2 photographs and identified objects that appeared to be medium range ballistic missiles. This identification was made partly on the strength of reporting provided by Oleg Penkovsky, a double agent in the GRU working for the CIA and MI6. Although he provided no direct reports of Soviet missile deployments to Cuba, technical and doctrinal details of Soviet missile regiments that had been provided by Penkovsky in the months and years prior to the crisis helped NPIC analysts to identify the missiles in U-2 imagery.\nThat evening, the CIA notified the Department of State and at 8:30pm EDT, Bundy chose to wait until the next morning to tell the President. McNamara was briefed at midnight. The next morning, Bundy showed Kennedy the U-2 photographs and briefed him on the CIA's analysis of the images. At 6:30\u00a0pm EDT, Kennedy convened a meeting of the nine members of the National Security Council and five other key advisers, in a group he named the Executive Committee of the National Security Council (EXCOMM) after the fact on 22 October by National Security Action Memorandum 196. Without informing the members of EXCOMM, President Kennedy tape-recorded all of their proceedings, and Sheldon M. Stern, head of the Kennedy library transcribed some of them.\nOn 16 October, President Kennedy notified Attorney General Robert Kennedy that he was convinced the Soviets were placing missiles on Cuba, that it was a legitimate threat and that the possibility of nuclear destruction by two world superpowers had become a reality. Robert Kennedy responded by contacting the Soviet Ambassador, Anatoly Dobrynin. Robert Kennedy expressed his \"concern about what was happening\" and Dobrynin \"was instructed by Soviet Chairman Nikita S. Khrushchev to assure President Kennedy that there would be no ground-to-ground missiles or offensive weapons placed in Cuba\". Khrushchev further assured Kennedy that the Soviet Union had no intention of \"disrupting the relationship of our two countries\" despite the photo evidence presented before President Kennedy.\nResponses considered.\nThe US had no plan for a response in place because it had never expected that the Soviets would install nuclear missiles on Cuba. EXCOMM discussed several possible courses of action:\nThe Joint Chiefs of Staff unanimously agreed that a full-scale attack and invasion was the only solution. They believed that the Soviets would not attempt to stop the US from conquering Cuba. Kennedy was skeptical:\nKennedy concluded that attacking Cuba by air would signal the Soviets to presume \"a clear line\" to conquer Berlin. Kennedy also believed that US allies would think of the country as \"trigger-happy cowboys\" who lost Berlin because they could not peacefully resolve the Cuban situation.\nEXCOMM considered the effect on the strategic balance of power, both political and military. The Joint Chiefs of Staff believed that the missiles would seriously alter the military balance, but McNamara disagreed. An extra 40, he reasoned, would make little difference to the overall strategic balance. The US already had approximately 5,000 strategic warheads, but the Soviet Union had only 300. McNamara concluded that the Soviets having 340 would not therefore substantially alter the strategic balance. In 1990, he reiterated that \"it made \"no\" difference... The military balance wasn't changed. I didn't believe it then, and I don't believe it now.\"\nIt was agreed that the missiles would affect the \"political\" balance. Kennedy had explicitly promised the American people less than a month before the crisis that \"if Cuba should possess a capacity to carry out offensive actions against the United States... the United States would act.\" Further, US credibility among its allies and people would be damaged if the Soviet Union appeared to redress the strategic imbalance by placing missiles in Cuba. Kennedy explained after the crisis that \"it would have politically changed the balance of power. It would have appeared to, and appearances contribute to reality.\"\nOn 18 October 1962, Kennedy met Soviet Minister of Foreign Affairs Andrei Gromyko, who claimed that the weapons were for defensive purposes only. Not wanting to expose what he already knew and to avoid panicking the American public, Kennedy did not reveal that he was already aware of the missile buildup.\nOperational plans.\nTwo Operational Plans (OPLAN) were considered. OPLAN 316 envisioned a full invasion of Cuba by Army and Marine units, supported by the Navy, following Air Force and naval airstrikes. Army units in the US would have had difficulty fielding mechanised and logistical assets, and the US Navy could not supply enough amphibious shipping to transport even a modest armoured contingent from the Army.\nOPLAN 312, primarily an Air Force and Navy carrier operation, was designed with enough flexibility to do anything from engaging individual missile sites to providing air support for OPLAN 316's ground forces.\nBlockade.\nKennedy conferred with members of EXCOMM and other top advisers throughout 21 October and considered the two remaining options: an air strike primarily against the Cuban missile bases or a naval blockade of Cuba. A full-scale invasion was not the administration's first option. McNamara supported the naval blockade as a strong but limited military action that would leave the US in control. The term \"blockade\" was problematic \u2013 according to international law, a blockade is an act of war, but the Kennedy administration did not think that the Soviets would be provoked to attack by a mere blockade. Legal experts at the State Department and Justice Department concluded that a declaration of war could be avoided if another legal justification, based on the Rio Treaty for defence of the Western Hemisphere, was obtained from a resolution by a two-thirds vote from the members of the Organization of American States (OAS).\nAdmiral George Anderson, Chief of Naval Operations wrote a position paper that helped Kennedy to differentiate between what they termed a \"quarantine\" of offensive weapons and a blockade of all materials, claiming that a classic blockade was not the original intention. Since it would take place in international waters, Kennedy obtained the approval of the OAS for military action under the hemispheric defence provisions of the Rio Treaty:\nOn 19 October, the EXCOMM formed separate working groups to examine the air strike and blockade options, and by the afternoon most support in the EXCOMM had shifted to a blockade. Reservations about the plan continued to be voiced as late as 21 October, the paramount concern being that once the blockade was put into effect, the Soviets would rush to complete some of the missiles and the US could find itself bombing operational missiles if the blockade had not already forced their removal.\nSpeech to the nation.\nAt 3:00\u00a0pm EDT on 22 October 1962, President Kennedy formally established the executive committee (EXCOMM) with National Security Action Memorandum (NSAM) 196. At 5:00\u00a0pm, he met Congressional leaders, who opposed a blockade and demanded a stronger response. In Moscow, US Ambassador Foy D. Kohler briefed Khrushchev on the pending blockade and Kennedy's speech to the nation. Ambassadors around the world gave notice to non-Eastern Bloc leaders. Before the speech, US delegations met Canadian Prime Minister John Diefenbaker, British Prime Minister Harold Macmillan, West German Chancellor Konrad Adenauer, French President Charles de Gaulle and Secretary-General of the Organization of American States, Jos\u00e9 Antonio Mora to brief them on this intelligence and the US's proposed response. All were supportive of the US position. Over the course of the crisis, Kennedy had daily telephone conversations with Macmillan, who was publicly supportive of US actions.\nShortly before his speech, Kennedy telephoned former President Dwight Eisenhower. Kennedy's conversation with the former president also revealed that the two had been consulting during the Cuban Missile Crisis. The two also anticipated that Khrushchev would respond to the Western world in a manner similar to his response during the Suez Crisis, and would possibly wind up trading off Berlin.\nAt 7:00\u00a0pm EDT on 22 October, Kennedy delivered a nationwide televised address on all of the major networks announcing the discovery of the missiles. He noted:\nKennedy described the administration's plan:\nDuring the speech, a directive went out to all US forces worldwide, placing them on DEFCON 3. The heavy cruiser was the designated flagship for the blockade, with as \"Newport News\"s destroyer escort. Kennedy's speech writer Ted Sorensen stated in 2007 that the address to the nation was \"Kennedy's most important speech historically, in terms of its impact on our planet.\"\nCrisis deepens.\nAt 11:24\u00a0am EDT on 24 October , a cable from US Secretary of State George Ball to the US Ambassadors in Turkey and NATO notified them that they were considering making an offer to withdraw missiles from Italy and Turkey in exchange for Soviet withdrawal from Cuba. Turkish officials replied that they would \"deeply resent\" any trade involving the US missile presence in their country. One day later, on the morning of 25 October, American journalist Walter Lippmann proposed the same thing in his syndicated column. Castro reaffirmed Cuba's right to self-defense and said that all of its weapons were defensive and Cuba would not allow an inspection.\nInternational response.\nThree days after Kennedy's speech, the Chinese \"People's Daily\" announced that \"650,000,000 Chinese men and women were standing by the Cuban people.\" In West Germany, newspapers supported the US response by contrasting it with the weak American actions in the region during the preceding months. They also expressed some fear that the Soviets might retaliate in Berlin. In France on 23 October, the crisis made the front page of all the daily newspapers. The next day, an editorial in \"Le Monde\" expressed doubt about the authenticity of the CIA's photographic evidence. Two days later, after a visit by a high-ranking CIA agent, the newspaper accepted the validity of the photographs. In the 29 October issue of \"Le Figaro\", Raymond Aron wrote in support of the American response. On 24 October, Pope John XXIII sent a message to the Soviet embassy in Rome, to be transmitted to the Kremlin, in which he voiced his concern for peace. In this message he stated, \"We beg all governments not to remain deaf to this cry of humanity. That they do all that is in their power to save peace.\"\nSoviet broadcast and communications.\nThe crisis continued unabated, and on the evening of 24 October 1962, the Soviet TASS news agency broadcast a telegram from Khrushchev to Kennedy in which Khrushchev warned that the United States' \"outright piracy\" would lead to war. Khrushchev then sent at 9:24\u00a0pm a telegram to Kennedy, which was received at 10:52\u00a0pm EDT. Khrushchev stated, \"if you weigh the present situation with a cool head without giving way to passion, you will understand that the Soviet Union cannot afford not to decline the despotic demands of the USA\". The Soviet Union viewed the blockade as \"an act of aggression\" and their ships would be instructed to ignore it. After 23 October, Soviet communications with the US increasingly showed indications of having been rushed. Undoubtedly a result of pressure, it was not uncommon for Khrushchev to repeat himself and to send messages lacking basic editing. With President Kennedy making known his aggressive intentions of a possible airstrike followed by an invasion on Cuba, Khrushchev sought a diplomatic compromise. Communications between the two superpowers had entered a new and revolutionary period, with the threat of mutual destruction now accompanying the deployment of nuclear weapons.\nUS alert level raised.\nThe US requested an emergency meeting of the United Nations Security Council on 25 October and Ambassador to the United Nations, Adlai Stevenson, confronted Soviet Ambassador Valerian Zorin and challenged him to admit the existence of the missiles. Ambassador Zorin refused to answer. At 10:00\u00a0pm EDT the next day, the US raised the readiness level of Strategic Air Command (SAC) forces to DEFCON 2. For the only confirmed time in US history, B-52 bombers were put on continuous airborne alert. B-47 medium bombers were dispersed to military and civilian airfields and made ready to take off, fully equipped, at 15\u00a0minutes' notice. One-eighth of SAC's 1,436 bombers were on airborne alert. Some 145 intercontinental ballistic missiles, some of which targeted Cuba, were placed on alert. Air Defense Command (ADC) redeployed 161 nuclear-armed interceptors to 16 dispersal fields within nine hours, with one third on 15-minute alert status. Twenty-three nuclear-armed B-52 bombers were sent to orbit points within striking distance of the Soviet Union to demonstrate that the US was serious. Jack J. Catton later estimated that about 80 per cent of SAC's planes were ready for launch during the crisis. David A. Burchinal recalled that, by contrast:\nBy 22 October, Tactical Air Command (TAC) had 511 fighters plus supporting tankers and reconnaissance aircraft deployed to face Cuba on one-hour alert status. TAC and the Military Air Transport Service had problems: the concentration of aircraft in Florida strained command and support echelons, which were facing critical undermanning in security, armaments, and communications. Absence of permission to use war-reserve stocks of conventional munitions forced TAC to scrounge supplies, and the lack of airlift assets to support a major airborne drop necessitated the call-up of 24 reserve squadrons.\nOn 25 October at 1:45\u00a0am EDT, Kennedy responded to Khrushchev's telegram by stating that the US was forced into action after receiving repeated false assurances that no offensive missiles were being placed in Cuba. Deployment of the missiles \"required the responses I have announced... I hope that your government will take necessary action to permit a restoration of the earlier situation.\"\nBlockade challenged.\nAt 7:15\u00a0am EDT on 25 October, and attempted to intercept \"Bucharest\" but failed to do so. Fairly certain that the tanker did not contain any military material, the US allowed it through the blockade. Later that day, at 5:43\u00a0pm, the commander of the blockade ordered the destroyer to intercept and board the Lebanese freighter \"Marucla\". That took place the next day, and \"Marucla\" was cleared through the blockade after its cargo was checked.\nAt 5:00\u00a0pm EDT on 25 October, William Clements announced that the missiles in Cuba were still being worked on. This was later verified by a CIA report that suggested there had been no slowdown. In response, Kennedy issued Security Action Memorandum 199, authorizing the loading of nuclear weapons onto aircraft under the command of SACEUR, which had the duty of carrying out first air strikes on the Soviet Union. Kennedy claimed that the blockade had succeeded when the USSR turned back fourteen ships presumed to be carrying offensive weapons. The first indication of this was in a report from British GCHQ sent to the White House Situation Room which contained intercepted communications from Soviet ships reporting their positions. On 24 October, \"Kislovodsk,\" a Soviet cargo ship, reported a position north-east of where it had been 24 hours earlier, indicating it had \"discontinued\" its voyage and turned back towards the Baltic. The next day, further reports showed that more ships originally bound for Cuba had altered their course.\nRaising the stakes.\nThe next morning, 26 October, Kennedy informed EXCOMM that he believed only an invasion would remove the missiles from Cuba. He was persuaded to wait and continue with military and diplomatic pressure. He agreed and ordered low-level flights over the island to be increased from two per day to every two hours. He also ordered a crash program to institute a new civil government in Cuba if an invasion went ahead.\nAt this point the crisis appeared to be at a stalemate. The Soviets had shown no indication that they would back down and had made public media and private inter-government statements to that effect. The US had no reason to disbelieve them and was in the early stages of preparing an invasion of Cuba and a nuclear strike on the Soviet Union if it responded militarily, which the US assumed it would. Kennedy had no intention of keeping these plans secret, and with an array of Cuban and Soviet spies present Khrushchev was made aware of them.\nThe implicit threat of air strikes on Cuba followed by an invasion allowed the United States to exert pressure in future talks, and the prospect of military action helped to accelerate Khrushchev's proposal for a compromise. Throughout the closing stages of October 1962, Soviet communications to the United States became increasingly defensive, and Khrushchev's tendency to use poorly phrased and ambiguous language during negotiations increased the United States' confidence and clarity in messaging. Leading Soviet figures failed to mention that only the Cuban government could agree to inspections of the territory, and continued to make arrangements relating to Cuba without Castro's knowledge. According to Dean Rusk, Khrushchev \"blinked\": he began to panic from the consequences of his own plan, and this was reflected in the tone of Soviet messages. This allowed the US to dominate negotiations in late October.\nThe escalating situation also caused Khrushchev to abandon plans for a possible Warsaw Pact invasion of Albania, which was being discussed in the Eastern Bloc following the Vlora incident the previous year.\nSecret negotiations.\nAt 1:00\u00a0pm EDT on 26 October, John A. Scali of ABC News met Aleksandr Fomin, the cover name of Alexander Feklisov, the KGB station chief in Washington, at Fomin's request. Following the instructions of the Politburo of the CPSU, Fomin noted, \"War seems about to break out.\" He asked Scali to use his contacts to talk to his \"high-level friends\" at the State Department to see if the US would be interested in a diplomatic solution. He suggested that the language of the deal would contain an assurance from the Soviet Union to remove the weapons under UN supervision and that Castro would publicly announce that he would not accept such weapons again, in exchange for a public statement by the US that it would not invade Cuba. The US responded by asking the Brazilian government to pass a message to Castro that the US would be \"unlikely to invade\" if the missiles were removed.\nAt 6:00\u00a0pm EDT on 26 October, the State Department started receiving a message that appeared to be written personally by Khrushchev. It was Saturday 2:00\u00a0am in Moscow. The long letter took several minutes to arrive, and it took translators additional time to translate and transcribe it.\nRobert F. Kennedy described the letter as \"very long and emotional\". Khrushchev reiterated the basic outline that had been stated to Scali earlier in the day: \"I propose: we, for our part, will declare that our ships bound for Cuba are not carrying any armaments. You will declare that the United States will not invade Cuba with its troops and will not support any other forces which might intend to invade Cuba. Then the necessity of the presence of our military specialists in Cuba will disappear.\" At 6:45\u00a0pm EDT, news of Fomin's offer to Scali was finally heard and was interpreted as a \"set up\" for the arrival of Khrushchev's letter. The letter was then considered official and accurate, although it was later learned that Fomin was almost certainly operating without official backing. Additional study of the letter was ordered and continued into the night.\nCrisis continues.\nCastro, on the other hand, was convinced that an invasion of Cuba was imminent, and on 26 October he sent a telegram to Khrushchev that appeared to call for a pre-emptive nuclear strike on the US in case of attack. In a 2010 interview, Castro expressed regret about his 1962 stance on first use: \"After I've seen what I've seen, and knowing what I know now, it wasn't worth it at all.\" Castro also ordered all anti-aircraft weapons in Cuba to fire on any US aircraft. Previous orders had been to fire only on groups of two or more. At 6:00\u00a0am EDT on 27 October, the CIA delivered a memo reporting that three of the four missile sites at San Cristobal and both sites at Sagua la Grande appeared to be fully operational. It also noted that the Cuban military continued to organise for action but was under order not to act unless attacked.\nAt 9:00\u00a0am EDT on 27 October, Radio Moscow began broadcasting a message from Khrushchev. Contrary to the letter of the night before, the message offered a new trade: the missiles on Cuba would be removed in exchange for the removal of the Jupiter missiles from Italy and Turkey. At 10:00\u00a0am EDT, the executive committee met again to discuss the situation and came to the conclusion that the change in the message was because of internal debate between Khrushchev and other party officials in the Kremlin. Kennedy realised that he would be in an \"insupportable position if this becomes Khrushchev's proposal\" because the missiles in Turkey were not militarily useful and were being removed anyway, and \"It's gonna \u2013 to any man at the United Nations or any other rational man, it will look like a very fair trade.\" Bundy explained why Khrushchev's public acquiescence could not be considered: \"The current threat to peace is not in Turkey, it is in Cuba.\"\nMcNamara noted that another tanker, the \"Grozny\", was about out and should be intercepted. He also noted that they had not made the Soviets aware of the blockade line and suggested relaying that information to them via U Thant at the United Nations.\nWhile the meeting progressed, at 11:03\u00a0am EDT a new message began to arrive from Khrushchev. The message stated, in part:\n\"You are disturbed over Cuba. You say that this disturbs you because it is ninety-nine miles by sea from the coast of the United States of America. But... you have placed destructive missile weapons, which you call offensive, in Italy and Turkey, literally next to us... I therefore make this proposal: We are willing to remove from Cuba the means which you regard as offensive... Your representatives will make a declaration to the effect that the United States... will remove its analogous means from Turkey... and after that, persons entrusted by the United Nations Security Council could inspect on the spot the fulfillment of the pledges made.\"\nThe executive committee continued to meet through the day.\nThroughout the crisis, Turkey had repeatedly stated that it would be upset if the Jupiter missiles were removed. Italy's Prime Minister Amintore Fanfani, who was also Foreign Minister \"ad interim\", offered to allow withdrawal of the missiles deployed in Apulia as a bargaining chip. He gave the message to one of his most trusted friends, Ettore Bernabei, general manager of RAI-TV, to convey to Arthur M. Schlesinger Jr. Bernabei was in New York to attend an international conference on satellite TV broadcasting.\nOn the morning of 27 October, a U-2F (the third CIA U-2A, modified for air-to-air refuelling) piloted by USAF Major Rudolf Anderson, departed its forward operating location at McCoy AFB, Florida. At approximately 12:00\u00a0pm EDT, the aircraft was struck by an SA-2 surface-to-air missile launched from Cuba. The aircraft crashed, and Anderson was killed. Stress in negotiations between the Soviets and the US intensified; only later was it assumed that the decision to fire the missile was made locally by an undetermined Soviet commander, acting on his own authority. Later that day, at about 3:41\u00a0pm EDT, several US Navy RF-8A Crusader aircraft, on low-level photo-reconnaissance missions, were fired upon.\nAt 4:00\u00a0pm EDT, Kennedy recalled members of EXCOMM to the White House and ordered that a message should immediately be sent to U Thant asking the Soviets to suspend work on the missiles while negotiations were carried out. During the meeting, General Maxwell Taylor delivered the news that the U-2 had been shot down. Kennedy had earlier claimed he would order an attack on such sites if fired upon, but he decided to not act unless another attack was made. On 28 October 1962, Khrushchev told his son Sergei that the shooting down of Anderson's U-2 was by the \"Cuban military at the direction of Ra\u00fal Castro\". \nOn 27 October Bobby Kennedy relayed a message to the Soviet Ambassador that President Kennedy was under pressure from the military to use force against Cuba and that \"an irreversible chain of events could occur against his will\" as \"the president is not sure that the military will not overthrow him and seize power\". He therefore implored Khrushchev to accept Kennedy's proposed agreement. \nForty years later, McNamara said:\nDaniel Ellsberg said that Robert Kennedy (RFK) told him in 1964 that after the U-2 was shot down and the pilot killed, he (RFK) told Soviet ambassador Dobrynin, \"You have drawn first blood ... . [T]he president had decided against advice ... not to respond militarily to that attack, but he [Dobrynin] should know that if another plane was shot at, ... we would take out all the SAMs and anti-aircraft ... . And that would almost surely be followed by an invasion.\"\nDrafting response.\nEmissaries sent by both Kennedy and Khrushchev agreed to meet at the Yenching Palace Chinese restaurant in the Cleveland Park neighbourhood of Washington, DC, on Saturday evening, 27 October. Kennedy suggested taking Khrushchev's offer to trade away the missiles. Unknown to most members of the EXCOMM, but with the support of his brother the president, Robert Kennedy had been meeting the Soviet Ambassador Dobrynin in Washington to discover whether the intentions were genuine. The EXCOMM was against the proposal because it would undermine NATO's authority, and the Turkish government had repeatedly stated that it was against any such trade.\nAs the meeting progressed, a new plan emerged, and Kennedy was slowly persuaded. The new plan called for him to ignore the latest message and instead to return to Khrushchev's earlier one. Kennedy was initially hesitant, feeling that Khrushchev would no longer accept the deal because a new one had been offered, but Llewellyn Thompson argued that it was still possible. White House Special Counsel and Adviser Ted Sorensen and Robert Kennedy left the meeting and returned 45\u00a0minutes later, with a draft letter to that effect. The President made several changes, had it typed, and sent it.\nAfter the EXCOMM meeting, a smaller meeting continued in the Oval Office. The group argued that the letter should be underscored with an oral message to Dobrynin that stated that if the missiles were not withdrawn, military action would be used to remove them. Rusk added one proviso that no part of the language of the deal would mention Turkey, but there would be an understanding that the missiles would be removed \"voluntarily\" in the immediate aftermath. The president agreed, and the message was sent.\nAt Rusk's request, Fomin and Scali met again. Scali asked why the two letters from Khrushchev were so different, and Fomin claimed it was because of \"poor communications\". Scali replied that the claim was not credible and shouted that he thought it was a \"stinking double cross\". He went on to claim that an invasion was only hours away, and Fomin stated that a response to the US message was expected from Khrushchev shortly and urged Scali to tell the State Department that no treachery was intended. Scali said that he did not think anyone would believe him, but he agreed to deliver the message. The two went their separate ways, and Scali immediately typed out a memo for the EXCOMM.\nWithin the US establishment, it was understood that ignoring the second offer and returning to the first put Khrushchev in a terrible position. Military preparations continued, and all active duty Air Force personnel were recalled to their bases for possible action. Robert Kennedy later recalled the mood: \"We had not abandoned all hope, but what hope there was now rested with Khrushchev's revising his course within the next few hours. It was a hope, not an expectation. The expectation was military confrontation by Tuesday [30 October], and possibly tomorrow [29 October] ...\"\nAt 8:05\u00a0pm EDT, the letter drafted earlier in the day was delivered. The message read, \"As I read your letter, the key elements of your proposals\u2014which seem generally acceptable as I understand them\u2014are as follows: 1) You would agree to remove these weapons systems from Cuba under appropriate United Nations observation and supervision; and undertake, with suitable safe-guards, to halt the further introduction of such weapon systems into Cuba. 2) We, on our part, would agree\u2014upon the establishment of adequate arrangements through the United Nations, to ensure the carrying out and continuation of these commitments (a) to remove promptly the quarantine measures now in effect and (b) to give assurances against the invasion of Cuba.\" The letter was also released directly to the press to ensure it could not be \"delayed\". With the letter delivered, a deal was on the table. As Robert Kennedy noted, there was little expectation it would be accepted. At 9:00\u00a0pm EDT, the EXCOMM met again to review the actions for the following day. Plans were drawn up for air strikes on the missile sites as well as other economic targets, notably petroleum storage. McNamara stated that they had to \"have two things ready: a government for Cuba, because we're going to need one; and secondly, plans for how to respond to the Soviet Union in Europe, because sure as hell they're going to do something there\".\nAt 12:12\u00a0am EDT, on 27 October, the US informed its NATO allies that \"the situation is growing shorter... the United States may find it necessary within a very short time in its interest and that of its fellow nations in the Western Hemisphere to take whatever military action may be necessary.\" To add to the concern, at 6:00\u00a0am, the CIA reported that all missiles in Cuba were ready for action.\nOn 27 October, Khrushchev also received a letter from Castro, what is now known as the Armageddon Letter (dated the day before), which urged the use of nuclear force in the event of an attack on Cuba: \"I believe the imperialists' aggressiveness is extremely dangerous and if they actually carry out the brutal act of invading Cuba in violation of international law and morality, that would be the moment to eliminate such danger forever through an act of clear legitimate defense, however harsh and terrible the solution would be,\" Castro wrote.\nAverted nuclear launch.\nLater that same day, what the White House later called \"Black Saturday\", the US Navy dropped a series of \"signalling\" depth charges (\"practice\" depth charges the size of hand grenades) on a Soviet submarine () at the blockade line, unaware that it was armed with a nuclear-tipped torpedo that could be launched if the submarine was damaged by depth charges or surface fire. The submarine was too deep to monitor radio traffic and the captain of the \"B-59\", Valentin Grigoryevich Savitsky, assuming after live ammunition fire at his submarine that a war had started, proposed to launch the nuclear torpedo at the US ships. The decision to launch the \"special weapon\" normally only required the agreement of the ship's commanding officer and political officer, but the commander of the submarine flotilla, Vasily Arkhipov, was aboard \"B-59\" and he also had to agree. Arkhipov did not give his consent and the nuclear torpedo was not launched. (These events only became publicly known in 2002. See Submarine close call.) \nOn the same day a U-2 spy plane made an accidental and unauthorised 90-minute overflight of the Soviet Union's far eastern coast. The Soviets responded by scrambling MiG fighters from Wrangel Island; in turn, the Americans launched F-102 fighters armed with nuclear air-to-air missiles over the Bering Sea.\nResolution.\nOn Saturday, 27 October, after much deliberation between the Soviet Union and Kennedy's cabinet, Kennedy secretly agreed to remove all missiles in Turkey, on the border of the Soviet Union, and possibly those in southern Italy, in exchange for Khrushchev removing all missiles in Cuba. There is some dispute as to whether removing the missiles from Italy was part of the secret agreement. Khrushchev wrote in his memoirs that it was, and when the crisis had ended McNamara gave the order to dismantle the missiles in both Italy and Turkey.\nAt this point, Khrushchev knew things the US did not. First, that the shooting down of the U-2 by a Soviet missile violated direct orders from Moscow, and Cuban anti-aircraft fire against other US reconnaissance aircraft also violated direct orders from Khrushchev to Castro. Second, the Soviets already had 162 nuclear warheads on Cuba that the US did not know were there. Third, the Soviets and Cubans on the island would almost certainly have responded to an invasion by using them, even though Castro believed that everyone in Cuba would die as a result. Khrushchev also knew, but may not have considered, that he had submarines nearby armed with nuclear weapons of which the US Navy may not have been aware.\nKhrushchev knew he was losing control. President Kennedy had been told in early 1961 that a nuclear war would probably kill a third of humanity, with most or all of those deaths concentrated in the US, the USSR, Europe and China, and Khrushchev may have received a similar estimate.\nWith this background, when Khrushchev heard of Kennedy's threats as relayed by Robert Kennedy to Soviet Ambassador Dobrynin, he immediately drafted his acceptance of Kennedy's latest terms from his dacha without involving the Politburo, as he had previously, and had them immediately broadcast over Radio Moscow, which he believed the US would hear. In that broadcast at 9:00\u00a0am EST, on 28 October 1962, Khrushchev stated that \"the Soviet government, in addition to previously issued instructions on the cessation of further work at the building sites for the weapons, has issued a new order on the dismantling of the weapons which you describe as 'offensive' and their crating and return to the Soviet Union.\" At 10:00\u00a0am on 28 October, Kennedy first learned of Khrushchev's solution to the crisis: the US would remove the 15 Jupiters in Turkey and the Soviets would remove the missiles from Cuba. Khrushchev had made the offer in a public statement for the world to hear. Despite almost solid opposition from his senior advisers, Kennedy accepted the Soviet offer. \"This is a pretty good play of his,\" Kennedy said, according to a tape recording that he made secretly of the Cabinet Room meeting. Kennedy had deployed the Jupiters in March 1962, causing a stream of angry outbursts from Khrushchev. \"Most people will think this is a rather even trade and we ought to take advantage of it,\" Kennedy said. Vice President Lyndon Johnson was the first to endorse the missile swap, but others continued to oppose it. Finally, Kennedy ended the debate. \"We can't very well invade Cuba with all its toil and blood,\" Kennedy said, \"when we could have gotten them out by making a deal on the same missiles on Turkey. If that's part of the record, then you don't have a very good war.\"\nKennedy immediately responded to Khrushchev's letter, issuing a statement calling it \"an important and constructive contribution to peace\". He continued this with a formal letter:\nKennedy's planned statement would also contain suggestions he had received from his adviser Arthur Schlesinger Jr. in a \"Memorandum for the President\" describing the \"Post Mortem on Cuba\".\nOn 28 October, Kennedy participated in telephone conversations with Eisenhower and fellow former US President Harry Truman. In these calls, Kennedy revealed that he thought the crisis would result in the two superpowers being \"toe to toe\" in Berlin by the end of the following month and expressed concern that the Soviet setback in Cuba would \"make things tougher\" there. He also informed his predecessors that he had rejected the public Soviet offer to withdraw from Cuba in exchange for the withdrawal of US missiles from Turkey.\nThe US continued the blockade of Cuba. In the following days aerial reconnaissance showed that the Soviets were making progress in removing the missile systems. The 42 missiles and their support equipment were loaded onto eight Soviet ships. On 2 November 1962, Kennedy addressed the US via radio and television broadcasts concerning the dismantling of the Soviet R-12 missile bases located in the Caribbean. The ships left Cuba on November 5 to 9. The US made a final visual check as each of the ships passed the blockade line. Further diplomatic efforts were required to remove the Soviet Il-28 bombers, and they were loaded on three Soviet ships on 5 and 6 December. Concurrent with the Soviet commitment on the Il-28s, the US government announced the end of the blockade from 6:45\u00a0pm EST on 20 November 1962.\nAt the time when the Kennedy administration believed that the Cuban Missile Crisis was resolved, nuclear tactical rockets remained in Cuba which were not part of the Kennedy-Khrushchev understanding and the Americans did not know about them. The Soviets changed their minds, fearing possible future Cuban militant steps, and on 22 November 1962, Deputy Premier of the Soviet Union Anastas Mikoyan told Castro that the rockets with the nuclear warheads were being removed as well.\nThe Cuban Missile Crisis was solved in part by a secret agreement between John F. Kennedy and Nikita Khrushchev. The Kennedy-Khrushchev Pact was known to only nine US officials at the time of its creation in October 1963 and was first officially acknowledged at a conference in Moscow in January 1989 by Soviet Ambassador Anatoly Dobrynin and Kennedy's speechwriter Theodore Sorensen. In his negotiations with Dobrynin, Robert Kennedy informally proposed that the Jupiter missiles in Turkey would be removed \"within a short time after this crisis was over\". Under an operation code-named \"Operation Pot Pie,\" the removal of the Jupiters from Italy and Turkey began on 1 April, and was completed by 24 April 1963. The initial plans were to recycle the missiles for use in other programs, but NASA and the USAF were not interested in retaining the missile hardware. The missile bodies were destroyed on site, while warheads, guidance packages, and launching equipment worth $14 million were returned to the United States. The dismantling operations were named Pot Pie I for Italy and Pot Pie II for Turkey by the United States Air Force.\nThe outcome of the Kennedy-Khrushchev Pact was that the US would remove their rockets from Italy and Turkey and that the Soviets had no intention of resorting to nuclear war if they were out-gunned by the US. Because the withdrawal of the Jupiter missiles from NATO bases in Italy and Turkey was not made public at the time, Khrushchev appeared to have lost the conflict and become weakened. The perception was that Kennedy had won the contest between the superpowers and that Khrushchev had been humiliated. Both Kennedy and Khrushchev took every step to avoid full conflict despite pressures from their respective governments. Khrushchev held power for another two years. As a direct result of the crisis, the United States and the Soviet Union set up a direct line of communication. The hotline between the Soviet Union and the United States was a way for the President and the Premier to have negotiations should a crisis like this ever happen again.\nNuclear forces.\nBy the time of the crisis in October 1962, the total number of nuclear weapons in the stockpiles of each country numbered approximately 26,400 for the United States and 3,300 for the Soviet Union. For the US, around 3,500 (with a combined yield of approximately 6,300 megatons) would have been used in attacking the Soviet Union. The Soviets had considerably less strategic firepower at their disposal: some 300\u2013320 bombs and warheads, without submarine-based weapons in a position to threaten the US mainland and most of their intercontinental delivery systems based on bombers that would have difficulty penetrating North American air defence systems. They had already moved 158 warheads to Cuba and between 95 and 100 would have been ready for use if the US had invaded Cuba, most of them short-range. The US had approximately 4,375 nuclear weapons deployed in Europe, most of which were tactical weapons such as nuclear artillery, with around 450 of them for ballistic missiles, cruise missiles, and aircraft; the Soviets had more than 550 similar weapons in Europe.\nAftermath.\nCuban leadership.\nDecisions on how to resolve the crisis had been made exclusively by Kennedy and Khrushchev and Cuba perceived the outcome as a betrayal by the Soviets. Castro was especially upset that certain questions of interest to Cuba, such as the status of the US Naval Base in Guant\u00e1namo, were not addressed, and Cuban\u2013Soviet relations deteriorated.\nHistorian Arthur Schlesinger believed that when the missiles were withdrawn, Castro was more angry with Khrushchev than with Kennedy because Khrushchev had not consulted him before making the decision. Although Castro was infuriated by Khrushchev, he had still planned to strike the US with the remaining missiles if Cuba was invaded.\nA few weeks after the crisis, during an interview with British communist newspaper the \"Daily Worker\", Guevara was still fuming over the perceived Soviet betrayal and told correspondent Sam Russell that, if the missiles had been under Cuban control they would have been launched. Guevara said later that the cause of socialist liberation from global \"imperialist aggression\" would have been worth the possibility of \"millions of atomic war victims\". The missile crisis further convinced Guevara that the world's two superpowers, the United States and the Soviet Union, were using Cuba as a pawn in their global strategies, and after this he denounced the Soviets almost as frequently as he denounced the Americans.\nRomanian leadership.\nDuring the crisis, Gheorghe Gheorghiu-Dej, general secretary of Romania's communist party, sent a letter to President Kennedy dissociating Romania from Soviet actions. This convinced the American administration of Bucharest's intentions of detaching itself from Moscow.\nSoviet leadership.\nThe realisation that the world had come close to thermonuclear war caused Khrushchev to propose a far-reaching easing of tensions with the US. In a letter to President Kennedy dated 30 October 1962, Khrushchev suggested initiatives that were intended to prevent the possibility of another nuclear crisis. These included a non-aggression treaty between the North Atlantic Treaty Organization (NATO) and the Warsaw Pact, or even disbanding these military blocs; a treaty to cease all nuclear weapons testing and possibly eliminate all nuclear weapons; resolution of the difficult question of Germany by both sides accepting the existence of West Germany and East Germany; and US recognition of the government of mainland China. The letter invited counter-proposals and further exploration of these and other questions through peaceful negotiations. Khrushchev invited Norman Cousins, the editor of a major US periodical and an anti-nuclear weapons activist, to serve as liaison with Kennedy. Cousins met with Khrushchev for four hours in December 1962.\nKennedy's response to Khrushchev's proposals was lukewarm, but he told Cousins that he felt obliged to consider them because he was under pressure from hardliners in the US national security apparatus. The United States and the Soviet Union subsequently agreed to a treaty banning atmospheric testing of nuclear weapons, known as the \"Partial Nuclear Test Ban Treaty\". The US and the USSR also created a communications link, the Moscow\u2013Washington hotline, to enable the leaders of the two Cold War countries to speak directly to each other in any future crisis.\nThese compromises embarrassed Khrushchev and the Soviet Union because the withdrawal of US missiles from Italy and Turkey had remained a secret deal between Kennedy and Khrushchev. Khrushchev went to Kennedy because he thought that the crisis was getting out of hand, but the Soviets were seen to be retreating from circumstances that they had started.\nKhrushchev's fall from power two years later was partly because of the Soviet Politburo's embarrassment at his eventual concessions to the US and his ineptitude in precipitating the crisis in the first place. According to Dobrynin, the top Soviet leadership took the Cuban outcome as \"a blow to its prestige bordering on humiliation\".\nUS leadership.\nThe worldwide DEFCON 3 status of US Forces was returned to DEFCON 4 on 20 November 1962. General Curtis LeMay told Kennedy that the resolution of the crisis was the \"greatest defeat in our history\" but his was a minority view. LeMay had pressed for an immediate invasion of Cuba as soon as the crisis began, and he still favored invading Cuba even after the Soviets had withdrawn their missiles. Twenty-five years later, LeMay still believed that \"We could have gotten not only the missiles out of Cuba, we could have gotten the Communists out of Cuba at that time.\"\nBy 1962, President Kennedy had faced four crisis situations: the failure of the Bay of Pigs Invasion; settlement negotiations between the pro-Western government of Laos and the Pathet Lao communist movement (\"Kennedy sidestepped Laos, whose rugged terrain was no battleground for American soldiers.\"); the construction of the Berlin Wall; and the Cuban Missile Crisis. Kennedy believed that another failure to gain control and stop communist expansion would irreparably damage US credibility. He was determined to \"draw a line in the sand\" and prevent a communist victory in Vietnam. He told James Reston of \"The New York Times\" immediately after his Vienna summit meeting with Khrushchev, \"Now we have a problem making our power credible and Vietnam looks like the place.\"\nAt least four contingency strikes were armed and launched from Florida against Cuban airfields and suspected missile sites in 1963 and 1964, although all were diverted to the Pinecastle Range Complex after the planes had passed Andros island. Critics, including Seymour Melman and Seymour Hersh, suggested that the Cuban Missile Crisis had encouraged the United States to use military means, as in the later Vietnam War. Similarly, Lorraine Bayard de Volo suggested that the masculine brinksmanship of the Cuban Missile Crisis had become a \"touchstone of toughness by which presidents are measured\". Actions in 1962 had a significant influence on the policy decisions of future occupants of the White House, and led to foreign policy decisions such as President Lyndon B. Johnson's escalation of the war in Vietnam three years later.\nHuman casualties.\nThe body of U-2 pilot Anderson was returned to the US and was buried with full military honours in South Carolina. He was the first recipient of the newly created Air Force Cross, which was awarded posthumously. Although Anderson was the only combatant fatality during the crisis, 11 crew members of three reconnaissance Boeing RB-47 Stratojets of the 55th Strategic Reconnaissance Wing were also killed in crashes during the period between 27 September and 11 November 1962. Seven crew died when a Military Air Transport Service Boeing C-135B Stratolifter delivering ammunition to Guantanamo Bay Naval Base stalled and crashed on landing approach on 23 October.\nLater revelations.\nSubmarine close call.\nWhat may have been the most dangerous moment in the crisis was not recognized until the Cuban Missile Crisis Havana conference in October 2002, which marked its 40th anniversary. The three-day conference was sponsored by the private National Security Archive, Brown University and the Cuban government and attended by many of the veterans of the crisis. They learned that on 27 October 1962, a group of eleven United States Navy destroyers and the aircraft carrier USS \"Randolph\" had located a diesel-powered, nuclear-armed Soviet Project 641 (NATO designation ) submarine, the , near Cuba. Despite being in international waters, the Americans started dropping depth charges to attempt to force the submarine to surface. \nThere had been no contact from Moscow for a number of days and the submarine was running too deep to monitor radio traffic, so those on board did not know whether war had broken out. The captain of the submarine, Valentin Savitsky, had no way of knowing that the depth charges were non-lethal \"practice\" rounds, intended as warning shots to force him to surface. Running out of air, the Soviet submarine was surrounded by American warships and desperately needed to surface. While surfacing, the \"B-59\" \u201ccame under machine-gun fire from [U.S. ASW S-2] Tracker aircraft. The fire rounds landed either to the sides of the submarine\u2019s hull or near the bow. All these provocative actions carried out by surface ships in immediate proximity, and ASW aircraft flying some 10 to 15 meters above the boat had a detrimental impact on the commander, prompting him to take extreme measures\u2026 the use of special weapons.\u201d As firing live ammunition at a submarine was strictly prohibited, Captain Savitsky assumed that his submarine was doomed and that World War III had started. The Americans, for their part, did not know, that the \"B-59\" was armed with a 15-kiloton nuclear torpedo, of roughly the power of the bomb at Hiroshima. The was joined by other US destroyers who pummelled the submerged \"B-59\" with more explosives. \nSavitsky ordered the nuclear torpedo to be prepared for firing; its target was to be the USS \"Randolph\", the aircraft carrier leading the task force. An argument broke out in the sweltering control room of the \"B-59\" submarine among three senior officers, \"B-59\" captain Savitsky, political officer Ivan Semyonovich Maslennikov, and Deputy brigade commander Captain 2nd rank (US Navy Commander rank equivalent) Vasily Arkhipov. Accounts differ about whether Arkhipov convinced Savitsky not to make the attack or whether Savitsky himself finally concluded that the only reasonable choice left open to him was to come to the surface. The decision to launch the nuclear torpedo required the consent of all three senior officers, and of the three, Arkhipov alone refused to give his consent. Arkhipov's reputation was a key factor in the control room debate. The previous year he had exposed himself to severe radiation in order to save a submarine with an overheating nuclear reactor.\nDuring the conference October 2002, McNamara stated that nuclear war had come much closer than people had thought. Thomas Blanton, director of the National Security Archive, said, \"A guy called Vasily Arkhipov saved the world.\"\nPossibility of nuclear launch.\nIn early 1992 it was confirmed that Soviet forces in Cuba had already received tactical nuclear warheads for their artillery rockets and Il-28 bombers when the crisis broke. Castro stated that he would have recommended their use if the US had invaded, even if Cuba was destroyed.\nFifty years after the crisis, Graham Allison wrote:\nBBC journalist Joe Matthews published the story, on 13 October 2012, after news of the 100 tactical nuclear warheads mentioned by Graham Allison in the excerpt above. Khrushchev feared that Castro's hurt pride and widespread Cuban indignation over the concessions he had made to Kennedy might lead to a breakdown of the agreement between the Soviet Union and the United States. To prevent this, Khrushchev decided to offer to give Cuba more than 100 tactical nuclear weapons that had been shipped there with the long-range missiles but, crucially, had escaped the notice of US intelligence. Khrushchev determined that because the Americans had not listed the missiles on their list of demands, keeping them in Cuba would be in the Soviet Union's interests.\nAnastas Mikoyan had the task of negotiating with Castro over the missile transfer deal to prevent a breakdown in relations between Cuba and the Soviet Union. While in Havana, Mikoyan witnessed the mood swings and paranoia of Castro, who was convinced that Moscow had made the agreement with the US at the expense of Cuba's defence. Mikoyan, on his own initiative, decided that Castro and his military should not under any circumstances be given control of weapons with an explosive force equal to 100 Hiroshima-sized bombs. He defused the seemingly intractable situation, which risked re-escalating the crisis, on 22 November 1962. During a tense, four-hour meeting, Mikoyan convinced Castro that despite Moscow's desire to help, it would be in breach of an unpublished Soviet law, which did not actually exist, to transfer the missiles permanently into Cuban hands and provide them with an independent nuclear deterrent. Castro was forced to give way and, much to the relief of Khrushchev and the rest of the Soviet government, the tactical nuclear weapons were crated and returned by sea to the Soviet Union during December 1962.\nIn popular culture.\nThe American popular media, especially television, made frequent use of the events of the missile crisis in both fictional and documentary forms. Jim Willis includes the Crisis as one of the 100 \"media moments that changed America\". Sheldon Stern found that a half century later there were still many \"misconceptions, half-truths, and outright lies\" that had shaped media versions of what happened in the White House during those two weeks.\nHistorian William Cohn argued in a 1976 article that television programs were typically the main source used by the American public to know about and interpret the past. According to Cold War historian Andrei Kozovoi, the Soviet media proved somewhat disorganized as it was unable to generate a coherent popular history. Khrushchev lost power and was airbrushed out of the story and Cuba was no longer portrayed as a heroic David against the American Goliath. One contradiction that pervaded the Soviet media campaign was between the pacifistic rhetoric of the peace movement that emphasized the horrors of nuclear war and the militancy of the need to prepare Soviets for war against American aggression."}
{"id": "6828", "revid": "297354", "url": "https://en.wikipedia.org/wiki?curid=6828", "title": "Aquilegia", "text": "Aquilegia (common names: granny's bonnet, columbine) is a genus of about 130 species of perennial plants that are found in meadows, woodlands, and at higher elevations throughout the Northern Hemisphere, known for the spurred petals of their flowers.\nEtymology.\nThe genus name \"Aquilegia\" comes from the Latin \"Aquila\", or \"eagle\"; this is in obvious reference to the spurred, \"hook\" shapes within the blooms, that many gardeners say resemble an eagle's talons.\nDescription.\nPerennial herbs, with woody, erect stock, roots forming thick rhizomes. The basal leaves are compound, 1\u20133 ternate, blades 3-lobed -partite, and lobes lobulate and obtuse. The cauline leaves are similar to the basal ones, while the upper ones are bract like.\nThe hermaphrodite (bisexual) flowers are terminal to stem and branches. They are usually pentamerous (with five spreading perianth petaloid sepal segments). Five tubular honey-leaves are semi erect with a flat limb and spurred or saccate at the base. The spur is directed backwards and secretes nectar. Stamens are numerous (often more than 50) in whorls of 5, the innermost being scarious staminodes. There are ten membranaceous intrastaminal scales. There are five pistils and the carpels are free.\nThe fruit has several (five to 15) follicles which are semi erect and slightly connate downwards. These hold many seeds and are formed at the end of the pistils. The nectar is mainly consumed by long-beaked birds such as hummingbirds. Almost all \"Aquilegia\" species have a ring of staminodia around the base of the stigma, which may help protect against insects. Chromosome number is x=7.\nRelatives.\nColumbines are closely related to plants in the genera \"Actaea\" (baneberries) and \"Aconitum\" (wolfsbanes/monkshoods), which like \"Aquilegia\" produce cardiogenic toxins.\nInsects.\nThey are used as food plants by some Lepidoptera (butterfly and moth) caterpillars. These are mainly of noctuid moths \u2013 noted for feeding on many poisonous plants without harm \u2013 such as cabbage moth (\"Mamestra brassicae\"), dot moth (\"Melanchra persicariae\") and mouse moth (\"Amphipyra tragopoginis\"). The engrailed (\"Ectropis crepuscularia\"), a geometer moth, also uses columbine as a larval food plant. The larvae of the \"Papaipema leucostigma\" also feed on columbine.\nPlants in the genus \"Aquilegia\" are a major food source for \"Bombus hortorum\", a species of bumblebee. Specifically, they have been found to forage on species of \"Aquilegia vulgaris\" in Belgium and \"Aquilegia chrysantha\" in North America and Belgium. The bees do not show any preference in color of the flowers.\nCultivation.\nColumbine is a hardy perennial, which propagates by seed. It will grow to a height of . It will grow in full sun; however, it prefers growing in partial shade and well drained soil, and is able to tolerate average soils and dry soil conditions. Columbine is rated at hardiness zone 3 in the United States so does not require mulching or protection in the winter.\nLarge numbers of hybrids are available for the garden, since the European \"A. vulgaris\" was hybridized with other European and North American varieties.\n \"Aquilegia\" species are very interfertile, and will self-sow. Some varieties are short-lived so are better treated as biennials.\nThe British National Collection of \"Aquilegia\"s was held by Mrs Carrie Thomas at Killay near Swansea. Some time during or before 2014 the collection started to succumb to Aquilegia Downy Mildew \"Peronospora aquilegiicola\" which was at the time an emerging disease to which the plants had no resistance. By 2018 the entire collection had been lost. Aquilegia can be grown from seeds or rhizomes.\nUses.\nThe flowers of various species of columbine were consumed in moderation by Native Americans as a condiment with other fresh greens, and are reported to be very sweet, and safe if consumed in small quantities. The plant's seeds and roots, however, are highly poisonous and contain cardiogenic toxins which cause both severe gastroenteritis and heart palpitations if consumed as food. Native Americans used very small amounts of \"Aquilegia\" root as a treatment for ulcers. However, the medical use of this plant is better avoided due to its high toxicity; columbine poisonings may be fatal.\nAn acute toxicity test in mice has demonstrated that ethanol extract mixed with isocytisoside, the main flavonoid compound from the leaves and stems of \"Aquilegia vulgaris\", can be classified as non-toxic, since a dose of 3000\u00a0mg/kg did not cause mortality.\nCulture.\nThe Colorado blue columbine (\"A. coerulea\") is the official state flower of Colorado (see also Columbine, Colorado). It is also used as a symbol of the former city of Scarborough in the Canadian province of Ontario.\nEvolution.\nColumbines have been important in the study of evolution. It was found that the Sierra columbine (\"A. pubescens\") and crimson columbine (\"A. formosa\") each has adapted specifically to a pollinator. Bees and hummingbirds are the visitors to \"A. formosa\", while hawkmoths would only visit \"A. pubescens\" when given a choice. Such a \"pollination syndrome\", being due to flower color and orientation controlled by their genetics, ensures reproductive isolation and can be a cause of speciation.\n\"Aquilegia\" petals show an enormous range of petal spur length diversity ranging from a centimeter to the 15\u00a0cm spurs of \"Aquilegia longissima\". Selection from pollinator shifts is suggested to have driven these changes in nectar spur length. \nIt was shown that this spur length diversity is achieved solely through changing cell shape, not cell number or cell size. This suggests that a simple microscopic change can result in a dramatic evolutionarily relevant morphological change.\nSpecies.\n130 columbine species are accepted."}
{"id": "6829", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=6829", "title": "Cache (computing)", "text": "In computing, a cache ( ) is a hardware or software component that stores data so that future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere. A cache hit occurs when the requested data can be found in a cache, while a cache miss occurs when it cannot. Cache hits are served by reading data from the cache, which is faster than recomputing a result or reading from a slower data store; thus, the more requests that can be served from the cache, the faster the system performs.\nTo be cost-effective, caches must be relatively small. Nevertheless, caches are effective in many areas of computing because typical computer applications access data with a high degree of locality of reference. Such access patterns exhibit temporal locality, where data is requested that has been recently requested, and spatial locality, where data is requested that is stored near data that has already been requested.\nMotivation.\nIn memory design, there is an inherent trade-off between capacity and speed because larger capacity implies larger size and thus greater physical distances for signals to travel causing propagation delays. There is also a tradeoff between high-performance technologies such as SRAM and cheaper, easily mass-produced commodities such as DRAM, flash, or hard disks.\nThe buffering provided by a cache benefits one or both of latency and throughput (bandwidth).\nA larger resource incurs a significant latency for access \u2013 e.g. it can take hundreds of clock cycles for a modern 4\u00a0GHz processor to reach DRAM. This is mitigated by reading large chunks into the cache, in the hope that subsequent reads will be from nearby locations and can be read from the cache. Prediction or explicit prefetching can be used to guess where future reads will come from and make requests ahead of time; if done optimally, the latency is bypassed altogether.\nThe use of a cache also allows for higher throughput from the underlying resource, by assembling multiple fine-grain transfers into larger, more efficient requests. In the case of DRAM circuits, the additional throughput may be gained by using a wider data bus.\nOperation.\nHardware implements cache as a block of memory for temporary storage of data likely to be used again. Central processing units (CPUs), solid-state drives (SSDs) and hard disk drives (HDDs) frequently include hardware-based cache, while web browsers and web servers commonly rely on software caching.\nA cache is made up of a pool of entries. Each entry has associated \"data\", which is a copy of the same data in some \"backing store\". Each entry also has a \"tag\", which specifies the identity of the data in the backing store of which the entry is a copy.\nWhen the cache client (a CPU, web browser, operating system) needs to access data presumed to exist in the backing store, it first checks the cache. If an entry can be found with a tag matching that of the desired data, the data in the entry is used instead. This situation is known as a cache hit. For example, a web browser program might check its local cache on disk to see if it has a local copy of the contents of a web page at a particular URL. In this example, the URL is the tag, and the content of the web page is the data. The percentage of accesses that result in cache hits is known as the hit rate or hit ratio of the cache.\nThe alternative situation, when the cache is checked and found not to contain any entry with the desired tag, is known as a cache miss. This requires a more expensive access of data from the backing store. Once the requested data is retrieved, it is typically copied into the cache, ready for the next access.\nDuring a cache miss, some other previously existing cache entry is typically removed in order to make room for the newly retrieved data. The heuristic used to select the entry to replace is known as the replacement policy. One popular replacement policy, least recently used (LRU), replaces the oldest entry, the entry that was accessed less recently than any other entry. More sophisticated caching algorithms also take into account the frequency of use of entries.\nWrite policies.\nCache writes must eventually be propagated to the backing store. The timing for this is governed by the \"write policy\". The two primary write policies are:\nA write-back cache is more complex to implement since it needs to track which of its locations have been written over and mark them as \"dirty\" for later writing to the backing store. The data in these locations are written back to the backing store only when they are evicted from the cache, a process referred to as a \"lazy write\". For this reason, a read miss in a write-back cache may require two memory accesses to the backing store: one to write back the dirty data, and one to retrieve the requested data. Other policies may also trigger data write-back. The client may make many changes to data in the cache, and then explicitly notify the cache to write back the data.\nWrite operations do not return data. Consequently, a decision needs to be made for write misses: whether or not to load the data into the cache. This is determined by these \"write-miss policies\":\nWhile both write policies can Implement either write-miss policy, they are typically paired as follows:\nEntities other than the cache may change the data in the backing store, in which case the copy in the cache may become out-of-date or \"stale\". Alternatively, when the client updates the data in the cache, copies of that data in other caches will become stale. Communication protocols between the cache managers that keep the data consistent are associated with cache coherence.\nPrefetch.\nOn a cache read miss, caches with a \"demand paging policy\" read the minimum amount from the backing store. A typical demand-paging virtual memory implementation reads one page of virtual memory (often 4\u00a0KB) from disk into the disk cache in RAM. A typical CPU reads a single L2 cache line of 128\u00a0bytes from DRAM into the L2 cache, and a single L1 cache line of 64\u00a0bytes from the L2 cache into the L1 cache.\nCaches with a prefetch input queue or more general \"anticipatory paging policy\" go further\u2014they not only read the data requested, but guess that the next chunk or two of data will soon be required, and so prefetch that data into the cache ahead of time. Anticipatory paging is especially helpful when the backing store has a long latency to read the first chunk and much shorter times to sequentially read the next few chunks, such as disk storage and DRAM.\nA few operating systems go further with a loader that always pre-loads the entire executable into RAM. A few caches go even further, not only pre-loading an entire file, but also starting to load other related files that may soon be requested, such as the page cache associated with a prefetcher or the web cache associated with link prefetching.\nExamples of hardware caches.\nCPU cache.\nSmall memories on or close to the CPU can operate faster than the much larger main memory. Most CPUs since the 1980s have used one or more caches, sometimes in cascaded levels; modern high-end embedded, desktop and server microprocessors may have as many as six types of cache (between levels and functions). Some examples of caches with a specific function are the D-cache, I-cache and the translation lookaside buffer for the memory management unit (MMU).\nGPU cache.\nEarlier graphics processing units (GPUs) often had limited read-only texture caches and used swizzling to improve 2D locality of reference. Cache misses would drastically affect performance, e.g. if mipmapping was not used. Caching was important to leverage 32-bit (and wider) transfers for texture data that was often as little as 4 bits per pixel.\nAs GPUs advanced, supporting general-purpose computing on graphics processing units and compute kernels, they have developed progressively larger and increasingly general caches, including instruction caches for shaders, exhibiting functionality commonly found in CPU caches. These caches have grown to handle synchronization primitives between threads and atomic operations, and interface with a CPU-style MMU.\nDSPs.\nDigital signal processors have similarly generalized over the years. Earlier designs used scratchpad memory fed by direct memory access, but modern DSPs such as Qualcomm Hexagon often include a very similar set of caches to a CPU (e.g. Modified Harvard architecture with shared L2, split L1 I-cache and D-cache).\nTranslation lookaside buffer.\nA memory management unit (MMU) that fetches page table entries from main memory has a specialized cache, used for recording the results of virtual address to physical address translations. This specialized cache is called a translation lookaside buffer (TLB).\nIn-network cache.\nInformation-centric networking.\nInformation-centric networking (ICN) is an approach to evolve the Internet infrastructure away from a host-centric paradigm, based on perpetual connectivity and the end-to-end principle, to a network architecture in which the focal point is identified information. Due to the inherent caching capability of the nodes in an ICN, it can be viewed as a loosely connected network of caches, which has unique requirements for caching policies. However, ubiquitous content caching introduces the challenge to content protection against unauthorized access, which requires extra care and solutions.\nUnlike proxy servers, in ICN the cache is a network-level solution. Therefore, it has rapidly changing cache states and higher request arrival rates; moreover, smaller cache sizes impose different requirements on the content eviction policies. In particular, eviction policies for ICN should be fast and lightweight. Various cache replication and eviction schemes for different ICN architectures and applications have been proposed.\nPolicies.\nTime aware least recently used.\nThe time aware least recently used (TLRU) is a variant of LRU designed for the situation where the stored contents in cache have a valid lifetime. The algorithm is suitable in network cache applications, such as ICN, content delivery networks (CDNs) and distributed networks in general. TLRU introduces a new term: time to use (TTU). TTU is a time stamp on content which stipulates the usability time for the content based on the locality of the content and information from the content publisher. Owing to this locality-based time stamp, TTU provides more control to the local administrator to regulate in-network storage.\nIn the TLRU algorithm, when a piece of content arrives, a cache node calculates the local TTU value based on the TTU value assigned by the content publisher. The local TTU value is calculated by using a locally-defined function. Once the local TTU value is calculated the replacement of content is performed on a subset of the total content stored in cache node. The TLRU ensures that less popular and short-lived content should be replaced with incoming content.\nLeast frequent recently used.\nThe least frequent recently used (LFRU) cache replacement scheme combines the benefits of LFU and LRU schemes. LFRU is suitable for network cache applications, such as ICN, CDNs and distributed networks in general. In LFRU, the cache is divided into two partitions called privileged and unprivileged partitions. The privileged partition can be seen as a protected partition. If content is highly popular, it is pushed into the privileged partition. Replacement of the privileged partition is done by first evicting content from the unprivileged partition, then pushing content from the privileged partition to the unprivileged partition, and finally inserting new content into the privileged partition. In the above procedure, the LRU is used for the privileged partition and an approximated LFU (ALFU) scheme is used for the unprivileged partition. The basic idea is to cache the locally popular content with the ALFU scheme and push the popular content to the privileged partition.\nWeather forecast.\nIn 2011, the use of smartphones with weather forecasting options was overly taxing AccuWeather servers; two requests from the same area would generate separate requests. An optimization by edge-servers to truncate the GPS coordinates to fewer decimal places meant that the cached results from a nearby query would be used. The number of to-the-server lookups per day dropped by half.\nSoftware caches.\nDisk cache.\nWhile CPU caches are generally managed entirely by hardware, a variety of software manages other caches. The page cache in main memory is managed by the operating system kernel.\nWhile the disk buffer, which is an integrated part of the hard disk drive or solid state drive, is sometimes misleadingly referred to as \"disk cache\", its main functions are write sequencing and read prefetching. High-end disk controllers often have their own on-board cache for the hard disk drive's data blocks.\nFinally, a fast local hard disk drive can also cache information held on even slower data storage devices, such as remote servers (web cache) or local tape drives or optical jukeboxes; such a scheme is the main concept of hierarchical storage management. Also, fast flash-based solid-state drives (SSDs) can be used as caches for slower rotational-media hard disk drives, working together as hybrid drives.\nWeb cache.\nWeb browsers and web proxy servers employ web caches to store previous responses from web servers, such as web pages and images. Web caches reduce the amount of information that needs to be transmitted across the network, as information previously stored in the cache can often be re-used. This reduces bandwidth and processing requirements of the web server, and helps to improve responsiveness for users of the web.\nWeb browsers employ a built-in web cache, but some Internet service providers (ISPs) or organizations also use a caching proxy server, which is a web cache that is shared among all users of that network.\nAnother form of cache is P2P caching, where the files most sought for by peer-to-peer applications are stored in an ISP cache to accelerate P2P transfers. Similarly, decentralised equivalents exist, which allow communities to perform the same task for P2P traffic, for example, Corelli.\nMemoization.\nA cache can store data that is computed on demand rather than retrieved from a backing store. Memoization is an optimization technique that stores the results of resource-consuming function calls within a lookup table, allowing subsequent calls to reuse the stored results and avoid repeated computation. It is related to the dynamic programming algorithm design methodology, which can also be thought of as a means of caching.\nContent delivery network.\nA content delivery network (CDN) is a network of distributed servers that deliver pages and other Web content to a user, based on the geographic locations of the user, the origin of the web page and the content delivery server.\nCDNs began in the late 1990s as a way to speed up the delivery of static content, such as HTML pages, images and videos. By replicating content on multiple servers around the world and delivering it to users based on their location, CDNs can significantly improve the speed and availability of a website or application. When a user requests a piece of content, the CDN will check to see if it has a copy of the content in its cache. If it does, the CDN will deliver the content to the user from the cache.\nCloud storage gateway.\nA cloud storage gateway, also known as an edge filer, is a hybrid cloud storage device that connects a local network to one or more cloud storage services, typically object storage services such as Amazon S3. It provides a cache for frequently accessed data, providing high speed local access to frequently accessed data in the cloud storage service. Cloud storage gateways also provide additional benefits such as accessing cloud object storage through traditional file serving protocols as well as continued access to cached data during connectivity outages.\nOther caches.\nThe BIND DNS daemon caches a mapping of domain names to IP addresses, as does a resolver library.\nWrite-through operation is common when operating over unreliable networks (like an Ethernet LAN), because of the enormous complexity of the coherency protocol required between multiple write-back caches when communication is unreliable. For instance, web page caches and client-side network file system caches (like those in NFS or SMB) are typically read-only or write-through specifically to keep the network protocol simple and reliable.\nSearch engines also frequently make web pages they have indexed available from their cache. For example, Google provides a \"Cached\" link next to each search result. This can prove useful when web pages from a web server are temporarily or permanently inaccessible.\nDatabase caching can substantially improve the throughput of database applications, for example in the processing of indexes, data dictionaries, and frequently used subsets of data.\nA distributed cache uses networked hosts to provide scalability, reliability and performance to the application. The hosts can be co-located or spread over different geographical regions.\nBuffer vs. cache.\nThe semantics of a \"buffer\" and a \"cache\" are not totally different; even so, there are fundamental differences in intent between the process of caching and the process of buffering.\nFundamentally, caching realizes a performance increase for transfers of data that is being repeatedly transferred. While a caching system may realize a performance increase upon the initial (typically write) transfer of a data item, this performance increase is due to buffering occurring within the caching system.\nWith read caches, a data item must have been fetched from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the cache's (faster) intermediate storage rather than the data's residing location. With write caches, a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the cache's intermediate storage, deferring the transfer of the data item to its residing storage at a later stage or else occurring as a background process. Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the cache's intermediate storage and the location where the data resides. Buffering, on the other hand,\nWith typical caching implementations, a data item that is read or written for the first time is effectively being buffered; and in the case of a write, mostly realizing a performance increase for the application from where the write originated. Additionally, the portion of a caching protocol where individual writes are deferred to a batch of writes is a form of buffering. The portion of a caching protocol where individual reads are deferred to a batch of reads is also a form of buffering, although this form may negatively impact the performance of at least the initial reads (even though it may positively impact the performance of the sum of the individual reads). In practice, caching almost always involves some form of buffering, while strict buffering does not involve caching.\nA buffer is a temporary memory location that is traditionally used because CPU instructions cannot directly address data stored in peripheral devices. Thus, addressable memory is used as an intermediate stage. Additionally, such a buffer may be feasible when a large block of data is assembled or disassembled (as required by a storage device), or when data may be delivered in a different order than that in which it is produced. Also, a whole buffer of data is usually transferred sequentially (for example to hard disk), so buffering itself sometimes increases transfer performance or reduces the variation or jitter of the transfer's latency as opposed to caching where the intent is to reduce the latency. These benefits are present even if the buffered data are written to the buffer once and read from the buffer once.\nA cache also increases transfer performance. A part of the increase similarly comes from the possibility that multiple small transfers will combine into one large block. But the main performance-gain occurs because there is a good chance that the same data will be read from cache multiple times, or that written data will soon be read. A cache's sole purpose is to reduce accesses to the underlying slower storage. Cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers."}
{"id": "6830", "revid": "38599771", "url": "https://en.wikipedia.org/wiki?curid=6830", "title": "Columbus, Indiana", "text": "Columbus () is a city in, and the county seat of, Bartholomew County, Indiana, United States. The population was 50,474 at the 2020 census. The city is known for its architectural significance, having commissioned noted works of modern architecture and public art since the mid-20th century; the annual program Exhibit Columbus celebrates this legacy. Located about south of Indianapolis, on the east fork of the White River, it is the state's 20th-largest city. It is the principal city of the Columbus, Indiana metropolitan statistical area, which encompasses all of Bartholomew County. Columbus is the birthplace of former Indiana Governor and former Vice President of the United States, Mike Pence.\nColumbus is the headquarters of the engine company Cummins. In 2004 the city was named as one of \"The Ten Most Playful Towns\" by \"Nick Jr. Family Magazine\". In the July 2005 edition of \"GQ\" magazine, Columbus was named as one of the \"62 Reasons to Love Your Country\". Columbus won the national contest \"America in Bloom\" in 2006, and in late 2008, \"National Geographic Traveler\" ranked Columbus 11th on its historic destinations list, describing the city as \"authentic, unique, and unspoiled.\"\nHistory.\nThe land developed as Columbus was bought by General John Tipton and Luke Bonesteel in 1820. Tipton built a log cabin on Mount Tipton, a small hill overlooking White River and the surrounding flat, heavily forested and swampy valley. It held wetlands of the river. The town was first known as Tiptona, named in honor of Tipton. The town's name was changed to Columbus on March 20, 1821. Many people believe Tipton was upset by the name change, but no evidence exists to prove this. Nonetheless, he decided to leave the newly founded town and did not return.\nTipton was later appointed as the highway commissioner for the State of Indiana and was assigned to building a highway from Indianapolis, Indiana to Louisville, Kentucky. When the road approached Columbus, Tipton constructed the first bypass road ever built; it detoured south around the west side of Columbus en route to Seymour.\nJoseph McKinney was the first to plot the town of Columbus, but no date was recorded. Local history books for years said that the land on which Columbus sits was donated by Tipton. But in 2003, Historic Columbus Indiana acquired a deed showing that Tipton had sold the land.\nA ferry was established below the confluence of the Flatrock and Driftwood rivers, which form the White River. A village of three or four log cabins developed around the ferry landing, and a store was added in 1821. Later that year, Bartholomew County was organized by an act of the State Legislature and named to honor the famous Hoosier militiaman, General Joseph Bartholomew. Columbus was incorporated on June 28, 1864.\nThe first railroad in Indiana was constructed to Columbus from Madison, Indiana in 1844. This eventually became the Madison branch of the Pennsylvania Railroad. The railroad fostered the growth of the community into one of the largest in Indiana, and three more railroads reached the city by 1850.\nThe Crump Theatre in Columbus, built in 1889 by John Crump, is the oldest theater in Indiana. Today the building is included within the Columbus Historic District. Before it closed permanently in 2010, it was an all-ages venue with occasional musical performances.\nThe Cummins Bookstore began operations in the city in 1892. Until late 2007, when it closed, it was the oldest continually operated bookstore in Indiana.\nThe Irwin Union Bank building was built in 1954. It was designated as a National Historic Landmark by the National Park Service in 2001 in recognition of its unique architecture. The building consists of a one-story bank structure adjacent to a three-story office annex. A portion of the office annex was built along with the banking hall in 1954. The remaining larger portion, designed by Kevin Roche John Dinkeloo and Associates, was built in 1973. Eero Saarinen designed the bank building with its glazed hall to be set off against the blank background of its three-story brick annex. Two steel and glass vestibule connectors lead from the north side of this structure to the annex. The building was designed to distance the Irwin Union Bank from traditional banking architecture, which mostly echoed imposing, neoclassical style buildings of brick or stone. Tellers were behind iron bars and removed from their customers. Saarinen worked to develop a building that would welcome customers rather than intimidate them.\nEconomy.\nColumbus has been home to many manufacturing companies, including Noblitt-Sparks Industries, which built radios under the Arvin brand in the 1930s, and Arvin Industries, now Meritor After merging with Meritor Automotive on July 10, 2000, the headquarters of the newly created ArvinMeritor Industries was established in Troy, Michigan, the home of parent company, Rockwell International. It was announced in February 2011 that the company name would revert to Meritor, Inc.\nCummins is by far the region's largest employer, and the Infotech Park in Columbus accounts for a sizable number of research jobs in the city itself. Just south of Columbus are the North American headquarters of Toyota Material Handling, the world's largest material handling (forklift) manufacturer.\nOther notable industries include architecture, a discipline for which Columbus is famous worldwide. The late Joseph Irwin Miller (then president and chairman of Cummins) launched the Cummins Foundation, a charitable program that helps subsidize a large number of architectural projects throughout the city by up-and-coming engineers and architects.\nEarly in the 20th century, Columbus also was home to a number of pioneering car manufacturers, including Reeves, which produced the unusual four-axle Octoauto and the twin rear-axle Sextoauto, both around 1911.\nGeography.\nThe Driftwood and Flatrock Rivers converge at Columbus to form the East Fork of the White River.\nAccording to the 2010 census, Columbus has a total area of , of which (or 98.62%) is land and (or 1.38%) is water.\nDemographics.\n2010 census.\nAs of the census of 2010, there were 44,061 people, 17,787 households, and 11,506 families residing in the city. The population density was . There were 19,700 housing units at an average density of . The racial makeup of the city was 86.9% White, 2.7% African American, 0.2% Native American, 5.6% Asian, 0.1% Pacific Islander, 2.5% from other races, and 2.0% from two or more races. Hispanic or Latino of any race were 5.8% of the population.\nThere were 17,787 households, of which 33.5% had children under the age of 18 living with them, 48.5% were married couples living together, 11.7% had a female householder with no husband present, 4.5% had a male householder with no wife present, and 35.3% were non-families. 29.7% of all households were made up of individuals, and 11.5% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 3.00.\nThe median age in the city was 37.1 years. 25.2% of residents were under the age of 18; 8.1% were between the ages of 18 and 24; 27.3% were from 25 to 44; 24.9% were from 45 to 64; and 14.4% were 65 years of age or older. The gender makeup of the city was 48.4% male and 51.6% female.\n2000 census.\nAs of the census of 2000, there were 39,059 people, 15,985 households, and 10,566 families residing in the city. The population density was . There were 17,162 housing units at an average density of . The racial makeup of the city was 91.32% White, 2.71% Black or African American, 0.13% Native American, 3.23% Asian, 0.05% Pacific Islander, 1.39% from other races, and 1.19% from two or more races. 2.81% of the population were Hispanic or Latino of any race.\nThere were 15,985 households, out of which 31.8% had children under the age of 18 living with them, 51.9% were married couples living together, 11.0% had a female householder with no husband present, and 33.9% were non-families. 29.1% of all households were composed of individuals, and 10.7% had someone living alone who was 65 years of age or older. The average household size was 2.39, and the average family size was 2.94.\nIn the city, the population was spread out, with 25.7% under the age of 18, 8.0% from 18 to 24 years, 29.5% from 25 to 44 years, 23.0% from 45 to 64 years, and 13.7% over the age of 65. The median age was 36 years. There were 92.8 males for every 100 females and 89.6 males for every 100 females over age 18.\nThe median income for a household in the city was $41,723, and the median income for a family was $52,296. Males had a median income of $40,367 versus $24,446 for females, and the per capita income was $22,055. About 6.5% of families and 8.1% of the population were below the poverty line, including 9.7% of those under age 18 and 8.8% of those age 65 or over.\nArts and culture.\nColumbus is a city known for its modern architecture and public art. J. Irwin Miller, 2nd CEO and a nephew of a co-founder of Cummins, the Columbus-headquartered diesel engine manufacturer, instituted a program in which the Cummins Foundation paid the architects' fees, provided the client selected a firm from a list compiled by the foundation. The plan was initiated with public schools and was so successful that the foundation decided to offer such design support to other non-profit and civic organizations. The high number of notable public buildings and public art in the Columbus area, designed by such individuals as Eero Saarinen, I.M. Pei, Robert Venturi, Cesar Pelli, and Richard Meier, led to Columbus earning the nickname \"Athens on the Prairie.\"\nSeven buildings, constructed between 1942 and 1965, are National Historic Landmarks, and approximately 60 other buildings sustain the Bartholomew County seat's reputation as a showcase of modern architecture. National Public Radio once devoted an article to the town's architecture.\nIn 2015, Landmark Columbus was created as a program of Heritage Fund - The Community Foundation of Bartholomew county.\nIn addition to the Columbus Historic District and Irwin Union Bank, the city has numerous buildings listed on the National Register of Historic Places, including seven National Historic Landmarks of modernist architecture: Bartholomew County Courthouse, Columbus City Hall, First Baptist Church, First Christian Church, Haw Creek Leather Company, Mabel McDowell Elementary School, McEwen-Samuels-Marr House, McKinley School, Miller House, North Christian Church, and The Republic Newspaper Office.\nThe city is the basis for the 2017 film \"Columbus\" by independent filmmaker Kogonada. The film was shot on location in Columbus over 18 days in the summer of 2016.\nExhibit Columbus.\nIn May 2016, Landmark Columbus launched Exhibit Columbus as a way to continue the ambitious traditions of the past into the future. Exhibit Columbus features annual programming that alternates between symposium and exhibition years.\nSports.\nColumbus High School was home to footwear pioneer Chuck Taylor, who played basketball in Columbus before setting out to promote his now famous shoes and the sport of basketball before being inducted into the Naismith Memorial Basketball Hall of Fame.\nTwo local high schools compete within the state in various sports. Columbus North and Columbus East both have competitive athletics and have many notable athletes that go on to compete in college and beyond. Columbus North High School houses one of the largest high school gyms in the United States. CNHS vs CEHS\nIndiana Diesels of the Premier Basketball League play their home games at the gymnasium at Ceraland Park, with plans to move to a proposed downtown sports complex in the near future.\nParks and recreation.\nColumbus boasts over of parks and green space and over 20 miles of People Trails. These amenities, in addition to several athletic and community facilities, including Donner Aquatic Center, Lincoln Park Softball Complex, Hamilton Center Ice Arena, Clifty Park, Foundation for Youth/Columbus Gymnastics Center and The Commons, are managed and maintained by the Columbus Parks and Recreation Department.\nTransportation.\nTransit.\nColumBUS provides bus service in the city with five routes operating Monday through Saturday.\nRoads and highways.\nThe north\u2013south US Route 31 has been diverted to the northeastern part of the city. Interstate 65 bypasses Columbus to the west. Indiana Route 46 runs-east-west through the southern section of the city.\nRailroads.\nFreight rail service is provided by the Louisville and Indiana Railroad (LIRC). The LIRC line runs in a north\u2013south orientation along the western edge of Columbus.\nThe Pennsylvania Railroad's \"Kentuckyian\" (Chicago-Louisville) made stops in the city until 1968. The PRR and its successor, the Penn Central, ran the Florida-bound \"South Wind\" up to 1971.\nThe city has been earmarked as a location for a new Amtrak station along the Chicago-Indianapolis-Louisville rail corridor.\nAirport.\nColumbus is served by the Columbus Municipal Airport (KBAK). It is located approximately north of Columbus. The airport handles approximately 40,500 operations per year, with roughly 87% general aviation, 4% air taxi, 8% military and less than 1% commercial service. The airport has two concrete runways; a 6,401-foot runway with approved ILS and GPS approaches (Runway 5-23) and a 5,001-foot crosswind runway, also with GPS approaches, (Runway 14-32).\nThe nearest commercial airport which currently has scheduled airline service is Indianapolis International Airport (IND), located approximately northwest of Columbus. Louisville Muhammad Ali International Airport and Cincinnati/Northern Kentucky International Airport are to the south and to the southeast, respectively.\nNotable people.\nThis is a list of notable people who were born in, or who currently live, or have lived in Columbus.\nEducation.\nThe Bartholomew Consolidated School Corporation (BCSC) is the local school district. High schools include:\nColumbus has a public library, a branch of the Bartholomew County Public Library.\nSecondary education includes Indiana University\u2013Purdue University Columbus (IUPUC), an Ivy Tech campus, a Purdue Polytechnic campus, and an Indiana Wesleyan University education center."}
{"id": "6834", "revid": "4246661", "url": "https://en.wikipedia.org/wiki?curid=6834", "title": "List of computer scientists", "text": "This is a list of computer scientists, people who do work in computer science, in particular researchers and authors.\nSome persons notable as programmers are included here because they work in research as well as program. A few of these people pre-date the invention of the digital computer; they are now regarded as computer scientists because their work can be seen as leading to the invention of the computer. Others are mathematicians whose work falls within what would now be called theoretical computer science, such as complexity theory and algorithmic information theory.\nExternal links.\n[[Category:Lists of computer scientists| ]]\n[[Category:Computer scientists|*]]\n[[Category:Lists of people by occupation]]"}
{"id": "6835", "revid": "24043204", "url": "https://en.wikipedia.org/wiki?curid=6835", "title": "Coracinus capensis", "text": ""}
{"id": "6839", "revid": "10289486", "url": "https://en.wikipedia.org/wiki?curid=6839", "title": "Reaction kinetics in uniform supersonic flow", "text": "Reaction kinetics in uniform supersonic flow (, CRESU) is an experiment investigating chemical reactions taking place at very low temperatures.\nThe technique involves the expansion of a gas or mixture of gases through a de Laval nozzle from a high-pressure reservoir into a vacuum chamber. As it expands, the nozzle collimates the gas into a uniform supersonic beam, which is essentially collision-free and has a temperature that, in the centre-of-mass frame, can be significantly below that of the reservoir gas. Each nozzle produces a characteristic temperature. This way, any temperature between room temperature and about 10\u00a0K can be achieved.\nApparatus.\nThere are relatively few CRESU apparatuses in existence for the simple reason that the gas throughput and pumping requirements are huge, which makes them expensive to run. Two of the leading centres have been the University of Rennes (France) and the University of Birmingham (UK). A more recent development has been a pulsed version of the CRESU, which requires far less gas and therefore smaller pumps.\nKinetics.\nMost species have a negligible vapour pressure at such low temperatures, and this means that they quickly condense on the sides of the apparatus. Essentially, the CRESU technique provides a \"wall-less flow tube\", which allows the kinetics of gas-phase reactions to be investigated at much lower temperatures than otherwise possible.\nChemical kinetics experiments can then be carried out in a pump\u2013probe fashion, using a laser to initiate the reaction (for example, by preparing one of the reagents by photolysis of a precursor), followed by observation of that same species (for example, by laser-induced fluorescence) after a known time delay. The fluorescence signal is captured by a photomultiplier a known distance downstream of the de Laval nozzle. The time delay can be varied up to the maximum corresponding to the flow time over that known distance. By studying how quickly the reagent species disappears in the presence of differing concentrations of a (usually stable) co-reagent species, the reaction rate constant at the low temperature of the CRESU flow can be determined.\nReactions studied by the CRESU technique typically have no significant activation energy barrier. In the case of neutral\u2013neutral reactions (i.e., not involving any charged species, ions), these type of barrier-free reactions usually involve free radical species, such as molecular oxygen (O2), the cyanide radical (CN) or the hydroxyl radical (OH). The energetic driving force for these reactions is typically an attractive long-range intermolecular potential.\nCRESU experiments have been used to show deviations from Arrhenius kinetics at low temperatures: as the temperature is reduced, the rate constant actually increases. They can explain why chemistry is so prevalent in the interstellar medium, where many different polyatomic species have been detected (by radio astronomy)."}
{"id": "6840", "revid": "1255878", "url": "https://en.wikipedia.org/wiki?curid=6840", "title": "Cygwin", "text": "Cygwin ( ) is a free and open-source Unix-like environment and command-line interface (CLI) for Microsoft Windows. The project also provides a software repository containing open-source packages. Cygwin allows source code for Unix-like operating systems to be compiled and run on Windows. Cygwin provides native integration of Windows-based applications. \nThe terminal emulator Mintty is the default command-line interface (CLI) provided to interact with the environment. The Cygwin installation directory layout mimics the root file system of Unix-like systems, with directories such as codice_1, codice_2, codice_3, codice_4, and codice_5.\nCygwin is released under the GNU Lesser General Public License version 3. It was originally developed by Cygnus Solutions, which was later acquired by Red Hat (now part of IBM), to port the GNU toolchain to Win32, including the GNU Compiler Suite. Rather than rewrite the tools to use the Win32 runtime environment, Cygwin implemented a POSIX-compatible environment in the form of a DLL.\nThe brand motto is \"Get that Linux feeling \u2013 on Windows\", although Cygwin doesn't have Linux in it.\nHistory.\nCygwin began in 1995 as a project of Steve Chamberlain, a Cygnus engineer who observed that Windows NT and 95 used COFF as their object file format, and that GNU already included support for x86 and COFF, and the C library newlib. He thought that it would be possible to retarget GCC and produce a cross compiler generating executables that could run on Windows. A prototype was later developed. Chamberlain bootstrapped the compiler on a Windows system, to emulate Unix to let the GNU configure shell script run.\nInitially, Cygwin was called \"gnuwin32\". When Microsoft registered the trademark Win32, the \"32\" was dropped to simply become \"Cygwin\".\nIn 1999, Cygnus offered Cygwin 1.0 as a commercial product. Subsequent versions have not been released, instead relying on continued open source releases.\nGeoffrey Noer was the project lead from 1996 to 1999. Christopher Faylor was lead from 1999 to 2004; he left Red Hat and became co-lead with Corinna Vinschen. Corinna Vinschen has been the project lead from mid-2014 to date (as of September, 2024).\nFrom June 23, 2016, the Cygwin library version 2.5.2 was licensed under the GNU Lesser General Public License (LGPL) version 3.\nDescription.\nCygwin is provided in two versions: the full 64-bit version and a stripped-down 32-bit version, whose final version was released in 2022. Cygwin consists of a library that implements the POSIX system call API in terms of Windows system calls to enable the running of a large number of application programs equivalent to those on Unix systems, and a GNU development toolchain (including GCC and GDB). Programmers have ported the X Window System, K Desktop Environment 3, GNOME, Apache, and TeX. Cygwin permits installing inetd, syslogd, sshd, Apache, and other daemons as standard Windows services. Cygwin programs have full access to the Windows API and other Windows libraries.\nCygwin programs are installed by running Cygwin's \"setup\" program, which downloads them from repositories on the Internet.\nThe Cygwin API library is licensed under the GNU Lesser General Public License version 3 (or later), with an exception to allow linking to any free and open-source software whose license conforms to the Open Source Definition.\nCygwin consists of two parts:\nCygwin supports POSIX symbolic links, representing them as plain-text files with the system attribute set. Cygwin 1.5 represented them as Windows Explorer shortcuts, but this was changed for reasons of performance and POSIX correctness. Cygwin also recognises NTFS junction points and symbolic links and treats them as POSIX symbolic links, but it does not create them. The POSIX API for handling access control lists (ACLs) is supported.\nTechnical details.\nA Cygwin-specific version of the Unix codice_6 command allows mounting Windows paths as \"filesystems\" in the Unix file space. Initial mount points can be configured in codice_7, which has a format very similar to Unix systems, except that Windows paths appear in place of devices. Filesystems can be mounted in binary mode (by default), or in text mode, which enables automatic conversion between LF and CRLF endings (which only affects programs that open files without explicitly specifying text or binary mode).\nCygwin 1.7 introduced comprehensive support for POSIX locales, and the UTF-8 Unicode encoding became the default.\nThe fork system call for duplicating a process is fully implemented, but the copy-on-write optimization strategy could not be used.\nThe Cygwin DLL contains a console driver that emulates a Unix-style terminal within the Windows console. Cygwin's default user interface is the bash shell running in the Cygwin console. The DLL also implements pseudo terminal (pty) devices. Cygwin ships with a number of terminal emulators that are based on them, including mintty, rxvt/urxvt, and xterm. The version of GCC that comes with Cygwin has various extensions for creating Windows DLLs, such as specifying whether a program is a windowing or console-mode program. Support for compiling programs that do not require the POSIX compatibility layer provided by the Cygwin DLL used to be included in the default GCC, but , it is provided by cross-compilers contributed by the MinGW-w64 project.\nSoftware packages.\nCygwin's base package selection is approximately 100MB, containing the bash (interactive user) and dash (installation) shells and the core file and text manipulation utilities. Additional packages are available as optional installs from within the Cygwin \"setup\" program and package manager (\"setup-x86_64.exe\" \u2013 64 bit). The Cygwin Ports project provided additional packages that were not available in the Cygwin distribution itself. Examples included GNOME, K Desktop Environment 3, MySQL database, and the PHP scripting language. Most ports have been adopted by volunteer maintainers as Cygwin packages, and Cygwin Ports are no longer maintained. Cygwin ships with GTK+ and Qt.\nThe Cygwin/X project allows graphical Unix programs to display their user interfaces on the Windows desktop for both local and remote programs."}
{"id": "6841", "revid": "86247", "url": "https://en.wikipedia.org/wiki?curid=6841", "title": "Communists", "text": ""}
{"id": "6844", "revid": "9092818", "url": "https://en.wikipedia.org/wiki?curid=6844", "title": "Conspiracy theories", "text": ""}
{"id": "6845", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=6845", "title": "Corinth", "text": "Corinth ( ; , ) is a municipality in Corinthia in Greece. The successor to the ancient city of Corinth, it is a former municipality in Corinthia, Peloponnese, which is located in south-central Greece. Since the 2011 local government reform, it has been part of the municipality of Corinth, of which it is the seat and a municipal unit. It is the capital of Corinthia.\nIt was founded as Nea Korinthos (), or New Corinth, in 1858 after an earthquake destroyed the existing settlement of Corinth, which had developed in and around the site of the ancient city.\nHistory.\nCorinth derives its name from Ancient Corinth, a city-state of antiquity. The site was occupied from before 3000 BC.\nAncient Greece.\nHistorical references begin with the early 8th century BC, when ancient Corinth began to develop as a commercial center. Between the 8th and 7th centuries, the Bacchiad family ruled Corinth. Cypselus overthrew the Bacchiad family, and between 657 and 585 BC, he and his son Periander ruled Corinth as the Tyrants.\nIn about 585 BC, an oligarchical government seized power. This government later allied with Sparta within the Peloponnesian League, and Corinth participated in the Persian Wars and Peloponnesian War as an ally of Sparta. After Sparta's victory in the Peloponnesian war, the two allies fell out with one another, and Corinth pursued an independent policy in the various wars of the early 4th century BC. After the Macedonian conquest of Greece, the Acrocorinth was the seat of a Macedonian garrison until 243 BC, when the city joined the Achaean League.\nAncient Rome.\nNearly a century later, in 146 BC, Corinth was captured and was completely destroyed by the Roman army.\nAs a newly rebuilt Roman colony in 44 BC, Corinth flourished and became the administrative capital of the Roman province of Achaea.\nMedieval times.\nAn important earthquake touched Corinth and its region in 856, causing around 45000 deaths.\nModern era.\nIn 1858, the old city, now known as Ancient Corinth (\u0391\u03c1\u03c7\u03b1\u03af\u03b1 \u039a\u03cc\u03c1\u03b9\u03bd\u03b8\u03bf\u03c2, \"Archaia Korinthos\"), located southwest of the modern city, was totally destroyed by a magnitude 6.5 earthquake. New Corinth (\"Nea Korinthos\") was then built to the north-east of it, on the coast of the Gulf of Corinth. In 1928, a magnitude 6.3 earthquake devastated the new city, which was then rebuilt on the same site. In 1933, there was a great fire, and the new city was rebuilt again.\nDuring the German occupation in World War II, the Germans operated a Dulag transit camp for British, Australian, New Zealander and Serbian prisoners of war and a forced labour camp in the town.\nGeography.\nLocated about west of Athens, Corinth is surrounded by the coastal townlets of (clockwise) Lechaio, Isthmia, Kechries, and the inland townlets of Examilia and the archaeological site and village of ancient Corinth. Natural features around the city include the narrow coastal plain of Vocha, the Corinthian Gulf, the Isthmus of Corinth cut by its canal, the Saronic Gulf, the Oneia Mountains, and the monolithic rock of Acrocorinth, where the medieval acropolis was built.\nClimate.\nAccording to the nearby weather station of Velo, operated by the Hellenic National Meteorological Service, Corinth has a hot-summer Mediterranean climate (K\u00f6ppen climate classification: \"Csa\"), with hot, dry summers and cool, rainy winters. The hottest month is July with an average temperature of while the coldest month is January with an average temperature of . Corinth receives about 463\u00a0mm of rainfall per year and has an average annual temperature of .\nDemographics.\nThe Municipality of Corinth (\u0394\u03ae\u03bc\u03bf\u03c2 \u039a\u03bf\u03c1\u03b9\u03bd\u03b8\u03af\u03c9\u03bd) had a population of 55,941 according to the 2021 census, the second most populous municipality in the Peloponnese Region after Kalamata. The municipal unit of Corinth had 38,485 inhabitants, of which Corinth itself had 30,816 inhabitants, placing it in second place behind Kalamata among the cities of the Peloponnese Region.\nThe municipal unit of Corinth (\u0394\u03b7\u03bc\u03bf\u03c4\u03b9\u03ba\u03ae \u03b5\u03bd\u03cc\u03c4\u03b7\u03c4\u03b1 \u039a\u03bf\u03c1\u03b9\u03bd\u03b8\u03af\u03c9\u03bd) includes apart from Corinth proper the town of Archaia Korinthos, the town of Examilia, and the smaller settlements of Xylokeriza and Solomos. The municipal unit has an area of 102.187\u00a0km2.\nEconomy.\nIndustry.\nCorinth is a major industrial hub at a national level. The Corinth Refinery is one of the largest oil refining industrial complexes in Europe. Ceramic tiles, copper cables, gums, gypsum, leather, marble, meat products, medical equipment, mineral water and beverages, petroleum products, and salt are produced nearby. , a period of Economic changes commenced as a large pipework complex, a textile factory and a meat packing facility diminished their operations.\nTransport.\nRoads.\nCorinth is a major road hub. The A7 toll motorway for Tripoli and Kalamata, (and Sparta via the A71 toll), branches off the A8/E94 toll motorway from Athens at Corinth. Corinth is the main entry point to the Peloponnesian peninsula, the southernmost area of continental Greece.\nBus.\nKTEL Korinthias provides intercity bus service in the peninsula and to Athens via the Isthmos station southeast of the city center. Local bus service is also available.\nRailways.\nThe metre gauge railway from Athens and Pireaeus reached Corinth in 1884. This station closed to regular public transport in 2007. In 2005, two years prior, the city was connected to the Athens Suburban Railway, following the completion of the new Corinth railway station. The journey time from Athens to Corinth is about 55 minutes. The train station is 5 minutes by car from the city centre and parking is available for free.\nPort.\nThe port of Corinth, located north of the city centre and close to the northwest entrance of the Corinth Canal, at 37 56.0\u2019 N / 22 56.0\u2019 E, serves the local needs of industry and agriculture. It is mainly a cargo exporting facility.\nIt is an artificial harbour (depth approximately , protected by a concrete mole (length approximately 930 metres, width 100 metres, mole surface 93,000 m2). A new pier finished in the late 1980s doubled the capacity of the port. The reinforced mole protects anchored vessels from strong northern winds.\nWithin the port operates a customs office facility and a Hellenic Coast Guard post. Sea traffic is limited to trade in the export of local produce, mainly citrus fruits, grapes, marble, aggregates and some domestic imports. The port operates as a contingency facility for general cargo ships, bulk carriers and ROROs, in case of strikes at Piraeus port.\nFerries.\nThere was formerly a ferry link to Catania, Sicily and Genoa in Italy.\nCanal.\nThe Corinth Canal, carrying ship traffic between the western Mediterranean Sea and the Aegean Sea, is about east of the city, cutting through the Isthmus of Corinth that connects the Peloponnesian peninsula to the Greek mainland, thus effectively making the former an island. The builders dug the canal through the Isthmus at sea level; no locks are employed. It is in length and only wide at its base, making it impassable for most modern ships. It now has little economic importance.\nThe canal was mooted in ancient times and an abortive effort was made to dig it in around 600 BC by Periander which led him to pave the Diolkos highway instead. Julius Caesar and Caligula both considered digging the canal but died before starting the construction. The emperor Nero then directed the project, which consisted initially of a workforce of 6,000 Jewish prisoners of war, but it was interrupted because of his death. The project resumed only in 1882, after Greece gained independence from the Ottoman Empire, but was hampered by geological and financial problems that bankrupted the original builders. It was finally completed in 1893, but due to the canal's narrowness, navigational problems and periodic closures to repair landslips from its steep walls, it failed to attract the level of traffic anticipated by its operators. It is now used mainly for tourist traffic.\nSport.\nThe city's association football team is Korinthos F.C. (\"\u03a0.\u0391.E. \u039a\u03cc\u03c1\u03b9\u03bd\u03b8\u03bf\u03c2\"), established in 1999 after the merger of Pankorinthian Football Club (\"\u03a0\u03b1\u03b3\u03ba\u03bf\u03c1\u03b9\u03bd\u03b8\u03b9\u03b1\u03ba\u03cc\u03c2\") and Corinth Football Club (\"\u039a\u03cc\u03c1\u03b9\u03bd\u03b8\u03bf\u03c2\"). During the 2006\u20132007 season, the team played in the Greek Fourth Division's Regional Group 7. The team went undefeated that season and it earned the top spot. This granted the team a promotion to the Gamma Ethnik\u00ed (Third Division) for the 2007\u20132008 season. For the 2008\u20132009 season, Korinthos F.C. competed in the Gamma Ethniki (Third Division) southern grouping.\nTwin towns/sister cities.\nCorinth is twinned with:\nOther locations named after Corinth.\nDue to its ancient history and the presence of St. Paul the Apostle in Corinth some locations all over the world have been named Corinth."}
{"id": "6846", "revid": "1265005226", "url": "https://en.wikipedia.org/wiki?curid=6846", "title": "Colossae", "text": "Colossae (; ) was an ancient city of Phrygia in Asia Minor, and one of the most celebrated cities of southern Anatolia (modern Turkey). The Epistle to the Colossians, an early Christian text which identifies its author as Paul the Apostle, is addressed to the church in Colossae. A significant city from the 5th century BC onwards, it had dwindled in importance by the time of Paul, but was notable for the existence of its local angel cult. It was part of the Roman and Byzantine province of Phrygia Pacatiana, before being destroyed in 1192/3 and its population relocating to nearby Chonae (Chonai, modern-day Honaz).\nLocation and geography.\nColossae was located in Phrygia, in Asia Minor. It was located southeast of Laodicea on the road through the Lycus Valley near the Lycus River at the foot of Mt. Cadmus, the highest mountain in Turkey's western Aegean Region, and between the cities Sardeis and Celaenae, and southeast of the ancient city of Hierapolis. At Colossae, Herodotus describes how, \"the river Lycos falls into an opening of the earth and disappears from view, and then after an interval of about five furlongs it comes up to view again, and this river also flows into the Maiander.\" Despite a treacherously ambiguous cartography and history, Colossae has been clearly distinguished in modern research from nearby \"Chonai\" (), now called Honaz, with what remains of the buried ruins of Colossae (\"the mound\") lying to the north of Honaz.\nOrigin and etymology of place name.\nThe medieval poet Manuel Philes incorrectly imagined that the name \"Colossae\" was connected to the Colossus of Rhodes. More recently, in an interpretation that ties Colossae to an Indo-European root that happens to be shared with the word \"kolossos\", Jean-Pierre Vernant has connected the name to the idea of setting up a sacred space or shrine. Another proposal relates the name to the Greek \"kolazo\" 'to punish'. Others believe the name derives from the manufacture of its famous dyed wool, or \"colossinus\".\nHistory.\nBefore the Pauline period.\nThe first mention of the city may be in a 17th-century BC Hittite inscription, which speaks of a city called Huwalu\u0161ija, which some archeologists believe is a reference to early Colossae. The 5th-century geographer Herodotus first mentions Colossae by name and as a \"great city in Phrygia\", which accommodates the Persian king Xerxes I while en route to wage war against the Greeks in the Greco-Persian Wars\u2013 showing the city had already reached a certain level of wealth and size by this time. \nWriting in the 5th century BC, Xenophon refers to Colossae as \"a populous city, wealthy and of considerable magnitude\". It was famous for its wool trade. Strabo notes that the city drew great revenue from the flocks, and that the wool of Colossae gave its name to colour \"colossinus\".\nIn 396 BC, Colossae was the site of the execution of the rebellious Persian satrap Tissaphernes, who was lured there and slain by an agent of the party of Cyrus the Younger.\nPauline period.\nAlthough during the Hellenistic period, the town was of some mercantile importance, by the 1st century it had dwindled greatly in size and significance. Paul's letter to the Colossians points to the existence of an early Christian community. Colossae was home to the miracle near the Archangel church, where a sacristan named Archipos witnessed, how the Archangel Michael thwarted a plan by the heathens to destroy the church by flooding it with the waters of near-by mountain rivers. The Eastern Orthodox Church commemorates this feast on 6(19) September.\nThe canonical biblical text Epistle to the Colossians is addressed to the Christian community in Colossae. The epistle has traditionally been attributed to Paul the Apostle due to its autobiographical salutation and style, but some modern critical scholars now believe it to be written by another author some time after Paul's death. It is believed that one aim of the letter was to address the challenges that the Colossian community faced in its context of the syncretistic Gnostic religions that were developing in Asia Minor.\nAccording to the Epistle to the Colossians, Epaphras seems to have been a person of some importance in the Christian community in Colossae, and tradition presents him as its first bishop. The epistle also seems to imply that Paul had never visited the city, because it only speaks of him having \"heard\" of the Colossians' faith, and in the Epistle to Philemon Paul tells Philemon of his hope to visit Colossae upon being freed from prison. Tradition also gives Philemon as the second bishop of the see.\nThe city was decimated by an earthquake in the 60s AD, and was rebuilt independent of the support of Rome.\nThe Apostolic Constitutions list Philemon as a bishop of Colossae. On the other hand, the \"Catholic Encyclopedia\" considers Philemon doubtful.\nThe first historically documented bishop is Epiphanius, who was not personally at the Council of Chalcedon, but whose metropolitan bishop Nunechius of Laodicea, the capital of the Roman province of Phrygia Pacatiana, signed the acts on his behalf.\nByzantine period and decline.\nThe city's fame and renowned status continued into the Byzantine period, and in 858, it was distinguished as a Metropolitan See. The Byzantines also built the church of St. Michael in the vicinity of Colossae, one of the largest church buildings in the Middle East. Nevertheless, sources suggest that the town may have decreased in size or may even been completely abandoned due to Arab invasions in the seventh and eighth centuries, forcing the population to flee to resettle in the nearby city of Chonai (modern day Honaz).\nColossae's famous church was destroyed in 1192/3 during the Byzantine civil wars. It was a suffragan diocese of Laodicea in Phyrigia Pacatiana but was replaced in the Byzantine period by the Chonae settlement on higher ground.\nModern study and archeology.\nMost archeological attention has been focused on nearby Laodicea and Hierapolis. Excavations of Colossae began in 2021 led by Bari\u015f Yener of Pammukale University in Denizli. The first several years involve surface surveys to analyze pottery and survey the landscape. They hope to start digging in 2023-24.\nThe site exhibits a biconical acropolis almost high, and encompasses an area of almost . On the eastern slope there sits a theater which probably seated around 5,000 people, suggesting a total population of 25,000\u201330,000 people. The theater was probably built during the Roman period, and may be near an agora that abuts the \"cardo maximus\", or the city's main north\u2013south road. Ceramic finds around the theater confirm the city's early occupation in the third and second millennia BC. Northeast of the tell, and most likely outside the city walls, a necropolis displays Hellenistic tombs with two main styles of burial: one with an antecedent room connected to an inner chamber, and tumuli, or underground chambers accessed by stairs leading to the entrance. Outside the tell, there are also remains of sections of columns that may have marked a processional way, or the \"cardo\". Today, the remains of one column marks the location where locals believe a church once stood, possibly that of St. Michael. Near the Lycus River, there is evidence that water channels had been cut out of the rock with a complex of pipes and sluice gates to divert water for bathing and for agricultural and industrial purposes.\nModern legacy.\nThe holiness and healing properties associated with the waters of Colossae during the Byzantine era continue to this day, particularly at a pool fed by the Lycus River at the G\u00f6z picnic grounds west of Colossae at the foot of Mt. Cadmus. Locals consider the water to be therapeutic."}
{"id": "6847", "revid": "97297", "url": "https://en.wikipedia.org/wiki?curid=6847", "title": "Colossians", "text": ""}
{"id": "6848", "revid": "34162051", "url": "https://en.wikipedia.org/wiki?curid=6848", "title": "Charge of the Goddess", "text": "The Charge of the Goddess (or Charge of the Star Goddess) is an inspirational text often used in the neopagan religion of Wicca. The Charge of the Goddess is recited during most rituals in which the Wiccan priest/priestess is expected to represent, and/or embody, the Goddess within the sacred circle, and is often spoken by the High Priest/Priestess after the ritual of Drawing Down the Moon.\nThe Charge is the promise of the Goddess (who is embodied by the high priestess) to all witches that she will teach and guide them. It has been called \"perhaps the most important single theological document in the neo-Pagan movement\". It is used not only in Wicca, but as part of the foundational documents of the Reclaiming tradition of witchcraft co-founded by Starhawk.\nSeveral versions of the Charge exist, though they all have the same basic premise, that of a set of instructions given by the Great Goddess to her worshippers. The earliest version is that compiled by Gerald Gardner. This version, titled \"Leviter Veslis\" or \"Lift Up the Veil\", includes material paraphrased from works by Aleister Crowley, primarily from Liber AL (The Book of the Law, particularly from Ch 1, spoken by Nuit, the Star Goddess), and from Liber LXV (The Book of the Heart Girt with a Serpent) and from Crowley's essay \"The Law of Liberty\", thus linking modern Wicca to the cosmology and revelations of Thelema. It has been shown that Gerald Gardner's book collection included a copy of Crowley's \"The Blue Equinox\" (1919) which includes all of the Crowley quotations transferred by Gardner to the Charge of the Goddess.\nThere are also two versions written by Doreen Valiente in the mid-1950s, after her 1953 Wiccan initiation. The first was a poetic paraphrase which eliminated almost all the material derived from Leland and Crowley. The second was a prose version which is contained within the traditional Gardnerian Book of Shadows and more closely resembles Gardner's \"Leviter Veslis\" version of 1949.\nSeveral different versions of a Wiccan Charge of the God have since been created to mirror and accompany the Charge of the Goddess.\nThemes.\nThe opening paragraph names a collection of goddesses, some derived from Greek or Roman mythology, others from Celtic or Arthurian legends, affirming a belief that these various figures represent a single Great Mother:\nThis theme echoes the ancient Roman belief that the Goddess Isis was known by ten thousand names and also that the Goddess still worshipped today by Wiccans and other neopagans is known under many guises but is in fact one universal divinity.\nThe second paragraph is largely derived and paraphrased from the words that Aradia, the messianic daughter of Diana, speaks to her followers in Charles Godfrey Leland's 1899 book \"Aradia, or the Gospel of the Witches\" (London: David Nutt; various reprints). The third paragraph is largely written by Doreen Valiente, with a significant content of phrases loosely from \"The Book of the Law\" and \"The Book of the Heart Girt with the Serpent\" by Aleister Crowley.\nThe charge affirms that \"all\" acts of love and pleasure are sacred to the Goddess, e.g.:\nHistory.\nAncient precedents.\nIn book eleven, chapter 47 of Apuleius's \"The Golden Ass\", Isis delivers what Ceisiwr Serith calls \"essentially a charge of a goddess\". This is rather different from the modern version known in Wicca, though they have the same premise, that of the rules given by a great Mother Goddess to her faithful.\nThe Charge of the Goddess is also known under the title \"Leviter Veslis\". This has been identified by the historian Ronald Hutton, cited in an article by Roger Dearnsley \"The Influence of Aleister Crowley on \"Ye Bok of Ye Art Magical\", as a piece of medieval ecclesiastical Latin used to mean \"lifting the veil.\" However, Hutton's interpretation does not reflect the Latin grammar as it currently stands. It may represent Gardner's attempt to write \"Levetur Velis\", which has the literal meaning of \"Let the veil be lifted.\" This expression would, by coincidence or design, grammatically echo the famous \"fiat lux\" (\"Gen. 1:3\") of the Latin Vulgate.\nOrigins.\nThe earliest known Wiccan version is found in a document dating from the late 1940s, Gerald Gardner's ritual notebook titled \"Ye Bok of Ye Art Magical\". The oldest identifiable source contained in this version is the final line, which is traceable to the 17th-century \"Centrum Naturae Concentratum\" of Alipili (or Ali Puli). This version also draws extensively from Charles Godfrey Leland's \"Aradia, or the Gospel of the Witches\" (1899) and other modern sources, particularly from the works of Aleister Crowley.\nIt is believed to have been compiled by Gerald Gardner or possibly another member of the New Forest coven. Gardner intended his version to be a theological statement justifying the Gardnerian sequence of initiations. Like the Charge found in Freemasonry, where the charge is a set of instructions read to a candidate standing in a temple, the Charge of the Goddess was intended to be read immediately before an initiation.\nValiente felt that the influence of Crowley on the Charge was too obvious, and she did not want \"the Craft\" (a common term for Wicca) associated with Crowley. Gardner invited her to rewrite the Charge. She proceeded to do so, her first version being into verse.\nThe initial verse version by Doreen Valiente consisted of eight verses, the second of which was:\nValiente was unhappy with this version, saying that \"people seemed to have some difficulty with this, because of the various goddess-names which they found hard to pronounce\", and so she rewrote it as a prose version, much of which differs from her initial version, and is more akin to Gardner's version. This prose version has since been modified and reproduced widely by other authors."}
{"id": "6849", "revid": "2842084", "url": "https://en.wikipedia.org/wiki?curid=6849", "title": "Cy Young", "text": "Denton True \"Cy\" Young (March 29, 1867 \u2013 November 4, 1955) was an American Major League Baseball (MLB) pitcher. Born in Gilmore, Ohio, he worked on his family's farm as a youth before starting his professional baseball career. Young entered the major leagues in 1890 with the National League's Cleveland Spiders and pitched for them until 1898. He was then transferred to the St. Louis Cardinals franchise. In 1901, Young jumped to the American League and played for the Boston Red Sox franchise until 1908, helping them win the 1903 World Series. He finished his career with the Cleveland Naps and Boston Rustlers, retiring in 1911.\nYoung was one of the hardest-throwing pitchers in the game early in his career. After his speed diminished, he relied more on his control and remained effective into his forties. By the time Young retired, he had established numerous pitching records, some of which have stood for over a century. He holds MLB records for the most career wins, with 511, along with most career losses, earned runs, hits allowed, innings pitched, games started, batters faced, and complete games. He led his league in wins during five seasons and pitched three no-hitters, including a perfect game in 1904.\nIn 1937, Young was elected to the National Baseball Hall of Fame. He is often regarded as one of the greatest pitchers of all time, as well as a pioneer of modern pitching. In 1956, one year after his death, the Cy Young Award was created to annually honor the best pitcher in the Major Leagues (later each League) of the previous season, cementing his name as synonymous with excellence in pitching.\nEarly life.\nCy Young was the oldest child born to Nancy (Mottmiller) and McKinzie Young Jr., and was christened Denton True Young. He was of part German descent. The couple had four more children: Jesse Carlton, Alonzo, Ella, and Anthony. When the couple married, McKinzie's father gave him the of farm land he owned. Young was born in Gilmore, a tiny farming community located in Washington Township, Tuscarawas County, Ohio. He was raised on one of the local farms and went by the name Dent Young in his early years. Young was also known as \"Farmer Young\" and \"Farmboy Young\". Young stopped his formal education after he completed the sixth grade so he could help out on the family's farm. In 1885, Young moved with his father to Nebraska, and in the summer of 1887, they returned to Gilmore.\nYoung played for many amateur baseball leagues during his youth, including a semi-professional Carrollton team in 1888. Young pitched and played second base. The first box score known containing the name Young came from that season. In that game, Young played first base and had three hits in three at-bats. After the season, Young received an offer to play for the minor league Canton team, which started Young's professional career.\nProfessional career.\nMinor leagues.\nYoung began his professional career in 1890 with the Canton, Ohio based Canton Nadjys, team of the Tri-State League, a professional minor league. During his tryout, Young impressed the scouts, recalling years later, \"I almost tore the boards off the grandstand with my fast ball.\" Cy Young's nickname came from the fences that he had destroyed using his fastball. The fences looked like a cyclone had hit them. Reporters later shortened the name to \"Cy\", which became the nickname Young used for the rest of his life. During his one year with Canton, he was 15-15.\nFranchises in the National League, the major professional baseball league at the time, wanted the best players available to them. Therefore, in 1890, Young signed with the Cleveland Spiders, a team that had moved from the American Association to the National League the previous year.\nCleveland Spiders (1890\u20131898).\nOn August 6, 1890, Young's major league debut, he pitched a three-hit 8\u20131 victory over the Chicago Colts. While Young was with the Spiders, Chief Zimmer was his catcher more often than any other player. Bill James, a baseball statistician, estimated that Zimmer caught Young in more games than any other battery in baseball history. Early on, Young established himself as one of the harder-throwing pitchers in the game. Bill James wrote that Zimmer often put a piece of beefsteak inside his baseball glove to protect his catching hand from Young's fastball. In the absence of radar guns, however, it is impossible to say just how hard Young actually threw. Young continued to perform at a high level during the 1890 season. On the last day of the season, Young won both games of a doubleheader. In the first weeks of Young's career, Cap Anson, the player-manager of the Chicago Colts spotted Young's ability. Anson told Spiders manager Gus Schmelz, \"He's too green to do your club much good, but I believe if I taught him what I know, I might make a pitcher out of him in a couple of years. He's not worth it now, but I'm willing to give you $1,000 ($ today) for him.\" Schmelz replied, \"Cap, you can keep your thousand and we'll keep the rube.\"\nTwo years after Young's debut, the National League moved the pitcher's position back by . Since 1881, pitchers had pitched within a \"box\" whose front line was from home base, and since 1887 they had been compelled to toe the back line of the box when delivering the ball. The back line was away from home. In 1893, was added to the back line, yielding the modern pitching distance of . In the book \"The Neyer/James Guide to Pitchers\", sports journalist Rob Neyer wrote that the speed with which pitchers like Cy Young, Amos Rusie, and Jouett Meekin threw was the impetus that caused the move.\nThe 1892 regular season was a success for Young, who led the National League in wins (36), ERA (1.93), and shutouts (9). Just as many contemporary Minor League Baseball leagues operate today, the National League was using a split season format during the 1892 season. The Boston Beaneaters won the first half and the Spiders won the second-half, with a best-of-nine series determining the league champion. Despite the Spiders' second-half run, the Beaneaters swept the series, five games to none. Young pitched three complete games: he lost two and one ended in a scoreless tie.\nThe Spiders faced the Baltimore Orioles in the Temple Cup, a precursor to the World Series, in 1895. Young won three games in the series and Cleveland won the Cup, four games to one. It was around this time that Young added what he called a \"slow ball\" to his pitching repertoire to reduce stress on his arm. The pitch today is called a changeup. In 1896, Young lost a no-hitter with two outs in the ninth inning when Ed Delahanty of the Philadelphia Phillies hit a single. On September 18, 1897, Young pitched the first no-hitter of his career in a game against the Cincinnati Reds. Although Young did not walk a batter, the Spiders committed four errors while on defense. One of the errors had originally been ruled a hit, but the Cleveland third baseman sent a note to the press box after the eighth inning, saying he had made an error, and the ruling was changed. Young later said that, despite his teammate's gesture, he considered the game to be a one-hitter.\nSt. Louis Perfectos / Cardinals (1899\u20131900).\nPrior to the 1899 season, Frank Robison, the Spiders owner, bought the St. Louis Browns, thus owning two clubs simultaneously. The Browns were renamed the \"Perfectos\", and restocked with Cleveland talent. Just weeks before the season opener, most of the better Spiders players were transferred to St. Louis, including three future Hall of Famers: Young, Jesse Burkett, and Bobby Wallace. The roster maneuvers failed to create a powerhouse Perfectos team, as St. Louis finished fifth in both 1899 and 1900. Meanwhile, the depleted Spiders lost 134 games, the most in MLB history, before folding. Young spent two years with St. Louis, which is where he found his favorite catcher, Lou Criger. The two men were teammates for a decade.\nBoston Americans / Red Sox (1901\u20131908).\nIn 1901, the rival American League declared major league status and set about raiding National League rosters. Young left St. Louis and joined the American League's Boston Americans for a $3,500 contract ($ today). Young would remain with the Boston team until 1909. In his first year in the American League, Young was dominant. Pitching to Criger, who had also jumped to Boston, Young led the league in wins, strikeouts, and ERA, thus earning the colloquial AL Triple Crown for pitchers. Young won almost 42% of his team's games in 1901, accounting for 33 of his team's 79 wins. In February 1902, before the start of the baseball season, Young served as a pitching coach at Harvard University. The sixth-grade graduate instructing Harvard students delighted Boston newspapers. The following year, Young coached at Mercer University during the spring. The team went on to win the Georgia state championship in 1903, 1904, and 1905.\nThe Boston Americans played the Pittsburgh Pirates in the first modern World Series in 1903. Young, who started Game One against the visiting Pirates, thus threw the first pitch in modern World Series history. The Pirates scored four runs in that first inning, and Young lost the game. Young performed better in subsequent games, winning his next two starts. He also drove in three runs in Game Five. Young finished the series with a 2\u20131 record and a 1.85 ERA in four appearances, and Boston defeated Pittsburgh, five games to three.\nAfter one-hitting Boston on May 2, 1904, Philadelphia Athletics pitcher Rube Waddell taunted Young to face him so that he could repeat his performance against Boston's ace. Three days later, Young pitched a perfect game against Waddell and the Athletics. It was the first perfect game in American League history. Waddell was the 27th and last batter, and when he flied out, Young shouted, \"How do you like that, you hayseed?\"\nWaddell had picked an inauspicious time to issue his challenge. Young's perfect game was the centerpiece of a pitching streak. Young set major league records for the most consecutive scoreless innings pitched and the most consecutive innings without allowing a hit; the latter record still stands at innings, or 76 hitless batters. Even after he allowed a hit, Young's scoreless streak reached a then-record 45 shutout innings.\nBefore Young, only two pitchers had thrown perfect games. This occurred in 1880, when Lee Richmond and John Montgomery Ward pitched perfect games within five days of each other, although under somewhat different rules: the front edge of the pitcher's box was only from home base (the modern release point is about farther away); walks required eight balls; and pitchers were obliged to throw side-armed. Young's perfect game was the first under the modern rules established in 1893. One year later, on July 4, 1905, Rube Waddell beat Young and the Americans, 4\u20132, in a 20-inning matchup. Young pitched 13 consecutive scoreless innings before he gave up a pair of unearned runs in the final inning. Young did not walk a batter and was later quoted: \"For my part, I think it was the greatest game of ball I ever took part in.\" In 1907, Young and Waddell faced off in a scoreless 13-inning tie.\nIn 1908, Young pitched the third no-hitter of his career. Three months past his 41st birthday, he was the oldest pitcher to record a no-hitter, a record which would stand 82 years until 43-year-old Nolan Ryan broke it. Only a walk kept Young from his second perfect game. After that runner was caught stealing, no other batter reached base. At the time, Young was the second-oldest player in either league. In another game one month before his no-hitter, he allowed just one single while facing 28 batters. On August 13, 1908, the league celebrated \"Cy Young Day\". No American League games were played on that day, and a group of All-Stars from the league's other teams gathered in Boston to play against Young and the Red Sox. When the season ended, he posted a 1.26 ERA, which gave him not only the lowest in his career, but also a major league record of being the oldest pitcher with 150+ innings and an ERA under 1.50.\nCleveland Naps (1909\u20131911).\nYoung was traded back to Cleveland, the place where he played over half his career, before the 1909 season, to the Cleveland Naps of the American League. The following season, 1910, he won his 500th career game on July 19 against Washington.\nBoston Rustlers (1911) and retirement.\nHe split 1911, his final year, between the Naps and the Boston Rustlers. On September 22, 1911, Young shut out the Pittsburgh Pirates, 1\u20130, for his last career victory. In his final start two weeks later, the last eight batters of Young's career combined to hit a triple, four singles, and three doubles. By the time of his retirement, Young's control had faltered. He had also gained weight. In two of his last three years, he was the oldest player in the league.\nCareer accomplishments.\nYoung established numerous pitching records, some of which have stood for over a century. Young compiled 511 wins, which is the most in major league history and 94 ahead of Walter Johnson, second on the list. At the time of Young's retirement, Pud Galvin had the second most career wins with 364. In addition to wins, Young still holds the major league records for most career innings pitched (7,356), most career games started (815), and most complete games (749). He also retired with 316 losses, the most in MLB history. Young's career record for strikeouts was broken by Johnson in 1921. Young's 76 career shutouts are fourth all-time.\nYoung led his league in wins five times (1892, 1895, and 1901\u20131903), finishing second twice. His career high was 36 in 1892. He won at least 30 games in a season five times. He had 15 seasons with 20 or more wins, two more than Christy Mathewson and Warren Spahn. Young won two ERA titles during his career, in 1892 (1.93) and in 1901 (1.62), and was three times the runner-up. Young's earned run average was below 2.00 six times, but it was not uncommon during the dead-ball era. Although Young threw over 400 innings in each of his first four full seasons, he did not lead his league until 1902. He had 40 or more complete games nine times. Young also led his league in strikeouts twice (140 in 1896 and 158 in 1901), and in shutouts seven times. Young led his league in fewest walks per nine innings fourteen times and finished second once. Only twice in his 22-year career did he finish lower than 5th in the category. Although the WHIP ratio was not calculated until well after Young's death, he was retroactively league leader seven times and was second or third another seven times.\nYoung is tied with Roger Clemens for the most career wins by a Boston Red Sox pitcher: they each won 192 games while with the franchise. In addition, Young pitched three no-hitters, including the third perfect game in baseball history, first in baseball's \"modern era\". Young also was an above average hitting pitcher. He posted a .210 batting average (623-for-2960) with 325 runs, 290 RBIs, 18 home runs, and 81 walks. From 1891 through 1905, he drove in 10 or more runs for 15 straight seasons, with a high of 28 in 1896.\nPitching style.\nParticularly after his fastball slowed, Young relied upon his control. He was once quoted as saying, \"Some may have thought it was essential to know how to curve a ball before anything else. Experience, to my mind, teaches to the contrary. Any young player who has good control will become a successful curve pitcher long before the pitcher who is endeavoring to master both curves and control at the same time. The curve is merely an accessory to control.\" In addition to his exceptional control, Young was also a workhorse who avoided injury, owing partly to his ability to pitch in different arm positions (overhand, three-quarters, sidearm and even submarine). For 19 consecutive years, from 1891 through 1909, Young was in his league's top 10 for innings pitched; in 14 of the seasons, he was in the top five. Not until 1900, a decade into his career, did Young pitch two consecutive incomplete games. By habit, Young restricted his practice throws in spring training. \"I figured the old arm had just so many throws in it,\" said Young, \"and there wasn't any use wasting them.\" He once described his approach before a game:\nI never warmed up ten, fifteen minutes before a game like most pitchers do. I'd loosen up, three, four minutes. Five at the outside. And I never went to the bullpen. Oh, I'd relieve all right, plenty of times, but I went right from the bench to the box, and I'd take a few warm-up pitches and be ready. Then I had good control. I aimed to make the batter hit the ball, and I threw as few pitches as possible. That's why I was able to work every other day.\nLater life.\nIn 1910, it was reported that Young became a vegetarian, after baseball and working on his farm. In 1913, he served as manager of the Cleveland Green Sox of the Federal League, which was at the time an outlaw league. However, he never worked in baseball after that.\nYoung was a Freemason.\nIn 1916, he ran for county treasurer in Tuscarawas County, Ohio.\nYoung's wife, Roba, whom he had known since childhood, died in 1933. After she died, Young tried several jobs, and eventually moved in with friends John and Ruth Benedum and did odd jobs for them. Young took part in many baseball events after his retirement. In 1937, 26 years after he retired from baseball, Young was inducted into the Baseball Hall of Fame. He was among the first to donate mementos to the Hall.\nBy 1940, Young's only source of income was stock dividends of $300 per year ($ today). He appeared on the television show \"I've Got a Secret\" on April 13, 1955. On November 4, 1955, Young died on the Benedums' farm at the age of 88. He was buried in Peoli, Ohio.\nLegacy.\nYoung's career is seen as a bridge from baseball's earliest days to its modern era; he pitched against stars such as Cap Anson, already an established player when the National League was first formed in 1876, as well as against Eddie Collins, who played until 1930. When Young's career began, pitchers delivered the baseball underhand and fouls were not counted as strikes. The pitcher's mound was not moved back to its present position of until Young's fourth season; he did not wear a glove until his sixth season.\nYoung was elected to the National Baseball Hall of Fame in 1937. In 1956, about one year after Young's death, the Cy Young Award was created to honor the best pitcher in Major League Baseball for each season. The first award was given to Brooklyn's Don Newcombe. Originally, it was a single award covering all of baseball. The honor was divided into two Cy Young Awards in 1967, one for each league.\nOn September 23, 1993, a statue dedicated to him was unveiled by Northeastern University on the site of the Red Sox's original stadium, the Huntington Avenue Grounds. It was there that Young had pitched the first game of the 1903 World Series, as well as the first perfect game in the modern era of baseball. A home plate-shaped plaque next to the statue reads:\nOn October 1, 1903 the first modern World Series between the American League champion Boston Pilgrims (later known as the Red Sox) and the National League champion Pittsburgh Pirates was played on this site. General admission tickets were fifty cents. The Pilgrims, led by twenty-eight game winner Cy Young, trailed the series three games to one but then swept four consecutive victories to win the championship five games to three.\nIn 1999, 88 years after his final major league appearance and 44 years after his death, editors at \"The Sporting News\" ranked Young 14th on their list of \"Baseball's 100 Greatest Players\". That same year, baseball fans named him to the Major League Baseball All-Century Team."}
{"id": "6851", "revid": "1272679469", "url": "https://en.wikipedia.org/wiki?curid=6851", "title": "Coronation Street", "text": "Coronation Street (colloquially referred to as Corrie) is a British television soap opera created by Granada Television and shown on ITV since 9 December 1960. The programme centres on a cobbled, terraced street in the fictional town of Weatherfield in Greater Manchester. The location was itself based on Salford, the hometown of the show's first screenwriter and creator, Tony Warren.\nBroadcast twice weekly from its inception until 1989, \"Coronation Street\" has aired six times a week since 2017. Warren developed the concept for the series, which was initially rejected by Granada's founder Sidney Bernstein. Producer Harry Elton convinced Bernstein to commission 13 pilot episodes. The show has since become a significant part of British culture.\nCurrently produced by ITV Studios, the successor to Granada, the series is filmed at MediaCityUK and broadcast across all ITV regions, as well as internationally. In 2010, \"Coronation Street\" was recognised by \"Guinness World Records\" as the world's longest-running television soap opera upon its 50th anniversary.\n\"Coronation Street\" was originally influenced by kitchen-sink realism and is known for portraying a working-class community with a blend of humour and strong, relatable characters. As of 2019, it averages approximately five million viewers per episode. The series aired its 10,000th episode on 7 February 2020 and marked its 60th anniversary later that year.\nHistory.\n1960s.\nThe first episode of \"Coronation Street\" aired on 9 December 1960 at 7\u00a0pm. It initially received mixed reviews; \"Daily Mirror\" columnist Ken Irwin predicted the series would last only three weeks. The \"Daily Mirror\" also printed: \"The programme is doomed from the outset\u00a0... For there is little reality in this new serial, which apparently, we have to suffer twice a week.\" Granada Television had commissioned 13 episodes, with some inside the company doubting the show would last beyond its planned production run. However, viewers quickly connected with the programme's portrayal of relatable, everyday characters. The programme also made use of Northern English language and dialect; affectionate local terms like \"eh, chuck?\", \"nowt\" (, from \"nought\", meaning \"nothing\"), and \"by 'eck!\" became widely heard on British television for the first time.\nEarly storylines included student Ken Barlow (William Roache), whose university education set him apart from his working-class family, including his brother David (Alan Rothwell) and parents Frank (Frank Pemberton) and Ida (Noel Dyson). Barlow's character offered commentary on broader social changes, including globalisation, as exemplified by his 1961 line: \"You can't go on just thinking about your own street these days. We're living with people on the other side of the world.\" Roache remains the only original cast member and holds the record as the longest-serving actor in \"Coronation Street\" and global soap opera history.\nIn March 1961, the show reached number one in the television ratings and remained there for the rest of the year. Earlier that year, a television audience measurement (TAM) showed that 75% of available viewers (approximately 15\u00a0million people) watched the programme. By 1964, \"Coronation Street\" attracted over 20 million regular viewers, the programme had over 20\u00a0million regular viewers, with ratings peaking on 2 December 1964, at 21.36\u00a0million viewers. \nIn 1964, Tim Aspinall became the series producer and implemented significant changes to the programme. Nine cast members were fired, the first being Lynne Carol, who had played Martha Longhurst since early in \"Coronation Street\"'s run. Carol\u2019s firing caused controversy, prompting her co-star Violet Carson (Ena Sharples) to threaten to quit, although she ultimately remained. The sacking was widely covered in the media, and Carol was mobbed by fans while out in public. Some, including \"Coronation Street\" writer H.V. Kershaw, criticised the decision as a bid to boost ratings.\nBy 1968, critics contended that the programme offered a nostalgic and outdated depiction of the urban working class, failing to reflect the contemporary realities of British society amid the decade's economic and social changes. Granada considered modernising the show with issue-driven plots, including Lucille Hewitt (Jennifer Moss) becoming addicted to drugs, Jerry Booth (Graham Haberfield) being in a storyline about homosexuality, Emily Nugent (Eileen Derbyshire) having an out-of-wedlock child, and introducing a black family. However, these ideas were abandoned due to concerns about viewer reactions. \nThe first episode filmed in color was broadcast on 3 November 1969. Since then, all episodes have been produced in color, with the exception of those created during the Colour Strike.\n1970s.\nSeveral main cast members departed \"Coronation Street\" in the early 1970s. In 1970, Arthur Leslie, who played Jack Walker, the landlord of the Rovers Return Inn, died suddenly, and his character was written out shortly thereafter. Anne Reid left the series in 1971, with her character, Valerie Barlow, dying due to accidental electrocution from a faulty hairdryer. In 1973, Pat Phoenix, who played Elsie Tanner, departed, and Doris Speed (Annie Walker) took a two-month leave of absence. During this period, ITV's other flagship soap opera, \"Crossroads\", experienced an increase in viewership, while \"Coronation Street\" saw a decline in ratings.\nThe departure of these cast members in the early 1970s prompted the writing team to expand the roles of supporting characters and introduce new ones. Deirdre Hunt (Anne Kirkbride) was introduced in 1972 and became a regular character in 1973. Bet Lynch (Julie Goodyear), who had became a regular character in 1970, became increasingly prominent as the decade progressed. Rita Littlewood (Barbara Knox), who had made a single appearance in 1964, returned and joined the regular cast in 1972. Mavis Riley (Thelma Barlow) became a regular character in 1973. Ivy Tyldesley (Lynne Perrie, later renamed \"Tilsley\") was introduced as a recurring character in 1971. Longtime characters Gail Potter (Helen Worth), Blanche Hunt (initially played by Patricia Cutts and later by Maggie Jones), and Vera Duckworth (Liz Dawn) were introduced in 1974.\nComic storylines, a hallmark of the series in the 1960s, had become less frequent in the early 1970s. They were revived under new producer Bill Podmore, who joined the programme in 1976 after previously working on Granada\u2019s comedy productions.\nIn September 1977, the \"News of the World\" quoted actor Stephen Hancock (Ernest Bishop) as saying \"The Street kills an actor. I'm just doing a job, not acting. The scriptwriters have turned me into Ernie Bishop. I've tried to resist it but it is very hard not to play the part all the time, even at home.\" Hancock also expressed frustration with the payment system, which guaranteed some long-serving actors\u2014including Pat Phoenix, Doris Speed, and Peter Adamson\u2014payment for every episode regardless of their appearances, while others were compensated only for episodes in which they appeared. Hancock's complaints led to a dispute with Podmore, dubbed \"The Godfather\" by the media, who refused to alter the system. Hancock ultimately resigned.\nTo write out Ernest Bishop while preserving the role of his wife, Emily (Eileen Derbyshire), the writers decided his character would be fatally shot during a payroll robbery at Mike Baldwin's (Johnny Briggs) factory. The episode, which aired on 11 January 1978, marked the first instance of such explicit violence on \"Coronation Street\", leading to a significant viewer backlash. Granada's switchboard was overwhelmed with complaints, and the Lobby Against TV Violence criticised the decision to air the storyline. Granada defended the plot, emphasising its focus on the grief and loss experienced by Emily.\nDespite its enduring popularity, critics argued that \"Coronation Street\" had grown complacent during this period, with the show relying on nostalgic depictions of working-class life rather than addressing contemporary social issues.\n1980s.\nBetween 1980 and 1984, \"Coronation Street\" faced the loss of many original cast members. Violet Carson (Ena Sharples) retired in 1980 and Doris Speed (Annie Walker) retired in 1983, Pat Phoenix (Elsie Tanner) left the programme permanently in 1984. Jack Howarth died in 1984 and his character, Albert Tatlock, was written out off-screen. By May 1984, William Roache (Ken Barlow) was the sole remaining actor from the programme's original cast.\nCharacters like Phyllis Pearce (Jill Summers), Vera and Jack Duckworth (Liz Dawn and Bill Tarmey), and Percy Sugden (Bill Waddington) took on roles reminiscent of earlier characters. The show introduced its first major black character, Shirley Armitage (Lisa Lewis), as a machinist at Baldwin\u2019s Casuals in 1983.\nEstablished characters were assigned new roles, and new characters were introduced to fill the gaps left by those who departed. Phyllis Pearce (Jill Summers) was hailed as the new Ena Sharples in 1982, the Duckworths moved into No.9 in 1983 and slipped into the role once held by the Ogdens, while Percy Sugden (Bill Waddington) appeared in 1983 and took over the grumpy war veteran role from Albert Tatlock. The question of who would take over the Rovers Return after Annie Walker's 1983 exit was answered in 1985 when Bet Lynch (who also mirrored the vulnerability and strength of Elsie Tanner) was installed as landlady. In 1983, Shirley Armitage (Lisa Lewis) became the first major Black character in the programme.\nIn 1983, Peter Adamson, who had played Len Fairclough since 1961, was dismissed for breaching his contract. Granada had previously warned Adamson for publishing unauthorised newspaper articles that criticised the show and its cast. Producer Bill Podmore terminated Adamson's contract after discovering he had sold his memoirs despite the prior warning. The sacking coincided with allegations of Adamson having indecently assaulted two eight-year-old girls in a swimming pool. Granada Television gave Adamson financial support through his legal problems, with a Crown Court jury finding him not guilty in July 1983. Adamson's dispute over his memoirs and newspaper articles was not known to the public and the media reported that Adamson had been dismissed because of the allegations. Len Fairclough was killed off-screen in a motorway crash while returning home from an affair in December 1983. Adamson celebrated the character's death by delivering an obituary on TV-am dressed as an undertaker. \nNew soap operas began airing on British television in the 1980s, with Channel 4 launching \"Brookside\" in 1982 and the BBC debuting \"EastEnders\" in 1985. Both soaps presented a grittier, more contemporary view of British life, contrasting with \"Coronation Street\"'s nostalgic tone. \"EastEnders\" regularly obtained higher viewing figures than \"Coronation Street\" due to its omnibus episodes shown at weekends. Despite this, \"Coronation Street\" maintained strong ratings.\nBetween 1988 and 1989, many aspects of the show were modernised by new producer David Liddiment. A new exterior set had been built in 1982, and in 1989 it was redeveloped to include new houses and shops. Production techniques were also changed with a new studio being built, and the inclusion of more location filming, which had moved exterior scenes from being shot on film to videotape in 1988. Due to new pressures, an introduction of the third weekly episode aired on 20 October 1989, to broadcast each Friday at 7:30\u00a0pm.\nIn 1988, Christopher Quinten, who had played Brian Tilsley since 1978, informed Granada of his intention to move to the United States to marry Leeza Gibbons and pursue an acting career in Los Angeles. Quinten sought assurances that his role would remain open for a potential return. However, producers decided that Tilsley would be killed off. Quinten was in Los Angeles when the decision was made and threatened to quit abruptly. Co-star Helen Worth convinced him to film his final scenes. Brian Tilsley's death, aired on 15 February 1989, depicted him being fatally stabbed while defending a young woman outside a nightclub. The storyline attracted viewer complaints, with Mary Whitehouse condemning the portrayal of violence.\nOne of \"Coronation Street's\" most prominent storylines in the 1980s was the engagement and marriage of Ken Barlow and Deirdre Langton (Anne Kirkbride). In July 1981, their wedding was watched by over 15\u00a0million viewers\u00a0\u2013 more viewers than ITV's coverage of the wedding of Prince Charles and Lady Diana Spencer two days later. Deirdre Barlow\u2019s affair with Mike Baldwin (Johnny Briggs) in 1983, garnered significant media attention, and began an ongoing feud that followed between Ken Barlow and Mike Baldwin.\nOther notable marriages included Alf Roberts (Bryan Mosley) to Audrey Potter (Sue Nicholls) in 1985, Mike Baldwin to Ken Barlow's daughter Susan (Wendy Jane Walker) in 1986, Kevin Webster (Michael Le Vell) to Sally Seddon (Sally Whittaker) in 1986, Bet Lynch to Alec Gilroy (Roy Barraclough) in 1987, and Ivy Tilsley to Don Brennan (Geoffrey Hinsliff) in 1988. The long-awaited marriage of Mavis Riley and Derek Wilton (Peter Baldwin) occurred in 1988 after over a decade of on-and-off romance and a failed marriage attempt in 1984. \nThe psychological abuse of Rita Fairclough by Alan Bradley culminated in his death under a Blackpool tram in December 1989, achieving a combined viewership of 26.93 million for the episodes where Alan went into hiding and later tried to kill Rita.\nJean Alexander, who played Hilda Ogden on the programme starting in 1964, left \"Coronation Street\" in 1987. Her final aired on Christmas Day 1987 with a combined audience (original and omnibus) of 26.7\u00a0million. Between 1986 and 1989, the storyline of Rita Fairclough's (Barbara Knox) domestic abuse at the hands of her partner Alan Bradley (Mark Eden), followed by his death after being struck by a Blackpool tram in December 1989, unfolded. This plotline brought the show its highest-ever combined viewing figure, with nearly 27 million viewers watching a March 1989 episode where Bradley is on the run from the police after attempting to kill Rita. This record is sometimes mistakenly attributed to the tram death episode aired on 8 December 1989.\n1990s.\nIn 1992, William Rees-Mogg, Chairman of the Broadcasting Standards Council, criticised \"Coronation Street\" for its low representation of ethnic minorities and its nostalgic portrayal of a bygone era. This was seen as unreflective of Greater Manchester, where many neighbourhoods had significant Black and Asian populations. Headlines such as \"Coronation Street shuts out blacks\" (\"The Times\") and \"'Put colour in t'Street\" (\"Daily Mirror\") reflected the controversy. Patrick Stoddart of \"The Times\" defended the show, stating: \"he millions who watch \"Coronation Street\"\u00a0\u2013 and who will continue to do so despite Lord Rees-Mogg\u00a0\u2013 know real life when they see it\u00a0... in the most confident and accomplished soap opera television has ever seen\" While Black and Asian characters had appeared sporadically, the first regular non-white family, the Desai family, was introduced in 1999.\nIn 1990, new characters Des Barnes (Philip Middlemiss) and Steph Barnes (Amelia Bullmore) moved to Coronation Street and were labeled yuppies from the media. Raquel Wolstenhulme (Sarah Lancashire) debuted in 1991 and became one of the era's most popular characters, departing in 1996 with a brief return in 2000. The McDonald family\u2013Liz (Beverley Callard), Jim (Charles Lawson), Steve (Simon Gregson), and Andy (Nicholas Cochrane)\u2013were introduced in 1989 and became major characters in the 1990s. Other notable arrivals included Maud Grimes (Elizabeth Bradley), a wheelchair user and pensioner, in 1993; Roy Cropper (David Neilson), a caf\u00e9 owner, in 1995; young married couple Gary and Judy Mallett (Ian Mercer and Gaynor Faye) in 1995; and butcher Fred Elliott (John Savident) in 1994 and his son Ashley Peacock (Steven Arnold) in 1995. The 1990s also saw an increase in slapstick and physical humour, exemplified by comedic characters including Reg Holdsworth (Ken Morley), a supermarket manager.\nIn 1997, Brian Park became producer with a vision to modernise the show and focus on younger characters. On his first day, he axed several long-standing characters, including Derek Wilton (Peter Baldwin), Don Brennan (Geoffrey Hinsliff), Percy Sugden (Bill Waddington), Bill Webster (Peter Armitage), Billy Williams (Frank Mills) and Maureen Holdsworth (Sherrie Hewson). The decision prompted Thelma Barlow, who played Mavis Wilton, to resign in protest at her co-star's dismissal. Several longtime writers, including Barry Hill, Adele Rose, and Julian Roach, resigned during this period.\nPark introduced younger characters between 1997 and 1998, such as a recast Nick Tilsley (Adam Rickitt), single mother Zoe Tattersall (Joanne Froggatt), and the problematic Battersby family. The show also began addressing more contemporary issues, including drug dealing, eco-activism, and religious cults. Hayley Patterson (Julie Hesmondhalgh), introduced during this era, became the first transgender character in a British soap opera and soon married Roy Cropper. Park, who resigned in 1998, cited this storyline as one of his most significant achievements.\nThe changes divided audiences, with some alienated by the modernised approach. Critics accused \"Coronation Street\" of losing its traditional charm while trying to emulate edgier rivals like \"Brookside\" and \"EastEnders\". Victor Lewis-Smith wrote in the \"Daily Mirror\": \"Apparently it doesn't matter that this is a first-class soap opera, superbly scripted and flawlessly performed by a seasoned repertory company.\"\nOne of the decade's most famous storylines occurred in 1998, when Deirdre Rachid (Anne Kirkbride) was wrongfully imprisoned after being deceived by con-man Jon Lindsay (Owen Aaronovitch). The episode depicting her sentencing attracted 19\u00a0million viewers and inspired the \"Free the Weatherfield One\" campaign, which generated significant media attention. Then-Prime Minister Tony Blair commented on the fictional case in Parliament. Deirdre was released after three weeks, with Granada confirming that her release had always been planned despite the media frenzy.\n2000s.\nOn 8 December 2000, \"Coronation Street\" celebrated its 40th anniversary with a live, hour-long episode. King Charles III (then Prince of Wales) appeared as himself. Earlier that year, 13-year-old Sarah-Louise Platt (Tina O'Brien) became pregnant, giving birth to a daughter, Bethany, on 4 June. The February episode where Gail was told of her daughter's pregnancy was watched by 15\u00a0million viewers. The programme continued to tackle issue-led storylines, including Toyah Battersby (Georgia Taylor) getting raped, Roy and Hayley Cropper (David Neilson and Julie Hesmondhalgh) abducting their foster child, Sarah Platt\u2019s Internet chat room abduction, and Alma Halliwell's (Amanda Barrie) 2001 death from cervical cancer. These storylines proved unpopular with viewers and led to a decline in ratings. As a result, in October 2001, producer Jane Macnaught was reassigned, and Carolyn Reynolds took over. In 2002, Kieran Roberts became producer, aiming to reintroduce \"gentle storylines and humour,\" steering the show away from competing with other soaps.\nIn July 2002, Gail Platt married Richard Hillman (Brian Capron), a financial advisor who had left Duggie Ferguson (John Bowe) to die after a fall during an argument, murdered his ex-wife Patricia (Annabelle Apsion), and later killed their neighbor Maxine Peacock (Tracy Shaw). He also attempted to kill his mother-in-law, Audrey Roberts (Sue Nicholls), and longtime family friend, Emily Bishop (Eileen Derbyshire), all for financial gain as his debts mounted. Hillman confessed his crimes to Gail in a two-hander episode in February 2003 before returning weeks later with the intention of killing Gail, her children Sarah and David (Jack P. Shepherd), and granddaughter Bethany by driving them into a canal. While the Platt family survived, Hillman drowned. This storyline received widespread media attention, with viewing figures peaking at 19.4\u00a0million. \nTodd Grimshaw (Bruno Langley) became \"Corrie's\" first regular homosexual character. In 2003, another gay male character was introduced, Sean Tully (Antony Cotton). The bigamy of Peter Barlow (Chris Gascoyne) and his addiction to alcohol, later in the decade, Maya Sharma's (Sasha Behar) revenge on former lover Dev Alahan (Jimmi Harkishin), Charlie Stubbs's (Bill Ward) psychological abuse of Shelley Unwin (Sally Lindsay), and the deaths of Mike Baldwin (Johnny Briggs), Vera Duckworth (Liz Dawn) and Fred Elliott (John Savident). In 2007, Tracy Barlow (Kate Ford) murdered Charlie Stubbs and claiming it was self-defence; the audience during this storyline peaked at 13.3\u00a0million. At the 2007 British Soap Awards, it won Best Storyline, and Ford was voted Best Actress for her portrayal.\nIn July 2007, after 34 years in the role of Vera Duckworth, Liz Dawn left the show due to ill health. After conversation between Dawn and producers Kieran Roberts and Steve Frost, the decision was made to kill Vera off.\nTina O'Brien revealed in the British press on 4 April 2007 that she would be leaving \"Coronation Street\" later in the year. Sarah-Louise, who was involved in some of the decade's most controversial stories, left in December 2007 with her daughter, Bethany. In 2008, Michelle learning that Ryan (Ben Thompson) was not her biological son, having been accidentally swapped at birth with Alex Neeson (Dario Coates). Carla Connor (Alison King) turned to Liam for comfort and developed feelings for him. In spite of knowing about her feelings, Liam married Maria Sutherland (Samia Longchambon). Maria and Liam's baby son was stillborn in April, and during an estrangement from Maria upon the death of their baby, Liam had a one-night stand with Carla, a story which helped pave the way for his departure.\nIn August 2008, Jed Stone (Kenneth Cope) returned after 42 years. Liam Connor and his ex-sister-in-law Carla gave into their feelings for each other and began an affair. Carla's fianc\u00e9e Tony Gordon (Gray O'Brien) discovered the affair and had Liam killed in a hit-and-run in October. Carla struggled to come to terms with Liam's death, but decided she still loved Tony and married him on 3 December, in an episode attracting 10.3\u00a0million viewers. In April 2009 it was revealed that Eileen Grimshaw's (Sue Cleaver) father, Colin (Edward de Souza) \u2013 the son of Elsie Tanner's (Pat Phoenix) cousin Arnley \u2013 had slept with Eileen's old classmate, Paula Carp (Sharon Duce) while she was still at school, and that Paula's daughter Julie (Katy Cavanagh) was in fact also Colin's daughter. Other stories in 2009 included Maria giving birth to Liam's son and her subsequent relationship with Liam's killer Tony, Steve McDonald's (Simon Gregson) marriage to Becky Granger (Katherine Kelly) and Kevin Webster's (Michael Le Vell) affair with Molly Dobbs (Vicky Binns). On Christmas Day 2009, Sally Webster (Sally Dynevor) told husband Kevin that she had breast cancer, just as he was about to leave her for lover Molly.\n2010s.\nThe show began broadcasting in high-definition in May 2010, and on 17 September that year, \"Coronation Street\" entered \"Guinness World Records\" as the world's longest-running television soap opera after the American soap opera \"As the World Turns\" concluded. William Roache was listed as the world's longest-running soap actor. \"Coronation Street\" 50th anniversary week was celebrated with seven episodes, plus a special one-hour live episode, broadcast from 6\u201310 December. The episodes averaged 14\u00a0million viewers, a 52.1% share of the audience. The anniversary was also publicised with ITV specials and news broadcasts. In the storyline, Nick Tilsley and Leanne Battersby's bar \u2014 The Joinery \u2014 exploded during Peter Barlow's stag party. As a result, the viaduct was destroyed, sending a Metrolink tram careering onto the street, destroying D&amp;S Alahan's Corner Shop and The Kabin. Two characters, Ashley Peacock (Steven Arnold) and Molly Dobbs (Vicky Binns), along with an unknown taxi driver, were killed as a result of the disaster. Rita Sullivan (Barbara Knox) survived, despite being trapped under the rubble of her destroyed shop. Fiz Stape (Jennie McAlpine) prematurely gave birth to a baby girl, Hope. The episode of \"EastEnders\" broadcast on the same day as \"Coronation Street\" 50th anniversary episode included a tribute, with the character Dot Branning (June Brown, who briefly appeared in the show during the 1970s) saying that she never misses an episode of \"Coronation Street\".\n2020s.\nOn 7 February 2020, with its 60th anniversary ten months away, \"Coronation Street\" aired its landmark 10,000th episode, the runtime of which was extended to 60 minutes. Producers stated that the episode would contain \"a nostalgic trip down memory lane\" and \"a nod to its own past\". A month later, ITV announced that production on the soap would have to be suspended, as the United Kingdom was put into a national lockdown due to the COVID-19 pandemic (see impact of the COVID-19 pandemic on television).\nAfter an 11-week intermission for all cast and crew members, filming resumed in June 2020. The episodes would feature social distancing to adhere to the guidelines set by the British government, and it was confirmed that all actors over 70, as well as those with underlying health conditions, would not be allowed to be on set until it was safe to do so. This included \"Coronation Street\" veterans William Roache (Ken Barlow) at 88, Barbara Knox (Rita Tanner) at 87, Malcolm Hebden (Norris Cole) at 80 and Sue Nicholls (Audrey Roberts) at 76. It was deemed safe for Maureen Lipman (Evelyn Plummer) and David Neilson (Roy Cropper) to continue. By December all cast members had returned to set and on Wednesday 9 December 2020, the soap celebrated its 60th anniversary, with original plans for the episode forced to change due to COVID-19 guidelines. The anniversary week saw the conclusion of a long-running coercive control storyline that began in May 2019, with Geoff Metcalfe (Ian Bartholomew) abusing Yasmeen Nazir (Shelley King). The showdown, which resulted in the death of Geoff allowed social distancing rules to be relaxed on the condition that the crew members involved formed a social bubble prior to the filming. In late 2021 series producer Iain MacLeod announced that the original plans for the 60th Anniversary would now take place in a special week of episodes in October 2021.\nOn 12 October 2021, it was announced that \"Coronation Street\" would partake in a special crossover event involving 7 British soaps to promote the topic of climate change ahead of the 2021 United Nations Climate Change Conference. During the week, beginning from 1 November, social media clips featuring Liam Cavanagh (Jonny McPherson) and Amelia Spencer (Daisy Campbell) from \"Emmerdale\", as well as Daniel Granger (Matthew Chambers) from \"Doctors\" were featured on the programme, while events from \"Holby City\" were also referenced. A similar clip featuring Maria Connor (Samia Longchambon) was also featured on \"EastEnders\". On 24 January 2022, ITV announced that as part of an overhaul of their evening programming, \"Coronation Street\" would permanently air as three 60-minute episodes per week from March 2022 onwards.\nIn June 2024, ITV announced that \"Coronation Street\"s third longest-serving cast member, Helen Worth, had decided to leave the soap after fifty years of portraying Gail Platt. The character made her departure in December 2024. Following this, several other cast exits began to be confirmed, with a mixture of producers axing the characters and cast members deciding to quit. In what the \"Metro\" described as a \"cast exodus\", these have included Sue Cleaver leaving her long-term role as Eileen Grimshaw and Charlotte Jordan leaving her role as Daisy Midgeley, as well as Debbie Webster (Sue Devaney) and Craig Tinker (Colson Smith) being written out of the series.\nCharacters.\nSince 1960, \"Coronation Street\" has featured many characters whose popularity with viewers and critics has differed greatly. The original cast was created by Tony Warren, with the characters of Ena Sharples (Violet Carson), Elsie Tanner (Pat Phoenix) and Annie Walker (Doris Speed) as central figures. These three women remained with the show for at least 20 years, and became archetypes of British soap opera, often being emulated by other serials. Ena was the street's busybody, battle-axe and self-proclaimed moral voice. Elsie was the tart with a heart, who was constantly hurt by men in the search for true love. Annie Walker, landlady of the Rovers Return Inn, had delusions of grandeur and saw herself as better than the other residents.\n\"Coronation Street\" became known for the portrayal of strong female characters, including original cast characters like Ena, Annie and Elsie, and later Hilda Ogden (Jean Alexander), who first appeared in 1964; all four became household names during the 1960s. Warren's programme was largely matriarchal, which some commentators put down to the female-dominant environment in which he grew up. Consequently, the show has a long tradition of downtrodden husbands, most famously Stan Ogden (Bernard Youens) and Jack Duckworth (Bill Tarmey), husbands of Hilda and Vera Duckworth (Liz Dawn), respectively.\nCoronation Street's longest-serving character, Ken Barlow (William Roache) entered the storyline as a young radical, reflecting the youth of 1960s Britain, where figures like the Beatles, the Rolling Stones and the model Twiggy were to reshape the concept of youthful rebellion. Though the rest of the original Barlow family were killed off before the end of the 1970s, Ken, who for 27 years was the only character from the first episode remaining, has remained the constant link throughout the entire series. In 2011, Dennis Tanner (Philip Lowrie), another character from the first episode, returned to \"Coronation Street\" after a 43-year absence. Since 1984, Ken Barlow has been the show's only remaining original character. Emily Bishop (Eileen Derbyshire) had appeared in the series since January 1961, when the show was just weeks old, and was the show's longest-serving female character before she departed in January 2016 after 55 years. Rita Tanner (Barbara Knox) appeared on the show for one episode in December 1964, before returning as a full-time cast member in January 1972. She is currently the second longest-serving original cast member on the show. Roache and Knox are also the two oldest-working cast members on the soap at 92 and 91 years-old respectively.\nStan and Hilda Ogden were introduced in 1964, with Hilda becoming one of the most famous British soap opera characters of all time. In a 1982 poll, she was voted fourth-most recognisable woman in Britain, after Queen Elizabeth The Queen Mother, Queen Elizabeth II and Diana, Princess of Wales. Hilda's best-known attributes were her pinny, hair curlers, and the \"muriel\" in her living room with three \"flying\" duck ornaments. Hilda Ogden's departure on Christmas Day 1987, remains the highest-rated episode of \"Coronation Street\" ever, with nearly 27,000,000 viewers. Stan Ogden had been killed off in 1984 following the death of actor Bernard Youens after a long illness which had restricted his appearances towards the end.\nBet Lynch (Julie Goodyear) first appeared in 1966, before becoming a regular in 1970, and went on to become one of the most famous \"Corrie\" characters. Bet stood as the central character of the show from 1985 until departing in 1995, often being dubbed as \"Queen of the Street\" by the media, and indeed herself. The character briefly returned in June 2002 and November 2003.\n\"Coronation Street\" and its characters often rely heavily on archetypes, with the characterisation of some of its current and recent cast based loosely on former characters. Phyllis Pearce (Jill Summers), Blanche Hunt (Maggie Jones) and Sylvia Goodwin (Stephanie Cole) embodied the role of the acid-tongued busybody originally held by Ena, Sally Webster (Sally Dynevor) has grown snobbish, like Annie, and a number of the programme's female characters, such as Carla Connor (Alison King), mirror the vulnerability of Elsie and Bet. Other recurring archetypes include the war veteran such as Albert Tatlock (Jack Howarth), Percy Sugden (Bill Waddington) and Gary Windass (Mikey North), the bumbling retail manager like Leonard Swindley (Arthur Lowe), Reg Holdsworth (Ken Morley) and Norris Cole (Malcolm Hebden), quick-tempered, tough tradesmen like Len Fairclough (Peter Adamson), Jim McDonald (Charles Lawson), Tommy Harris (Thomas Craig) and Owen Armstrong (Ian Puleston-Davies), and the perennial losers such as Stan and Hilda, Jack and Vera, Les Battersby (Bruce Jones), Beth Tinker (Lisa George) and Kirk Sutherland (Andrew Whyment).\nVillains are also common character types, such as Tracy Barlow (Kate Ford), Alan Bradley (Mark Eden), Jenny Bradley (Sally Ann Matthews), Rob Donovan (Marc Baylis), Frank Foster (Andrew Lancel), Tony Gordon (Gray O'Brien), Caz Hammond (Rhea Bailey), Richard Hillman (Brian Capron), Greg Kelly (Stephen Billington), Will Chatterton (Leon Ockenden), Nathan Curtis (Christopher Harper), Callum Logan (Sean Ward), Karl Munro (John Michie), Pat Phelan (Connor McIntyre), David Platt (Jack P. Shepherd), Maya Sharma (Sasha Behar), Kirsty Soames (Natalie Gumede), John Stape (Graeme Hawley), Geoff Metcalfe (Ian Bartholomew) and Gary Windass (Mikey North). The show's former archivist and scriptwriter Daran Little disagreed with the characterisation of the show as a collection of stereotypes. \"Rather, remember that Elsie, Ena and others were the first of their kind ever seen on British television. If later characters are stereotypes, it's because they are from the same original mould. It is the hundreds of programmes that have followed which have copied \"Coronation Street\".\"\nIn 2024, it was reported that the number of actors appearing in each storyline had been cut in order to reduce costs due to declining viewing figures.\nStorylines.\nMany topical issues have been tackled on Coronation Street, such as rape, including male and marital, historic sexual abuse, underage pregnancy, transgender issues, the right to die, racism, coercive control, cancer, homosexuality, domestic abuse, child grooming, and suicide, among others.\nKey storylines include: Mike and Deirdre's affair (1983), the death of Brian Tilsley (1989), Alan Bradley's abuse of Rita (1989), Kevin and Natalie's affair (1997), Deirdre is wrongfully jailed for fraud (1998), Sarah Platt's underage pregnancy (2000), Toyah's rape (2001), Alma's cancer (2001), Richard Hillman's serial killer storyline (2002\u20132003), Peter Barlow's bigamy (2003), Kevin and Molly's affair (2009), Kirsty's abuse of Tyrone (2012), Hayley's cancer (2013), Faye's underage pregnancy (2015), Bethany's grooming (2017), David's rape (2018), Aidan's suicide (2018), Sinead's diagnosis with cervical cancer (2019), Yasmeen's abuse (2020), Daisy's stalking hell (2023), Paul's MND (2023), Liam's bullying and suicidal thoughts (2023), and Lauren's disappearance and possible murder (2024).\nProduction.\nBroadcast format.\nBetween 9 December 1960 and 3 March 1961, \"Coronation Street\" was broadcast twice weekly, on Wednesday and Friday. During this period, the Friday episode was broadcast live, with the Wednesday episode being pre-recorded 15\u00a0minutes later. When the programme went fully networked on 6 March 1961, broadcast days changed to Monday and Wednesday. The last regular episode to be shown live was broadcast on 3 February 1961.\nThe series was transmitted in black and white for the majority of the 1960s. Preparations were made to film episode 923, to be transmitted Wednesday 29 October 1969, in colour. This installment featured the street's residents on a coach trip to the Lake District. In the end, suitable colour film stock for the cameras could not be found and the footage was shot in black and white. The following episode, transmitted Monday 3 November, was videotaped in colour but featured black and white film inserts and title sequence. Like BBC1, the ITV network was officially broadcast in black and white at this point (though programmes were actually broadcast in colour as early as July that year for colour transmission testing and adjustment) so the episode was seen by most in black and white.\nThe ITV network, like BBC1, began full colour transmissions on 15 November 1969. Daran Little, for many years the official programme archivist, claims that the first episode to be transmitted in colour was episode 930 shown on 24 November 1969. In October 1970, a technicians' dispute turned into the Colour Strike when sound staff were denied a pay rise given to camera staff the year before for working with colour recording equipment. The terms of the work-to-rule were that staff refused to work with the new equipment (though the old black and white equipment had been disposed of by then) and therefore programmes were recorded and transmitted in black and white, including \"Coronation Street\". The dispute was resolved in early 1971 and the last black and white episode was broadcast on 10 February 1971, although the episodes transmitted on 22 and 24 February 1971 had contained black and white location inserts.\nFrom 22 March 2010, \"Coronation Street\" was produced in 1080/50i for transmission on HDTV platforms on ITV HD. The first transmission in this format was episode 7351 on 31 May 2010 with a new set of titles and re-recorded theme tune. On 26 May 2010 ITV previewed the new HD titles on the \"Coronation Street\" website. Due to copyright reasons only viewers residing in the UK could see them on the ITV site.\nProduction staff.\n\"Coronation Street's\" creator, Tony Warren, wrote the first 13 episodes of the programme in 1960, and continued to write for the programme intermittently until 1976. He later became a novelist, but retained links with \"Coronation Street.\" Warren died in 2016.\nHarry Kershaw was the script editor for \"Coronation Street\" when the programme began in 1960, working alongside Tony Warren. Kershaw was also a script writer for the programme and the show's producer between 1962 and 1971. He remains the only person, along with John Finch, to have held the three posts of script editor, writer and producer. Adele Rose was \"Coronation Street\"'s first female writer and the show's longest-serving writer, completing 455 scripts between 1961 and 1998. She also created \"Byker Grove\". Rose also won a BAFTA award in 1993 for her work on the show.\nBill Podmore was the show's longest serving producer. By the time he stepped down in 1988 he had completed 13 years at the production helm. Nicknamed the \"godfather\" by the tabloid press, he was renowned for his tough, uncompromising style and was feared by both crew and cast alike. He is known for sacking Peter Adamson, the show's Len Fairclough, in 1983. Iain MacLeod is the current series producer.\nMichael Apted, known for the \"Up!\" series of documentaries, was a director on the programme in the early 1960s. This period of his career marked the first of his many collaborations with writer Jack Rosenthal. Rosenthal, noted for such television plays as \"Bar Mitzvah Boy\", began his career on the show, writing over 150 episodes between 1961 and 1969. Paul Abbott was a story editor on the programme in the 1980s and began writing episodes in 1989, but left in 1993 to produce \"Cracker\", for which he later wrote, before creating his own dramas such as \"Touching Evil\" and \"Shameless\". Russell T Davies was briefly a storyliner on the programme in the mid-1990s, also writing the script for the direct-to-video special \"\" He, too, has become a noted writer of his own high-profile television drama programmes, including \"Queer as Folk\" and the 2005 revival of \"Doctor Who\". Jimmy McGovern also wrote some episodes.\nTheme music.\nThe show's theme music, a cornet piece, accompanied by a brass band plus clarinet and double bass, reminiscent of northern band music, was written by Eric Spear. The original theme tune was called \"Lancashire Blues\" and Spear was paid a \u00a36 commission in 1960 to write it.\nThe identity of the trumpeter was not public knowledge until 1994, when jazz musician and journalist Ron Simmonds revealed that it was the Surrey musician Ronnie Hunt. He added, \"an attempt was made in later years to re-record that solo, using Stan Roderick, but it sounded too good, and they reverted to the old one.\" In 2004, the \"Manchester Evening News\" published a contradictory story that a young musician from Wilmslow called David Browning had played the original version. However, after investigating further, his story was found to be false, Browning not knowing that the original trumpet player Ronnie Hunt was still alive, proving that he was the true and rightful player that performed the solo. With his union pay stubs and contract, Browning was proven false.\nA new, completely re-recorded version of the theme tune replaced the original when the series started broadcasting in HD on 31 May 2010. It accompanied a new montage-style credits sequence featuring images of Manchester and Weatherfield. A reggae version of the theme tune was recorded by The I-Royals and released by Media Marvels and WEA in 1983.\nViewing figures.\nEpisodes in the 1960s, 1970s and 1980s, regularly attracted figures of between 18 and 21\u00a0million viewers, and during the 1990s and early 2000s, 14 to 16\u00a0million per episode would be typical. Like most terrestrial television in the UK, a decline in viewership has taken place and the show posts an average audience of just under 9\u00a0million per episode , remaining one of the highest rated programmes in the UK. \"EastEnders\" and \"Coronation Street\" have often competed for the highest rated show.\nThe episode that aired on 2 January 1985, in which Bet Lynch (Julie Goodyear) finds out she has got the job as manager of the Rovers Return, is the highest-rated single episode in the show's history, attracting 21.40 million viewers. The 25 December 1987 episode, where Hilda Ogden (Jean Alexander) leaves the street to start a new life as a housekeeper for long-term employer Dr Lowther, attracted a combined audience of 26.65 million for its original airing and omnibus repeat on 27 December 1987. This is the second-highest combined rating in the show's history. The show attracted its highest-ever combined rating of 26.93 million for the episode that aired on 15 (and 19) March 1989, where Rita Fairclough (Barbara Knox) is in hospital and Alan Bradley (Mark Eden) is hiding from the police after trying to kill Rita in the previous episode.\nBy the 2020s viewing figures dropped due to increased competition from streaming services and satellite channels, with the usually high-rated Christmas episode being viewed by only 2.6 million households in 2023, down from 2.8 million in 2022 and 8 million a decade previously. However, these figures are based on overnight ratings and do not include viewing via ITV's \"catch-up\" streaming service.\nSets.\nThe regular exterior buildings shown in Coronation Street include a row of terrace houses, several townhouses, and communal areas including a newsagents (The Kabin), a caf\u00e9 (Roy's Rolls), a general grocery shop (D&amp;S Alahan's), a factory (Underworld) and Rovers Return Inn public house. The Rovers Return Inn is the main meeting place for the show's characters.\nBetween 1960 and 1968, street scenes were transmitted/taped before a set constructed in a studio, with the house fronts reduced in scale to 3/4 and constructed from wood. In 1968 Granada built an outside set not all that different from the interior version previously used, with the wooden fa\u00e7ades from the studio simply being erected on the new site. When the show began broadcasting in colour, these were replaced with brick fa\u00e7ades, and back yards were added in the 1970s.\nIn 1982, a permanent full-street set was built in the Granada backlot, an area between Quay Street and Liverpool Road in Manchester. The set was constructed from reclaimed Salford brick. The set was updated in 1989 with the construction of a new factory, two shop units and three modern town houses on the south side of the street.\nBetween 1989 and 1999, the Granada Studios Tour allowed members of the public to visit the set. The exterior set was extended and updated in 1999. This update added to the Rosamund Street and Victoria Street fa\u00e7ades, and added a viaduct on Rosamund Street. Most interior scenes are shot in the adjoining purpose-built studio.\nIn 2008, Victoria Court, an apartment building full of luxury flats, was started on Victoria Street.\nIn 2014, production moved to a new site at Trafford Wharf, a former dock area about two miles to the east, part of the MediaCityUK complex. The Trafford Wharf backlot is built upon a former truck stop site next to the Imperial War Museum North. It took two years from start to finish to recreate the iconic Street. The houses were built to almost full scale after previously being three-quarter size. On 5 April 2014, the staff began to allow booked public visits to the old Quay Street set. An advert, with a voiceover from Victoria Wood, appeared on TV to advertise the tour. The tour was discontinued in December 2015.\nOn 12 March 2018, the extension of the Victoria Street set was officially unveiled. The new set featured a garden, featuring a memorial bench paying tribute to the 22 victims of the Manchester Arena bombing, including \"Coronation Street\" \"super fan\" Martyn Hett. The precinct includes a Greater Manchester Police station called Weatherfield Police station. As part of a product placement deal between three companies and ITV Studios, new additions include a tram stop station which is named Weatherfield North with Transport for Greater Manchester Metrolink branding, while shop front facades of Costa Coffee and the Weatherfield branded Co-op Food store interior scenes have been screened. Exterior scenes at the new set first aired on 20 April 2018.\nOn 20 April 2018, ITV announced that they had been granted official approval of planning permission to allow booked public visits to the MediaCityUK Trafford Wharf set. Tours commenced on weekends from 26 May 2018 onwards. The set was further expanded in March 2022, with the addition of the Weatherfield Precinct, which took six months to build, and was inspired by Salford. The new section of the set included a two-storey construction featuring maisonettes, a staircase and balcony leading to the properties, a piazza and an array of shops and units.\nBroadcast.\nUnited Kingdom.\nFor 60 years, \"Coronation Street\" has remained at the centre of ITV's prime time schedule. The programme is currently shown in the UK in three hour-long episodes, over three evenings a week on ITV in the 8\u00a0pm time slot - Mondays, Wednesdays and Fridays. Additional episodes have been broadcast at other times, such as between 22 and 26 November 2004, when eight episodes were shown including three 10pm outings. These late night episodes allowed for more graphic content when \u2018Mad\u2019 Maya (Maya Sharma) sought her revenge on Dev Alahan and Sunita Alahan.\nFrom Friday 9 December 1960 until Friday 3 March 1961, the programme was shown in two episodes broadcast on Wednesday and Friday at 7\u00a0pm. Schedules were changed, and from Monday 6 March 1961 until Wednesday 11 October 1989, the programme was shown in two episodes broadcast Monday and Wednesday at 7:30\u00a0pm. A third weekly episode was introduced on Friday 20 October 1989, broadcast at 7:30\u00a0pm. From 1996, an extra episode was broadcast at 7:30\u00a0pm on Sunday nights.\nAside from Granada, the programme originally appeared on the following stations of the ITV network: Anglia Television, Associated-Rediffusion, Television Wales and the West, Scottish Television, Southern Television and Ulster Television. From episode 14 on Wednesday 25 January 1961, Tyne Tees Television broadcast the programme. That left ATV in the Midlands as the only ITV station not carrying the show. When they decided to broadcast the programme, national transmission was changed from Wednesday and Friday at 7\u00a0pm to Monday and Wednesday at 7:30\u00a0pm and the programme became fully networked under this new arrangement from episode 25 on Monday 6 March 1961.\nAs the ITV network grew over the next few years, the programme was transmitted by these new stations on these dates onward: Westward Television from episode 40 on 1 May 1961, Border Television from episode 76 on 4 September 1961, Grampian Television from episode 84 on 2 October 1961, Channel Television from episode 180 on 3 September 1962 and Teledu Cymru (north and west Wales) from episode 184 on 17 September 1962. At this point, the ITV network became complete and the programme was broadcast almost continuously across the country at 7:30\u00a0pm on Monday and Wednesday for the next twenty-eight years.\nFrom episode 2981 on Friday 20 October 1989 at 7:30\u00a0pm, a third weekly episode was introduced and this increased to four episodes a week from episode 4096 on Sunday 24 November 1996, again at 7:30\u00a0pm. A second Monday episode was introduced in 2002 and was broadcast at 8:30\u00a0pm to usher in the return of Bet Lynch. The Monday 8:30\u00a0pm episode was used intermittently during the popular Richard Hillman storyline and became a regular feature from episode 5568 on Monday 25 August 2003.\nIn January 2008, ITV axed the Sunday episode, and instead aired a second episode on Fridays, at 8:30\u00a0pm, with the final Sunday episode airing on 6 January 2008, though some episodes thereafter continued to air occasionally on Sundays, usually for when an episode was displaced from one of its regular slots by a live football match. From 23 July 2009 to September 2012 the Wednesday show was replaced with an episode at 8:30\u00a0pm on Thursdays. A sixth weekly episode was added on Wednesdays at 8:30\u00a0pm from 20 September 2017.\nIn March 2020, it was revealed that episodes that were currently filming for future broadcast (as episodes are filmed a few weeks in advance) during the COVID-19 pandemic would be shown differently. Instead of six episodes a week, only three episodes would be broadcast, airing as normal on a Monday, Wednesday and Friday at the normal timeslot of 7:30\u00a0pm. The actions provided would be made effective starting from 30 March. Simultaneously, the announcement also mentioned that the elderly cast of the show would be \"written off\" due to health advice issued by Public Health England and the NHS. On 22 March, ITV released a statement confirming that filming of both \"Coronation Street\" and \"Emmerdale\" was suspended.\nIn June 2020, ITV announced that filming would resume on 9 June. However, due to the new health and safety measures, cast members over the age of 70 or with underlying health conditions did not come back on set, until the production could determine it is safe for them to return.\nIn July 2020, ITV announced that \"Coronation Street\" would return to the normal output of six episodes a week in September that year.\nIn October 2020, Maureen Lipman and David Neilson made their first appearances since July that year, as all cast members over the age of 70 had temporarily left the series earlier in the year. William Roache, Barbara Knox and Sue Nicholls returned in December.\nOn 22 January 2021, ITV announced that filming would be suspended from 25 January in order to rewrite \"stories and scripts as a consequence of the coronavirus pandemic\" and to \"review all health and safety requirements\". ITV also confirmed that this decision would not affect their ability to deliver six episodes a week.\nIn January 2022, it was announced that after 60 years in the 7.30\u00a0pm slot, \"Coronation Street\"s transmission time would move to 8pm due to the \"ITV Evening News\" having a longer duration, pushing \"Emmerdale\" into the 7.30\u00a0pm slot on weeknights. The double-bill episodes on Mondays, Wednesdays and Fridays have merged into hour-long slots on these days. The new scheduling went live on Monday 7 March 2022.\nRepeats and classic episodes.\nRepeat episodes, omnibus broadcasts and specials have been shown on various ITV channels. After several years on ITV2, in January 2008 the omnibus returned to the main ITV channel, where it was aired on Saturday mornings or afternoons, depending on the schedule and times.\nIn May 2008, it moved to Sunday mornings, until August 2008, when it returned to Saturdays. In January 2009, it moved back to Sunday mornings, usually broadcasting at around 9.25am until December 2010. In January 2011, the omnibus moved to Saturday mornings on ITV at 9.25am. During the Rugby World Cup, which took place in New Zealand, matches had to be broadcast on a Saturday morning, so the omnibus moved to Saturday lunchtimes/afternoons during September and October 2011. On 22 October 2011, the omnibus moved back to Saturday mornings at 9.25am on ITV. In January 2012, the omnibus moved to ITV2, and then moved to ITV3 in January 2020. In January 2022, the omnibus moved back to ITV2.\nOlder episodes were broadcast by satellite and cable channel Granada Plus from its launch in 1996. The first episodes shown were from episode 1588 (originally transmitted on Monday 5 April 1976) onwards. Originally listed and promoted as \"Classic Coronation Street\", the \"classic\" was dropped in early 2002, at which stage the episodes were from late 1989. By the time of the channel's closure in 2004, the repeats had reached February 1994.\nIn addition to this, \"specials\" were broadcast on Saturday afternoons in the early years of the channel, with several episodes based on a particular theme or character(s) shown. The last episode shown in these specials was from 1991. In addition, on 27 and 28 December 2003, several Christmas Day editions of the show were broadcast.\nITV3 began airing afternoon timeslot sequential reruns of \"Classic Coronation Street\" from 2 October 2017. Two classic episodes were retransmitted from Mondays to Fridays at 2:40\u00a0pm until 3:45\u00a0pm, starting from episode 2587 (originally transmitted on Wednesday 15 January 1986) onwards.\nTo mark the 60th anniversary of \"Coronation Street\", between 7 and 11 December 2020 at 10:00\u00a0pm\u201311:05\u00a0pm, ITV3 aired special episodes of the soap including \"Episode 1\", the tenth anniversary episode from December 1970, two episodes from the twentieth anniversary in December 1980, two episodes from the thirtieth anniversary in December 1990, the \"2000 live episode\" from the fortieth anniversary in December 2000, and the \"fiftieth anniversary episode\" which aired after a repeat of \"The Road to Coronation Street\".\nOn Easter Monday 2022, to commemorate the upcoming 90th birthday of William Roache, eight special \"Coronation Street\" Ken Barlow episodes were aired on 18 April 2022, at 10:25\u00a0am\u20132:35\u00a0pm. The episodes shown were \"Episode 1\" from December 1960, \"Ken and Deirdre Tie the Knot\" from July 1981, \"Ken's Affair\" from December 1989, \"Deirdre's Fling\" from January 2003, \"Steve and Karen's Wedding Shocker\" from February 2004, \"Ken and Deirdre's Second Wedding\" from April 2005, \"Ken and Deirdre's Holiday\" from August 2014, and\" Deirdre's Death\" from July 2015.\nInternational.\n\"Coronation Street\" is shown in various countries worldwide. YouTube has the first episode and many others available as reruns.\nThe programme was first aired in Australia in 1963 on TCN-9 Sydney, GTV-9 Melbourne and NWS-9 Adelaide, and by 1966 \"Coronation Street\" was more popular in Australia than in the UK. The show eventually left free-to-air television in Australia. It briefly returned to the Nine Network in a daytime slot during 1994\u20131995. In 2005, STW-9 Perth began to show episodes before the 6\u00a0pm news to improve the lead in to Nine News Perth, but this did not work and the show was cancelled a few months later. In 1996, pay-TV began and Arena began screening the series in one-hour instalments on Saturdays and Sundays at 6:30\u00a0pm EST. The series was later moved to pay-TV channel UKTV (now BBC UKTV), where it is still shown. \"Coronation Street\" is shown Mon-Thu at 7:20\u00a0pm EST and a double episode on Fridays, with episodes on the channel being one week behind UK broadcast.\nIn Canada, \"Coronation Street\" is broadcast on CBC Television. Until 2011, episodes were shown in Canada approximately 10 months after they aired in Britain; however, beginning in the fall of 2011, the CBC began showing two episodes every weekday, in order to catch up with the ITV showings, at 6:30\u00a0pm and 7\u00a0pm local time Monday-Friday, with an omnibus on Sundays at 7.30am. By May 2014, the CBC was only two weeks behind Britain, so the show was reduced to a single showing weeknights at 6:30\u00a0pm local time. The show debuted on Toronto's CBLT in July 1966. The 2002 edition of the \"Guinness Book of Records\" recognises the 1,144 episodes sold to the now-defunct CBC-owned Saskatoon, Saskatchewan, TV station CBKST by Granada TV on 31 May 1971 to be the largest number of TV shows ever purchased in one transaction. The show traditionally aired on weekday afternoons in Canada, with a Sunday morning omnibus. In 2004, CBC moved the weekday airings from their daytime slot to prime time. In light of austerity measures imposed on the CBC in 2012, which includes further cutbacks on non-Canadian programming, one of the foreign shows to remain on the CBC schedule is \"Coronation Street\", according to the CBC's director of content planning Christine Wilson, who commented: \"Unofficially I can tell you \"Coronation Street\" is coming back. If it didn't come back, something would happen on Parliament Hill.\" Kirstine Stewart, the head of the CBC's English-language division, once remarked: \"\"Coronation Street\" fans are the most loyal, except maybe for curling viewers, of all CBC viewers.\" As of mid 2022, Canada is about three weeks behind the UK and airs six episodes per week.\nIn Ireland, \"Coronation Street\" is currently shown on Virgin Media One. The show was first aired in 1978, when RT\u00c92 began showing episodes from 1976, although Ireland caught up with the current UK episodes in 1983. In 1992 it moved to RT\u00c9 One, but in 2001 Granada TV bought 45 percent of TV3, and so TV3 broadcast the series from 2001 to 2014. In 2006, ITV sold its share of the channel but TV3 continued to buy the soap until the end of 2014 when it moved to UTV Ireland. Coronation Street has broadcast on each of the main Irish networks, except for the Irish language network TG4. In December 2016, \"Coronation Street\" returned to TV3 (now Virgin Media One). The show is consistently the channel's most viewed programme every week.\nTwo Dutch stations have broadcast \"Coronation Street\": VARA showed 428 episodes between 1967 and 1975, and SBS6 ran the show for a period starting in 2010. From 2006 the series was also broadcast by Vitaya, a small Flemish Belgian channel.\nIn New Zealand, \"Coronation Street\" has been shown locally since 1964, first on NZBC television until 1975, and then on TV One, which broadcasts it in a 4-episode/2-hour block on Fridays from 7:30\u00a0pm. In September 2014, TV One added a 2-episode/1-hour block on Saturday from 8:30\u00a0pm. Because TV One did not upgrade to showing the equivalent of five or six episodes per week, New Zealand continued to fall further and further behind with episodes, and was 23 months behind Britain as of March 2014. During the weekday nights of the week ending 11 April 2014 and previous weeks, Coronation Street was the least watched programme on TV One in the 7:30\u00a0pm slot by a considerable margin in comparison to other weeknights, The serial aired on Tuesdays and Thursdays at 7:30\u00a0pm until October 2011, when the show moved to a 5:30\u00a0pm half-hour slot every weekday. The move proved unpopular with fans, and the series was quickly moved into its present prime-time slot within weeks. Episodes 7883, 7884, 7885 and 7886 were screened on 16 May 2014. These were originally aired in the UK between 4 and 11 June 2012. On 10 May 2018 it was announced that the current 2016 episodes would be moved to 1 p.m. Monday-Friday titled 'Catch-up Episodes' and for primetime Wednesday-Friday express episodes would be airing in New Zealand a week behind the United Kingdom titled '2018 Episodes' these changes would be taking place from 11 June 2018.\nIn South Africa, \"Coronation Street\" episodes were broadcast three days after the UK air date on ITV Choice until the channel ceased broadcasting in June 2020, episodes temporarily went off the air until they moved to M-Net City, starting in October 2020.\nIn the United States, \"Coronation Street\" is available by broadcast or cable only in northern markets where CBC coverage from Canada overlaps the border or is available on local cable systems. It was broadcast on CBC's US cable channel, Trio until the CBC sold its stake in the channel to Universal, before it was shut down in 2006. Beginning in 2009, episodes were available in the United States through Amazon.com's on-demand service, a month behind their original UK airdates. The final series of shows available from Amazon appears to be from November 2012, as no new episodes have been uploaded. On 15 January 2013, online distributor Hulu began airing episodes of the show, posting a new episode daily, two weeks after their original airdates. For a time, Hulu's website stated: \"New episodes of \"Coronation Street\" will be unavailable as of April 7th, 2016\", with the same being said for British soap \"Hollyoaks\", but Hulu is once again showing new episodes of \"Coronation Street\" as of April 2017, two weeks behind the UK airdate. The BBC/ITV service Britbox shows new episodes on the same day as the UK airing. \"Coronation Street\" was also shown on USA Network for an unknown period starting in 1982.\nHM Forces and their families stationed overseas can watch \"Coronation Street\" on ITV, carried by the British Forces Broadcasting Service, which is also available to civilians in the Falkland Islands. It used to be shown on BFBS1.\nSatellite channel ITV Choice showed the programme in Asia, Middle East, Cyprus, and Malta, before the channel ceased broadcasting in 2019.\nMerchandise.\n\"The Street\", a magazine dedicated to the show, was launched in 1989. Edited by Bill Hill, the magazine contained a summary of recent storylines, interviews, articles about classic episodes, and stories that occurred from before 1960. The format was initially A5 size, expanding to A4 from the seventh issue. The magazine folded after issue 23 in 1993 when the publisher's contract with Granada Studios Tour expired and Granada wanted to produce their own magazine.\nOn 25 June 2010, a video game of the show was released on Nintendo DS. The game was developed by Mindscape, and allowed players to complete tasks in the fictitious town of Weatherfield.\nDiscography.\nIn 1995, to commemorate the programme's 35th anniversary, a CD titled \"The Coronation Street Album\" was released, featuring cover versions of modern songs and standards by contemporary cast members.\nThe album charted a Top 40 hit when \"The Coronation Street Single\" (a double a-side featuring a cover of Monty Python's \"Always Look on the Bright Side of Life\" by Bill Waddington \u2013 with various cast members on backing vocals \u2013 on one side and \"Something Stupid\" by Johnny Briggs &amp; Amanda Barrie on the other) reached number 35 in the Official UK charts.\nIn 2010, an album featuring songs sung by cast members was released to celebrate 50 years of \"Coronation Street\". The album is titled \"Rogues, Angels, Heroes &amp; Fools\", and was later developed into a musical.\nSpin-offs.\nTelevision.\nGranada launched one spin-off in 1965, \"Pardon the Expression\", following the story of clothing store manager Leonard Swindley (Arthur Lowe) after he left Weatherfield. Swindley's management experience was tested when he was appointed assistant manager at a fictional department store, Dobson and Hawks. Granada produced two series of the spin-off, which ended in 1966.\nIn 1967, Arthur Lowe returned as Leonard Swindley in \"Turn Out the Lights\", a short-lived sequel to \"Pardon the Expression\". It ran for just one series of six episodes before it was cancelled.\nIn 1972, Neville Buswell and Graham Haberfield starred as Ray Langton and Jerry Booth in a pilot for a potential spin-off series called \"Rest Assured\". Written and produced by H.V. Kershaw the pilot had an episode title of \"Lift Off\", and featured Fred Feast (later cast as Fred Gee in Coronation Street) as the lift engineer. No series was commissioned.\nFrom 1985 to 1988, Granada TV produced a sitcom called \"The Brothers McGregor\" featuring a pair of half-brothers (one black, one white) who had appeared in a single episode of \"Coronation Street\" as old friends of Eddie Yeats and guests at his wedding. The original actors were unavailable so the characters were recast with Paul Barber and Philip Whitchurch. The show ran for 26 episodes over four series.\nIn 1985, a sister series, \"Albion Market\", was launched. It ran for one year, with 100 episodes produced.\nCrossovers.\nIn 2010, several actors from the show appeared on \"The Jeremy Kyle Show\" as their soap characters: David Platt (Jack P. Shepherd), Nick Tilsley (Ben Price), Tina McIntyre (Michelle Keegan) and Graeme Proctor (Craig Gazey). In the fictional, semi-improvised scenario, David accused Nick (his brother) and Tina (his ex-girlfriend) of sleeping together.\n\"Coronation Street\" and rival soap opera \"EastEnders\" had a crossover for \"Children in Need\" in November 2010 called \"East Street\". \"EastEnders\" stars that visited Weatherfield include Laurie Brett as Jane Beale, Charlie G. Hawkins as Darren Miller, Kylie Babbington as Jodie Gold, Nina Wadia as Zainab Masood and John Partridge as Christian Clarke.\nOn 21 December 2012, \"Coronation Street\" produced a Text Santa special entitled \"A Christmas Corrie\" which featured Norris Cole in the style of Scrooge, being visited by the ghosts of dead characters. The ghosts were Mike Baldwin, Maxine Peacock, Derek Wilton and Vera Duckworth. Other special guests include Torvill and Dean, Lorraine Kelly and Sheila Reid. The episode concluded with Norris learning the error of his ways and dancing on the cobbles. The original plan for this feature was to have included Jack Duckworth, along with Vera, but actor Bill Tarmey died before filming commenced. In the end a recording of his voice was played.\nDocumentaries.\n\"Coronation Street: Family Album\" was several documentaries about various families living on the street.\n\"Farewell\u00a0...\" was several documentaries featuring the best moments of a single character who had recently left the series\u2014most notably, Farewell Mike (Baldwin), Farewell Vera (Duckworth), Farewell Blanche (Hunt), Farewell Jack (Duckworth), Farewell Janice (Battersby), Farewell Liz (McDonald), Farewell Becky (McDonald), and Farewell Tina (McIntyre). Most of these were broadcast on the same day as the character's final scenes in the series.\n\"Stars on the Street\" was aired around Christmas 2009. It featured actors from the soap talking about the famous guest stars who had appeared in the series including people who were in it before they were famous.\nIn December 2010, ITV made a few special programmes to mark the 50th anniversary. \"Coronation Street Uncovered: Live\", hosted by Stephen Mulhern was shown after the episode with the tram crash was aired on ITV2. On 7 and 9 December, a countdown on the greatest Corrie moments, \"Coronation Street: 50 Years, 50 Moments\", the viewers voted \"The Barlows at Alcoholics Anonymous\" as the greatest moment. On 10 December Paul O'Grady hosted a quiz show, \"Coronation Street: The Big 50\" with three teams from the soap and a celebrity team answering questions about Coronation Street and other soaps. Also, \"Come Dine with Me\" and \"Celebrity Juice\" aired Coronation Street specials in the anniversary week.\nInternational adaptation.\nThe German TV series \"Lindenstra\u00dfe\" took \"Coronation Street\" as the model. \"Lindenstra\u00dfe\" started in 1985 and broadcast its final episode on 29 March 2020, after airing for nearly 35 years.\nFilms.\nOver the years, \"Coronation Street\" has released several straight-to-video films. Unlike other soaps, which often used straight-to-video films to cover more contentious plot lines that may not be allowed by the broadcaster, \"Coronation Street\" has largely used these films to reset their characters in other locations.\nIn 1995, \"Coronation Street: The Cruise\" also known as \"Coronation Street: The Feature Length Special\" was released on VHS to celebrate the 35th anniversary of the show, featuring Rita Sullivan, Mavis Wilton, Alec Gilroy, Curly Watts and Raquel Watts. ITV heavily promoted the programme as a direct-to-video exclusive, but broadcast a brief version of it on 24 March 1996. The Independent Television Commission investigated the broadcast, as viewers complained that ITV misled them.\nIn 1997, following the controversial cruise spin-off, \"Coronation Street: Viva Las Vegas!\" was released on VHS, featuring Vera Duckworth, Jack Duckworth, Fiona Middleton and Maxine Peacock on a trip to Las Vegas, which included the temporary return of Ray Langton.\nIn 1999, six special episodes of \"Coronation Street\" were produced, following the story of Steve McDonald and Vikram Desai in Brighton, which included the temporary returns of Bet Gilroy, Reg Holdsworth and Vicky McDonald. This video was titled \"Coronation Street: Open All Hours\" and released on VHS.\nIn 2008, ITV announced filming was to get underway for a new special DVD episode, \", featuring Kirk Sutherland, Fiz Brown, Chesney Brown, which included the temporary return of Cilla Battersby-Brown. Sophie Webster, Becky Granger and Tina McIntyre also make brief appearances.\nIn 2009, another DVD special, \", was released. The feature-length comedy drama followed Roy, Hayley and Becky as they travelled to Romania for the wedding of a face from their past. Eddie Windass also briefly appears.\nThe BBC commissioned a one-off drama called \"The Road to Coronation Street\", about how the series first came into being. Jessie Wallace plays Pat Phoenix (Elsie Tanner) with Lynda Baron as Violet Carson (Ena Sharples), Celia Imrie as Doris Speed (Annie Walker) and James Roache as his own father William Roache (Ken Barlow). It was broadcast on 16 September 2010 on BBC Four.\nOn 1 November 2010, \"Coronation Street: A Knight's Tale\" was released. Reg Holdsworth and Curly Watts returned in the film. Mary tries to take Norris to an apparently haunted castle where she hoped to seduce him. Rosie gets a job there and she takes Jason with her. Brian Capron also guest starred as an assumed relative of Richard Hillman. He rises out of a lake with a comedic \"wink to the audience\" after Hillman drowned in 2003. Rita Sullivan also briefly appears.\nOnline.\nOn 21 December 2008, a web-based miniseries ran on ITV.com; called \"Corrie Confidential\"; the first episode featured the characters Rosie and Sophie Webster in \"Underworld\".\nITV.com launched a small spin-off drama series called 'Gary's Army Diaries' which revolves around Gary Windass's experiences in Afghanistan and the loss of his best friend, Quinny. Due to their popularity, the three five-minute episodes were recut into a single 30-minute episode, which was broadcast on ITV2.\nWilliam Roache and Anne Kirkbride starred as Ken and Deirdre in a series of ten three-minute internet 'webisodes'. The first episode of the series titled, \"Ken and Deirdre's Bedtime Stories\" was activated on Valentine's Day 2011.\nIn 2011, an internet based spin-off starring Helen Flanagan as Rosie Webster followed her on her quest to be a supermodel called \"Just Rosie\".\nOn 3 February 2014, another web-based miniseries ran on ITV.com; called \"Streetcar Stories\". It showed what Steve and Lloyd get up to during the late nights in their Streetcar cab office. The first episode shows Steve and Lloyd making a cup of tea with \"The Stripper\" playing in the background, referencing Morecambe and Wise's Breakfast Sketch. The second episode involves the pair having a biscuit dunking competition.\nDuring the 'Who Attacked Ken' storyline, a mini series of police files was run on the official Coronation Street YouTube channel. They outlined the suspects' details and possible motives.\nStage.\nIn August 2010, many \"Coronation Street\" characters were brought to the stage in Jonathan Harvey's comedy play \"Corrie!\". The play was commissioned to celebrate the 50th Anniversary of the TV series and was presented at The Lowry in Salford, England by ITV Studios and Phil McIntyre Entertainments. Featuring a cast of six actors who alternate roles of favourite characters including Ena Sharples, Hilda Ogden, Hayley and Roy, Richard Hillman, Jack and Vera, Bet Lynch, Steve, Karen and Becky, the play weaves together some of the most memorable moments from the TV show. It toured UK theatres between February 2011 and July 2011 with guest star narrators including Roy Barraclough, Ken Morley and Gaynor Faye.\nIn popular culture.\nThe British rock band Queen produced a single \"I Want to Break Free\" in 1984 that reached number 3 in the UK Singles Chart. The song is memorable for its music video in which the band members dressed in women's clothing, which parodied characters in Coronation Street and is considered an homage to the show. The video depicts Freddie Mercury as a housewife, loosely based on Bet Lynch, who wants to \"break free\" from his life. Although Lynch was a blonde in the soap opera, Mercury thought he would look too silly as a blonde and chose a dark wig. Guitarist Brian May plays another, more relaxed housewife based on Hilda Ogden.\nIn December 2022, the American singer Bob Dylan was offered a cameo on Coronation Street after revealing to \"The Wall Street Journal\" that he is a fan of the ITV soap.\nSponsorship.\nCadbury was the first sponsor of \"Coronation Street\", beginning in July 1996. In the summer of 2006, Cadbury Trebor Bassetts had to recall over one million chocolate bars, due to suspected salmonella contamination, and \"Coronation Street\" stopped the sponsorship for several months. In 2006, Cadbury did not renew their contract, but agreed to sponsor the show until \"Coronation Street\" found a new sponsor.\nHarveys then sponsored \"Coronation Street\" from 30 September 2007 until December 2012. In the \"Coronation Street: Romanian Holiday\" film, Roy and Hayley Cropper are filmed in front of a Harveys store, and in \"Coronation Street: A Knights Tale\", a Harveys truck can be seen driving past Mary Taylor's motorhome. Compare The Market took over as sponsor from 26 November 2012 until 30 November 2020. On 10 December 2020, it was announced that Argos would be the new sponsor of \"Coronation Street\", starting on 1 January 2021.\nIn November 2011, a Nationwide Building Society ATM in Dev Alahan's corner shop became the first use of paid-for product placement in a UK primetime show. In 2018, the shop fronts of Co-Op and Costa Coffee were added to the sets, along with characters using shopping bags with the respective logos on as props.\nHyundai have been the sponsor since January 2015 in the Republic of Ireland, aired on Virgin Media One."}
{"id": "6852", "revid": "15996738", "url": "https://en.wikipedia.org/wiki?curid=6852", "title": "Caligula", "text": "Gaius Caesar Augustus Germanicus (31 August 12 \u2013 24 January 41), better known by his nickname Caligula (), was Roman emperor from AD 37 until his assassination in AD 41. He was the son of the Roman general Germanicus and Augustus' granddaughter Agrippina the Elder, members of the first ruling family of the Roman Empire. He was born two years before Tiberius was made emperor. Gaius accompanied his father, mother and siblings on campaign in Germania, at little more than four or five years old. He had been named after Gaius Julius Caesar, but his father's soldiers affectionately nicknamed him \"Caligula\" ('little boot').\nGermanicus died in Antioch in 19, and Agrippina returned with her six children to Rome, where she became entangled in a bitter feud with Emperor Tiberius, who was Germanicus' biological uncle and adoptive father. The conflict eventually led to the destruction of her family, with Caligula as the sole male survivor. In 26, Tiberius withdrew from public life to the island of Capri, and in 31, Caligula joined him there. Tiberius died in 37 and Caligula succeeded him as emperor, at the age of 24.\nOf the few surviving sources about Caligula and his four-year reign, most were written by members of the nobility and senate, long after the events they purport to describe. For the early part of his reign, he is said to have been \"good, generous, fair and community-spirited\" but increasingly self-indulgent, cruel, sadistic, extravagant and sexually perverted thereafter; an insane, murderous tyrant who demanded and received worship as a living god, humiliated his Senate, and planned to make his horse a consul. Most modern commentaries seek to explain Caligula's position, personality and historical context. Some historians dismiss many of the allegations against him as misunderstandings, exaggeration, mockery or malicious fantasy.\nDuring his brief reign, Caligula worked to increase the unconstrained personal power of the emperor, as opposed to countervailing powers within the principate. He directed much of his attention to ambitious construction projects and public works to benefit Rome's ordinary citizens, including racetracks, theatres, amphitheatres, and improvements to roads and ports. He began the construction of two aqueducts in Rome: the Aqua Claudia and the Anio Novus. During his reign, the empire annexed the client kingdom of Mauretania as a province. He had to abandon an attempted invasion of Britain, and the installation of his statue in the Temple of Jerusalem. In early 41, Caligula was assassinated as a result of a conspiracy by officers of the Praetorian Guard, senators, and courtiers. At least some of the conspirators might have planned this as an opportunity to restore the Roman Republic and aristocratic privileges; but if so, their plan was thwarted by the Praetorians, who seem to have spontaneously chosen Caligula's uncle Claudius as the next emperor. Caligula's death marked the official end of the Julii Caesares in the male line, though the Julio-Claudian dynasty continued to rule until the demise of Caligula's nephew, the Emperor Nero.\nEarly life.\nCaligula was born in Antium on 31 August AD 12, the third of six surviving children of Germanicus and his wife and second cousin, Agrippina the Elder. Germanicus was a grandson of Mark Antony, and Agrippina was the daughter of Marcus Vipsanius Agrippa and Julia the Elder, making her the granddaughter of Augustus. The future emperor Claudius was Caligula's paternal uncle. Caligula had two older brothers, Nero and Drusus, and three younger sisters, Agrippina the Younger, Julia Drusilla and Julia Livilla. At the age of two or three, he accompanied his father, Germanicus, on campaigns in the north of Germania. He wore a miniature soldier's outfit devised by his mother to please the troops, including army boots (\"caligae\") and armour. The soldiers nicknamed him \"Caligula\" (\"little boot\"). Winterling believes he would have enjoyed the attention of the soldiers, to whom he was something of a mascot, though he later grew to dislike the nickname.\nGermanicus was a respected, immensely popular figure among his troops and Roman civilians of every class, and was widely expected to eventually succeed his uncle Tiberius as emperor. For his successful northern campaigns, he was awarded the great honour of a triumph. During the triumphal procession through Rome, Caligula and his siblings shared their father's chariot, and the applause of the populace. A few months later, Germanicus was despatched to tour Rome's allies and provinces with his family. They were received with great honour; at Assos Caligula gave a public speech, aged only 6. Somewhere \"en route\", Germanicus contracted what proved to be a fatal illness. He lingered awhile, and died at Antioch, Syria, in AD\u00a019, aged 33, convinced that he had been poisoned by the provincial governor, Gnaius Calpurnius Piso. Many believed that he had been killed at the behest of Tiberius, as a potential rival.\nGermanicus was cremated, and his ashes were taken to Rome, escorted by his wife and children, Pretorian guards, civilian mourners and senators, then placed in the Mausoleum of Augustus. Caligula lived with his mother Agrippina in Rome, in a milieu very different from that of his earlier years. Agrippina made no secret of her imperial ambitions for herself and her sons, and in consequence, her relations with Tiberius rapidly deteriorated. Tiberius believed himself under constant threat from treason, conspiracy and political rivalry. He forbade Agrippina to remarry, for fear that a remarriage would serve her personal ambition, and introduce yet another threat to himself. The last years of his principate were dominated by treason trials, whose outcomes were determined by senatorial vote. Agrippina, and Caligula's brother Nero, were tried and banished in the year 29 on charges of treason. The adolescent Caligula was sent to live with his great-grandmother (Tiberius' mother), Livia. After her death two years later, he was sent to live with his grandmother Antonia Minor. In the year 30, Tiberius had Caligula's brothers, Drusus and Nero, declared public enemies by the Senate, and exiled. Caligula and his three sisters remained in Italy as hostages of Tiberius, kept under close watch.\nCapri.\nIn 31, Caligula's brother Nero died in exile. Caligula was remanded to the personal care of Tiberius at Villa Jovis on Capri. \nHe was befriended by Tiberius' Praetorian prefect, Naevius Sutorius Macro. Macro had been active in the downfall of Sejanus, his ambitious and manipulative predecessor in office, and was a trusted communicant between the emperor, and his senate in Rome. Philo, Jewish diplomat and later witness to several events in Caligula's court, writes that Macro protected and supported Caligula, allaying any suspicions Tiberius might harbour concerning his young ward's ambitions. Macro represented Caligula to Tiberius as \"friendly, obedient\" and devoted to Tiberius' grandson, Tiberius Gemellus, who was seven years younger than himself. Caligula is described during this time as a first-rate orator, well-informed, cultured and intelligent, a natural actor who recognized the danger he was in, and hid his resentment of Tiberius' maltreatment of himself and his family behind such an obsequious manner that it was said of him that there had never been \"a better slave or a worse master\". Caligula's failure to protest the destruction of his family is taken by Tacitus as evidence that his \"monstrous character was masked by a hypocritical modesty\". Winterling observes that a forthright protest would \"certainly have cost him his life\".\nIn 33, Caligula's mother and his brother Drusus died, while still in exile. In the same year, Tiberius arranged the marriage of Caligula and Junia Claudilla, daughter of one of Tiberius' most influential allies in the Senate, Marcus Junius Silanus. Caligula was given an honorary quaestorship in the \"\", a series of political promotions that could lead to consulship. He would hold this very junior senatorial post until his sudden nomination as emperor. Junia died in childbirth the following year, along with her baby. In 35, Tiberius named Caligula as joint heir with Tiberius' grandson, Gemellus, who was Caligula's junior by seven years and not yet an adult. At the time, Tiberius seemed to be in good health, and likely to survive until Gemellus' majority.\nIn Philo's account, Tiberius was genuinely fond of Gemellus, but doubted his personal capacity to rule and feared for his safety should Caligula come to power. Suetonius claims that Tiberius, ever mistrustful but still shrewd in his mid-70s, saw through Caligula's apparent self-possession to an underlying \"erratic and unreliable\" temperament, not one to be trusted in government; and he claims that Caligula took pleasure in cruelty, torture, and sexual vice of every kind. Tiberius is said to have indulged the young man's appetite for theatre, dance and singing, in the hope that this would help soften his otherwise savage nature; \"he used to say now and then that to allow Gaius to live would prove the ruin of himself and of all men, and that he was rearing a viper for the Roman people and a Phaethon for the world.\" Winterling points out that this judgment draws on later, not particularly accurate accounts of Caligula's rule; Suetonius credits Tiberius with a knowledge of human nature which in reality was not only foreign to him, but famously unsound. At Capri, Caligula learned to dissimulate. He probably owed his life to that and, as all the ancient sources agree, to Macro. Many believed, or claimed to believe, that given a little more time, Tiberius would have eliminated Caligula as a possible successor, but died before this could be done.\nEmperor.\nEarly reign.\nTiberius died on 16 March AD\u00a037, a day before the Liberalia festival. He was 77 years old. Suetonius, Tacitus and Cassius Dio repeat variously elaborated rumours which held that Caligula, perhaps with Macro, was directly responsible for his death. Philo and Josephus, the latter a Romano-Jewish writer who served Vespasian a generation later, describe Tiberius' death as natural. On the same day, Caligula was hailed as emperor by members of the Praetorian guard at Misenum. His leadership of the \"domus Caesaris\" (\"Caesar's household\") as its sole heir and pater familias was ratified by the senate, who acclaimed him \"imperator\" two days after the death of Tiberius. Caligula entered Rome on 28 or 29 March, and with the consensus of \"the three orders\" (senate, equestrians and common citizens) the Senate conferred on him the \"right and power to decide on all affairs\".\n\"Princeps\".\nIn a single day, and with a single piece of legislation, the 25-year-old Caligula, previously a virtual unknown in Rome's political life, and with no military service, was thus granted the same trappings, authority and powers that Augustus had accumulated piecemeal, over a lifetime and sometimes reluctantly. Until his first formal meeting with the Senate, Caligula refrained from using the titles they had granted him. His studied deference must have gone some way to reassure the more astute that he should prove amenable to their guidance. Some must have resented the political manipulations that led to this extraordinary settlement. Caligula was now entitled to make, break or ignore any laws he chose. Augustus had shown, and Tiberius had failed to realise, that the roles of \"primus inter pares\" (\"first among equals\") and \"princeps legibus solutus\" (\"a princeps not bound by the laws\") required the exercise of personal responsibility, self-restraint, and above all, tact; as if the Senate still held the power they had voluntarily surrendered. In the words of scholar Anthony A. Barrett, \"Caligula would be restrained only by his own sense of discretion, which became in lamentably short supply as his reign progressed\".\nCaligula dutifully asked the Senate to approve divine honours for his predecessor but was turned down, in line with senatorial and popular opinion regarding the dead emperor's worth. Caligula did not push the issue; he had made the necessary gesture of filial respect. Tiberius' will named two heirs, Caligula and Gemellus, but the latter was still a minor, and could not hold any kind of office. The will was annulled with the standard justification that Tiberius must have been insane when he composed it, incapable of good judgment. Although Tiberius' will had been legally set aside, Caligula honoured many of its terms, and in some cases, improved on them. Tiberius had provided each praetorian guardsman with a generous gratitude payment of 500 sesterces. Caligula doubled this, and took credit for its payment as an act of personal generosity; he also paid bonuses to the city troops and the army outside Italy. Every citizen in Rome was given 150 sesterces, and heads of households twice that amount. Building projects on the Palatine hill and elsewhere were also announced, which would have been the largest of these expenditures.\nThanks to Macro's preparations on his behalf, Caligula's accession was a \"brilliantly stage-managed affair\". The legions had already sworn loyalty to Caligula as their imperator. Now Caligula gave the miserly Tiberius a magnificent funeral at public expense, and a tearful eulogy, and met with an ecstatic popular reception along the funeral route and in Rome itself. Among Caligula's first acts as emperor was the provision of public games on a grand scale. Philo describes Caligula in these early days as universally admired. Suetonius writes that Caligula was loved by many, for being the beloved son of the popular Germanicus. Three months of public rejoicing ushered in the new reign. Philo describes the first seven months of Caligula's reign as a \"Golden Age\" of happiness and prosperity. Josephus claims that in the first two years of his reign, Caligula's \"high-minded... even-handed\" rule earned him goodwill throughout the Empire.\nCaligula took up his first consulship on 1 July, two months after his succession. He accepted all titles and honours offered him except \"pater patriae\" (\"father of the fatherland\"), which had been conferred on Augustus. Caligula refused it, protesting his youth, until 21 September 37. He commemorated his own father, Germanicus, with portraits on coinage, adopted his name, and renamed the month of September after him. He granted his sisters and his grandmother Antonia Minor extraordinary privileges, normally reserved for the Vestals, and female priesthoods of the deified Augustus; their powers were entirely ceremonial, not executive, but their names were included in the standard formulas used in the senate house to invoke divine blessings on debates and proceedings, and the annual prayers for the safety of emperor and state. Caligula named his favourite sister, Drusilla, as heir to his \"imperium\". Oaths were sworn in the name of Caligula, and his entire family. One of his sesterces not only identifies each sister by name, but associates her with a particular imperial virtue; \"security\", \"concord\" or \"fortune\". Caligula ordered that an image of his deceased mother, Agrippina, must accompany all festival processions. He made his uncle Claudius his consular colleague, tasked with siting statues of Caligula's two dead brothers, and occasionally standing in for Caligula at games, feasts and ceremonies. Claudius' own family found his limp and stammer \"something of a public embarrassment\"; he mismanaged the statue commission and his first consulship ended soon after, alongside Caligula's but his appointment elevated him from mere equestrian to senator, and eligible for consulship. Barrett and Yardley describe Claudius' consulship as an \"astonishingly enlightened gesture\" on Caligula's part, not one of Caligula's attempts to court popularity, as Suetonius would have it.\nCaligula made a public show of burning Tiberius' secret papers, which gave details of his infamous treason trials. They included accusations of villainy and betrayal against various senators, many of whom had willingly assisted in prosecutions of their own number to gain financial advantage, imperial favour, or to divert suspicion away from themselves; any expression of dissatisfaction with the emperor's rule or decisions could be taken as undermining the State, and lead to prosecution for \"maiestas\" (treason). Caligula claimed \u2013 falsely, as it later turned out \u2013 that he had read none of these documents before burning them. He used a coin issue to advertise his claim that he had restored the security of the laws, which had suffered during Tiberius' prolonged absence from Rome; he reduced a backlog of court cases in Rome by adding more jurors and suspending the requirement that sentences be confirmed by imperial office.\nStressing his descent from Augustus, Caligula retrieved the remains of his mother and brothers from their places of exile for interment in the Mausoleum of Augustus. Caligula began work on a temple to Livia, widow of Augustus; she held the honorific title of Augusta while still living, and when she died was eventually made a \"diva\" (goddess) of the Roman state under Claudius. The temple had been vowed in her lifetime, but not constructed.\nIllness and recovery.\nBetween approximately mid-October and mid-November 37, Caligula fell seriously ill through unknown causes and hovered for a month or so between life and death. Rome's public places filled with citizens who implored the gods for his recovery, some even offering their own lives in exchange. By late October, their emperor had recovered, and embarked on what might have been a purge of suspected opponents or conspirators. Caligula's relations with his senate had been congenial but were now sullied by the forced suicide, for reasons unknown, of the eminent senator Silanus, formerly Caligula's father-in-law. Gemellus, Caligula's adopted son and heir, now 18 years old and legally adult, was also disposed of. Suetonius offers several versions of Gemellus' death. In one, Gemellus was given the adult \"toga virilis\" then charged with having taken an antidote, \"implicitly accusing Caligula of wanting to poison him\", and forced to kill himself. Several months later, in early 38, Caligula forced suicide on his Praetorian Prefect, Macro, without whose help and protection he would not have survived, let alone gained the throne as sole ruler. Any link between the deaths is speculative, but it is possible that Silanus had conspired to make Gemellus emperor, should Caligula fail to recover; and Caligula might simply have tired of Macro's control and influence.\nIn 38, Caligula nominated Marcus Aemilius Lepidus as his heir, and married him to his beloved sister Drusilla, but on 19 June that year, Drusilla died. She was deified and renamed Panthea (\"All Goddesses\"); the first mortal woman in Roman history to be made a \"diva\" (goddess of state). Caligula, bereft, declared a period of compulsory, universal mourning. Drusilla's death is one of several events approximate to the time of Caligula's illness, besides the death of Antonia and any unreported effects of the illness itself, thought by some to contribute to a fundamental change in Caligula's attitudes. Purges so early in Caligula's reign suggest to Weidemann that \"the new emperor had learnt a great deal from Tiberius\" and \"that attempts to divide his reign into a 'good' beginning followed by unremitting atrocities [...] are misplaced\".\nPublic profile.\nCaligula shared many of the popular passions and enthusiasms of the lower classes and young aristocrats: public spectacles, particularly gladiator contests, chariot and horse racing, the theatre and gambling, but all on a scale with which the nobility could not match. He trained with professional gladiators and staged exceptionally lavish gladiator games, being granted exemption by the senate from the sumptuary laws that limited the number of gladiators to be kept in Rome. He was openly and vocally partisan in his uninhibited support or disapproval of particular charioteers, racing teams, gladiators and actors, shouting encouragement or scorn, sometimes singing along with paid performers or declaiming the actors' lines, and generally behaving as \"one of the crowd\". In gladiator contests, he supported the \"parmularius\" type, who fought using small, round shields. In chariot races, he supported the Greens, and personally drove his favourite racehorse, Incitatus (\"Speedy\") as a member of the Green faction. Most of Rome's aristocracy would have found this an unprecedented, unacceptable indignity for any of their number, let alone their emperor.\nCaligula showed little respect for distinctions of rank, status or privilege among the senate, whose members Tiberius had once described as \"men ready to be slaves\". Among those whom Caligula recalled from exile were actors and other public performers who had somehow caused Tiberius offence. Caligula seems to have built a loyal following among his own loyal freedmen, citizen-commoners, disreputable public performers on whom he lavished money and other gifts; and the lower nobility (equestrians) rather than the senators and nobles whom he clearly and openly mistrusted, despised and humiliated for their insincere simulations of loyalty. Dio notes, with approval, that Caligula allowed some equestrians senatorial honours, anticipating their later promotion to senator based on their personal merits. To reverse declining membership of the equestrian order, Caligula recruited new, wealthy members empire-wide, and scrupulously vetted the order's membership lists for signs of dishonesty or scandal. He seems to have ignored trivial misdemeanours, and would have anticipated the creation of \"new men\" (\"novi homines\"), first of their families to serve as senators. They would owe him a debt of gratitude and loyalty for their advancement.\nBarrett describes some of the supposed equestrian offences punished by Caligula as \"decidedly trivial\", and their punishments as sensationalist. Dio claims that Caligula had more than 26 equestrians executed in a circus \"fracas\"; in Suetonius' biography \"more than 20\" lives were lost in what is almost certainly the same event, described as a violent but accidental crush. Some sources claim that Caligula forced equestrians and senators to fight in the arena as gladiators. Condemnation to the gladiator arena as a combatant was a standard punishment, doubling as public entertainment, for non-citizens found guilty of certain offences. Laws of AD 19 by Augustus and Tiberius banned voluntary participation of the elite in any public spectacles, but the ban was never particularly effective, and was broadly ignored in Caligula's reign. During Caligula's illness two citizens, one of whom was an equestrian, offered to fight as gladiators if only the gods would spare the emperor's life. The offers were insincere, intended to flatter and invite reward. When Caligula recovered, he insisted that they be taken at face value, to avoid accusations of perjury: \"cynical, but not without wit of a kind\".\nPublic reform and finance.\nIn 38, Caligula lifted censorship, and published accounts of public funds and expenditure. Suetonius congratulates this as the first such act by any emperor. Very soon after his succession, he restored the right of the popular assembly (comitia) to elect magistrates on behalf of the common citizenry, a right that had been taken over by the Senate under Tiberius and Augustus. The aediles, elected officials who managed public games and festivals, and maintained the fabric of roads and shrines, would now have incentive to spend their own money on lavish, high-profile spectacles and other \"munera\" (gifts to the state or people), to win the popular vote. Dio writes that this, \"though delighting the rabble, grieved the sensible, who stopped to reflect, that if the offices should fall once more into the hands of the many... many disasters would result\". When the Senate outright refused to accept this, Caligula restored control of elections to them. Either way, the emperor ultimately chose which candidates stood for election, and which were elected. Caligula was quite capable of recognising his own plans and decisions as flawed, and abandoning, revising or reversing them when faced with opposition. He was open to good advice, but could just as easily take its offering as an insult to his youth or understanding \u2013 Philo quotes his warning \"Who dares teach me?\" Caligula abandoned his plan to convert the Temple of Jerusalem to a temple of the Imperial cult, with a statue of himself as Zeus, when warned that the plan would arouse extreme protests, and injure the local economy. He gave funds where they were needed; he helped those who lost property in fires, and abolished a deeply unpopular tax on sales, but whether his extravagant gifts to favourites during his earliest reign \u2013 be they actors, charioteers or other public performers \u2013 drew on his personal wealth or state coffers is not known. Personal generosity and magnanimity, coupled with discretion and responsibility, were expected of the ruling elite, and the emperor in particular. At some time, Caligula ruled that bequests to office-holders remain property of the office, not of the office-holder.\nTax and treasury.\nSuetonius claims that Caligula squandered 2.7\u00a0billion sesterces in his first year. and addressed the consequent treasury deficit by confiscating the estates of wealthy individuals, after false accusations, fines or outright seizure, even the death penalty, as a means of raising money. This seems to have started in earnest around the time of Caligula's confrontation with the senate (in early 39). Suetonius's retrospective balance sheet overlooks what would have been owed to Caligula, personally and in his capacity as emperor, on Tiberius' death, and the release of the former emperor's hoarded wealth into the economy at large. Caligula's inheritance included the deceased empress Livia's vast bequest, which Caligula distributed among its nominated public, private and religious beneficiaries. Barrett in \"Caligula: The Abuse of Power\" asserts that this \"massive cash injection would have given the Roman economy a tremendous boost\".\nDio remarks the beginnings of a financial crisis in 39, and connects it to the cost of Caligula's extravagant bridge-building project at Baiae. Suetonius has presumably the same financial crisis starting in 38; he does not mention a bridge but lists a broad range of Caligula's extravagances, said to have exhausted the state treasury.\nTo Wilkinson, Caligula's uninterrupted use of precious metals in coin issues does not suggest a bankrupt treasury, though there must have been a blurring of boundaries between Caligula's personal wealth, and his income as head of state. Caligula's immediate successor, Claudius, abolished taxes, embarked on various costly building projects and donated 15,000 sesterces to each Praetorian Guard in 41 as his own reign began, which suggests that Caligula had left him a solvent treasury.\nIn the long term, the occasional windfall aside, Caligula's spending exceeded his income. Fund-raising through taxation became a major preoccupation. Provincial citizens were liable for direct payment of taxes used to fund the military, a payment from which Italians were exempt. Caligula abolished some taxes, including the deeply unpopular sales tax, but he introduced an unprecedented range of new ones, and rather than employ professional tax farmers (publicani) in their collection, he made this a duty of the notoriously forceful Praetorian Guard. Dio and Suetonius describe these taxes as \"shameful\": some were remarkably petty. Caligula taxed \"taverns, artisans, slaves and the hiring of slaves\", edibles sold in the city, litigation anywhere in the Empire, weddings or marriages, the wages of porters \"or perhaps couriers\", and most infamously, a tax on prostitutes (active, retired or married) or their pimps, liable for \"a sum equivalent to a single transaction\". Citizens of provincial Italy lost their previous tax exemptions. Most individual tax bills were fairly small but cumulative; over Caligula's brief reign, taxes were doubled overall. Even then, the revenue was nowhere near enough, and the imposition was deeply resented by Rome's commoners. Josephus claims that this led to riotous protests at the Circus. Barrett remarks that stories of consequent \"mass executions\" there by the military should \"almost certainly\" be dismissed as \"standard exaggeration\".\nProperty or money left to Tiberius as emperor but not collected on his death would have passed to Caligula as office-holder. Roman inheritance law recognised a legator's obligation to provide for his family; Caligula seems to have considered his fatherly duties to the state entitled him to a share of every will from pious subjects. The army was not exempt; centurions who left nothing or too little to the emperor could be judged guilty of ingratitude, and have their wills set aside. Centurions who had acquired property by plunder were forced to turn over their spoils to the state.\nStories of a brothel in the Imperial palace, staffed by Roman aristocrats, matrons and their children, are taken literally by Suetonius and Dio; McGinn believes they could be based on a single incident, extended to an institution in the telling. Similar allegations would be made in the future against Commodus and Elagabalus. Winterling, citing Dio 59.28.9, traces the outline of the story to Cassius Dio's account for AD 40, and his allegation that the noble tenants of newly built suites of rooms at the palace were compelled to pay exorbitant rents for the privilege of living so close to Caligula, and under the protection of the praetorians. No brothel is mentioned in this account. Suetonius appears to reverse the traditional aristocratic client-patron ceremonies of mutual obligation, and have Caligula accepting payments for maintenance from his loyal consular \"friends\" at morning salutations, evening banquets, and bequest announcements. The sheer numbers of \"friends\" involved meant that meticulous records were kept of who had paid, how much, and who still owed. His agents would then visit the very same consuls who had been involved in conspiracies against him, rail against the Senate's treachery \"en masse\" but ask for \"gifts\" from individuals to express their loyal friendship in return. A refusal was unthinkable. Winterling describes the families who occupied these rooms as hostage, under the supervision of the Praetorians; some paid up willingly, some reluctantly, but all paid. Caligula made loans available at high interest to those who lacked the necessary funds, to complete the humiliation of Rome's elite, especially the old Republican families.\nDespite his biographers' attempts to ridicule Caligula's taxes, many were continued after his death. The military remained responsible for all tax collection, and the tax on prostitution continued up to the reign of Severus Alexander. Caligula's ruling that bequests made to any reigning emperor became property of his office, not himself as a private individual, was made constitutional under Antoninus Pius.\nCoinage.\nCaligula did not change the structure of the monetary system established by Augustus and continued by Tiberius, but the contents of his coinage differed from theirs. The location of the imperial mint for the coins of precious metals (gold and silver) is a matter of debate among ancient numismatists. It seems that Caligula initially produced his precious coins from Lugdunum (now Lyon, France), like his predecessors, then moved the mint to Rome in 37\u201338, although it is possible that this move occurred later, under Nero. His base metal coinage was struck in Rome.\nUnlike Tiberius, whose coins remained almost unchanged throughout his reign, Caligula used a variety of types, mostly featuring Divus Augustus, as well as his parents Germanicus and Agrippina, his dead brothers Nero and Drusus, and his three sisters Agrippina, Drusilla, and Livilla. The reason for the extensive emphasis on his relatives was to highlight Caligula's double claim to the Principate, from both the Julian and Claudian sides of the dynasty, and to call for the unity of the family. The sesterce with his three sisters was discontinued after 39, due to Caligula's suspicion regarding their loyalty. He also made a sesterce celebrating the Praetorian cohorts as a mean to give them the bequest of Tiberius at the beginning of his reign. Caligula minted a quadrans, a small bronze coin, to mark the abolition of the \"ducentesima\", a 0.5% tax on sales. The output of the precious metal mints was small and his sesterces were mostly made in limited quantities, which make his coins now very rare. This rarity cannot be attributed to Caligula's alleged \"damnatio memoriae\" reported by Dio, as removing his coins from circulation would have been impossible; besides, Mark Antony's coins continued to circulate for two centuries after his death. Caligula's common coins are base metal types with Vesta, Germanicus, and Agrippina the Elder, and the most common is an as with his grandfather Agrippa. Finally, Caligula kept open the mint at Caesarea in Cappadocia, which had been created by Tiberius, in order to pay military expenses in the province with silver drachmae.\nNumismatists Harold Mattingly and Edward Sydenham consider that the artistic style of Caligula's coins is below those of Tiberius and Claudius; they especially criticize the portraits, which are too hard and lack details.\nConstruction.\nCaligula had a fondness for grandiose, costly building projects, many of which were intended to benefit or entertain the general population but are described in Roman sources as wasteful. In the city of Rome, he completed the temple of Augustus and the theatre of Pompey. He is said to have built a bridge between the temple of Castor and Pollux and the Capitol. Barrett (2015) believes that this bridge existed only in Suetonius' account, and should perhaps be dismissed as a fantasy, with possible origins in some jocular remark by Caligula.\nCaligula began an amphitheatre beside the Saepta Julia; he cleared the latter space for use as an arena, and filled it with water for a single naumachia (a sham naval battle fought as entertainment). He supervised the extension and rebuilding of the imperial palace to include a gallery for his art collection. Philo and his party were given a tour of the gallery during their diplomatic visit. Barrett (2015) considers Philo's description of Caligula as a \"would-be connoisseur and aesthete\" as \"probably not very wide of the mark.\" To help meet Rome' burgeoning demand for fresh water, he began the construction of aqueducts Aqua Claudia and Anio Novus, which Pliny the Elder considered to be engineering marvels. He built a large racetrack, now known as the Circus of Gaius and Nero. In its central spine he incorporated an Egyptian obelisk, now known as the Vatican obelisk, which he had brought by sea on a gigantic, purpose-built ship, which used 120,000 modi of lentils as ballast.\nAt Syracuse, he repaired the city walls and temples. He pushed to keep roads in good condition throughout the empire, and extended the existing network: to this end, Caligula investigated the financial affairs of current and past highway commissioners. Those guilty of negligence, embezzlement or misuse of funds were forced to repay what they had dishonestly used for other purposes, or fulfil their commissions at their own expense. Caligula planned to rebuild the palace of Polycrates at Samos, to finish the temple of Didymaean Apollo at Ephesus, and house his own cult and image there: and to found a city high up in the Alps. He intended to dig a canal through the Isthmus of Corinth in Greece and sent a chief centurion to survey the site. None of these plans came to fruition.\nTreason trials.\nIn the course of 39, Caligula's increasingly tense relationship with his Senate deteriorated into outright hostility and confrontation. This is one of Dio's more confusing accounts, involving conspiracies, denunciations and trials for treason (\"maiestas\"), following Caligula's launch of invective at the entire senate, reviewing and condemning their current and past behaviour. He accused them of servility, treachery and hypocrisy in voting honours to Tiberius and Sejanus while they lived, and rescinding those honours once their recipients were safely dead. He declared that it would be folly to seek the love or approval of such men: they hated him, and wanted him dead, so it would be better that they should fear him. Caligula's diatribes exposed the idealised \"princeps\" or First Senator as illusion and imposture. When the senate returned next day, they seemed to confirm his suspicions, and voted him a special guard of armed pretorians to protect him and guard his statues. Apparently seeking to please him and assure his safety, the Senate proposed that his senatorial chair be raised \"on a high platform even in the very Senate house\". They offered a thanksgiving to Caligula, as to a monarch, expressing gratitude for allowing them to live when others had died. Winterling suggests that Caligula's three subsequent consulships, sworn at the Rostra, were vain attempts to make amends, public statements of respect for the senators as his equals. Barrett perceives these later consulships as symbolic of Caligula's continued intention to dominate the senate and the state; Barrett describes the change in Caligula's rule as a gradual unravelling, a \"descent into serious mismanagement and impenetrable mistrust\" \u2013 and, latterly, into \"arbitrary terror\"; but Dio's claim that in fact, \"there was nothing but slaughter\" is undermined by evidence that most senators managed to survive Caligula's reign with their persons and fortunes intact.\nCaligula had not, after all, destroyed Tiberius' records of treason trials. He reviewed them and decided that numerous senators discharged from Tiberius' court hearings seemed to have been guilty of conspiracy all along, against emperor and state \u2013 the worst form of \"maiestas\" (treason). Tiberius' treason trials had encouraged professional \"delatores\" (informers), who were loathed by the populace, but many of the accused had testified against each other, and against Caligula's own family, even to the point of initiating the prosecutions themselves. If they had acted against Caligula's family, they might act against Caligula himself. New investigations were launched; Dio names five once-trusted, consular senators tried for \"maiestas\", but his allegation that senators or others were put to death in \"great numbers\" is unsupported. Two of the five prospered under his rule, and beyond. Caligula preferred to publicly humiliate his enemies in the senate, especially those of ancient families, by stripping them of their inherited honours, dignities and titles. In early September, he dismissed the two suffect consuls, citing their inadequate, low-key celebration of his birthday (August 31) and excessive attention to the anniversary of Actium (September 2). This was the last battle in a damaging civil war between two of Caligula's close ancestors, which he found no cause for celebration. One of the dismissed consuls killed himself: Caligula may have suspected him of conspiracy.\nIncitatus.\nSuetonius and Dio outline Caligula's supposed proposal to promote his favourite racehorse, Incitatus (\"Swift\"), to consul, and later, a priest of his own cult. This could have been an extended joke, created by Caligula himself in mockery of the senate. A persistent, popular belief that Caligula actually promoted his horse to consul has become \"a byword for the promotion of incompetents\", especially in political life. It may have been one of Caligula's many oblique, malicious or darkly humorous insults, mostly directed at the senatorial class, but also against himself and his family. Winterling sees it as an insult to the consulars themselves. An aristocrat's highest ambition, the consulship, could be laid open to ruinous competition and at the same time, to ridicule. David Woods believes it unlikely that Caligula meant to insult the post of consul, as he had held it himself. Suetonius, possibly failing to get the joke, presents it as further proof of Caligula's insanity, adding circumstantial details more usually expected of the senatorial nobility, including palaces, servants and golden goblets, and invitations to banquets.\nBridge at Baiae.\nIn 39 or 40, by Suetonius' reckoning, Caligula ordered a temporary floating bridge to be built using a double line of ships as pontoons, earth-paved and stretching for over two miles from the resort of Baiae, near Naples, to the neighbouring port of Puteoli, with resting places between. Some ships were built on site but grain ships were also requisitioned, brought to site, secured and temporarily resurfaced. Any practical purpose for the bridge is unclear; Winterling believes that it might have been intended to mark Caligula's attempted invasion of Britain. A two-day ceremonial was performed, with offerings to the sea-god Neptune and Invidia (Envy), and a satisfactory result, in that the sea remained completely calm. The bridge was said to rival the Persian king Xerxes' pontoon bridge across the Hellespont.\nFor the opening ceremony, Caligula donned the supposed breastplate of Alexander the Great, and rode his favourite horse, Incitatus, across the bridge, perhaps defying a prediction, attributed by Suetonius to Tiberius' soothsayer Thrasyllus of Mendes, that Caligula had \"no more chance of becoming emperor than of riding a horse across the Bay of Baiae\". On the second day, he rode the bridge from end to end several times \"at full tilt\", accompanied by the soldiery, famous nobles and hostages. Seneca and Dio claim that grain imports were dangerously depleted by Caligula's re-purposing of Rome's grain ships as pontoons. Barrett finds these accusations absurd; if the bridge was finished in 39, that was far too early to have had any effect on the annual grain supply, and \"a genuine grain crisis was simply blamed on the most outlandish episode at hand.\" Dio places this episode soon after Caligula's furious denunciation of the Senate; Barrett speculates that Caligula may have intended the whole event as an object lesson on how completely he was in charge: it may also provide \"the most striking example of his wasteful extravagance\"; its pointlessness might have been the whole point.\nProvinces.\nJudaea and Egypt.\nCaligula's reign saw an increase of tensions between Jews native to their homeland of Judea, Jews of the diaspora, and ethnic Greeks. Greeks and Jews had settled throughout the Roman Empire and Judaea was ruled as a Roman client kingdom. Jews and Greeks had settled in Egypt following its conquest by Macedonian Greeks, and remained there after its conquest by Rome. While the Alexandrian Greeks held citizen status, Alexandrian Jews were classified as mere settlers, with no statutory or citizen rights other than those granted them by their Roman governors. The Greeks feared that official recognition of Jews as citizens would undermine their own status and privilege.\nCaligula had replaced the prefect of Egypt, Aulus Avilius Flaccus, with Herod Agrippa, who was governor of Batanaea and Trachonitis, and was a personal friend. Flaccus had conspired against Caligula's mother and had connections with Egyptian separatists. In 38, Caligula sent Agrippa to Alexandria unannounced to check on Flaccus. According to Philo, the visit was met with jeers and mockery from the Greek population who saw Agrippa as a gimcrack \"king of the Jews.\u201d In Philo's account, a mob of Greeks broke into synagogues to erect statues and shrines of Caligula, against Jewish religious law. Flaccus responded by declaring the Jews \"foreigners and aliens\", and expelled them from all but one of Alexandria's five districts, where they lived under dreadful conditions. Philo gives an account of various atrocities inflicted on Alexandria's Jews within and around this ghetto by the city's Greek population. Caligula held Flaccus responsible for the disturbances, exiled him, and eventually executed him.\nIn 39, Agrippa accused his uncle Herod Antipas, the tetrarch of Galilee and Perea, of planning a rebellion against Roman rule with the help of Parthia. Herod Antipas confessed, Caligula exiled him, and Agrippa was rewarded with his territories. Riots again erupted in Alexandria in 40 between Jews and Greeks, when Jews who refused to venerate the emperor as a god were accused of dishonouring him. In the Judaean city of Jamnia, resident Greeks built a shoddy, sub-standard altar to the Imperial cult, intending to provoke a reaction from the Jews; they immediately tore it down. This was interpreted as an act of rebellion. In response, Caligula ordered the erection of a statue of himself in the Jewish Temple of Jerusalem, a political, rather than a religious act for Rome, but a blasphemy for the Jews, and in conflict with Jewish monotheism. In this context, Philo wrote that Caligula \"regarded the Jews with most especial suspicion, as if they were the only persons who cherished wishes opposed to his\".\nIn May of 40, Philo accompanied a deputation of Alexandrian Jews and Greeks to Caligula, and a second deputation after 31 August that year, during the worst of the Alexandrian riots. Neither of these encounters proved decisive. Both gave Caligula ample opportunity for casual, friendly banter, which seems to have included humiliating levity, always at the Jewish delegation's expense; but he made no claims of divinity, either in his dress nor his speech, merely asking at the second encounter, more or less rhetorically, why Jews found his veneration so difficult. Philo and Josephus each saw Caligula's behaviour as driven by his claims to divinity, which for a Jew would have virtually defined him as fundamentally insane, despite appearances otherwise.\nThe ethnically Greek population of Alexandria had already made their loyalty to the new emperor clear, with displays of his image as focus for his cult. The destruction of the altar at Jamlia and, presumably, removal of \"idolatrous\" images placed in synagogues by Greek citizens, might have been intended as an expression of Jewish religious fervour, rather than a response aimed at one tyrant's offensive claims of personal godhood. Philo seems to have loathed Caligula from the start, but his belief that Caligula hated the Jews and was preparing their destruction has no basis in evidence. To place Caligula's statue in Temple precincts, showing him dressed as Jupiter, would have been consistent with the Empire-wide religious phenomenon known as Imperial cult, from whose full expression Jews had so far been exempted; they could offer prayer \"for\" the emperor, rather than \"to\" him; far from a perfect compromise but the highest honour that Jewish tradition permitted in honour of a mortal. Caligula found this most unsatisfactory, and demanded that his statue be installed in the Temple of Jerusalem forthwith.\nThe Governor of Syria, Publius Petronius, ordered a statue from Sidon, then postponed its installation for as long he could, rather than risk a serious Jewish rebellion. In some versions, Caligula proved amenable to rational discussion with Agrippa and Jewish authorities, and faced with threats of rebellion, destruction of property and loss of the grain-harvest if the plan went ahead, abandoned the project. In more hostile versions Caligula, being demonstrably insane, and incapable of rational discussion, impulsively changed his mind once again, and reissued the order to Petronius along with the threat of enforced suicide if he failed. An even larger statue of Caligula-Zeus was ordered from Rome; the ship carrying it was still under way when news of Caligula's death reached Petronius. Caligula's plan was abandoned, Petronius survived and the statue was never installed.\nPhilo reports a rumour that in 40, Caligula announced to the Senate that he planned to move to Alexandria, and rule the Empire from there as a divine monarch, a Roman pharaoh. Very similar rumours attended Julius Caesar's last days, up to his assassination and very much to his discredit. Caligula's ancestor Mark Antony took refuge in Egypt with Cleopatra, and Augustus had made it a so-called \"Imperial province\", under his direct control. It was the main source of Italy's grain supply, and was administered by members of the equestrian order, directly responsible to the ruling emperor. Egypt was, more or less, Caligula's property, to dispose of as he wished. Roman knowledge of pharaonic brother-sister marriages to maintain the royal bloodline would have shored up the many flimsy, scandalised allegations of adolescent incest between Caligula and Drusilla, supposedly discovered by Antonia but reported as rumour, and only by Suetonius. Barrett finds no further evidence for these allegations, and advises a skeptical attitude.\nGermany and the Rhine frontier.\nIn late 39 or early 40, Caligula ordered the concentration of military forces and supplies in upper Germany, and made his way there with a baggage train that supposedly included actors, gladiators, women, and a detachment of Praetorians. He might have meant to follow the paths of his father and grandfather, and attack the Germanic tribes along the upper Rhine; but he was ill-prepared, and retreated in a panic. According to Dio his achievement was negligible, but Caligula used the opportunity to seize the wealth of rich allies whom he conveniently suspected of treason, \"putting some to death on the grounds that they were 'plotting' or 'rebelling'\". Caligula accused the Imperial legate, Gaetulicus, of \"nefarious plots\", and had him executed \u2013 according to Dio, he was killed for being popular with his troops. Lepidus, along with Caligula's two sisters, Agrippina and Livilla, was accused of being part of this conspiracy; he too was executed and Caligula's two sisters were exiled after being condemned \"pro forma\" for adultery.\nA senatorial embassy arrived from Rome, headed by Caligula's uncle Claudius, to congratulate the emperor for suppressing this latest conspiracy. It met with a hostile reception, in which Claudius was supposedly ducked in the Rhine (though this might have been the loser's award in a contest of Latin and Greek oratory held by Caligula in Gaul that winter). On Caligula's return from the north, he abandoned the theatre seating plans that Augustus had introduced so that rank alone would determine one's place. In the consequent free-for-all, seating was left to chance; doubtless to Caligula's pleasure, fights broke out as senators competed with common citizens for the best seats. Very late in his reign, possibly in its last few days, Caligula sent a communique in preparation for his imminent ovation in Rome, following his military activities in the North and his suppression of Lepidus. He announced that he would only be returning \"to those who wanted him back\"; to the \"Equestrians and the People\"; he did not mention the Senate or senators, of whom he had grown increasingly mistrustful.\nAuctions.\nIn late 39, Caligula wintered at Lugdunum (modern Lyon) in Gaul, where he auctioned off his sisters' portable property, including their jewellery, slaves and freedmen. Dio claims that wealthy bidders at these auctions were willing to offer far more than items were worth; some to show their loyalty, and others to rid themselves of some of the wealth that could render their execution worthwhile. Caligula is said to have used intimidation and various auctioneer's tricks and tactics to boost prices. In an event that Suetonius describes as \"well known\", a Praetorian gentleman, nodding off to sleep after a gladiator match, woke to find that he had bought 13 gladiators for the vastly over-inflated sum of 9 million sesterces. Caligula's first Lugdunum auction proved such a successful fundraiser that he had many of the furnishings of his palace in Rome carted to Lugdunum and auctioned off; they included many precious family heirlooms. Caligula recited their provenance during the auction, in an attempt to help ensure a fair return on objects intrinsically valuable, and seemingly much sought after by the wealthy for their Imperial associations.\nIncome from this second auction was relatively moderate. Kleijwegt (1996) describes Caligula's performance as vendor and auctioneer at this second auction as \"completely out of character with the image of a tyrant\". Auctions of Imperial property were acceptable ways to \"balance the books\", practiced by Augustus and later, by Trajan; they were expected to benefit the bidders as well as the vendor; Roman auctioneers were held in very low esteem, but Kleijwegt claims that Caligula seems to have behaved more like a benevolent \"princeps\" in this second auction, without malice, greed or intimidation.\nBritannia.\nIn the spring of 40, Caligula tried to extend Roman rule into Britannia. Two legions had been raised for this purpose, both likely named \"Primigeniae\" in honour of Caligula's newborn daughter. Ancient sources depict Caligula as being too cowardly to have attacked or as mad, but stories of his threatening a decimation of his troops indicate mutinies. Broadly, \"it is impossible to judge why the army never embarked\" on the invasion. Beyond mutinies, it may have simply been that British chieftains acceded to Rome's demands, removing any justification for war. Alternatively, it could have been merely a training and scouting mission or a short expedition to accept the surrender of the British chieftain Adminius. Suetonius reports that Caligula ordered his men to collect seashells as \"spoils of the sea\"; this may also be a mistranslation of , meaning siege engines. The conquest of Britannia was later achieved during the reign of Caligula's successor, Claudius.\nMauretania.\nIn 40, Caligula annexed Mauretania, a wealthy, strategically significant client kingdom of Rome, inhabited by fiercely independent semi-nomads who resisted Romanisation. Its ruler, Ptolemy of Mauretania, was a noble descendant of Juba II, popular, extremely wealthy and with a reputation as \"feckless and incompetent\". Ptolemy failed to deal effectively with an uprising and was removed. The usual fate of incompetent client kings was retirement and a comfortable exile, but Caligula ordered Ptolemy to Rome and had him executed, some time after the spring of 40. His removal proved unpopular enough in Mauretania to provoke an uprising.\nRome divided Mauretania into two provinces, Mauretania Tingitana and Mauretania Caesariensis, separated by the river Malua. Pliny claims that division was the work of Caligula, but Dio states that the uprising was subdued in 42 (after Caligula's death), by Gaius Suetonius Paulinus and Gnaeus Hosidius Geta, and the division only took place after this. This confusion might mean that Caligula decided to divide the province, but postponed the division because of the rebellion. The first known equestrian governor of the joint provinces was Marcus Fadius Celer Flavianus, in office in 44.\nDetails on the Mauretanian events of 39\u201344 are lost, including an entire chapter by Dio on the annexation. Dio and Tacitus suggest that Caligula may have been motivated by fear, envy, and consideration of his own ignominious military performance in the north, rather than pressing military or economic needs. The rebellion of Tacfarinas had shown how exposed Africa Proconsularis was to its west and how the Mauretanian client kings were unable to provide protection to the province, and it is thus possible that Caligula's expansion was a prudent and ultimately successful response to potential future threats.\nReligion.\nAccording to Barrett, \"[o]f all the manifestations of wild and extravagant behaviour exhibited by Caligula during his brief reign, nothing has better served to confirm the popular notion of his insanity than his apparent demand to be recognised as a god.\"\nPhilo, Caligula's contemporary, claims that Caligula costumed himself as various heroes and deities, starting with demigods such as Dionysos, Herakles and the Dioscuri, and working up to major deities such as Mercury, Venus and Apollo. Philo describes these impersonations in a context of private pantomime or theatrical performances he may have witnessed or heard of during his diplomatic visit, as evidence that Caligula wanted to be venerated as a living god. Philo, as a Jew and a monotheist, took this as proof of the emperor's insanity.\nCaligula's impersonations had a precedent; Augustus had once thrown a party in which he and his guests dressed up as the Olympian gods; Augustus was made up and dressed as Apollo. No-one was thought insane in consequence, and none claimed to be the god they impersonated; but the event was not repeated. It showed near-blasphemous disrespect to the gods in question, and insensitivity to the population at large \u2013 the feast was staged during a famine. Coin issues of the official Roman mint, dated to the early 20s BC, show Octavian as Apollo, Jupiter and Neptune. This too may have been thought a transgression, and was not repeated. Caligula took his own impersonations less seriously than some, certainly less seriously than Philo did. According to Dio, when a Gallic shoemaker laughed to see Caligula dressed as Jupiter, pronouncing oracles at the crowd from a lofty place, Caligula asked \"and who do you think I am?\" The shoemaker answered \"a complete idiot\". Caligula seems to have appreciated his straightforward honesty.\nDio claims that Caligula impersonated Jupiter to seduce various women; that he sometimes referred to himself as a divinity in public meetings; and that he was sometimes referred to as \"Jupiter\" in public documents. Caligula's special interest in Jupiter as Rome's chief deity is confirmed by all surviving sources. Simpson believes that Caligula may have considered Jupiter an equal, perhaps a rival.\nAccording to Ittai Gradel, Caligula's performances as various deities prove no more than a penchant for theatrical fancy-dress and a mischievous desire to shock; as emperor, Caligula was also \"pontifex maximus\", one of Rome's most powerful and influential state priests. The promotion of mortal rulers to godlike status, to honour their superior standing and perceived merits, was a commonplace phenomenon among Rome's eastern allies and client states; during their eastern tour, Germanicus, Agrippina and their children, including Caligula, were officially received as living deities by several cities of the Greek East. In Roman culture a client could flatter their living patron as \"Jupiter on Earth\", without reprimand. The \"divi\" (deceased members of the Imperial family promoted to divine status) were creations of the Senate, who voted them into official existence, appointed their priesthood and granted them cult at state expense. Cicero could protest at the implications of Caesar's divine honours while living but address Publius Lentulus as \"parens ac deus\" (parent and god) to thank him for his help, as aedile, against the conspirator Catiline. Daily reverence was offered as a matter of course to patrons, heads of household and the powerful by their clients, families and social inferiors. In 30 BC, libation-offerings to the \"genius\" of Octavian (later Augustus) became a duty at public and private banquets, and from 12 BC, state oaths were sworn by the \"genius\" of Augustus as the living emperor. Notwithstanding Dio's claims that cult to living emperors was forbidden in Rome itself, there is abundant evidence of municipal cult to Augustus in his lifetime, in Italy and elsewhere, locally organised and financed. As Gradel observes, no Roman was ever prosecuted for sacrificing to his emperor.\nCaligula seems to have taken his religious duties very seriously. He found a replacement for the aged priest of Diana at Lake Nemi, reorganised the Salii (priests of Mars), and pedantically insisted that as it was \"nefas\" (religiously improper) for Jupiter's leading priest, the Flamen Dialis, to swear any oath, he could not swear the imperial oath of loyalty. Caligula wished to take over or share the half-finished but splendid Temple of Apollo in Greek Didyma for his own cult. Seemingly, his statue was prepared, but possibly not installed. When Pausanias visited the still-unfinished temple a century later, its cult statue was of Apollo.\nSuetonius and Dio mention a temple to Caligula in the city of Rome. Most modern scholarship agrees that if such a temple existed, it was probably on the Palatine. Augustus had already linked the Temple of Castor and Pollux directly to his imperial residence on the Palatine, and established an official priesthood of lesser magistrates, the \"seviri Augustales\", usually drawn from his own freedmen to serve the \"genius Augusti\" (his \"family spirit\") and Lares (the twin ancestral spirits of his household). Dio claims that Caligula stationed himself to receive veneration, dressed as Jupiter Latiaris, between the images of Castor and Pollux, the twin Dioscuri, to whom he referred \u2013 humorously \u2013 as his doorkeepers. Dio's claim that two temples were built for Caligula in Rome, is unconfirmed. Simpson believes it likely that Caligula, voted a temple on the Palatine by the Senate, funded it himself.\nAn embassy from Greek states to Rome greeted Caligula as the \"new god Augustus\". In the Greek city of Cyzicus, a public inscription from the beginning of Caligula's reign gives thanks to him as a \"New Sun-god\". Egyptian provincial coinage and some state \"dupondii\" show Caligula enthroned; the first reigning Roman \"princeps\" to be described as the \"New Sun\", () with the radiate crown of the Sun-god, or of Caligula's divine antecedent, the Augustus. Caligula's image on other state coinage carries no such \"trappings of divinity\". Compared to the full-blown cults to major deities of state, \"genius\" cults were quite modest in scope. Augustus, once deceased, was officially worshipped as a \u2013 immortal, but somewhat less than a full-blown deity; Tiberius, his successor, forbade his own personal cult outright in Rome itself, probably in consideration of Julius Caesar's assassination following his hubristic promotion as a living divinity. Augustus, and after him, Tiberius, insisted that if temples to honour them in the provinces were proposed by the local elite, they must be shared by the \"genius of the Senate\", or the personification of the Roman people, or the \"genius\" of Rome itself.\nDio claims that Caligula sold priesthoods for his unofficial \"genius\" cult to the wealthiest nobles, for a \"per capita\" fee of 10 million sesterces, and made loans available to those who could not afford immediate full payment. His priests supposedly included his wife, Caesonia, and his uncle Claudius, whom Dio claims was bankrupted by the cost. The circumstances mark this out as private cult and personal humiliation among the wealthy elite, not subsidised by the Roman state. Throughout his reign, Caligula seems to have remained popular with the masses, in Rome and the empire. There is no sound evidence that he caused the removal, replacement or imposition of Roman or other deities, or even that he threatened to do so, outside the hostile anecdotes of his biographers. Barrett (2015) asserts that the \"emphatic and unequivocal message of the material evidence is that Caligula had no desire for the world to identify him as a god, even if, like most people, he enjoyed being treated like one.\" He did not demand worship as a living god; but he permitted it when it was offered; Imperial etiquette, and the examples of Augustus and Tiberius, would have him refuse divine honours but thank those who offered them, inferring their status as equal to his. He seems to have taken his own \"genius\" cult very seriously but his fatal offense was to willfully \"insult or offend everyone who mattered\", including the military officers who assassinated him.\nAssassination and aftermath.\nOn 24 January 41, the day before his due departure for Alexandria, Caligula was assassinated by the Praetorian tribunes Cassius Chaerea and Cornelius Sabinus, and a number of centurions. Josephus names many of Caligula's inner circle as conspirators, and Dio seems to have had access to a senatorial version which purported to name many others. More likely, very few conspirators would have been involved, and not all need have been directly in touch with each other. The fewer who knew, the greater the chance of success. Previous attempts had foundered or faded out when faced with the rewards and risks of betrayal by colleagues, whether through torture, fear of torture or promised reward. The Senate was a disunited body of self-interested, wealthy and mistrustful aristocrats, unwilling to risk their own prospects, and determined to present a virtuous, united front. In Josephus' account of Caligula's assassination, Chaerea was a \"noble idealist\", deeply committed to \"Republican liberties\"; he was also motivated by resentment of Caligula's routine personal insults and mockery. Suetonius and all other sources confirm that Caligula had insulted Chaerea, giving him watchwords like the ribald \"Priapus\" or \"Venus\", the latter said to refer to Chaerea's weak, high voice, and either his soft-hearted attitude when collecting taxes, or his duty to collect the tax on prostitutes. He was also known to do Caligula's \"dirty work\" for him, including torture.\nChaerea, Sabinus and others accosted Caligula as he addressed an acting troupe of young men beneath the palace during a series of games and dramatics being held for the \"Divus\" Augustus. The source details vary, but all agree that Chaerea was first to stab Caligula. The narrow space available offered little room for escape or rescue, and by the time Caligula's loyal Germanic guard could come to his defence, their Emperor was already dead. They killed several of Caligula's party, including some innocent senators and bystanders. The killing only stopped when the Praetorians took control.\nJosephus reports that the Senate tried to use Caligula's death as an opportunity to restore the Republic. This would have meant the abolition of the office of emperor, the end of dynastic rule, and restoration of the former social stature and privilege of nobles and senators. At least one senator, Lucius Annius Vinicianus, seems to have thought it an opportunity for a takeover. Some modern scholars believe he was the conspiracy's main instigator. Most ordinary citizens were taken aback by Caligula's murder, and found no cause to celebrate in losing the benefits of his rule. Almost all the named conspirators were from the elite. When Caligula's death was confirmed, the nobles and senators who had prospered through hypocrisy and sycophancy during his reign dared to claim prior knowledge of the plot, and share the credit for its success with their peers. Others sought to distance themselves from anything to do with it.\nThe assassins, fearing continued support for Caligula's family and allies, sought out and murdered Caligula's wife, Caesonia, and their young daughter Julia Drusilla, but were unable to reach Caligula's uncle, Claudius. In the traditional account, a soldier, Gratus, found Claudius hiding behind a palace curtain. A sympathetic faction of the Praetorian Guard smuggled him out to their nearby camp, and nominated him as emperor. The Senate, faced with what now seemed inevitable, confirmed their choice. Caligula's \"most powerful and universally feared adviser\", the freedman Callistus, may have engineered this succession, having discreetly shifted his loyalty from Caligula to Claudius while Caligula lived.\nThe killing of Caligula had been extralegal, tantamount to regicide, and those who carried it out had broken their oaths of loyalty to him. Claudius, as a prospective replacement for Caligula, could acknowledge his predecessor's failings but could not be seen to condone his murder, or find fault with the principate as an institution. Caligula had been popular with a clear majority of Rome's lesser citizenry, and the Senate could not afford to ignore the fact. Claudius appointed a new Praetorian prefect, and executed Chaerea, a tribune named Lupus, and the centurions involved. He allowed Sabinus to commit suicide.\nClaudius refused the Senate's requests to formally declare Caligula \"hostis\" (a public enemy), or condemn his memory (see \"damnatio memoriae\"). He also turned down a proposal to officially condemn all the Caesars and destroy their temples. Caligula's name was removed from the official lists of oaths and dedications; some inscriptions were removed or obliterated; most of his statues had the heads recut, to resemble Augustus, or Claudius, or in one case, Nero, who would suffer a similar fate.\nAccording to Suetonius, Caligula's body was placed under turf until it was burned and entombed by his sisters.\nPersonal life.\nCaligula's childhood health may have been delicate; Augustus appointed two physicians to accompany his journey north to join his parents, in AD 14; Suetonius connects this to possible childhood bouts of epilepsy. As an adult, he was subject to fainting fits. He was a habitually light sleeper, prone to nodding off during banquets, sleeping no more than 3 hours in any one night, and subject to vivid nightmares. Barrett describes him as \"nervous and highly strung\". When speaking in public, he would fidget and move about, overcome by the flood of his own words and ideas; despite that, he was an eloquent speaker. He grew stronger with age, but was probably never robust or athletic, despite his practise as a charioteer. Little is known of his illness in 38, nor what it changed, if anything, but it was a serious, possibly life-threatening event. Philo blames it on Caligula's habitual over-indulgence in rich foods and wine, general intemperance and a stress-induced nervous breakdown. Philo believed that the illness removed Caligula's pretence of decency, and revealed his inner cruelty and ruthlessness, evident in the murders of his own father-in-law, Silanus, and young cousin Gemellus.\nThe sources are somewhat contradictory on the matter of Caligula's sex life. Seneca claims that during a public banquet he humiliated senator Decimus Valerius Asiaticus, his \"especial friend\", with a loud first-hand account of Valerius' wife's disappointing performance in bed. Caligula is said to have had \"enormous\" sexual appetites, several mistresses and male lovers, but in relation to the alleged \"perversions\" practised at Corfu by Tiberius and, in some sources, shared by Caligula, Barrett finds him remarkably prudish in expelling the so-called \"spintriae\" from the island on his accession.\nCaligula's first wife was Junia Claudia, daughter of ex-consul Marcus Junius Silanus. Like most marriages in Rome's upper echelons and, perhaps, all but one of Caligula's four marriages, this was a political alliance, intended to produce a legitimate heir and further Caligula's dynasty. Junia and her baby died in child-birth, less than a year later. Soon after, Macro seems to have persuaded his own wife, Ennia Thrasylla, to take up a sexual affair with Caligula, perhaps to help him through the loss. \nSuetonius and Dio claim that Caligula met Livia Orestilla at her marriage to Gaius Calpurnius Piso, and abducted her so that he could marry her instead and father a legitimate heir. When she proved faithful to her former husband, Caligula banished her. The Arval Brethren's records confirm her marriage to Piso, but under ordinary Roman custom. Susan Wood dismisses Caligula's \"marriage\" to her as a drunken party stunt. Caligula's marriage to the \"beautiful... very wealthy\" and extravagant Lollia Paulina was quickly followed by divorce, on the grounds of her infertility. His fourth and last marriage, to Caesonia, seems to have been a love-match, in which he was both \"uxorious and monogamous\", and fathered a daughter whom he named Julia Drusilla, in commemoration of his late sister. Caligula's contemporaries could not understand his attraction to Caesonia; she had proved herself fertile in previous marriages but also had a reputation for \"high living and low morals\", very far from the model of an aristocratic Roman wife. Tales reported by Josephus, Suetonius and the satirist Juvenal regarding Caligula's sexual dynamism are inconsistent with rumours that Caesonia had to arouse his interest with a love potion, which turned his mind and brought on his \"madness\". Barrett suggests that this rumour might have had no foundation other than Caligula's quip that \"he felt like torturing Caesonia to discover why he loved her so passionately.\nAllegations of incest between Caligula and his sisters, or just he and his favourite, Drusilla, go back no further than Suetonius, who admits that in his own time, they were hearsay. Seneca and Philo, moralising contemporaries of Caligula, do not mention these stories even after Caligula's death, when it would have been safe to do so. Caligula's devotion to his youngest sister was evident but then as now, allegations of incest fit the amoral, \"mad Emperor\" stereotype, promiscuous with money, sex and the lives of his subjects. Dio repeats, as fact, the rumour that Caligula also had \"improper relations\" with his two older sisters, Agrippina and Livilla.\nMental condition.\nThere is no reliable evidence of Caligula's mental state at any time in his life. Had he been thought truly insane, his misdeeds would not have been thought his fault: Winterling points out that in Roman law, the insane were not legally responsible for their actions, no matter how extreme. Responsibility for their control and restraint fell on those around them. In the course of their narratives, all the primary and contemporary sources give reasons to discredit and ultimately condemn Caligula, for offences against proprieties of class, religion or his role as emperor. \"Thus, his acts should be seen from other angles, and the search for 'mad Caligula' abandoned\" Barrett suggests that from a very early age, with the loss of his father, then of his mother and what remained of his family, Caligula was preoccupied with his own survival. Given near limitless powers to use as he saw fit, he used them to feed his sense of self-importance, \"practically devoid of any sense of moral responsibility, a man for whom the tenure of the principate was little more than an opportunity to exercise power\". Caligula \"clearly had a highly developed sense of the absurd, resulting in a form of humour that was often cruel, sadistic and malicious, and which made its impact essentially by cleverly scoring points over those who were in no position to respond in kind.\"\nPhilo saw Caligula's illness of 37 as a form of nervous collapse, a response to the extreme stresses and strains of Imperial rule, for which Caligula was temperamentally ill-equipped. Philo, Josephus and Seneca see Caligula's apparent \"insanity\" as an underlying personality trait accentuated through self-indulgence and the unlimited exercise of power. Seneca acknowledges that Caligula's promotion to emperor seemed to make him more arrogant, angry and insulting. Several modern sources suggest underlying medical conditions as explanations for some aspects of his behaviour and appearance. They include mania, bipolar disorder, schizophrenia, encephalitis, meningitis, and epilepsy, the so-called \"falling sickness\". Benediktson refines Suetonius' statement that Caligula could not swim to a diagnosis of interictal temporal lobe epilepsy, and a consequent fear of seizures that prevented his learning to swim. In Romano-Greek medical theory, severe epilepsy attacks were associated with the full moon and the moon goddess Selene, with whom Caligula was claimed to converse and enjoy sexual congress. Suetonius' descriptions of Caligula as physically repulsive are neither reliable nor likely, considering his ecstatic and enthusiastic reception as a youthful \"princeps\" by the populace. In the ancient world, a person's physique was believed to be a reliable guide to their character and behaviour.\nContemporary historiography.\nMost facts and circumstances of Caligula's reign are lost to history. The two most important literary sources on Caligula and his reign are Suetonius, a government official of equestrian rank, born around 70 AD; and Cassius Dio, a Bithynian senator who held consulships in AD 205 and 229. Suetonius tends to arrange his material thematically, with little or no chronological framework, more biographer than historian. Dio provides a somewhat inconsistent chronology of Caligula's reign. He dedicates 13\u201321 chapters to positive features of Caligula's reign but nearly 40 to Caligula as \"monster\".\nPhilo's works \"On the Embassy to Gaius\" and \"Flaccus\" give some details on Caligula's early reign, but more on events involving Jews in Judea and Egypt, whose political and religious interests conflicted with those of the ethnically Greek, pro-Roman population. Philo saw Caligula as responsible for the suffering of the Jews, whom he invariably portrays in a morally positive light. Seneca's various works give mostly scattered anecdotes on Caligula's personality, probably written in the reign of Claudius, who had a vested interest in the portrayal of his predecessor as \"cruel and despotic, even mad\". Seneca was prone to \"grovelling flattery\" of whoever reigned at the time. His experience under Caligula \"could have clouded his judgment\". He narrowly avoided a death sentence in AD\u00a039, probably imposed for his association with known conspirators. Caligula had a low opinion of his literary style.\nFurther contemporaneous histories of Caligula's reign are attested by Tacitus, who describes them as biased for or against Caligula; of Tacitus' own work, little of relevance to Caligula survives but Tacitus' works testify to his general hostility to the imperial system. Among the known losses of his works is a substantial portion of the \"Annals\". Fabius Rusticus and Cluvius Rufus wrote histories, now lost, condemning Caligula. Tacitus describes Fabius Rusticus as a friend of Seneca, prone to embellishments and misrepresentations. Cluvius Rufus was a senator involved in Caligula's assassination; his original works are lost, but he was a competent historian, used as a primary source by Josephus, Tacitus, Suetonius and Plutarch.\nCaligula's sister, Agrippina the Younger, wrote an autobiography that included a detailed account of Caligula's reign, but it too is lost. Agrippina was banished by Caligula for her connection to Marcus Lepidus, who conspired against him. Caligula also seized the inheritance of Agrippina's son, the future emperor Nero. Gaetulicus flattered Caligula in writings now lost. Suetonius wrote his biography of Caligula 80 years after his assassination, and Cassius Dio over 180 years after; the latter offers a loose chronology. Josephus gives a detailed account of Caligula's assassination and its aftermath, published around 93 AD, but it is thought to draw upon a \"richly embroidered and historically imaginative\" anonymous biography of Herod Agrippa, presented as a Jewish \"national hero\". Pliny the Elder's \"Natural History\" has a few brief references to Caligula, possibly based these on the accounts by his friend Suetonius, or an unnamed, shared source. Of the few surviving sources on Caligula, none paints Caligula in a favourable light. Little has survived on the first two years of his reign, and only limited details on later significant events, such as the annexation of Mauretania, Caligula's military actions in Britannia, and the basis of his feud with the Senate.\nExternal links.\n "}
{"id": "6854", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=6854", "title": "Church\u2013Turing thesis", "text": "In computability theory, the Church\u2013Turing thesis (also known as computability thesis, the Turing\u2013Church thesis, the Church\u2013Turing conjecture, Church's thesis, Church's conjecture, and Turing's thesis) is a thesis about the nature of computable functions. It states that a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine. The thesis is named after American mathematician Alonzo Church and the British mathematician Alan Turing. Before the precise definition of computable function, mathematicians often used the informal term effectively calculable to describe functions that are computable by paper-and-pencil methods. In the 1930s, several independent attempts were made to formalize the notion of computability:\nChurch, Kleene, and Turing proved that these three formally defined classes of computable functions coincide: a function is \u03bb-computable if and only if it is Turing computable, and if and only if it is \"general recursive\". This has led mathematicians and computer scientists to believe that the concept of computability is accurately characterized by these three equivalent processes. Other formal attempts to characterize computability have subsequently strengthened this belief (see below).\nOn the other hand, the Church\u2013Turing thesis states that the above three formally-defined classes of computable functions coincide with the \"informal\" notion of an effectively calculable function. Although the thesis has near-universal acceptance, it cannot be formally proven, as the concept of effective calculability is only informally defined.\nSince its inception, variations on the original thesis have arisen, including statements about what can physically be realized by a computer in our universe (physical Church-Turing thesis) and what can be efficiently computed (Church\u2013Turing thesis (complexity theory)). These variations are not due to Church or Turing, but arise from later work in complexity theory and digital physics. The thesis also has implications for the philosophy of mind (see below).\nStatement in Church's and Turing's words.\n addresses the notion of \"effective computability\" as follows: \"Clearly the existence of CC and RC (Church's and Rosser's proofs) presupposes a precise definition of 'effective'. 'Effective method' is here used in the rather special sense of a method each step of which is precisely predetermined and which is certain to produce the answer in a finite number of steps\". Thus the adverb-adjective \"effective\" is used in a sense of \"1a: producing a decided, decisive, or desired effect\", and \"capable of producing a result\".\nIn the following, the words \"effectively calculable\" will mean \"produced by any intuitively 'effective' means whatsoever\" and \"effectively computable\" will mean \"produced by a Turing-machine or equivalent mechanical device\". Turing's \"definitions\" given in a footnote in his 1938 Ph.D. thesis \"Systems of Logic Based on Ordinals\", supervised by Church, are virtually the same:\n We shall use the expression \"computable function\" to mean a function calculable by a machine, and let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions.\nThe thesis can be stated as: \"Every effectively calculable function is a computable function\".\nChurch also stated that \"No computational procedure will be considered as an algorithm unless it can be represented as a Turing Machine\".\nTuring stated it this way:\nIt was stated\u00a0... that \"a function is effectively calculable if its values can be found by some purely mechanical process\". We may take this literally, understanding that by a purely mechanical process one which could be carried out by a machine. The development\u00a0... leads to\u00a0... an identification of computability with effective calculability. [ is the footnote quoted above.]\nHistory.\nOne of the important problems for logicians in the 1930s was the Entscheidungsproblem of David Hilbert and Wilhelm Ackermann, which asked whether there was a mechanical procedure for separating mathematical truths from mathematical falsehoods. This quest required that the notion of \"algorithm\" or \"effective calculability\" be pinned down, at least well enough for the quest to begin. But from the very outset Alonzo Church's attempts began with a debate that continues to this day. the notion of \"effective calculability\" to be (i) an \"axiom or axioms\" in an axiomatic system, (ii) merely a \"definition\" that \"identified\" two or more propositions, (iii) an \"empirical hypothesis\" to be verified by observation of natural events, or (iv) just \"a proposal\" for the sake of argument (i.e. a \"thesis\")?\nCirca 1930\u20131952.\nIn the course of studying the problem, Church and his student Stephen Kleene introduced the notion of \u03bb-definable functions, and they were able to prove that several large classes of functions frequently encountered in number theory were \u03bb-definable. The debate began when Church proposed to G\u00f6del that one should define the \"effectively computable\" functions as the \u03bb-definable functions. G\u00f6del, however, was not convinced and called the proposal \"thoroughly unsatisfactory\". Rather, in correspondence with Church (c. 1934\u20131935), G\u00f6del proposed \"axiomatizing\" the notion of \"effective calculability\"; indeed, in a 1935 letter to Kleene, Church reported that:\nBut G\u00f6del offered no further guidance. Eventually, he would suggest his recursion, modified by Herbrand's suggestion, that G\u00f6del had detailed in his 1934 lectures in Princeton NJ (Kleene and Rosser transcribed the notes). But he did not think that the two ideas could be satisfactorily identified \"except heuristically\".\nNext, it was necessary to identify and prove the equivalence of two notions of effective calculability. Equipped with the \u03bb-calculus and \"general\" recursion, Kleene with help of Church and J. Barkley Rosser produced proofs (1933, 1935) to show that the two calculi are equivalent. Church subsequently modified his methods to include use of Herbrand\u2013G\u00f6del recursion and then proved (1936) that the Entscheidungsproblem is unsolvable: there is no algorithm that can determine whether a well formed formula has a beta normal form.\nMany years later in a letter to Davis (c. 1965), G\u00f6del said that \"he was, at the time of these [1934] lectures, not at all convinced that his concept of recursion comprised all possible recursions\". By 1963\u20131964 G\u00f6del would disavow Herbrand\u2013G\u00f6del recursion and the \u03bb-calculus in favor of the Turing machine as the definition of \"algorithm\" or \"mechanical procedure\" or \"formal system\".\nA hypothesis leading to a natural law?: In late 1936 Alan Turing's paper (also proving that the Entscheidungsproblem is unsolvable) was delivered orally, but had not yet appeared in print. On the other hand, Emil Post's 1936 paper had appeared and was certified independent of Turing's work. Post strongly disagreed with Church's \"identification\" of effective computability with the \u03bb-calculus and recursion, stating:\nRather, he regarded the notion of \"effective calculability\" as merely a \"working hypothesis\" that might lead by inductive reasoning to a \"natural law\" rather than by \"a definition or an axiom\". This idea was \"sharply\" criticized by Church.\nThus Post in his 1936 paper was also discounting Kurt G\u00f6del's suggestion to Church in 1934\u20131935 that the thesis might be expressed as an axiom or set of axioms.\nTuring adds another definition, Rosser equates all three: Within just a short time, Turing's 1936\u20131937 paper \"On Computable Numbers, with an Application to the Entscheidungsproblem\" appeared. In it he stated another notion of \"effective computability\" with the introduction of his a-machines (now known as the Turing machine abstract computational model). And in a proof-sketch added as an \"Appendix\" to his 1936\u20131937 paper, Turing showed that the classes of functions defined by \u03bb-calculus and Turing machines coincided. Church was quick to recognise how compelling Turing's analysis was. In his review of Turing's paper he made clear that Turing's notion made \"the identification with effectiveness in the ordinary (not explicitly defined) sense evident immediately\".\nIn a few years (1939) Turing would propose, like Church and Kleene before him, that \"his\" formal definition of mechanical computing agent was the correct one. Thus, by 1939, both Church (1934) and Turing (1939) had individually proposed that their \"formal systems\" should be \"definitions\" of \"effective calculability\"; neither framed their statements as \"theses\".\nRosser (1939) formally identified the three notions-as-definitions:\nKleene proposes \"Thesis I\": This left the overt expression of a \"thesis\" to Kleene. In 1943 Kleene proposed his \"Thesis I\":\nThe Church\u2013Turing Thesis: Stephen Kleene, in \"Introduction To Metamathematics\", finally goes on to formally name \"Church's Thesis\" and \"Turing's Thesis\", using his theory of recursive realizability. Kleene having switched from presenting his work in the terminology of Church-Kleene lambda definability, to that of G\u00f6del-Kleene recursiveness (partial recursive functions). In this transition, Kleene modified G\u00f6del's general recursive functions to allow for proofs of the unsolvability of problems in the Intuitionism of E. J. Brouwer. In his graduate textbook on logic, \"Church's thesis\" is introduced and basic mathematical results are demonstrated to be unrealizable. Next, Kleene proceeds to present \"Turing's thesis\", where results are shown to be uncomputable, using his simplified derivation of a Turing machine based on the work of Emil Post. Both theses are proven equivalent by use of \"Theorem XXX\".\nKleene, finally, uses for the first time the term the \"Church-Turing thesis\" in a section in which he helps to give clarifications to concepts in Alan Turing's paper \"The Word Problem in Semi-Groups with Cancellation\", as demanded in a critique from William Boone.\nLater developments.\nAn attempt to better understand the notion of \"effective computability\" led Robin Gandy (Turing's student and friend) in 1980 to analyze \"machine\" computation (as opposed to human-computation acted out by a Turing machine). Gandy's curiosity about, and analysis of, cellular automata (including Conway's game of life), parallelism, and crystalline automata, led him to propose four \"principles (or constraints)\u00a0... which it is argued, any machine must satisfy\". His most-important fourth, \"the principle of causality\" is based on the \"finite velocity of propagation of effects and signals; contemporary physics rejects the possibility of instantaneous action at a distance\". From these principles and some additional constraints\u2014(1a) a lower bound on the linear dimensions of any of the parts, (1b) an upper bound on speed of propagation (the velocity of light), (2) discrete progress of the machine, and (3) deterministic behavior\u2014he produces a theorem that \"What can be calculated by a device satisfying principles I\u2013IV is computable.\"\nIn the late 1990s Wilfried Sieg analyzed Turing's and Gandy's notions of \"effective calculability\" with the intent of \"sharpening the informal notion, formulating its general features axiomatically, and investigating the axiomatic framework\". In his 1997 and 2002 work Sieg presents a series of constraints on the behavior of a \"computor\"\u2014\"a human computing agent who proceeds mechanically\". These constraints reduce to:\nThe matter remains in active discussion within the academic community.\nThe thesis as a definition.\nThe thesis can be viewed as nothing but an ordinary mathematical definition. Comments by G\u00f6del on the subject suggest this view, e.g. \"the correct definition of mechanical computability was established beyond any doubt by Turing\". The case for viewing the thesis as nothing more than a definition is made explicitly by Robert I. Soare, where it is also argued that Turing's definition of computability is no less likely to be correct than the epsilon-delta definition of a continuous function.\nSuccess of the thesis.\nOther formalisms (besides recursion, the \u03bb-calculus, and the Turing machine) have been proposed for describing effective calculability/computability. Kleene (1952) adds to the list the functions \"\"reckonable\" in the system S1\" of Kurt G\u00f6del 1936, and Emil Post's (1943, 1946) \"\"canonical\" [also called \"normal\"] \"systems\"\". In the 1950s Hao Wang and Martin Davis greatly simplified the one-tape Turing-machine model (see Post\u2013Turing machine). Marvin Minsky expanded the model to two or more tapes and greatly simplified the tapes into \"up-down counters\", which Melzak and Lambek further evolved into what is now known as the counter machine model. In the late 1960s and early 1970s researchers expanded the counter machine model into the register machine, a close cousin to the modern notion of the computer. Other models include combinatory logic and Markov algorithms. Gurevich adds the pointer machine model of Kolmogorov and Uspensky (1953, 1958): \"...\u00a0they just wanted to\u00a0... convince themselves that there is no way to extend the notion of computable function.\"\nAll these contributions involve proofs that the models are computationally equivalent to the Turing machine; such models are said to be Turing complete. Because all these different attempts at formalizing the concept of \"effective calculability/computability\" have yielded equivalent results, it is now generally assumed that the Church\u2013Turing thesis is correct. In fact, G\u00f6del (1936) proposed something stronger than this; he observed that there was something \"absolute\" about the concept of \"reckonable in S1\":\nInformal usage in proofs.\nProofs in computability theory often invoke the Church\u2013Turing thesis in an informal way to establish the computability of functions while avoiding the (often very long) details which would be involved in a rigorous, formal proof. To establish that a function is computable by Turing machine, it is usually considered sufficient to give an informal English description of how the function can be effectively computed, and then conclude \"by the Church\u2013Turing thesis\" that the function is Turing computable (equivalently, partial recursive).\nDirk van Dalen gives the following example for the sake of illustrating this informal use of the Church\u2013Turing thesis:\nIn order to make the above example completely rigorous, one would have to carefully construct a Turing machine, or \u03bb-function, or carefully invoke recursion axioms, or at best, cleverly invoke various theorems of computability theory. But because the computability theorist believes that Turing computability correctly captures what can be computed effectively, and because an effective procedure is spelled out in English for deciding the set B, the computability theorist accepts this as proof that the set is indeed recursive.\nVariations.\nThe success of the Church\u2013Turing thesis prompted variations of the thesis to be proposed. For example, the physical Church\u2013Turing thesis states: \"All physically computable functions are Turing-computable.\"\nThe Church\u2013Turing thesis says nothing about the efficiency with which one model of computation can simulate another. It has been proved for instance that a (multi-tape) universal Turing machine only suffers a logarithmic slowdown factor in simulating any Turing machine.\nA variation of the Church\u2013Turing thesis addresses whether an arbitrary but \"reasonable\" model of computation can be efficiently simulated. This is called the feasibility thesis, also known as the (classical) complexity-theoretic Church\u2013Turing thesis or the extended Church\u2013Turing thesis, which is not due to Church or Turing, but rather was realized gradually in the development of complexity theory. It states: \"A probabilistic Turing machine can efficiently simulate any realistic model of computation.\" The word 'efficiently' here means up to polynomial-time reductions. This thesis was originally called computational complexity-theoretic Church\u2013Turing thesis by Ethan Bernstein and Umesh Vazirani (1997). The complexity-theoretic Church\u2013Turing thesis, then, posits that all 'reasonable' models of computation yield the same class of problems that can be computed in polynomial time. Assuming the conjecture that probabilistic polynomial time (BPP) equals deterministic polynomial time (P), the word 'probabilistic' is optional in the complexity-theoretic Church\u2013Turing thesis. A similar thesis, called the invariance thesis, was introduced by Cees F. Slot and Peter van Emde Boas. It states: Reasonable' machines can simulate each other within a polynomially bounded overhead in time and a constant-factor overhead in space.\" The thesis originally appeared in a paper at STOC'84, which was the first paper to show that polynomial-time overhead and constant-space overhead could be \"simultaneously\" achieved for a simulation of a Random Access Machine on a Turing machine.\nIf BQP is shown to be a strict superset of BPP, it would invalidate the complexity-theoretic Church\u2013Turing thesis. In other words, there would be efficient quantum algorithms that perform tasks that do not have efficient probabilistic algorithms. This would not however invalidate the original Church\u2013Turing thesis, since a quantum computer can always be simulated by a Turing machine, but it would invalidate the classical complexity-theoretic Church\u2013Turing thesis for efficiency reasons. Consequently, the quantum complexity-theoretic Church\u2013Turing thesis states: \"A quantum Turing machine can efficiently simulate any realistic model of computation.\"\nEugene Eberbach and Peter Wegner claim that the Church\u2013Turing thesis is sometimes interpreted too broadly,\nstating \"Though [...] Turing machines express the behavior of algorithms, the broader assertion that algorithms precisely capture what can be computed is invalid\". They claim that forms of computation not captured by the thesis are relevant today,\nterms which they call super-Turing computation.\nPhilosophical implications.\nPhilosophers have interpreted the Church\u2013Turing thesis as having implications for the philosophy of mind. B. Jack Copeland states that it is an open empirical question whether there are actual deterministic physical processes that, in the long run, elude simulation by a Turing machine; furthermore, he states that it is an open empirical question whether any such processes are involved in the working of the human brain. There are also some important open questions which cover the relationship between the Church\u2013Turing thesis and physics, and the possibility of hypercomputation. When applied to physics, the thesis has several possible meanings:\nThere are many other technical possibilities which fall outside or between these three categories, but these serve to illustrate the range of the concept.\nPhilosophical aspects of the thesis, regarding both physical and biological computers, are also discussed in Odifreddi's 1989 textbook on recursion theory.\nNon-computable functions.\nOne can formally define functions that are not computable. A well-known example of such a function is the Busy Beaver function. This function takes an input \"n\" and returns the largest number of symbols that a Turing machine with \"n\" states can print before halting, when run with no input. Finding an upper bound on the busy beaver function is equivalent to solving the halting problem, a problem known to be unsolvable by Turing machines. Since the busy beaver function cannot be computed by Turing machines, the Church\u2013Turing thesis states that this function cannot be effectively computed by any method.\nSeveral computational models allow for the computation of (Church-Turing) non-computable functions. These are known as\nhypercomputers.\nMark Burgin argues that super-recursive algorithms such as inductive Turing machines disprove the Church\u2013Turing thesis. His argument relies on a definition of algorithm broader than the ordinary one, so that non-computable functions obtained from some inductive Turing machines are called computable. This interpretation of the Church\u2013Turing thesis differs from the interpretation commonly accepted in computability theory, discussed above. The argument that super-recursive algorithms are indeed algorithms in the sense of the Church\u2013Turing thesis has not found broad acceptance within the computability research community."}
{"id": "6856", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=6856", "title": "Chomsky (surname)", "text": "Chomsky (, , , , , \"from (Vyoska) / (nearby Brest, now Belarus)\") is a surname of Slavic origin. Notable people with the surname include:"}
{"id": "6857", "revid": "42724404", "url": "https://en.wikipedia.org/wiki?curid=6857", "title": "Computer multitasking", "text": "In computing, multitasking is the concurrent execution of multiple tasks (also known as processes) over a certain period of time. New tasks can interrupt already started ones before they finish, instead of waiting for them to end. As a result, a computer executes segments of multiple tasks in an interleaved manner, while the tasks share common processing resources such as central processing units (CPUs) and main memory. Multitasking automatically interrupts the running program, saving its state (partial results, memory contents and computer register contents) and loading the saved state of another program and transferring control to it. This \"context switch\" may be initiated at fixed time intervals (pre-emptive multitasking), or the running program may be coded to signal to the supervisory software when it can be interrupted (cooperative multitasking).\nMultitasking does not require parallel execution of multiple tasks at exactly the same time; instead, it allows more than one task to advance over a given period of time. Even on multiprocessor computers, multitasking allows many more tasks to be run than there are CPUs.\nMultitasking is a common feature of computer operating systems since at least the 1960s. It allows more efficient use of the computer hardware; when a program is waiting for some external event such as a user input or an input/output transfer with a peripheral to complete, the central processor can still be used with another program. In a time-sharing system, multiple human operators use the same processor as if it was dedicated to their use, while behind the scenes the computer is serving many users by multitasking their individual programs. In multiprogramming systems, a task runs until it must wait for an external event or until the operating system's scheduler forcibly swaps the running task out of the CPU. Real-time systems such as those designed to control industrial robots, require timely processing; a single processor might be shared between calculations of machine movement, communications, and user interface.\nOften multitasking operating systems include measures to change the priority of individual tasks, so that important jobs receive more processor time than those considered less significant. Depending on the operating system, a task might be as large as an entire application program, or might be made up of smaller threads that carry out portions of the overall program.\nA processor intended for use with multitasking operating systems may include special hardware to securely support multiple tasks, such as memory protection, and protection rings that ensure the supervisory software cannot be damaged or subverted by user-mode program errors.\nThe term \"multitasking\" has become an international term, as the same word is used in many other languages such as German, Italian, Dutch, Romanian, Czech, Danish and Norwegian.\nMultiprogramming.\nIn the early days of computing, CPU time was expensive, and peripherals were very slow. When the computer ran a program that needed access to a peripheral, the central processing unit (CPU) would have to stop executing program instructions while the peripheral processed the data. This was usually very inefficient. Multiprogramming is a computing technique that enables multiple programs to be concurrently loaded and executed into a computer's memory, allowing the CPU to switch between them swiftly. This optimizes CPU utilization by keeping it engaged with the execution of tasks, particularly useful when one program is waiting for I/O operations to complete.\nThe Bull Gamma 60, initially designed in 1957 and first released in 1960, was the first computer designed with multiprogramming in mind. Its architecture featured a central memory and a Program Distributor feeding up to twenty-five autonomous processing units with code and data, and allowing concurrent operation of multiple clusters.\nAnother such computer was the LEO III, first released in 1961. During batch processing, several different programs were loaded in the computer memory, and the first one began to run. When the first program reached an instruction waiting for a peripheral, the context of this program was stored away, and the second program in memory was given a chance to run. The process continued until all programs finished running.\nThe use of multiprogramming was enhanced by the arrival of virtual memory and virtual machine technology, which enabled individual programs to make use of memory and operating system resources as if other concurrently running programs were, for all practical purposes, nonexistent.\nMultiprogramming gives no guarantee that a program will run in a timely manner. Indeed, the first program may very well run for hours without needing access to a peripheral. As there were no users waiting at an interactive terminal, this was no problem: users handed in a deck of punched cards to an operator, and came back a few hours later for printed results. Multiprogramming greatly reduced wait times when multiple batches were being processed.\nCooperative multitasking.\nEarly multitasking systems used applications that voluntarily ceded time to one another. This approach, which was eventually supported by many computer operating systems, is known today as cooperative multitasking. Although it is now rarely used in larger systems except for specific applications such as CICS or the JES2 subsystem, cooperative multitasking was once the only scheduling scheme employed by Microsoft Windows and classic Mac OS to enable multiple applications to run simultaneously. Cooperative multitasking is still used today on RISC OS systems.\nAs a cooperatively multitasked system relies on each process regularly giving up time to other processes on the system, one poorly designed program can consume all of the CPU time for itself, either by performing extensive calculations or by busy waiting; both would cause the whole system to hang. In a server environment, this is a hazard that makes the entire environment unacceptably fragile.\nPreemptive multitasking.\nPreemptive multitasking allows the computer system to more reliably guarantee to each process a regular \"slice\" of operating time. It also allows the system to deal rapidly with important external events like incoming data, which might require the immediate attention of one or another process. Operating systems were developed to take advantage of these hardware capabilities and run multiple processes preemptively. Preemptive multitasking was implemented in the PDP-6 Monitor and Multics in 1964, in OS/360 MFT in 1967, and in Unix in 1969, and was available in some operating systems for computers as small as DEC's PDP-8; it is a core feature of all Unix-like operating systems, such as Linux, Solaris and BSD with its derivatives, as well as modern versions of Windows.\nAt any specific time, processes can be grouped into two categories: those that are waiting for input or output (called \"I/O bound\"), and those that are fully utilizing the CPU (\"CPU bound\"). In primitive systems, the software would often \"poll\", or \"busywait\" while waiting for requested input (such as disk, keyboard or network input). During this time, the system was not performing useful work. With the advent of interrupts and preemptive multitasking, I/O bound processes could be \"blocked\", or put on hold, pending the arrival of the necessary data, allowing other processes to utilize the CPU. As the arrival of the requested data would generate an interrupt, blocked processes could be guaranteed a timely return to execution.\nPossibly the earliest preemptive multitasking OS available to home users was Microware's OS-9, available for computers based on the Motorola 6809 such as the TRS-80 Color Computer 2, with the operating system supplied by Tandy as an upgrade for disk-equipped systems. Sinclair QDOS on the Sinclair QL followed in 1984, but it was not a big success. Commodore's Amiga was released the following year, offering a combination of multitasking and multimedia capabilities. Microsoft made preemptive multitasking a core feature of their flagship operating system in the early 1990s when developing Windows NT 3.1 and then Windows 95. In 1988 Apple offered A/UX as a UNIX System V-based alternative to the Classic Mac OS. In 2001 Apple switched to the NeXTSTEP-influenced Mac OS X.\nA similar model is used in Windows 9x and the Windows NT family, where native 32-bit applications are multitasked preemptively. 64-bit editions of Windows, both for the x86-64 and Itanium architectures, no longer support legacy 16-bit applications, and thus provide preemptive multitasking for all supported applications.\nReal time.\nAnother reason for multitasking was in the design of real-time computing systems, where there are a number of possibly unrelated external activities needed to be controlled by a single processor system. In such systems a hierarchical interrupt system is coupled with process prioritization to ensure that key activities were given a greater share of available process time.\nMultithreading.\nAs multitasking greatly improved the throughput of computers, programmers started to implement applications as sets of cooperating processes (e.\u00a0g., one process gathering input data, one process processing input data, one process writing out results on disk). This, however, required some tools to allow processes to efficiently exchange data.\nThreads were born from the idea that the most efficient way for cooperating processes to exchange data would be to share their entire memory space. Thus, threads are effectively processes that run in the same memory context and share other resources with their parent processes, such as open files. Threads are described as \"lightweight processes\" because switching between threads does not involve changing the memory context.\nWhile threads are scheduled preemptively, some operating systems provide a variant to threads, named \"fibers\", that are scheduled cooperatively. On operating systems that do not provide fibers, an application may implement its own fibers using repeated calls to worker functions. Fibers are even more lightweight than threads, and somewhat easier to program with, although they tend to lose some or all of the benefits of threads on machines with multiple processors.\nSome systems directly support multithreading in hardware.\nMemory protection.\nEssential to any multitasking system is to safely and effectively share access to system resources. Access to memory must be strictly managed to ensure that no process can inadvertently or deliberately read or write to memory locations outside the process's address space. This is done for the purpose of general system stability and data integrity, as well as data security.\nIn general, memory access management is a responsibility of the operating system kernel, in combination with hardware mechanisms that provide supporting functionalities, such as a memory management unit (MMU). If a process attempts to access a memory location outside its memory space, the MMU denies the request and signals the kernel to take appropriate actions; this usually results in forcibly terminating the offending process. Depending on the software and kernel design and the specific error in question, the user may receive an access violation error message such as \"segmentation fault\".\nIn a well designed and correctly implemented multitasking system, a given process can never directly access memory that belongs to another process. An exception to this rule is in the case of shared memory; for example, in the System V inter-process communication mechanism the kernel allocates memory to be mutually shared by multiple processes. Such features are often used by database management software such as PostgreSQL.\nInadequate memory protection mechanisms, either due to flaws in their design or poor implementations, allow for security vulnerabilities that may be potentially exploited by malicious software.\nMemory swapping.\nUse of a swap file or swap partition is a way for the operating system to provide more memory than is physically available by keeping portions of the primary memory in secondary storage. While multitasking and memory swapping are two completely unrelated techniques, they are very often used together, as swapping memory allows more tasks to be loaded at the same time. Typically, a multitasking system allows another process to run when the running process hits a point where it has to wait for some portion of memory to be reloaded from secondary storage.\nProgramming.\nVarious concurrent computing techniques are used to avoid potential problems caused by multiple tasks attempting to access the same resource.\nBigger systems were sometimes built with a central processor(s) and some number of I/O processors, a kind of asymmetric multiprocessing.\nOver the years, multitasking systems have been refined. Modern operating systems generally include detailed mechanisms for prioritizing processes, while symmetric multiprocessing has introduced new complexities and capabilities."}
{"id": "6859", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=6859", "title": "Chiang Kai-shek", "text": "Chiang Kai-shek (31 October 18875 April 1975) was a Chinese politician, revolutionary, and general who led the Republic of China (ROC) from 1928 until his death in 1975. His government was based in mainland China until it was defeated in the Chinese Civil War by the Chinese Communist Party (CCP) in 1949, after which he continued to lead the Republic of China on the island of Taiwan. Chiang served as the de facto leader of the Nationalist Kuomintang (KMT) party and the commander-in-chief of the National Revolutionary Army (NRA) from 1926 until his death, during which he was known as Generalissimo.\nBorn in Zhejiang, Chiang received a military education in China and Japan and joined Sun Yat-sen's Tongmenghui organization in 1908. After the 1911 Revolution, he was a founding member of the KMT and head of the Whampoa Military Academy from 1924. After Sun's death in 1925, Chiang became leader of the party and commander-in-chief of the NRA, and from 1926 to 1928 led the Northern Expedition, which nominally reunified China under a Nationalist government based in Nanjing. The KMT\u2013CCP alliance broke down in 1927 as Chiang massacred the communists in Shanghai, starting the Chinese Civil War. Chiang sought to modernise and unify the ROC during the Nanjing decade, although hostilities with the CCP continued. After Japan's invasion of Manchuria in 1931, his government tried to avoid a war while pursuing economic and social reconstruction. In 1936, Chiang was kidnapped by his generals in the Xi'an Incident and forced to form an anti-Japanese Second United Front with the CCP, and between 1937 and 1945 led China in the Second Sino-Japanese War, mostly from the wartime capital of Chongqing. As the leader of a major Allied power, he attended the 1943 Cairo Conference to discuss the terms for Japan's surrender in 1945, including the return of Taiwan, where he suppressed the February 28 uprising in 1947. \nWhen World War II ended, the civil war with the CCP (led by Mao Zedong) resumed. In 1949, Chiang's government was defeated and retreated to Taiwan, where he imposed martial law and the White Terror, a campaign of mass political repression; they lasted until 1987 and 1992, respectively. Beginning in 1948, he was re-elected five times by the same Eternal Parliament with six-year terms as President of the ROC, the head of a de facto one-party state, for 25 years till his death. Chiang presided over land reform, economic growth, and crises in the Taiwan Strait in 1954\u20131955 and again in 1958. He was considered the legitimate leader of China by the United Nations until 1971, when the ROC's seat was transferred to the People's Republic of China. After Chiang's death in 1975, he was succeeded as leader of the KMT by his son Chiang Ching-kuo, who was elected president in following terms by the same parliament since 1978.\nChiang is a controversial figure. Supporters credit him with unifying the nation and ending the century of humiliation, leading the resistance against Japan, fostering economic development and promoting Chinese culture in contrast to Mao\u2019s Cultural Revolution. He is also credited with protecting the national treasures from the Forbidden City during the wars with Japan and the CCP, eventually bringing them to Taiwan, where he established the National Palace Museum. Critics fault him for his early pacifism toward Japan's occupation of Manchuria, flooding of the Yellow River, cronyism and corruption with the Four Big Families, and his right-wing dictatorship on both mainland China and Taiwan.\nNames.\nLike many other Chinese historical figures, Chiang used several names throughout his life. The name inscribed in the genealogical records of his family is Chiang Chou-t\u2018ai (). This so-called \"register name\" () is the one by which his extended relatives knew him, and the one he used in formal occasions, such as when he was married. In deference to tradition, family members did not use the register name in conversation with people outside of the family. The concept of a \"real\" or original name is/was not as clear-cut in China as it is in the Western world. In honor of tradition, Chinese families waited a number of years before officially naming their children. In the meantime, they used a \"milk name\" (), given to the infant shortly after his birth and known only to the close family. So the name that Chiang received at birth was Chiang Jui-y\u00fcan ().\nIn 1903, the 16-year-old Chiang went to Ningbo as a student, and chose a \"school name\" (). This was the formal name of a person, used by older people to address him, and the one he would use the most in the first decades of his life (as a person grew older, younger generations would use one of the courtesy names instead). Colloquially, the school name is called \"big name\" (), whereas the \"milk name\" is known as the \"small name\" (). The school name that Chiang chose for himself was Chih-ch\u2018ing (, which means \"purity of aspirations\"). For the next fifteen years or so, Chiang was known as Chiang Chih-ch\u2018ing . This is the name by which Sun Yat-sen knew him when Chiang joined the republicans in Kwangtung in the 1910s.\nIn 1912, when Chiang was in Japan, he started to use the name Chiang Kai-shek () as a pen name for the articles that he published in a Chinese magazine he founded: \"Voice of the Army\" (). \"Jieshi\" is the pinyin romanization of this name, based on Standard Chinese, but the most recognized romanized rendering is \"Kai-shek\" which is in Cantonese romanization. Because the Republic of China was based in Canton (a Cantonese-speaking area), Chiang (who never spoke Cantonese but was a native Wu speaker) became known by Westerners under the Cantonese romanisation of his courtesy name, while the family name as known in English seems to be the Mandarin pronunciation of his Chinese family name, transliterated in Wade\u2013Giles.\n\"Kai-shek\" soon became Chiang's courtesy name (). Some think the name was chosen from the classic Chinese book the \"I Ching\"; , is the beginning of line 2 of Hexagram 16, \"\". Others note that the first character of his courtesy name is also the first character of the courtesy name of his brother and other male relatives on the same generational line, while the second character of his courtesy name \"shih\" (\u2014meaning \"stone\") suggests the second character of his \"register name\" \"tai\" (\u2014the famous Mount Tai). Courtesy names in China often bore a connection with the personal name of the person. As the courtesy name is the name used by people of the same generation to address the person, Chiang soon became known under this new name.\nSometime in 1917 or 1918, as Chiang became close to Sun Yat-sen, he changed his name from Chiang Chih-ch\u2018ing to Chiang Chung-cheng (). By adopting the name Chung-cheng, he was choosing a name very similar to the name of Sun Yat-sen, who is known among Chinese as Chung-shan (\u2014meaning \"central mountain\"), thus establishing a link between the two. The meaning of uprightness, rectitude, or orthodoxy, implied by his name, also positioned him as the legitimate heir of Sun Yat-sen and his ideas. It was readily accepted by members of the Kuomintang, and is the name under which Chiang is still commonly known in Taiwan. Often the name is shortened to \"Chung-cheng\" only. Many public places in Taiwan are named Chungcheng after Chiang. For many years passengers arriving at the Chiang Kai-shek International Airport were greeted by signs in Chinese welcoming them to the \"Chung Cheng International Airport\". Similarly, the monument erected to Chiang's memory in Taipei, known in English as Chiang Kai-shek Memorial Hall, was named \"Chung Cheng Memorial Hall\" in Chinese. In Singapore, Chung Cheng High School was named after him.\nHis name is also written in the free area of the Republic of China as \"The Late President Honorable Chiang\" (), where the one-character-wide space in front of his name known as Nuo tai shows respect. He is often called \"Honorable Chiang\".\nThe \"Chiang Kai-shek\" in this article is spelled using a Cantonese transliteration he adopted as opposed to Hanyu Pinyin, though \"pinyin \"was adopted by the Republic of China government in 2009 as its official romanization.\nEarly life.\nChiang was born on 31 October 1887, in Hsikow, a town in Fenghua, Zhejiang, China, about west of central Ningbo. He was born into a family of Wu Chinese-speaking people with their ancestral home\u2014a concept important in Chinese society\u2014in Heqiao, a town in Yixing, Jiangsu, about southwest of central Wuxi and from the shores of Lake Tai. He was the third child and second son of his father (also Chiang Su-an; 1842\u20131895; ) and the first child of his father's third wife (1863\u20131921; ) who were members of a prosperous family of salt merchants. Chiang's father died when he was eight, and he wrote of his mother as the \"embodiment of Confucian virtues\". The young Chiang was inspired throughout his youth by the realization that the reputation of an honored family rested upon his shoulders. He was a naughty child. At a young age he was interested in the military. As he grew older, Chiang became more aware of the issues that surrounded him and in his speech to the Kuomintang in 1945 said:\nIn early 1906, Chiang cut off his queue, the required hairstyle of men during the Ch\u2018ing dynasty, and had it sent home from school, shocking the people in his hometown.\nEducation in Japan.\nChiang grew up at a time in which military defeats, natural disasters, famines, revolts, unequal treaties and civil wars had left the Manchu-dominated Ch\u2018ing dynasty unstable and in debt. Successive demands of the Western powers and Japan since the Opium War had left China owing millions of taels of silver. During his first visit to Japan to pursue a military career from April 1906 to later that year, he describes himself as having strong nationalistic feelings with a desire, among other things, to 'expel the Manchu Ch\u2018ing and to restore China'.\n In a 1969 speech, Chiang related a story about his boat trip to Japan at nineteen years old. Another passenger on the ship, a Chinese fellow student who was in the habit of spitting on the floor, was chided by a Chinese sailor who said that Japanese people did not spit on the floor, but instead would spit into a handkerchief. Chiang used the story as an example of how the common man in 1969 Taiwan had not developed the spirit of public sanitation that Japan had. Chiang decided to pursue a military career. He began his military training at the Baoding Military Academy in 1906, the same year Japan left its bimetallic currency standard, devaluing the Japanese yen. He left for Tokyo Shinbu Gakko, a preparatory school for the Imperial Japanese Army Academy intended for Chinese students, in 1907. There, he came under the influence of compatriots to support the revolutionary movement to overthrow the Manchu-dominated Qing dynasty and to set up a Han-dominated Chinese republic. He befriended Chen Qimei, and in 1908 Chen brought Chiang into the Tungmenghui, an important revolutionary brotherhood of the era. Finishing his military schooling at Tokyo Shinbu Gakko, Chiang served in the Imperial Japanese Army from 1909 to 1911.\nReturning to China.\nAfter learning of the Wuchang uprising, Chiang returned to China in 1911, intending to fight as an artillery officer. He served in the revolutionary forces, leading a regiment in Shanghai under his friend and mentor Chen Qimei, as one of Chen's chief lieutenants. In early 1912 a dispute arose between Chen and Tao Chengzhang, an influential member of the Revolutionary Alliance who opposed both Sun Yat-sen and Chen. Tao sought to avoid escalating the quarrel by hiding in a hospital, but Chiang discovered him there. Chen dispatched assassins. Chiang may not have taken part in the assassination, but would later assume responsibility to help Chen avoid trouble. Chen valued Chiang despite Chiang's already legendary temper, regarding such bellicosity as useful in a military leader.\nChiang's friendship with Chen Qimei signaled an association with Shanghai's criminal syndicate (the Green Gang headed by Du Yuesheng and Huang Jinrong). During Chiang's time in Shanghai, the Shanghai International Settlement police observed him and eventually charged him with various felonies. These charges never resulted in a trial, and Chiang was never jailed.\nChiang became a founding member of the Nationalist Party (a forerunner of the KMT) after the success (February 1912) of the 1911 Revolution. After the takeover of the Republican government by Yuan Shikai and the failed Second Revolution in 1913, Chiang, like his KMT comrades, divided his time between exile in Japan and the havens of the Shanghai International Settlement. In Shanghai, Chiang cultivated ties with the city's underworld gangs, which were dominated by the notorious Green Gang and its leader Du Yuesheng. On 18 May 1916 agents of Yuan Shikai assassinated Chen Qimei. Chiang then succeeded Chen as leader of the Chinese Revolutionary Party in Shanghai. Sun Yat-sen's political career reached its lowest point during this time\u2014most of his old Revolutionary Alliance comrades refused to join him in the exiled Chinese Revolutionary Party.\nEstablishing the Kuomintang's position.\nIn 1917, Sun Yat-sen moved his base of operations to Canton, where Chiang joined him in 1918. At this time Sun remained largely sidelined; without arms or money, he was soon expelled from the city and exiled again to Shanghai, only to return to Canton with mercenary help in 1920. After his return, a rift developed between Sun, who sought to militarily unify China under the KMT, and Canton Governor Chen Chiung-ming, who wanted to implement a federalist system with Canton as a model province. On 16 June 1922 Ye Ju, a general of Chen's whom Sun had attempted to exile, led an assault on Canton's Presidential Palace. Sun had already fled to the naval yard and boarded the SS \"Haichi\", but his wife narrowly evaded shelling and rifle-fire as she fled. They met on the SS \"Yungfeng\", where Chiang joined them as soon as he could return from Shanghai, where he was ritually mourning his mother's death. For about 50 days, Chiang stayed with Sun, protecting and caring for him and earning his lasting trust. They abandoned their attacks on Chen on 9 August, taking a British ship to Hong Kong and traveling to Shanghai by steamer.\nSun regained control of Canton in early 1923, again with the help of mercenaries from Yunnan and of the Comintern. Undertaking a reform of the KMT, he established a revolutionary government aimed at unifying China under the KMT. That same year Sun sent Chiang to Moscow, where he spent three months studying the Soviet political and military system. There Chiang met Leon Trotsky and other Soviet leaders, but quickly came to the conclusion that the Russian model of government was not suitable for China. Chiang later sent his eldest son, Chiang Ching-Kuo, to study in Russia. After his father's split from the First United Front in 1927, Ching-Kuo was retained there, as a hostage until 1937. Chiang wrote in his diary, \"It is not worth it to sacrifice the interest of the country for the sake of my son.\"\nWhen Chiang returned in 1924 Sun appointed him Commandant of the Whampoa Military Academy. Chiang resigned after one month in disagreement with Sun's close cooperation with the Comintern, but returned at Sun's demand, and accepted Chou En-lai as his political commissar. The early years at Whampoa allowed Chiang to cultivate a cadre of young officers loyal to both the KMT and himself.\nThroughout his rise to power, Chiang also benefited from membership within the nationalist Tien-ti-hui fraternity, to which Sun Yat-sen also belonged, and which remained a source of support during his leadership of the Kuomintang.\nRising power.\nSun Yat-sen died on 12 March 1925, creating a power vacuum in the Kuomintang. A contest ensued among Wang Ching-wei, Liao Chung-k\u2018ai, and Hu Han-min. In August, Liao was assassinated and Hu was arrested for his connections to the murderers. Wang Ching-wei, who had succeeded Sun as chairman of the Canton regime, seemed ascendant but was forced into exile by Chiang following the Canton Coup. The , renamed the \"Chung-shan\" in Sun's honour, had appeared off Changzhou, the location of the Whampoa Academy, on apparently-falsified orders and amid a series of unusual phone calls trying to ascertain Chiang's location. He initially considered fleeing Guangdong and even booked passage on a Japanese steamer but then decided to use his military connections to declare martial law on 20 March 1926 and to crack down on Communist and Soviet influence over the National Revolutionary Army, the military academy, and the party. The right wing of the party supported him, and Joseph Stalin, anxious to maintain Soviet influence in the area, had his lieutenants agree to Chiang's demands on a reduced Communist presence in the KMT leadership in exchange for certain other concessions. The rapid replacement of leadership enabled Chiang to effectively end civilian oversight of the military after 15 May, though his authority was somewhat limited by the army's own regional composition and divided loyalties.\nOn 5 June 1926, he was named commander-in-chief of the National Revolutionary Army [NRA] and, on 27 July, he finally launched Sun's long-delayed Northern Expedition, aimed at conquering the northern warlords and bringing China together under the KMT.\nThe NRA branched into three divisions: to the west was the returned Wang Jingwei, who led a column to take Wuhan; Bai Chongxi's column went east to take Shanghai; Chiang himself led in the middle route, planning to take Nanjing before pressing ahead to capture Beijing. However, in January 1927, Wang Jingwei and his KMT leftist allies took the city of Wuhan amid much popular mobilization and fanfare. Allied with a number of Chinese Communists and advised by Soviet agent Mikhail Borodin, Wang declared the national government as having moved to Wuhan.\nIn 1927, when he was setting up the Nationalist government in Nanjing, he was preoccupied with \"the elevation of our leader Dr. Sun Yat-sen to the rank of 'Father of our Chinese Republic'. Dr. Sun worked for 40 years to lead our people in the Nationalist cause, and we cannot allow any other personality to usurp this honored position\". He asked Chen Guofu to purchase a photograph that had been taken in Japan or 1898. It showed members of the Revive China Society with Yeung Ku-wan as president, in the place of honor, and Sun, as secretary, on the back row, along with members of the Japanese Chapter of the Revive China Society. When told that it was not for sale, Chiang offered a million dollars to recover the photo and its negative, \"The party must have this picture and the negative at any price. They must be destroyed as soon as possible. It would be embarrassing to have our Father of the Chinese Republic shown in a subordinate position\".\nOn 12 April 1927, Chiang carried out a purge of thousands of suspected Communists and dissidents in Shanghai, and began large-scale massacres across the country collectively known as the \"White Terror\". During April, more than people were killed in Shanghai. The killings drove most Communists from urban cities and into the rural countryside, where the KMT was less powerful. In the year after April 1927, over 300,000 people died across China in the anti-communist suppression campaigns, executed by the KMT. One of the most famous quotes from Chiang (during that time) was, that he would rather mistakenly kill 1,000 innocent people, than allow one Communist to escape. Some estimates claim the White Terror in China took millions of lives, most of them in rural areas. No concrete number can be verified. Chiang allowed Soviet agent and advisor Mikhail Borodin and Soviet general Vasily Bl\u00fccher (Galens) to \"escape\" to safety after the purge.\nThe NRA formed by the KMT swept through southern and central China until it was checked in Shandong, where confrontations with the Japanese garrison escalated into armed conflict. The conflicts were collectively known as the Jinan incident of 1928.\nNow with an established national government in Nanjing, and supported by conservative allies including Hu Hanmin, Chiang's expulsion of the Communists and their Soviet advisers led to the beginning of the Chinese Civil War. Wang Jingwei's National Government was weak militarily, and was soon ended by Chiang with the support of a local warlord (Li Zongren of Guangxi). Eventually, Wang and his leftist party surrendered to Chiang and joined him in Nanjing. However, the cracks between Chiang and Hu's traditionally Right-Wing KMT faction, the Western Hills Group, began to show soon after the cleansing against the communists, and Chiang later imprisoned Hu.\nThough Chiang had consolidated the power of the KMT in Nanjing, it was still necessary to capture Beijing to claim the legitimacy needed for international recognition. Beijing was taken in June 1928, from an alliance of the warlords Feng Yuxiang and Yan Xishan. Yan Xishan moved in and captured Beiping on behalf of his new allegiance after the death of Zhang Zuolin in 1928. His successor, Zhang Xueliang, accepted the authority of the KMT leadership, and the Northern Expedition officially concluded, completing Chiang's nominal unification of China and ending the Warlord Era.\nAfter the Northern Expedition ended in 1928, Yan, Feng, Li Zongren and Zhang Fakui broke off relations with Chiang shortly after a demilitarization conference in 1929, and together they formed an anti-Chiang coalition to openly challenge the legitimacy of the Nanjing government. In the Central Plains War, they were defeated.\nChiang made great efforts to gain recognition as the official successor of Sun Yat-sen. In a pairing of great political significance, Chiang was Sun's brother-in-law. He had married Soong Mei-ling, the younger sister of Soong Ching-ling, Sun's widow, on 1 December 1927. Originally rebuffed in the early 1920s, Chiang managed to ingratiate himself to some degree with Soong Mei-ling's mother by first divorcing his wife and concubines and promising to sincerely study the precepts of Christianity. He read the copy of the Bible that May-ling had given him twice before making up his mind to become a Christian, and three years after his marriage he was baptized in the Soong's Methodist church. Although some observers felt that he adopted Christianity as a political move, studies of his recently opened diaries suggest that his faith was strong and sincere and that he felt that Christianity reinforced Confucian moral teachings.\nUpon reaching Beijing, Chiang paid homage to Sun Yat-sen and had his body moved to the new capital of Nanjing to be enshrined in a mausoleum, the Sun Yat-sen Mausoleum.\nIn the West and in the Soviet Union, Chiang Kai-shek was known as the \"Red General\". Movie theaters in the Soviet Union showed newsreels and clips of Chiang. At Moscow, Sun Yat-sen University portraits of Chiang were hung on the walls; and, in the Soviet May Day parades that year, Chiang's portrait was to be carried along with the portraits of Karl Marx, Vladimir Lenin, Joseph Stalin, and other Communist leaders. The United States consulate and other Westerners in Shanghai were concerned about the approach of \"Red General\" Chiang as his army was seizing control of large areas of the country in the Northern Expedition.\nRule.\nHaving gained control of China, Chiang's party remained surrounded by defeated warlords who remained relatively autonomous within their own regions. On 10 October 1928, Chiang was named director of the State Council, the equivalent to President of the country, in addition to his other titles. As with his predecessor Sun Yat-sen, the Western media dubbed him \"generalissimo\".\nAccording to Sun Yat-sen's plans, the KMT was to rebuild China in three steps: military rule, political tutelage, and constitutional rule. The ultimate goal of the KMT revolution was democracy, which was not considered to be feasible in China's fragmented state. Since the KMT had completed the first step of revolution through seizure of power in 1928, Chiang's rule thus began a period of what his party considered to be \"political tutelage\" in Sun Yat-sen's name. During this so-called Republican Era, many features of a modern, functional Chinese state emerged and developed.\nFrom 1928 to 1937, known as the Nanjing decade, various aspects of foreign imperialism, concessions and privileges in China were moderated by diplomacy. The government acted to modernize the legal and penal systems and attempted to stabilize prices, amortize debts, reform the banking and currency systems, build railroads and highways, improve public health facilities, legislate against traffic in narcotics, and augment industrial and agricultural production. Efforts were made to improve education standards, and the national academy of sciences, Academia Sinica, was founded. In an effort to unify Chinese society, the New Life Movement was launched to encourage Confucian moral values and personal discipline. \"Guoyu\" (\"national language\") was promoted as the official language, and the establishment of communications facilities (including radio) was used to encourage a sense of Chinese nationalism in a way that had not been possible when the nation lacked an effective central government. Under that context, the Chinese Rural Reconstruction Movement was implemented by some social activists who graduated as professors of the United States with tangible but limited progress in modernizing the tax, infrastructural, economic, cultural, and educational equipment and the mechanisms of rural regions. The social activists actively co-ordinated with the local governments in the towns and villages since the early 1930s. However, the policy was subsequently neglected and canceled by Chiang's government because of rampant wars and the lack of resources after the Japanese War and the civil war.\nDespite being a conservative, Chiang supported modernization policies such as scientific advancement, universal education, and women's rights. The Kuomintang supported women's suffrage and education and the abolition of polygamy and foot binding. Under Chiang's leadership, the Republic of China government also enacted a women's quota in the parliament, with reserved seats for women. During the Nanjing Decade, average Chinese citizens received education that they had been denied by the dynasties. That increased the literacy rate across China and also promoted the ideals of Tridemism of democracy, republicanism, science, constitutionalism, and Chinese nationalism based on the Dang Guo of the KMT.\nAny successes that the Nationalists achieved, however, were met with constant political and military upheavals. Many of the urban areas were now under the control of the KMT, but much of the countryside remained under the influence of weakened-but\n-undefeated warlords, landlords, and Communists. Chiang often resolved issues of warlord obstinacy through military action, but such action was costly in terms of men and material. The Central Plains War alone nearly bankrupted the Nationalist government and caused almost casualties on both sides. In 1931, Hu Hanmin, an old supporter of Chiang, publicly voiced a popular concern that Chiang's position as both premier and president flew in the face of the democratic ideals of the Nationalist government. Chiang had Hu put under house arrest, but Hu was released after national condemnation. Hu then left Nanjing and supported a rival government in Canton. The split resulted in a military conflict between Hu's Guangdong government and Chiang's Nationalist government.\nThroughout his rule, complete eradication of the Communists remained Chiang's dream. After he had assembled his forces in Jiangxi, Chiang led his armies against the newly established Chinese Soviet Republic. With help from foreign military advisers such as Max Bauer and Alexander von Falkenhausen, Chiang's Fifth Campaign finally surrounded the Chinese Red Army in 1934. The Communists, tipped off that a Nationalist offensive was imminent, retreated in the Long March during which Mao rose from a mere military official to the most influential leader of the Chinese Communist Party.\nSome academics and historians have classified Chiang's rule as fascist. The New Life Movement, initiated by Chiang, was based upon Confucianism mixed with Christianity, nationalism, and authoritarianism that have some similarities to fascism. Frederic Wakeman argued that the New Life Movement was \"Confucian fascism\". Chiang also sponsored the creation of the Blue Shirts Society, in conscious imitation of the Blackshirts in the Italian Fascist Party and the \"Sturmabteilung\" of the Nazi Party. Its ideology was to expel foreign (Japanese and Western) imperialists from China and to crush communism. Close ties with Nazi Germany also gave the Nationalist government access to German military and economic assistance during the mid-1930s. In a 1935 speech, Chiang stated that \"fascism is what China now most needs\" and described fascism as the stimulant for a declining society. Mao once derogatorily compared Chiang to Adolf Hitler, referring to him as the \"F\u00fchrer of China\". Sino-German relations rapidly deteriorated as Germany grew closer to Japan and almost completely broke down when Japan launched a full-scale invasion of China in 1937, which Germany failed to mediate. However, China did not declare war on Germany, Italy, or even Japan until after the attack on Pearl Harbor in December 1941.\nChinese Communists and many conservative anti-communist writers have argued that Chiang was pro-capitalist based on the alliance thesis (the alliance between Chiang and the capitalists to purge the communist and the leftist elements in Shanghai, as well as in the resulting civil war). However, Chiang also antagonized the capitalists of Shanghai by often attacking them and confiscating their capital and assets for government use even while he denounced and fought against communists. Critics have called that \"bureaucratic capitalism\". Historian Parks M. Coble argues that the phrase \"bureaucratic capitalism\" is too simplistic to adequately characterize this phenomenon. Instead, he says, the regime weakened all social forces so that the government could pursue policies without being responsible nor responsive to any outside political groups. By defeating any potential challenge to its power, government officials could amass sizable fortunes. With that motive, Chiang cracked down pro-communist worker and peasant organizations, as well as rich Shanghai capitalists. Chiang also continued the anti-capitalist rhetoric of Sun Yat-sen and directed the Kuomintang media to attack the capitalists and capitalism openly. He supported government-controlled industries instead. Coble says that the rhetoric had no impact on governmental policy and that its use was to prevent the capitalists from claiming legitimacy within the party or society and to control them and their wealth.\nAuthority within the Nationalist government ultimately lay with Chiang. All major policy changes on military, diplomatic, or economic issues required his approval. According to historian Odd Arne Westad, \"no other leader within the [KMT] had the authority to force through even the simplest decisions. The practical power of high-ranking officials like ministers or the head of the Executive Yuan was more closely tied to their relationship with Chiang than with the formal authority of their position. Chiang created multiple layers of power in his administration which he sometimes played off against each other to prevent individuals or cliques from gathering power that could oppose his authority.\nContrary to the critique that Chiang was highly corrupt, he was not involved in corruption himself. However his wife, Soong Mei-ling, ignored her family's involvement in corruption. The Soong family embezzled $20 million in the course of the 1930s and the 1940s when the Nationalist government's revenues were less than $30 million per year. The Soong family's eldest son, T.V. Soong, was the Chinese premier and finance minister, and the eldest daughter, Soong Ai-ling, was the wife of Kung Hsiang-hsi, the wealthiest man in China. The second daughter, Soong Ching-ling, was the wife of Sun Yat-sen, China's founding father. The youngest daughter, Soong Mei-ling, married Chiang in 1927, and following the marriage, both families became intimately connected, which created the \"Soong dynasty\" and the \"Four Families\". However, Soong was also credited for her campaign for women's rights in China, including her attempts to improve the education, culture, and social benefits of Chinese women. Critics have said that the \"Four Families\" monopolized the regime and looted it. The US sent considerable aid to the Nationalist government but soon realized the widespread corruption. Military supplies that were sent appeared on the black market. Significant sums of money that had been transmitted through T. V. Soong, China's finance minister, soon disappeared. President Truman famously referred to the Nationalist leaders, \"They're thieves, every damn one of them.\" He also said, \"They stole $750 million out of the billions that we sent to Chiang. They stole it, and it's invested in real estate down in S\u00e3o Paolo and some right here in New York.\" Soong Mei-ling and Soong Ai-ling lived luxurious lifestyles and held millions in property, clothes, art, and jewelry. Soong Ai-ling and Soong Mei-ling were also the two richest women in China. Despite living a luxurious life for almost her entire life, Soong Mei-ling left only a $120,000 inheritance, and the reason is that according to her niece, that she donated most of her wealth when she was still alive. Chiang, requiring support, tolerated corruption with people in his inner circles, as well as high-ranking nationalist officials, but not of lower-ranking officers. In 1934, he ordered seven military officers who embezzled state property to be shot. In another case, several division commanders pleaded with Chiang to pardon a criminal officer, but as soon as the division commanders had left, Chiang ordered him shot. The deputy editor and chief reporter at the Central Daily News, Lu Keng, made headline international news by exposing the corruption of two senior officials, Kong Xiangxi (H. H. Kung) and T. V. Soong. Chiang then ordered a thorough investigation of the Central Daily News to find the source. However, Lu, risked execution by refusing to comply and protecting his journalists. Chiang wanting to avoid an international response and so jailed Lu instead. Chiang realized the widespread problems that corruption was creating and so he undertook several anti-corruption campaigns before and after World War II with varying success. Before the war, both campaigns, the Nanjing Decade Cleanup of 1927\u20131930 and the Wartime Reform Movement of 1944\u20131947, failed. After the World War II and the Civil War, both campaigns, the Kuomintang Reconstruction of 1950\u20131952 and the Governmental Rejuvenation of 1969\u20131973, succeeded.\nChiang, who viewed all of the foreign great powers with suspicion, wrote in a letter that they \"all have it in their minds to promote the interests of their own respective countries at the cost of other nations\" and saw it as hypocritical for any of them to condemn one another's foreign policy. He used diplomatic persuasion on the United States, Nazi Germany, and the Soviet Union to regain lost Chinese territories, as he viewed all foreign powers as imperialists that were attempting to exploit China.\nFirst phase of Chinese Civil War.\nDuring April 1931, Chiang Kai-shek attended a national leadership conference in Nanjing with Zhang Xueliang and General Ma Fuxiang during which Chiang and Zhang dauntlessly upheld that Manchuria was part of China in the face of the Japanese invasion. After the Japanese invasion of Manchuria in 1931, Chiang resigned as Chairman of the National Government. He returned shortly afterward and adopted the slogan \"first internal pacification, then external resistance.\" However, his policy of avoiding a frontal war against Japan and prioritizing anti-communist suppression was widely unpopular and provoked nationwide protests. In 1932, while Chiang was seeking first to defeat the Communists, Japan launched an advance on Shanghai and bombarded Nanjing. That disrupted Chiang's offensives against the Communists for a time, but it was the northern factions of Hu Hanmin's Guangdong government (notably the 19th Route Army) that primarily led the offensive against the Japanese during the skirmish. Brought into the NRA immediately after the battle, the 19th Route Army's career under Chiang would be cut short by being disbanded for demonstrating socialist tendencies. \nIn December 1936, Chiang flew to Xi'an to co-ordinate a major assault on the Red Army and the CPC, which had retreated into Yan'an. However, Chiang's allied commander Zhang Xueliang, whose forces were used in his attack and whose homeland of Manchuria had been recently invaded by the Japanese, did not support the attack on the Communists. On 12 December, Zhang and several other Nationalist generals, headed by Yang Hucheng of Shaanxi kidnapped Chiang for two weeks in what is known as the Xi'an Incident. They forced Chiang into making a \"Second United Front\" with the Communists against Japan. After releasing Chiang and returning to Nanjing with him, Zhang was placed under house arrest, and the generals who had assisted him were executed. The Second United Front had a commitment by Chiang that was nominal at best and was all but dissolved in 1941.\nSecond Sino-Japanese War.\nThe Second Sino-Japanese War broke out in July 1937, and in August, Chiang sent of his best-trained and equipped soldiers to defend Shanghai. With over 200,000 Chinese casualties, Chiang lost the political cream of his Whampoa-trained officers. Although Chiang lost militarily, the battle dispelled Japan's claims that it could conquer China in three months and also demonstrated to the Western powers that the Chinese would continue the fight. By December, the capital city of Nanjing had fallen to the Japanese resulting in the Nanjing Massacre. Chiang moved the government inland first to Wuhan and later to Chongqing.\nHaving lost most of China's economic and industrial centers, Chiang withdrew into the hinterlands, stretched the Japanese supply lines, and bogged down Japanese soldiers in the vast Chinese interior. As part of a policy of protracted resistance, Chiang authorized the use of scorched-earth tactics, which resulted in many civilian deaths. During the Nationalists' retreat from Zhengzhou, the dams around the city were deliberately destroyed by the National Revolutionary Army to delay the Japanese advance, and the subsequent 1938 Yellow River flood killed 800,000 to one million people. Four million Chinese were left homeless. Chiang and the KMT were slow to provide disaster relief.\nAfter heavy fighting, the Japanese occupied Wuhan in the fall of 1938, and the Nationalists retreated farther inland to Chongqing. En route to Chongqing, the Nationalist Army intentionally started the Changsha Fire as a part of its scorched-earth policy. The fire destroyed much of the city, killed 20,000 civilians, and left hundreds of thousands of people homeless. An organizational error (it was claimed) caused the fire to be started without any warning to the residents of the city. The Nationalists eventually blamed three local commanders for the fire and executed them. Newspapers across China blamed the fire on (non-KMT) arsonists, but the blaze contributed to a nationwide loss of support for the KMT.\nIn 1939, the Muslim leaders Isa Yusuf Alptekin and Ma Fuliang were sent by Chiang to several Middle Eastern countries, including Egypt, Turkey, and Syria, to gain support for the war against Japan and to express his support for Muslims.\nThe Japanese, controlling the puppet state of Manchukuo and much of China's eastern seaboard, appointed Wang Jingwei as a puppet ruler of the occupied Chinese territories around Nanjing. Wang named himself President of the Executive Yuan and chairman, and he led a surprisingly large minority of anti-Chiang and anti-Communist Chinese against his old comrades. He died in 1944, a year before the end of World War II.\nThe Hui Xidaotang sect pledged allegiance to the Kuomintang after the party's rise to power, and Hui general Bai Chongxi acquainted Chiang with the Xidaotang Juaozhu Ma Mingren in 1941 in Chongqing.\nIn 1942 Chiang went on tour in northwestern China in Xinjiang, Gansu, Ningxia, Shaanxi, and Qinghai, where he met the Muslim Generals Ma Buqing and Ma Bufang. He also met the Muslim Generals Ma Hongbin and Ma Hongkui separately.\nA border crisis erupted with Tibet in 1942. Under orders from Chiang, Ma Bufang repaired Yushu Airport to prevent Tibetan separatists from seeking independence. Chiang also ordered Ma Bufang to put his Muslim soldiers on alert for an invasion of Tibet in 1942. Ma Bufang complied and moved several thousand troops to the Tibetan border. Chiang also threatened the Tibetans with aerial bombardment if they worked with the Japanese. Ma Bufang attacked the Tibetan Buddhist Tsang monastery in 1941. He also constantly attacked the Labrang Monastery.\nAfter the attack on Pearl Harbor and the opening of the Pacific War, China became one of the Allies. During and after World War II, Chiang and his American-educated wife, Soong Mei-ling, known in the United States as \"Madame Chiang\", held the support of the American China Lobby, which saw in them the hope of a Christian and democratic China. Chiang was even named the Supreme Commander of Allied forces in the China war zone. He was appointed Knight Grand Cross of the Order of the Bath in 1942.\nGeneral Joseph Stilwell, an American military advisor to Chiang during World War II, strongly criticized Chiang and his generals for what Stilwell saw as their incompetence and corruption. In 1944, the United States Army Air Corps commenced Operation Matterhorn to bomb Japan's steel industry from bases to be constructed in mainland China. That was meant to fulfill US President Franklin D. Roosevelt's promise to Chiang to begin bombing operations against Japan by November 1944. However, Chiang's subordinates refused to take air base construction seriously until enough capital had been delivered to permit embezzlement on a massive scale. Stilwell estimated that at least half of the $100 million spent on construction of air bases was embezzled by Nationalist party officials.\nThe poor performance of Nationalist forces during the Japanese Ichigo campaign contributed to the view that Chiang was incompetent. Their poor performance irreparably damaged Chiang and the Nationalists in the view of the Roosevelt administration. Chiang argued that the United States, and Stillwell in particular, were at fault for the failure because they had moved too many Chinese troops into the Burma campaign.\nAfter the Japanese surrender, Chiang had to rely on the assistance of the United States in order to transport his troops to regain control of occupied areas. Non-Chinese found the behavior of these troops and accompanying officials as undercutting Nationalist legitimacy, as Nationalist forces engaged in a \"botched liberation\" characterized by corruption, looting, and inefficiency.\nChiang tried to balance the influence of the Soviets and the Americans in China during the war. He first told the Americans that they would be welcome in talks between the Soviet Union and China and then secretly told the Soviets that the Americans were unimportant and that their opinions would not be considered. Chiang also used American support and military power in China against Soviet ambitions to dominate the talks. That stopped the Soviets from taking full advantage of the situation in China by the threat of American military action against them.\nChiang's Nationalist government made laws on abortion in China more restrictive during the Second Sino-Japanese War. In 1945, Chiang adopted a eugenic population policy that was intended to promote hybrid vigor by encouraging intermarriage between whites and Chinese to combine European fair skin with superior Chinese intelligence. Although adopted, the policy was never successfully implemented.\nFrench Indochina.\nPresident Roosevelt, through General Stilwell, privately made it clear that he preferred for the French not to reacquire French Indochina (now Vietnam, Cambodia and Laos) after the war was over. Roosevelt offered Chiang control of all of Indochina. It was said that Chiang replied in English, \"Under no circumstances!\"\nAfter the war, 200,000 Chinese troops under General Lu Han were sent by Chiang to northern Indochina (north of the 16th parallel) to accept the surrender of Japanese occupying forces there, and the Chinese forces remained in Indochina until 1946, when the French returned. The Chinese used the VNQDD, the Vietnamese branch of the Kuomintang, to increase their influence in Indochina and to put pressure on their opponents. Chiang threatened the French with war in response to maneuvering by the French and Ho Chi Minh's forces against each other and forced them to come to a peace agreement. In February 1946, he also forced the French to surrender all of their concessions in China and to renounce their extraterritorial privileges in exchange for the Chinese withdrawing from northern Indochina and allowing French troops to reoccupy the region. After France's agreement to those demands, 20,000 French soldiers landed in Haiphong, North Vietnam, on 6 March 1946, under the leadership of general Philippe Leclerc de Hauteclocque, followed by the withdrawal of Chinese troops which began in March 1946.\nRyukyus.\nAccording to Republic of China's notes of a dinner meeting during the Cairo Conference in 1943, Roosevelt asked Chiang whether China desired the Ryukyu Islands as territories restored from Japan. Chiang said he would be agreeable to joint occupation and administration by China and the United States.\nSecond phase of Chinese Civil War.\nTreatment and use of Japanese soldiers.\nBecause of Chiang's focus on his communist opponents, he allowed some Japanese forces and forces from the Japanese puppet regimes to remain on duty in occupied areas in an effort to prevent the communists from accepting their surrender.\nAmerican troops and weapons soon bolstered the Nationalist forces, which allowed them to reclaim the cities. The countryside, however, remained largely under Communist control. Chiang implemented his war-time phrase \"repay evil with good\" and made a huge effort to protect elements of the Japanese invading army. In 1949, a Nationalist court acquitted General Okamura Yasuji, the chief commander of Japanese forces in China, of alleged war crimes, retaining him as an advisor. Nationalist China repeatedly intervened to protect Okamura from repeated American requests to testify at the Tokyo war crimes trial.\nMany top Nationalist generals, including Chiang, had studied and trained in Japan before the Nationalists had returned to the mainland in the 1920s and maintained close personal friendships with top Japanese officers. The Japanese general in charge of all forces in China, General Okamura had personally trained officers who later became generals in Chiang's staff. Reportedly, Chiang seriously considered accepting this offer but declined only because he knew that the United States would certainly be outraged by the gesture. Even so, armed Japanese troops remained in China well into 1947, with some non-commissioned officers finding their way into the Nationalist officer corps. The Japanese in China came to regard Chiang as a magnanimous figure to whom many of them owed their lives and livelihoods; that fact was attested by both Nationalist and Communist sources.\nConditions during Chinese Civil War.\nChiang did not de-mobilize his troops after the defeat of the Japanese, instead remaining on a war footing to prepare for the resumption of civil war against the Communists. This further strained the economy of Nationalist era China, worsening deficits. A significant body of evidence suggests that much of the Nationalist military budget in this period was wasted. One factor in military budget waste included that troop counts were inflated above actual head counts and that officers embezzled the salaries of the non-existent soldiers. Another was the power of military commanders over local branches of the Bank of China, which they could require to provide currency outside of the normal budget process.\nAlthough Chiang had achieved status abroad as a world leader, his government deteriorated as the result of corruption and hyperinflation. In his diary in June 1948, Chiang wrote that the KMT had failed not because of external enemies but because of rot from within. The war had severely weakened the Nationalists, and the Communists were strengthened by their popular land reform policies and by a rural population that supported and trusted them. The Nationalists initially had superiority in arms and men, but their lack of popularity, infiltration by Communist agents, low morale, and disorganization soon allowed the Communists to gain the upper hand in the civil war.\nAfter World War II, the United States encouraged peace talks between Chiang and the Communist leader, Mao Zedong, in Chongqing. Concerns about widespread and well-documented corruption in Chiang's government throughout his rule made the US government limit aid to Chiang for much of the period of 1946 to 1948 despite the fighting against Mao's Red Army. Alleged infiltration of the US government by CCP agents may have also played a role in the suspension of American aid.\nChiang's right-hand man, the secret police chief Dai Li, was anti-American and anti-Communist and a self-declared fascist. Dai ordered Kuomintang agents to spy on American officers. Earlier, Dai had been involved with the Blue Shirts Society, a fascist-inspired paramilitary group within the Kuomintang that wanted to expel Western and Japanese imperialists, crush the Communists, and eliminate feudalism. Dai Li died in a plane crash, which some suspect to be an assassination orchestrated by Chiang; however, the assassination was also rumoured to have been arranged by the American Office of Strategic Services because of Dai's anti-Americanism and since it happened on an American plane.\nConflict with Li Zongren.\nA new constitution was promulgated in 1947, and Chiang was elected by the National Assembly as the first President of the Republic of China on 20 May 1948. That marked the beginning of what was termed the \"democratic constitutional government\" period by the KMT political orthodoxy, but the Communists refused to recognize the new Constitution, and its government as legitimate. Chiang resigned as president on 21 January 1949, as Nationalist forces suffered terrible losses and defections to the Communists. After Chiang's resignation, vice-president Li Zongren became China's acting president.\nShortly after Chiang's resignation, the Communists halted their advances and attempted to negotiate the Nationalists' virtual surrender. Li tried to negotiate milder terms to end the civil war but had no success. When it became clear that Li was unlikely to accept Mao's terms, the Communists issued an ultimatum in April 1949 that warned that they would resume their attacks if Li did not agree within five days. Li refused.\nLi's attempts to carry out his policies faced varying degrees of opposition from Chiang's supporters and were generally unsuccessful. Taylor has noted that Chiang had a superstitious belief in holding Manchuria. After the Nationalist military defeat in the province, Chiang lost faith in winning the war and started to prepare for the retreat to Taiwan. Chiang especially antagonized Li by taking possession of and moving to Taiwan US$200\u00a0million of gold and US dollars that belonged to the central government. Li desperately needed them to cover the government's soaring expenses. When the Communists captured the Nationalist capital of Nanjing in April 1949, Li refused to accompany the central government as it fled to Guangdong and instead expressed his dissatisfaction with Chiang by retiring to Guangxi.\nThe former warlord Yan Xishan, who had fled to Nanjing only one month earlier, quickly insinuated himself within the Li-Chiang rivalry and attempted to have Li and Chiang reconcile their differences in the effort to resist the Communists. At Chiang's request, Yan visited Li to convince Li not to withdraw from public life. Yan broke down in tears while he talked of the loss of his home province of Shanxi to the Communists, and he warned Li that the Nationalist cause was doomed unless Li went to Guangdong. Li agreed to return if Chiang surrendered most of the gold and US dollars in his possession that belonged to the central government, and Chiang stopped overriding Li's authority. After Yan communicated those demands and Chiang agreed to comply with them, Li departed for Guangdong.\nIn Guangdong, Li attempted to create a new government composed of both supporters and opponents of Chiang. Li's first choice of premier was Chu Cheng, a veteran member of the Kuomintang who had been virtually driven into exile for his strong opposition to Chiang. After the Legislative Yuan jas rejected Chu, Li was obliged to choose Yan Xishan instead. By then, Yan was well known for his adaptability, and Chiang welcomed his appointment.\nThe conflict between Chiang and Li persisted. Although he had agreed to do so as a prerequisite of Li's return, Chiang refused to surrender more than a fraction of the wealth that he had sent to Taiwan. Without being backed by gold or foreign currency, the money that was issued by Li and Yan quickly declined in value until it became virtually worthless. Although he did not hold a formal executive position in the government, Chiang continued to issue orders to the army, and many officers continued to obey Chiang, rather than Li. The inability of Li to co-ordinate KMT military forces led him to put into effect a plan of defense that he had contemplated in 1948. Instead of attempting to defend all of southern China, Li ordered what remained of the Nationalist armies to withdraw to Guangxi and Guangdong. He hoped that he could concentrate all available defenses on the smaller area, which would be more easily defensible. The object of Li's strategy was to maintain a foothold on the Chinese mainland in the hope that the United States would eventually be compelled to enter the war in China on the Nationalist side.\nFinal Communist advance.\nChiang opposed Li's plan of defense because it would have placed most of the troops who were still loyal to Chiang under the control of Li and Chiang's other opponents in the central government. To overcome Chiang's intransigence Li began ousting Chiang's supporters within the central government. Yan Xishan continued in his attempts to work with both sides, which created the impression among Li's supporters that he was a stooge of Chiang, and those who supported Chiang began to bitterly resent Yan for his willingness to work with Li. Because of the rivalry between Chiang and Li, Chiang refused to allow Nationalist troops loyal to him to aid in the defense of Guangxi and Canton. That let Communist forces occupy Canton in October 1949.\nAfter Canton fell to the Communists, Chiang relocated the government to Chongqing, and Li effectively surrendered his powers and flew to New York for treatment of his chronic duodenum illness at the Hospital of Columbia University. Li visited President Truman, and denounced Chiang as a dictator and an usurper. Li vowed that he would \"return to crush\" Chiang once he returned to China. Li remained in exile and did not return to Taiwan.\nIn the early morning of 10 December 1949, Communist troops laid siege to Chengdu, the last KMT-controlled city in mainland China, where Chiang Kai-shek and his son Chiang Ching-kuo directed the defense at the Chengtu Central Military Academy. Flying out of Chengdu Fenghuangshan Airport, father and son were evacuated to Taiwan via Guangdong on the aircraft \"May-ling\" and arrived the same day. Chiang Kai-shek would never return to the mainland.\nHistorian Odd Arne Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang had. Also, his search for a powerful centralized government made Chiang antagonize too many interest groups in China. Furthermore, his party was weakened by the war against Japan. Meanwhile, the Communists told different groups, such as peasants, exactly what they wanted to hear and cloaked themselves in the cover of Chinese nationalism.\nChiang did not reassume the presidency until 1 March 1950. In January 1952, Chiang commanded the Control Yuan, now in Taiwan, to impeach Li in the \"Case of Li Zongren's Failure to carry out Duties due to Illegal Conduct\" (\u674e\u5b97\u4ec1\u9055\u6cd5\u5931\u8077\u6848). Chiang relieved Li of the position as vice-president of the National Assembly in March 1954.\nIn Taiwan.\nPreparations to retake the mainland.\nChiang moved the government to Taipei, Taiwan, where he resumed his duties as president on 1 March 1950. Chiang was re-elected by the National Assembly to be the President of the Republic of China on 20 May 1954, and again in 1960, 1966, and 1972. He continued to claim sovereignty over all of China, including the territories held by his government and the People's Republic, as well as territory the latter ceded to foreign governments, such as Tuva and Outer Mongolia. In the context of the Cold War, most of the Western world recognized that position, and the ROC represented China in the United Nations and other international organizations until the 1970s.\nDuring his presidency on Taiwan, Chiang continued making preparations to take back mainland China. He developed the JROTC army to prepare for an invasion of the mainland and to defend Taiwan in case of an attack by the Communist forces. He also financed armed groups in mainland China, such as Muslim soldiers of the ROC Army who had been left in Yunnan under Li Mi and continued to fight. It was not until the 1980s that those troops were finally airlifted to Taiwan. He promoted the Uyghur Yulbars Khan to governor during the Islamic insurgency on the mainland for resisting the Communists even though the government had already evacuated to Taiwan. He planned an invasion of the mainland in 1962. In the 1950s, Chiang's airplanes dropped supplies to Kuomintang Muslim insurgents in Qinghai, in the traditional Tibetan area of Amdo.\nRegime in Taiwan.\nDespite an ostensibly democratic constitution, the government under Chiang was a de facto one-party state, consisting almost completely of mainlanders; the \"Temporary Provisions Effective During the Period of Communist Rebellion\" greatly enhanced the executive's powers, and the goal of retaking mainland China allowed the KMT to maintain a monopoly on power and to prohibit real parliamentary opposition. The government's official line for the martial law provisions stemmed from the claim that emergency provisions were necessary since the Communists and the Nationalists were still in a state of war. Seeking to promote Chinese nationalism, Chiang's government actively ignored and suppressed local cultural expression and even forbade the use of local languages in mass media broadcasts or during class sessions. As a result of Taiwan's anti-government uprising in 1947, known as the February 28 incident, the KMT-led political repression resulted in the death or the disappearance of up to 30,000 Taiwanese intellectuals, activists, and people suspected of opposition to the KMT.\nThe first decades after the Nationalists had moved the seat of government to the province of Taiwan are associated with the organized effort to resist Communism, which was known as the \"White Terror\"; about 140,000 Taiwanese were imprisoned for their real or perceived opposition to the Kuomintang. Most of those prosecuted were labeled by the Kuomintang as \"bandit spies\" (\u532a\u8adc), meaning spies for Chinese Communists, and punished as such or \"Taiwanese Separatists\" (\u53f0\u7368\u5206\u5b50).\nUnder the pretext that new elections could not be held in Communist-occupied constituencies, the National Assembly, Legislative Yuan, and Control Yuan members held their posts indefinitely. The Temporary Provisions also allowed Chiang to remain as president beyond the two-term limit in the Constitution. He was re-elected by the National Assembly as president four times: in 1954, 1960, 1966, and 1972.\nBelieving that corruption and the lack of morals were key reasons that the KMT had lost mainland China to the Communists, Chiang attempted to purge corruption by dismissing members of the KMT who were accused of graft. Some major figures in the previous mainland Chinese government, such as Chiang's brothers-in-law H. H. Kung and T. V. Soong, exiled themselves to the United States. Although politically authoritarian and, to some extent, dominated by government-owned industries, Chiang's new Taiwanese state also encouraged economic development, especially in the export sector. A popular sweeping Land Reform Act, as well as American foreign aid during the 1950s, laid the foundation for Taiwan's economic success to become one of the Four Asian Tigers. After retreating to Taiwan, Chiang learned from his mistakes and failures in the mainland and blamed them for failing to pursue Sun Yat-sen's ideals of Tridemism and welfarism. Chiang's land reform more than doubled the land ownership of Taiwanese farmers. It removed the rent burdens on them, with former landowners using the government compensation to become the new capitalist class. He promoted a mixed economy of state and private ownership with economic planning. Chiang also promoted a nine-year free education and the importance of science in Taiwanese education and values. Those measures generated great success, with consistent and strong growth and the stabilization of inflation.\nAfter the government of the Republic of China had moved to Taiwan, Chiang Kai-shek's economic policy turned towards to economic liberalism and used Sho-Chieh Tsiang and other liberal economists to promote economic liberalization reforms in Taiwan.\nHowever, Taylor has noted that the developmental model of Chiangism in Taiwan still had elements of socialism, and the Gini index of Taiwan was around 0.28 by the 1970s, which was lower than the relatively-egalitarian West Germany. ROC (Taiwan) was one of the most equal countries in the pro-western bloc. Those in the lower 40% of income doubled their share to 22% of the total income, with the upper 20% shrinking their share from 61% to 39%, from the time of Japanese rule. The Chiangist economic model can be seen as a form of dirigisme, with the state playing a crucial role in directing the market economy. Small businesses and state-owned enterprises in Taiwan flourished under the economic model, but the economy did not see the emergence of corporate monopolies, unlike in most other major capitalist countries.\nAfter the democratization of Taiwan, it began to slowly drift away from the Chiangist economic policy to embrace a more free market system, as part of the economic globalization process under the context of neoliberalism.\nChiang had the personal power to review the rulings of all military tribunals, which during the martial law period tried civilians as well. In 1950, Lin Pang-chun and two other men were arrested on charges of financial crimes and sentenced to 3\u201310 years in prison. Chiang reviewed the sentences of all three and ordered them executed instead. In 1954, the Changhua monk Kao Chih-te and two others were sentenced to 12 years in prison for providing aid to accused communists. Chiang sentenced them to death after he had reviewed the case. That control over the decision of military tribunals violated the ROC constitution.\nAfter Chiang's death, the next president, his son, Chiang Ching-kuo, and Chiang Ching-kuo's successor, Lee Teng-hui, a native Taiwanese, would in the 1980s and 1990s increase native Taiwanese representation in the government and loosen the many authoritarian controls of the early era of ROC control in Taiwan, paving way for the democratization process.\nRelations with Japan.\nIn 1971, the former Australian opposition leader Gough Whitlam became Prime Minister in 1972, and swiftly relocated the Australian mission from Taipei to Beijing, visited Japan. After meeting with Japanese Prime Minister Eisaku Sato Whitlam observed that the reason that Japan was hesitant to withdraw recognition from the Nationalist government was \"the presence of a treaty between the Japanese government and that of Chiang Kai-shek.\" Sato explained that the continued recognition of Japan towards the Nationalist government was largely because of the personal relationship that various members of the Japanese government felt towards Chiang. This relationship was rooted largely in the generous and lenient treatment of Japanese prisoners-of-war by the Nationalist government in the years immediately after the Japanese surrender in 1945, and was felt especially strongly as a bond of personal obligation by the most senior members who were in power.\nAlthough Japan recognized the People's Republic in 1972, shortly after Kakuei Tanaka had succeeded Sato as Prime Minister of Japan, the memory of the relationship was strong enough to be reported by \"The New York Times\" (15 April 1978) as a significant factor inhibiting trade between Japan and the mainland. There is speculation that a clash between Communist forces and a Japanese warship in 1978 was caused by Chinese anger by Japanese Prime Minister Takeo Fukuda attending Chiang's funeral. Historically, Japan's attempts to normalize its relationship with the People's Republic were met with accusations of ingratitude in Taiwan.\nRelations with United States.\nChiang was suspicious that covert operatives of the United States were plotting a coup against him.\nIn 1950, Chiang Ching-kuo became director of the secret police (Bureau of Investigation and Statistics), which he remained until 1965. Chiang Kai-shek was also suspicious of politicians who were overly friendly to the United States and considered them his enemies. In 1953, seven days after surviving an assassination attempt, Wu Kuo-chen lost his position as governor of Taiwan Province to Chiang Ching-kuo. After fleeing to United States the same year, Wu became a vocal critic of Chiang's family and government.\nChiang Ching-kuo, who had been educated in the Soviet Union, initiated Soviet-style military organization in the Republic of China Armed Forces. He reorganized and Sovietized the political officer corps and propagated Kuomintang ideology throughout the military. Sun Li-jen, who had been educated at the American Virginia Military Institute, opposed those practices.\nChiang Ching-kuo orchestrated the controversial court-martial and arrest of General Sun Li-jen in August 1955 for plotting a coup d'\u00e9tat with the CIA against his father, Chiang Kai-shek, and the Kuomintang. The CIA allegedly wanted to help Sun take control of Taiwan and declare its independence.\nDeath.\nIn 1975, 26 years after Chiang had come to Taiwan, he died in Taipei at the age of 87. He had suffered a heart attack and pneumonia in the foregoing months, and died from kidney failure aggravated by advanced heart failure on 5 April. Chiang's funeral was held on 16 April.\nA month of mourning was declared. The Chinese music composer Hwang Yau-tai wrote the \"Chiang Kai-shek Memorial Song\". In mainland China, however, Chiang's death was met with little apparent mourning, and Communist state-run newspapers gave the brief headline \"Chiang Kai-shek Has Died\". Chiang's body was put in a copper coffin and temporarily interred at his favorite residence in Cihu, Daxi, Taoyuan. His funeral was attended by dignitaries from many nations, including US Vice President Nelson Rockefeller, South Korean Prime Minister Kim Jong-pil, and two former Japanese prime ministers: Nobusuke Kishi and Eisaku Sato. was established on 5 April. The memorial day was disestablished in 2007.\nThe response by Japanese media was swift and shaped by a cult of personality around Chiang Kai-shek. Japanese conservatives had long promoted to counter the China policy and the historical narratives of their leftist pro-PRC opponents. The nationalist leader of Taiwan had been trained in Japanese military schools and shared a particular fondness for the Japanese Empire.\nWhen his son, Chiang Ching-kuo, died in 1988, he was entombed in a separate mausoleum in nearby Touliao. The hope was to have both of them buried at their birthplace in Fenghua when that would be possible. In 2004, Chiang Fang-liang, the widow of Chiang Ching-kuo, asked for both father and son to be buried at Wuzhi Mountain Military Cemetery in Xizhi, Taipei County (now New Taipei City). Chiang's ultimate funeral ceremony became a political battle between the wishes of the state and those of his family.\nChiang was succeeded as president by Vice President Yen Chia-kan and as Kuomintang party ruler by his son Chiang Ching-kuo, who retired Chiang Kai-shek's title of Director-General and instead assumed the position of chairman. Yen's presidency was interim; Chiang Ching-kuo, who was the Premier, became president after the end of Yen's term three years later.\nCult of personality.\nChiang's portrait hung over Tiananmen Square until 1949, when it was replaced with Mao's portrait. Portraits of Chiang were common in private homes and in public on the streets. After his death, the Chiang Kai-shek Memorial Song was written in 1988 to commemorate Chiang Kai-shek. In Cihu, there are several statues of Chiang Kai-shek.\nChiang was popular among many people and dressed in plain, simple clothes, unlike contemporary Chinese warlords who dressed extravagantly.\nQuotes from the Quran and hadith were used by Muslims in the Kuomintang-controlled Muslim publication, the \"Yuehua\", to justify Chiang Kai-shek's rule over China. When the Muslim general and warlord Ma Lin was interviewed, he was described as having \"high admiration for and unwavering loyalty to Chiang Kai-shek\".\nPhilosophy.\nThe Kuomintang used traditional Chinese religious ceremonies, and promulgated martyrdom. Kuomintang ideology subserved and promulgated the view that the souls of Party martyrs who died fighting for the Kuomintang, the revolution, and the party founder Sun Yat-sen were sent to heaven. Chiang Kai-shek believed that these martyrs witnessed events on Earth from heaven after their deaths.\nUnlike Sun's original Tridemist ideology that was heavily influenced by Western enlightenment theorists such as Henry George, Abraham Lincoln, Bertrand Russell, and John Stuart Mill, the traditional Chinese Confucian influence on Chiang's ideology is much stronger. Chiang rejected the Western progressive ideologies of individualism, liberalism, and the cultural aspects of Marxism. Therefore, Chiang is generally more culturally and socially conservative than Sun Yat-sen. Jay Taylor has described Chiang Kai-shek as a revolutionary nationalist and a \"left-leaning Confucian-Jacobinist\".\nWhen the Northern Expedition was complete, Kuomintang Generals led by Chiang Kai-shek paid tribute to Sun's soul in heaven with a sacrificial ceremony at the Xiangshan Temple in Beijing in July 1928. Among the Kuomintang Generals present were the Muslim Generals Bai Chongxi and Ma Fuxiang.\nChiang Kai-shek considered both Han Chinese and all ethnic minorities of China, the Five Races Under One Union, as descendants of the Yellow Emperor, the mythical founder of the Chinese nation, and belonging to the Chinese Nation Zhonghua Minzu. He introduced this into Kuomintang ideology which was propagated into the educational system of the Republic of China.\nChiang, as a Chinese nationalist and a Confucian, was against the iconoclasm of the May Fourth Movement. Motivated by his sense of nationalism, he viewed some Western ideas as foreign and believed that the great introduction of Western ideas and literature, which the May Fourth Movement promoted, was not beneficial to China. He and Sun criticized the May Fourth intellectuals as corrupting the morals of China's youth.\nChiang Kai-shek once said:\nContemporary perception.\nChiang's legacy has been subjected to heated debates. For some, Chiang was a national hero who led the victorious Northern Expedition against the Beiyang warlords in 1927 and helped achieve Chinese unification. His initial image as the leader of China against Japan's invasion, both before and after the attack on Pearl Harbor, led him to be featured on the cover of \"Time\" magazine ten times. Even though China received little American aid compared to Britain and the Soviet Union, it did not fold, as Chiang called on his countrymen to fight to the \"bitter end\" until their ultimate victory against Japan in 1945.\nSome also see him as a champion of anti-communism, being a key figure during the formative years of the World Anti-Communist League. During the subsequent Cold War, he was seen as the leader who led Free China and the bulwark against a possible communist invasion. However, historian Rudolph Rummel documented that the Nationalist government under Chiang led to millions of excess deaths from calamities such as its persecution against actual or perceived communists and its conscription of soldiers, confiscation of food, and flooding of downstream regions of the Yellow River during the Second Sino-Japanese War. His government was also accused of being corrupt and allying with known criminals such as Du Yuesheng for political and financial gains, and his critics often accuse him of fascism. In Taiwan, he ruled throughout a period of martial law. Some opponents charge that Chiang's efforts in developing the island were mostly to turn it into a strong base from which to recover mainland China and that he had little regard for the Taiwanese people.\nUnlike Chiang's son Chiang Ching-kuo, who is respected across the political spectrum, Chiang Kai-shek's image is perceived rather negatively in Taiwan. He was rated the lowest in two opinion polls about the perception of former presidents. His popularity in Taiwan is divided along political lines, enjoying better support in the Kuomintang (KMT) while being widely unpopular among Democratic Progressive Party (DPP) voters and those who blame him for the thousands killed during the February 28 Incident and criticise his dictatorial rule.\nIn contrast, his image has partially improved in mainland China. He had been portrayed as a villain and a \"bourgeoisie reactionary lackey\" who fought against the \"liberation\" of China by the communists, but since the 2000s, the media and popular culture have depicted him in a less negative manner. For example, many praised the 2009 movie sponsored by the Chinese Communist Party, \"The Founding of a Republic\", for moving away from casting Chiang as 'evil' versus Mao and emphasizing instead that the contingencies of war led the communists to victory. In the context of the Second Sino-Japanese War, aspects of Chiang's trip to India, or meeting with Roosevelt and Churchill in Cairo can be viewed positively. The shift also takes into account Chiang's commitment to a unified China and his stance against Taiwanese separatism. Chiang's ancestral home in Fenghua, Zhejiang, has become a museum and tourist attraction. Historian Rana Mitter notes that the displays inside were very positive about Chiang's role during the Second Sino-Japanese War.\nMitter further observed that, ironically, today's China is closer to Chiang's vision than to Mao's and wrote, \"One can imagine Chiang Kai-shek's ghost wandering round China today nodding in approval, while Mao's ghost follows behind him, moaning at the destruction of his vision\". Liang Shuming opined that Chiang Kai-shek's \"greatest contribution was to make the CCP successful. If he had been a bit more trustworthy, if his character was somewhat better, the CCP would have been unable to beat him\". Some Chinese historians argue that the main determinants for Chiang's defeat were not corruption or the lack of US support, but his decision to start the civil war with 70% of government expenditures in the military, his overestimation of the Nationalist forces equipped with US arms, and the loss of popularity and morales of his soldiers. Other historians argue that his failure was largely caused by external factors outside of Chiang's control. They include the refusal of the Truman administration to support Chiang by withdrawing aid, the foisting of an arms embargo by George C. Marshall, the failed pursuit of a d\u00e9tente between the nationalists and the communists, the American push for a coalition government with the CCP, and the USSR's consistent aid and support for the CCP during the civil war.\nIn the United States and Europe, Chiang was often perceived negatively as the one who lost China to the communists. His persistent demands for United States support and funding also prompted jokes from American officials that Chiang's name was actually General \"Cash-My-Check\". He has also been criticized for his poor military skills, such as issuing unrealistic orders and persistently attempting to fight unwinnable battles, leading to the loss of his best troops. In recent years, Chiang's image has been somewhat rehabilitated, and he has been increasingly perceived as a man overwhelmed by the events in China, having to fight the communists, Japanese, and provincial warlords simultaneously while trying to reconstruct and unify the country. His sincere, albeit often unsuccessful attempts to build a more powerful and modern nation have been noted by scholars such as Jonathan Fenby, Rana Mitter, and biographer Jay Taylor.\nFamily.\nWives.\nIn 1901, in an arranged marriage at age 14, Chiang was married to Mao Fumei, an illiterate villager five years his senior. While married to Mao, Chiang adopted two concubines (concubinage was still a common practice for well-to-do, non-Christian males in China): he took Yao Yecheng (, 1887\u20131966) as concubine in late 1912 and married Chen Jieru (1906\u20131971) in December 1921. While he was still living in Shanghai, Chiang and Yao adopted a son, Wei-kuo. Chen adopted a daughter in 1924, named Yaoguang, who later adopted her mother's surname. Chen's autobiography refuted the idea that she was a concubine. Chen claiming that, by the time she married Chiang, he had already divorced Yao, and that Chen was therefore his wife. Chiang and Mao had a son, Ching-kuo.\nAccording to the memoirs of Chen Jieru, Chiang's second wife, she contracted gonorrhea from Chiang soon after their marriage. He told her that he acquired this disease after separating from his first wife and living with his concubine Yao Yecheng, as well as with many other women he consorted with. His doctor explained to her that Chiang had sex with her before completing his treatment for the disease. As a result, both Chiang and Chen Jieru believed that they had become sterile; however, a purported miscarriage by Soong Mei-ling in August 1928 would, if it actually occurred, cast serious doubt on whether this was true.\nFamily tree.\nThe Xikou Chiangs were descended from Chiang Shih-chieh, who during the 1600s moved there from Fenghua district, and whose ancestors in turn came to southeastern China's Zhejiang (Chekiang) province after moving out of Northern China in the 13th century CE. The 12th century BCE Duke of Zhou's (Duke of Chou) third son was the ancestors of the Chiangs.\nHis great-grandfather was Chiang Qi-zeng, his grandfather was Chiang Si-qian, his uncle was Chiang Zhao-hai, and his father was Chiang Zhao-cong.\nReligion and relationships with religious communities.\nChiang personally dealt extensively with religions, power figures, and factions in China during his regime.\nReligious views.\nChiang Kai-shek was born and raised as a Buddhist, but became a Methodist upon his marriage to his fourth wife, Soong Mei-ling. It was previously believed that this was a political move, but further studies of his personal diaries suggest that his faith was sincere.\nRelationship with Muslims.\nChiang developed relationships with other generals. Chiang became a sworn brother of the Chinese Muslim general Ma Fuxiang and appointed him to high ranking positions. Chiang addressed Ma Fuxiang's son Ma Hongkui as Shao Yun Shixiong Ma Fuxiang attended national leadership conferences with Chiang during battles against Japan. Ma Hongkui was eventually scapegoated for the failure of the Ningxia Campaign against the Communists, so he moved to the US instead of remaining in Taiwan with Chiang.\nWhen Chiang became President of China after the Northern Expedition, he carved out Ningxia and Qinghai out of Gansu province, and appointed Muslim generals as military governors of all three provinces: Ma Hongkui, Ma Hongbin, and Ma Qi. The three Muslim governors, known as Xibei San Ma (lit. \"the three Mas of the Northwest\"), controlled armies composed entirely of Muslims. Chiang called on the three and their subordinates to wage war against the Soviet peoples, Tibetans, Communists, and the Japanese. Chiang continued to appoint Muslims as governors of the three provinces, including Ma Lin and Ma Fushou. Chiang's appointments, the first time that Muslims had been appointed as governors of Gansu, increased the prestige of Muslim officials in northwestern China. The armies raised by this \"Ma Clique\", most notably their Muslim cavalry, were incorporated into the KMT army. Chiang appointed Hui general Bai Chongxi as the Minister of National Defence of the Republic of China, which controlled the ROC military.\nChiang also supported the Muslim General Ma Zhongying, whom he had trained at Whampoa Military Academy during the Kumul Rebellion, in a jihad against Jin Shuren, Sheng Shicai, and the Soviet Union during the Soviet Invasion of Xinjiang. Chiang designated Ma's Muslim army as the 36th Division (National Revolutionary Army) and gave his troops KMT flags and uniforms. Chiang then supported Muslim General Ma Hushan against Sheng and the Soviet Union in the Xinjiang War (1937). All Muslim generals commissioned by Chiang in the National Revolutionary Army swore allegiance to him. Several, like Ma Shaowu and Ma Hushan were loyal to Chiang and Kuomintang hardliners.\nThe Ili Rebellion and Pei-ta-shan Incident plagued relations with the Soviet Union during Chiang's rule and caused trouble with the Uyghurs. During the Ili Rebellion and Peitashan incident, Chiang deployed Hui troops against Uyghur mobs in Turfan, and against Soviet Russian and Mongols at Peitashan.\nDuring Chiang's rule, attacks on foreigners and ethnic minorities by the allied warlords of the Nationalist government such as the Ma Clique flared up in several incidents. One of these was the Battle of Kashgar where a Muslim army loyal to the Kuomintang massacred 4,500 Uyghurs, and killed several Britons at the British consulate in Kashgar.\nHu Songshan, a Muslim Imam, backed Chiang Kai-shek's regime and gave prayers for his government. ROC flags were saluted by Muslims in Ningxia during prayer along with exhortations to nationalism during Chiang's rule. Chiang sent Muslim students abroad to study at places like Al-Azhar University and Muslim schools throughout China that taught loyalty to his regime.\nThe Yuehua, a Chinese Muslim publication, quoted the Quran and hadith to justify submitting to Chiang Kai-shek as the leader of China, and as justification for Jihad in the war against Japan.\nThe Yihewani (Ikhwan al Muslimun a.k.a. Muslim brotherhood) was the predominant Muslim sect backed by the Chiang government during Chiang's regime. Other Muslim sects, like the Xidaotang and Sufi brotherhoods like Jahriyya and Khuffiya were also supported by his regime. The Chinese Muslim Association, a pro-Kuomintang and anti-Communist organization, was set up by Muslims working in his regime. Salafists attempted to gain a foothold in China during his regime, but the Yihewani and Hanafi Sunni Gedimu denounced the Salafis as radicals, engaged in fights against them, and declared them heretics, forcing the Salafis to form a separate sect. Ma Ching-chiang, a Muslim General, served as an advisor to Chiang Kai-shek. Ma Buqing was another Muslim General who fled to Taiwan along with Chiang. His government donated money to build the Taipei Grand Mosque on Taiwan.\nRelationship with Buddhists and Christians.\nChiang had uneasy relations with the Tibetans. He fought against them in the Sino-Tibetan War, and he supported the Muslim General Ma Bufang in his war against Tibetan rebels in Qinghai. Chiang ordered Ma Bufang to prepare his Islamic army to invade Tibet several times, to deter Tibetan independence, and threatened the Tibetans with aerial bombardment. Ma Bufang attacked the Tibetan Buddhist Tsang monastery in 1941. After the war, Chiang appointed Ma Bufang as ambassador to Saudi Arabia.\nChiang incorporated Methodist values into the New Life Movement under the influence of his wife. Dancing and Western music were discouraged. In one incident, several youths splashed acid on people wearing Western clothing, although Chiang was not directly responsible for these incidents. Despite being a Methodist, he made reference to the Buddha in his diary, and encouraged the establishment of a Buddhist political party under Master Taixu.\nAccording to Jehovah's Witnesses' magazine \"The Watchtower\", some of their members travelled to Chongqing and spoke to him personally while distributing their literature there during World War II."}
{"id": "6863", "revid": "48812404", "url": "https://en.wikipedia.org/wiki?curid=6863", "title": "Compression ratio", "text": "The compression ratio is the ratio between the maximum and minimum volume during the compression stage of the power cycle in a piston or Wankel engine. \nA fundamental specification for such engines, it can be measured in two different ways. The simpler way is the static compression ratio:\nin a reciprocating engine, this is the ratio of the volume of the cylinder when the piston is at the bottom of its stroke to that volume when the piston is at the top of its stroke. The dynamic compression ratio is a more advanced calculation which also takes into account gases entering and exiting the cylinder during the compression phase.\nEffect and typical ratios.\nA high compression ratio is desirable because it allows an engine to extract more mechanical energy from a given mass of air\u2013fuel mixture due to its higher thermal efficiency. This occurs because internal combustion engines are heat engines, and higher compression ratios permit the same combustion temperature to be reached with less fuel, while giving a longer expansion cycle, creating more mechanical power output and lowering the exhaust temperature.\nPetrol engines.\nIn petrol (gasoline) engines used in passenger cars for the past 20 years, compression ratios have typically been between 8:1 and 12:1. Several production engines have used higher compression ratios, including:\nWhen forced induction (e.g. a turbocharger or supercharger) is used, the compression ratio is often lower than naturally aspirated engines. This is due to the turbocharger or supercharger already having compressed the air before it enters the cylinders. Engines using port fuel-injection typically run lower boost pressures and/or compression ratios than direct injected engines because port fuel injection causes the air\u2013fuel mixture to be heated together, leading to detonation. Conversely, directly injected engines can run higher boost because heated air will not detonate without a fuel being present.\nHigher compression ratios can make gasoline (petrol) engines subject to engine knocking (also known as \"detonation\", \"pre-ignition\", or \"pinging\") if lower octane-rated fuel is used. This can reduce efficiency or damage the engine if knock sensors are not present to modify the ignition timing.\nDiesel engines.\nDiesel engines use higher compression ratios than petrol engines, because the lack of a spark plug means that the compression ratio must increase the temperature of the air in the cylinder sufficiently to ignite the diesel using compression ignition. Compression ratios are often between 14:1 and 23:1 for direct injection diesel engines, and between 18:1 and 23:1 for indirect injection diesel engines.\nAt the lower end of 14:1, NOx emissions are reduced at a cost of more difficult cold-start. Mazda's Skyactiv-D, the first such commercial engine from 2013, used adaptive fuel injectors among other techniques to ease cold start.\nOther fuels.\nThe compression ratio may be higher in engines running exclusively on liquefied petroleum gas (LPG or \"propane autogas\") or compressed natural gas, due to the higher octane rating of these fuels.\nKerosene engines typically use a compression ratio of 6.5 or lower. The petrol-paraffin engine version of the Ferguson TE20 tractor had a compression ratio of 4.5:1 for operation on tractor vaporising oil with an octane rating between 55 and 70.\nMotorsport engines.\nMotorsport engines often run on high-octane petrol and can therefore use higher compression ratios. For example, motorcycle racing engines can use compression ratios as high as 14.7:1, and it is common to find motorcycles with compression ratios above 12.0:1 designed for 95 or higher octane fuel.\nEthanol and methanol can take significantly higher compression ratios than gasoline. Racing engines burning methanol and ethanol fuel often have a compression ratio of 14:1 to 16:1.\nMathematical formula.\nIn a reciprocating engine, the static compression ratio (formula_1) is the ratio between the volume of the cylinder and combustion chamber when the piston is at the bottom of its stroke, and the volume of the combustion chamber when the piston is at the top of its stroke. It is therefore calculated by the formula\nformula_2\nwhere\nformula_3 can be estimated by the cylinder volume formula:\nformula_6\nwhere\nBecause of the complex shape of formula_4 it is usually measured directly. This is often done by filling the cylinder with liquid and then measuring the volume of the used liquid.\nVariable compression ratio engines.\nMost engines use a fixed compression ratio, however a variable compression ratio engine is able to adjust the compression ratio while the engine is in operation. The first production engine with a variable compression ratio was introduced in 2019.\nVariable compression ratio is a technology to adjust the compression ratio of an internal combustion engine while the engine is in operation. This is done to increase fuel efficiency while under varying loads. Variable compression engines allow the volume above the piston at top dead centre to be changed.\nHigher loads require lower ratios to increase power, while lower loads need higher ratios to increase efficiency, i.e. to lower fuel consumption. For automotive use this needs to be done as the engine is running in response to the load and driving demands.\nThe 2019 Infiniti QX50 is the first commercially available car that uses a variable compression ratio engine.\nDynamic compression ratio.\nThe \"static compression ratio\" discussed above \u2014 calculated solely based on the cylinder and combustion chamber volumes \u2014 does not take into account any gases entering or exiting the cylinder during the compression phase. In most automotive engines, the intake valve closure (which seals the cylinder) takes place during the compression phase (i.e. after bottom dead centre, BDC), which can cause some of the gases to be pushed back out through the intake valve. On the other hand, intake port tuning and scavenging can cause a greater amount of gas to be trapped in the cylinder than the static volume would suggest. The \"dynamic compression ratio\" accounts for these factors.\nThe dynamic compression ratio is higher with more conservative intake camshaft timing (i.e. soon after BDC), and lower with more radical intake camshaft timing (i.e. later after BDC). Regardless, the dynamic compression ratio is always lower than the static compression ratio.\nAbsolute cylinder pressure is used to calculate the dynamic compression ratio, using the following formula:\nformula_10\nwhere formula_11 is a polytropic value for the ratio of specific heats for the combustion gases at the temperatures present (this compensates for the temperature rise caused by compression, as well as heat lost to the cylinder)\nUnder ideal (adiabatic) conditions, the ratio of specific heats would be 1.4, but a lower value, generally between 1.2 and 1.3 is used, since the amount of heat lost will vary among engines based on design, size and materials used. For example, if the static compression ratio is 10:1, and the dynamic compression ratio is 7.5:1, a useful value for cylinder pressure would be 7.51.3 \u00d7 atmospheric pressure, or 13.7\u00a0bar (relative to atmospheric pressure).\nThe two corrections for dynamic compression ratio affect cylinder pressure in opposite directions, but not in equal strength. An engine with high static compression ratio and late intake valve closure will have a dynamic compression ratio similar to an engine with lower compression but earlier intake valve closure."}
{"id": "6864", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=6864", "title": "Chromosome walking", "text": ""}
{"id": "6865", "revid": "38627444", "url": "https://en.wikipedia.org/wiki?curid=6865", "title": "Concordat of Worms", "text": "The Concordat of Worms (; ), also referred to as the Pactum Callixtinum or Pactum Calixtinum, was an agreement between the Catholic Church and the Holy Roman Empire which regulated the procedure for the appointment of bishops and abbots in the Empire. Signed on 23 September 1122 in the German city of Worms by Pope Callixtus II and Emperor Henry V, the agreement set an end to the Investiture Controversy, a conflict between state and church over the right to appoint religious office holders that had begun in the middle of the 11th century.\nBy signing the concordat, Henry renounced his right to invest bishops and abbots with ring and crosier, and opened ecclesiastical appointments in his realm to canonical elections. Callixtus, in turn, agreed to the presence of the emperor or his officials at the elections and granted the emperor the right to intervene in the case of disputed outcomes. The emperor was also allowed to perform a separate ceremony in which he would invest bishops and abbots with a sceptre, representing the lands that constituted the temporalities associated with their episcopal see.\nBackground.\nDuring the middle of the 11th century, a reformist movement within the Christian Church sought to reassert the rights of the Holy See at the expense of the European monarchs. Having been elected in 1073, the reformist Pope Gregory VII proclaimed several edicts aimed at strengthening the authority of the papacy, some of which were formulated in the \"Dictatus papae\" of 1075. Gregory's edicts postulated that secular rulers were answerable to the pope and forbade them to make appointments to clerical offices (a process known as investiture).\nThe pope's doctrines were vehemently rejected by Henry IV, the Holy Roman Emperor, who habitually invested the bishops and abbots of his realm. The ensuing conflict between the Empire and the papacy is known as the Investiture Controversy. The dispute continued after the death of Gregory VII in 1084 and the abdication of Henry IV in 1105.\nEven though Henry's son and successor, the Emperor Henry V, looked towards reconciliation with the reformist movement, no lasting compromise was achieved in the first 16 years of his reign. In 1111, Henry V brokered an agreement with Pope Paschal II at Sutri, whereby he would abstain from investing clergy in his realm in exchange for the restoration of church property that had originally belonged to the Empire. The Sutri agreement, Henry hoped, would convince Paschal to assent to Henry's official coronation as emperor.\nThe agreement failed to be implemented, leading Henry to imprison the pope. After two months of captivity, Paschal vowed to grant the coronation and to accept the emperor's role in investiture ceremonies. He also agreed never to excommunicate Henry. Given that these concessions had been won by force, ecclesiastical opposition to the Empire continued. The following year, Paschal reneged on his promises.\nMouzon summit.\nIn January 1118, Pope Paschal died. He was succeeded by Gelasius II, who died in January 1119. His successor, the Burgundian Callixtus II, resumed negotiations with the Emperor with the aim of settling the dispute between the church and the Empire. In the autumn of 1119, two papal emissaries, William of Champeaux and Pons of Cluny, met Henry at Strasbourg, where the emperor agreed in principle to abandon the secular investiture ceremony that involved giving new bishops and abbots a ring and a crosier.\nThe two parties scheduled a final summit between Henry and Callixtus at Mouzon, but the meeting ended abruptly after the emperor refused to accept a short-notice change in Callixtus's demands. The church leaders, who were deliberating their position at a council in Reims, reacted by excommunicating Henry. However, they did not endorse the pope's insistence upon the complete abandonment of secular investiture. The negotiations ended in failure.\nHistorians disagree as to whether Calixtus actually wanted peace or fundamentally mistrusted Henry. Due to his uncompromising position in 1111, Calixtus has been termed an \"ultra\", and his election to the papacy may indicate that the College of Cardinals saw no reason to show weakness to the emperor. This optimism about victory was founded on the very visible, and very vocal opposition to Henry from within his own nobility, and the cardinals may have seen the emperor's internal weaknesses as an opportunity for outright victory.\nFurther negotiations.\nAfter the failure of the Mouzon negotiations, and the disappearance into the horizon of the chances of Henry's unconditional surrender, the majority of the clergy became willing to compromise in order to settle the dispute. The polemic writings and pronouncements that had figured so highly during the Investiture Dispute had died down by this point. Historian Gerd Tellenbach argues that, despite appearances, these years were \"no longer marked by an atmosphere of bitter conflict\".\nThis was in part the result of the papacy's realization that it could not win two different disputes on two separate fronts, as it had been trying to do. Calixtus had been personally involved in negotiations with the Emperor over the last decade, and his intimate knowledge of the delicate situation made him the perfect candidate for the attempt. The difference between 1119 and 1122, argues Stroll, was not Henry, who had been willing to make concessions in 1119, but Calixtus, who had then been intransigent, but who now was intent upon reaching an agreement\".\nThe same sentiment prevailed in much of the German nobility. In 1121, pressured by a faction of nobles from the Lower Rhine and Duchy of Saxony under the leadership of the archbishop Adalbert of Mainz, Henry agreed to submit to make peace with the pope. In response in February 1122, Calixtus wrote to Henry in a conciliatory tone via the Bishop of Acqui. His letter has been described as \"a carefully crafted overture\".\nIn his letter, Calixtus drew attention to their blood relationship, suggesting that while their shared ancestry compelled them to love each other as brothers, it was fundamental that the German kings draw their authority from God, but via his servants, not directly. However, Calixtus also emphasised for the first time that he blamed not Henry personally for the dispute but his bad advisors who had dictated unsound policy to him. In a major shift in policy since the Council of Reims of 1119, the pope stated that the church gifts what it possesses to all its children, without making claims upon them. This was intended to reassure Henry that in the event of peace between them, his position and Empire were secure.\nShifting from the practical to the spiritual, Calixtus next asked Henry to bear in mind that he was a king, but like all men limited on his earthly capability; he had armies, and kings below him, but the church had Christ and the Apostles. Continuing his theme, he referred, indirectly, to Henry's excommunication by himself (twice), he begged Henry to allow the conditions for peace to be created, as a result of which the church's, and God's glory would be increased, as concomitantly would the Emperor's. Conversely, he made sure to include a threat: if Henry did not change his ways, Calixtus threatened to place \"the protection of the church in the hands of wise men\".\nHistorian Mary Stroll argues that, in taking this approach, Calixtus was taking advantage of the fact that, while he himself \"was hardly in a position to sabre rattle\" due to his military defeat in the south and his difficulty with his own Cardinals, Henry was also under pressure in Germany in both the military and spiritual spheres.\nThe Emperor replied through the Bishop of Speyer and the Abbot of Fulda, who travelled to Rome and collected the pope's emissaries under the Cardinal Bishop of Ostia. Speyer was a representative of Henry's political opponents in Germany, whereas Fulda was a negotiator rather than politically partisan. Complicating matters was a disputed election to the bishopric of Wurzburg in February 1122 of the kind that was at the heart of the Investiture Dispute. Although this almost led to an outbreak of civil war, a truce was arranged in August, allowing the parties to return to the papal negotiations.\nIn the summer of 1122, a synod was convened in Mainz, at which imperial emissaries concluded the terms of their agreement with representatives of the church. In a sign that the Pope intended the impending negotiations to be successful, a Lateran council was announced for the following year.\nWorms.\nThe Emperor received the papal legates in Worms with due ceremony, where he awaited the outcome of the negotiations which appear to have actually taken place in nearby Mainz, which was hostile territory to Henry. As such, he had to communicate via messenger to keep up with events. Abbot Ekkehard of Aura chronicles that discussions took over a week to conclude. On 8 September, he met the papal legates and their final agreements were codified for publication.\nAlthough a possible compromise solution had already been received from England, this does not seem to have ever been considered in depth, probably on account of it containing an oath of Homage between Emperor and Pope, which had been a historical sticking point in earlier negotiations. The papal delegation was led by Cardinal bishop Lamberto Scannabecchi of Ostia, the future Pope Honorius II.\nBoth sides studied previous negotiations between them, including those from 1111, which were considered to have created precedent. On 23 September 1122, papal and imperial delegates signed a series of documents outside the walls of Worms. There was insufficient room in the city for the number of attendees and watchers. Adalbert, Archbishop of Mainz wrote to Calixtus of how complex the negotiations had been, given that, as he said, Henry regarded the powers he was being asked to renounce as being hereditary in the Imperial throne. It is probable that what was eventually promulgated was the result of almost every word being carefully considered. The main difference between what was to be agreed at Worms and previous negotiations were the concessions from the pope.\nConcordat.\nThe agreements come to at Worms were in the nature of both concessions and assurances to the other party. Henry, on oath before God, the apostles and the church renounced his right to invest bishops and abbots with ring and crosier, and opened ecclesiastical appointments in his realm to canonical elections, \"regno vel imperio\". He also recognised the traditional extent and boundaries of the papal patrimony as a legal entity rather than one malleable to the emperor. Henry promised to return to the church those lands rightfully belonging to the church seized by himself or his father to the church; furthermore, he would assist the pope in regaining those that were taken by others, and \"he will do the same thing for all other churches and princes, both ecclesiastical and lay\". If the pope requested Imperial assistance, he would receive it, and if the church came to the empire for justice, it would be treated fairly. He also swore to abstain from \"all investiture by ring and staff\", marking the end of an ancient imperial tradition.\nCallixtus made similar reciprocal promises regarding the empire in Italy. He agreed to the presence of the emperor or his officials at the elections and granted the emperor the right to ajudge in the case of disputed outcomes on episcopal advice\u2014as long as they had been held peacefully and without simony\u2014which had officially been the case ever since precedent had been set by the London Accord of 1107. This right to judge was constrained by an assurance that he would support the majority vote among electors, and further that he would take the advice of his other bishops before doing so. The emperor was also allowed to perform a separate ceremony in which he would invest bishops and abbots with their \"regalia\", a sceptre representing the imperial lands associated with their episcopal see. This clause also contained a \"cryptic\" condition that once the elect had been so endowed, the new bishop \"should do what he ought to do according to imperial rights\". In the German imperial lands this was to take place prior to the bishop-elect's consecration; elsewhere in the empire\u2014Burgundy and Italy, exempting the Papal States\u2014within six months of the ceremony. The differentiating between the German portion of the Empire and the rest was of particular importance to Calixtus as the papacy had traditionally felt threatened more from it in the peninsular than the broader Empire. Finally, the pope granted \"true peace\" on the emperor and all those who had supported him. Calixtus had effectively overturned wholesale the strategy he had pursued during the Mouzon negotiation; episcopal investitures in Germany were to take place with very little substantive change in ceremony, while temporal involvement remained, only replacing investiture with homage, although the word itself\u2014\"hominium\"\u2014was studiously avoided. Adalbert, from whom Calixtus first received news of the final concordat, emphasized that it still had to be approved in Rome; this suggests, argues Stroll, that the Archbishop\u2014and probably the papal legation as a whole\u2014were against making concessions to the emperor, and probably wanted Calixtus to disown the agreement. Adalbert believed the agreement would make it easier for the Emperor to legalise intimidation of episcopal electors, writing that \"through the opportunity of [the emperor's] presence, the Church of God must undergo the same slavery as before, or an even more oppressive one\".\nHowever, argues Stroll, the concessions Calixtus made were an \"excellent bargain\" in return for eradicating the danger on the papacy's northern border and therefore allowing him to focus, without threat or distraction, on the Normans to the south. It had achieved its peace, argues Norman Cantor, by allowing local national custom and practice to determine future relations between crown and pope; in most cases, he notes, this \"favored the continuance of royal control over the church\".\nThe concordat was published as two distinct charters, each laying out the concessions the one party was making to the other. They are known respectively as the Papal (or the \"Calixtinum\") and the Imperial (\"Henricianum\") charters. Calixtus's is addressed to the emperor\u2014in quite personal terms\u2014while Henry's is made out to God. The bishop of Ostia gave the emperor the kiss of peace on behalf of the pope and said Mass. By these rites was Henry returned to the church, the negotiators were lauded for succeeding in their delicate mission and the concordat was called \"peace at the will of the pope\". Neither charter was signed; both contained probably intentional vagaries and unanswered questions\u2014such as the position of the papacy's churches that lay outside both the patrimony and Germany\u2014which were subsequently addressed on a case-by-case basis. Indeed, Robert Benson has suggested that the brevity of the charters was deliberate and that the agreement as a whole is as important for what it omits as for what it includes. The term \"regalia\", for example, was not only undefined but literally meant two different things to each party. In the \"Henricianum\" it referred to the feudal duty owed to a monarch; in the Calixtinium, it was the episcopal temporalities. Broader question, such as the nature of the church and Empire relationship, were also not addressed, although some ambiguity was removed by an 1133 Papal privilege.\nThe Concordat was widely, and deliberately, publicised around Europe. Calixtus was not in Rome when the concordat was delivered. He had left the city by late August and was not to return until mid- to late October, making a progress to Anagni, taking the bishopric of Anagni and Casamari Abbey under his protection.\nPreservation.\nThe concordat was ratified at the First Council of the Lateran and the original \"Henricianum\" charter is preserved at the Vatican Apostolic Archive; the \"Calixtinum\" has not survived except in subsequent copies. A copy of the former is also held in the \"Codex Udalrici\", but this is an abridged version for political circulation, as it reduces the number of imperial concessions made. Indicating the extent that he saw the agreement as a papal victory, Calixtus had a copy of the \"Henricianum\" painted on a Lateran Palace chamber wall; while nominally portraying the concordat as a victory for the papacy, it also ignored the numerous concessions made to the emperor. This was part of what Hartmut Hoffmann has called \"a conspiracy of silence\" regarding papal concessions. Indeed, while the Pope is pictured enthroned, and Henry only standing, the suggestion is still that they were jointly wielding their respective authority to come to this agreement. An English copy of the \"Calixtinum\" made by William of Malmsbury is reasonably accurate but omits the clause mentioning the use of a sceptre in the granting of the \"regalia\". He then, having condemned Henry's \"Teuton fury\", proceeds to praise him, comparing him favourably to Charlemagne for his devotion to God and the peace of Christendom.\nAftermath.\nThe first invocation of the concordat was not in the empire, as it turned out, but by Henry I of England the following year. Following a long-running dispute between Canterbury\u2013York which ended up in the Papal court, Joseph Huffman argues that it would have been controversial for the Pope \"to justify one set of concessions in Germany and another in England\". The concordat ended once and for all the \"Imperial church system of the Ottonians and Salians\". The First Lateran Council was convoked to confirm the Concordat of Worms. The council was most representative with nearly 300 bishops and 600 abbots from every part of Catholic Europe being present. It convened on March 18, 1123. One of its primary concerns was to emphasise the independence of diocesan clergy, and to do so it forbade monks to leave their monasteries to provide pastoral care, which would in future be the sole preserve of the diocese. In ratifying the Concordat, the Council confirmed that in future bishops would be elected by their clergy, although, also per the Concordat, the Emperor could refuse the homage of German bishops.\nDecrees were passed directed against simony, concubinage among the clergy, church robbers, and forgers of Church documents; the council also reaffirmed indulgences for Crusaders. These, argues C. Colt Anderson \"established important precedents in canon law restricting the influence of the laity and the monks\". While this led to a busy period of reform, it was important for those advocating reform not to allow themselves to be confused with the myriad heretical sects and schismatics who were making similar criticisms.\nThe Concordat was the last major achievement for Emperor Henry, as he died in 1125; an attempted invasion of France came to nothing in 1124 in the face of \"determined opposition\". Fuhrmann comments that, as Henry had shown in his life \"even less interest in new currents of thought and feeling than his father\", he probably did not understand the significance of the events he had lived through. The peace only lasted until his death; when Imperial Electors met to choose his successor, reformists took the opportunity to attack the imperial gains of Worms on the grounds that they had been granted to him personally rather than Emperors generally. However, later emperors, such as Frederick I and Henry VI, continued to wield as much, if intangible, power as their predecessors in episcopal elections, and to a greater degree to that allowed them by Calixtus' charter. Successive emperors found the Concordat sufficiently favourable that it remained, almost unaltered until the empire was dissolved by Francis II in 1806 on account of Napoleon. Popes, likewise, were able to use the powers codified to them in the Concordatto their advantage in future internal disputes with their Cardinals.\nReception.\nThe most detailed contemporary description of the Concordat comes to historians through a brief chronicle known as the 1125 continuation chronicle. This pro-papal document lays the blame for the schism squarely upon Henry\u2014by his recognition of Gregory VIII\u2014and the praise for ending it on Calixtus, through his making only temporary compromises. I. S. Robinson, writing in The New Cambridge Medieval History, suggests that this was a deliberate ploy to leave further negotiations open with a more politically malleable Emperor in future. To others it was not so clear cut; Honorius of Autun, for example, writing later in the century discussed lay investiture as an aspect of papal-Imperial relations and, even a century later the \"Sachsenspiegel\" still stated that Emperors nominated bishops in Germany. Robinson suggests that, by the end of the 12th century, \"it was the imperial, rather than the papal version of the Concordat of Worms that was generally accepted by German churchmen\".\nThe contemporary English historian William of Malmesbury praised the Concordat for curtailing what he perceived as the emperor's overreach, or as he put it, \"severing the sprouting necks of Teuton fury with the axe of Apostolic power\". However, he regarded the final settlement not as a defeat of the Empire at the hands of the church, but rather as a reconciliatory effort by the two powers. Although polemicism had died down in the years preceding the Concordat, it did not finish them completely, and factionalism within the church especially continued. Gerhoh of Reichersberg believed that the emperor now had the right to request German bishops pay homage to him, something that would never have been allowed under Paschal, due to the vague clause instructing newly-elects to the things the emperor wished. Gerhoh argued that now imperial intervention in episcopal elections had been curtailed, Henry would use this clause to extend his influence in the church by means of homage. Gerhoh was torn between viewing the concordat as the end of a long struggle between pope and empire, or whether it marked the beginning of a new one within the church itself. Likewise Adelbert of Mainz\u2014who had casually criticised the agreement in his report to Calixtus\u2014continued to lobby against it, and continued to bring complaints against Henry, whom, for example, he alleged had illegally removed the Bishop of Strassburg who was suspected of complicity in the death of Duke Berthold of Zaehringen.\nThe reformist party within the church took a similar view, criticising the Concordat for failing to remove all secular influence on the church. For this reason, a group of followers of Paschal II unsuccessfully attempted to prevent the agreement's ratification at the Lateran Council, crying \"non placet!\" when asked to do so: \"it was only when it was pointed out that much had to be accepted for the sake of peace that the atmosphere quietened\". Calixtus told them that they had \"not to approve but tolerate\" it. At a council in Bamberg in 1122 Henry gathered those nobles who had not attended the Concordat to seek their approval of the agreement, which they did. The following month he sent cordial letters to Calixtus agreeing with the pope's position that as brothers in Christ they were bound by God to work together, etc., and that he would soon visit personally to discuss the repatriation of papal land. These letters were, in turn, responded to positively by Calixtus, who instructed his delegates to make good the promises they had made at Worms.\nHistoriography.\nGottfried Wilhelm Leibniz called the agreements made at Worms \"the oldest concordat in German history, an international treaty\", while Augustin Fliche argued that the Concordat effectively instituted the statutes of Ivo of Chartres, a prominent reformer in the early years of the Investiture Contest, a view, it has been suggested, that most historians agree with. The historian Uta-Renate Blumenthal writes that, despite its shortcomings, the Concordat freed \"[the church and the Empire] from antiquated concepts with their increasingly anachronistic restrictions\". According to the historian William Chester Jordan, the Concordat was \"of enormous significance\" because it demonstrated that the emperor, in spite of his great secular power, did not have any religious authority. On the other hand, argues Karl F. Morrison, any victory the papacy felt it had won was pyrrhic, as \"the king was left in possession of the field\". The new peace also now allowed the papacy to expand its territories in Italy, such as the Sabina, which were unobtainable while the dispute with Henry was ongoing, while in Germany, a new class of ecclesiastics was created, what Horst Fuhrmann calls the \"ecclesiastical princes of the Empire\".\nWhile most historians agree that the Concordat marks a clear close to the fifty-year-old struggle between church and empire, disagreement continues on just how decisive a termination that was. Historians are also unclear as to the commitment of the pope to the concordat. Stroll, for example, notes that, while Henry's oaths were made to the church corporate, so in perpetuity, while Calixtus's may have been in a personal capacity. This, Stroll argues, would mean that it could be argued that while Henry's commitments to the church applied forever, Calixtus's applied only for the duration of Henry's reign, and at least one contemporary, Otto of Freising, wrote later in the century that he believed this to be the church's position. Stroll considers it \"implausible\" that Henry and his counsel would ever have entered into such a one-sided agreement. Indeed, John O'Malley has argued that the emperor had effectively been granted a veto from Calixtus; while in the strictest interpretation of the Gregorian reformers the only two important things in the making of a bishop were his election and consecration, Calixtus had effectively codified a role\u2014however small\u2014for the emperor in this process. Conversely, Benson reckons that while Henry's agreement was with the church in perpetuity, Calixtus'\u2014based on the personal mode of address\u2014was with him personally, and as such not binding on his successors. However, this was also an acknowledgement, he suggests, that much of what the pope did not address was already considered customary, and so did not need addressing.\nThere has also been disagreement in why the Investiture contest ended with the Concordat as it did. Benson notes that, as a truce, it was primarily intended to stop the fighting rather than to address its original causes. It was \"a straightforward, political engagement...a pragmatic agreement\" between two political bodies. Indeed, controversy over investiture continued for at least another decade; in that light, suggests Benson, it could be argued that the Concordat did not end the dispute at all. There were \"many problems unsolved, and [it] left much room for the free play of power\". Political scientist Bruce Bueno de Mesquita has argued that, in the long term, the Concordat was an essential component to the later\u2014gradual\u2014creation of the European nation state."}
{"id": "6867", "revid": "1739907", "url": "https://en.wikipedia.org/wiki?curid=6867", "title": "Context-free language", "text": "In formal language theory, a context-free language (CFL), also called a Chomsky type-2 language, is a language generated by a context-free grammar (CFG).\nContext-free languages have many applications in programming languages, in particular, most arithmetic expressions are generated by context-free grammars.\nBackground.\nContext-free grammar.\nDifferent context-free grammars can generate the same context-free language. Intrinsic properties of the language can be distinguished from extrinsic properties of a particular grammar by comparing multiple grammars that describe the language.\nAutomata.\nThe set of all context-free languages is identical to the set of languages accepted by pushdown automata, which makes these languages amenable to parsing. Further, for a given CFG, there is a direct way to produce a pushdown automaton for the grammar (and thereby the corresponding language), though going the other way (producing a grammar given an automaton) is not as direct.\nExamples.\nAn example context-free language is formula_1, the language of all non-empty even-length strings, the entire first halves of which are 's, and the entire second halves of which are 's. is generated by the grammar formula_2.\nThis language is not regular.\nIt is accepted by the pushdown automaton formula_3 where formula_4 is defined as follows:\nUnambiguous CFLs are a proper subset of all CFLs: there are inherently ambiguous CFLs. An example of an inherently ambiguous CFL is the union of formula_6 with formula_7. This set is context-free, since the union of two context-free languages is always context-free. But there is no way to unambiguously parse strings in the (non-context-free) subset formula_8 which is the intersection of these two languages.\nDyck language.\nThe language of all properly matched parentheses is generated by the grammar formula_9.\nProperties.\nContext-free parsing.\nThe context-free nature of the language makes it simple to parse with a pushdown automaton.\nDetermining an instance of the membership problem; i.e. given a string formula_10, determine whether formula_11 where formula_12 is the language generated by a given grammar formula_13; is also known as \"recognition\". Context-free recognition for Chomsky normal form grammars was shown by Leslie G. Valiant to be reducible to Boolean matrix multiplication, thus inheriting its complexity upper bound of \"O\"(\"n\"2.3728596).\nConversely, Lillian Lee has shown \"O\"(\"n\"3\u2212\u03b5) Boolean matrix multiplication to be reducible to \"O\"(\"n\"3\u22123\u03b5) CFG parsing, thus establishing some kind of lower bound for the latter.\nPractical uses of context-free languages require also to produce a derivation tree that exhibits the structure that the grammar associates with the given string. The process of producing this tree is called \"parsing\". Known parsers have a time complexity that is cubic in the size of the string that is parsed.\nFormally, the set of all context-free languages is identical to the set of languages accepted by pushdown automata (PDA). Parser algorithms for context-free languages include the CYK algorithm and Earley's Algorithm.\nA special subclass of context-free languages are the deterministic context-free languages which are defined as the set of languages accepted by a deterministic pushdown automaton and can be parsed by a LR(k) parser.\nSee also parsing expression grammar as an alternative approach to grammar and parser.\nClosure properties.\nThe class of context-free languages is closed under the following operations. That is, if \"L\" and \"P\" are context-free languages, the following languages are context-free as well:\nNonclosure under intersection, complement, and difference.\nThe context-free languages are not closed under intersection. This can be seen by taking the languages formula_22 and formula_23, which are both context-free. Their intersection is formula_24, which can be shown to be non-context-free by the pumping lemma for context-free languages. As a consequence, context-free languages cannot be closed under complementation, as for any languages \"A\" and \"B\", their intersection can be expressed by union and complement: formula_25. In particular, context-free language cannot be closed under difference, since complement can be expressed by difference: formula_26.\nHowever, if \"L\" is a context-free language and \"D\" is a regular language then both their intersection formula_27 and their difference formula_28 are context-free languages.\nDecidability.\nIn formal language theory, questions about regular languages are usually decidable, but ones about context-free languages are often not. It is decidable whether such a language is finite, but not whether it contains every possible string, is regular, is unambiguous, or is equivalent to a language with a different grammar.\nThe following problems are undecidable for arbitrarily given context-free grammars A and B:\nThe following problems are \"decidable\" for arbitrary context-free languages:\nAccording to Hopcroft, Motwani, Ullman (2003), \nmany of the fundamental closure and (un)decidability properties of context-free languages were shown in the 1961 paper of Bar-Hillel, Perles, and Shamir\nLanguages that are not context-free.\nThe set formula_8 is a context-sensitive language, but there does not exist a context-free grammar generating this language. So there exist context-sensitive languages which are not context-free. To prove that a given language is not context-free, one may employ the pumping lemma for context-free languages or a number of other methods, such as Ogden's lemma or Parikh's theorem."}
{"id": "6868", "revid": "48528162", "url": "https://en.wikipedia.org/wiki?curid=6868", "title": "Caffeine", "text": "Caffeine is a central nervous system (CNS) stimulant of the methylxanthine class and is the most commonly consumed psychoactive substance globally. It is mainly used for its eugeroic (wakefulness promoting), ergogenic (physical performance-enhancing), or nootropic (cognitive-enhancing) properties. Caffeine acts by blocking binding of adenosine at a number of adenosine receptor types, inhibiting the centrally depressant effects of adenosine and enhancing the release of acetylcholine. Caffeine has a three-dimensional structure similar to that of adenosine, which allows it to bind and block its receptors. Caffeine also increases cyclic AMP levels through nonselective inhibition of phosphodiesterase, increases calcium release from intracellular stores, and antagonizes GABA receptors, although these mechanisms typically occur at concentrations beyond usual human consumption.\nCaffeine is a bitter, white crystalline purine, a methylxanthine alkaloid, and is chemically related to the adenine and guanine bases of deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). It is found in the seeds, fruits, nuts, or leaves of a number of plants native to Africa, East Asia and South America and helps to protect them against herbivores and from competition by preventing the germination of nearby seeds, as well as encouraging consumption by select animals such as honey bees. The best-known source of caffeine is the coffee bean, the seed of the \"Coffea\" plant. People may drink beverages containing caffeine to relieve or prevent drowsiness and to improve cognitive performance. To make these drinks, caffeine is extracted by steeping the plant product in water, a process called infusion. Caffeine-containing drinks, such as coffee, tea, and cola, are consumed globally in high volumes. In 2020, almost 10 million tonnes of coffee beans were consumed globally. Caffeine is the world's most widely consumed psychoactive drug. Unlike most other psychoactive substances, caffeine remains largely unregulated and legal in nearly all parts of the world. Caffeine is also an outlier as its use is seen as socially acceptable in most cultures with it even being encouraged.\nCaffeine has both positive and negative health effects. It can treat and prevent the premature infant breathing disorders bronchopulmonary dysplasia of prematurity and apnea of prematurity. Caffeine citrate is on the WHO Model List of Essential Medicines. It may confer a modest protective effect against some diseases, including Parkinson's disease. Some people experience sleep disruption or anxiety if they consume caffeine, but others show little disturbance. Evidence of a risk during pregnancy is equivocal; some authorities recommend that pregnant women limit caffeine to the equivalent of two cups of coffee per day or less. Caffeine can produce a mild form of drug dependence\u00a0\u2013 associated with withdrawal symptoms such as sleepiness, headache, and irritability\u00a0\u2013 when an individual stops using caffeine after repeated daily intake. Tolerance to the autonomic effects of increased blood pressure and heart rate, and increased urine output, develops with chronic use (i.e., these symptoms become less pronounced or do not occur following consistent use).\nCaffeine is classified by the U.S. Food and Drug Administration (FDA) as generally recognized as safe. Toxic doses, over 10 grams per day for an adult, are much higher than the typical dose of under 500 milligrams per day. The European Food Safety Authority reported that up to 400\u00a0mg of caffeine per day (around 5.7\u00a0mg/kg of body mass per day) does not raise safety concerns for non-pregnant adults, while intakes up to 200\u00a0mg per day for pregnant and lactating women do not raise safety concerns for the fetus or the breast-fed infants. A cup of coffee contains 80\u2013175\u00a0mg of caffeine, depending on what \"bean\" (seed) is used, how it is roasted, and how it is prepared (e.g., drip, percolation, or espresso). Thus it requires roughly 50\u2013100 ordinary cups of coffee to reach the toxic dose. However, pure powdered caffeine, which is available as a dietary supplement, can be lethal in tablespoon-sized amounts.\nUses.\nMedical.\nCaffeine is used for both prevention and treatment of bronchopulmonary dysplasia in premature infants. It may improve weight gain during therapy and reduce the incidence of cerebral palsy as well as reduce language and cognitive delay. On the other hand, subtle long-term side effects are possible.\nCaffeine is used as a primary treatment for apnea of prematurity, but not prevention. It is also used for orthostatic hypotension treatment.\nSome people use caffeine-containing beverages such as coffee or tea to try to treat their asthma. Evidence to support this practice is poor. It appears that caffeine in low doses improves airway function in people with asthma, increasing forced expiratory volume (FEV1) by 5% to 18% for up to four hours. \nThe addition of caffeine (100\u2013130\u00a0mg) to commonly prescribed pain relievers such as paracetamol or ibuprofen modestly improves the proportion of people who achieve pain relief.\nConsumption of caffeine after abdominal surgery shortens the time to recovery of normal bowel function and shortens length of hospital stay.\nCaffeine was formerly used as a second-line treatment for ADHD. It is considered less effective than methylphenidate or amphetamine but more so than placebo for children with ADHD. Children, adolescents, and adults with ADHD are more likely to consume caffeine, perhaps as a form of self-medication.\nEnhancing performance.\nCognitive performance.\nCaffeine is a central nervous system stimulant that may reduce fatigue and drowsiness. At normal doses, caffeine has variable effects on learning and memory, but it generally improves reaction time, wakefulness, concentration, and motor coordination. The amount of caffeine needed to produce these effects varies from person to person, depending on body size and degree of tolerance. The desired effects arise approximately one hour after consumption, and the desired effects of a moderate dose usually subside after about three or four hours.\nCaffeine can delay or prevent sleep and improves task performance during sleep deprivation. Shift workers who use caffeine make fewer mistakes that could result from drowsiness.\nCaffeine in a dose dependent manner increases alertness in both fatigued and normal individuals.\nA systematic review and meta-analysis from 2014 found that concurrent caffeine and -theanine use has synergistic psychoactive effects that promote alertness, attention, and task switching; these effects are most pronounced during the first hour post-dose.\nPhysical performance.\nCaffeine is a proven ergogenic aid in humans. Caffeine improves athletic performance in aerobic (especially endurance sports) and anaerobic conditions. Moderate doses of caffeine (around 5\u00a0mg/kg) can improve sprint performance, cycling and running time trial performance, endurance (i.e., it delays the onset of muscle fatigue and central fatigue), and cycling power output. Caffeine increases basal metabolic rate in adults. Caffeine ingestion prior to aerobic exercise increases fat oxidation, particularly in persons with low physical fitness.\nCaffeine improves muscular strength and power, and may enhance muscular endurance. Caffeine also enhances performance on anaerobic tests. Caffeine consumption before constant load exercise is associated with reduced perceived exertion. While this effect is not present during exercise-to-exhaustion exercise, performance is significantly enhanced. This is congruent with caffeine reducing perceived exertion, because exercise-to-exhaustion should end at the same point of fatigue. Caffeine also improves power output and reduces time to completion in aerobic time trials, an effect positively (but not exclusively) associated with longer duration exercise.\nSpecific populations.\nAdults.\nFor the general population of healthy adults, Health Canada advises a daily intake of no more than 400\u00a0mg. This limit was found to be safe by a 2017 systematic review on caffeine toxicology.\nChildren.\nIn healthy children, moderate caffeine intake under 400\u00a0mg produces effects that are \"modest and typically innocuous\". As early as six months old, infants can metabolize caffeine at the same rate as that of adults. Higher doses of caffeine (&gt;400\u00a0mg) can cause physiological, psychological and behavioral harm, particularly for children with psychiatric or cardiac conditions. There is no evidence that coffee stunts a child's growth. The American Academy of Pediatrics recommends that caffeine consumption, particularly in the case of energy and sports drinks, is not appropriate for children and adolescents and should be avoided. This recommendation is based on a clinical report released by American Academy of Pediatrics in 2011 with a review of 45 publications from 1994 to 2011 and includes inputs from various stakeholders (Pediatricians, Committee on nutrition, Canadian Pediatric Society, Centers for Disease Control &amp; Prevention, Food and Drug Administration, Sports Medicine &amp; Fitness committee, National Federations of High School Associations). For children age 12 and under, Health Canada recommends a maximum daily caffeine intake of no more than 2.5 milligrams per kilogram of body weight. Based on average body weights of children, this translates to the following age-based intake limits:\nAdolescents.\nHealth Canada has not developed advice for adolescents because of insufficient data. However, they suggest that daily caffeine intake for this age group be no more than 2.5\u00a0mg/kg body weight. This is because the maximum adult caffeine dose may not be appropriate for light-weight adolescents or for younger adolescents who are still growing. The daily dose of 2.5\u00a0mg/kg body weight would not cause adverse health effects in the majority of adolescent caffeine consumers. This is a conservative suggestion since older and heavier-weight adolescents may be able to consume adult doses of caffeine without experiencing adverse effects.\nPregnancy and breastfeeding.\nThe metabolism of caffeine is reduced in pregnancy, especially in the third trimester, and the half-life of caffeine during pregnancy can be increased up to 15 hours (as compared to 2.5 to 4.5 hours in non-pregnant adults). Evidence regarding the effects of caffeine on pregnancy and for breastfeeding are inconclusive. There is limited primary and secondary advice for, or against, caffeine use during pregnancy and its effects on the fetus or newborn.\nThe UK Food Standards Agency has recommended that pregnant women should limit their caffeine intake, out of prudence, to less than 200\u00a0mg of caffeine a day\u00a0\u2013 the equivalent of two cups of instant coffee, or one and a half to two cups of fresh coffee. The American Congress of Obstetricians and Gynecologists (ACOG) concluded in 2010 that caffeine consumption is safe up to 200\u00a0mg per day in pregnant women. For women who breastfeed, are pregnant, or may become pregnant, Health Canada recommends a maximum daily caffeine intake of no more than 300\u00a0mg, or a little over two 8\u00a0oz (237\u00a0mL) cups of coffee. A 2017 systematic review on caffeine toxicology found evidence supporting that caffeine consumption up to 300\u00a0mg/day for pregnant women is generally not associated with adverse reproductive or developmental effect.\nThere are conflicting reports in the scientific literature about caffeine use during pregnancy. A 2011 review found that caffeine during pregnancy does not appear to increase the risk of congenital malformations, miscarriage or growth retardation even when consumed in moderate to high amounts. Other reviews, however, concluded that there is some evidence that higher caffeine intake by pregnant women may be associated with a higher risk of giving birth to a low birth weight baby, and may be associated with a higher risk of pregnancy loss. A systematic review, analyzing the results of observational studies, suggests that women who consume large amounts of caffeine (greater than 300\u00a0mg/day) prior to becoming pregnant may have a higher risk of experiencing pregnancy loss.\nAdverse effects.\nPhysiological.\nCaffeine in coffee and other caffeinated drinks can affect gastrointestinal motility and gastric acid secretion. In postmenopausal women, high caffeine consumption can accelerate bone loss. Caffeine, alongside other factors such as stress and fatigue, can also increase the pressure in various muscles, including the eyelids.\nAcute ingestion of caffeine in large doses (at least 250\u2013300\u00a0mg, equivalent to the amount found in 2\u20133 cups of coffee or 5\u20138 cups of tea) results in a short-term stimulation of urine output in individuals who have been deprived of caffeine for a period of days or weeks. This increase is due to both a diuresis (increase in water excretion) and a natriuresis (increase in saline excretion); it is mediated via proximal tubular adenosine receptor blockade. The acute increase in urinary output may increase the risk of dehydration. However, chronic users of caffeine develop a tolerance to this effect and experience no increase in urinary output.\nPsychological.\nMinor undesired symptoms from caffeine ingestion not sufficiently severe to warrant a psychiatric diagnosis are common and include mild anxiety, jitteriness, insomnia, increased sleep latency, and reduced coordination. Caffeine can have negative effects on anxiety disorders. According to a 2011 literature review, caffeine use may induce anxiety and panic disorders in people with Parkinson's disease. At high doses, typically greater than 300\u00a0mg, caffeine can both cause and worsen anxiety. For some people, discontinuing caffeine use can significantly reduce anxiety. \nIn moderate doses, caffeine has been associated with reduced symptoms of depression and lower suicide risk. Two reviews indicate that increased consumption of coffee and caffeine may reduce the risk of depression.\nSome textbooks state that caffeine is a mild euphoriant, while others state that it is not a euphoriant.\nCaffeine-induced anxiety disorder is a subclass of the DSM-5 diagnosis of substance/medication-induced anxiety disorder.\nReinforcement disorders.\nAddiction.\nWhether caffeine can result in an addictive disorder depends on how addiction is defined. Compulsive caffeine consumption under any circumstances has not been observed, and caffeine is therefore not generally considered addictive. However, some diagnostic models, such as the and ICD-10, include a classification of caffeine addiction under a broader diagnostic model. Some state that certain users can become addicted and therefore unable to decrease use even though they know there are negative health effects.\nCaffeine does not appear to be a reinforcing stimulus, and some degree of aversion may actually occur, with people preferring placebo over caffeine in a study on drug abuse liability published in an NIDA research monograph. Some state that research does not provide support for an underlying biochemical mechanism for caffeine addiction. Other research states it can affect the reward system.\n\"Caffeine addiction\" was added to the ICDM-9 and ICD-10. However, its addition was contested with claims that this diagnostic model of caffeine addiction is not supported by evidence. The American Psychiatric Association's does not include the diagnosis of a \"caffeine addiction\" but proposes criteria for the disorder for more study.\nDependence and withdrawal.\nWithdrawal can cause mild to clinically significant distress or impairment in daily functioning. The frequency at which this occurs is self-reported at 11%, but in lab tests only half of the people who report withdrawal actually experience it, casting doubt on many claims of dependence. and most cases of caffeine withdrawal were 13% in the moderate sense. Moderately physical dependence and withdrawal symptoms may occur upon abstinence, with greater than 100\u00a0mg caffeine per day, although these symptoms last no longer than a day. Some symptoms associated with psychological dependence may also occur during withdrawal. The diagnostic criteria for caffeine withdrawal require a previous prolonged daily use of caffeine. Following 24 hours of a marked reduction in consumption, a minimum of 3 of these signs or symptoms is required to meet withdrawal criteria: difficulty concentrating, depressed mood/irritability, flu-like symptoms, headache, and fatigue. Additionally, the signs and symptoms must disrupt important areas of functioning and are not associated with effects of another condition.\nThe ICD-11 includes caffeine dependence as a distinct diagnostic category, which closely mirrors the DSM-5's proposed set of criteria for \"caffeine-use disorder\".\u00a0 Caffeine use disorder refers to dependence on caffeine characterized by failure to control caffeine consumption despite negative physiological consequences. The APA, which published the DSM-5, acknowledged that there was sufficient evidence in order to create a diagnostic model of caffeine dependence for the DSM-5, but they noted that the clinical significance of the disorder is unclear. Due to this inconclusive evidence on clinical significance, the DSM-5 classifies caffeine-use disorder as a \"condition for further study\".\nTolerance to the effects of caffeine occurs for caffeine-induced elevations in blood pressure and the subjective feelings of nervousness. Sensitization, the process whereby effects become more prominent with use, may occur for positive effects such as feelings of alertness and wellbeing. Tolerance varies for daily, regular caffeine users and high caffeine users. High doses of caffeine (750 to 1200\u00a0mg/day spread throughout the day) have been shown to produce complete tolerance to some, but not all of the effects of caffeine. Doses as low as 100\u00a0mg/day, such as a cup of coffee or two to three servings of caffeinated soft-drink, may continue to cause sleep disruption, among other intolerances. Non-regular caffeine users have the least caffeine tolerance for sleep disruption. Some coffee drinkers develop tolerance to its undesired sleep-disrupting effects, but others apparently do not.\nRisk of other diseases.\nA neuroprotective effect of caffeine against Alzheimer's disease and dementia is possible but the evidence is inconclusive.\nCaffeine may lessen the severity of acute mountain sickness if taken a few hours prior to attaining a high altitude. One meta analysis has found that caffeine consumption is associated with a reduced risk of type 2 diabetes. Regular caffeine consumption may reduce the risk of developing Parkinson's disease and may slow the progression of Parkinson's disease.\nCaffeine increases intraocular pressure in those with glaucoma but does not appear to affect normal individuals.\nThe DSM-5 also includes other caffeine-induced disorders consisting of caffeine-induced anxiety disorder, caffeine-induced sleep disorder and unspecified caffeine-related disorders. The first two disorders are classified under \"Anxiety Disorder\" and \"Sleep-Wake Disorder\" because they share similar characteristics. Other disorders that present with significant distress and impairment of daily functioning that warrant clinical attention but do not meet the criteria to be diagnosed under any specific disorders are listed under \"Unspecified Caffeine-Related Disorders\".\nEnergy crash.\nCaffeine is reputed to cause a fall in energy several hours after drinking, but this is not well researched.\nOverdose.\nConsumption of per day is associated with a condition known as \"&lt;dfn&gt;caffeinism&lt;/dfn&gt;.\" Caffeinism usually combines caffeine dependency with a wide range of unpleasant symptoms including nervousness, irritability, restlessness, insomnia, headaches, and palpitations after caffeine use.\nCaffeine overdose can result in a state of central nervous system overstimulation known as caffeine intoxication, a clinically significant temporary condition that develops during, or shortly after, the consumption of caffeine. This syndrome typically occurs only after ingestion of large amounts of caffeine, well over the amounts found in typical caffeinated beverages and caffeine tablets (e.g., more than 400\u2013500\u00a0mg at a time). According to the DSM-5, caffeine intoxication may be diagnosed if five (or more) of the following symptoms develop after recent consumption of caffeine: restlessness, nervousness, excitement, insomnia, flushed face, diuresis, gastrointestinal disturbance, muscle twitching, rambling flow of thought and speech, tachycardia or cardiac arrhythmia, periods of inexhaustibility, and psychomotor agitation.\nAccording to the International Classification of Diseases (ICD-11), cases of very high caffeine intake (e.g. &gt; 5 g) may result in caffeine intoxication with symptoms including mania, depression, lapses in judgment, disorientation, disinhibition, delusions, hallucinations or psychosis, and rhabdomyolysis.\nEnergy drinks.\nHigh caffeine consumption in energy drinks (at least one liter or 320\u00a0mg of caffeine) was associated with short-term cardiovascular side effects including hypertension, prolonged QT interval, and heart palpitations. These cardiovascular side effects were not seen with smaller amounts of caffeine consumption in energy drinks (less than 200\u00a0mg).\nSevere intoxication.\n there is no known antidote or reversal agent for caffeine intoxication. Treatment of mild caffeine intoxication is directed toward symptom relief; severe intoxication may require peritoneal dialysis, hemodialysis, or hemofiltration. Intralipid infusion therapy is indicated in cases of imminent risk of cardiac arrest in order to scavenge the free serum caffeine.\nLethal dose.\nDeath from caffeine ingestion appears to be rare, and most commonly caused by an intentional overdose of medications. In 2016, 3702 caffeine-related exposures were reported to Poison Control Centers in the United States, of which 846 required treatment at a medical facility, and 16 had a major outcome; and several caffeine-related deaths are reported in case studies. The LD50 of caffeine in rats is 192 milligrams per kilogram of body mass. The fatal dose in humans is estimated to be 150\u2013200 milligrams per kilogram, which is 10.5\u201314 grams for a typical adult, equivalent to about 75\u2013100 cups of coffee. There are cases where doses as low as 57 milligrams per kilogram have been fatal. A number of fatalities have been caused by overdoses of readily available powdered caffeine supplements, for which the estimated lethal amount is less than a tablespoon. The lethal dose is lower in individuals whose ability to metabolize caffeine is impaired due to genetics or chronic liver disease. A death was reported in 2013 of a man with liver cirrhosis who overdosed on caffeinated mints.\nInteractions.\nCaffeine is a substrate for CYP1A2, and interacts with many substances through this and other mechanisms.\nAlcohol.\nAccording to DSST, alcohol causes a decrease in performance on their standardized tests, and caffeine causes a significant improvement. When alcohol and caffeine are consumed jointly, the effects of the caffeine are changed, but the alcohol effects remain the same. For example, consuming additional caffeine does not reduce the effect of alcohol. However, the jitteriness and alertness given by caffeine is decreased when additional alcohol is consumed. Alcohol consumption alone reduces both inhibitory and activational aspects of behavioral control. Caffeine antagonizes the effect of alcohol on the activational aspect of behavioral control, but has no effect on the inhibitory behavioral control. The Dietary Guidelines for Americans recommend avoidance of concomitant consumption of alcohol and caffeine, as taking them together may lead to increased alcohol consumption, with a higher risk of alcohol-associated injury.\nSmoking.\nSmoking tobacco has been shown to increase caffeine clearance by 56% as a result of polycyclic aromatic hydrocarbons inducing the CYP1A2 enzyme. The CYP1A2 enzyme that is induced by smoking is responsible for the metabolism of caffeine; increased enzyme activity leads to increased caffeine clearance, and is associated with greater coffee consumption for regular smokers.\nBirth control.\nBirth control pills can extend the half-life of caffeine by as much as 40%, requiring greater attention to caffeine consumption.\nMedications.\nCaffeine sometimes increases the effectiveness of some medications, such as those for headaches. Caffeine was determined to increase the potency of some over-the-counter analgesic medications by 40%.\nThe pharmacological effects of adenosine may be blunted in individuals taking large quantities of methylxanthines like caffeine. Some other examples of methylxanthines include the medications theophylline and aminophylline, which are prescribed to relieve symptoms of asthma or COPD.\nPharmacology.\nPharmacodynamics.\nIn the absence of caffeine and when a person is awake and alert, little adenosine is present in CNS neurons. With a continued wakeful state, over time adenosine accumulates in the neuronal synapse, in turn binding to and activating adenosine receptors found on certain CNS neurons; when activated, these receptors produce a cellular response that ultimately increases drowsiness. When caffeine is consumed, it antagonizes adenosine receptors; in other words, caffeine prevents adenosine from activating the receptor by blocking the location on the receptor where adenosine binds to it. As a result, caffeine temporarily prevents or relieves drowsiness, and thus maintains or restores alertness.\nReceptor and ion channel targets.\nCaffeine is an antagonist of adenosine A2A receptors, and knockout mouse studies have specifically implicated antagonism of the A2A receptor as responsible for the wakefulness-promoting effects of caffeine. Antagonism of A2A receptors in the ventrolateral preoptic area (VLPO) reduces inhibitory GABA neurotransmission to the tuberomammillary nucleus, a histaminergic projection nucleus that activation-dependently promotes arousal. This disinhibition of the tuberomammillary nucleus is the downstream mechanism by which caffeine produces wakefulness-promoting effects. Caffeine is an antagonist of all four adenosine receptor subtypes (A1, A2A, A2B, and A3), although with varying potencies. The affinity (KD) values of caffeine for the human adenosine receptors are 12\u00a0\u03bcM at A1, 2.4\u00a0\u03bcM at A2A, 13\u00a0\u03bcM at A2B, and 80\u00a0\u03bcM at A3.\nAntagonism of adenosine receptors by caffeine also stimulates the medullary vagal, vasomotor, and respiratory centers, which increases respiratory rate, reduces heart rate, and constricts blood vessels. Adenosine receptor antagonism also promotes neurotransmitter release (e.g., monoamines and acetylcholine), which endows caffeine with its stimulant effects; adenosine acts as an inhibitory neurotransmitter that suppresses activity in the central nervous system. Heart palpitations are caused by blockade of the A1 receptor.\nBecause caffeine is both water- and lipid-soluble, it readily crosses the blood\u2013brain barrier that separates the bloodstream from the interior of the brain. Once in the brain, the principal mode of action is as a nonselective antagonist of adenosine receptors (in other words, an agent that reduces the effects of adenosine). The caffeine molecule is structurally similar to adenosine, and is capable of binding to adenosine receptors on the surface of cells without activating them, thereby acting as a competitive antagonist.\nIn addition to its activity at adenosine receptors, caffeine is an inositol trisphosphate receptor 1 antagonist and a voltage-independent activator of the ryanodine receptors (RYR1, RYR2, and RYR3). It is also a competitive antagonist of the ionotropic glycine receptor.\nEffects on striatal dopamine.\nWhile caffeine does not directly bind to any dopamine receptors, it influences the binding activity of dopamine at its receptors in the striatum by binding to adenosine receptors that have formed GPCR heteromers with dopamine receptors, specifically the A1\u2013D1 receptor heterodimer (this is a receptor complex with one adenosine A1 receptor and one dopamine D1 receptor) and the A2A\u2013D2 receptor heterotetramer (this is a receptor complex with two adenosine A2A receptors and two dopamine D2 receptors). The A2A\u2013D2 receptor heterotetramer has been identified as a primary pharmacological target of caffeine, primarily because it mediates some of its psychostimulant effects and its pharmacodynamic interactions with dopaminergic psychostimulants.\nCaffeine also causes the release of dopamine in the dorsal striatum and nucleus accumbens core (a substructure within the ventral striatum), but not the nucleus accumbens shell, by antagonizing A1 receptors in the axon terminal of dopamine neurons and A1\u2013A2A heterodimers (a receptor complex composed of one adenosine A1 receptor and one adenosine A2A receptor) in the axon terminal of glutamate neurons. During chronic caffeine use, caffeine-induced dopamine release within the nucleus accumbens core is markedly reduced due to drug tolerance.\nEnzyme targets.\nCaffeine, like other xanthines, also acts as a phosphodiesterase inhibitor. As a competitive nonselective phosphodiesterase inhibitor, caffeine raises intracellular cyclic AMP, activates protein kinase A, inhibits TNF-alpha and leukotriene synthesis, and reduces inflammation and innate immunity. Caffeine also affects the cholinergic system where it is a moderate inhibitor of the enzyme acetylcholinesterase.\nPharmacokinetics.\nCaffeine from coffee or other beverages is absorbed by the small intestine within 45 minutes of ingestion and distributed throughout all bodily tissues. Peak blood concentration is reached within 1\u20132\u00a0hours. It is eliminated by first-order kinetics. Caffeine can also be absorbed rectally, evidenced by suppositories of ergotamine tartrate and caffeine (for the relief of migraine) and of chlorobutanol and caffeine (for the treatment of hyperemesis). However, rectal absorption is less efficient than oral: the maximum concentration (Cmax) and total amount absorbed (AUC) are both about 30% (i.e., 1/3.5) of the oral amounts.\nCaffeine's biological half-life\u00a0\u2013 the time required for the body to eliminate one-half of a dose\u00a0\u2013 varies widely among individuals according to factors such as pregnancy, other drugs, liver enzyme function level (needed for caffeine metabolism) and age. In healthy adults, caffeine's half-life is between 3 and 7\u00a0hours. The half-life is decreased by 30-50% in adult male smokers, approximately doubled in women taking oral contraceptives, and prolonged in the last trimester of pregnancy. In newborns the half-life can be 80\u00a0hours or more, dropping rapidly with age, possibly to less than the adult value by age 6 months. The antidepressant fluvoxamine (Luvox) reduces the clearance of caffeine by more than 90%, and increases its elimination half-life more than tenfold, from 4.9\u00a0hours to 56\u00a0hours.\nCaffeine is metabolized in the liver by the cytochrome P450 oxidase enzyme system (particularly by the CYP1A2 isozyme) into three dimethylxanthines, each of which has its own effects on the body:\n1,3,7-Trimethyluric acid is a minor caffeine metabolite. 7-Methylxanthine is also a metabolite of caffeine. Each of the above metabolites is further metabolized and then excreted in the urine. Caffeine can accumulate in individuals with severe liver disease, increasing its half-life.\nA 2011 review found that increased caffeine intake was associated with a variation in two genes that increase the rate of caffeine catabolism. Subjects who had this mutation on both chromosomes consumed 40\u00a0mg more caffeine per day than others. This is presumably due to the need for a higher intake to achieve a comparable desired effect, not that the gene led to a disposition for greater incentive of habituation.\nChemistry.\nPure anhydrous caffeine is a bitter-tasting, white, odorless powder with a melting point of 235\u2013238\u00a0\u00b0C. Caffeine is moderately soluble in water at room temperature (2\u00a0g/100 mL), but quickly soluble in boiling water (66\u00a0g/100 mL). It is also moderately soluble in ethanol (1.5\u00a0g/100 mL). It is weakly basic (pKa of conjugate acid = ~0.6) requiring strong acid to protonate it. Caffeine does not contain any stereogenic centers and hence is classified as an achiral molecule.\nThe xanthine core of caffeine contains two fused rings, a pyrimidinedione and imidazole. The pyrimidinedione in turn contains two amide functional groups that exist predominantly in a zwitterionic resonance the location from which the nitrogen atoms are double bonded to their adjacent amide carbons atoms. Hence all six of the atoms within the pyrimidinedione ring system are sp2 hybridized and planar. The imidazole ring also has a resonance. Therefore, the fused 5,6 ring core of caffeine contains a total of ten pi electrons and hence according to H\u00fcckel's rule is aromatic.\nSynthesis.\nThe biosynthesis of caffeine is an example of convergent evolution among different species.\nCaffeine may be synthesized in the lab starting with 1,3-dimethylurea and malonic acid. Production of synthesized caffeine largely takes place in pharmaceutical plants in China. Synthetic and natural caffeine are chemically identical and nearly indistinguishable. The primary distinction is that synthetic caffeine is manufactured from urea and chloroacetic acid, while natural caffeine is extracted from plant sources, a process known as decaffeination.\nDespite the different production methods, the final product and its effects on the body are similar. Research on synthetic caffeine supports that it has the same stimulating effects on the body as natural caffeine. And although many claim that natural caffeine is absorbed slower and therefore leads to a gentler caffeine crash, there is little scientific evidence supporting the notion.\nDecaffeination.\nGermany, the birthplace of decaffeinated coffee, is home to several decaffeination plants, including the world's largest, Coffein Compagnie. Over half of the decaf coffee sold in the U.S. first travels from the tropics to Germany for caffeine removal before making its way to American consumers.\nExtraction of caffeine from coffee, to produce caffeine and decaffeinated coffee, can be performed using a number of solvents. Following are main methods:\n\"Decaffeinated\" coffees do in fact contain caffeine in many cases\u00a0\u2013 some commercially available decaffeinated coffee products contain considerable levels. One study found that decaffeinated coffee contained 10\u00a0mg of caffeine per cup, compared to approximately 85\u00a0mg of caffeine per cup for regular coffee.\nDetection in body fluids.\nCaffeine can be quantified in blood, plasma, or serum to monitor therapy in neonates, confirm a diagnosis of poisoning, or facilitate a medicolegal death investigation. Plasma caffeine levels are usually in the range of 2\u201310\u00a0mg/L in coffee drinkers, 12\u201336\u00a0mg/L in neonates receiving treatment for apnea, and 40\u2013400\u00a0mg/L in victims of acute overdosage. Urinary caffeine concentration is frequently measured in competitive sports programs, for which a level in excess of 15\u00a0mg/L is usually considered to represent abuse.\nAnalogs.\nSome analog substances have been created which mimic caffeine's properties with either function or structure or both. Of the latter group are the xanthines DMPX and 8-chlorotheophylline, which is an ingredient in dramamine. Members of a class of nitrogen substituted xanthines are often proposed as potential alternatives to caffeine. Many other xanthine analogues constituting the adenosine receptor antagonist class have also been elucidated.\nSome other caffeine analogs:\nPrecipitation of tannins.\nCaffeine, as do other alkaloids such as cinchonine, quinine or strychnine, precipitates polyphenols and tannins. This property can be used in a quantitation method.\nNatural occurrence.\nAround thirty plant species are known to contain caffeine. Common sources are the \"beans\" (seeds) of the two cultivated coffee plants, \"Coffea arabica\" and \"Coffea canephora\" (the quantity varies, but 1.3% is a typical value); and of the cocoa plant, \"Theobroma cacao\"; the leaves of the tea plant; and kola nuts. Other sources include the leaves of yaupon holly, South American holly yerba mate, and Amazonian holly guayusa; and seeds from Amazonian maple guarana berries. Temperate climates around the world have produced unrelated caffeine-containing plants.\nCaffeine in plants acts as a natural pesticide: it can paralyze and kill predator insects feeding on the plant. High caffeine levels are found in coffee seedlings when they are developing foliage and lack mechanical protection. In addition, high caffeine levels are found in the surrounding soil of coffee seedlings, which inhibits seed germination of nearby coffee seedlings, thus giving seedlings with the highest caffeine levels fewer competitors for existing resources for survival. Caffeine is stored in tea leaves in two places. Firstly, in the cell vacuoles where it is complexed with polyphenols. This caffeine probably is released into the mouth parts of insects, to discourage herbivory. Secondly, around the vascular bundles, where it probably inhibits pathogenic fungi from entering and colonizing the vascular bundles. Caffeine in nectar may improve the reproductive success of the pollen producing plants by enhancing the reward memory of pollinators such as honey bees.\nThe differing perceptions in the effects of ingesting beverages made from various plants containing caffeine could be explained by the fact that these beverages also contain varying mixtures of other methylxanthine alkaloids, including the cardiac stimulants theophylline and theobromine, and polyphenols that can form insoluble complexes with caffeine.\nProducts.\nProducts containing caffeine include coffee, tea, soft drinks (\"colas\"), energy drinks, other beverages, chocolate, caffeine tablets, other oral products, and inhalation products. According to a 2020 study in the United States, coffee is the major source of caffeine intake in middle-aged adults, while soft drinks and tea are the major sources in adolescents. Energy drinks are more commonly consumed as a source of caffeine in adolescents as compared to adults.\nBeverages.\nCoffee.\nThe world's primary source of caffeine is the coffee \"bean\" (the seed of the coffee plant), from which coffee is brewed. Caffeine content in coffee varies widely depending on the type of coffee bean and the method of preparation used; even beans within a given bush can show variations in concentration. In general, one serving of coffee ranges from 80 to 100 milligrams, for a single shot (30 milliliters) of arabica-variety espresso, to approximately 100\u2013125 milligrams for a cup (120 milliliters) of drip coffee. \"Arabica\" coffee typically contains half the caffeine of the \"robusta\" variety.\nIn general, dark-roast coffee has slightly less caffeine than lighter roasts because the roasting process reduces caffeine content of the bean by a small amount.\nTea.\nTea contains more caffeine than coffee by dry weight. A typical serving, however, contains much less, since less of the product is used as compared to an equivalent serving of coffee. Also contributing to caffeine content are growing conditions, processing techniques, and other variables. Thus, teas contain varying amounts of caffeine.\nTea contains small amounts of theobromine and slightly higher levels of theophylline than coffee. Preparation and many other factors have a significant impact on tea, and color is a poor indicator of caffeine content. Teas like the pale Japanese green tea, \"gyokuro\", for example, contain far more caffeine than much darker teas like \"lapsang souchong\", which has minimal caffeine content.\nSoft drinks and energy drinks.\nCaffeine is also a common ingredient of soft drinks, such as cola, originally prepared from kola nuts. Soft drinks typically contain 0 to 55 milligrams of caffeine per 12 ounce () serving. By contrast, energy drinks, such as Red Bull, can start at 80 milligrams of caffeine per serving. The caffeine in these drinks either originates from the ingredients used or is an additive derived from the product of decaffeination or from chemical synthesis. Guarana, a primary ingredient of energy drinks, contains large amounts of caffeine with small amounts of theobromine and theophylline in a naturally occurring slow-release excipient.\nCacao solids.\nCocoa solids (derived from cocoa bean) contain 230 mg caffeine per 100 g.\nThe caffeine content varies between cocoa bean strains. Caffeine content mg/g (sorted by lowest caffeine content):\nChocolate.\nCaffeine per 100 g:\nThe stimulant effect of chocolate may be due to a combination of theobromine and theophylline, as well as caffeine.\nTablets.\nTablets offer several advantages over coffee, tea, and other caffeinated beverages, including convenience, known dosage, and avoidance of concomitant intake of sugar, acids, and fluids. The use of caffeine in this form is said to improve mental alertness. These tablets are commonly used by students studying for their exams and by people who work or drive for long hours.\nOther oral products.\nOne U.S. company is marketing oral dissolvable caffeine strips. Another intake route is SpazzStick, a caffeinated lip balm. Alert Energy Caffeine Gum was introduced in the United States in 2013, but was voluntarily withdrawn after an announcement of an investigation by the FDA of the health effects of added caffeine in foods.\nInhalants.\nSimilar to an e-cigarette, a caffeine inhaler may be used to deliver caffeine or a stimulant like guarana by vaping. In 2012, the FDA sent a warning letter to one of the companies marketing an inhaler, expressing concerns for the lack of safety information available about inhaled caffeine.\nHistory.\nDiscovery and spread of use.\nAccording to Chinese legend, the Chinese emperor Shennong, reputed to have reigned in about 3000 BCE, inadvertently discovered tea when he noted that when certain leaves fell into boiling water, a fragrant and restorative drink resulted. Shennong is also mentioned in Lu Yu's \"Cha Jing\", a famous early work on the subject of tea.\nThe earliest credible evidence of either coffee drinking or knowledge of the coffee plant appears in the middle of the fifteenth century, in the Sufi monasteries of the Yemen in southern Arabia. From Mocha, coffee spread to Egypt and North Africa, and by the 16th century, it had reached the rest of the Middle East, Persia and Turkey. From the Middle East, coffee drinking spread to Italy, then to the rest of Europe, and coffee plants were transported by the Dutch to the East Indies and to the Americas.\nKola nut use appears to have ancient origins. It is chewed in many West African cultures, in both private and social settings, to restore vitality and ease hunger pangs.\nThe earliest evidence of cocoa bean use comes from residue found in an ancient Mayan pot dated to 600 BCE. Also, chocolate was consumed in a bitter and spicy drink called \"xocolatl\", often seasoned with vanilla, chile pepper, and achiote. \"Xocolatl\" was believed to fight fatigue, a belief probably attributable to the theobromine and caffeine content. Chocolate was an important luxury good throughout pre-Columbian Mesoamerica, and cocoa beans were often used as currency.\n\"Xocolatl\" was introduced to Europe by the Spaniards, and became a popular beverage by 1700. The Spaniards also introduced the cacao tree into the West Indies and the Philippines.\nThe leaves and stems of the yaupon holly (\"Ilex vomitoria\") were used by Native Americans to brew a tea called \"asi\" or the \"black drink\". Archaeologists have found evidence of this use far into antiquity, possibly dating to Late Archaic times.\nChemical identification, isolation, and synthesis.\nIn 1819, the German chemist Friedlieb Ferdinand Runge isolated caffeine for the first time; he called it \"Kaffebase\" (i.e., a base that exists in coffee). According to Runge, he did this at the behest of Johann Wolfgang von Goethe. In 1821, caffeine was isolated both by the French chemist Pierre Jean Robiquet and by another pair of French chemists, Pierre-Joseph Pelletier and Joseph Bienaim\u00e9 Caventou, according to Swedish chemist J\u00f6ns Jacob Berzelius in his yearly journal. Furthermore, Berzelius stated that the French chemists had made their discoveries independently of any knowledge of Runge's or each other's work. However, Berzelius later acknowledged Runge's priority in the extraction of caffeine, stating: \"However, at this point, it should not remain unmentioned that Runge (in his \"Phytochemical Discoveries\", 1820, pages 146\u2013147) specified the same method and described caffeine under the name \"Caffeebase\" a year earlier than Robiquet, to whom the discovery of this substance is usually attributed, having made the first oral announcement about it at a meeting of the Pharmacy Society in Paris.\"\nPelletier's article on caffeine was the first to use the term in print (in the French form from the French word for coffee: \"\"). It corroborates Berzelius's account:\nRobiquet was one of the first to isolate and describe the properties of pure caffeine, whereas Pelletier was the first to perform an elemental analysis.\nIn 1827, M. Oudry isolated \"th\u00e9ine\" from tea, but in 1838 it was proved by Mulder and by Carl Jobst that theine was actually the same as caffeine.\nIn 1895, German chemist Hermann Emil Fischer (1852\u20131919) first synthesized caffeine from its chemical components (i.e. a \"total synthesis\"), and two years later, he also derived the structural formula of the compound. This was part of the work for which Fischer was awarded the Nobel Prize in 1902.\nHistoric regulations.\nBecause it was recognized that coffee contained some compound that acted as a stimulant, first coffee and later also caffeine has sometimes been subject to regulation. For example, in the 16th century Islamists in Mecca and in the Ottoman Empire made coffee illegal for some classes. Charles II of England tried to ban it in 1676, Frederick II of Prussia banned it in 1777, and coffee was banned in Sweden at various times between 1756 and 1823.\nIn 1911, caffeine became the focus of one of the earliest documented health scares, when the US government seized 40 barrels and 20 kegs of Coca-Cola syrup in Chattanooga, Tennessee, alleging the caffeine in its drink was \"injurious to health\". Although the Supreme Court later ruled in favor of Coca-Cola in \"United States v. Forty Barrels and Twenty Kegs of Coca-Cola\", two bills were introduced to the U.S. House of Representatives in 1912 to amend the Pure Food and Drug Act, adding caffeine to the list of \"habit-forming\" and \"deleterious\" substances, which must be listed on a product's label.\nSociety and culture.\nRegulations.\nUnited States.\nThe US Food and Drug Administration (FDA) considers safe beverages containing less than 0.02% caffeine; but caffeine powder, which is sold as a dietary supplement, is unregulated. It is a regulatory requirement that the label of most prepackaged foods must declare a list of ingredients, including food additives such as caffeine, in descending order of proportion. However, there is no regulatory provision for mandatory quantitative labeling of caffeine, (e.g., milligrams caffeine per stated serving size). There are a number of food ingredients that naturally contain caffeine. These ingredients must appear in food ingredient lists. However, as is the case for \"food additive caffeine\", there is no requirement to identify the quantitative amount of caffeine in composite foods containing ingredients that are natural sources of caffeine. While coffee or chocolate are broadly recognized as caffeine sources, some ingredients (e.g., guarana, yerba mat\u00e9) are likely less recognized as caffeine sources. For these natural sources of caffeine, there is no regulatory provision requiring that a food label identify the presence of caffeine nor state the amount of caffeine present in the food. The FDA guidance was updated in 2018.\nConsumption.\nGlobal consumption of caffeine has been estimated at 120,000\u00a0tonnes per year, making it the world's most popular psychoactive substance. The consumption of caffeine has remained stable between 1997 and 2015. Coffee, tea and soft drinks are the most common caffeine sources, with energy drinks contributing little to the total caffeine intake across all age groups.\nReligions.\nThe Seventh-day Adventist Church asked for its members to \"abstain from caffeinated drinks\", but has removed this from baptismal vows (while still recommending abstention as policy). Some from these religions believe that one is not supposed to consume a non-medical, psychoactive substance, or believe that one is not supposed to consume a substance that is addictive. The Church of Jesus Christ of Latter-day Saints has said the following with regard to caffeinated beverages: \"... the Church revelation spelling out health practices (Doctrine and Covenants 89) does not mention the use of caffeine. The Church's health guidelines prohibit alcoholic drinks, smoking or chewing of tobacco, and 'hot drinks' \u2013 taught by Church leaders to refer specifically to tea and coffee.\"\nGaudiya Vaishnavas generally also abstain from caffeine, because they believe it clouds the mind and overstimulates the senses. To be initiated under a guru, one must have had no caffeine, alcohol, nicotine or other drugs, for at least a year.\nCaffeinated beverages are widely consumed by Muslims. In the 16th century, some Muslim authorities made unsuccessful attempts to ban them as forbidden \"intoxicating beverages\" under Islamic dietary laws.\nOther organisms.\nThe bacteria \"Pseudomonas putida\" CBB5 can live on pure caffeine and can cleave caffeine into carbon dioxide and ammonia.\nCaffeine is toxic to birds and to dogs and cats, and has a pronounced adverse effect on mollusks, various insects, and spiders. This is at least partly due to a poor ability to metabolize the compound, causing higher levels for a given dose per unit weight. Caffeine has also been found to enhance the reward memory of honey bees.\nResearch.\nCaffeine has been used to double chromosomes in haploid wheat."}
{"id": "6871", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=6871", "title": "Composers", "text": ""}
{"id": "6873", "revid": "1398", "url": "https://en.wikipedia.org/wiki?curid=6873", "title": "Caapi", "text": ""}
{"id": "6874", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=6874", "title": "Cyc", "text": "Cyc (pronounced ) is a long-term artificial intelligence project that aims to assemble a comprehensive ontology and knowledge base that spans the basic concepts and rules about how the world works. Hoping to capture common sense knowledge, Cyc focuses on implicit knowledge. The project began in July 1984 at MCC and was developed later by the Cycorp company. \nThe name \"Cyc\" (from \"encyclopedia\") is a registered trademark owned by Cycorp. CycL has a publicly released specification, and dozens of HL (Heuristic Level) modules were described in Lenat and Guha's textbook, but the Cyc inference engine code and the full list of HL modules are Cycorp-proprietary.\nHistory.\nThe project began in July 1984 by Douglas Lenat as a project of the Microelectronics and Computer Technology Corporation (MCC), a research consortium started by two United States\u2013based corporations \"to counter a then ominous Japanese effort in AI, the so-called 'fifth-generation' project.\" The US passed the National Cooperative Research Act of 1984, which for the first time allowed US companies to \"collude\" on long-term research. Since January 1995, the project has been under active development by Cycorp, where Douglas Lenat was the CEO.\nThe CycL representation language started as an extension of RLL (the Representation Language Language, developed in 1979\u20131980 by Lenat and his graduate student Russell Greiner while at Stanford University). In 1989, CycL had expanded in expressive power to higher-order logic (HOL).\nCyc's ontology grew to about 100,000 terms in 1994, and as of 2017, it contained about 1,500,000 terms. The Cyc knowledge base involving ontological terms was largely created by hand axiom-writing; it was at about 1 million in 1994, and as of 2017, it is at about 24.5 million.\nIn 2008, Cyc resources were mapped to many Wikipedia articles. Cyc is presently connected to Wikidata.\nKnowledge base.\nThe knowledge base is divided into \"microtheories\". Unlike the knowledge base as a whole, each microtheory must be free from monotonic contradictions. Each microtheory is a first-class object in the Cyc ontology; it has a name that is a regular constant. The concept names in Cyc are CycL \"terms\" or \"constants\". Constants start with an optional codice_1 and are case-sensitive. There are constants for:\nFor every instance of the collection codice_9 (i.e., for every chordate), there exists a female animal (instance of codice_10), which is its mother (described by the predicate codice_11).\nInference engine.\nAn inference engine is a computer program that tries to derive answers from a knowledge base. The Cyc inference engine performs general logical deduction. It also performs inductive reasoning, statistical machine learning and symbolic machine learning, and abductive reasoning.\nThe Cyc inference engine separates the epistemological problem from the heuristic problem. For the latter, Cyc used a community-of-agents architecture in which specialized modules, each with its own algorithm, became prioritized if they could make progress on the sub-problem.\nReleases.\nOpenCyc.\nThe first version of OpenCyc was released in spring 2002 and contained only 6,000 concepts and 60,000 facts. The knowledge base was released under the Apache License. Cycorp stated its intention to release OpenCyc under parallel, unrestricted licences to meet the needs of its users. The CycL and SubL interpreter (the program that allows users to browse and edit the database as well as to draw inferences) was released free of charge, but only as a binary, without source code. It was made available for Linux and Microsoft Windows. The open source Texai project released the RDF-compatible content extracted from OpenCyc. The user interface was in Java 6.\nCycorp was a participant of a working group for the Semantic Web, Standard Upper Ontology Working Group, which was active from 2001 to 2003.\nA Semantic Web version of OpenCyc was available starting in 2008, but ending sometime after 2016.\nOpenCyc 4.0 was released in June 2012. OpenCyc 4.0 contained 239,000 concepts and 2,093,000 facts; however, these are mainly taxonomic assertions.\n4.0 was the last released version, and around March of 2017, OpenCyc was shutdown for the purported reason that \"because such \u201cfragmenting\u201d led to divergence, and led to confusion amongst its users and the technical community generally that that OpenCyc fragment \"was\" Cyc.\".\nResearchCyc.\nIn July 2006, Cycorp released the executable of ResearchCyc 1.0, a version of Cyc aimed at the research community, at no charge. (ResearchCyc was in beta stage of development during all of 2004; a beta version was released in February 2005.) In addition to the taxonomic information, ResearchCyc includes more semantic knowledge; it also includes a large lexicon, English parsing and generation tools, and Java-based interfaces for knowledge editing and querying. It contains a system for ontology-based data integration.\nApplications.\nIn 2001, GlaxoSmithKline was funding the Cyc, though for unknown applications. In 2007, the Cleveland Clinic has used Cyc to develop a natural-language query interface of biomedical information on cardiothoracic surgeries. A query is parsed into a set of CycL fragments with open variables. The Terrorism Knowledge Base was an application of Cyc that tried to contain knowledge about \"terrorist\"-related descriptions. The knowledge is stored as statements in mathematical logic. The project lasted from 2004 to 2008. Lycos used Cyc for search term disambiguation, but stopped in 2001. CycSecure was produced in 2002, a network vulnerability assessment tool based on Cyc, with trials at the US STRATCOM Computer Emergency Response Team.\nOne Cyc application has the stated aim to help students doing math at a 6th grade level. The application, called MathCraft, was supposed to play the role of a fellow student who is slightly more confused than the user about the subject. As the user gives good advice, Cyc allows the avatar to make fewer mistakes.\nCriticisms.\nThe Cyc project has been described as \"one of the most controversial endeavors of the artificial intelligence history\". Catherine Havasi, CEO of Luminoso, says that Cyc is the predecessor project to IBM's Watson. Machine-learning scientist Pedro Domingos refers to the project as a \"catastrophic failure\" for the unending amount of data required to produce any viable results and the inability for Cyc to evolve on its own.\nGary Marcus, a cognitive scientist and the cofounder of an AI company called Geometric Intelligence, says \"it represents an approach that is very different from all the deep-learning stuff that has been in the news.\" This is consistent with Doug Lenat's position that \"Sometimes the \"veneer\" of intelligence is not enough\".\nNotable employees.\nThis is a list of some of the notable people who work or have worked on Cyc either while it was a project at MCC (where Cyc was first started) or Cycorp."}
{"id": "6875", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=6875", "title": "Cheliceriformes", "text": ""}
{"id": "6876", "revid": "38455", "url": "https://en.wikipedia.org/wiki?curid=6876", "title": "CE", "text": "CE, Ce, ce, \u0106e, or \"variants\" may refer to:"}
{"id": "6878", "revid": "753665", "url": "https://en.wikipedia.org/wiki?curid=6878", "title": "Carlos Valderrama", "text": "Carlos Alberto Valderrama Palacio ( ; born 2 September 1961), also known as \"El Pibe\" (\"The Kid\"), is a Colombian former professional footballer and sports commentator for F\u00fatbol de Primera, who played as an attacking midfielder. Valderrama is considered by many to be one of the greatest South American players in history and one of the best players of his era. In 2004, he was named by Pel\u00e9 in the FIFA 100 list of the world's greatest living players.\nA creative playmaker, he is regarded as one of the best Colombian footballers of all time, and by some, as Colombia's greatest player ever. His distinctive hairstyle, as well as his precise passing and technical skills made him one of South America's most recognisable footballers in the late 1980s and early 1990s. He won the South American Footballer of the Year award in 1987 and 1993, He is the fifth highest assister in the history of national teams and the twelfth overall, including clubs, and in 1999, he was also named one of the top 100 players of the 20th century by World Soccer.\nValderrama was a member of the Colombia national football team from 1985 until 1998. He represented Colombia in 111 full internationals and scored 11 times, making him the second-most capped player in the country's history, behind only David Ospina. He played a major role during the golden era of Colombian football in the 1990s, representing his national side in three FIFA World Cups and five Copa Am\u00e9rica tournaments.\nAfter spending most of his career playing club football in South America and Europe, towards the end of his career Valderrama played in Major League Soccer, joining the league in its first season. One of the most recognisable players in the league at the time of its inception, he helped popularise the league during the second half of the 1990s. To this day, he is an icon and is considered one of the most decorated players to ever play in MLS; in 2005, he was named to the MLS All-Time Best XI.\nClub career.\nColombia and Europe.\nBorn in Santa Marta, Colombia, Valderrama began his career at Uni\u00f3n Magdalena of the Colombian First Division in 1981. He also later played for Millonarios in 1984. He joined Deportivo Cali in 1985, where he played most of his Colombian football. In 1988, he moved to the French First Division side Montpellier. He struggled to adapt to the less technical and the faster, more physical, and tactical brand of football being played in Europe, losing his place in the squad. However, his passing ability later saw him become the club's main creative force, and he played a decisive role as his side won the Coupe de France in 1990. In 1991, he remained in Europe and joined Spanish side Real Valladolid for a season. He then returned to Colombia in 1992 and went on to play for Independiente Medell\u00edn, and subsequently Atl\u00e9tico Junior in 1993, with whom he won the Colombian championship in 1993 and 1995.\nMLS career.\nValderrama began his Major League Soccer career with the US side Tampa Bay Mutiny in the league's inaugural 1996 season. The team won the first ever Supporters' Shield, awarded for having the league's best regular season record, while Valderrama was the league's first Most Valuable Player, finishing the season with 4 goals and 17 assists. He remained with the club for the 1997 season, and also spent a spell on loan back at Deportivo Cali in Colombia, before moving to another MLS side, Miami Fusion, in 1998, where he also remained for two seasons. He returned to Tampa Bay in 2000, spending two more seasons with the club; while a member of the Mutiny, the team would sell Carlos Valderrama wigs at Tampa Stadium. In the 2000 MLS season, the 38-year-old Valderrama recorded the only 20+ assist season in MLS history\u2014ending the season with 26 \u2014 a single season assist record that remains intact to this day, and which MLS itself suggested was an \"unbreakable\" record in a 2012 article.\nIn 2001, Valderrama joined the Colorado Rapids, and remained with the team until 2002, when he retired. He played his last career match in a 1\u20131 draw with the Kansas City Wizards on 20 September 2002, with Valderrama assisting Mark Chung's goal, and in doing so at the age of 41 years and 18 days, he became the oldest player in the league's history at the time, a record that has since been surpassed by four other players, including three goalkeepers. His American soccer league career spanned a total of eight years, during which he made 175 appearances. In the MLS, Valderrama scored relatively few goals (16) for a midfielder, but is the league's fourth all-time leader in assists (114) after Brad Davis (123), Steve Ralston (135) \u2013 a former teammate, and Landon Donovan (145). In 2005, he was named to the MLS All-Time Best XI.\nInternational career.\nValderrama was a member of the Colombia national football team from 1985 until 1998; he made 111 international appearances, scoring 11 goals, making him the most capped outfield player in the country's history. He represented and captained his national side in the 1990, 1994, and \n1998 FIFA World Cups, and also took part in the 1987, 1989, 1991, 1993, and 1995 Copa Am\u00e9rica tournaments.\nValderrama made his international debut on 27 October 1985, in a 3\u20130 defeat to Paraguay in a 1986 World Cup qualifying match, at the age of 24. In his first major international tournament, he helped Colombia to a third-place finish at the 1987 Copa Am\u00e9rica in Argentina, as his team's captain, where he was named the tournament's best player; during the tournament, he scored the opening goal in Colombia's 2\u20130 over Bolivia on 1 July, their first match of the group stage.\nSome of Valderrama's most impressive international performances came during the 1990 FIFA World Cup in Italy, during which he served as Colombia's captain. He helped his team to a 2\u20130 win against the UAE in Colombia's opening match of the group stage, scoring the second goal of the match with a strike from 20 yards. Colombia lost their second match against Yugoslavia, however, needing at least a draw against the eventual champions West Germany in their final group match in order to advance to the next round of the competition. In the decisive game, German striker Pierre Littbarski scored what appeared to be the winning goal in the 88th minute of the game; however, within the last minute of injury time, Valderrama beat several opposing players and made a crucial left-footed pass to Freddy Rinc\u00f3n, who subsequently equalised, sealing a place for Colombia in the second round of the tournament with a 1\u20131 draw. Colombia were eliminated in the round of 16, following a 2\u20131 extra time loss to Cameroon.\nOn 5 September 1993, Valderrama contributed to Colombia's historic 5\u20130 victory over South American rivals Argentina at the Monumental in Buenos Aires, which allowed them to qualify for the 1994 World Cup. Although much was expected of Valderrama at the World Cup, an injury during a pre-tournament warm-up game put his place in the squad in jeopardy; although he was able to regain match fitness in time for the tournament, Colombia disappointed and suffered a first-round elimination following defeats to Romania and the hosts USA. However, it is widely believed that internal problems and threats by drug cartel groups at the time contributed to the team's underwhelming results in the competition, in particular following the murder of Andr\u00e9s Escobar after Colombia's 2\u20131 defeat to the host nation in the second group match; during the match, the Colombian defender had netted an own goal to open the scoring, which ultimately proved to be decisive, despite a 2\u20130 win over Switzerland in the final first round fixture.\nFour years later, Valderrama led his nation to qualify for the 1998 World Cup in France, scoring three goals during the qualifying stages. His impact in the final tournament at the advancing age of 37, however, was less decisive, and, despite defeating Tunisia, Colombia once again suffered a first round exit, following a 2\u20130 defeat against England, which was Valderrama's final international appearance.\nPlaying style.\nAlthough Valderrama is often defined as a 'classic number 10 playmaker', due to his creativity and offensive contribution, in reality he was not a classic playmaker in the traditional sense. Although he often wore the number 10 shirt throughout his career and was deployed as an attacking midfielder at times, he played mostly in deeper positions in the centre of the pitch \u2013 often operating in a free role as a deep-lying playmaker, rather than in more advanced midfield positions behind the forwards \u2013 in order to have a greater influence on the game. A team-player, Valderrama was also known to be an extremely selfless midfielder, who preferred assisting his teammates over going for goal himself; his tactical intelligence, positioning, reading of the game, efficient movement, and versatile range of passing enabled him to find space for himself to distribute and receive the ball, which allowed him both to set the tempo of his team in midfield with short, first time exchanges, or create chances with long lobbed passes or through balls.\nValderrama's most instantly recognisable physical features were his big afro-blonde hairstyle, jewelry, and moustache, but he was best known for his grace and elegance on the ball, as well as his agility, and quick feet as a footballer. His control, dribbling ability and footwork were similar to those of smaller players, which for a player of Valderrama's size and physical build was fairly uncommon, and he frequently stood out throughout his career for his ability to use his strength, balance, composure, and flamboyant technique to shield the ball from opponents when put under pressure, and retain possession in difficult situations, often with elaborate skills, which made him an extremely popular figure with the fans. Valderrama's mix of physical strength, two-footed ability, unpredictability and flair enabled him to produce key and incisive performances against top-tier teams, while his world class vision and exceptional passing and crossing ability with his right foot made him one of the best assist providers of his time; his height, physique and elevation also made him effective in the air, and he was also an accurate free kick taker and striker of the ball, despite not being a particularly prolific goalscorer.\nDespite his natural talent and ability as a footballer, Valderrama earned a reputation for having a \"languid\" playing style, as well as lacking notable pace, being unfit, and for having a poor defensive work-rate on the pitch, in particular, after succumbing to the physical effects of ageing in his later career in the MLS. In his first season in France, he also initially struggled to adapt to the faster-paced, more physical, and tactically rigorous European brand of football, which saw him play in an unfamiliar position, and gave him less space and time on the ball to dictate attacking passing moves; he was criticised at times for his lack of match fitness and his low defensive contribution, which initially limited his appearances with the club, although he later successfully became a key creative player in his team's starting line-up due to his discipline, skill, and his precise and efficient passing. Despite these claims, earlier in his career, however, Valderrama demonstrated substantial pace, stamina, and defensive competence.\nFormer French defender Laurent Blanc, who played with Valderrama in Montpellier, described him thusly: \"In the fast and furious European game he wasn't always at his ease. He was a natural exponent of 'toque', keeping the ball moving. But he was so gifted that we could give him the ball when we didn't know what else to do with it knowing he wouldn't lose it... and often he would do things that most of us only dream about.\"\nRetirement and legacy.\nIn February 2004, Valderrama ended his 22-year career in a tribute match at the Metropolitan stadium of Barranquilla, with some of the most important football players of South America, such as Diego Maradona, Enzo Francescoli, Iv\u00e1n Zamorano, and Jos\u00e9 Luis Chilavert.\nIn 2006, a 22-foot bronze statue of Valderrama, created by Colombian artist Amilkar Ariza, was erected outside Estadio Eduardo Santos in Valderrama's birthplace of Santa Marta.\nValderrama was the only Colombian to be featured by Pel\u00e9 in FIFA's 125 Top Living Football Players list in March 2004.\nMedia.\nValderrama appeared on the cover of Konami's \"International Superstar Soccer Pro 98\". In the Nintendo 64 version of the game, he is referred to by his nickname, \"El Pibe\".\nValderrama has also appeared in EA Sports' FIFA football video game series; he was named one of the Ultimate Team Legend cards in \"FIFA 15\".\nBesides his link to videogames, Valderrama has been present in sports media through his work with F\u00fatbol de Primera, Andr\u00e9s Cantor's radio station. He works as a color commentator during broadcasts of different matches, mostly participating during the FIFA World Cup, alongside play-by-play commentators like Sammy Sadovnik or Cantor himself.\nCoaching career.\nSince retiring from professional football, Valderrama has become assistant manager of Atl\u00e9tico Junior. On 1 November 2007, Valderrama accused a referee of corruption by waving cash in the face of Oscar Julian Ruiz when the official awarded a penalty to Am\u00e9rica de Cali. Junior lost the match 4\u20131, which ended the club's hopes of playoff qualification. He later also served as a coach for a football academy called Clearwater Galactics in Clearwater, Florida.\nPersonal life.\nValderrama is married and has six children.\nHonours.\nMontpellier\nAtletico Junior\nTampa Bay Mutiny\nIndividual"}
{"id": "6879", "revid": "37599202", "url": "https://en.wikipedia.org/wiki?curid=6879", "title": "Cyborgs in fiction", "text": ""}
{"id": "6880", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=6880", "title": "Caesar salad", "text": "A Caesar salad (also spelled Cesar, C\u00e9sar and Cesare) is a green salad of romaine lettuce and croutons dressed with lemon juice (or lime juice), olive oil, eggs, Worcestershire sauce, anchovies, garlic, Dijon mustard, Parmesan and black pepper.\nThe salad was created on July 4, 1924, by Caesar Cardini at Caesar's in Tijuana, Mexico, when the kitchen was overwhelmed and short on ingredients. It was originally prepared tableside, and it is still prepared tableside at the original venue.\nHistory.\nThe salad's creation is generally attributed to the restaurateur Caesar Cardini, an Italian immigrant who operated restaurants in Mexico and the United States. Cardini lived in San Diego, but ran one of his restaurants, Caesar's, in Tijuana, Mexico, to attract American customers seeking to circumvent the restrictions of Prohibition. His daughter, Rosa, recounted that her father invented the salad at the Tijuana restaurant when a Fourth of July rush in 1924 depleted the kitchen's supplies. Cardini made do with what he had, adding the dramatic flair of table-side tossing by the chef. Some other accounts of the history state that Alex Cardini, Caesar Cardini's brother, made the salad, and that the salad was previously named the \"Aviator Salad\" because it was made for aviators who traveled over during Prohibition. A number of Cardini's staff have also said that they invented the dish. A popular myth attributes its invention to Julius Caesar. A 2024 book confirmed the claim that Caesar Cardini originated the recipe. Livio Santini's son, Aldo, countered that his father provided the recipe while working as a cook in Cardini's restaurant.\nThe American chef and writer Julia Child said that she had eaten a Caesar salad at Cardini's restaurant in her youth during the 1920s, made with whole romaine lettuce leaves, which were meant to be lifted by the stem and eaten with the fingers, tossed with olive oil, salt, pepper, lemon juice, Worcestershire sauce, coddled eggs, Parmesan, and croutons made with garlic-infused oil. In 1946, the newspaper columnist Dorothy Kilgallen wrote of a Caesar containing anchovies, differing from Cardini's version:\nThe big food rage in Hollywood\u2014the Caesar salad\u2014will be introduced to New Yorkers by Gilmore's Steak House. It's an intricate concoction that takes ages to prepare and contains (zowie!) lots of garlic, raw or slightly coddled eggs, croutons, romaine, anchovies, parmeasan cheese, olive oil, vinegar and plenty of black pepper.\nIn a 1952 interview, Cardini said the salad became well known in 1937, when Manny Wolf, story editor and Paramount Pictures writer's department head, provided the recipe to Hollywood restaurants.&lt;ref name=\"stuff/10429532\"&gt;&lt;/ref&gt;&lt;ref name=\"kitchenproject/CaesarSalad\"&gt;&lt;/ref&gt;\nIn the 1970s, Child published a recipe in her book \"From Julia Child's Kitchen\", based on an interview with Cardini's daughter, in which the ingredients are tossed one-at-a-time with the lettuce leaves. Cardini's daughter and several other sources have testified that the original recipe used only Worcestershire sauce, not anchovies, mustard, or herbs, which Cardini considered too bold in flavor. Modern recipes typically include anchovies as a key ingredient, and are frequently emulsified or based on mayonnaise.\nDressing.\nBottled Caesar dressings are produced and marketed by many companies, including Cardini's, Bolthouse Farms, Ken's Foods, Marzetti, Newman's Own, Panera Bread, Trader Joe's, and Whole Foods Market. The trademark brands, \"Cardini's\", \"Caesar Cardini's\" and \"The Original Caesar Dressing\" are all claimed to date to February 1950, although they were only registered decades later.\nIngredients.\nCommon ingredients in many recipes:\nVariations include varying the leaf, adding meat such as grilled chicken or bacon, or omitting ingredients such as anchovies and eggs.\nWhile the original Caesar's in Tijuana uses lime juice in their current recipe, most modern recipes use lemon juice or vinegar. Modern chefs sometimes put experimental salads on menus under the \"Caesar\" even when there is no resemblance to the original recipe. Unrelated variations, called \"mutants\" and \"bastardized\" in \"The Atlantic\", use the familiar, appealing name to attract diners to dishes with a similar hit of \"umami, fat, and tons of salt.\"\nVegan versions can replace anchovies with capers and the eggs with tahini."}
{"id": "6881", "revid": "196446", "url": "https://en.wikipedia.org/wiki?curid=6881", "title": "Cecilia Beaux", "text": "Eliza Cecilia Beaux (May 1, 1855 \u2013 September 17, 1942) was an American artist and the first woman to teach art at the Pennsylvania Academy of the Fine Arts. Known for her elegant and sensitive portraits of friends, relatives, and Gilded Age patrons, Beaux painted many famous subjects including First Lady Edith Roosevelt, Admiral Sir David Beatty and Georges Clemenceau.\nBeaux was trained in Philadelphia and went on to study in Paris where she was influenced by academic artists Tony Robert-Fleury and William-Adolphe Bouguereau as well as the work of \u00c9douard Manet and Edgar Degas. Her style was compared to that of John Singer Sargent; at one exhibition, Bernard Berenson joked that her paintings were the best Sargents in the room. Like her instructor William Sartain, she believed there was a connection between physical characteristics and behavioral traits.\nBeaux was awarded a gold medal for lifetime achievement by the National Institute of Arts and Letters, and honored by Eleanor Roosevelt as \"the American woman who had made the greatest contribution to the culture of the world\".\nEarly life and education.\nBeaux was born on May 1, 1855, in Philadelphia, the younger daughter of French silk manufacturer Jean Adolphe Beaux and teacher Cecilia Kent Leavitt. Her mother was the daughter of prominent businessman John Wheeler Leavitt of New York City and his wife, Cecilia Kent of Suffield, Connecticut. Cecilia Kent Leavitt died from puerperal fever 12 days after giving birth at age 33.\nCecilia and her sister Etta were subsequently raised by their maternal grandmother and aunts, primarily in Philadelphia. Her father, unable to bear the grief of his loss, and feeling adrift in a foreign country, returned to his native France for 16 years, with only one visit back to Philadelphia. He returned when Cecilia was two, but left four years later after his business failed. As she confessed later, \"We didn't love Papa very much, he was so foreign. We thought him \"peculiar\".\" Her father did have a natural aptitude for drawing and the sisters were charmed by his whimsical sketches of animals. Later, Beaux would discover that her French heritage would serve her well during her pilgrimage and training in France.\nIn Philadelphia, Beaux's aunt Emily married mining engineer William Foster Biddle, whom Beaux would later describe as \"after my grandmother, the strongest and most beneficent influence in my life.\" For fifty years, he cared for his nieces-in-law with consistent attention and occasional financial support. Her grandmother, on the other hand, provided day-to-day supervision and kindly discipline. Whether with housework, handiwork, or academics, Grandma Leavitt offered a pragmatic framework, stressing that \"everything undertaken must be completed, conquered.\" The Civil War years were particularly challenging, but the extended family survived despite little emotional or financial support from Beaux's father.\nAfter the war, Beaux began to spend some time in the household of \"Willie\" and Emily, both proficient musicians. Beaux learned to play the piano but preferred singing. The musical atmosphere later proved an advantage for her artistic ambitions. Beaux recalled, \"They understood perfectly the spirit and necessities of an artist's life.\" In her early teens, she had her first major exposure to art during visits with Willie to the nearby Pennsylvania Academy of the Fine Arts, one of America's foremost art schools and museums. Though fascinated by the narrative elements of some of the pictures, particularly the Biblical themes of the massive paintings of Benjamin West, at this point Beaux had no aspirations of becoming an artist.\nHer childhood was a sheltered though generally happy one. As a teen she already manifested the traits, as she described, of \"both a realist and a perfectionist, pursued by an uncompromising passion for carrying through.\" She attended the Misses Lyman School and was just an average student, though she did well in French and Natural History. However, she was unable to afford the extra fee for art lessons.\nAt age 16, Beaux began art lessons with a relative, Catherine Ann Drinker, an accomplished artist who had her own studio and a growing clientele. Drinker became Beaux's role model, and she continued lessons with Drinker for a year. She then studied for two years with the painter Francis Adolf Van der Wielen, who offered lessons in perspective and drawing from casts during the time that the new Pennsylvania Academy of the Fine Arts was under construction. Given the bias of the Victorian age, female students were denied direct study in anatomy and could not attend drawing classes with live models (who were often prostitutes) until a decade later.\nAt 18, Beaux was appointed as a drawing teacher at Miss Sanford's School, taking over Drinker's post. She also gave private art lessons and produced decorative art and small portraits. Her own studies were mostly self-directed. Beaux received her first introduction to lithography doing copy work for Philadelphia printer Thomas Sinclair and she published her first work in \"St. Nicholas\" magazine in December 1873. Beaux demonstrated accuracy and patience as a scientific illustrator, creating drawings of fossils for Edward Drinker Cope, for a multi-volume report sponsored by the U.S. Geological Survey. However, she did not find technical illustration suitable for a career (the extreme exactitude required gave her pains in the \"solar plexus\"). At this stage, she did not yet consider herself an artist.\nBeaux began attending the Pennsylvania Academy of the Fine Arts in Philadelphia in 1876, then under the dynamic influence of Thomas Eakins, whose work \"The Gross Clinic\" had \"horrified Philadelphia Exhibition-goers as a gory spectacle\" at the Centennial Exhibition of 1876. She steered clear of the controversial Eakins, though she much admired his work. His progressive teaching philosophy, focused on anatomy and live study and allowed the female students to partake in segregated studios, eventually led to his firing as director of the academy. She did not ally herself with Eakins' ardent student supporters, and later wrote, \"A curious instinct of self-preservation kept me outside the magic circle.\" Instead, she attended costume and portrait painting classes for three years taught by the ailing director Christian Schussele. Beaux won the Mary Smith Prize at the Pennsylvania Academy of the Fine Arts exhibitions in 1885, 1887, 1891, and 1892.\nAfter leaving the academy, the 24-year-old Beaux decided to try her hand at porcelain painting and she enrolled in a course at the National Art Training School. She was well suited to the precise work but later wrote, \"this was the lowest depth I ever reached in commercial art, and although it was a period when youth and romance were in their first attendance on me, I remember it with gloom and record it with shame.\" She studied privately with William Sartain, a friend of Eakins and a New York artist invited to Philadelphia to teach a group of art students, starting in 1881. Though Beaux admired Eakins more and thought his painting skill superior to Sartain's, she preferred the latter's gentle teaching style which promoted no particular aesthetic approach. Unlike Eakins, however, Sartain believed in phrenology and Beaux adopted a lifelong belief that physical characteristics correlated with behaviors and traits.\nBeaux attended Sartain's classes for two years, then rented her own studio and shared it with a group of women artists who hired a live model and continued without an instructor. After the group disbanded, Beaux set in earnest to prove her artistic abilities. She painted a large canvas in 1884, \"Les Derniers Jours d'Enfance\", a portrait of her sister and nephew whose composition and style revealed a debt to James McNeill Whistler and whose subject matter was akin to Mary Cassatt's mother-and-child paintings. It was awarded a prize for the best painting by a female artist at the academy, and further exhibited in Philadelphia and New York. Following that seminal painting, she painted over 50 portraits in the next three years with the zeal of a committed professional artist. Her invitation to serve as a juror on the hanging committee of the academy confirmed her acceptance amongst her peers. In the mid-1880s, she was receiving commissions from notable Philadelphians and earning $500 per portrait, comparable to what Eakins commanded. When her friend Margaret Bush-Brown insisted that \"Les Derniers\" was good enough to be exhibited at the famed Paris Salon, Beaux relented and sent the painting abroad in the care of her friend, who managed to get the painting into the exhibition.\nParis.\nAt 32, despite her success in Philadelphia, Beaux decided that she still needed to advance her skills. She left for Paris with cousin May Whitlock, forsaking several suitors and overcoming the objections of her family. There she trained at the Acad\u00e9mie Julian, the largest art school in Paris, and at the Acad\u00e9mie Colarossi, receiving weekly critiques from established masters like Tony Robert-Fleury and William-Adolphe Bouguereau. She wrote, \"Fleury is much less benign than Bouguereau and don't temper his severities\u2026he hinted of possibilities before me and as he rose said the nicest thing of all, 'we will do all we can to help you'\u2026I want these men\u2026to know me and recognize that I can do something.\" Though advised regularly of Beaux's progress abroad and to \"not be worried about any indiscretions of ours\", her Aunt Eliza repeatedly reminded her niece to avoid the temptations of Paris, \"Remember you are first of all a Christian \u2013 then a woman and last of all an Artist.\"\nWhen Beaux arrived in Paris, the Impressionists, a group of artists who had begun their own series of independent exhibitions from the official Salon in 1874, were beginning to lose their solidarity. Also known as the \"Independents\" or \"Intransigents\", the group which at times included Degas, Monet, Sisley, Caillebotte, Pissarro, Renoir, and Berthe Morisot, had been receiving the wrath of the critics for several years. Their art, though varying in style and technique, was the antithesis of the type of Academic art in which Beaux was trained and of which her teacher William-Adolphe Bouguereau was a leading master. In the summer of 1888, with classes in summer recess, Beaux worked in the fishing village of Concarneau with the American painters Alexander Harrison and Charles Lazar. She tried applying the plein-air painting techniques used by the Impressionists to her own landscapes and portraiture, with little success. Unlike her predecessor Mary Cassatt, who had arrived near the beginning of the Impressionist movement 15 years earlier and who had absorbed it, Beaux's artistic temperament, precise and true to observation, would not align with Impressionism and she remained a realist painter for the rest of her career, even as C\u00e9zanne, Matisse, Gauguin, and Picasso were beginning to take art into new directions. Beaux mostly admired classic artists like Titian and Rembrandt. Her European training did influence her palette, however, and she adopted more white and paler coloration in her oil painting, particularly in depicting female subjects, an approach favored by Sargent as well.\nReturn to Philadelphia.\nBack in the United States in 1889, Beaux proceeded to paint portraits in the grand manner, taking as her subjects members of her sister's family and of Philadelphia's elite. In making her decision to devote herself to art, she also thought it was best not to marry, and in choosing male company she selected men who would not threaten to sidetrack her career. She resumed life with her family, and they supported her fully, acknowledging her chosen path and demanding of her little in the way of household responsibilities, \"I was never once asked to do an errand in town, some bit of shopping\u2026so well did they understand.\" She developed a structured, professional routine, arriving promptly at her studio, and expected the same from her models.\nThe five years that followed were highly productive, resulting in over forty portraits. In 1890 she exhibited at the Paris Exposition, obtained in 1893 the gold medal of the Philadelphia Art Club, and also the Dodge prize at the New York National Academy of Design. She exhibited her work at the Palace of Fine Arts and The Woman's Building at the 1893 World's Columbian Exposition in Chicago, Illinois. Her portrait of \"The Reverend Matthew Blackburne Grier\" was particularly well-received, as was \"Sita and Sarita\", a portrait of her cousin Charles W. Leavitt's wife Sarah (Allibone) Leavitt in white, with a small black cat perched on her shoulder, both gazing out mysteriously. The mesmerizing effect prompted one critic to point out \"the witch-like weirdness of the black kitten\" and for many years, the painting solicited questions by the press. But the result was not pre-planned, as Beaux's sister later explained, \"Please make no mystery about it\u2014it was only an idea to put the black kitten on her cousin's shoulder. Nothing deeper.\" Beaux donated \"Sita and Sarita\" to the Mus\u00e9e du Luxembourg, but only after making a copy for herself. Another highly regarded portrait from that period is \"New England Woman\" (1895), a nearly all-white oil painting which was purchased by the Pennsylvania Academy of the Fine Arts.\nIn 1895, Beaux became the first woman to have a regular teaching position at the Pennsylvania Academy of the Fine Arts, where she instructed in portrait drawing and painting for the next twenty years. That rare type of achievement by a woman prompted one local newspaper to state, \"It is a legitimate source of pride to Philadelphia that one of its most cherished institutions has made this innovation.\" She was a popular instructor. In 1896, Beaux returned to France to see a group of her paintings presented at the Salon. Influential French critic M. Henri Rochefort commented, \"I am compelled to admit, not without some chagrin, that not one of our female artists\u2026is strong enough to compete with the lady who has given us this year the portrait of Dr. Grier. Composition, flesh, texture, sound drawing\u2014everything is there without affectation, and without seeking for effect.\"\nIn 1898, Beaux painted probably her finest portrait, Man with the Cat (Henry Sturgis Drinker), now in Smithsonian American Art Museum. Drinker was Beaux's very successful brother-in-law.\nCecilia Beaux considered herself a \"New Woman\", a 19th-century woman who explored educational and career opportunities that had generally been denied to women. In the late 19th century Charles Dana Gibson depicted the \"New Woman\" in his painting, \"The Reason Dinner was Late\", which is \"a sympathetic portrayal of artistic aspiration on the part of young women\" as she paints a visiting policeman. This \"New Woman\" was successful, highly trained, and often did not marry; other such women included Ellen Day Hale, Mary Cassatt, Elizabeth Nourse and Elizabeth Coffin.\nBeaux was a member of Philadelphia's The Plastic Club. Other members included Elenore Abbott, Jessie Willcox Smith, Violet Oakley, Emily Sartain, and Elizabeth Shippen Green. Many of the women who founded the organization had been students of Howard Pyle. It was founded to provide a means to encourage one another professionally and create opportunities to sell their works of art.\nNew York City.\nBy 1900 the demand for Beaux's work brought clients from Washington, D.C., to Boston, prompting the artist to move to New York City, where she spent the winters, while summering at Green Alley, the home and studio she had built in Gloucester, Massachusetts. Beaux's friendship with Richard Gilder, editor-in-chief of the literary magazine \"The Century\", helped promote her career and he introduced her to the elite of society. Among her portraits which followed from that association are those of Georges Clemenceau; First Lady Edith Roosevelt and her daughter; and Admiral Sir David Beatty. She also sketched President Teddy Roosevelt during her White House visits in 1902, during which \"He sat for two hours, talking most of the time, reciting Kipling, and reading scraps of Browning.\" Beaux also became very close with Gilder's daughter Dorothea, and the two women exchanged affectionate letters for many years. Her portraits \"Fanny Travis Cochran\", \"Dorothea and Francesca\", and \"Ernesta and her Little Brother\", are fine examples of her skill in painting children; \"Ernesta with Nurse\", one of a series of essays in luminous white, was a highly original composition, seemingly without precedent. She became a member of the National Academy of Design in 1902. and won the Logan Medal of the arts at the Art Institute of Chicago in 1921.\nGreen Alley.\nBy 1906, Beaux began to live year-round at Green Alley, in a comfortable colony of \"cottages\" belonging to her wealthy friends and neighbors. All three aunts had died and she needed an emotional break from Philadelphia and New York City. She managed to find new subjects for portraiture, working in the mornings and enjoying a leisurely life the rest of the time. She carefully regulated her energy and her activities to maintain a productive output, and considered that a key to her success. On why so few women succeeded in art as she did, she stated, \"Strength is the stumbling block. They (women) are sometimes unable to stand the hard work of it day in and day out. They become tired and cannot reenergize themselves.\"\nWhile Beaux stuck to her portraits of the elite, American art was advancing into urban and social subject matter, led by artists such as Robert Henri who espoused a totally different aesthetic, \"Work with great speed..Have your energies alert, up and active. Do it all in one sitting if you can. In one minute if you can. There is no use delaying\u2026Stop studying water pitchers and bananas and paint everyday life.\" He advised his students, among them Edward Hopper and Rockwell Kent, to live with the common man and paint the common man, in total opposition to Cecilia Beaux's artistic methods and subjects. The clash of Henri and William Merritt Chase (representing Beaux and the traditional art establishment) resulted in 1907 in the independent exhibition by the urban realists known as \"The Eight\" or the Ashcan School. Beaux and her art friends defended the old order, and many thought (and hoped) the new movement to be a passing fad, but it turned out to be a revolutionary turn in American art.\nIn 1910, her beloved Uncle Willie died. Though devastated by the loss, at 55 year old, Beaux remained highly productive. In the next five years she painted almost 25 percent of her lifetime output and received a steady stream of honors. She had a major exhibition of 35 paintings at the Corcoran Gallery of Art in Washington, D.C., in 1912. Despite her continuing production and accolades, however, Beaux was working against the current of tastes and trends in art. The famed \"Armory Show\" of 1913 in New York City was a landmark presentation of 1,200 paintings showcasing Modernism. Beaux believed that the public, initially of mixed opinion about the \"new\" art, would ultimately reject it and return its favor to the Pre-Impressionists.\nBeaux was crippled after breaking her hip while walking in Paris in 1924. With her health impaired, her work output dwindled for the remainder of her life. That same year Beaux was asked to produce a self-portrait for the Medici collection in the Uffizi Gallery in Florence. In 1930 she published an autobiography, \"Background with Figures\". Her later life was filled with honors. In 1930 she was elected a member of the National Institute of Arts and Letters; in 1933 came membership in the American Academy of Arts and Letters, which two years later organized the first major retrospective of her work. Also in 1933 Eleanor Roosevelt honored Beaux as \"the American woman who had made the greatest contribution to the culture of the world\". In 1942 The National Institute of Arts and Letters awarded her a gold medal for lifetime achievement.\nDeath.\nBeaux died at the age of 87 on September 17, 1942, in Gloucester, Massachusetts. She was buried at West Laurel Hill Cemetery in Bala Cynwyd, Pennsylvania. In her will she left a Duncan Phyfe rosewood secretaire made for her father to her cherished nephew Cecil Kent Drinker, a Harvard University physician whom she had painted as a young boy and who later founded the Harvard School of Public Health.\nLegacy.\nBeaux was included in the 2018 exhibit \"Women in Paris 1850-1900\" at the Clark Art Institute.\nThough Beaux was an individualist, comparisons to Sargent would prove inevitable, and often favorable. Her strong technique, her perceptive reading of her subjects, and her ability to flatter without falsifying, were traits similar to his.\n\"The critics are very enthusiastic. (Bernard) Berenson, Mrs. Coates tells me, stood in front of the portraits \u2013 Miss Beaux's three \u2013 and wagged his head. 'Ah, yes, I see!' Some Sargents. The ordinary ones are signed John Sargent, the best are signed Cecilia Beaux, which is, of course, nonsense in more ways than one, but it is part of the generous chorus of praise.\" Though overshadowed by Mary Cassatt and relatively unknown to museum-goers today, Beaux's craftsmanship and extraordinary output were highly regarded in her time. While presenting the Carnegie Institute's Gold Medal to Beaux in 1899, William Merritt Chase stated \"Miss Beaux is not only the greatest living woman painter, but the best that has ever lived. Miss Beaux has done away entirely with sex [gender] in art.\"\nDuring her long productive life as an artist, she maintained her personal aesthetic and high standards against all distractions and countervailing forces. She constantly struggled for perfection. \"A perfect technique in anything,\" she stated in an interview, \"means that there has been no break in continuity between the conception and the act of performance.\" She summed up her driving work ethic, \"I can say this: When I attempt anything, I have a passionate determination to overcome every obstacle\u2026And I do my own work with a refusal to accept defeat that might almost be called painful.\""}
{"id": "6882", "revid": "1272981063", "url": "https://en.wikipedia.org/wiki?curid=6882", "title": "Chrysler", "text": "FCA US, LLC, doing business as Stellantis North America and known historically as Chrysler ( ), is one of the \"Big Three\" automobile manufacturers in the United States, headquartered in Auburn Hills, Michigan. It is the American subsidiary of the multinational automotive company Stellantis. Stellantis North America sells vehicles worldwide under the Chrysler, Dodge, Jeep, and Ram Trucks nameplates. It also includes Mopar, its automotive parts and accessories division, and SRT, its performance automobile division. The division also distributes Alfa Romeo, Fiat, and Maserati vehicles in North America.\nThe original Chrysler Corporation was founded in 1925 by Walter Chrysler from the remains of the Maxwell Motor Company. In 1998, it merged with Daimler-Benz, which renamed itself DaimlerChrysler but in 2007 sold off its Chrysler stake. The company operated as Chrysler LLC through 2009, then as Chrysler Group LLC. In 2014, it was acquired by Fiat S.p.A.; it subsequently operated as a subsidiary of the new Fiat Chrysler Automobiles (\"FCA\"), then as a subsidiary of Stellantis, the company formed from the 2021 merger of FCA and PSA Group (Peugeot Soci\u00e9t\u00e9 Anonyme).\nAfter founding the company, Walter Chrysler used the General Motors brand diversification and hierarchy strategy that he had become familiar with when he worked in the Buick division at General Motors. He then acquired Fargo Trucks and the Dodge Brothers Company, and created the Plymouth and DeSoto brands in 1928. Facing postwar declines in market share, productivity, and profitability, as GM and Ford were growing, Chrysler borrowed $250 million in 1954 from Prudential Insurance to pay for expansion and updated car designs.\nChrysler expanded into Europe by taking control of French, British, and Spanish auto companies in the 1960s; Chrysler Europe was sold in 1978 to PSA Peugeot Citro\u00ebn for a nominal $1. The company struggled to adapt to changing markets, increased U.S. import competition, and safety and environmental regulation in the 1970s. It began an engineering partnership with Mitsubishi Motors, and began selling Mitsubishi vehicles branded as Dodge and Plymouth in North America. On the verge of bankruptcy in the late 1970s, it was saved by $1.5 billion in loan guarantees from the U.S. government. New CEO Lee Iacocca was credited with returning the company to profitability in the 1980s. In 1985, Diamond-Star Motors was created, further expanding the Chrysler-Mitsubishi relationship. In 1987, Chrysler acquired American Motors Corporation (AMC), which brought the profitable Jeep, as well as the newly formed Eagle, brands under the Chrysler umbrella. In 1998, Chrysler merged with German automaker Daimler-Benz to form DaimlerChrysler AG; the merger proved contentious with investors. As a result, Chrysler was sold to Cerberus Capital Management and renamed Chrysler LLC in 2007.\nLike the other Big Three automobile manufacturers, Chrysler was impacted by the automotive industry crisis of 2008\u20132010. The company remained in business through a combination of negotiations with creditors, filing for Chapter 11 bankruptcy reorganization on April 30, 2009, and participating in a bailout from the U.S. government through the Troubled Asset Relief Program. On June 10, 2009, Chrysler emerged from the bankruptcy proceedings with the United Auto Workers pension fund, Fiat S.p.A., and the U.S. and Canadian governments as principal owners. The bankruptcy resulted in Chrysler defaulting on over $4 billion in debts. In May 2011, Chrysler finished repaying its obligations to the U.S. government five years early, although the cost to the American taxpayer was $1.3 billion.\nOver the next few years, Fiat S.p.A. gradually acquired the other parties' shares. In January 2014, Fiat acquired the rest of Chrysler from the United Auto Workers retiree health trust, making Chrysler Group a subsidiary of Fiat S.p.A. In May 2014, Fiat Chrysler Automobiles was established by merging Fiat S.p.A. into the company. Chrysler Group LLC remained a subsidiary until December 15, 2014, when it was renamed FCA US LLC, to reflect the Fiat-Chrysler merger.\nAs a result of the merger between FCA and PSA, on 17 January 2021 it became a subsidiary of the Stellantis Group.\nHistory.\n1925\u20131998: Chrysler Corporation.\nThe Chrysler company was founded by Walter Chrysler on June 6, 1925, when the Maxwell Motor Company (est. 1904) was re-organized into the Chrysler Corporation. The company was headquartered in the Detroit enclave of Highland Park, where it remained until completing the move to its present Auburn Hills location in 1996.\nChrysler had arrived at the ailing Maxwell-Chalmers company in the early 1920s, hired to overhaul the company's troubled operations (after a similar rescue job at the Willys-Overland car company). In late 1923, production of the Chalmers automobile was ended.\nIn January 1924, Walter Chrysler launched the well-received Chrysler automobile. The Chrysler Six was designed to provide customers with an advanced, well-engineered car, at an affordable price. Elements of this car are traceable to a prototype which had been under development at Willys during Chrysler's tenure The original 1924 Chrysler included a carburetor air filter, high compression engine, full pressure lubrication, and an oil filter, features absent from most autos at the time. Among the innovations in its early years were the first practical mass-produced four-wheel hydraulic brakes, a system nearly completely engineered by Chrysler with patents assigned to Lockheed, and rubber engine mounts, called \"Floating Power\" to reduce vibration. Chrysler also developed a wheel with a ridged rim, designed to keep a deflated tire from flying off the wheel. This wheel was eventually adopted by the auto industry worldwide.\nThe Maxwell brand was dropped after the 1925 model year, with the new, lower-priced four-cylinder Chryslers introduced for the 1926 year being badge-engineered Maxwells. The advanced engineering and testing that went into Chrysler Corporation cars helped to push the company to the second-place position in U.S. sales by 1936, which it held until 1949.\nIn 1928, the Chrysler Corporation began dividing its vehicle offerings by price class and function. The Plymouth brand was introduced at the low-priced end of the market (created essentially by once again reworking and rebadging the Chrysler Series 50 four-cylinder model). At the same time, the DeSoto brand was introduced in the medium-price field. Also in 1928, Chrysler bought the Dodge Brothers automobile and truck company and continued the successful Dodge line of automobiles and Fargo range of trucks. By the mid-1930s, the DeSoto and Dodge divisions would trade places in the corporate hierarchy.\nThe Imperial name had been used since 1926 but was never a separate make, just the top-of-the-line Chrysler. However, in 1955, the company decided to offer it as its own make/brand and division to better compete with its rivals, Lincoln and Cadillac. This addition changed the company's traditional four-make lineup to five (in order of price from bottom to top): Plymouth, Dodge, DeSoto, Chrysler, and the now-separate Imperial.\nIn 1954, Chrysler was the exclusive provider of its Hemi engine in the Facel Vega, a Paris coachbuilder that offered their own line of hand-built luxury performance cars, with the PowerFlite and TorqueFlite transmissions offered. The Facel Vega Excellence was a four-door hardtop with rear-hinged coach doors that listed for US$12,800 ($ in dollars ).\nOn April 28, 1955, Chrysler and Philco announced the development and production of the World's First All-Transistor car radio. The all-transistor car radio, Mopar model 914HR, developed and produced by Chrysler and Philco, was a $150 option on the 1956 Imperial automobile models. Philco began manufacturing this radio in the fall of 1955 at its Sandusky Ohio plant.\nOn September 28, 1957, Chrysler announced the first production electronic fuel injection (EFI), as an option on some of its new 1958 car models (Chrysler 300D, Dodge D500, DeSoto Adventurer, Plymouth Fury). The first attempt to use this system was by American Motors on the 1957 Rambler Rebel. Bendix Corporation's Electrojector used a transistor computer brain modulator box, but teething problems on pre-production cars meant very few cars were made. The EFI system in the Rambler ran fine in warm weather, but suffered hard starting in cooler temperatures and AMC decided not to use this EFI system on its 1957 Rambler Rebel production cars that were sold to the public. Chrysler also used the Bendix \"Electrojector\" fuel injection system and only around 35 vehicles were built with this option, on its 1958 production-built car models. Owners of EFI Chryslers were so dissatisfied that all but one were retrofitted with carburetors (while that one has been completely restored, with original EFI electronic problems resolved).\nThe Valiant was also introduced for the 1960 model year as a distinct brand. In the U.S. market, Valiant was made a model in the Plymouth line for 1961 and the DeSoto make was discontinued in 1961. With those exceptions per applicable year and market, Chrysler's range from lowest to highest price from the 1940s through the 1970s was Valiant, Plymouth, Dodge, DeSoto, Chrysler, and Imperial.\nFrom 1963 through 1969, Chrysler increased its existing stakes to take full control of the French Simca, British Rootes, and Spanish Barreiros companies, merging them into Chrysler Europe in 1967. In the 1970s, an engineering partnership was established with Mitsubishi Motors, and Chrysler began selling Mitsubishi vehicles branded as Dodge and Plymouth in North America.\nChrysler struggled to adapt to the changing environment of the 1970s. When consumer tastes shifted to smaller cars in the early 1970s, particularly after the 1973 oil crisis, Chrysler could not meet the demand, although their compact models on the \"A\" body platform, the Dodge Dart and Plymouth Valiant, had proven economy and reliability and sold very well. Additional burdens came from increased US import competition, and tougher government regulation of car safety, fuel economy, and emissions. As the smallest of the Big 3 US automakers, Chrysler lacked the financial resources to meet all of these challenges. In 1976, with the demise of the reliable Dart/Valiant, quality control declined. Their replacements, the Dodge Aspen and Plymouth Volare, were comfortable and had good roadability, but owners soon experienced major reliability problems which crept into other models as well. Engines failed and/or did not run well, and premature rust plagued bodies. In 1978, Lee Iacocca was brought in to turn the company around, and in 1979 Iacocca sought US government help. Congress later passed the \"Loan Guarantee Act\" providing $1.5 billion in loan guarantees. The \"Loan Guarantee Act\" required that Chrysler also obtain $2 billion in concessions or aid from sources outside the federal government, which included interest rate reductions for $650 million of the savings, asset sales of $300 million, local and state tax concessions of $250 million, and wage reductions of about $590 million along with a $50 million stock offering. $180 million was to come from concessions from dealers and suppliers. Also in 1978, Iacocca offloaded the ailing European operation to PSA Peugeot Citro\u00ebn for a nominal $1, taking with it the group's substantial losses and debts which had been dragging the rest of the business down.\nAfter a period of plant closures and salary cuts agreed to by both management and the auto unions, the loans were repaid with interest in 1983. In November 1983, the Dodge Caravan/Plymouth Voyager was introduced, establishing the minivan as a major category, and initiating Chrysler's return to stability.\nIn 1985, Diamond-Star Motors was created, further expanding the Chrysler-Mitsubishi relationship.\nIn 1985, Chrysler entered an agreement with American Motors Corporation to produce Chrysler M platform rear-drive, as well as Dodge Omnis front wheel drive cars, in AMC's Kenosha, Wisconsin, plant. In 1987, Chrysler acquired the 47% ownership of AMC that was held by Renault. The remaining outstanding shares of AMC were bought on the NYSE by August 5, 1987, making the deal valued somewhere between US$1.7 billion and US$2 billion, depending on how costs were counted. Chrysler CEO Lee Iacocca wanted the Jeep brand, particularly the Jeep Grand Cherokee (ZJ) that was under development, the new world-class manufacturing plant in Bramalea, Ontario, and AMC's engineering and management talent that became critical for Chrysler's future success. Chrysler established the Jeep/Eagle division as a \"specialty\" arm to market products distinctly different from the K-car-based products with the Eagle cars targeting import buyers. Former AMC dealers sold Jeep vehicles and various new Eagle models, as well as Chrysler products, strengthening the automaker's retail distribution system.\nEurostar, a joint venture between Chrysler and Steyr-Daimler-Puch, began producing the Chrysler Voyager in Austria for European markets in 1992.\n1998\u20132007: DaimlerChrysler.\nIn 1998, Chrysler and its subsidiaries entered into a partnership dubbed a \"merger of equals\" with German-based Daimler-Benz AG, creating the combined entity DaimlerChrysler AG. To the surprise of many stockholders, Daimler acquired Chrysler in a stock swap before Chrysler CEO Bob Eaton retired. Under DaimlerChrysler, the company was named DaimlerChrysler Motors Company LLC, with its U.S. operations generally called \"DCX\". The Eagle brand was retired soon after Chrysler's merger with Daimler-Benz in 1998 Jeep became a stand-alone division, and efforts were made to merge the Chrysler and Jeep brands as one sales unit. In 2001, the Plymouth brand was also discontinued.\nEurostar also built the Chrysler PT Cruiser in 2001 and 2002. The Austrian venture was sold to Magna International in 2002 and became Magna Steyr. The Voyager continued in production until 2007, whereas the Chrysler 300C, Jeep Grand Cherokee, and Jeep Commander were also built at the plant from 2005 until 2010.\nOn May 14, 2007, DaimlerChrysler announced the sale of 80.1% of Chrysler Group to American private equity firm Cerberus Capital Management, L.P., thereafter known as Chrysler LLC, although Daimler (renamed as Daimler AG) continued to hold a 19.9% stake.\n2007\u20132014: Effects of Great Recession.\nThe economic collapse during the Financial crisis of 2007\u20132008 pushed the company to the brink. On April 30, 2009, the automaker filed for Chapter 11 bankruptcy protection to be able to operate as a going concern, while renegotiating its debt structure and other obligations, which resulted in the corporation defaulting on over $4 billion in secured debts. The U.S. government described the company's action as a \"prepackaged surgical bankruptcy\".\nOn June 10, 2009, substantially all of Chrysler's assets were sold to \"New Chrysler\", organized as Chrysler Group LLC. The federal government provided support for the deal with US$8 billion in financing at nearly 21%. Under CEO Sergio Marchionne, \"World Class Manufacturing\" or WCM, a system of thorough manufacturing quality, was introduced and several products were re-launched with quality and luxury. The Ram, Jeep, Dodge, SRT, and Chrysler divisions were separated to focus on their own identity and brand, and 11 major model refreshes occurred in 21 months. The PT Cruiser, Nitro, Liberty and Caliber models (created during DCX) were discontinued. On May 24, 2011, Chrysler repaid its $7.6 billion loans to the United States and Canadian governments. The US Treasury, through the Troubled Asset Relief Program (TARP), invested $12.5 billion in Chrysler and recovered $11.2 billion when the company shares were sold in May 2011, resulting in a $1.3 billion loss. On July 21, 2011, Fiat bought the Chrysler shares held by the US Treasury. The purchase made Chrysler foreign-owned again, this time as the luxury division. The Chrysler 300 was badged Lancia Thema in some European markets (with additional engine options), giving Lancia a much-needed replacement for its flagship.\n2014\u20132021: Fiat Chrysler Automobiles.\nOn January 21, 2014, Fiat bought the remaining shares of Chrysler owned by the VEBA worth $3.65 billion. Several days later, the intended reorganization of Fiat and Chrysler under a new holding company, Fiat Chrysler Automobiles, together with a new FCA logo were announced. The most challenging launch for this new company came immediately in January 2014 with a completely redesigned Chrysler 200. The vehicle's creation is from the completely integrated company, FCA, executing from a global compact-wide platform.\nOn December 16, 2014, Chrysler Group LLC announced a name change to FCA US LLC.\nOn January 12, 2017, FCA shares traded at the New York Stock Exchange lost value after the EPA accused FCA US of using emissions cheating software to evade diesel-emissions tests, however the company countered the accusations, and the chairman and CEO Sergio Marchionne sternly rejected them. The following day, shares rose as investors played down the effect of the accusations. Analysts gave estimates of potential fines from several hundred million dollars to $4 billion, although the likelihood of a hefty fine was low. Senior United States Senator Bill Nelson urged the FTC to look into possible deceptive marketing of the company's diesel-powered SUVs. Shares dropped 2.2% after the announcement. FCA US would in 2022, plead guilty to a criminal charge of conspiring to defraud the US, to wire fraud, and to violate the Clean Air Act.\nOn July 21, 2018, Sergio Marchionne stepped down as chairman and CEO for health reasons, and was replaced by John Elkann and Michael Manley, respectively.\nAs a result of ending domestic production of more fuel-efficient passenger automobiles such as the Dodge Dart and Chrysler 200 sedans, FCA US elected to pay $77 million in fines for violating the anti-backsliding provision of fuel economy standards set under the Energy Independence and Security Act of 2007 for its model year 2016 fleet. It was again fined for the 2017 model year for not meeting the minimum domestic passenger car standard. FCA described the $79 million civil penalty as \"not expected to have a material impact on its business.\"\nAs part of a January 2019 settlement, Fiat Chrysler was to recall and repair approximately 100,000 automobiles equipped with a 3.0-liter V6 EcoDiesel engine having a prohibited defeat device, pay $311 million in total civil penalties to US regulators and CARB, pay $72.5 million for state civil penalties, implement corporate governance reforms, and pay $33.5 million to mitigate excess pollution. The company was also to pay affected consumers up to $280 million and offer extended warranties on such vehicles worth $105 million. The total value of the settlement was about $800 million, though FCA did not admit liability, and it did not resolve an ongoing criminal investigation.\nIn February 2024, Chrysler unveiled a concept for its first electric vehicle, the Chrysler Halcyon, a battery-electric sedan.\nCorporate governance.\n, management positions of Stellantis North America include:\nSales and marketing.\nUnited States sales.\nChrysler is the smallest of the \"Big Three\" U.S. automakers (Stellantis North America, Ford Motor Company, and General Motors). In 2020, FCA US sold just over 1.8 million vehicles.\nGlobal sales.\nChrysler was the world's 11th largest vehicle manufacturer as ranked by OICA in 2012. Total Chrysler vehicle production was about 2.37 million that year. The company has since become a wholly-owned subsidiary and no longer reports global sales.\nMarketing.\nLifetime powertrain warranty.\nIn 2007, Chrysler began to offer vehicle lifetime powertrain warranty for the first registered owner or retail lessee. The deal covered owner or lessee in U.S., Puerto Rico, and the Virgin Islands, for 2009 model year vehicles, and 2006, 2007, and 2008 model year vehicles purchased on or after July 26, 2007. Covered vehicles excluded SRT models, Diesel vehicles, Sprinter models, Ram Chassis Cab, Hybrid System components (including transmission), and certain fleet vehicles. The warranty is non-transferable. After Chrysler's restructuring, the warranty program was replaced by five-year/100,000 mile transferable warranty for 2010 or later vehicles.\n\"Let's Refuel America\".\nIn 2008, as a response to customer feedback citing the prospect of rising gas prices as a top concern, Chrysler launched the \"Let's Refuel America\" incentive campaign, which guaranteed new-car buyers a gasoline price of $2.99 for three years. With the U.S. purchase of eligible Chrysler, Jeep, and Dodge vehicles, customers could enroll in the program and receive a gas card that immediately lowers their gas price to $2.99 a gallon, and keeps it there for the three years.\nLancia co-branding.\nChrysler plans for Lancia to codevelop products, with some vehicles being shared. Olivier Francois, Lancia's CEO, was appointed to the Chrysler division in October 2009. Francois plans to reestablish the Chrysler brand as an upscale brand.\nRam trucks.\nIn October 2009, Dodge's car and truck lines were separated, with the name \"Dodge\" being used for cars, minivans, and crossovers and \"Ram\" for light- and medium-duty trucks and other commercial-use vehicles.&lt;ref name=\"autoblog.com/2009\"&gt;&lt;/ref&gt;\n\"Imported From Detroit\".\nIn 2011, Chrysler unveiled their \"Imported From Detroit\" campaign with ads featuring Detroit rapper Eminem, one of which aired during the Super Bowl. The campaign highlighted the rejuvenation of the entire product lineup, which included the new, redesigned, and repackaged 2011 model year 200 sedans and 200 convertibles, the Chrysler 300 sedan, and the Chrysler Town &amp; Country minivan. As part of the campaign, Chrysler sold a line of clothing items featuring the Monument to Joe Louis, with proceeds being funneled to Detroit-area charities, including the Boys and Girls Clubs of Southeast Michigan, Habitat for Humanity Detroit and the Marshall Mathers Foundation.\nIn March 2011, Chrysler Group LLC filed a lawsuit against Moda Group LLC (owner of Pure Detroit clothing retailer) for copying and selling merchandise with the \"Imported from Detroit\" slogan. Chrysler claimed it had notified defendant of its pending trademark application February 14, but the defendant argued Chrysler had not secured a trademark for the \"Imported From Detroit\" phrase. On June 18, 2011, U.S. District Judge Arthur Tarnow ruled that Chrysler's request did not show that it would suffer irreparable harm or that it had a strong likelihood of winning its case. Therefore, Pure Detroit's owner, Detroit retailer Moda Group LLC, can continue selling its \"Imported from Detroit\" products. Tarnow also noted that Chrysler does not have a trademark on \"Imported from Detroit\" and rejected the automaker's argument that trademark law is not applicable to the case. In March 2012, Chrysler Group LLC and Pure Detroit agreed to a March 27 mediation to try to settle the lawsuit over the clothing company's use of \"Imported from Detroit\" slogan. Pure Detroit stated that Chrysler has made false claims about the origins of three vehicles - Chrysler 200, Chrysler 300 and Chrysler Town &amp; Country - none of which are built in Detroit. Pure Detroit also said that Chrysler's Imported From Detroit merchandise is not being made in Detroit. In 2012 Chrysler and Pure Detroit came to an undisclosed settlement.\nChrysler's Jefferson North Assembly, which makes the Jeep Grand Cherokee and Dodge Durango, is the only car manufacturing plant of any company remaining entirely in Detroit (General Motors operates a plant that is partly in Detroit and partly in Hamtramck).\nIn 2011, Eminem settled a lawsuit against Audi alleging the defendant had ripped off the Chrysler 300 Super Bowl commercial in the Audi A6 Avant ad.\n\"Halftime in America\".\nAgain in 2012, Chrysler advertised during the Super Bowl. Its two-minute February 5, 2012 Super Bowl XLVI advertisement was titled \"Halftime in America\". The ad drew criticism from several leading U.S. conservatives, who suggested that its messaging implied that President Barack Obama deserved a second term and, as such, was political payback for Obama's support for the federal bailout of the company. Asked about the criticism in a \"60 Minutes\" interview with Steve Kroft, Sergio Marchionne responded \"just to rectify the record I paid back the loans at 19.7% Interest. I don't think I committed to do to a commercial on top of that\" and characterized the Republican reaction as \"unnecessary and out of place\".\nAmerica's Import.\nIn 2014, Chrysler started using a new slogan, \"America's Import\" in ads introducing their all-new 2015 Chrysler 200, targeting foreign automakers from Germany to Japan with such ads (German performance and Japanese quality), and at the ending of selected ads, the advertisement will say, \"We Built This\", indicating being built in America, instead of overseas.\nProduct line.\nChrysler Uconnect.\nFirst introduced as MyGig, Chrysler Uconnect is a system that brings interactive ability to the in-car radio and telemetric-like controls to car settings. As of mid-2015, it was installed in hundreds of thousands of Fiat Chrysler vehicles. It connects to the Internet via the mobile network of AT&amp;T, providing the car with its own IP address. Internet connectivity using any Chrysler, Dodge, Jeep or Ram vehicle, via a Wi-Fi \"hot-spot\", is also available via Uconnect Web. According to Chrysler LLC, the hotspot range extends approximately from the vehicle in all directions, and combines both Wi-Fi and Sprint's 3G cellular connectivity. Uconnect is available on several current and was available on several discontinued Chrysler models including the current Dodge Dart, Chrysler 300, Aspen, Sebring, Town and Country, Dodge Avenger, Caliber, Grand Caravan, Challenger, Charger, Journey, Nitro, and Ram.\nIn July 2015, IT security researchers announced a severe security flaw assumed to affect every Chrysler vehicle with Uconnect produced from late 2013 to early 2015. It allows hackers to gain access to the car over the Internet, and in the case of a Jeep Cherokee was demonstrated to enable an attacker to take control not just of the radio, A/C, and windshield wipers, but also of the car's steering, brakes and transmission. Chrysler published a patch that car owners can download and install via a USB stick, or have a car dealer install for them.\nBrands.\nCurrent and former brands of Stellantis North America:\nBrand predecessors.\nUnited States Motor Company.\n(1908\u20131913); reorganized and folded into Maxwell\nRootes Group.\n(1913\u20131971), UK; minority interest purchased by Chrysler in 1964, progressively taking controlling interest in 1967, renamed Chrysler Europe in 1971\nAmerican Motors Corporation.\n(1954\u20131988), US; purchased by Chrysler and renamed Jeep-Eagle Division\nGraham-Paige.\n(1927\u20131947), mid-priced cars; purchased by Henry Kaiser and reorganized into Kaiser-Frazer Motors\nWillys-Overland Motors.\n(1912\u20131963) US; acquired by Kaiser Motors, later Kaiser Jeep, then by AMC in 1970\nEnvironmental initiatives.\nIn 1979, Chrysler, in cooperation with the United States Department of Energy, produced an experimental battery electric vehicle, the Chrysler ETV-1.\nIn 1992, Chrysler developed the Dodge EPIC concept minivan. In 1993, Chrysler sold a limited-production electric minivan called the TEVan; only 56 were produced, mostly for electric utilities. A second generation, the EPIC (unrelated to the concept), was released in 1997 and discontinued in 1999.\nChrysler once owned the Global Electric Motorcars company, building low-speed neighborhood electric vehicles, but sold GEM to Polaris Industries in 2011.\nIn September 2007, Chrysler established ENVI, an in-house organization focused on electric-drive vehicles and related technologies which was disbanded by late 2009. In August 2009, Chrysler took US$70 million in grants from the U.S. Department of Energy to develop a test fleet of 220 hybrid pickup trucks and minivans.\nThe first hybrid models, the Chrysler Aspen hybrid and the Dodge Durango hybrid, were discontinued a few months after production in 2008, sharing their GM-designed hybrid technology with GM, Daimler and BMW.\nChrysler was on the Advisory Council of the PHEV Research Center, and undertook a government sponsored demonstration project with Ram and minivan vehicles.\nIn 2012, FCA CEO Sergio Marchionne stated that Chrysler and Fiat planned to focus primarily on alternative fuels, such as compressed natural gas and Diesel, instead of hybrid and electric drivetrains for their consumer products.\nFiat Chrysler bought a total of 8.2 million megagrams of U.S. greenhouse gas emission credits from competitors including Toyota, Honda, Tesla and Nissan for the 2010, 2011, 2013, and 2014 model years. It had the worst fleet average fuel economy among major manufacturers selling in the US from model years 2012\u20132022.\nChrysler Defense.\nThe dedicated tank building division of Chrysler, this division was founded as the Chrysler Tank division in 1940, originally with the intention of providing another production line for the M2 Medium Tank, so that the U.S. Army could more rapidly build up its inventory of the type. Its first plant was the Detroit Arsenal Tank Plant. When the M2A1 was unexpectedly declared obsolete in August of the same year, plans were altered (though not without considerable difficulty) to produce the M3 Grant instead, primarily for the British as part of the United States under the counter support for the United Kingdom against Nazi Germany (the U.S. not yet being formally in the war), with the balance of the revised order going to the U.S. Army as the \"Lee\". After December 1941 and the United States' entry into the war against the Axis powers, the Tank division rapidly expanded, with new facilities such as the Tank Arsenal Proving Ground at (then) Utica, Michigan. It also quickly widened the range of products it was developing and producing, including the M4 Sherman tank and the Chrysler A57 multibank tank engine.\nSpecial programs.\nDuring World War II, essentially all of Chrysler's facilities were devoted to building military vehicles (the Jeep brand came later, after Chrysler acquired American Motors Corporation). They were also designing V12 and V16 hemi-engines producing for airplanes, but they did not make it into production as jets were developed and were seen as the future for air travel. During the 1950s Cold War period, Chrysler made air raid sirens powered by its Hemi V-8 engines.\nRadar antennas.\nWhen the Radiation Laboratory at MIT was established in 1941 to develop microwave radars, one of the first projects resulted in the SCR-584, the most widely recognized radar system of the war era. This system included a parabolic antenna six feet in diameter that was mechanically aimed in a helical pattern (round and round as well as up and down).\nOne of Chrysler's most significant contributions to the war effort was in radar technology. For the final production design of this antenna and its highly complex drive mechanism, the Army's Signal Corps Laboratories turned to Chrysler's Central Engineering Office. There, the parabola was changed from aluminum to steel, allowing production to form using standard automotive presses. To keep weight down, 6,000 equally spaced holes were drilled in the face (this had no effect on the radiation pattern). The drive mechanism was completely redesigned, using technology derived from Chrysler's research in automotive gears and differentials. The changes resulted in improved performance, reduced weight, and easier maintenance. A large portion of the Dodge plant was used in building 1,500 of the SCR-584 antennas as well as the vans used in the systems.\nMissiles.\nIn April 1950, the U.S. Army established the Ordnance Guided Missile Center (OGMC) at Redstone Arsenal, adjacent to Huntsville, Alabama. To form OGMC, over 1,000 civilian and military personnel were transferred from Fort Bliss, Texas. Included was a group of German scientists and engineers led by Wernher von Braun; this group had been brought to America under Project Paperclip. OGMC designed the Army's first short-range ballistic missile, the PGM-11 Redstone, based on the WWII German V-2 missile. Chrysler established the Missile Division to serve as the Redstone prime contractor, setting up an engineering operation in Huntsville and for production obtaining use from the U.S. Navy of a large plant in Sterling Heights, Michigan. The Redstone was in active service from 1958 until 1964; it was also the first missile to test-launch a live nuclear weapon, first detonated in a 1958 test in the South Pacific.\nWorking together, the Missile Division and von Braun's team greatly increased the capability of the Redstone, resulting in the PGM-19 Jupiter, a medium-range ballistic missile. In May 1959, a Jupiter missile launched two small monkeys into space in a nose cone; this was America's first successful flight and recovery of live space payloads. Responsibility for deploying Jupiter missiles was transferred from the Army to the Air Force; armed with nuclear warheads, they were first deployed in Italy and Turkey during the early 1960s.\nSpace boosters.\nIn July 1959, NASA chose the Redstone missile as the basis for the Mercury-Redstone Launch Vehicle to be used for suborbital test flights of the Project Mercury spacecraft. Three uncrewed MRLV launch attempts were made between November 1960 and March 1961, two of which were successful. The MRLV successfully launched the chimpanzee Ham, and astronauts Alan Shepard and Gus Grissom on three suborbital flights in January, May, and July 1961, respectively.\nAmerica's more ambitious crewed space travel plans included the design of the Saturn series of heavy-lift launch vehicles by a team headed by Wernher von Braun. Chrysler's Huntsville operation, then designated the Space Division, became Marshall Space Flight Center's prime contractor for the first stage of the Saturn I and Saturn IB versions. The design was based on a cluster of Redstone and Jupiter fuel tanks and Chrysler built it for the Apollo program in the Michoud Assembly Facility in East New Orleans, one of the largest manufacturing plants in the world. Between October 1961 and July 1975, NASA used ten Saturn Is and nine Saturn IBs for suborbital and orbital flights, all of which were successful; Chrysler missiles and boosters never suffered a launch failure. The division was also a subcontractor which modified one of the mobile launcher platforms for use with the Saturn IB rockets using Saturn V infrastructure."}
{"id": "6883", "revid": "40307312", "url": "https://en.wikipedia.org/wiki?curid=6883", "title": "City of London", "text": "The City of London, also known as the City, is a city, ceremonial county and local government district that contains the ancient centre, and constitutes, along with Canary Wharf, the primary central business district (CBD) of London and one of the leading financial centres of the world. It constituted most of London from its settlement by the Romans in the 1st century AD to the Middle Ages, but the modern area referred to as London has since grown far beyond the City of London boundary. The City is now only a small part of the metropolis of Greater London, though it remains a notable part of central London. The City of London is not one of the London boroughs, a status reserved for the other 32 districts (including Greater London's only other city, the City of Westminster). It is also a separate ceremonial county, being an enclave surrounded by the ceremonial county of Greater London, and is the smallest ceremonial county in England.\nThe City of London is known colloquially as the Square Mile, as it is in area. Both the terms \"the City\" and the \"Square Mile\" are often used as metonyms for the UK's trading and financial services industries, which continue a notable history of being largely based in the City. The name \"London\" is now ordinarily used for a far wider area than just the City. \"London\" most often denotes the sprawling London metropolis, or the 32 Greater London boroughs, in addition to the City of London itself. \nThe local authority for the City, namely the City of London Corporation, is unique in the UK and has some unusual responsibilities for a local council, such as being the police authority. It is also unusual in having responsibilities and ownerships beyond its boundaries, e.g. Hampstead Heath. The corporation is headed by the Lord Mayor of the City of London (an office separate from, and much older than, the Mayor of London). The current Lord Mayor is Alastair King. The City is made up of 25 wards, with administration at the historic Guildhall. Other historic sites include St Paul's Cathedral, Royal Exchange, Mansion House, Old Bailey, and Smithfield Market. Although not within the City, the adjacent Tower of London, built to dominate the City, is part of its old defensive perimeter. The City has responsibility for five bridges across the Thames in its capacity as trustee of the Bridge House Estates: Blackfriars Bridge, Millennium Bridge, Southwark Bridge, London Bridge and Tower Bridge.\nThe City is a major business and financial centre, with both the Bank of England and the London Stock Exchange based in the City. Throughout the 19th century, the City was the world's primary business centre, and it continues to be a major meeting point for businesses. London was ranked second (after New York) in the Global Financial Centres Index, published in 2022. The insurance industry is concentrated in the eastern side of the city, around Lloyd's building. Since about the 1980s, a secondary financial district has existed outside the city, at Canary Wharf, to the east. The legal profession has a major presence in the northern and western sides of the City, especially in the Temple and Chancery Lane areas where the Inns of Court are located, two of which (Inner Temple and Middle Temple) fall within the City of London boundary.\nPrimarily a business district, the City has a small resident population of 8,583 based on 2021 census figures, but over 500,000 are employed there (as of 2019) and some estimates put the number of workers in the City to be over 1 million. About three-quarters of the jobs in the City of London are in the financial, professional, and associated business services sectors.\nHistory.\nOrigins.\nThe Roman legions established a settlement known as \"Londinium\" on the current site of the City of London around AD 43. Its bridge over the River Thames turned the city into a road nexus and major port, serving as a major commercial centre in Roman Britain until its abandonment during the 5th century. Archaeologist Leslie Wallace notes that, because extensive archaeological excavation has not revealed any signs of a significant pre-Roman presence, \"arguments for a purely Roman foundation of London are now common and uncontroversial.\"\nAt its height, the Roman city had a population of approximately 45,000\u201360,000 inhabitants. Londinium was an ethnically diverse city, with inhabitants from across the Roman Empire, including natives of Britannia, continental Europe, the Middle East, and North Africa. The Romans built the London Wall some time between AD 190 and 225. The boundaries of the Roman city were similar to those of the City of London today, though the City extends further west than Londinium's Ludgate, and the Thames was undredged and thus wider than it is today, with Londinium's shoreline slightly north of the city's present shoreline. The Romans built a bridge across the river, as early as AD 50, near to today's London Bridge.\nDecline.\nBy the time the London Wall was constructed, the city's fortunes were in decline, and it faced problems of plague and fire. The Roman Empire entered a long period of instability and decline, including the Carausian Revolt in Britain. In the 3rd and 4th centuries, the city was under attack from Picts, Scots, and Saxon raiders. The decline continued, both for Londinium and the Empire, and in AD 410 the Romans withdrew entirely from Britain. Many of the Roman public buildings in Londinium by this time had fallen into decay and disuse, and gradually after the formal withdrawal the city became almost (if not, at times, entirely) uninhabited. The centre of trade and population moved away from the walled Londinium to Lundenwic (\"London market\"), a settlement to the west, roughly in the modern-day Strand/Aldwych/Covent Garden area.\nAnglo-Saxon restoration.\nDuring the Anglo-Saxon Heptarchy, the London area came in turn under the Kingdoms of Essex, Mercia, and later Wessex, though from the mid 8th century it was frequently under threat from raids by different groups including the Vikings.\nBede records that in AD 604 St Augustine consecrated Mellitus as the first bishop to the Anglo-Saxon kingdom of the East Saxons and their king, S\u00e6berht. S\u00e6berht's uncle and overlord, \u00c6thelberht, king of Kent, built a church dedicated to St Paul in London, as the seat of the new bishop. It is assumed, although unproven, that this first Anglo-Saxon cathedral stood on the same site as the later medieval and the present cathedrals.\nAlfred the Great, King of Wessex occupied and began the resettlement of the old Roman walled area, in 886, and appointed his son-in-law Earl \u00c6thelred of Mercia over it as part of their reconquest of the Viking occupied parts of England. The refortified Anglo-Saxon settlement was known as Lundenburh (\"London Fort\", a borough). The historian Asser said that \"Alfred, king of the Anglo-Saxons, restored the city of London splendidly\u00a0... and made it habitable once more.\" Alfred's \"restoration\" entailed reoccupying and refurbishing the nearly deserted Roman walled city, building quays along the Thames, and laying a new city street plan.\nAlfred's taking of London and the rebuilding of the old Roman city was a turning point in history, not only as the permanent establishment of the City of London, but also as part of a unifying moment in early England, with Wessex becoming the dominant English kingdom and the repelling (to some degree) of the Viking occupation and raids. While London, and indeed England, were afterwards subjected to further periods of Viking and Danish raids and occupation, the establishment of the City of London and the Kingdom of England prevailed.\nIn the 10th century, Athelstan permitted eight mints to be established, compared with six in his capital, Winchester, indicating the wealth of the city. London Bridge, which had fallen into ruin following the Roman evacuation and abandonment of Londinium, was rebuilt by the Saxons, but was periodically destroyed by Viking raids and storms.\nAs the focus of trade and population was moved back to within the old Roman walls, the older Saxon settlement of Lundenwic was largely abandoned and gained the name of \"Ealdwic\" (the \"old settlement\"). The name survives today as Aldwych (the \"old market-place\"), a name of a street and an area of the City of Westminster between Westminster and the City of London.\nMedieval era.\nFollowing the Battle of Hastings, William the Conqueror marched on London, reaching as far as Southwark, but failed to get across London Bridge or defeat the Londoners. He eventually crossed the River Thames at Wallingford, pillaging the land as he went. Rather than continuing the war, Edgar the \u00c6theling, Edwin of Mercia and Morcar of Northumbria surrendered at Berkhamsted. William granted the citizens of London a charter in 1075; the city was one of a few examples of the English retaining some authority. The city was not covered by the Domesday Book.\nWilliam built three castles around the city, to keep Londoners subdued:\nAround 1132 the City was given the right to appoint its own sheriffs rather than having sheriffs appointed by the monarch. London's chosen sheriffs also served as the sheriffs for the county of Middlesex. This meant that the City and Middlesex were regarded as one administratively for addressing crime and keeping the peace (not that the county was a dependency of the city). London's sheriffs continued to serve Middlesex until the county was given its own sheriffs again following the Local Government Act 1888. By 1141 the whole body of the citizenry was considered to constitute a single community. This 'commune' was the origin of the City of London Corporation and the citizens gained the right to appoint, with the king's consent, a mayor in 1189\u2014and to directly elect the mayor from 1215.\nFrom medieval times, the city has been composed of 25 ancient wards, each headed by an alderman, who chairs Wardmotes, which still take place at least annually. A Folkmoot, for the whole of the City held at the outdoor cross of St Paul's Cathedral, was formerly also held. Many of the medieval offices and traditions continue to the present day, demonstrating the unique nature of the City and its Corporation.\nIn 1381, the Peasants' Revolt affected London. The rebels took the City and the Tower of London, but the rebellion ended after its leader, Wat Tyler, was killed during a confrontation that included Lord Mayor William Walworth. In 1450, rebel forces again occupied the City during Jack Cade's Rebellion before being ousted by London citizens following a bloody battle on London Bridge. In 1550, the area south of London Bridge in Southwark came under the control of the City with the establishment of the ward of Bridge Without. \nThe city was burnt severely on a number of occasions, the worst being in 1123 and in the Great Fire of London in 1666. Both of these fires were referred to as \"the\" Great Fire. After the fire of 1666, a number of plans were drawn up to remodel the city and its street pattern into a renaissance-style city with planned urban blocks, squares and boulevards. These plans were almost entirely not taken up, and the medieval street pattern re-emerged almost intact.\nEarly modern period.\nIn the 1630s the Crown sought to have the Corporation of the City of London extend its jurisdiction to surrounding areas. In what is sometimes called the \"great refusal\", the Corporation said no to the King, which in part accounts for its unique government structure to the present.\nBy the late 16th century, London increasingly became a major centre for banking, international trade and commerce. The Royal Exchange was founded in 1565 by Sir Thomas Gresham as a centre of commerce for London's merchants, and gained Royal patronage in 1571. Although no longer used for its original purpose, its location at the corner of Cornhill and Threadneedle Street continues to be the geographical centre of the city's core of banking and financial services, with the Bank of England moving to its present site in 1734, opposite the Royal Exchange. Immediately to the south of Cornhill, Lombard Street was the location from 1691 of Lloyd's Coffee House, which became the world-leading insurance market. London's insurance sector continues to be based in the area, particularly in Lime Street.\nIn 1708, Christopher Wren's masterpiece, St Paul's Cathedral, was completed on his birthday. The first service had been held on 2 December 1697, more than 10 years earlier. It replaced the original St Paul's, which had been completely destroyed in the Great Fire of London, and is considered to be one of the finest cathedrals in Britain and a fine example of Baroque architecture.\nGrowth of London.\nThe 18th century was a period of rapid growth for London, reflecting an increasing national population, the early stirrings of the Industrial Revolution, and London's role at the centre of the evolving British Empire. The urban area expanded beyond the borders of the City of London, most notably during this period towards the West End and Westminster.\nExpansion continued and became more rapid by the beginning of the 19th century, with London growing in all directions. To the East the Port of London grew rapidly during the century, with the construction of many docks, needed as the Thames at the City could not cope with the volume of trade. The arrival of the railways and the Tube meant that London could expand over a much greater area. By the mid-19th century, with London still rapidly expanding in population and area, the City had already become only a small part of the wider metropolis.\n19th and 20th centuries.\nAn attempt was made in 1894 with the Royal Commission on the Amalgamation of the City and County of London to end the distinction between the city and the surrounding County of London, but a change of government at Westminster meant the option was not taken up. The city as a distinct polity survived despite its position within the London conurbation and numerous local government reforms. Supporting this status, the city was a special parliamentary borough that elected four members to the unreformed House of Commons, who were retained after the Reform Act 1832; reduced to two under the Redistribution of Seats Act 1885; and ceased to be a separate constituency under the Representation of the People Act 1948. Since then the city is a minority (in terms of population and area) of the Cities of London and Westminster.\nThe city's population fell rapidly in the 19th century and through most of the 20th century, as people moved outwards in all directions to London's vast suburbs, and many residential buildings were demolished to make way for office blocks. Like many areas of London and other British cities, the City fell victim to large scale and highly destructive aerial bombing during World War II, especially in the Blitz. Whilst St Paul's Cathedral survived the onslaught, large swathes of the area did not and the particularly heavy raids of late December 1940 led to a firestorm called the Second Great Fire of London.\nThere was a major rebuilding programme in the decades following the war, in some parts (such as at the Barbican) dramatically altering the urban landscape. But the destruction of the older historic fabric allowed the construction of modern and larger-scale developments, whereas in those parts not so badly affected by bomb damage the City retains its older character of smaller buildings. The street pattern, which is still largely medieval, was altered slightly in places, although there is a more recent trend of reversing some of the post-war modernist changes made, such as at Paternoster Square.\nThe City suffered terrorist attacks including the 1993 Bishopsgate bombing (IRA) and the 7 July 2005 London bombings (Islamist). In response to the 1993 bombing, a system of road barriers, checkpoints and surveillance cameras referred to as the \"ring of steel\" has been maintained to control entry points to the city.\nThe 1970s saw the construction of tall office buildings including the 600-foot (183\u00a0m), 47-storey NatWest Tower, the first skyscraper in the UK. By the 2010s, office space development had intensified in the City, especially in the central, northern and eastern parts, with skyscrapers including 30 St. Mary Axe (\"the Gherkin\"'), Leadenhall Building (\"the Cheesegrater\"), 20 Fenchurch Street (\"the Walkie-Talkie\"), the Broadgate Tower, the Heron Tower and 22 Bishopsgate.\nThe main residential section of the City today is the Barbican Estate, constructed between 1965 and 1976. The Museum of London was based there until March 2023 (due to reopen in West Smithfield in 2026), whilst a number of other services provided by the corporation are still maintained on the Barbican Estate.\nGovernance.\nThe city has a unique political status, a legacy of its uninterrupted integrity as a corporate city since the Anglo-Saxon period and its singular relationship with the Crown. Historically its system of government was not unusual, but it was not reformed by the Municipal Corporations Act 1835 and little changed by later reforms, so that it is the only local government in the UK where elections are not run on the basis of one vote for every adult citizen.\nIt is administered by the City of London Corporation, headed by the Lord Mayor of London (not to be confused with the separate Mayor of London, an office created only in the year 2000), which is responsible for a number of functions and has interests in land beyond the city's boundaries. Unlike other English local authorities, the corporation has two council bodies: the (now largely ceremonial) Court of Aldermen and the Court of Common Council. The Court of Aldermen represents the wards, with each ward (irrespective of size) returning one alderman. The chief executive of the Corporation holds the ancient office of Town Clerk of London.\nThe city is a ceremonial county which has a Commission of Lieutenancy headed by the Lord Mayor instead of a Lord-Lieutenant and has two Sheriffs instead of a High Sheriff (see list of Sheriffs of London), quasi-judicial offices appointed by the livery companies, an ancient political system based on the representation and protection of trades (guilds). Senior members of the livery companies are known as liverymen and form the Common Hall, which chooses the lord mayor, the sheriffs and certain other officers.\nWards.\nThe city is made up of 25 wards. They are survivors of the medieval government system that allowed a very local area to exist as a self-governing unit within the wider city. They can be described as electoral/political divisions; ceremonial, geographic and administrative entities; sub-divisions of the city. Each ward has an Alderman, who until the mid-1960s held office for life but since put themselves up for re-election at least every 6 years, and are the only directly elected Aldermen in the United Kingdom. Wards continue to have a Beadle, an ancient position which is now largely ceremonial whose main remaining function is the running of an annual Wardmote of electors, representatives and officials. At the Wardmote the ward's Alderman appoints at least one Deputy for the year ahead, and Wardmotes are also held during elections. Each ward also has a Ward Club, which is similar to a residents' association.\nThe wards are ancient and their number has changed three times since time immemorial:\nFollowing boundary changes in 1994, and later reform of the business vote in the city, there was a major boundary and electoral representation revision of the wards in 2003, and they were reviewed again in 2010 for change in 2013, though not to such a dramatic extent. The review was conducted by senior officers of the corporation and senior judges of the Old Bailey; the wards are reviewed by this process to avoid malapportionment. The procedure of review is unique in the United Kingdom as it is not conducted by the Electoral Commission or a local government boundary commission every 8 to 12 years, which is the case for all other wards in Great Britain. Particular churches, livery company halls and other historic buildings and structures are associated with a ward, such as St Paul's Cathedral with Castle Baynard, and London Bridge with Bridge; boundary changes in 2003 removed some of these historic connections.\nEach ward elects an alderman to the Court of Aldermen, and commoners (the City equivalent of a councillor) to the Court of Common Council of the corporation. Only electors who are Freemen of the City of London are eligible to stand. The number of commoners a ward sends to the Common Council varies from two to ten, depending on the number of electors in each ward. Since the 2003 review it has been agreed that the four more residential wards: Portsoken, Queenhithe, Aldersgate and Cripplegate together elect 20 of the 100 commoners, whereas the business-dominated remainder elect the remaining 80 commoners. 2003 and 2013 boundary changes have increased the residential emphasis of the mentioned four wards.\nCensus data provides eight nominal rather than 25 real wards, all of varying size and population. Being subject to renaming and definition at any time, these census 'wards' are notable in that four of the eight wards accounted for 67% of the 'square mile' and held 86% of the population, and these were in fact similar to and named after four City of London wards:\nElections.\nThe city has a unique electoral system. Most of its voters are representatives of businesses and other bodies that occupy premises in the city. Its ancient wards have very unequal numbers of voters. In elections, both the businesses based in the city and the residents of the City vote.\nThe City of London Corporation was not reformed by the Municipal Corporations Act 1835, because it had a more extensive electoral franchise than any other borough or city; in fact, it widened this further with its own equivalent legislation allowing one to become a freeman without being a liveryman. In 1801, the city had a population of about 130,000, but increasing development of the city as a central business district led to this falling to below 5,000 after the Second World War. It has risen slightly to around 9,000 since, largely due to the development of the Barbican Estate. In 2009, the business vote was about 24,000, greatly exceeding residential voters. As the City of London Corporation has not been affected by other municipal legislation over the period of time since then, its electoral practice has become increasingly anomalous. Uniquely for city or borough elections, its elections remain independent-dominated.\nThe business or \"non-residential vote\" was abolished in other UK local council elections by the Representation of the People Act 1969, but was preserved in the City of London. The principal reason given by successive UK governments for retaining this mechanism for giving businesses representation, is that the city is \"primarily a place for doing business\". About 330,000 non-residents constitute the day-time population and use most of its services, far outnumbering residents, who number around 7,000 (2011). By contrast, opponents of the retention of the business vote argue that it is a cause of institutional inertia.\nThe City of London (Ward Elections) Act 2002 (c. vi), a local act of Parliament, reformed the voting system and greatly increased the business franchise, allowing many more businesses to be represented. Under the new system, the number of non-resident voters has doubled from 16,000 to 32,000. Previously disenfranchised firms (and other organisations) are entitled to nominate voters, in addition to those already represented, and all such bodies are now required to choose their voters in a representative fashion. Bodies employing fewer than 10 people may appoint 1 voter; those employing 10 to 50 people 1 voter for every 5 employees; those employing more than 50 people 10 voters and 1 additional voter for each 50 employees beyond the first 50. The Act also changed other aspects of an earlier act relating to elections in the city, from 1957.\nThe Temple.\nInner Temple and Middle Temple (which neighbour each other) in the western ward of Farringdon Without are within the boundaries and liberties of the City, but can be thought of as independent enclaves. They are two of the few remaining liberties, an old name for a geographic division with special rights. They are extra-parochial areas, historically not governed by the City of London Corporation (and are today regarded as local authorities for most purposes) and equally outside the ecclesiastical jurisdiction of the Bishop of London.\nOther functions.\nWithin the city, the Corporation owns and runs both Smithfield Market and Leadenhall Market. It owns land beyond its boundaries, including open spaces (parks, forests and commons) in and around Greater London, including most of Epping Forest and Hampstead Heath. The Corporation owns Old Spitalfields Market and Billingsgate Fish Market, in the neighbouring London Borough of Tower Hamlets. It owns and helps fund the Old Bailey, the Central Criminal Court for England and Wales, as a gift to the nation, having begun as the City and Middlesex Sessions. The Honourable The Irish Society, a body closely linked with the corporation, also owns many public spaces in Northern Ireland.\nThe city has its own independent police force, the City of London Police\u2014the Common Council (the main body of the corporation) is the police authority. The corporation also run the Hampstead Heath Constabulary, Epping Forest Keepers and the City of London market constabularies (whose members are no longer attested as constables but retain the historic title). The majority of Greater London is policed by the Metropolitan Police Service, based at New Scotland Yard.\nThe city has one hospital, St Bartholomew's Hospital, also known as 'Barts'. Founded in 1123, it is located at Smithfield, and is undergoing a long-awaited regeneration after doubts as to its continuing use during the 1990s.\nThe city is the third largest UK patron of the arts. It oversees the Barbican Centre and subsidises several important performing arts companies.\nThe London Port Health Authority, which is the responsibility of the corporation, is responsible for all port health functions on the tidal part of the Thames, including the Port of London and related seaports, and London City Airport. The Corporation oversees the Bridge House Estates, which maintains Blackfriars Bridge, Millennium Bridge, Southwark Bridge, London Bridge and Tower Bridge. The City's flag flies over Tower Bridge, although neither footing is in the city.\nThe boundary of the City.\nThe size of the city was constrained by a defensive perimeter wall, known as London Wall, which was built by the Romans in the late 2nd century to protect their strategic port city. However the boundaries of the City of London no longer coincide with the old city wall, as the City expanded its jurisdiction slightly over time. During the medieval era, the city's jurisdiction expanded westwards, crossing the historic western border of the original settlement\u2014the River Fleet\u2014along Fleet Street to Temple Bar. The city also took in the other \"City bars\" which were situated just beyond the old walled area, such as at Holborn, Aldersgate, West Smithfield, Bishopsgate and Aldgate. These were the important entrances to the city and their control was vital in maintaining the city's special privileges over certain trades.\nMost of the wall has disappeared, but several sections remain visible. A section near the Museum of London was revealed after the devastation of an air raid on 29 December 1940 at the height of the Blitz. Other visible sections are at St Alphage, and there are two sections near the Tower of London. The River Fleet was canalised after the Great Fire of 1666 and then in stages was bricked up and has been since the 18th century one of London's \"lost rivers or streams\", today underground as a storm drain.\nThe boundary of the city was unchanged until minor boundary changes on 1 April 1994, when it expanded slightly to the west, north and east, taking small parcels of land from the London Boroughs of Westminster, Camden, Islington, Hackney and Tower Hamlets. The main purpose of these changes was to tidy up the boundary where it had been rendered obsolete by changes in the urban landscape. In this process the city also lost small parcels of land, though there was an overall net gain (the City grew from 1.05 to 1.12 square miles). Most notably, the changes placed the (then recently developed) Broadgate estate entirely in the city.\nSouthwark, to the south of the city on the other side of the Thames, was within the City between 1550 and 1899 as the Ward of Bridge Without, a situation connected with the Guildable Manor. The city's administrative responsibility there had in practice disappeared by the mid-Victorian period as various aspects of metropolitan government were extended into the neighbouring areas. Today it is part of the London Borough of Southwark. The Tower of London has always been outside the city and comes under the London Borough of Tower Hamlets.\nArms, motto and flag.\nThe Corporation of the City of London has a full achievement of armorial bearings consisting of a shield on which the arms are displayed, a crest displayed on a helm above the shield, supporters on either side and a motto displayed on a scroll beneath the arms.\nThe coat of arms is \"anciently recorded\" at the College of Arms. The arms consist of a silver shield bearing a red cross with a red upright sword in the first quarter. They combine the emblems of the patron saints of England and London: the Cross of St George with the symbol of the martyrdom of Saint Paul. The sword is often erroneously supposed to commemorate the killing of Peasants' Revolt leader Wat Tyler by Lord Mayor of London William Walworth. However the arms were in use some months before Tyler's death, and the tradition that Walworth's dagger is depicted may date from the late 17th century.\nThe Latin motto of the city is \"Domine dirige nos\", which translates as \"Lord, direct us\". It is thought to have been adopted in the 17th century, as the earliest record of it is in 1633.\nA banner of the arms (the design on the shield) is flown as a flag.\nGeography.\nThe City of London is the smallest ceremonial county of England by area and population, and the fourth most densely populated. Of the 326 English districts, it is the second smallest by population, after the Isles of Scilly, and the smallest by area. It is also the smallest English city by population (and in Britain, only two cities in Wales are smaller), and the smallest in the UK by area.\nThe elevation of the City ranges from sea level at the Thames to at the junction of High Holborn and Chancery Lane. Two small but notable hills are within the historic core, Ludgate Hill to the west and Cornhill to the east. Between them ran the Walbrook, one of the many \"lost\" rivers or streams of London (another is the Fleet).\nBoundary.\nBeginning in the west, where the City borders Westminster, the boundary crosses the Victoria Embankment from the Thames, passes to the west of Middle Temple, then turns for a short distance along the Strand and near Temple Bar then north up Chancery Lane, where it borders Camden. It turns east along Holborn to Holborn Circus and then goes northeast to Charterhouse Street. As it crosses Farringdon Road it becomes the boundary with Islington. It continues to Aldersgate, goes north, and turns east into some back streets soon after Aldersgate becomes Goswell Road, since 1994 embracing all of the corporation's Golden Lane Estate. Here, at Baltic Street West, is the most northerly extent. The boundary includes all of the Barbican Estate and continues east along Ropemaker Street and its continuation on the other side of Moorgate, becomes South Place. It goes north, reaching the border with Hackney, then east, north, east on back streets, with Worship Street forming a northern boundary, so as to include the Broadgate estate. The boundary then turns south at Norton Folgate and becomes the border with Tower Hamlets. It continues south into Bishopsgate, and takes some backstreets to Middlesex Street (Petticoat Lane) where it continues south-east then south. It then turns south-west, crossing the Minories so as to exclude the Tower of London, and then reaches the Thames.\nThe boundary then runs up the centre of the low-tide channel of the Thames, with the exception that Blackfriars Bridge (including the river beneath and land at its south end) is entirely part of the City, making the City and Borough of Richmond upon Thames the only London districts to span north and south of the river. The span and southern abutment of London Bridge is part of the city for some purposes (and as such is part of Bridge ward).\nThe boundaries are marked by black bollards bearing the city's emblem, and by dragon boundary marks at major entrances, such as Holborn and the south end of London Bridge. A more substantial monument marks the boundary at Temple Bar on Fleet Street.\nIn some places, the financial district extends slightly beyond the boundaries, notably to the north and east, into the London boroughs of Tower Hamlets, Hackney and Islington, and informally these locations are regarded as being part of the \"Square Mile\". Since the 1990s the eastern fringe, extending into Hackney and Tower Hamlets, has increasingly been a focus for large office developments due to the availability of large sites compared to within the city.\nGardens and public art.\nThe city has no sizeable parks within its boundary, but does have a network of a large number of gardens and small open spaces, many of them maintained by the corporation. These range from formal gardens such as the one in Finsbury Circus, containing a bowling green and bandstand, to churchyards such as St Olave Hart Street, to water features and artwork in courtyards and pedestrianised lanes.\nGardens include:\nThere are a number of private gardens and open spaces, often within courtyards of the larger commercial developments. Two of the largest are those of the Inner Temple and Middle Temple Inns of Court, in the far southwest.\nThe Thames and its riverside walks are increasingly being valued as open space and in recent years efforts have been made to increase the ability for pedestrians to access and walk along the river.\nClimate.\nThe nearest weather station has historically been the London Weather Centre at Kingsway/ Holborn, although observations ceased in 2010. Now St. James Park provides the nearest official readings.\nThe city has an oceanic climate (K\u00f6ppen \"Cfb\") modified by the urban heat island in the centre of London. This generally causes higher night-time minima than outlying areas. For example, the August mean minimum of compares to a figure of for Greenwich and Heathrow whereas is at Wisley in the middle of several square miles of Metropolitan Green Belt. All figures refer to the observation period 1971\u20132000.\nAccordingly, the weather station holds the record for the UK's warmest overnight minimum temperature, , recorded on 4 August 1990. The maximum is , set on 10 August 2003. The absolute minimum for the weather station is a mere , compared to readings around towards the edges of London. Unusually, this temperature was during a windy and snowy cold spell (mid-January 1987), rather than a cold clear night\u2014cold air drainage is arrested due to the vast urban area surrounding the city.\nThe station holds the record for the highest British mean monthly temperature, (mean maximum , mean minimum during July 2006). However, in terms of daytime maximum temperatures, Cambridge NIAB and Botanical Gardens with a mean maximum of , and Heathrow with all exceeded this.\nPublic services.\nPolice and security.\nThe city is a police area and has its own police force, the City of London Police, separate from the Metropolitan Police Service covering the majority of Greater London. The City Police previously had three police stations, at Snow Hill, Wood Street and Bishopsgate. They now only retain Bishopsgate along with an administrative headquarters at Guildhall Yard East. The force comprises 735 police officers including 273 detectives. It is the smallest territorial police force in England and Wales, in both geographic area and the number of police officers.\nWhere the majority of British police forces have silver-coloured badges, those of the City of London Police are black and gold featuring the City crest. The force has rare red and white chequered cap bands and unique red and white striped duty arm bands on the sleeves of the tunics of constables and sergeants (red and white being the colours of the city), which in most other British police forces are black and white. City police sergeants and constables wear crested custodian helmets whilst on foot patrol. These helmets do not feature either St Edward's Crown or the Brunswick Star, which are used on most other police helmets in England and Wales.\nThe city's position as the United Kingdom's financial centre and a critical part of the country's economy, contributing about 2.5% of the UK's gross national product, has resulted in it becoming a target for political violence. The Provisional IRA exploded several bombs in the early 1990s, including the 1993 Bishopsgate bombing.\nThe area is also spoken of as a possible target for al-Qaeda. For instance, when in May 2004 the BBC's \"Panorama\" programme examined the preparedness of Britain's emergency services for a terrorist attack on the scale of the 11 September 2001 attacks, they simulated a chemical explosion on Bishopsgate in the east of the city. The \"Ring of Steel\" was established in the wake of the IRA bombings to guard against terrorist threats.\nFire brigade.\nThe city has fire risks in many historic buildings, including St Paul's Cathedral, Old Bailey, Mansion House, Smithfield Market, the Guildhall, and also in numerous high-rise buildings. There is one London Fire Brigade station in the city, at Dowgate, with one pumping appliance. The City relies upon stations in the surrounding London boroughs to support it at some incidents. The first fire engine is in attendance in roughly five minutes on average, the second when required in a little over five and a half minutes. There were 1,814 incidents attended in the City in 2006/2007\u2014the lowest in Greater London. No-one died in an event arising from a fire in the four years prior to 2007.\nPower.\nThere is power station located in Charterhouse Street that also provides heat to some of the surrounding buildings.\nDemography.\nThe Office for National Statistics recorded the population in 2011 as 7,375; slightly higher than in the previous census, 2001, and estimates the population as at mid-2016 to be 9,401. At the 2001 census the ethnic composition was 84.6% White, 6.8% South Asian, 2.6% Black, 2.3% Mixed, 2.0% Chinese and 1.7% were listed as \"other\". To the right is a table showing the change in population since 1801, based on decadal censuses. The first half of the 19th century shows a population of between 120,000 and 140,000, decreasing dramatically from 1851 to 1991, with a small increase between 1991 and 2001. The only notable boundary change since the first census in 1801 occurred in 1994.\nThe city's full-time working residents have much higher gross weekly pay than in London and Great Britain (England, Wales and Scotland): \u00a3773.30 compared to \u00a3598.60 and \u00a3491.00 respectively. There is a large inequality of income between genders (\u00a31,085.90 in men compared to \u00a3653.50 in women), and this can be explained by job type and length of employment respectively. The 2001 Census showed the city as a unique district amongst 376 districts surveyed in England and Wales. The city had the highest proportional population increase, one-person households, people with qualifications at degree level or higher and the highest indications of overcrowding. It recorded the lowest proportion of households with cars or vans, people who travel to work by car, married couple households and the lowest average household size: just 1.58 people. It also ranked highest within the Greater London area for the percentage of people with no religion and people who are employed.\nEconomy.\nThe City of London vies with New York City's Lower Manhattan for the distinction of the world's pre-eminent financial centre. The London Stock Exchange (shares and bonds), Lloyd's of London (insurance) and the Bank of England are all based in the city. Over 500 banks have offices in the city. The Alternative Investment Market, a market for trades in equities of smaller firms, is a recent development. In 2009, the City of London accounted for 2.4% of UK GDP.\nLondon's foreign exchange market has been described by Reuters as 'the crown jewel of London's financial sector'. Of the $3.98\u00a0trillion daily global turnover, as measured in 2009, trading in London accounted for around $1.85\u00a0trillion, or 46.7% of the total. The pound sterling, the currency of the United Kingdom, is globally the fourth-most traded currency and the fourth most held reserve currency.\nCanary Wharf, a few miles east of the City in Tower Hamlets, which houses many banks and other institutions formerly located in the Square Mile, has since 1991 become another centre for London's financial services industry. Although growth has continued in both locations, and there have been relocations in both directions, the Corporation has come to realise that its planning policies may have been causing financial firms to choose Canary Wharf as a location.\nIn 2022, 12.3% of City of London residents had been granted non-domicile status in order to avoid their paying tax in the UK.\nHeadquarters.\nMany major global companies have their headquarters in the city, including Aviva, BT Group, Lloyds Banking Group, Quilter, Prudential, Schroders, Standard Chartered, and Unilever.\nA number of the world's largest law firms are headquartered in the city, including four of the \"Magic Circle\" law firms (Allen &amp; Overy, Freshfields Bruckhaus Deringer, Linklaters and Slaughter &amp; May), as well as other firms such as Ashurst LLP, DLA Piper, Eversheds Sutherland, Herbert Smith Freehills and Hogan Lovells.\nOther sectors.\nWhilst the financial sector, and related businesses and institutions, continue to dominate, the economy is not limited to that sector. The legal profession has a strong presence, especially in the west and north (i.e., towards the Inns of Court). Retail businesses were once important, but have gradually moved to the West End of London, though it is now Corporation policy to encourage retailing in some locations, for example at Cheapside near St Paul's. The city has a number of visitor attractions, mainly based on its historic heritage as well as the Barbican Centre and adjacent Museum of London, though tourism is not at present a major contributor to the city's economy or character. The city has many pubs, bars and restaurants, and the \"night-time\" economy does feature in the Bishopsgate area, towards Shoreditch. The meat market at Smithfield, wholly within the city, continues to be one of London's main markets (the only one remaining in central London) and the country's largest meat market. In the east is Leadenhall Market, a fresh food market that is also a visitor attraction.\nRetail and residential.\nThe trend for purely office development is beginning to reverse as the Corporation encourages residential use, albeit with development occurring when it arises on windfall sites. The city has a target of 90 additional dwellings per year. Some of the extra accommodation is in small pre-World War II listed buildings, which are not suitable for occupation by the large companies which now provide much of the city's employment. Recent residential developments include \"the Heron\", a high-rise residential building on the Milton Court site adjacent to the Barbican, and the Heron Plaza development on Bishopsgate is also expected to include residential parts.\nSince the 1990s, the City has diversified away from near exclusive office use in other ways. For example, several hotels and the first department store opened in the 2000s. A shopping centre was more recently opened at One New Change, Cheapside (near St Paul's Cathedral) in October 2010, which is open seven days a week. However, large sections remain quiet at weekends, especially in the eastern section, and it is quite common to find shops, pubs and cafes closed on these days.\nLandmarks.\nHistoric buildings.\nFire, bombing and post-World War II redevelopment have meant that the city, despite its history, has fewer intact historic structures than one might expect. Nonetheless, there remain many dozens of (mostly Victorian and Edwardian) fine buildings, typically in historicist and neoclassical style. They include the Monument to the Great Fire of London (\"the Monument\"), St Paul's Cathedral, the Guildhall, the Royal Exchange, Dr. Johnson's House, Mansion House and a , many designed by Sir Christopher Wren, who also designed St Paul's. \nPrince Henry's Room and 2 King's Bench Walk are notable historic survivors of heavy bombing of the Temple area, which has largely been rebuilt to its historic form. Another example of a bomb-damaged place having been restored is Staple Inn on Holborn. A few small sections of the Roman London Wall exist, for example near the Tower of London and in the Barbican area. Among the twentieth-century listed buildings are Bracken House, the first post World War II buildings in the country to be given statutory protection, and the whole of the Barbican and Golden Lane Estate.\nThe Tower of London is not in the city, but is a notable visitor attraction which brings tourists to the southeast of the city. Other landmark buildings with historical significance include the Bank of England, the Old Bailey, the Custom House, Smithfield Market, Leadenhall Market and St Bartholomew's Hospital. Noteworthy contemporary buildings include a number of modern high-rise buildings (see section below) as well as the Lloyd's building.\nSkyscrapers and tall buildings.\nA growing number of tall buildings and skyscrapers are principally used by the financial sector. Almost all are situated in the eastern side around Bishopsgate, Leadenhall Street and Fenchurch Street, in the financial core of the city. In the north there is a smaller cluster comprising the Barbican Estate's three tall residential towers and the commercial CityPoint tower. In 2007, the tall Drapers' Gardens building was demolished and replaced by a shorter tower.\nThe city's buildings of at least in height are:\nThe timeline of the tallest building in the city is as follows:\nTransport.\nRail and Tube.\nThe city is well served by the London Underground (\"tube\") and National Rail networks.\nSeven London Underground lines serve the city; the underground stations include:\nIn addition, Aldgate East ( ), Farringdon ( ), Temple ( ) and Tower Hill ( ) tube stations are all situated within metres of the City of London boundary.\nThe Docklands Light Railway (DLR ) has two termini in the city: Bank and Tower Gateway. The DLR links the City directly to the East End. Destinations include Canary Wharf and London City Airport.\nThe Elizabeth line (constructed by the Crossrail project) runs east\u2013west underneath the City of London. The line serves two stations in the City \u2013 Farringdon and Liverpool Street \u2013 which additionally serves the Barbican and Moorgate areas. Elizabeth line services link the City directly to destinations such as Canary Wharf, Heathrow Airport, and the M4 Corridor high-technology hub (serving Slough and Reading).\nThe city is served by a frequent Thameslink rail service which runs north\u2013south through London. Thameslink services call at Farringdon, City Thameslink, and London Blackfriars. This provides the city with a direct link to key destinations across London, including Elephant &amp; Castle, London Bridge, and St Pancras International (for the Eurostar to mainland Europe). There are also regular, direct trains from these stations to major destinations across East Anglia and the South East, including Bedford, Brighton, Cambridge, Gatwick Airport, Luton Airport, and Peterborough.\nThere are several \"London Terminals\" in the city:\nAll stations in the city are in London fare zone 1.\nRoad.\nThe national A1, A10 A3, A4, and A40 road routes begin in the city. The city is in the London congestion charge zone, with the small exception on the eastern boundary of the sections of the A1210/A1211 that are part of the Inner Ring Road. The following bridges, listed west to east (downstream), cross the River Thames: Blackfriars Bridge, Blackfriars Railway Bridge, Millennium Bridge (footbridge), Southwark Bridge, Cannon Street Railway Bridge and London Bridge; Tower Bridge is not in the city. The city, like most of central London, is well served by buses, including night buses. Two bus stations are in the city, at Aldgate on the eastern boundary with Tower Hamlets, and at Liverpool Street by the railway station.\nHowever although the London Road Traffic Act 1924 removed from existing local authorities the powers to prevent the development of road passengers transport services within the London Metropolitan Area, the City of London retained most such powers. As a consequence, neither Trolleybus nor Green Line Coach services were permitted to enter the City to pick up or set down passengers. Hence the building of Aldgate (Minories) Trolleybus and Coach station as well as the complex terminal arrangements at Parliament Hill Fields. This restriction was removed by the Transport Act 1985\nCycling.\nCycling infrastructure in the city is maintained by the City of London Corporation and Transport for London (TfL).\nThe Sandander Cycles and Beryl bike sharing systems operate in the City of London.\nRiver.\nOne London River Services pier is on the Thames in the city, Blackfriars Millennium Pier, though the Tower Millennium Pier lies adjacent to the boundary near the Tower of London. One of the Port of London's 25 safeguarded wharves, Walbrook Wharf, is adjacent to Cannon Street station, and is used by the corporation to transfer waste via the river. Swan Lane Pier, just upstream of London Bridge, is proposed to be replaced and upgraded for regular passenger services, planned to take place in 2012\u20132015. Before then, Tower Pier is to be extended.\nThere is a public riverside walk along the river bank, part of the Thames Path, which opened in stages \u2013 the route within the city was completed by the opening of a stretch at Queenhithe in 2023. The walk along Walbrook Wharf is closed to pedestrians when waste is being transferred onto barges.\nTravel to work (by residents).\nAccording to a survey conducted in March 2011, the methods by which employed residents 16\u201374 get to work varied widely: 48.4% go on foot; 19.5% via light rail, (i.e. the Underground, DLR, etc.); 9.2% work mainly from home; 5.8% take the train; 5.6% travel by bus, minibus, or coach; and 5.3% go by bicycle; with just 3.4% commuting by car or van, as driver or passenger.\nEducation.\nThe city is home to a number of higher education institutions including: the Guildhall School of Music and Drama, the Cass Business School, The London Institute of Banking &amp; Finance and parts of three of the universities in London: the Maughan Library of King's College London on Chancery Lane, the business school of London Metropolitan University, and a campus of the University of Chicago Booth School of Business. The College of Law has its London campus in Moorgate. Part of Barts and The London School of Medicine and Dentistry is on the Barts hospital site at West Smithfield.\nThe city has only one directly maintained primary school, The Aldgate School (formerly Sir John Cass's Foundation Primary School) at Aldgate (ages 4 to 11). It is a Voluntary-Aided (VA) Church of England school, maintained by the Education Service of the City of London.\nCity residents send their children to schools in neighbouring Local Education Authorities, such as Islington, Tower Hamlets, Westminster and Southwark.\nThe City controls three independent schools, City of London School (a boys' school) and City of London School for Girls in the city, and the City of London Freemen's School (co-educational day and boarding) in Ashtead, Surrey. The City of London School for Girls and City of London Freemen's School have their own preparatory departments for entrance at age seven. It is the principal sponsor of The City Academy, Hackney, City of London Academy Islington, and City of London Academy, Southwark.\nPublic libraries.\nLibraries operated by the Corporation include three lending libraries; Barbican Library, Shoe Lane Library and Artizan Street Library and Community Centre. Membership is open to all \u2013 with one official proof of address required to join.\nGuildhall Library, and City Business Library are also public reference libraries, specialising in the history of London and business reference resources.\nMoney laundering.\nThe City of London's role in illicit financial activity such as money laundering has earned the financial hub sobriquets such as 'The Laundromat' and 'Londongrad'. \nLondon's role as the world's dirty money clearing house is well-documented but efforts are being made to clean up through legislation, e.g. authorising unexplained wealth orders. High-value properties are sought after by criminals and money launderers legitimising their gains by investing in the city's prestigious real estate.\nIn May 2024, the UK's then deputy foreign secretary, Andrew Mitchell, said that 40% of the dirty money in the world goes through the City of London and other crown dependencies."}
