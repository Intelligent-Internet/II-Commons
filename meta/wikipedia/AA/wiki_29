{"id": "3794", "revid": "2153309", "url": "https://en.wikipedia.org/wiki?curid=3794", "title": "Brassicaceae", "text": "Brassicaceae () or (the older) Cruciferae () is a medium-sized and economically important family of flowering plants commonly known as the mustards, the crucifers, or the cabbage family. Most are herbaceous plants, while some are shrubs. The leaves are simple (although are sometimes deeply incised), lack stipules, and appear alternately on stems or in rosettes. The inflorescences are terminal and lack bracts. The flowers have four free sepals, four free alternating petals, two shorter free stamens and four longer free stamens. The fruit has seeds in rows, divided by a thin wall (or septum).\nThe family contains 372 genera and 4,060 accepted species. The largest genera are \"Draba\" (440 species), \"Erysimum\" (261 species), \"Lepidium\" (234 species), \"Cardamine\" (233 species), and \"Alyssum\" (207 species).\nThe family contains the cruciferous vegetables, including species such as \"Brassica oleracea\" (cultivated as cabbage, kale, cauliflower, broccoli and collards), \"Brassica rapa\" (turnip, Chinese cabbage, etc.), \"Brassica napus\" (rapeseed, etc.), \"Raphanus sativus\" (common radish), \"Armoracia rusticana\" (horseradish), but also a cut-flower \"Matthiola\" (stock) and the model organism \"Arabidopsis thaliana\" (thale cress).\n\"Pieris rapae\" and other butterflies of the family Pieridae are some of the best-known pests of Brassicaceae species planted as commercial crops. \"Trichoplusia ni\" (cabbage looper) moth is also becoming increasingly problematic for crucifers due to its resistance to commonly used pest control methods. Some rarer \"Pieris\" butterflies, such as \"P.\u00a0virginiensis\", depend upon native mustards for their survival in their native habitats. Some non-native mustards such as \"Alliaria petiolata\" (garlic mustard), an extremely invasive species in the United States, can be toxic to their larvae.\nDescription.\nSpecies belonging to the Brassicaceae are mostly annual, biennial, or perennial herbaceous plants, some are dwarf shrubs or shrubs, and very few vines. Although generally terrestrial, a few species such as water awlwort live submerged in fresh water. They may have a taproot or a sometimes woody caudex that may have few or many branches, some have thin or tuberous rhizomes, or rarely develop runners. Few species have multi-cellular glands. Hairs consist of one cell and occur in many forms: from simple to forked, star-, tree- or T-shaped, rarely taking the form of a shield or scale. They are never topped by a gland. The stems may be upright, rise up towards the tip, or lie flat, are mostly herbaceous but sometimes woody. Stems carry leaves or the stems may be leafless (in \"Caulanthus\"), and some species lack stems altogether. The leaves do not have stipules, but there may be a pair of glands at base of leaf stalks and flower stalks. The leaf may be seated or have a leafstalk. The leaf blade is usually simple, entire or dissected, rarely trifoliolate or pinnately compound. A leaf rosette at the base may be present or absent. The leaves along the stem are almost always alternately arranged, rarely apparently opposite. The stomata are of the anisocytic type. The genome size of Brassicaceae compared to that of other Angiosperm families is very small to small (less than 3.425 million base pairs per cell), varying from 150 Mbp in \"Arabidopsis thaliana\" and \"Sphaerocardamum\" spp., to 2375 Mbp \"Bunias orientalis\". The number of homologous chromosome sets varies from four (n=4) in some \"Physaria\" and \"Stenopetalum\" species, five (n=5) in other \"Physaria\" and \"Stenopetalum\" species, \"Arabidopsis thaliana\" and a \"Mathiola\" species, to seventeen (n=17). About 35% of the species in which chromosomes have been counted have eight sets (n=8). Due to polyploidy, some species may have up to 256 individual chromosomes, with some very high counts in the North American species of \"Cardamine\", such as \"C. diphylla\". Hybridisation is not unusual in Brassicaceae, especially in \"Arabis\", \"Rorippa\", \"Cardamine\" and \"Boechera\". Hybridisation between species originating in Africa and California, and subsequent polyploidisation is surmised for \"Lepidium\" species native to Australia and New Zealand.\nInflorescence and flower.\nFlowers may be arranged in racemes, panicles, or corymbs, with pedicels sometimes in the axil of a bract, and few species have flowers that sit individually on flower stems that spring from the axils of rosette leaves. The orientation of the pedicels when fruits are ripe varies dependent on the species. The flowers are bisexual, star symmetrical (zygomorphic in \"Iberis\" and \"Teesdalia\") and the ovary positioned above the other floral parts. Each flower has four free or seldom merged sepals, the lateral two sometimes with a shallow spur, which are mostly shed after flowering, rarely persistent, may be reflexed, spreading, ascending, or erect, together forming a tube-, bell- or urn-shaped calyx. Each flower has four petals, set alternating with the sepals, although in some species these are rudimentary or absent. They may be differentiated into a blade and a claw or not, and consistently lack basal appendages. The blade is entire or has an indent at the tip, and may sometimes be much smaller than the claws. The mostly six stamens are set in two whorls: usually the two lateral, outer ones are shorter than the four inner stamens, but very rarely the stamens can all have the same length, and very rarely species have different numbers of stamens such as sixteen to twenty four in \"Megacarpaea\", four in \"Cardamine hirsuta\", and two in \"Coronopus\". The filaments are slender and not fused, while the anthers consist of two pollen producing cavities, and open with longitudinal slits. The pollen grains are tricolpate. The receptacle carries a variable number of nectaries, but these are always present opposite the base of the lateral stamens.\nOvary, fruit and seed.\nThere is one superior pistil that consists of two carpels that may either sit directly above the base of the stamens or on a stalk. It initially consists of only one cavity but during its further development a thin wall grows that divides the cavity, both placentas and separates the two valves (a so-called false septum). Rarely, there is only one cavity without a septum. The 2\u2013600 ovules are usually along the side margin of the carpels, or rarely at the top. Fruits are capsules that open with two valves, usually towards the top. These are called silique if at least three times longer than wide, or silicle if the length is less than three times the width. The fruit is very variable in its other traits. There may be one persistent style that connects the ovary to the globular or conical stigma, which is undivided or has two spreading or connivent lobes. The variously shaped seeds are usually yellow or brown in color, and arranged in one or two rows in each cavity. The seed leaves are entire or have a notch at the tip. The seed does not contain endosperm.\nDifferences with similar families.\nBrassicaceae have a bisymmetrical corolla (left is mirrored by right, stem-side by out-side, but each quarter is not symmetrical), a septum dividing the fruit, lack stipules and have simple (although sometimes deeply incised) leaves. The sister family Cleomaceae has bilateral symmetrical corollas (left is mirrored by right, but stem-side is different from out-side), stipules and mostly palmately divided leaves, and mostly no septum. Capparaceae generally have a gynophore, sometimes an androgynophore, and a variable number of stamens.\nPhytochemistry.\nAlmost all Brassicaceae have C3 carbon fixation. The only exceptions are a few \"Moricandia\" species, which have a hybrid system between C3 and C4 carbon fixation, C4 fixation being more efficient in drought, high temperature and low nitrate availability. Brassicaceae contain different cocktails of dozens of glucosinolates. They also contain enzymes called myrosinases, that convert the glucosinolates into isothiocyanates, thiocyanates and nitriles, which are toxic to many organisms, and so help guard against herbivory.\nTaxonomy.\nCarl Linnaeus in 1753 regarded the Brassicaceae as a natural group, naming them \"Klass\" Tetradynamia. Alfred Barton Rendle placed the family in the order Rhoeadales, while George Bentham and Joseph Dalton Hooker in their system published from 1862 to 1883, assigned it to their cohort Parietales (now the class Violales). Following Bentham and Hooker, John Hutchinson in 1948 and again in 1964 thought the Brassicaceae to stem from near the Papaveraceae. In 1994, a group of scientists including Walter Stephen Judd suggested to include the Capparaceae in the Brassicaceae. Early DNA-analysis showed that the Capparaceae\u2014as defined at that moment\u2014were paraphyletic, and it was suggested to assign the genera closest to the Brassicaceae to the Cleomaceae. The Cleomaceae and Brassicaceae diverged approximately 41 million years ago. All three families have consistently been placed in one order (variably called Capparales or Brassicales). The APG II system merged Cleomaceae and Brassicaceae. Other classifications have continued to recognize the Capparaceae, but with a more restricted circumscription, either including \"Cleome\" and its relatives in the Brassicaceae or recognizing them in the segregate family Cleomaceae. The APG III system has recently adopted this last solution, but this may change as a consensus arises on this point. Current insights in the relationships of the Brassicaceae, based on a 2012 DNA-analysis, are summarized in the following tree.\nRelationships within the family.\nEarly classifications depended on morphological comparison only, but because of extensive convergent evolution, these do not provide a reliable phylogeny. Although a substantial effort was made through molecular phylogenetic studies, the relationships within the Brassicaceae have not always been well resolved yet. It has long been clear that the \"Aethionema\" are sister of the remainder of the family. One analysis from 2014 represented the relation between 39 tribes with the following tree.\nGenera.\nAs of October 2023 Plants of the World Online accepts 346 genera.\nEtymology.\nThe name \"Brassicaceae\" comes to international scientific vocabulary from Neo-Latin, from \"Brassica\", the type genus, + \"-aceae\", a standardized suffix for plant family names in modern taxonomy. The genus name comes from the Classical Latin word \"brassica\", referring to cabbage and other cruciferous vegetables. The alternative older name, Cruciferae, meaning \"cross-bearing\", describes the four petals of mustard flowers, which resemble a cross. Cruciferae is one of eight plant family names, not derived from a genus name and without the suffix \"-aceae\" that are authorized alternative names.\nDistribution.\nBrassicaceae can be found almost on the entire land surface of the planet, but the family is absent from Antarctica, and also absent from some areas in the tropics i.e. northeastern Brazil, the Congo basin, Maritime Southeast Asia and tropical Australasia. The area of origin of the family is possibly the Irano-Turanian region, where approximately 900 species occur in 150 different genera. About 530 of those 900 species are endemics. Next in abundance comes the Mediterranean region, with around 630 species (290 of which are endemic) in 113 genera. The family is less prominent in the Saharo-Arabian region\u201465 genera, 180 species of which 62 are endemic\u2014and North America (comprising the North American Atlantic region and the Rocky Mountain floristic region)\u201499 genera, 780 species of which 600 are endemic. South America has 40 genera containing 340 native species, Southern Africa 15 genera with over 100 species, and Australia and New-Zealand have 19 genera with 114 species between them.\nEcology.\nBrassicaceae are almost exclusively pollinated by insects. A chemical mechanism in the pollen is active in many species to avoid selfing. Two notable exceptions are exclusive self-pollination in closed flowers in \"Cardamine chenopodifolia\", and wind pollination in \"Pringlea antiscorbutica\". Although it can be cross-pollinated, \"Alliaria petiolata\" (garlic mustard) is self-fertile. Most species reproduce sexually through seed, but \"Cardamine bulbifera\" produces gemmae and in others, such as \"Cardamine pentaphyllos\", the coral-like roots easily break into segments, that will grow into separate plants. In some species, such as in the genus \"Cardamine\", seed pods open with force and so catapult the seeds quite far. Many of these have sticky seed coats, assisting long-distance dispersal by animals, and this may also explain several intercontinental dispersal events in the genus, and its near global distribution. Brassicaceae are common on serpentine and dolomite rich in magnesium. Over a hundred species in the family accumulate heavy metals, particularly zinc and nickel, which is a record percentage. Several \"Alyssum\" species can accumulate nickel up to 0.3% of their dry weight, and may be useful in soil remediation or even bio-mining.\nBrassicaceae contain glucosinolates as well as myrosinases inside their cells. When the cell is damaged, the myrosinases hydrolise the glucosinolates, leading to the synthesis of isothiocyanates, which are compounds toxic to most animals, fungi and bacteria. Some insect herbivores have developed counter adaptations such as rapid absorption of the glucosinates, quick alternative breakdown into non-toxic compounds and avoiding cell damage. In the whites family (Pieridae), one counter mechanism involves glucosinolate sulphatase, which changes the glucosinolate, so that it cannot be converted to isothiocyanate. A second is that the glucosinates are quickly broken down, forming nitriles. Differences between the mixtures of glucosinolates between species and even within species is large, and individual plants may produce in excess of fifty individual substances. The energy penalty for synthesising all these glucosinolates may be as high as 15% of the total needed to produce a leaf. \"Barbarea vulgaris\" (bittercress) also produces triterpenoid saponins. These adaptations and counter adaptations probably have led to extensive diversification in both the Brassicaceae and one of its major pests, the butterfly family Pieridae. A particular cocktail of volatile glucosinates triggers egg-laying in many species. Thus a particular crop can sometimes be protected by planting bittercress as a deadly bait, for the saponins kill the caterpillars, but the butterfly is still lured by the bittercress to lay its egg on the leaves.\nA moth that feeds on a range of Brassicaceae is the diamondback moth (\"Plutella xylostella\"). Like the Pieridae, it is capable of converting isothiocyanates into less problematic nitriles. Managing this pest in crops became more complicated after resistance developed against a toxin produced by \"Bacillus thuringiensis\", which is used as a wide spectrum biological plant protection against caterpillars. Parasitoid wasps that feed on such insect herbivores are attracted to the chemical compounds released by the plants, and thus are able to locate their prey. The cabbage aphid (\"Brevicoryne brassicae\") stores glucosinolates and synthesises its own myrosinases, which may deter its potential predators.\nSince its introduction in the 19th century, \"Alliaria petiolata\" has been shown to be extremely successful as an invasive species in temperate North America due, in part, to its secretion of allelopathic chemicals. These inhibit the germination of most competing plants and kill beneficial soil fungi needed by many plants, such as many tree species, to successfully see their seedlings grow to maturity. The monoculture formation of an herb layer carpet by this plant has been shown to dramatically alter forests, making them wetter, having fewer and fewer trees, and having more vines such as poison ivy (\"Toxicodendron radicans\"). The overall herb layer biodiversity is also drastically reduced, particularly in terms of sedges and forbs. Research has found that removing 80% of the garlic mustard infestation plants did not lead to a particularly significant recovery of that diversity. Instead, it required around 100% removal. Given that not one of an estimated 76 species that prey on the plant has been approved for biological control in North America and the variety of mechanisms the plant has to ensure its dominance without them (e.g. high seed production, self-fertility, allelopathy, spring growth that occurs before nearly all native plants, roots that break easily when pulling attempts are made, a complete lack of palatability for herbivores at all life stages, etc.) it is unlikely that such a high level of control can be established and maintained on the whole. It is estimated that adequate control can be achieved with the introduction of two European weevils, including one that is monophagous. The USDA's TAG group has blocked these introductions since 2004. In addition to being invasive, garlic mustard also is a threat to native North American \"Pieris\" butterflies such as \"P.\u00a0oleracea\", as they preferentially oviposit on it, although it is toxic to their larvae.\nInvasive aggressive mustard species are known for being self-fertile, seeding very heavily with small seeds that have a lengthy lifespan coupled with a very high rate of viability and germination, and for being completely unpalatable to both herbivores and insects in areas to which they are not native. Garlic mustard is toxic to several rarer North American \"Pieris\" species.\nUses.\nThis family includes important agricultural crops, among which many vegetables such as cabbage, broccoli, cauliflower, kale, Brussels sprouts, collard greens, Savoy, kohlrabi, and gai lan (\"Brassica oleracea\"), turnip, napa cabbage, mizuna, bok choy and rapini (\"Brassica rapa\"), rocket salad/arugula (\"Eruca sativa\"), garden cress (\"Lepidium sativum\"), watercress (\"Nasturtium officinale\") and radish (\"Raphanus\") and a few spices like horseradish (\"Armoracia rusticana\"), wasabi (\"Eutrema japonicum\"), white, Indian and black mustard (\"Sinapis alba\", \"Brassica juncea\" and \"B.\u00a0nigra\" respectively). Vegetable oil is produced from the seeds of several species such as \"Brassica napus\" (rapeseed oil), perhaps providing the largest volume of vegetable oils of any species. Woad (\"Isatis tinctoria\") was used in the past to produce a blue textile dye (indigo), but has largely been replaced by the same substance from unrelated tropical species like \"Indigofera tinctoria\".\n\"Pringlea antiscorbutica\", commonly known as Kerguelen cabbage, is edible, containing high levels of potassium. Its leaves contain a vitamin\u00a0C-rich oil, a fact which, in the days of sailing ships, made it very attractive to sailors suffering from scurvy, hence the species name's epithet \"antiscorbutica\", which means \"against scurvy\" in Low Latin. It was essential to the diets of the whalers on Kerguelen when pork, beef, or seal meat was used up.\nThe Brassicaceae also includes ornamentals, such as species of \"Aethionema\", \"Alyssum\", \"Arabis\", \"Aubrieta\", \"Aurinia\", \"Cheiranthus\", \"Erysimum\", \"Hesperis\", \"Iberis\", \"Lobularia\", \"Lunaria\", \"Malcolmia\", and \"Matthiola\". Honesty (\"Lunaria annua\") is cultivated for the decorative value of the translucent remains of the fruits after drying. It can be a pest species in areas where it is not native.\nThe small Eurasian weed \"Arabidopsis thaliana\" is widely used as model organism in the study of the molecular biology of flowering plants (Angiospermae).\nSome species are useful as food plants for Lepidoptera, such as certain wild mustard and cress species, such as \"Turritis glabra\" and \"Boechera laevigata\" that are utilized by several North American butterflies."}
{"id": "3796", "revid": "57939", "url": "https://en.wikipedia.org/wiki?curid=3796", "title": "Books of the Bible", "text": ""}
{"id": "3797", "revid": "31952441", "url": "https://en.wikipedia.org/wiki?curid=3797", "title": "Baseball statistics", "text": "Baseball statistics include a variety of metrics used to evaluate player and team performance in the sport of baseball. \nBecause the flow of a baseball game has natural breaks to it, and player activity is characteristically distinguishable individually, the sport lends itself to easy record-keeping and compiling statistics. Baseball \"stats\" have been recorded since the game's earliest beginnings as a distinct sport in the middle of the nineteenth century, and as such are extensively available through the historical records of leagues such as the National Association of Professional Base Ball Players and the Negro leagues, although the consistency, standards, and calculations are often incomplete or questionable. \nSince the National League (NL) was founded in 1876, statistics in the most elite levels of professional baseball have been kept at some level, with efforts to standardize the stats and their compilation improving during the early 20th century. Such efforts have evolved in tandem with advances in available technology ever since. The NL was joined by the American League (AL) in 1903; together the two constitute contemporary Major League Baseball.\nNew advances in both statistical analysis and technology made possible by the \"PC revolution\" of the 1980s and 1990s have driven teams and fans to evaluate players by an ever-increasing set of new statistics, which hold them to ever-evolving standards. With the advent of many of these methods, players can conditionally be compared across different time eras and run scoring environments.\nDevelopment.\nThe practice of keeping records of player achievements was started in the 19th century by English-American sportswriter Henry Chadwick. Based on his experience with the sport of cricket, Chadwick devised the predecessors to modern-day statistics including batting average, runs scored, and runs allowed.\nTraditionally, statistics such as batting average (the number of hits divided by the number of at bats) and earned run average (the average number of runs allowed by a pitcher per nine innings, less errors and other events out of the pitcher's control) have dominated attention in the statistical world of baseball. However, the recent advent of sabermetrics has created statistics drawing from a greater breadth of player performance measures and playing field variables. Sabermetrics and comparative statistics attempt to provide an improved measure of a player's performance and contributions to his team from year to year, frequently against a statistical performance average.\nComprehensive, historical baseball statistics were difficult for the average fan to access until 1951, when researcher Hy Turkin published \"The Complete Encyclopedia of Baseball\". In 1969, Macmillan Publishing printed its first \"Baseball Encyclopedia\", using a computer to compile statistics for the first time. Known as \"Big Mac\", the encyclopedia became the standard baseball reference until 1988, when \"Total Baseball\" was released by Warner Books using more sophisticated technology. The publication of \"Total Baseball\" led to the discovery of several \"phantom ballplayers\", such as Lou Proctor, who did not belong in official record books and were removed.\nUse.\nThroughout modern baseball, a few core statistics have been traditionally referenced \u2013 batting average, RBI, and home runs. To this day, a player who leads the league in all of these three statistics earns the \"Triple Crown\". For pitchers, wins, ERA, and strikeouts are the most often-cited statistics, and a pitcher leading his league in these statistics may also be referred to as a \"triple crown\" winner. General managers and baseball scouts have long used the major statistics, among other factors and opinions, to understand player value. Managers, catchers and pitchers use the statistics of batters of opposing teams to develop pitching strategies and set defensive positioning on the field. Managers and batters study opposing pitcher performance and motions in attempting to improve hitting. Scouts use stats when they are looking at a player who they may end up drafting or signing to a contract.\nSome sabermetric statistics have entered the mainstream baseball world that measure a batter's overall performance including on-base plus slugging, commonly referred to as OPS. OPS adds the hitter's on-base percentage (number of times reached base by any means divided by total plate appearances) to their slugging percentage (total bases divided by at-bats). Some argue that the OPS formula is flawed and that more weight should be shifted towards OBP (on-base percentage). The statistic wOBA (weighted on-base average) attempts to correct for this.\nOPS is also useful when determining a pitcher's level of success. \"Opponent on-base plus slugging\" (OOPS) is becoming a popular tool to evaluate a pitcher's actual performance. When analyzing a pitcher's statistics, some useful categories include K/9IP (strikeouts per nine innings), K/BB (strikeouts per walk), HR/9 (home runs per nine innings), WHIP (walks plus hits per inning pitched), and OOPS (opponent on-base plus slugging).\nHowever, since 2001, more emphasis has been placed on defense-independent pitching statistics, including defense-independent ERA (dERA), in an attempt to evaluate a pitcher's performance regardless of the strength of the defensive players behind them.\nAll of the above statistics may be used in certain game situations. For example, a certain hitter's ability to hit left-handed pitchers might incline a manager to increase their opportunities to face left-handed pitchers. Other hitters may have a history of success against a given pitcher (or vice versa), and the manager may use this information to create a favorable\nmatch-up. This is often referred to as \"playing the percentages\".\nContemporary statistics.\nThe following listings include abbreviations and/or acronyms for both historic baseball statistics and those based on modern mathematical formulas known popularly as \"metrics\".\nThe explanations below are for quick reference and do not fully or completely define the statistic; for the strict definition, see the linked article for each statistic.\nMLB statistical standards.\nIt is difficult to determine quantitatively what is considered to be a \"good\" value in a certain statistical category, and qualitative assessments may lead to arguments. Using full-season statistics available at the Official Site of Major League Baseball for the 2004 through 2015 seasons, the following tables show top ranges in various statistics, in alphabetical order. For each statistic, two values are given:"}
{"id": "3800", "revid": "28414705", "url": "https://en.wikipedia.org/wiki?curid=3800", "title": "At bat", "text": "In baseball, an at bat (AB) or time at bat is a batter's turn batting against a pitcher. An at bat is different from a plate appearance. A batter is credited with a plate appearance regardless of what happens during their turn at bat, but a batter is charged with an at bat only if that plate appearance does not have one of the results enumerated below. While at bats are used to calculate certain statistics, including batting average and slugging percentage, a player can qualify for the season-ending rankings in these categories only if they accumulate 502 plate appearances during the season.\nBatters will not be charged an at bat if their plate appearances end under the following circumstances:\nIn addition, if the inning ends during at bat (due to the third out being made by a runner caught stealing, for example), no at bat or plate appearance will result.\nAn at bat is a specific type of plate appearance in which the batter puts the ball in play intending to get on base. This is why at bats, and not plate appearances, are used to calculate batting average, as plate appearances in general can result in many outcomes that do not involve putting the ball in play, and batting average specifically measures a batter's contact hitting.\nRule 9.02(a)(1) of the official rules of Major League Baseball defines an at bat as: \"Number of times batted, except that no time at bat shall be charged when a player: (A) hits a sacrifice bunt or sacrifice fly; (B) is awarded first base on four called balls; (C) is hit by a pitched ball; or (D) is awarded first base because of interference or obstruction[.]\"\nExamples.\nAn at bat is counted when:\nRecords.\nPete Rose had 14,053 career at bats, the all-time major league and National League record. The American League record is held by Carl Yastrzemski, whose 11,988 career at bats were all in the AL.\nThe single season record is held by Jimmy Rollins, who had 716 at bats in 2007. Willie Wilson, Ichiro Suzuki and Juan Samuel also had more than 700 at bats in a season. 14 players share the single game record of 11 at bats in a single game, all of which were extra inning games. In games of 9 innings or fewer, the record is 7 at bats and has occurred more than 200 times.\nThe team record for most at bats in a single season is 5,781 by the 1997 Boston Red Sox.\nAt bat as a phrase.\n\"At bat\", \"up\", \"up at bat\", and \"at the plate\" are all phrases describing a batter who is facing the pitcher. Just because a player is described as being \"at bat\" in this sense, he will not necessarily be given an at bat in his statistics; the phrase actually signifies a plate appearance (assuming it is eventually completed). This ambiguous terminology is usually clarified by context. To refer explicitly to a statistical \"at bat\", the term \"official at bat\" is sometimes used.\n\"Time at bat\" in the rulebook.\nOfficial Baseball Rule 5.06(c) provides that \"[a] batter has legally completed his \"time at bat\" when he is put out or becomes a runner\" (emphasis added). The \"time at bat\" defined in this rule is more commonly referred to as a plate appearance, and the playing rules (Rules 1 through 8) uses the phrase \"time at bat\" in this sense. In contrast, the scoring rules use the phrase \"time at bat\" to refer to the statistic at bat, defined in Rule 9.02(a)(1), but sometimes uses the phrase \"official time at bat\" or refers back to Rule 9.02(a)(1) when mentioning the statistic. The phrase \"plate appearance\" is used in Rules 9.22 and 9.23 dealing with batting titles and hitting streaks, and in Rule 5.10(g) comment regarding the Three-Batter Minimum: \"[t]o qualify as one of three consecutive batters, the batter must complete his plate appearance, which ends only when the batter is put out or becomes a runner.\" The term is not elsewhere defined in the rulebook."}
{"id": "3801", "revid": "45940103", "url": "https://en.wikipedia.org/wiki?curid=3801", "title": "Earned run", "text": "In baseball, an earned run is any run that was fully enabled by the offensive team's production in the face of competent play from the defensive team. Conversely, an unearned run is a run that would not have been scored without the aid of an error or a passed ball committed by the defense; it is \"unearned\" in that it was, in a sense, \"given away\" by the defensive team.\nEarned and unearned runs count equally toward the game score; the difference is purely statistical. Both total runs and earned runs are tabulated as part of a pitcher's statistics, but earned runs are specially denoted because of their use in calculating a pitcher's earned run average (ERA), the number of earned runs allowed by the pitcher per nine innings pitched (i.e., averaged over a regulation game). Thus, in effect, the pitcher is held personally accountable for earned runs, while the responsibility for unearned runs is shared with the rest of the team. \nTo determine whether a run is earned, the official scorer must reconstruct the inning as it would have occurred without errors or passed balls.\nDetails.\nIf no errors and no passed balls occur during the inning, all runs scored are automatically earned (assigned responsible to the pitcher(s) who allowed each runner to reach base). Also, in some cases, an error can be rendered harmless as the inning progresses. For example, a runner on first base advances to second on a passed ball and the next batter walks. Since the runner would now have been at second anyway, the passed ball no longer has any effect on the earned/unearned calculation. On the other hand, a batter/runner may make his entire circuit around the bases without the aid of an error, yet the run would be counted as unearned if an error prevented the third out from being made before he crossed the plate to score.\nAn error made by the pitcher in fielding at his position is counted the same as an error by any other player.\nA run is counted as unearned when:\nWhile the inning is still being played, the second and the second-last scenario can cause a temporary situation where a run has already scored, but its earned/unearned status is not yet certain. Under the last circumstance, for example, with two outs, a runner on third base scores on a passed ball. For the time being, the run is unearned since the runner \"should\" still be at third. If the batter strikes out to end the inning, it will stay that way. If the batter gets a base hit, which would have scored the runner anyway, the run now becomes earned.\nUnder the second circumstance, if there are runners on base and a batter hits a foul fly ball that is dropped, and then bats in the runners on base through a base hit (including a home run), the runs are unearned for the time being, as the runners should not have advanced. If the results of the remaining at-bats in the inning would not have scored the runners, the runs remain unearned. However, if results of subsequent at-bats would have scored the runs anyway, the runs would count as earned, unless they only would have scored as a result of a subsequent error or passed ball.\nA baserunner who reaches on catcher's interference and subsequently scores with two outs scores an unearned run, but baserunners who subsequently score after the runner who has reached on catcher's interference exclusively on clean plays score earned runs; the baserunner cannot be assumed to have been put out except for the error. (2019 MLB Rule 9.16(a)(4)).\nIf a run is scored by a pinch-runner who replaces a baserunner who represents an unearned run, or by a pinch-hitter who continues the turn at bat of a batter who would be out except for an error, the run remains unearned, regardless of the substitution.\nPitching changes.\nWhen pitchers are changed in the middle of an inning, and one or more errors have already occurred, it is possible to have a run charged as earned against a specific pitcher, but unearned to the team. The simplest example is when the defensive team records two outs and makes an error on a play that would have been the third out. A new pitcher comes into the game, and the next batter hits a home run. The runner who reached on the error comes around to score, and his run is unearned to both the prior pitcher and the team. However, the run scored by the batter is counted as earned against the relief pitcher, but unearned to the team (since there should have already been three outs). Had the team not switched pitchers, neither run would be counted as an earned run because that pitcher should have already been out of that inning.\nA pitcher who is relieved mid-inning may be charged with earned runs equal to the number of batters who reached base while he was pitching, even if the specific batters he faced do not score. The batters he put on base may be erased by fielder's choice plays after he has been relieved by another pitcher, but if earned runs are scored in the inning the original pitcher is liable for as many earned runs as the number of batters he put on base. \nExample:\nWhen a pitching change occurs, the new pitcher is said to \"inherit\" any runners that are on base at the time, and if they later score, those runs are charged (earned or unearned) to the prior pitcher. Most box scores now list inherited runners, and the number that scored, as a statistic for the relief pitcher."}
{"id": "3802", "revid": "1398", "url": "https://en.wikipedia.org/wiki?curid=3802", "title": "Base on balls", "text": "A base on balls (BB), better known as a walk,\noccurs in baseball when a batter receives four pitches during a plate appearance that the umpire calls \"balls\", and is in turn awarded first base without the possibility of being called out. The base on balls is defined in Section 2.00 of baseball's Official Rules, and further detail is given in 6.08(a). Despite being known as a \"walk\", it is considered a faux pas for a professional player to actually walk to first base; the batter-runner and any advancing runners normally jog on such a play.\nThe term \"base on balls\" distinguishes a walk from the other manners in which a batter can be awarded first base without liability to be put out (e.g., hit by pitch (HBP), catcher's interference). Though a base on balls, catcher's interference, or a batter hit by a pitched ball all result in the batter (and possibly runners on base) being awarded a base, the term \"walk\" usually refers only to a base on balls, and not the other methods of reaching base without the bat touching the ball. An important difference is that for a hit batter or catcher's interference, the ball is dead and no one may advance unless forced; the ball is live after a walk (see below for details).\nA batter who draws a base on balls is commonly said to have been \"walked\" by the pitcher. When the batter is walked, runners advance one base without liability to be put out only if forced to vacate their base to allow the batter to take first base. If a batter draws a walk with the bases loaded, all preceding runners are forced to advance, including the runner on third base who is forced to home plate to score a run; when a run is forced on a walk, the batter is credited with a run batted in per rule 9.04.\nReceiving a base on balls does not count as a hit or an at bat for a batter but does count as a time on base and a plate appearance. Therefore, a base on balls does not affect a player's batting average, but it can increase his on-base percentage.\nA hit by pitch is not counted statistically as a walk, though the effect is mostly the same, with the batter receiving a free pass to first base. One exception is that on hit-by-pitch, the ball is dead, and any runners attempting to steal on the play must return to their original base unless forced to the next base anyway. When a walk occurs, the ball is still live: any runner not forced to advance may nevertheless attempt to advance at his own risk, which might occur on a steal play, passed ball, or wild pitch. Also, because a ball is live when a base on balls occurs, runners on base forced to advance one base may attempt to advance beyond one base, at their own risk. The batter-runner himself may attempt to advance beyond first base, at his own risk. Rule 6.08 addresses this matter as well. An attempt to advance an additional base beyond the base awarded might occur when ball four is a passed ball or a wild pitch.\nHistory.\nIn early baseball, there was no concept of a \"ball\". It was created by the NABBP in 1863, originally as a sort of unsportsmanlike-conduct penalty: \"Should the pitcher repeatedly fail to deliver to the striker fair balls, for the apparent purpose of delaying the game, or for any other cause, the umpire, after warning him, shall call one ball, and if the pitcher persists in such action, two and three balls; when three balls shall have been called, the striker shall be entitled to the first base; and should any base be occupied at that time, each player occupying them shall be entitled to one base without being put out.\" Note that this rule in effect gave the pitcher 9 balls, since each penalty ball could only be called on a third offense. In 1869 the rule was modified so that only those baserunners forced to advance could advance. From 1871 through 1886, the batter was entitled to call \"high\" or \"low\", i.e. above or below the waist; a pitch which failed to conform was \"unfair\". Certain pitches were defined as automatic balls in 1872: any ball delivered over the batter's head, that hit the ground in front of home plate, was delivered to the opposite side from the batter, or came within one foot of him. In 1880, the National League changed the rules so that eight \"unfair balls\" instead of nine were required for a walk. In 1884, the National League changed the rules so that six balls were required for a walk. In 1886, the American Association changed the rules so that six balls instead of seven were required for a walk; however, the National League changed the rules so that seven balls were required for a walk instead of six. In 1887, the National League and American Association agreed to abide by some uniform rule changes, including, for the first time, a strike zone which defined balls and strikes by rule rather than the umpire's discretion, and decreased the number of balls required for a walk to five. In 1889, the National League and the American Association decreased the number of balls required for a walk to four. \nIn 2017, Major League Baseball approved a rule change allowing for a batter to be walked intentionally by having the defending bench signal to the umpire. The move was met with some controversy.\nIntentional base on balls.\nA subset of the base on balls, an intentional base on balls (IBB), or intentional walk, is when the defensive team intentionally issues a walk to the batter. In Major League Baseball and many amateur leagues, an intentional base on balls is signaled to the home plate umpire by the defensive team's manager holding up four fingers, at which point the batter is awarded first base without any further pitches being thrown. In some leagues and in Major League Baseball prior to 2017, an intentional base on balls is issued when the pitcher deliberately pitches the ball away from the batter four times (or as many times as needed to get to ball four if the decision to issue the intentional walk is made with one or more balls already on the count). As with any other walk, an intentional walk entitles the batter to first base without liability to be put out, and entitles any runners to advance if forced. \nIntentional walks are a strategic defensive maneuver, commonly done to bypass one hitter for one the defensive team believes is less likely to initiate a run-scoring play (e.g., a home run, sacrifice fly, or RBI base hit). Teams also commonly use intentional walks to set up a double play or force out situation for the next batter.\nMajor League Baseball leaders.\nGame.\nJimmie Foxx, Andre Thornton, Jeff Bagwell and Bryce Harper have each been walked six times during a major league regular season game. Among pitchers, Tommy Byrne and Bruno Haas both gave up 16 bases on balls in a game. On September 17, 1920, the Boston Red Sox drew 20 walks in a 12-inning game against the Detroit Tigers. , this is the most walks drawn or allowed by a team in a single game in Major League history according to available data."}
{"id": "3803", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=3803", "title": "Ball (baseball statistics)", "text": ""}
{"id": "3804", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=3804", "title": "Baseball statistics/SLG", "text": ""}
{"id": "3805", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=3805", "title": "List of Major League Baseball career total bases leaders", "text": "In baseball statistics, total bases (TB) is the number of bases a player has gained with hits. It is a weighted sum for which the weight value is 1 for a single, 2 for a double, 3 for a triple, and 4 for a home run. Only bases attained from hits count toward this total. Reaching base by other means (such as a base on balls) or advancing further after the hit (such as when a subsequent batter gets a hit) does not increase the player's total bases.\nThe total bases divided by the number of at bats is the player's slugging average.\nHank Aaron is the career leader in total bases with 6,856. Albert Pujols (6,211), Stan Musial (6,134), and Willie Mays (6,080) are the only other players with at least 6,000 career total bases.\nAs of September 26, 2024, no active players are in the top 100 for career total bases. The active leader is Los Angeles Dodgers first baseman Freddie Freeman, tied in 109th with 3,866."}
{"id": "3806", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=3806", "title": "Hit by pitch", "text": "In baseball, hit by pitch (HBP) is an event in which a batter or his clothing or equipment (other than his bat) is struck directly by a pitch from the pitcher; the batter is called a hit batsman (HB). A hit batsman is awarded first base, provided that (in the plate umpire's judgment) he made an honest effort to avoid the pitch, although failure to do so is rarely called by an umpire. Being hit by a pitch is often caused by a batter standing too close to, or \"crowding\", home plate. \nThe rule dates from 1887; before that, a pitch that struck the batter was merely a ball.\nOfficial rule.\nPer baseball official rule 5.05(b), a batter becomes a baserunner and is awarded first base when he or his equipment (except for his bat):\nIf all these conditions are met, the ball is dead, and other baserunners advance if they are forced to vacate their base by the batter taking first. Rule 5.09(a) further clarifies that a hit by pitch is also called when a pitch touches a batter's clothing.\nIn the case where a batter swings and the pitch hits him anyway, the ball is dead and a strike is called. If the batter does not attempt to avoid the pitch, he is not awarded first base, and the pitch is ruled either a strike if in the strike zone or a ball if out of the strike zone. Umpires rarely make this call. A famous instance of a non-hit by pitch was on May 31, 1968, when Don Drysdale hit Dick Dietz with a pitch that would have forced in a run and ended Drysdale's scoreless innings streak at 44. Umpire Harry Wendelstedt ruled that Dietz made no effort to avoid the pitch; Dietz proceeded to fly out, and Drysdale's scoreless streak continued to a then-record 58 innings. Another notable example was the first game of the 2022 World Series. In the bottom of the 10th inning, Philadelphia Phillies pitcher David Robertson was pitching to Houston Astros pinch hitter Aldemys Diaz. With 2 balls and no strikes, two out and two runners in scoring position, Robertson threw a pitch inside that struck Diaz's left arm. Home plate umpire James Hoye ruled that Diaz did not attempt to avoid the pitch and called the pitch a ball. Diaz, who had begun to take first base before Hoye called time, disputed the call, and would go on to ground out to end the game.\nA hit-by-pitch can also be called on a pitch that has first touched the ground before hitting the batter. Such a bouncing pitch is like any other, and if a batter is hit by such a pitch, he will be awarded first unless he made no attempt to avoid it (and he had an opportunity to avoid it).\nA batter hit by a pitch is not credited with a hit or at bat, but is credited with a time on base and a plate appearance; therefore, being hit by a pitch does not increase or decrease a player's batting average but does increase his on-base percentage. A batter hit by a pitch with the bases loaded is also credited with an RBI per MLB rule 10.04(a)(2). A pitch ruled a hit by pitch is recorded as a ball in the pitcher's pitch count, since by definition the ball must be outside the strike zone and not have been swung at.\nThe rule awarding first base to a batter hit by a pitch was instituted in 1887.\nTactical use.\nInside pitching is a common and legal tactic in baseball, and many players make use of brushback pitches, or pitches aimed underneath the chin, commonly referred to as \"chin music\", to keep players away from the plate. \"Headhunter\" is a common term for pitchers who have a reputation for throwing these kinds of pitches. However, throwing at a batter intentionally is illegal, and can be very dangerous. When an umpire suspects that a pitcher has thrown at a batter intentionally, but is not certain, a warning is issued to the pitcher and the managers of both teams. From that point on, any pitch thrown at a batter can cause the pitcher and the manager of the offending team to be ejected immediately from the game. Serious offenses such as a ball thrown at the head (called a beanball) can result in the immediate ejection of the pitcher, and the manager if he ordered the beanball, even without a warning. If the umpire is certain that the pitcher intentionally hit the batter with the pitch, the pitcher is ejected from the game with no warning. This infamously happened on August 15, 2018, when Jos\u00e9 Ure\u00f1a was ejected from a game against the Atlanta Braves after hitting Ronald Acu\u00f1a Jr. on the elbow with the first pitch of the game, which led to the Braves' and Marlins' benches emptying.\nOccasionally, if a player is acting rude or unsportsmanlike, or having an extraordinarily good day, the pitcher may intentionally hit the batter, disguising it as a pitch that accidentally slipped his control. Managers may also order a pitcher to throw such a pitch (sometimes called a \"plunking\"). These pitches are typically aimed at the lower back and slower than normal, designed to send a message more than anything else. The opposing team usually hits a batter in retaliation for this act. The plunkings generally end there because of umpire warnings, but in some cases things can get out of hand, and sometimes they lead to the batter charging the mound, bench-clearing brawls, and several ejections.\nRecords.\nKorea Baseball Organization third baseman Choi Jeong holds the Korean Baseball Organization hit by pitch record with 348. It is also the world record. The all-time record for a player being hit by a pitch in MLB is held by Hughie Jennings, who was hit by 287 pitches between 1891 and 1903. The modern-era record is held by Craig Biggio of the Houston Astros, who had 285 as of the end of the 2007 season when he retired. Prior to Biggio, the modern-era record belonged to Don Baylor, who was hit 267 times.\nThe all-time single-season record also belongs to Jennings, who was hit 51 times during the 1896 season. Ron Hunt of the 1971 Montreal Expos was hit 50 times during that year, the modern-era record. The single-game record is three, held by numerous players.\nThe all-time record for pitchers is held by Gus Weyhing with 277 (1887\u20131901). The modern-era career pitching record for most hit batsmen is 205 by Hall-of-Famer Walter Johnson. The season record is 54 by Phil Knell in 1891, and the game record is six, held by Ed Knouff and John Grimes.\nBrady Anderson was the first player to be hit by a pitch two times in the same inning in an American League game. On April 25, 2014, Brandon Moss became the second when he was hit twice in the top of the 9th inning by Houston Astros pitchers. Five players have been hit by a pitch twice in the same inning in the National League. On September 1, 2021, Austin Adams became the first pitcher hitting batters 20 or more times with 120 or less IPs in a season. Ed Doheny hit batters 22 times in 133.2 IP in 1900.\nThree times has a perfect game been broken up by the 27th batter being hit by pitch. Hooks Wiltse, Max Scherzer, and Joe Musgrove hold this rare feat. All three finished with no-hitters after the hit by pitch. Scherzer's team was leading 6\u20130 and Musgrove's 3\u20130 when they pitched their no-hitters, but Wiltse's team was scoreless through 9; he pitched a 10-inning 1\u20130 no-hitter. The record for most hit batters in a no-hitter is three, held by Chris Heston of the San Francisco Giants for his 2015 effort against the New York Mets.\nPostseason career records are held by Greg Maddux and Tim Wakefield\u2014each of whom hit 9 batters\u2014and Shane Victorino, who was hit by pitch 11 times.\nDangers.\nOne major-league player died as a result of being struck by a pitch: Ray Chapman of the Cleveland Indians was hit in the head by Carl Mays on August 16, 1920, and died the next morning.\nSerious injuries may result from being hit by a pitch, even when wearing a batting helmet. On August 18, 1967, Boston Red Sox batter Tony Conigliaro was hit almost directly in the left eye by a fastball thrown by Jack Hamilton of the California Angels. His cheekbone was shattered; he nearly lost the sight of the eye, was unable to play for over a year, and never regained his earlier batting ability. At the time, batting helmets were not required to have an \"ear flap\"; it was not until 2002 that all major-league batters were required to wear helmets with side-protection. Ron Santo was the first player to wear a helmet with an improvised ear-flap; he had it made after he was struck by a pitch from Jack Fisher of the New York Mets on June 26, 1966, which briefly knocked Santo unconscious and left him with a fractured cheekbone.\nOther notable injuries include: \nOther comparably minor injuries that are possible include broken fingers or hands, broken feet, broken ribs, injuries to the knee, or groin injuries.\nLegal interpretation.\nSince inside pitching is a legitimate tactic in baseball, courts have recognized that being hit by a pitch is an inherent risk of the game, so that players cannot sue for any resulting injuries. On April 6, 2006, in a case arising from a game involving community college baseball teams, the Supreme Court of California ruled that baseball players in California assume the risk of being hit by baseballs \"even if\" the balls were intentionally thrown so as to cause injury. In the court's words: \"For better or worse, being intentionally thrown at is a fundamental part and inherent risk of the sport of baseball. It is not the function of tort law to police such conduct.\""}
{"id": "3807", "revid": "10951369", "url": "https://en.wikipedia.org/wiki?curid=3807", "title": "Hit (baseball)", "text": "In baseball statistics, a hit (denoted by H), also called a base hit, is credited to a batter when the batter safely reaches or passes first base after hitting the ball into fair territory with neither the benefit of an error nor a fielder's choice.\nScoring a hit.\nTo achieve a hit, the batter must reach first base before any fielder can either tag him with the ball, throw to another player protecting the base before the batter reaches it, or tag first base while carrying the ball. The hit is scored the moment the batter reaches first base safely; if he is put out while attempting to stretch his hit to a double or triple or home run on the same play, he still gets credit for a hit (according to the last base he reached safely on the play).\nIf a batter reaches first base because of offensive interference by a preceding runner (including if a preceding runner is hit by a batted ball), he is also credited with a hit.\nTypes of hits.\nA hit for one base is called a single, for two bases a double, and for three bases a triple. A home run is also scored as a hit. Doubles, triples, and home runs are also called extra base hits.\nAn \"infield hit\" is a hit where the ball does not leave the infield. Infield hits are uncommon by nature, and most often earned by speedy runners.\nPitching a no-hitter.\nA no-hitter is a game in which one of the teams prevented the other from getting a hit. Throwing a no-hitter is rare and considered an extraordinary accomplishment for a pitcher or pitching staff. In most cases in the professional game, no-hitters are accomplished by a single pitcher who throws a complete game. A pitcher who throws a no-hitter could still allow runners to reach base safely, by way of walks, errors, hit batsmen, or batter reaching base due to interference or obstruction. If the pitcher allows no runners to reach base in any manner whatsoever (hit, walk, hit batsman, error, etc.), the no-hitter is a perfect game.\n1887 discrepancy.\nIn 1887, Major League Baseball counted bases on balls (walks) as hits. The result was skyrocketing batting averages, including some near .500; Tip O'Neill of the St. Louis Browns batted .485 that season, which would still be a major league record if recognized. The experiment was abandoned the following season.\nThere is controversy regarding how the records of 1887 should be interpreted. The number of legitimate walks and at-bats are known for all players that year, so computing averages using the same method as in other years is straightforward. In 1968, Major League Baseball formed a Special Baseball Records Committee to resolve this (and other) issues. The Committee ruled that walks in 1887 should not be counted as hits. In 2000, Major League Baseball reversed its decision, ruling that the statistics which were recognized in each year's official records should stand, even in cases where they were later proven incorrect. Most current sources list O'Neill's 1887 average as .435, as calculated by omitting his walks. He would retain his American Association batting championship. However, the variance between methods results in differing recognition for the 1887 National League batting champion. Cap Anson would be recognized, with his .421 average, if walks are included, but Sam Thompson would be the champion at .372 if they are not.\nMajor League Baseball rules.\nThe official rulebook of Major League Baseball states in Rule 10.05:\nRule 10.05(a) Comment: In applying Rule 10.05(a), the official scorer shall always give the batter the benefit of the doubt. A safe course for the official scorer to follow is to score a hit when exceptionally good fielding of a ball fails to result in a putout."}
{"id": "3808", "revid": "46335398", "url": "https://en.wikipedia.org/wiki?curid=3808", "title": "On-base percentage", "text": "In baseball statistics, on-base percentage (OBP) measures how frequently a batter reaches base. An official Major League Baseball (MLB) statistic since 1984, it is sometimes referred to as on-base average (OBA), as it is rarely presented as a true percentage.\nGenerally defined as \"how frequently a batter reaches base per plate appearance\", OBP is specifically calculated as the ratio of a batter's times on base (the sum of hits, bases on balls, and times hit by pitch) to the sum of at bats, bases on balls, hit by pitch, and sacrifice flies. OBP does not credit the batter for reaching base on fielding errors, fielder's choice, uncaught third strikes, fielder's obstruction, or catcher's interference, and deducts from plate appearances a batter intentionally giving himself up in a sacrifice bunt. \nOBP is added to slugging average (SLG) to determine on-base plus slugging (OPS). \nThe OBP of all batters faced by one pitcher or team is referred to as \"on-base against\".\nOn-base percentage is calculable for professional teams dating back to the first year of National Association of Professional Base Ball Players competition in 1871, because the component values of its formula have been recorded in box scores ever since. \nHistory.\nThe statistic was invented in the late 1940s by Brooklyn Dodgers statistician Allan Roth with then-Dodgers general manager Branch Rickey. In 1954, Rickey, who was then the general manager of the Pittsburgh Pirates, was featured in a Life Magazine graphic in which the formula for on-base percentage was shown as the first component of an all-encompassing \"offense\" equation. However, it was not named as on-base percentage, and there is little evidence that Roth's statistic was taken seriously at the time by the baseball community at large.\nOn-base percentage became an official MLB statistic in 1984. Its perceived importance jumped after the influential 2003 book \"\" highlighted Oakland Athletics general manager Billy Beane's focus on the statistic. Many baseball observers, particularly those influenced by the field of sabermetrics, now consider on-base percentage superior to the statistic traditionally used to measure offensive skill, batting average, which accounts for hits but ignores other ways a batter can reach base.\nOverview.\nTraditionally, players with the best on-base percentages bat as leadoff hitter, unless they are power hitters, who traditionally bat slightly lower in the batting order. The league average for on-base percentage in Major League Baseball has varied considerably over time; at its peak in the late 1990s, it was around .340, whereas it was typically .300 during the dead-ball era. On-base percentage can also vary quite considerably from player to player. The highest career OBP of a batter with more than 3,000 plate appearances is .482 by Ted Williams. The lowest is by Bill Bergen, who had an OBP of .194.\nOn-base percentage is calculated using this formula:\nwhere\nIn certain unofficial calculations, the denominator is simplified and replaced by Plate Appearance (PA); however, the calculation PAs includes certain infrequent events that will slightly lower the calculated OBP (i.e. catcher's interference, and sacrifice bunts). Sacrifice bunts are excluded from consideration on the basis that they are usually imposed by the manager with the expectation that the batter will not reach base, and thus do not accurately reflect the batter's ability to reach base when attempting to do so. This is in contrast with the sacrifice fly, which is generally unintentional; the batter was trying for a hit."}
{"id": "3809", "revid": "40193331", "url": "https://en.wikipedia.org/wiki?curid=3809", "title": "Sacrifice fly", "text": "In baseball, a sacrifice fly (sometimes abbreviated to sac fly) is defined by Rule 9.08(d):\n\"Score a sacrifice fly when, before two are out, the batter hits a ball in flight handled by an outfielder or an infielder running in the outfield in fair or foul territory that\nThey are so named because the batter allows a teammate to score a run, while \"sacrificing\" their ability to do so. They are traditionally recorded in box scores with the designation \"SF\".\nRules.\nAs addressed within Rule 9.02(a)(1) of the Official Baseball Rules a sacrifice fly is not counted as a time at bat for the batter, though the batter is credited with a run batted in. The same is true with a bases-loaded walk or hit by pitch.\nThe purpose of not counting a sacrifice fly as an at-bat is to avoid penalizing hitters for a successful action. The sacrifice fly is one of two instances in baseball where a batter is not charged with a time at bat after putting a ball in play; the other is the sacrifice hit (also known as a sacrifice bunt). But, while a sacrifice fly does not affect a player's batting average, it counts as a plate appearance and lowers the on-base percentage. A player on a hitting streak will have it end with no official at-bats but a sacrifice fly.\nUnlike a sacrifice bunt, which may be scored if a runner advances from any base to any base, a sacrifice fly is credited only if a runner scores on the play. Therefore, when a runner on first or second base tags on a fly ball and advances no further than third base, no sacrifice is given, and the batter is charged with an at-bat. Also, if a runner tags and advances from second base (or, theoretically, from first base) all the way to home and scores (without an intervening error), the batter is credited with a sacrifice fly, as well as with a second RBI if a runner on third also scores. At the professional level this will typically occur only in unusual circumstances that prevent the defense from making an immediate throw back to the infield, such as an outfielder colliding with the wall while making a catch on the warning track.\nThe sacrifice fly is credited even if another runner is put out, so long as the run scores. The sacrifice fly is credited on a dropped ball even if another runner is forced out by reason of the batter becoming a runner.\nRecords.\nThe most sacrifice flies by a team in one game in Major League Baseball (MLB) is five; the record was established by the Seattle Mariners in 1988, tied by the Colorado Rockies in 2006, and tied again by the Mariners in 2008.\nFive MLB teams have collected three sacrifice flies in an inning: the Chicago White Sox (fifth inning, July 1, 1962, against the Cleveland Indians); the New York Yankees twice (fourth inning, June 29, 2000, against the Detroit Tigers and third inning, August 19, 2000, against the Anaheim Angels); the New York Mets (second inning, June 24, 2005, against the Yankees); and the Houston Astros (seventh inning, June 26, 2005, against the Texas Rangers). In these cases one or more of the flies did not result in a putout due to an error.\nSince the rule was reinstated in its present form in MLB in 1954, Gil Hodges of the Dodgers holds the record for most sacrifice flies in one season with 19, in 1954; Eddie Murray holds the MLB record for most sacrifice flies in a career with 128.\nAs of the end of the 2021 Major League Baseball season, the ten players who had hit the most sacrifice flies were as follows:\nOnly once has the World Series been won on a sac fly. In 1912, Larry Gardner of the Boston Red Sox hit a fly ball off a pitch from the New York Giants' Christy Mathewson. Steve Yerkes tagged up and scored from third base to win game 8 in the tenth inning and take the series for the Red Sox.\nHistory.\nBatters have not been charged with a time at-bat for a sacrifice hit since 1893, but baseball has changed the sacrifice fly rule multiple times. The sacrifice fly as a statistical category was instituted in 1908, only to be discontinued in 1931. The rule was again adopted in 1939, only to be eliminated again in 1940, before being adopted for the last time in 1954. For some baseball fans, it is significant that the sacrifice-fly rule was eliminated in 1940 because, in 1941, Ted Williams was hitting .39955 on the last day of the season and needed one hit in a doubleheader against the Philadelphia A's to become the first hitter since Bill Terry in 1930 to hit .400. He got six hits, finishing with an official .406 average, the last player in over 80 years to bat .400 or more in the American or National League. In his book \"Baseball and Other Matters in 1941\" author Robert Creamer, citing estimates, points out that if Williams' 14 at-bats on sacrifice flies that year were deducted from the 456 official at-bats he was charged with, his final average in 1941 would have been .419."}
{"id": "3810", "revid": "1271206813", "url": "https://en.wikipedia.org/wiki?curid=3810", "title": "On-base plus slugging", "text": "On-base plus slugging (OPS) is a sabermetric baseball statistic calculated as the sum of a player's on-base percentage and slugging percentage. The ability of a player both to get on base and to hit for power, two important offensive skills, are represented. An OPS of .800 or higher in Major League Baseball puts the player in the upper echelon of hitters. Typically, the league leader in OPS will score near, and sometimes above, the 1.000 mark.\nEquation.\nThe basic equation is\nformula_1\nwhere OBP is on-base percentage and SLG is slugging average. These averages are defined below as:\nformula_2\n- the numerator \"H + BB + HBP\" effectively means \"number of trips to first base at least\"\n- the denominator \"AB + BB + SF + HBP\" effectively means \"total plate appearances\", but does not include sacrifice bunts\nThis is because though a batter makes a trip to the plate he is not given an \"AB\" when he walks (BB or HBP) or when he hits the ball into play and is called out, but the action allows a run to score (SF). As a result, the 4 counts (AB + BB + SF +HBP) are needed to calculate a batter's total trips to the plate.\nand\nformula_3\nwhere:\nIn one equation, OPS can be represented as:\nformula_4\nHistory.\nOn-base plus slugging was first popularized in 1984 by John Thorn and Pete Palmer's book, \"The Hidden Game of Baseball\". \"The New York Times\" then began carrying the leaders in this statistic in its weekly \"By the Numbers\" box, a feature that continued for four years. Baseball journalist Peter Gammons used and evangelized the statistic, and other writers and broadcasters picked it up. The popularity of OPS gradually spread, and by 2004 it began appearing on Topps baseball cards.\nOPS was formerly sometimes known as \"production\". For instance, \"production\" was included in early versions of Thorn's \"Total Baseball\" encyclopedia, and in the \"Strat-O-Matic Computer Baseball\" game. This term has fallen out of use.\nOPS gained popularity because of the availability of its components, OBP and SLG, and that team OPS correlates well with team runs scored.\nAn OPS scale.\nBill James, in his essay titled \"The 96 Families of Hitters\" uses seven different categories for classification by OPS:\nThis effectively transforms OPS into a seven-point ordinal scale. Substituting quality labels such as \"excellent\" (A), \"very good\" (B), \"good\" (C), \"average\" (D), \"fair\" (E), \"poor\" (F) and \"very poor\" (G) for the A\u2013G categories creates a subjective reference for OPS values.\nLeaders.\nThe top ten Major League Baseball players in lifetime OPS, with at least 3,000 plate appearances , were:\nThe top five were all left-handed batters. Jimmie Foxx has the highest career OPS for a right-handed batter.\nThe top ten single-season performances in MLB are (all left-handed hitters):\nThe highest single-season mark for a right-handed hitter was 1.2449 by Rogers Hornsby in , 13th on the all-time list. Since 1935, the highest single-season OPS for a right-hander is 1.2224 by Mark McGwire in , which was 16th all-time.\nAdjusted OPS (OPS+).\nOPS+, adjusted OPS, is a closely related statistic. OPS+ is OPS adjusted for the park and the league in which the player played. An OPS+ of 100 is defined to be the league average. An OPS+ of 150 or more is excellent and 125 very good, while an OPS+ of 75 or below is poor.\nThe basic equation for OPS+ is\nformula_5\nwhere *lgOBP is the park-adjusted OBP of the league and *lgSLG is the park-adjusted SLG of the league.\nA common misconception is that OPS+ closely matches the ratio of a player's OPS to that of their league. In fact, due to the additive nature of the two components in OPS+, a player with an OBP and SLG both 50% better than the league average in those metrics will have an OPS+ of 200 (twice the league average OPS+) while still having an OPS that is only 50% better than the average OPS of the league. It would be a better (although not exact) approximation to say that a player with an OPS+ of 150 produces 50% more \"runs\", in a given set of plate appearances than a player with an OPS+ of 100 (though see clarification above, under \"History\").\nLeaders in OPS+.\nThrough the end of the 2019 season, the career top twenty leaders in OPS+ (minimum 3,000 plate appearances) were:\nThe only purely right-handed batters to appear on this list are Browning, Hornsby, Foxx, Orr, Trout, McGwire, Allen, Mays, and Thomas. Mantle is the only switch-hitter in the group.\nThe highest single-season performances were:\nIf Dunlap's and Barnes's seasons were to be eliminated from the list, two other Ruth seasons (1926 and 1927) would be on the list. This would also eliminate the only two right-handed batters in the list.\nCriticism.\nDespite its simple calculation, OPS is a controversial measurement. OPS weighs on-base percentage and slugging percentage equally. However, on-base percentage correlates better with scoring runs. Statistics such as wOBA build on this distinction using linear weights. Additionally, the components of OPS are not typically equal (league-average slugging percentages are usually 75\u2013100 points higher than league-average on-base percentages). As a point of reference, the OPS for all of Major League Baseball in 2019 was .758."}
{"id": "3811", "revid": "14462949", "url": "https://en.wikipedia.org/wiki?curid=3811", "title": "Stolen base", "text": "In baseball, a stolen base occurs when a runner advances to a base unaided by other actions and the official scorer rules that the advance should be credited to the action of the runner. The umpires determine whether the runner is safe or out at the next base, but the official scorer rules on the question of credit or blame for the advance under Rule 10 (Rules of Scoring) of the MLB's Official Rules.\nA stolen base most often occurs when a base runner advances to the next base while the pitcher is pitching the ball to home plate.\nSuccessful base stealers must be fast and have good timing.\nBackground.\nNed Cuthbert, playing for the Philadelphia Keystones in either 1863 or 1865, was the first player to steal a base in a baseball game, although the term \"stolen base\" was not used until 1870. For a time in the 19th century, stolen bases were credited when a baserunner reached an extra base on a base hit from another player. For example, if a runner on first base reached third base on a single, it counted as a steal. In 1887, Hugh Nicol set a still-standing Major League record with 138 stolen bases, many of which would not have counted under modern rules. Modern steal rules were fully implemented in 1898.\nBase stealing was popular in the game's early decades, with speedsters such as Ty Cobb and Clyde Milan stealing nearly 100 bases in a season. But the tactic fell into relative disuse after Babe Ruth introduced the era of the home run \u2013 in 1955, for example, no one in baseball stole more than 25 bases, and Dom DiMaggio won the AL stolen base title in 1950 with just 15. However, in the late 1950s and early 1960s, base-stealing was brought back to prominence primarily by Luis Aparicio and Maury Wills, who broke Cobb's modern single-season record by stealing 104 bases in 1962. Wills\u2019 record was broken in turn by Lou Brock in 1974 and Rickey Henderson in 1982. The stolen base remained a popular tactic through the 1980s, perhaps best exemplified by Vince Coleman and the St. Louis Cardinals, but began to decline again in the 1990s as the frequency of home runs reached record heights and the steal-friendly artificial turf ballparks began to disappear.\nBase stealing is an important characteristic of the \"small ball\" managing style (or \"manufacturing runs\"). Such managers emphasize \"doing the little things\" (including risky running plays like base-stealing) to advance runners and score runs, often relying on pitching and defense to keep games close. The Los Angeles Dodgers of the 1960s, led by pitcher Sandy Koufax and speedy shortstop Maury Wills, were a successful example of this style. The antithesis of this is reliance on power hitting, exemplified by the Baltimore Orioles of the 1970s, which aspired to score most of its runs via home runs. Often the \"small ball\" model is associated with the National League, while power hitting is associated with the American League. However, some successful recent American League teams, including the 2002 Anaheim Angels, the 2001 Seattle Mariners, the 2005 Chicago White Sox, and the 2015 Kansas City Royals, have excelled at \"small ball.\" The Royals in particular embodied this style within the last decade, leading the league in stolen bases but finishing last in home runs in 2013 and 2014, leading to a berth in two consecutive World Series, one of which they won. Successful teams often combine both styles, with speedy runners complementing power hitters\u2014such as the 2005 White Sox, who hit 200 home runs, which was fifth most in the majors, and had 137 stolen bases, which was fourth.\nBase-stealing technique.\nBaseball's Rule 8 (The Pitcher) specifies the pitching procedure in detail. For example, in the Set Position, the pitcher must \"com[e] to a complete stop\"; thereafter, \"any natural motion associated with his delivery of the ball to the batter commits him to the pitch without alteration or interruption.\" A runner intending to \"steal on the pitcher\" breaks for the next base the moment the pitcher commits to pitch to home plate. The pitcher cannot abort the pitch and try to put the runner out; this is a balk under Rule 8.\nIf the runner breaks too soon (before the pitcher is obliged to complete a pitch), the pitcher may throw to a base rather than pitch, and the runner is usually \"picked off\" by being tagged out between the bases. Past this moment, any delay in the runner's break makes it more likely that the catcher, after receiving the pitch, will be able to throw the runner out at the destination base.\nBefore the pitch, the runner takes a \"lead\", walking several steps away from the base as a head start toward the next base. Even a runner who does not intend to steal takes a \"secondary lead\" of a few more steps, once the pitcher has legally committed to complete the pitch.\nThe pitcher may throw to the runner's base. The runner must return to that base or risk being tagged out. As well as putting the runner out, an underlying goal is to dissuade the runner from too big a lead; that is, to \"hold the runner on\" the original base. (Historically, this gambit could be used without limit. An MLB rules change in 2023 limited the pitcher to two throws; the pitcher must then pitch to the batter.)\nThe more adept base stealers are proficient at \"reading the pickoff\", meaning that they can detect certain \"tells\" (tell-tale signs) in a pitcher's pre-pitch movements or mannerisms that indicate the pickoff attempt is or is not imminent. For example, one experienced base stealer noted that careless pitchers dig the toes on their back foot into the ground when they are about to pitch in order to get a better push off, but when they intend to turn and throw a pickoff, they do not.\nIf a batted ball is caught on the fly, the runner must return to his original base. In this case, a runner trying to steal is more likely to be caught off his original base, resulting in a double play. This is a minor risk of a steal attempt. It is offset by the fact that a ground ball double play is less likely.\nPlays involving baserunning.\nIn the \"hit-and-run play\", coaches coordinate the actions of runner and batter. The runner tries to steal and the batter swings at almost any pitch, if only to distract the catcher. If the batter makes contact, the runner has a greater chance of reaching the next base; if the batter gets a base hit, the runner will likely be able to take an extra base. If the batter fails to hit the ball, the hit-and-run becomes a pure steal attempt.\nThe less common cousin to the hit and run is the \"run and hit\" play. In the run and hit, the base runner attempts to advance when the pitcher commits the pitch to home plate, but the batter is instead directed to exercise his judgement as to whether or not to swing at the pitch. If the batter feels it is not advantageous to swing, AND he believes the base runner is very likely to succeed in the steal attempt, he does not swing. This play is typically utilized with elite base stealers and skilled batters only, wherein a highly experienced batsman is trusted to decide whether or not to \"protect\" the base runner. If the batter chooses not to swing, it becomes a pure steal attempt.\nIn the \"delayed steal\", the runner does not take advantage of the pitcher's duty to complete a pitch, but relies on surprise and takes advantage of any complacency by the fielders. The runner gives the impression he is not trying to steal, and does not break for the next base until the ball crosses the plate. It is rare for Major League defenses to be fooled, but the play is used effectively at the college level. The first delayed steal on record was performed by Miller Huggins in 1903. The delayed steal was famously practiced by Eddie Stanky of the Brooklyn Dodgers.\nSecond base is the base most often stolen, because once a runner is on second base he is considered to be in \"scoring position\", meaning that he is expected to be able to run home and score on most routine singles hit into the outfield. Second base is also the easiest to steal, as it is farthest from home plate and thus a longer throw from the catcher is required to prevent it. Third base is a shorter throw for the catcher, but the runner is able to take a longer lead off second base and can leave for third base earlier against a left-handed pitcher. A steal of home plate is the riskiest, as the catcher only needs to tag out the runner after receiving the ball from the pitcher. It is difficult for the runner to cover the distance between the bases before the ball arrives home. Ty Cobb holds the records for most steals of home in a single season (8) as well as for a career (54). Steals of home are not officially recorded statistics, and must be researched through individual game accounts. Thus Cobb's totals may be even greater than is recorded. Jackie Robinson famously stole home in Game 1 of the 1955 World Series. Thirty-five games have ended with a runner stealing home, but only two have occurred since 1980. In a variation on the steal of home, the batter is signaled to simultaneously execute a sacrifice bunt, which results in the \"squeeze play.\" The \"suicide squeeze\" is a squeeze in which the runner on third begins to steal home without seeing the outcome of the bunt; it is so named because if the batter fails to bunt, the runner will surely be out. In contrast, when the runner on third does not commit until seeing that the ball is bunted advantageously, it is called a \"safety squeeze.\"\nIn more recent years, most steals of home involve a \"delayed double steal\", in which a runner on first attempts to steal second, while the runner on third breaks for home as soon as the catcher throws to second base. If it is important to prevent the run from scoring, the catcher may hold on to the ball (conceding the steal of second) or may throw to the pitcher; this may deceive the runner at third and the pitcher may throw back to the catcher for the out.\nStatistics.\nIn baseball statistics, stolen bases are denoted by \"SB\". Attempts to steal that result in the baserunner being out are \"caught stealing\" (\"CS\"). The sum of these statistics is \"steal attempts.\" Successful steals as a percentage of total steal attempts is called the \"success rate\".\nThe rule on stolen bases states that:\nRelative skill at stealing bases can be judged by evaluating either a player's total number of steals or the success rate. Noted statistician Bill James has argued that unless a player has a high success rate (67\u201370% or better), attempting to steal a base is detrimental to a team.\nComparing skill against players from other eras is problematic, because the definition has not been constant. Caught stealing was not recorded regularly until the middle of the 20th century. Ty Cobb, for example, was known as a great base-stealer, with 892 steals and a success rate of over 83%. However, the data on Cobb's caught stealing is missing from 12 seasons, strongly suggesting he was unsuccessful many more times than his stats indicate. Carlos Beltr\u00e1n, with 286 steals, has the highest career success rate of all players with over 300 stolen base attempts, at 88.3%.\nEvolution of rules and scoring.\nThe first mention of the stolen base as a statistic was in the 1877 scoring rules adopted by the National League, which noted credit toward a player's total bases when a base is stolen. It was not until 1886 that the stolen base appeared as something to be tracked, but was only to \"appear in the summary of the game\".\nIn 1887, the stolen base was given its own individual statistical column in the box score, and was defined for purposes of scoring: \"... every base made after first base has been reached by a base runner, except for those made by reason of or with the aid of a battery error (wild pitch or passed ball), or by batting, balks or by being forced off. In short, shall include all bases made by a clean steal, or through a wild throw or muff of the ball by a fielder who is directly trying to put the base runner out while attempting to steal.\" The next year, it was clarified that any attempt to steal must be credited to the runner, and that fielders committing errors during this play must also be charged with an error. This rule also clarified that advancement of another base(s) beyond the one being stolen is not credited as a stolen base on the same play, and that an error is charged to the fielder who permitted the extra advancement. There was clarification that a runner is credited with a steal if the attempt began before a battery error. Finally, batters were credited with a stolen base if they were tagged out after over running the base.\nIn 1892, a rule credited runners with stolen bases if a base runner advanced on a fly out, or if they advanced more than one base on any safe hit or attempted out, providing an attempt was made by the defense to put the runner out. The rule was rescinded in 1897.\nIn 1898, stolen base scoring was narrowed to no longer include advancement in the event of a fielding error, or advancement caused by a hit batsman.\n1904 saw an attempt to reduce the already wordy slew of rules governing stolen bases, with the stolen base now credited when \"the advances a base unaided by a base hit, a put out, (or) a fielding or batter error.\"\n1910 saw the first addressing of the double and triple steal attempts. Under the new rule, when any runner is thrown out, and the other(s) are successful, the successful runners will not be credited with a stolen base.\nWithout using the term, 1920 saw the first rule that would be referred to today as defensive indifference, as stolen bases would not be credited, unless an effort was made to stop the runner by the defense. This is usually called if such is attempted in the ninth inning while that player's team is trailing, unless the runner represents the potential tying run.\n1931 saw a further narrowing of the criteria for awarding a stolen base. Power was given to the official scorer, in the event of a muff by the catcher in throwing, that in the judgment of the scorer the runner would have been out, to credit the catcher with an error, and not credit the runner with a stolen base. Further, any successful steal on a play resulting in a wild pitch, passed ball, or balk would no longer be credited as a steal, even if the runner had started to steal before the play.\nOne of the largest rewrites to the rules in history came in 1950. The stolen base was specifically to be credited \"to a runner whenever he advances one base unaided by a base hit, a putout, a forceout, a fielder's choice, a passed ball, a wild pitch, or a balk.\"\nThere were noted exceptions, such as denying a stolen base to an otherwise successful steal as a part of a double or triple steal, if one other runner was thrown out in the process. A stolen base would be awarded to runners who successfully stole second base as a part of a double steal with a man on third, if the other runner failed to steal home, but instead was able to return safely to third base. Runners who are tagged out oversliding the base after an otherwise successful steal would not be credited with a stolen base. Indifference was also credited as an exception. Runners would now be credited with stolen bases if they had begun the act of stealing, and the resulting pitch was wild, or a passed ball. Finally, for 1950 only, runners would be credited with a stolen base if they were \"well advanced\" toward the base they were attempting to steal, and the pitcher is charged with a balk, with the further exception of a player attempting to steal, who would otherwise have been forced to advance on the balk by a runner behind them. This rule was removed in 1951.\nA clarification came in 1955 that awarded a stolen base to a runner even if he became involved in a rundown, provided he evaded the rundown and advanced to the base he intended to steal.\nThe criteria for \"caught stealing\" were fine-tuned in 1979, with a runner being charged with being caught if he is put out while trying to steal, overslides a base (otherwise successfully stolen), or is picked off a base and tries to advance to the next base. It is explicitly not caught stealing to be put out after a wild pitch or passed ball.\nIn 2023, base stealing increased due to new rules affecting pitchers: A pitch clock limited the amount of time a pitcher had to pitch and pickoff attempts were limited to two per at-bat. In addition, the bases were enlarged from to , making stealing slightly easier.\n\"Stealing first\".\nWhile not recorded as a stolen base, the same dynamic between batter/runner and defense is on display in the case of an uncaught third strike. The batter/runner can avoid an out and become a baserunner by reaching first base ahead of the throw. This case is a strikeout that is not an out; the batter/runner's acquisition of first base is scored as a passed ball, a wild pitch, or an error.\nIn baseball's earlier decades, a runner on second base could \"steal\" first base, perhaps with the intention of drawing a throw that might allow a runner on third to score (a tactic famously employed by Germany Schaefer). However, such a tactic was not recorded as a stolen base. MLB rules now forbid running clockwise on the basepaths to \"confuse the defense or make a travesty of the game\". Further, after the pitcher assumes the pitching position, runners cannot return to any previous base.\nIn a game on August 16, 1987, Toronto Blue Jays center fielder Lloyd Moseby successfully stole second base on a throwing error by Chicago White Sox catcher Carlton Fisk that went well into center field. However, shortstop Ozzie Guillen faked as if the batter had hit a popfly, which would have required Moseby to return to first base to avoid getting doubled off. Moseby made it back to first base, but another throwing error sent the ball to the infield wall, giving Moseby another chance to steal second, which he did. This chaos led the announcer to say, \"He doesn't know where the throw is; he's going back to first base! Is he going to steal first? He steals first! Now he's going to steal second again! I've never seen it before!\" This bizarre play was officially scored as a baserunner advancing on a throwing error by the center fielder, ironically resulting in neither a stolen base awarded nor an error charged to the catcher.\nIn a game on April 19, 2013, Milwaukee Brewers shortstop Jean Segura stole second base in the bottom of the eighth inning. After the batter up, Ryan Braun, walked, Segura broke early for third base and the pitcher, Shawn Camp of the Chicago Cubs, threw ahead of him. As Segura was chased back to second base, Braun advanced to second as well and was tagged out. Segura, thinking \"he\" was out, began to return to the home dugout behind first base, but first base coach Garth Iorg directed him to stand at first. Segura had not intentionally run the bases backwards as a deception or mockery, but no fielder tried to tag him out. Later in the inning, he attempted to steal second for the second time, but was thrown out by catcher Welington Castillo.\nThe expression \"You can't steal first base\" is sometimes used in reference to a player who is fast but not very good at getting on base in the first place.\nFormer Pittsburgh Pirates and Seattle Mariners manager Lloyd McClendon is jokingly referred to as having \"stolen first\" in a June 26, 2001, game as the manager of the Pirates: after being ejected for disputing a call at first base, he yanked the base out of the ground and left the field with it, delaying the game. Of the incident, McClendon said \"I told him he wasn't using it, so I thought I'd take it.\" When a groundskeeper came out to replace the bag, the crowd booed him.\nThe independent Atlantic League instituted a new rule for the second half of the 2019 season, allowing batters to become runners on any pitch not \"caught in flight\" by the catcher, as they can throughout baseball after most uncaught third strikes. On July 13, 2019, outfielder Tony Thomas of the Southern Maryland Blue Crabs became the first player to reach first base under this rule. The press described this as \"stealing first base\", though it is scored as described above."}
{"id": "3812", "revid": "571915", "url": "https://en.wikipedia.org/wiki?curid=3812", "title": "Plate appearance", "text": "In baseball, a player is credited with a plate appearance (denoted by PA) each time he completes a turn batting. Under Rule 5.04(c) of the Official Baseball Rules, a player completes a turn batting when he is put out or becomes a runner. This happens when he strikes out or is declared out before reaching first base; or when he reaches first base safely or is awarded first base (by a base on balls, hit by pitch, catcher's interference, or obstruction); or when he hits a fair ball which causes a preceding runner to be put out for the third out before he himself is put out or reaches first base safely (\"see also\" left on base, fielder's choice, force play). A very similar baseball statistic, at bats, counts a subset of plate appearances that end under certain circumstances.\nUse as batting record qualifier.\nAt bats - rather than plate appearances - are used to calculate batting averages, slugging percentages. However, starting in 1957, at season's end a player must have accumulated a minimum number of plate appearances during a season to be ranked as a league-leader in certain statistical categories. For batting championships in MLB, this number is 3.1 plate appearances multiplied by the number of scheduled games in a season, rounded up or down to the nearest whole number. As of 2024, with a 162-game regular season, this means 502 plate appearances are required to qualify. A lesser criterion applies in the minor leagues, with 2.7 plate appearances per game required to qualify.\nFor example, Player A gets 100 hits in 400 at bats over 510 plate appearances, which works out to a .250 batting average (equivalent to one hit in every four at-bats). Alternatively, Player B gets 110 hits in 400 at bats over 490 plate appearances during the same season, finishing with a .275 batting average. Player B, even though he had the same amount of at bats as Player A and even though his batting average is higher, will not be eligible for certain percentage-based season-ending rankings because he did not accumulate the required 502 plate appearances, while Player A did and therefore will be eligible. There is, however, an exception:\nException for batting titles.\nRule 9.22(a) of the Official Baseball Rules make a single allowance to the minimum requirement of 502 plate appearances for the purposes of determining the batting, slugging or on-base percentage title. If a player:\nhe will win that title, but with his original statistic (before the extra at bats were added).\nIn the example above, Player B is 12 plate appearances short of the required 502, but were he be charged with 12 additional unproductive at bats, he would go 110-for-412 for a batting average of .267. If no one else has a batting average (similarly modified if appropriate) higher than .267, player B will be awarded the batting title (with his original batting average of .275) despite the lack of 502 plate appearances.\nIn a real-life example, in 2012, Melky Cabrera, then of the San Francisco Giants, finished the season with a league-high .346 batting average, but he had only 501 plate appearances, one short of the required 502. Per the rule, he would have won the batting title because after an extra at bat is added and his batting average recalculated, he still would have led the league in batting average. Cabrera's case, however, turned out differently. The reason Cabrera finished the season with only 501 plate appearances was because he was suspended in mid-August when he tested positive for illegal performance-enhancing drugs. Cabrera was still eligible for that extra at bat, but he requested that the extra at bat not be added to his total, and that he not be considered for the batting crown, because he admitted that his use of performance-enhancing drugs had given him an unfair advantage over other players. As a result, Cabrera's name is nowhere to be found on the list of 2012 National League batting leaders.\nScoring.\nA batter is not credited with a plate appearance if, while batting, a preceding runner is put out on the basepaths for the third out in a way other than by the batter putting the ball into play (i.e., picked off, caught stealing). In this case, the same batter continues his turn batting in the next inning with no balls or strikes against him.\nA batter is not credited with a plate appearance if, while batting, the game ends as the winning run scores from third base on a balk, stolen base, wild pitch or passed ball.\nA batter may or may not be credited with a plate appearance (and possibly at bat) in the rare instance when he is replaced by a pinch hitter after having already started his turn at bat. Under Rule 9.15(b), the pinch hitter would receive the plate appearance (and potential of an at-bat) unless the original batter is replaced when having 2 strikes against him and the pinch hitter subsequently completes the strikeout, in which case the plate appearance and at-bat are charged to the first batter.\nRelation to at bat.\nUnder Official Baseball Rule 9.02(a)(1), an at bat results from a completed plate appearance, unless the batter:\nIn common parlance, the term \"at bat\" is sometimes used to mean \"plate appearance\" (for example, \"he fouled off the ball to keep the \"at bat\" alive\"). The intent is usually clear from the context, although the term \"official at bat\" is sometimes used to explicitly refer to an \"at bat\" as distinguished from a \"plate appearance\". However, terms such as \"turn at bat\" or \"time at bat\" are synonymous with \"plate appearance\".\n\"Time at bat\" in the rulebook.\nOfficial Baseball Rule 5.06(c) provides that \"[a] batter has legally completed his \"time at bat\" when he is put out or becomes a runner\" (emphasis added). The \"time at bat\" defined in this rule is more commonly referred to as a plate appearance, and the playing rules (Rules 1 through 8) uses the phrase \"time at bat\" in this sense (e.g. Rule 5.04(a)(3), which states that \"[t]he first batter in each inning after the first inning shall be the player whose name follows that of the last player who legally completed his \"time at bat\" in the preceding inning\" (emphasis added)). In contrast, the scoring rules uses the phrase \"time at bat\" to refer to the statistic at bat, defined in Rule 9.02(a)(1), but sometimes uses the phrase \"official time at bat\" or refers back to Rule 9.02(a)(1) when mentioning the statistic. The phrase \"plate appearance\" is used in Rules 9.22 and 9.23 dealing with batting titles and hitting streaks, and in Rule 5.10(g) Comment in relation to the Three-Batter Minimum: \"[t]o qualify as one of three consecutive batters, the batter must complete his \"plate appearance\", which ends only when the batter is put out or becomes a runner.\" (emphasis added) The term is not elsewhere defined in the rulebook.\nIn on-base percentage.\nPlate appearances are a primary component in calculating on-base percentage (OBP), an alternative measurement of a player's offensive performance, but are not the only one in determining its denominator.\nBy rule, certain plate appearances, such as times reached base via either catcher's interference or fielder's obstruction or sacrifice bunts, are excluded from it, leaving the denominator determined instead as the sum of at-bats, walks, hit-by-pitches, and sacrifice flies. And the numerator represented by a batter's times on base (composed of the sum of hits, base on balls, and times hit by pitch).\nOther uses.\nPlate appearances are used by scorers for \"proving\" a box score. Under Rule 9.03(c), the following two items should be equal for each team, because each is equal to the team's total number of plate appearances:"}
{"id": "3814", "revid": "47853293", "url": "https://en.wikipedia.org/wiki?curid=3814", "title": "Games played", "text": "Games played (GP) is a statistic used in team sports to indicate the total number of games in which a player has participated (in any capacity); the statistic is generally applied irrespective of whatever portion of the game is contested.\nAssociation football.\nIn association football, a game played is counted if a player is in the starting 11, or if a reserve player enters the game before full-time.\nBaseball.\nIn baseball, the statistic applies to players, who prior to a game, are included on a starting lineup card or are announced as an \"ex ante\" substitute, whether or not they play. For pitchers only, the statistic games pitched is used.\nA notable example of the application of the above rule is pitcher Larry Yount, who suffered an injury while throwing warmup pitches after being summoned as a reliever in a Major League Baseball (MLB) game on September 15, 1971. He did not face a batter, but was credited with an appearance because he had been announced as a substitute. Yount never appeared in (or actually played in) any other MLB game.\nBasketball.\nRobert Parish has the NBA record for most regular season games played, with 1,611. A. C. Green has the NBA record for most consecutive games played, with 1,192."}
{"id": "3817", "revid": "1272687090", "url": "https://en.wikipedia.org/wiki?curid=3817", "title": "Boogie Down Productions", "text": "Boogie Down Productions (BDP) was an American hip hop group originally composed of KRS-One, D-Nice, and DJ Scott La Rock. DJ Scott La Rock was murdered on August 27, 1987, five months after the release of BDP's debut album, \"Criminal Minded\". The name of the group, Boogie Down, derives from a nickname for the South Bronx section of New York City. The group pioneered the fusion of dancehall reggae and hip hop music and their debut LP \"Criminal Minded\" contained frank descriptions of life in the South Bronx during the late 1980s, thus setting the stage for what would eventually become Hip Hop.\nMembers.\nBDP's membership changed throughout its existence, the only constant being KRS-One. The group was founded by KRS-One and DJ Scott La Rock, with producer Lee Smith, who was essential in the production of the songs on \"Criminal Minded\", being added as a member shortly after. From those beginnings, BDP members and collaborators included Ced Gee of Ultramagnetic MC's, Lee Smith, Scott La Rock, D-Nice, Henry Wilkerson PoppyDa, Kenny Parker (KRS-One's younger brother), Just-Ice, ICU, McBoo, Ms. Melodie, Heather B., Scottie Morris, Tony Rahsan, Willie D., RoboCop, Harmony, DJ Red Alert, Jay Kramer, D-Square, Rebekah Foster, Scott Whitehill, Scott King, Chris Tait and Sidney Mills. BDP as a group essentially ended because KRS-One began recording and performing under his own name rather than the group name. Lee Smith, who has co-producer credit on the original 12\" \"South Bronx\" single, was the first to be jettisoned by KRS-One and the future new label after Scott's death.\nIn the liner notes on BDP's 1992 album \"Sex and Violence\", KRS-One writes: \"BDP in 1992 is KRS-One, Willie D, and Kenny Parker! BDP is not D-Nice, Jamal-Ski, Harmony, Ms. Melodie, and Scottie Morris. They are not down with BDP so stop frontin'.\" Steve \"Flash\" Juon of RapReviews.com claimed that this initiated the ultimate breakup of the group.\nCultural influences and impact.\n\"The Bridge Wars\".\nA conflict arose in the late 1980s concerning the origins of hip-hop, and BDP made conscious efforts in its early work to establish its interpretation of the issue. The origins of hip-hop to many, including BDP, are believed to be from the Bronx. A rival hip-hop collective, known as the Juice Crew's lyrics, were misunderstood to contain a claim in the song \"The Bridge\" that hip hop was directly a result of artists originating from Queensbridge. Boogie Down and KRS retorted angrily with songs such as \"The Bridge is Over\" and \"South Bronx,\" which started one of the first notable hip hop wars as MC Shan, Marley Marl, Roxanne Shant\u00e9 and Blaq Poet all released songs featuring verses personally attacking KRS and Scott La Rock. But the Bridge Wars were short-lived, and after Scott La Rock's death, KRS began to concentrate on socially conscious music.\nWhile \"Criminal Minded\" contained vivid descriptions of South Bronx street life, BDP changed after Scott's death. Lee Smith was dropped and KRS-One adopted the Teacha moniker and made a deliberate attempt at creating politically and socially conscious hip-hop. BDP was influential in provoking political and social consciousness in hip-hop, for example in \"Stop The Violence\" on 1988's \"By All Means Necessary\".\nJamaican inspirations.\nThe Jamaican influence in \"Criminal Minded\" is well illustrated by the use of the \"Mad Mad\" or \"Diseases\" riddim started in 1981 with reggae star Yellowman's song \"Zunguzunguzeng.\" BDP used this riff in the song \"Remix for P is Free,\" and it was later resampled by artists such as Black Star and dead prez. As an album regarded by many as the start of the gangsta rap movement, \"Criminal Minded\" played an important role in reaffirming the social acceptance of having Jamaican roots. BDP referenced reggae in a way that helped to solidify Jamaica's place in modern hip-hop culture.\nPolitical and social activism.\nFrom its start, BDP affected the development of hip-hop and gave a sincere voice to the reality of life in the South Bronx, a section of New York City clouded with poverty and crime. With \"Criminal Minded\", the group combined the sounds of LaRock's harsh, spare, reggae-influenced beats and KRS-One's long-winded rhyme style on underground classics such as \"9mm Goes Bang\" and \"South Bronx,\" the album's gritty portrait of life on the streets (as well as the firearms that adorned its cover) influenced the gangsta rap movement that began in earnest two years later.\nBDP's influence in the creation and development of gangsta rap highlights the cultural significance and impact of the type of music BDP and other early hip-hop artists like it created. This subgenre of hip-hop is most closely associated with hard-core hip-hop and is widely misinterpreted as promoting violence and gang activity. This misinterpretation or stigma is closely related to Boogie Down Productions and the general purpose behind their underlying themes of violence. For instance, the cover art of \"Criminal Minded\" displays the two artists in the group brandishing drawn guns and displaying other firearms. This is not an encouragement of the violence described in BDP's music, but a portrayal of the violence in the South Bronx as a means of expression, escape, and even condemnation. This album art is not meant to advocate violence but to challenge the conception of a criminal, to assert that those who are really criminally minded are those who hold power.\nBDP's music became significantly more politically astute after Scott La Rock's death. KRS-One published four more albums under the title Boogie Down Productions, and each was increasingly innovative and expanded from the thuggish imagery of \"Criminal Minded,\" exploring themes like black-on-black crime and black radicalism, using a riff on the words of Malcolm X, \"by any means necessary\", which became the title of the second BDP album, and remains one of the most political hip-hop albums to date. It was in this album that KRS defined himself as the \"teacha\" or \"teacher\", symbolizing his emphasis on educating his audience members and fans about relevant social issues surrounding the African-American experience.\nDuring his time in association with Boogie Down Productions, KRS-One joined other rappers to create the Stop the Violence Movement, which addressed many of the issues brought up in BDP's music and is the most conscious effort displayed by KRS-One and BDP of political activism and engagement. The movement created the single \"Self-Destruction\" in 1989 through the collaboration of BDP (KRS-One, D-Nice &amp; Ms. Melodie), Stetsasonic (Delite, Daddy-O, Wise, and Frukwan), Kool Moe Dee, MC Lyte, Doug E. Fresh, Just-Ice, Heavy D, Biz Markie, and Public Enemy (Chuck D &amp; Flavor Flav), with the aim of spreading awareness about violence in African-American and hip-hop communities. All proceeds from this effort went to the National Urban League."}
{"id": "3818", "revid": "23743", "url": "https://en.wikipedia.org/wiki?curid=3818", "title": "BABELFISH", "text": ""}
{"id": "3820", "revid": "1812441", "url": "https://en.wikipedia.org/wiki?curid=3820", "title": "Brain event", "text": ""}
{"id": "3821", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=3821", "title": "Binary-coded decimal", "text": "In computing and electronic systems, binary-coded decimal (BCD) is a class of binary encodings of decimal numbers where each digit is represented by a fixed number of bits, usually four or eight. Sometimes, special bit patterns are used for a sign or other indications (e.g. error or overflow).\nIn byte-oriented systems (i.e. most modern computers), the term \"unpacked\" BCD usually implies a full byte for each digit (often including a sign), whereas \"packed\" BCD typically encodes two digits within a single byte by taking advantage of the fact that four bits are enough to represent the range 0 to 9. The precise four-bit encoding, however, may vary for technical reasons (e.g. Excess-3).\nThe ten states representing a BCD digit are sometimes called \"tetrades\" (the nibble typically needed to hold them is also known as a tetrade) while the unused, don't care-states are named \"pseudo-tetrad(e)s\", \"pseudo-decimals\", or \"pseudo-decimal digits\".\nBCD's main virtue, in comparison to binary positional systems, is its more accurate representation and rounding of decimal quantities, as well as its ease of conversion into conventional human-readable representations. Its principal drawbacks are a slight increase in the complexity of the circuits needed to implement basic arithmetic as well as slightly less dense storage.\nBCD was used in many early decimal computers, and is implemented in the instruction set of machines such as the IBM System/360 series and its descendants, Digital Equipment Corporation's VAX, the Burroughs B1700, and the Motorola 68000-series processors.\nBCD \"per se\" is not as widely used as in the past, and is unavailable or limited in newer instruction sets (e.g., ARM; x86 in long mode). However, decimal fixed-point and decimal floating-point formats are still important and continue to be used in financial, commercial, and industrial computing, where the subtle conversion and fractional rounding errors that are inherent in binary floating point formats cannot be tolerated.\nBackground.\nBCD takes advantage of the fact that any one decimal numeral can be represented by a four-bit pattern. An obvious way of encoding digits is \"Natural BCD\" (NBCD), where each decimal digit is represented by its corresponding four-bit binary value, as shown in the following table. This is also called \"8421\" encoding.\nThis scheme can also be referred to as \"Simple Binary-Coded Decimal\" (\"SBCD\") or \"BCD\u00a08421\", and is the most common encoding. Others include the so-called \"4221\" and \"7421\" encoding \u2013 named after the weighting used for the bits \u2013 and \"Excess-3\". For example, the BCD digit 6, in 8421 notation, is in 4221 (two encodings are possible), in 7421, while in Excess-3 it is (formula_1).\nThe following table represents decimal digits from 0 to 9 in various BCD encoding systems. In the headers, the \"codice_1\" indicates the weight of each bit. In the fifth column (\"BCD\u00a084\u22122\u22121\"), two of the weights are negative. Both ASCII and EBCDIC character codes for the digits, which are examples of zoned BCD, are also shown.\nAs most computers deal with data in 8-bit bytes, it is possible to use one of the following methods to encode a BCD number:\nAs an example, encoding the decimal number codice_2 using unpacked BCD results in the following binary pattern of two bytes:\n Decimal: 9 1\n Binary : 0000 1001 0000 0001\nIn packed BCD, the same number would fit into a single byte:\n Decimal: 9 1\n Binary : 1001 0001\nHence the numerical range for one unpacked BCD byte is zero through nine inclusive, whereas the range for one packed BCD byte is zero through ninety-nine inclusive.\nTo represent numbers larger than the range of a single byte any number of contiguous bytes may be used. For example, to represent the decimal number codice_3 in packed BCD, using big-endian format, a program would encode as follows:\n Decimal: 0 1 2 3 4 5\n Binary : 0000 0001 0010 0011 0100 0101\nHere, the most significant nibble of the most significant byte has been encoded as zero, so the number is stored as codice_4 (but formatting routines might replace or remove leading zeros). Packed BCD is more efficient in storage usage than unpacked BCD; encoding the same number (with the leading zero) in unpacked format would consume twice the storage.\nShifting and masking operations are used to pack or unpack a packed BCD digit. Other bitwise operations are used to convert a numeral to its equivalent bit pattern or reverse the process.\nPacked BCD.\nIn packed BCD (or packed decimal), each nibble represents a decimal digit. Packed BCD has been in use since at least the 1960s and is implemented in all IBM mainframe hardware since then. Most implementations are big endian, i.e. with the more significant digit in the upper half of each byte, and with the leftmost byte (residing at the lowest memory address) containing the most significant digits of the packed decimal value. The lower nibble of the rightmost byte is usually used as the sign flag, although some unsigned representations lack a sign flag.\nAs an example, a 4-byte value consists of 8 nibbles, wherein the upper 7 nibbles store the digits of a 7-digit decimal value, and the lowest nibble indicates the sign of the decimal integer value. Standard sign values are 1100 (hex C) for positive (+) and 1101 (D) for negative (\u2212). This convention comes from the zone field for EBCDIC characters and the signed overpunch representation.\nOther allowed signs are 1010 (A) and 1110 (E) for positive and 1011 (B) for negative. IBM System/360 processors will use the 1010 (A) and 1011 (B) signs if the A bit is set in the PSW, for the ASCII-8 standard that never passed. Most implementations also provide unsigned BCD values with a sign nibble of 1111 (F). ILE RPG uses 1111 (F) for positive and 1101 (D) for negative. These match the EBCDIC zone for digits without a sign overpunch. In packed BCD, the number 127 is represented by 0001 0010 0111 1100 (127C) and \u2212127 is represented by 0001 0010 0111 1101 (127D). Burroughs systems used 1101 (D) for negative, and any other value is considered a positive sign value (the processors will normalize a positive sign to 1100 (C)).\nNo matter how many bytes wide a word is, there is always an even number of nibbles because each byte has two of them. Therefore, a word of \"n\" bytes can contain up to (2\"n\")\u22121 decimal digits, which is always an odd number of digits. A decimal number with \"d\" digits requires (\"d\"+1) bytes of storage space.\nFor example, a 4-byte (32-bit) word can hold seven decimal digits plus a sign and can represent values ranging from \u00b19,999,999. Thus the number \u22121,234,567 is 7 digits wide and is encoded as:\n 0001 0010 0011 0100 0101 0110 0111 1101\n 1 2 3 4 5 6 7 \u2212\nLike character strings, the first byte of the packed decimal that with the most significant two digits is usually stored in the lowest address in memory, independent of the endianness of the machine.\nIn contrast, a 4-byte binary two's complement integer can represent values from \u22122,147,483,648 to +2,147,483,647.\nWhile packed BCD does not make optimal use of storage (using about 20% more memory than binary notation to store the same numbers), conversion to ASCII, EBCDIC, or the various encodings of Unicode is made trivial, as no arithmetic operations are required. The extra storage requirements are usually offset by the need for the accuracy and compatibility with calculator or hand calculation that fixed-point decimal arithmetic provides. Denser packings of BCD exist which avoid the storage penalty and also need no arithmetic operations for common conversions.\nPacked BCD is supported in the COBOL programming language as the \"COMPUTATIONAL-3\" (an IBM extension adopted by many other compiler vendors) or \"PACKED-DECIMAL\" (part of the 1985 COBOL standard) data type. It is supported in PL/I as \"FIXED DECIMAL\". Beside the IBM System/360 and later compatible mainframes, packed BCD is implemented in the native instruction set of the original VAX processors from Digital Equipment Corporation and some models of the SDS Sigma series mainframes, and is the native format for the Burroughs Medium Systems line of mainframes (descended from the 1950s Electrodata 200 series).\nTen's complement representations for negative numbers offer an alternative approach to encoding the sign of packed (and other) BCD numbers. In this case, positive numbers always have a most significant digit between 0 and 4 (inclusive), while negative numbers are represented by the 10's complement of the corresponding positive number.\nAs a result, this system allows for 32-bit packed BCD numbers to range from \u221250,000,000 to +49,999,999, and \u22121 is represented as 99999999. (As with two's complement binary numbers, the range is not symmetric about zero.)\nFixed-point packed decimal.\nFixed-point decimal numbers are supported by some programming languages (such as COBOL and PL/I). These languages allow the programmer to specify an implicit decimal point in front of one of the digits.\nFor example, a packed decimal value encoded with the bytes 12 34 56 7C represents the fixed-point value +1,234.567 when the implied decimal point is located between the fourth and fifth digits:\n 12 34 56 7C\n \"12 34.56 7+\"\nThe decimal point is not actually stored in memory, as the packed BCD storage format does not provide for it. Its location is simply known to the compiler, and the generated code acts accordingly for the various arithmetic operations.\nHigher-density encodings.\nIf a decimal digit requires four bits, then three decimal digits require 12 bits. However, since 210 (1,024) is greater than 103 (1,000), if three decimal digits are encoded together, only 10 bits are needed. Two such encodings are \"Chen\u2013Ho encoding\" and \"densely packed decimal\" (DPD). The latter has the advantage that subsets of the encoding encode two digits in the optimal seven bits and one digit in four bits, as in regular BCD.\nZoned decimal.\nSome implementations, for example IBM mainframe systems, support zoned decimal numeric representations. Each decimal digit is stored in one byte, with the lower four bits encoding the digit in BCD form. The upper four bits, called the \"zone\" bits, are usually set to a fixed value so that the byte holds a character value corresponding to the digit. EBCDIC systems use a zone value of 1111 (hex F); this yields bytes in the range F0 to F9 (hex), which are the EBCDIC codes for the characters \"0\" through \"9\". Similarly, ASCII systems use a zone value of 0011 (hex 3), giving character codes 30 to 39 (hex).\nFor signed zoned decimal values, the rightmost (least significant) zone nibble holds the sign digit, which is the same set of values that are used for signed packed decimal numbers (see above). Thus a zoned decimal value encoded as the hex bytes F1 F2 D3 represents the signed decimal value \u2212123:\n F1 F2 D3\n 1 2 \u22123\nEBCDIC zoned decimal conversion table.\n(*) \"Note: These characters vary depending on the local character code page setting.\"\nFixed-point zoned decimal.\nSome languages (such as COBOL and PL/I) directly support fixed-point zoned decimal values, assigning an implicit decimal point at some location between the decimal digits of a number.\nFor example, given a six-byte signed zoned decimal value with an implied decimal point to the right of the fourth digit, the hex bytes F1 F2 F7 F9 F5 C0 represent the value +1,279.50:\n F1 F2 F7 F9 F5 C0\n 1 2 7 9. 5 +0\nOperations with BCD.\nAddition.\nIt is possible to perform addition by first adding in binary, and then converting to BCD afterwards. Conversion of the simple sum of two digits can be done by adding 6 (that is, 16 \u2212 10) when the five-bit result of adding a pair of digits has a value greater than 9. The reason for adding 6 is that there are 16 possible 4-bit BCD values (since 24 = 16), but only 10 values are valid (0000 through 1001). For example:\n 1001 + 1000 = 10001\n 9 + 8 = 17\n10001 is the binary, not decimal, representation of the desired result, but the most significant 1 (the \"carry\") cannot fit in a 4-bit binary number. In BCD as in decimal, there cannot exist a value greater than 9 (1001) per digit. To correct this, 6 (0110) is added to the total, and then the result is treated as two nibbles:\n 10001 + 0110 = 00010111 =&gt; 0001 0111\n 17 + 6 = 23 1 7\nThe two nibbles of the result, 0001 and 0111, correspond to the digits \"1\" and \"7\". This yields \"17\" in BCD, which is the correct result.\nThis technique can be extended to adding multiple digits by adding in groups from right to left, propagating the second digit as a carry, always comparing the 5-bit result of each digit-pair sum to 9. Some CPUs provide a half-carry flag to facilitate BCD arithmetic adjustments following binary addition and subtraction operations. The Intel 8080, the Zilog Z80 and the CPUs of the x86 family provide the opcode DAA (Decimal Adjust Accumulator).\nSubtraction.\nSubtraction is done by adding the ten's complement of the subtrahend to the minuend. To represent the sign of a number in BCD, the number 0000 is used to represent a positive number, and 1001 is used to represent a negative number. The remaining 14 combinations are invalid signs. To illustrate signed BCD subtraction, consider the following problem: 357 \u2212 432.\nIn signed BCD, 357 is 0000 0011 0101 0111. The ten's complement of 432 can be obtained by taking the nine's complement of 432, and then adding one. So, 999 \u2212 432 = 567, and 567 + 1 = 568. By preceding 568 in BCD by the negative sign code, the number \u2212432 can be represented. So, \u2212432 in signed BCD is 1001 0101 0110 1000.\nNow that both numbers are represented in signed BCD, they can be added together:\n 0000 0011 0101 0111\n 0 3 5 7\n + 1001 0101 0110 1000\n 9 5 6 8\n = 1001 1000 1011 1111\n 9 8 11 15\nSince BCD is a form of decimal representation, several of the digit sums above are invalid. In the event that an invalid entry (any BCD digit greater than 1001) exists, 6 is added to generate a carry bit and cause the sum to become a valid entry. So, adding 6 to the invalid entries results in the following:\n 1001 1000 1011 1111\n 9 8 11 15\n + 0000 0000 0110 0110\n 0 0 6 6\n = 1001 1001 0010 0101\n 9 9 2 5\nThus the result of the subtraction is 1001 1001 0010 0101 (\u2212925). To confirm the result, note that the first digit is 9, which means negative. This seems to be correct since 357 \u2212 432 should result in a negative number. The remaining nibbles are BCD, so 1001 0010 0101 is 925. The ten's complement of 925 is 1000 \u2212 925 = 75, so the calculated answer is \u221275.\nIf there are a different number of nibbles being added together (such as 1053 \u2212 2), the number with the fewer digits must first be prefixed with zeros before taking the ten's complement or subtracting. So, with 1053 \u2212 2, 2 would have to first be represented as 0002 in BCD, and the ten's complement of 0002 would have to be calculated.\nBCD in computers.\nIBM.\nIBM used the terms \"Binary-Coded Decimal Interchange Code\" (BCDIC, sometimes just called BCD), for 6-bit \"alphanumeric\" codes that represented numbers, upper-case letters and special characters. Some variation of BCDIC \"alphamerics\" is used in most early IBM computers, including the IBM 1620 (introduced in 1959), IBM 1400 series, and non-decimal architecture members of the IBM 700/7000 series.\nThe IBM 1400 series are character-addressable machines, each location being six bits labeled \"B, A, 8, 4, 2\" and \"1,\" plus an odd parity check bit (\"C\") and a word mark bit (\"M\"). For encoding digits \"1\" through \"9\", \"B\" and \"A\" are zero and the digit value represented by standard 4-bit BCD in bits \"8\" through \"1\". For most other characters bits \"B\" and \"A\" are derived simply from the \"12\", \"11\", and \"0\" \"zone punches\" in the punched card character code, and bits \"8\" through \"1\" from the \"1\" through \"9\" punches. A \"12 zone\" punch set both \"B\" and \"A\", an \"11 zone\" set \"B\", and a \"0 zone\" (a 0 punch combined with any others) set \"A\". Thus the letter A, which is \"(12,1)\" in the punched card format, is encoded \"(B,A,1)\". The currency symbol $, \"(11,8,3)\" in the punched card, was encoded in memory as \"(B,8,2,1)\". This allows the circuitry to convert between the punched card format and the internal storage format to be very simple with only a few special cases. One important special case is digit \"0\", represented by a lone \"0\" punch in the card, and \"(8,2)\" in core memory.\nThe memory of the IBM 1620 is organized into 6-bit addressable digits, the usual \"8, 4, 2, 1\" plus \"F\", used as a flag bit and \"C\", an odd parity check bit. BCD \"alphamerics\" are encoded using digit pairs, with the \"zone\" in the even-addressed digit and the \"digit\" in the odd-addressed digit, the \"zone\" being related to the \"12\", \"11\", and \"0\" \"zone punches\" as in the 1400 series. Input/output translation hardware converted between the internal digit pairs and the external standard 6-bit BCD codes.\nIn the decimal architecture IBM 7070, IBM 7072, and IBM 7074 \"alphamerics\" are encoded using digit pairs (using two-out-of-five code in the digits, not BCD) of the 10-digit word, with the \"zone\" in the left digit and the \"digit\" in the right digit. Input/output translation hardware converted between the internal digit pairs and the external standard 6-bit BCD codes.\nWith the introduction of System/360, IBM expanded 6-bit BCD \"alphamerics\" to 8-bit EBCDIC, allowing the addition of many more characters (e.g., lowercase letters). A variable length packed BCD \"numeric\" data type is also implemented, providing machine instructions that perform arithmetic directly on packed decimal data.\nOn the IBM 1130 and 1800, packed BCD is supported in software by IBM's Commercial Subroutine Package.\nToday, BCD data is still heavily used in IBM databases such as IBM Db2 and processors such as z/Architecture and POWER6 and later Power ISA processors. In these products, the BCD is usually zoned BCD (as in EBCDIC or ASCII), packed BCD (two decimal digits per byte), or \"pure\" BCD encoding (one decimal digit stored as BCD in the low four bits of each byte). All of these are used within hardware registers and processing units, and in software.\nOther computers.\nThe Digital Equipment Corporation VAX series includes instructions that can perform arithmetic directly on packed BCD data and convert between packed BCD data and other integer representations. The VAX's packed BCD format is compatible with that on IBM System/360 and IBM's later compatible processors. The MicroVAX and later VAX implementations dropped this ability from the CPU but retained code compatibility with earlier machines by implementing the missing instructions in an operating system-supplied software library. This is invoked automatically via exception handling when the defunct instructions are encountered, so that programs using them can execute without modification on the newer machines.\nMany processors have hardware support for BCD-encoded integer arithmetic. For example, the 6502, the Motorola 68000 series, and the x86 series. The Intel x86 architecture supports a unique 18-digit (ten-byte) BCD format that can be loaded into and stored from the floating point registers, from where computations can be performed.\nIn more recent computers such capabilities are almost always implemented in software rather than the CPU's instruction set, but BCD numeric data are still extremely common in commercial and financial applications.\nThere are tricks for implementing packed BCD and zoned decimal add\u2013or\u2013subtract operations using short but difficult to understand sequences of word-parallel logic and binary arithmetic operations. For example, the following code (written in C) computes an unsigned 8-digit packed BCD addition using 32-bit binary operations:\nuint32_t BCDadd(uint32_t a, uint32_t b)\n uint32_t t1, t2; // unsigned 32-bit intermediate values\n t1 = a + 0x06666666;\n t2 = t1 ^ b; // sum without carry propagation\n t1 = t1 + b; // provisional sum\n t2 = t1 ^ t2; // all the binary carry bits\n t2 = ~t2 &amp; 0x11111110; // just the BCD carry bits\n t2 = (t2 \u00bb 2) | (t2 \u00bb 3); // correction\n return t1 - t2; // corrected BCD sum\nBCD in electronics.\nBCD is common in electronic systems where a numeric value is to be displayed, especially in systems consisting solely of digital logic, and not containing a microprocessor. By employing BCD, the manipulation of numerical data for display can be greatly simplified by treating each digit as a separate single sub-circuit.\nThis matches much more closely the physical reality of display hardware\u2014a designer might choose to use a series of separate identical seven-segment displays to build a metering circuit, for example. If the numeric quantity were stored and manipulated as pure binary, interfacing with such a display would require complex circuitry. Therefore, in cases where the calculations are relatively simple, working throughout with BCD can lead to an overall simpler system than converting to and from binary. Most pocket calculators do all their calculations in BCD.\nThe same argument applies when hardware of this type uses an embedded microcontroller or other small processor. Often, representing numbers internally in BCD format results in smaller code, since a conversion from or to binary representation can be expensive on such limited processors. For these applications, some small processors feature dedicated arithmetic modes, which assist when writing routines that manipulate BCD quantities.\nRepresentational variations.\nVarious BCD implementations exist that employ other representations for numbers. Programmable calculators manufactured by Texas Instruments, Hewlett-Packard, and others typically employ a floating-point BCD format, typically with two or three digits for the (decimal) exponent. The extra bits of the sign digit may be used to indicate special numeric values, such as infinity, underflow/overflow, and error (a blinking display).\nSigned variations.\nSigned decimal values may be represented in several ways. The COBOL programming language, for example, supports five zoned decimal formats, with each one encoding the numeric sign in a different way:\nTelephony binary-coded decimal (TBCD).\n3GPP developed TBCD, an expansion to BCD where the remaining (unused) bit combinations are used to add specific telephony characters, with digits similar to those found in telephone keypads original design.\nThe mentioned 3GPP document defines TBCD-STRING with swapped nibbles in each byte. Bits, octets and digits indexed from 1, bits from the right, digits and octets from the left.\nbits 8765 of octet \"n\" encoding digit 2\"n\"\nbits 4321 of octet \"n\" encoding digit 2(\"n\" \u2013 1) + 1\nMeaning number codice_5, would become codice_6 in TBCD.\nThis format is used in modern mobile telephony to send dialed numbers, as well as operator ID (the MCC/MNC tuple), IMEI, IMSI (SUPI), et.c.\nAlternative encodings.\nIf errors in representation and computation are more important than the speed of conversion to and from display, a scaled binary representation may be used, which stores a decimal number as a binary-encoded integer and a binary-encoded signed decimal exponent. For example, 0.2 can be represented as 2.\nThis representation allows rapid multiplication and division, but may require shifting by a power of 10 during addition and subtraction to align the decimal points. It is appropriate for applications with a fixed number of decimal places that do not then require this adjustment\u2014particularly financial applications where 2 or 4 digits after the decimal point are usually enough. Indeed, this is almost a form of fixed point arithmetic since the position of the radix point is implied.\nThe Hertz and Chen\u2013Ho encodings provide Boolean transformations for converting groups of three BCD-encoded digits to and from 10-bit values that can be efficiently encoded in hardware with only 2 or 3 gate delays. Densely packed decimal (DPD) is a similar scheme that is used for most of the significand, except the lead digit, for one of the two alternative decimal encodings specified in the IEEE 754-2008 floating-point standard.\nApplication.\nThe BIOS in many personal computers stores the date and time in BCD because the MC6818 real-time clock chip used in the original IBM PC AT motherboard provided the time encoded in BCD. This form is easily converted into ASCII for display.\nThe Atari 8-bit computers use a BCD format for floating point numbers. The MOS Technology 6502 processor has a BCD mode for the addition and subtraction instructions. The Psion Organiser 1 handheld computer's manufacturer-supplied software also uses BCD to implement floating point; later Psion models use binary exclusively.\nEarly models of the PlayStation 3 store the date and time in BCD. This led to a worldwide outage of the console on 1 March 2010. The last two digits of the year stored as BCD were misinterpreted as 16 causing an error in the unit's date, rendering most functions inoperable. This has been referred to as the Year 2010 problem.\nLegal history.\nIn the 1972 case \"Gottschalk v. Benson\", the U.S. Supreme Court overturned a lower court's decision that had allowed a patent for converting BCD-encoded numbers to binary on a computer.\nThe decision noted that a patent \"would wholly pre-empt the mathematical formula and in practical effect would be a patent on the algorithm itself\". This was a landmark judgement that determined the patentability of software and algorithms."}
{"id": "3822", "revid": "3311318", "url": "https://en.wikipedia.org/wiki?curid=3822", "title": "BCD", "text": "BCD may refer to:"}
{"id": "3823", "revid": "1247175821", "url": "https://en.wikipedia.org/wiki?curid=3823", "title": "Binary", "text": "Binary may refer to:"}
{"id": "3824", "revid": "135661153", "url": "https://en.wikipedia.org/wiki?curid=3824", "title": "Babelfish", "text": ""}
{"id": "3826", "revid": "26254750", "url": "https://en.wikipedia.org/wiki?curid=3826", "title": "Bumin Qaghan", "text": "Bumin Qaghan (, also known as Illig Qaghan (Chinese: \u4f0a\u5229\u53ef\u6c57, Pinyin: Y\u012bl\u00ec K\u00e8h\u00e1n, Wade\u2013Giles: i-li k'o-han) or Yam\u00ef Qaghan (, died 552 AD) was the founder of the Turkic Khaganate. He was the eldest son of Ashina Tuwu (\u5410\u52d9 / \u5410\u52a1). He was the chieftain of the Turks under the sovereignty of Rouran Khaganate. He is also mentioned as Tumen (, , commander of ten thousand) of the Rouran Khaganate.\nEarly life and reign.\nAccording to \"History of Northern Dynasties\" and \"Zizhi Tongjian\", in 545 Tumen's tribe started to rise and frequently invaded the western frontier of Wei. The chancellor of Western Wei, Yuwen Tai, sent An Nuopanto (\u5b89\u8afe\u76e4\u9640, Nanai-Banda, a Sogdian from Bukhara,) as an emissary to the G\u00f6kt\u00fcrk chieftain Tumen, in an attempt to establish a commercial relationship. In 546, Tumen paid tribute to the Western Wei state. In that same year, Tumen put down a revolt of the Tiele tribes against the Rouran Khaganate, their overlords. Following this, Tumen felt entitled to request of the Rouran a princess as his wife. The Rouran khagan, Anagui, sent a message refusing this request and adding: \"You are my blacksmith slave. How dare you utter these words?\" Bumin got angry, killed Anagui's emissary, and severed relations with the Rouran Khaganate. Anagui's \"blacksmith\" (\u935b\u5974 / \u953b\u5974, Pinyin: du\u00e0n n\u00fa, Wade\u2013Giles: tuan-nu) insult was recorded in Chinese chronicles. Some sources state that members of the Turks (referred as \"Tujue\" in Chinese sources) did serve as blacksmiths for the Rouran elite, and that \"blacksmith slavery\" may refer to a kind of vassalage that prevailed in Rouran society. Nevertheless, after this incident Bumin emerged as the leader of the revolt against Rouran.\nIn 551, Bumin requested a Western Wei princess in marriage. Yuwen Tai permitted it and sent of Western Wei to Bumin. In the same year when Emperor Wen of Western Wei died, Bumin sent mission and gave two hundred horses.\nThe beginning of formal diplomatic relations with China propped up Bumin's authority among the Turks. He eventually united the local Turkic tribes and threw off the yoke of the Rouran domination. In 552 Bumin's army defeated Anagui's forces at the north of Huaihuang and then Anagui committed suicide. With their defeat Bumin proclaimed himself \"Illig Qaghan\" and made his wife qaghatun. \"Illig\" means Ilkhan (i.e. ruler of people) in Old Turkic. According to the Bilge Qaghan's memorial complex and the Kul Tigin's memorial complex, Bumin and Istemi ruled people by Turkic laws and they developed them.\nDeath and family.\nBumin died within several months after proclaiming himself Illig Qaghan. He was married to Princess Changle of Western Wei. \nIssue:\nLegacy.\nHe was succeeded by his younger brother Istemi in the western part and by his son Issik Qaghan in the eastern part. In less than one century, his khaganate expanded to comprise most of Central Asia.\nReferences.\n "}
{"id": "3827", "revid": "114797", "url": "https://en.wikipedia.org/wiki?curid=3827", "title": "Bilge Qaghan", "text": "Bilge Qaghan (; ; 683 \u2013 25 November 734) was the fourth Qaghan of the Second Turkic Khaganate. His accomplishments were described in the Orkhon inscriptions. \nNames.\nAs was the custom, his personal name and the name after assuming the title Qaghan were different. His personal name was recorded in Chinese characters as (). His name after assuming the title was \"Bilg\u00e4 Qa\u03b3an\". ).\nEarly years.\nHe was born in 683, in the early years of the khaganate. He campaigned alongside his father from early childhood. He was created as Tardush shad and given command over the western wing of the empire in 697 by Qapaghan. He managed to annihilate Wei Yuanzhong's army in 701 with his brother. He also reconquered Basmyl tribes in 703. He also subdued Yenisei Kyrgyz forces in 709, after their disobedience had to reconquer and kill their Qaghan in 710. He killed T\u00fcrgesh khagan Suoge at Battle of Bolchu.\nIn later years of Qapaghan, he had to fight four battles in a year starting from 714, resubduing tribes and nearly was killed in an ambush from Uyghur forces in 716.\nReign.\nIn 716, Qapaghan Qaghan, the second Qaghan, was killed in his campaign against the Toquz Oghuz alliance and his severed head was sent to Chang'an. Although his son Inel Khagan succeeded him, Bilg\u00e4's brother Kul Tigin and Tonyukuk carried out a coup d'\u00e9tat against Inel Qaghan. They killed him and made him \"Bilg\u00e4\" \"Qaghan\". His name literally means \"wise king\".\nHe appointed his brother Kul Tigin to be Left Wise Prince, which made second most powerful person in realm. He re-subdued Huige in 716. He also appointed his father-in-law Tonyukuk to be Master Strategist.\nNew reforms and stabilization of the regime, caused tribes that fled Tujue to come back. Tang chancellor Wang Jun, believing that the G\u00f6kt\u00fcrks who surrendered would try to flee back to the G\u00f6kt\u00fcrk state, suggested that they be forcibly moved into the heart of the empire to prevent them from doing so. Before Wang's suggestion could be acted upon, however, there was an uprising by the G\u00f6kt\u00fcrks who surrendered, under the leadership of Xiedie Sitai (\ud860\udc42\u8dcc\u601d\u6cf0) and Axilan (\u963f\u6089\u721b). Xue and Wang tried to intercept them and dealt them defeats, but they were able to flee back to the G\u00f6kt\u00fcrk state anyway. This defeat led to Xue Ne's retirement.\nReligious policy.\nAt some point in his life, he thought about converting to Buddhism and settling in cities. However, Tonyukuk discouraged him from this, citing the Turks' few numbers and vulnerability to Chinese attacks. While the Turks' power rested on their mobility, conversion to Buddhism would bring pacifism among the population. Therefore, sticking to Tengrism was necessary for survival.\nLater reign.\nIn 720, Wang believed that the Pugu (\u50d5\u56fa) and Xiedie tribes of the region were planning to defect to Eastern Tujue and attack with Eastern Tujue troops. He thus held a feast and invited the chieftains, and, at the feast, massacred them. He then attacked the Pugu and Xiedie tribes in the area, nearly wiping them out. He then proposed a plan to attack Qaghan along with the Baximi, Xi, and Khitan. Emperor Xuanzong also recruited Qapaghan Khagan's sons Bilg\u00e4 Tigin and Mo Tigin, Yenisei Kyrgyz Qaghan Kutluk Bilg\u00e4 Qaghan and Huoba Guiren to fight against Tujue. Tonyukuk cunningly launched first attack on Baximi in 721 autumn, completely crushing them. Meanwhile, Bilg\u00e4 raided Gansu, taking much of the livestock. Later that year Khitans, next year Xi were also crushed.\nIn 726, his father-in-law and chancellor Tonyukuk died.\nIn 727, he sent Buyruk Chor () as an emissary to Xuanzong with 30 horses as a gift. He also warned him of Me Agtsom's proposal of an anti-Tang alliance. This alarm proved to be true when Tibetan general We Tadra Khonglo invaded Tang China in 727, sacked Guazhou (\u74dc\u5dde, in modern Gansu), Changle (\u5e38\u6a02, in south of modern Guazhou County), Changmenjun (\u9577\u9580\u8ecd, in north of modern Yumen) and Anxi (\u5b89\u897f, modern Lintan).\nOn 27 February 731, Kul Tigin died, for which Qaghan mourned and ordered a great funeral ceremony.\nIn 733, he defeated rebellious Khitan tribes.\nDeath.\nJust after sending an emissary to Xuanzong to gain heqin alliance, he was poisoned by Buyruk Chor. He did not die immediately and he had time to punish the family of Buyruk Chor with death. He died on 25 November 734, his burial ceremony took place on 22 June 735.\nFamily.\nHe was married to El Etmish Bilge Khatun, Tonyukuk's daughter. He had several children:\nLegacy.\nAfter his death from poisoning, several steles were erected in the capital area by the Orkhon River. These Orkhon inscriptions are the first known texts in the Old Turkic language.\nExternal links.\n "}
{"id": "3830", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=3830", "title": "Bryce Canyon", "text": ""}
{"id": "3831", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=3831", "title": "Britannica", "text": ""}
{"id": "3832", "revid": "31826409", "url": "https://en.wikipedia.org/wiki?curid=3832", "title": "Bauhaus", "text": "The Staatliches Bauhaus (), commonly known as the , was a German art school operational from 1919 to 1933 that combined crafts and the fine arts. The school became famous for its approach to design, which attempted to unify individual artistic vision with the principles of mass production and emphasis on function. Along with the doctrine of functionalism, the Bauhaus initiated the conceptual understanding of architecture and design.\nThe Bauhaus was founded by architect Walter Gropius in Weimar. It was grounded in the idea of creating a Gesamtkunstwerk (\"comprehensive artwork\") in which all the arts would eventually be brought together. The Bauhaus style later became one of the most influential currents in modern design, modernist architecture, and architectural education. The Bauhaus movement had a profound influence on subsequent developments in art, architecture, graphic design, interior design, industrial design, and typography. Staff at the Bauhaus included prominent artists such as Paul Klee, Wassily Kandinsky, Gunta St\u00f6lzl, and L\u00e1szl\u00f3 Moholy-Nagy at various points.\nThe school existed in three German cities\u2014Weimar, from 1919 to 1925; Dessau, from 1925 to 1932; and Berlin, from 1932 to 1933\u2014under three different architect-directors: Walter Gropius from 1919 to 1928; Hannes Meyer from 1928 to 1930; and Ludwig Mies van der Rohe from 1930 until 1933, when the school was closed by its own leadership under pressure from the Nazi regime, having been painted as a centre of communist intellectualism. Internationally, former key figures of Bauhaus were successful in the United States and became known as the \"avant-garde\" for the International Style. The White city of Tel Aviv to which numerous Jewish Bauhaus architects emigrated, has the highest concentration of the Bauhaus' international architecture in the world.\nThe changes of venue and leadership resulted in a constant shifting of focus, technique, instructors, and politics. For example, the pottery shop was discontinued when the school moved from Weimar to Dessau, even though it had been an important revenue source; when Mies van der Rohe took over the school in 1930, he transformed it into a private school and would not allow any supporters of Hannes Meyer to attend it.\nTerms and concepts.\nSeveral specific features are identified in the Bauhaus forms and shapes: simple geometric shapes like rectangles and spheres, without elaborate decorations. Buildings, furniture, and fonts often feature rounded corners, sometimes rounded walls, or curved chrome pipes. Some buildings are characterized by rectangular features, for example protruding balconies with flat, chunky railings facing the street, and long banks of windows. Some outlines can be defined as a tool for creating an ideal form, which is the basis of the architectural concept.\nBauhaus and German modernism.\nAfter Germany's defeat in World War I and the establishment of the Weimar Republic, a renewed liberal spirit allowed an upsurge of radical experimentation in all the arts, which had been suppressed by the old regime. Many Germans of left-wing views were influenced by the cultural experimentation that followed the Russian Revolution, such as constructivism. Such influences can be overstated: Gropius did not share these radical views, and said that Bauhaus was entirely apolitical. Just as important was the influence of the 19th-century English designer William Morris (1834\u20131896), who had argued that art should meet the needs of society and that there should be no distinction between form and function. Thus, the Bauhaus style, also known as the International Style, was marked by the absence of ornamentation and by harmony between the function of an object or a building and its design.\nHowever, the most important influence on Bauhaus was modernism, a cultural movement whose origins lay as early as the 1880s, and which had already made its presence felt in Germany before the World War, despite the prevailing conservatism. The design innovations commonly associated with Gropius and the Bauhaus\u2014the radically simplified forms, the rationality and functionality, and the idea that mass production was reconcilable with the individual artistic spirit\u2014were already partly developed in Germany before the Bauhaus was founded. The German national designers' organization Deutscher Werkbund was formed in 1907 by Hermann Muthesius to harness the new potentials of mass production, with a mind towards preserving Germany's economic competitiveness with England. In its first seven years, the Werkbund came to be regarded as the authoritative body on questions of design in Germany, and was copied in other countries. Many fundamental questions of craftsmanship versus mass production, the relationship of usefulness and beauty, the practical purpose of formal beauty in a commonplace object, and whether or not a single proper form could exist, were argued out among its 1,870 members (by 1914).\nGerman architectural modernism was known as Neues Bauen. Beginning in June 1907, Peter Behrens' pioneering industrial design work for the German electrical company AEG successfully integrated art and mass production on a large scale. He designed consumer products, standardized parts, created clean-lined designs for the company's graphics, developed a consistent corporate identity, built the modernist landmark AEG Turbine Factory, and made full use of newly developed materials such as poured concrete and exposed steel. Behrens was a founding member of the Werkbund, and both Walter Gropius and Adolf Meyer worked for him in this period.\nThe Bauhaus was founded at a time when the German zeitgeist had turned from emotional Expressionism to the matter-of-fact New Objectivity. An entire group of working architects, including Erich Mendelsohn, Bruno Taut and Hans Poelzig, turned away from fanciful experimentation and towards rational, functional, sometimes standardized building. Beyond the Bauhaus, many other significant German-speaking architects in the 1920s responded to the same aesthetic issues and material possibilities as the school. They also responded to the promise \"to promote the object of assuring to every German a healthful habitation\" written into the new Weimar Constitution (Article 155). Ernst May, Bruno Taut and Martin Wagner, among others, built large housing blocks in Frankfurt and Berlin. The acceptance of modernist design into everyday life was the subject of publicity campaigns, well-attended public exhibitions like the Weissenhof Estate, films, and sometimes fierce public debate.\nBauhaus and Vkhutemas.\nThe Vkhutemas, the Russian state art and technical school founded in 1920 in Moscow, has been compared to Bauhaus. Founded a year after the Bauhaus school, Vkhutemas has close parallels to the German Bauhaus in its intent, organization and scope. The two schools were the first to train artist-designers in a modern manner. Both schools were state-sponsored initiatives to merge traditional craft with modern technology, with a basic course in aesthetic principles, courses in color theory, industrial design, and architecture. Vkhutemas was a larger school than the Bauhaus, but it was less publicised outside the Soviet Union and consequently, is less familiar in the West.\nWith the internationalism of modern architecture and design, there were many exchanges between the Vkhutemas and the Bauhaus. The second Bauhaus director Hannes Meyer attempted to organise an exchange between the two schools, while Hinnerk Scheper of the Bauhaus collaborated with various Vkhutein members on the use of colour in architecture. In addition, El Lissitzky's book \"Russia: an Architecture for World Revolution\" published in German in 1930 featured several illustrations of Vkhutemas/Vkhutein projects there.\nHistory of the Bauhaus.\nWeimar.\nThe school was founded by Walter Gropius in Weimar on 1 April 1919, as a merger of the Grand-Ducal Saxon Academy of Fine Art and the Grand Ducal Saxon School of Arts and Crafts for a newly affiliated architecture department. Its roots lay in the arts and crafts school founded by the Grand Duke of Saxe-Weimar-Eisenach in 1906, and directed by Belgian Art Nouveau architect Henry van de Velde. When van de Velde was forced to resign in 1915 because he was Belgian, he suggested Gropius, Hermann Obrist, and August Endell as possible successors. In 1919, after delays caused by World War I and a lengthy debate over who should head the institution and the socio-economic meanings of a reconciliation of the fine arts and the applied arts (an issue which remained a defining one throughout the school's existence), Gropius was made the director of a new institution integrating the two called the Bauhaus. In the pamphlet for an April 1919 exhibition entitled \"Exhibition of Unknown Architects\", Gropius, still very much under the influence of William Morris and the British Arts and Crafts Movement, proclaimed his goal as being \"to create a new guild of craftsmen, without the class distinctions which raise an arrogant barrier between craftsman and artist.\" Gropius's neologism \"Bauhaus\" references both building and the Bauh\u00fctte, a premodern guild of stonemasons. The early intention was for the Bauhaus to be a combined architecture school, crafts school, and academy of the arts. Swiss painter Johannes Itten, German-American painter Lyonel Feininger, and German sculptor Gerhard Marcks, along with Gropius, comprised the faculty of the Bauhaus in 1919. By the following year their ranks had grown to include German painter, sculptor, and designer Oskar Schlemmer who headed the theatre workshop, and Swiss painter Paul Klee, joined in 1922 by Russian painter Wassily Kandinsky. The first major joint project completed by the Bauhaus was the Sommerfeld House, which was built between 1920 and 1921. A tumultuous year at the Bauhaus, 1922 also saw the move of Dutch painter Theo van Doesburg to Weimar to promote \"De Stijl\" (\"The Style\"), and a visit to the Bauhaus by Russian Constructivist artist and architect El Lissitzky.\nFrom 1919 to 1922 the school was shaped by the pedagogical and aesthetic ideas of Johannes Itten, who taught the \"Vorkurs\" or \"preliminary course\" that was the introduction to the ideas of the Bauhaus. Itten was heavily influenced in his teaching by the ideas of Franz Ci\u017eek and Friedrich Wilhelm August Fr\u00f6bel. He was also influenced in respect to aesthetics by the work of the Der Blaue Reiter group in Munich, as well as the work of Austrian Expressionist Oskar Kokoschka. The influence of German Expressionism favoured by Itten was analogous in some ways to the fine arts side of the ongoing debate. This influence culminated with the addition of Der Blaue Reiter founding member Wassily Kandinsky to the faculty and ended when Itten resigned in late 1923. Itten was replaced by the Hungarian designer L\u00e1szl\u00f3 Moholy-Nagy, who rewrote the \"Vorkurs\" with a leaning towards the New Objectivity favoured by Gropius, which was analogous in some ways to the applied arts side of the debate. Although this shift was an important one, it did not represent a radical break from the past so much as a small step in a broader, more gradual socio-economic movement that had been going on at least since 1907, when van de Velde had argued for a craft basis for design while Hermann Muthesius had begun implementing industrial prototypes.\nGropius was not necessarily against Expressionism, and in the same 1919 pamphlet proclaiming this \"new guild of craftsmen, without the class snobbery\", described \"painting and sculpture rising to heaven out of the hands of a million craftsmen, the crystal symbol of the new faith of the future.\" By 1923, however, Gropius was no longer evoking images of soaring Romanesque cathedrals and the craft-driven aesthetic of the \"V\u00f6lkisch movement\", instead declaring \"we want an architecture adapted to our world of machines, radios and fast cars.\" Gropius argued that a new period of history had begun with the end of the war. He wanted to create a new architectural style to reflect this new era. His style in architecture and consumer goods was to be functional, cheap and consistent with mass production. To these ends, Gropius wanted to reunite art and craft to arrive at high-end functional products with artistic merit. The Bauhaus issued a magazine called \"Bauhaus\" and a series of books called \"Bauhausb\u00fccher\". Since the Weimar Republic lacked the number of raw materials available to the United States and Great Britain, it had to rely on the proficiency of a skilled labour force and an ability to export innovative and high-quality goods. Therefore, designers were needed and so was a new type of art education. The school's philosophy stated that the artist should be trained to work with the industry.\nWeimar was in the German state of Thuringia, and the Bauhaus school received state support from the Social Democrat-controlled Thuringian state government. The school in Weimar experienced political pressure from conservative circles in Thuringian politics, increasingly so after 1923 as political tension rose. One condition placed on the Bauhaus in this new political environment was the exhibition of work undertaken at the school. This condition was met in 1923 with the Bauhaus' exhibition of the experimental Haus am Horn. The Ministry of Education placed the staff on six-month contracts and cut the school's funding in half. The Bauhaus issued a press release on 26 December 1924, setting the closure of the school for the end of March 1925. At this point it had already been looking for alternative sources of funding. After the Bauhaus moved to Dessau, a school of industrial design with teachers and staff less antagonistic to the conservative political regime remained in Weimar. This school was eventually known as the Technical University of Architecture and Civil Engineering, and in 1996 changed its name to Bauhaus-University Weimar.\nDessau.\nThe Bauhaus moved to Dessau in 1925 and new facilities there were inaugurated in late 1926. Gropius's design for the Dessau facilities was a return to the futuristic Gropius of 1914 that had more in common with the International style lines of the Fagus Factory than the stripped down Neo-classical of the Werkbund pavilion or the \"V\u00f6lkisch\" Sommerfeld House. During the Dessau years, there was a remarkable change in direction for the school. According to Elaine Hoffman, Gropius had approached the Dutch architect Mart Stam to run the newly founded architecture program, and when Stam declined the position, Gropius turned to Stam's friend and colleague in the ABC group, Hannes Meyer.\nMeyer became director when Gropius resigned in February 1928, and brought the Bauhaus its two most significant building commissions, both of which still exist: five apartment buildings in the city of Dessau, and the Bundesschule des Allgemeinen Deutschen Gewerkschaftsbundes (ADGB Trade Union School) in Bernau bei Berlin. Meyer favoured measurements and calculations in his presentations to clients, along with the use of off-the-shelf architectural components to reduce costs. This approach proved attractive to potential clients. The school turned its first profit under his leadership in 1929.\nBut Meyer also generated a great deal of conflict. As a radical functionalist, he had no patience with the aesthetic program and forced the resignations of Herbert Bayer, Marcel Breuer, and other long-time instructors. Even though Meyer shifted the orientation of the school further to the left than it had been under Gropius, he didn't want the school to become a tool of left-wing party politics. He prevented the formation of a student communist cell, and in the increasingly dangerous political atmosphere, this became a threat to the existence of the Dessau school. Dessau mayor Fritz Hesse fired him in the summer of 1930. The Dessau city council attempted to convince Gropius to return as head of the school, but Gropius instead suggested Ludwig Mies van der Rohe. Mies was appointed in 1930 and immediately interviewed each student, dismissing those that he deemed uncommitted. He halted the school's manufacture of goods so that the school could focus on teaching, and appointed no new faculty other than his close confidant Lilly Reich. By 1931, the Nazi Party was becoming more influential in German politics. When it gained control of the Dessau city council, it moved to close the school.\nBerlin.\nIn late 1932, Mies rented a derelict factory in Berlin (Birkbusch Street 49) to use as the new Bauhaus with his own money. The students and faculty rehabilitated the building, painting the interior white. The school operated for ten months without further interference from the Nazi Party. In 1933, the Gestapo closed down the Berlin school. Mies protested the decision, eventually speaking to the head of the Gestapo, who agreed to allow the school to re-open. However, shortly after receiving a letter permitting the opening of the Bauhaus, Mies and the other faculty agreed to voluntarily shut down the school.\nAlthough neither the Nazi Party nor Adolf Hitler had a cohesive architectural policy before they came to power in 1933, Nazi writers like Wilhelm Frick and Alfred Rosenberg had already labelled the Bauhaus \"un-German\" and criticized its modernist styles, deliberately generating public controversy over issues like flat roofs. Increasingly through the early 1930s, they characterized the Bauhaus as a front for communists and social liberals. Indeed, when Meyer was fired in 1930, a number of communist students loyal to him moved to the Soviet Union.\nEven before the Nazis came to power, political pressure on Bauhaus had increased. The Nazi movement, from nearly the start, denounced the Bauhaus for its \"degenerate art\", and the Nazi regime was determined to crack down on what it saw as the foreign, probably Jewish, influences of \"cosmopolitan modernism\". Despite Gropius's protestations that as a war veteran and a patriot his work had no subversive political intent, the Berlin Bauhaus was pressured to close in April 1933. Emigrants did succeed, however, in spreading the concepts of the Bauhaus to other countries, including the \"New Bauhaus\" of Chicago: Mies decided to emigrate to the United States for the directorship of the School of Architecture at the Armour Institute (now Illinois Institute of Technology) in Chicago and to seek building commissions. The simple engineering-oriented functionalism of stripped-down modernism, however, did lead to some Bauhaus influences living on in Nazi Germany. When Hitler's chief engineer, Fritz Todt, began opening the new autobahns (highways) in 1935, many of the bridges and service stations were \"bold examples of modernism\", and among those submitting designs was Mies van der Rohe.\nArchitectural output.\nThe paradox of the early Bauhaus was that, although its manifesto proclaimed that the aim of all creative activity was building, the school did not offer classes in architecture until 1927. During the years under Gropius (1919\u20131927), he and his partner Adolf Meyer observed no real distinction between the output of his architectural office and the school. The built output of Bauhaus architecture in these years is the output of Gropius: the Sommerfeld house in Berlin, the Otte house in Berlin, the Auerbach house in Jena, and the competition design for the Chicago Tribune Tower, which brought the school much attention. The definitive 1926 Bauhaus building in Dessau is also attributed to Gropius. Apart from contributions to the 1923 Haus am Horn, student architectural work amounted to un-built projects, interior finishes, and craft work like cabinets, chairs and pottery.\nIn the next two years under Meyer, the architectural focus shifted away from aesthetics and towards functionality. There were major commissions: one from the city of Dessau for five tightly designed \"Laubengangh\u00e4user\" (apartment buildings with balcony access), which are still in use today, and another for the Bundesschule des Allgemeinen Deutschen Gewerkschaftsbundes (ADGB Trade Union School) in Bernau bei Berlin. Meyer's approach was to research users' needs and scientifically develop the design solution. He intended to place emphasis on Gropius' objective analysis of the properties determining an object's use value, known as \"Wesensforschung\". Gropius believed that it was possible to design exemplary products of universal validity that should be standardized.\nMies van der Rohe repudiated Meyer's politics, his supporters, and his architectural approach. As opposed to Gropius's \"study of essentials\", and Meyer's research into user requirements, Mies advocated a \"spatial implementation of intellectual decisions\", which effectively meant an adoption of his own aesthetics. Neither Mies van der Rohe nor his Bauhaus students saw any projects built during the 1930s.\nThe Bauhaus\u00a0movement was not focused on developing worker housing. Only two projects, the apartment building project in Dessau and the T\u00f6rten row housing fall into the worker housing category. It was the Bauhaus contemporaries Bruno Taut, Hans Poelzig and particularly Ernst May, as the city architects of Berlin, Dresden and Frankfurt respectively, who are rightfully credited with the thousands of socially progressive housing units built in Weimar Germany. The housing Taut built in south-west Berlin during the 1920s, close to the U-Bahn stop Onkel Toms H\u00fctte, is still occupied.\nImpact.\nThe Bauhaus had a major impact on art and architecture trends in Western Europe, Canada, the United States and Israel in the decades following its demise, as many of the artists involved fled, or were exiled by the Nazi regime. In 1996, four of the major sites associated with Bauhaus in Germany were inscribed on the UNESCO World Heritage List (with two more added in 2017).\nIn 1928, the Hungarian painter Alexander Bortnyik founded a school of design in Budapest called M\u0171hely, which means \"the studio\". Located on the seventh floor of a house on Nagymezo Street, it was meant to be the Hungarian equivalent to the Bauhaus. The literature sometimes refers to it\u2014in an oversimplified manner\u2014as \"the Budapest Bauhaus\". Bortnyik was a great admirer of L\u00e1szl\u00f3 Moholy-Nagy and had met Walter Gropius in Weimar between 1923 and 1925. Moholy-Nagy himself taught at the M\u0171hely. Victor Vasarely, a pioneer of op art, studied at this school before establishing in Paris in 1930.\nWalter Gropius, Marcel Breuer, and Moholy-Nagy re-assembled in Britain during the mid-1930s and lived and worked in the Isokon housing development in Lawn Road in London before the war caught up with them. Gropius and Breuer went on to teach at the Harvard Graduate School of Design and worked together before their professional split. Their collaboration produced, among other projects, the Aluminum City Terrace in New Kensington, Pennsylvania and the Alan I W Frank House in Pittsburgh. The Harvard School was enormously influential in America in the late 1920s and early 1930s, producing such students as Philip Johnson, I. M. Pei, Lawrence Halprin and Paul Rudolph, among many others.\nIn the late 1930s, Mies van der Rohe re-settled in Chicago, enjoyed the sponsorship of the influential Philip Johnson, and became one of the world's pre-eminent architects. Moholy-Nagy also went to Chicago and founded the New Bauhaus school under the sponsorship of industrialist and philanthropist Walter Paepcke. This school became the Institute of Design, part of the Illinois Institute of Technology. Printmaker and painter Werner Drewes was also largely responsible for bringing the Bauhaus aesthetic to America and taught at both Columbia University and Washington University in St. Louis. Herbert Bayer, sponsored by Paepcke, moved to Aspen, Colorado in support of Paepcke's Aspen projects at the Aspen Institute. In 1953, Max Bill, together with Inge Aicher-Scholl and Otl Aicher, founded the Ulm School of Design (German: Hochschule f\u00fcr Gestaltung \u2013 HfG Ulm) in Ulm, Germany, a design school in the tradition of the Bauhaus. The school is notable for its inclusion of semiotics as a field of study. The school closed in 1968, but the \"Ulm Model\" concept continues to influence international design education. Another series of projects at the school were the Bauhaus typefaces, mostly realized in the decades afterward.\nThe influence of the Bauhaus on design education was significant. One of the main objectives of the Bauhaus was to unify art, craft, and technology, and this approach was incorporated into the curriculum of the Bauhaus. The structure of the Bauhaus \"Vorkurs\" (preliminary course) reflected a pragmatic approach to integrating theory and application. In their first year, students learnt the basic elements and principles of design and colour theory, and experimented with a range of materials and processes. This approach to design education became a common feature of architectural and design school in many countries. For example, the Shillito Design School in Sydney stands as a unique link between Australia and the Bauhaus. The colour and design syllabus of the Shillito Design School was firmly underpinned by the theories and ideologies of the Bauhaus. Its first year foundational course mimicked the \"Vorkurs\" and focused on the elements and principles of design plus colour theory and application. The founder of the school, Phyllis Shillito, which opened in 1962 and closed in 1980, firmly believed that \"A student who has mastered the basic principles of design, can design anything from a dress to a kitchen stove\". In Britain, largely under the influence of painter and teacher William Johnstone, Basic Design, a Bauhaus-influenced art foundation course, was introduced at Camberwell School of Art and the Central School of Art and Design, whence it spread to all art schools in the country, becoming universal by the early 1960s.\nOne of the most important contributions of the Bauhaus is in the field of modern furniture design. The characteristic Cantilever chair and Wassily Chair designed by Marcel Breuer are two examples. (Breuer eventually lost a legal battle in Germany with Dutch architect/designer Mart Stam over patent rights to the cantilever chair design. Although Stam had worked on the design of the Bauhaus's 1923 exhibit in Weimar, and guest-lectured at the Bauhaus later in the 1920s, he was not formally associated with the school, and he and Breuer had worked independently on the cantilever concept, leading to the patent dispute.) The most profitable product of the Bauhaus was its wallpaper.\nThe physical plant at Dessau survived World War II and was operated as a design school with some architectural facilities by the German Democratic Republic. This included live stage productions in the Bauhaus theater under the name of \"Bauhausb\u00fchne\" (\"Bauhaus Stage\"). After German reunification, a reorganized school continued in the same building, with no essential continuity with the Bauhaus under Gropius in the early 1920s. In 1979 Bauhaus-Dessau College started to organize postgraduate programs with participants from all over the world. This effort has been supported by the Bauhaus-Dessau Foundation which was founded in 1974 as a public institution.\nLater evaluation of the Bauhaus design credo was critical of its flawed recognition of the human element, an acknowledgment of \"the dated, unattractive aspects of the Bauhaus as a projection of utopia marked by mechanistic views of human nature\u2026Home hygiene without home atmosphere.\"\nSubsequent examples which have continued the philosophy of the Bauhaus include Black Mountain College, Hochschule f\u00fcr Gestaltung in Ulm and Domaine de Boisbuchet.\nThe White City.\nThe White City (Hebrew: \u05d4\u05e2\u05d9\u05e8 \u05d4\u05dc\u05d1\u05e0\u05d4), refers to a collection of over 4,000 buildings built in the Bauhaus or International Style in Tel Aviv from the 1930s by German Jewish architects who emigrated to the British Mandate of Palestine after the rise of the Nazis. Tel Aviv has the largest number of buildings in the Bauhaus/International Style of any city in the world. Preservation, documentation, and exhibitions have brought attention to Tel Aviv's collection of 1930s architecture. In 2003, the United Nations Educational, Scientific and Cultural Organization (UNESCO) proclaimed Tel Aviv's \"White City\" a World Cultural Heritage site, as \"an outstanding example of new town planning and architecture in the early 20th century.\" The citation recognized the unique adaptation of modern international architectural trends to the cultural, climatic, and local traditions of the city. Bauhaus Center Tel Aviv organizes regular architectural tours of the city, and the Bauhaus Foundation offers Bauhaus exhibits.\nCentenary.\nAs the centenary of the founding of Bauhaus, several events, festivals, and exhibitions were held around the world in 2019. The international opening festival at the Berlin Academy of the Arts from 16 to 24 January concentrated on \"the presentation and production of pieces by contemporary artists, in which the aesthetic issues and experimental configurations of the Bauhaus artists continue to be inspiringly contagious\". \"Original Bauhaus, The Centenary Exhibition\" at the Berlinische Galerie (6 September 2019 to 27 January 2020) presented 1,000 original artefacts from the Bauhaus-Archive's collection and recounted the history behind the objects. The Bauhaus Museum Dessau also opened in September 2019, operated by the Bauhaus Dessau Foundation and funded by the State of Saxony-Anhalt and the German Federal government. It is set to be the permanent home of the second largest Bauhaus collection at 49,000 objects, while paying homage to its strong influence in the city when Bauhaus arrived in 1925. \nIn 2024, the German far-right party Alternative for Germany (AfG) sought to attack celebrations of Bauhaus because of their view that Bauhaus did not follow tradition. Bauhaus was also crushed by the Nazi's before World War II, and according to political scientist Jan-Werner Mueller, AfG's condemnation seeks to use it in a culture war of far right-wing provocation.\nThe New European Bauhaus.\nIn September 2020, President of the European Commission Ursula von der Leyen introduced the New European Bauhaus (NEB) initiative during her State of the Union address. The NEB is a creative and interdisciplinary movement that connects the European Green Deal to everyday life. It is a platform for experimentation aiming to unite citizens, experts, businesses and institutions in imagining and designing a sustainable, aesthetic and inclusive future.\nSport and physical activity were an essential part of the original Bauhaus approach. Hannes Meyer, the second director of Bauhaus Dessau, ensured that one day a week was solely devoted to sport and gymnastics. 1 In 1930, Meyer employed two physical education teachers. The Bauhaus school even applied for public funds to enhance its playing field. The inclusion of sport and physical activity in the Bauhaus curriculum had various purposes. First, as Meyer put it, sport combatted a \u201cone-sided emphasis on brainwork.\u201d In addition, Bauhaus instructors believed that students could better express themselves if they actively experienced the space, rhythms and movements of the body. The Bauhaus approach also considered physical activity an important contributor to wellbeing and community spirit. Sport and physical activity were essential to the interdisciplinary Bauhaus movement that developed revolutionary ideas and continues to shape our environments today.\nBauhaus staff and students.\nPeople who were educated, or who taught or worked in other capacities, at the Bauhaus."}
{"id": "3833", "revid": "40800862", "url": "https://en.wikipedia.org/wiki?curid=3833", "title": "Beowulf", "text": "Beowulf (; ) is an Old English epic poem in the tradition of Germanic heroic legend consisting of 3,182 alliterative lines. It is one of the most important and most often translated works of Old English literature. The date of composition is a matter of contention among scholars; the only certain dating is for the manuscript, which was produced between 975 and 1025 AD. Scholars call the anonymous author the \"\"Beowulf\" poet\". \nThe story is set in pagan Scandinavia in the 5th and 6th centuries. Beowulf, a hero of the Geats, comes to the aid of Hrothgar, the king of the Danes, whose mead hall Heorot has been under attack by the monster Grendel for twelve years. After Beowulf slays him, Grendel's mother takes revenge and is in turn defeated. Victorious, Beowulf goes home to Geatland and becomes king of the Geats. Fifty years later, Beowulf defeats a dragon, but is mortally wounded in the battle. After his death, his attendants cremate his body and erect a barrow on a headland in his memory.\nScholars have debated whether \"Beowulf\" was transmitted orally, affecting its interpretation: if it was composed early, in pagan times, then the paganism is central and the Christian elements were added later, whereas if it was composed later, in writing, by a Christian, then the pagan elements could be decorative archaising; some scholars also hold an intermediate position. \n\"Beowulf\" is written mostly in the Late West Saxon dialect of Old English, but many other dialectal forms are present, suggesting that the poem may have had a long and complex transmission throughout the dialect areas of England.\nThere has long been research into similarities with other traditions and accounts, including the Icelandic \"Grettis saga\", the Norse story of Hrolf Kraki and his bear-shapeshifting servant Bodvar Bjarki, the international folktale the Bear's Son Tale, and the Irish folktale of the Hand and the Child. Persistent attempts have been made to link \"Beowulf\" to tales from Homer's \"Odyssey\" or Virgil's \"Aeneid\". More definite are biblical parallels, with clear allusions to the books of Genesis, Exodus, and Daniel.\nThe poem survives in a single copy in the manuscript known as the Nowell Codex. It has no title in the original manuscript, but has become known by the name of the story's protagonist. In 1731, the manuscript was damaged by a fire that swept through Ashburnham House in London, which was housing Sir Robert Cotton's collection of medieval manuscripts. It survived, but the margins were charred, and some readings were lost. The Nowell Codex is housed in the British Library. \nThe poem was first transcribed in 1786; some verses were first translated into modern English in 1805, and nine complete translations were made in the 19th century, including those by John Mitchell Kemble and William Morris.\nAfter 1900, hundreds of translations, whether into prose, rhyming verse, or alliterative verse were made, some relatively faithful, some archaising, some attempting to domesticate the work. Among the best-known modern translations are those of Edwin Morgan, Burton Raffel, Michael J. Alexander, Roy Liuzza, and Seamus Heaney. The difficulty of translating \"Beowulf\" has been explored by scholars including J. R. R. Tolkien (in his essay \"On Translating \"Beowulf\"), who worked on a verse and of his own.\nHistorical background.\nThe events in the poem take place over the 5th and 6th centuries, and feature predominantly non-English characters. Some suggest that \"Beowulf\" was first composed in the 7th century at Rendlesham in East Anglia, as the Sutton Hoo ship-burial shows close connections with Scandinavia, and the East Anglian royal dynasty, the Wuffingas, may have been descendants of the Geatish Wulfings. Others have associated this poem with the court of King Alfred the Great or with the court of King Cnut the Great.\nThe poem blends fictional, legendary, mythic and historical elements. Although Beowulf himself is not mentioned in any other Old English manuscript, many of the other figures named in \"Beowulf\" appear in Scandinavian sources. This concerns not only individuals (e.g., Healfdene, Hro\u00f0gar, Halga, Hro\u00f0ulf, Eadgils and Ohthere), but also clans (e.g., Scyldings, Scylfings and Wulfings) and certain events (e.g., the battle between Eadgils and Onela). The raid by King Hygelac into Frisia is mentioned by Gregory of Tours in his \"History of the Franks\" and can be dated to around 521.\nThe majority view appears to be that figures such as King Hrothgar and the Scyldings in \"Beowulf\" are based on historical people from 6th-century Scandinavia. Like the \"Finnesburg Fragment\" and several shorter surviving poems, \"Beowulf\" has consequently been used as a source of information about Scandinavian figures such as Eadgils and Hygelac, and about continental Germanic figures such as Offa, king of the continental Angles. However, one scholar, Roy Liuzza, feels that the poem is \"frustratingly ambivalent\", neither myth nor folktale, but is set \"against a complex background of legendary history ... on a roughly recognizable map of Scandinavia\", and comments that the Geats of the poem may correspond with the Gautar (of modern G\u00f6taland).\nNineteenth-century archaeological evidence may confirm elements of the \"Beowulf\" story. Eadgils was buried at Uppsala (Gamla Uppsala, Sweden) according to Snorri Sturluson. When the western mound (to the left in the photo) was excavated in 1874, the finds showed that a powerful man was buried in a large barrow, , on a bear skin with two dogs and rich grave offerings. The eastern mound was excavated in 1854, and contained the remains of a woman, or a woman and a young man. The middle barrow has not been excavated.\nIn Denmark, recent (1986\u201388, 2004\u201305) archaeological excavations at Lejre, where Scandinavian tradition located the seat of the Scyldings, Heorot, have revealed that a hall was built in the mid-6th century, matching the period described in \"Beowulf\", some centuries before the poem was composed. Three halls, each about long, were found during the excavation.\nSummary.\nThe protagonist Beowulf, a hero of the Geats, comes to the aid of Hrothgar, king of the Danes, whose great hall, Heorot, is plagued by the monster Grendel. Beowulf kills Grendel with his bare hands, then kills Grendel's mother with a giant's sword that he found in her lair.\nLater in his life, Beowulf becomes king of the Geats, and finds his realm terrorised by a dragon, some of whose treasure had been stolen from his hoard in a burial mound. He attacks the dragon with the help of his \"thegns\" or servants, but they do not succeed. Beowulf decides to follow the dragon to its lair at Earnan\u00e6s, but only his young Swedish relative Wiglaf, whose name means \"remnant of valour\", dares to join him. Beowulf finally slays the dragon, but is mortally wounded in the struggle. He is cremated and a burial mound by the sea is erected in his honour.\n\"Beowulf\" is considered an epic poem in that the main character is a hero who travels great distances to prove his strength at impossible odds against supernatural demons and beasts. The poem begins \"in medias res\" or simply, \"in the middle of things\", a characteristic of the epics of antiquity. Although the poem begins with Beowulf's arrival, Grendel's attacks have been ongoing. An elaborate history of characters and their lineages is spoken of, as well as their interactions with each other, debts owed and repaid, and deeds of valour. The warriors form a brotherhood linked by loyalty to their lord. The poem begins and ends with funerals: at the beginning of the poem for Scyld Scefing and at the end for Beowulf.\nThe poem is tightly structured. E. Carrigan shows the symmetry of its design in a model of its major components, with for instance the account of the killing of Grendel matching that of the killing of the dragon, the glory of the Danes matching the accounts of the Danish and Geatish courts. Other analyses are possible as well; Gale Owen-Crocker, for instance, sees the poem as structured by the four funerals it describes. For J. R. R. Tolkien, the primary division in the poem was between young and old Beowulf.\nFirst battle: Grendel.\n\"Beowulf\" begins with the story of Hrothgar, who constructed the great hall, Heorot, for himself and his warriors. In it, he, his wife Wealhtheow, and his warriors spend their time singing and celebrating. Grendel, a troll-like monster said to be descended from the biblical Cain, is pained by the sounds of joy. Grendel attacks the hall and devours many of Hrothgar's warriors while they sleep. Hrothgar and his people, helpless against Grendel, abandon Heorot.\nBeowulf, a young warrior from Geatland, hears of Hrothgar's troubles and with his king's permission leaves his homeland to assist Hrothgar.\nBeowulf and his men spend the night in Heorot. Beowulf refuses to use any weapon because he holds himself to be Grendel's equal. When Grendel enters the hall and kills one of Beowulf's men, Beowulf, who has been feigning sleep, leaps up to clench Grendel's hand. Grendel and Beowulf battle each other violently. Beowulf's retainers draw their swords and rush to his aid, but their blades cannot pierce Grendel's skin. Finally, Beowulf tears Grendel's arm from his body at the shoulder. Fatally hurt, Grendel flees to his home in the marshes, where he dies. Beowulf displays \"the whole of Grendel's shoulder and arm, his awesome grasp\" for all to see at Heorot. This display would fuel Grendel's mother's anger in revenge.\nSecond battle: Grendel's mother.\nThe next night, after celebrating Grendel's defeat, Hrothgar and his men sleep in Heorot. Grendel's mother, angry that her son has been killed, sets out to get revenge. \"Beowulf was elsewhere. Earlier, after the award of treasure, The Geat had been given another lodging\"; his assistance would be absent in this attack. Grendel's mother violently kills \u00c6schere, who is Hrothgar's most loyal advisor, and escapes, later putting his head outside her lair.\nHrothgar, Beowulf, and their men track Grendel's mother to her lair under a lake. Unferth, a warrior who had earlier challenged him, presents Beowulf with his sword Hrunting. After stipulating a number of conditions to Hrothgar in case of his death (including the taking in of his kinsmen and the inheritance by Unferth of Beowulf's estate), Beowulf jumps into the lake and, while harassed by water monsters, gets to the bottom, where he finds a cavern. Grendel's mother pulls him in, and she and Beowulf engage in fierce combat.\nAt first, Grendel's mother prevails, and Hrunting proves incapable of hurting her; she throws Beowulf to the ground and, sitting astride him, tries to kill him with a short sword, but Beowulf is saved by his armour. Beowulf spots another sword, hanging on the wall and apparently made for giants, and cuts her head off with it. Travelling further into Grendel's mother's lair, Beowulf discovers Grendel's corpse and severs his head with the sword. Its blade melts because of the monster's \"hot blood\", leaving only the hilt. Beowulf swims back up to the edge of the lake where his men wait. Carrying the hilt of the sword and Grendel's head, he presents them to Hrothgar upon his return to Heorot. Hrothgar gives Beowulf many gifts, including the sword N\u00e6gling, his family's heirloom. The events prompt a long reflection by the king, sometimes referred to as \"Hrothgar's sermon\", in which he urges Beowulf to be wary of pride and to reward his thegns.\nFinal battle: The dragon.\nBeowulf returns home and eventually becomes king of his own people. One day, fifty years after Beowulf's battle with Grendel's mother, a slave steals a golden cup from the lair of a dragon at Earnan\u00e6s. When the dragon sees that the cup has been stolen, it leaves its cave in a rage, burning everything in sight. Beowulf and his warriors come to fight the dragon, but Beowulf tells his men that he will fight the dragon alone and that they should wait on the barrow. Beowulf descends to do battle with the dragon, but finds himself outmatched. His men, upon seeing this and fearing for their lives, retreat into the woods. However, one of his men, Wiglaf, in great distress at Beowulf's plight, comes to his aid. The two slay the dragon, but Beowulf is mortally wounded. After Beowulf dies, Wiglaf remains by his side, grief-stricken. When the rest of the men finally return, Wiglaf bitterly admonishes them, blaming their cowardice for Beowulf's death. Beowulf is ritually burned on a great pyre in Geatland while his people wail and mourn him, fearing that without him, the Geats are defenceless against attacks from surrounding tribes. Afterwards, a barrow, visible from the sea, is built in his memory.\nDigressions.\nThe poem contains many apparent digressions from the main story. These were found troublesome by early \"Beowulf\" scholars such as Frederick Klaeber, who wrote that they \"interrupt the story\", W. W. Lawrence, who stated that they \"clog the action and distract attention from it\", and W. P. Ker who found some \"irrelevant ... possibly ... interpolations\". More recent scholars from Adrien Bonjour onwards note that the digressions can all be explained as introductions or comparisons with elements of the main story; for instance, Beowulf's swimming home across the sea from Frisia carrying thirty sets of armour emphasises his heroic strength. The digressions can be divided into four groups, namely the Scyld narrative at the start; many descriptions of the Geats, including the Swedish\u2013Geatish wars, the \"Lay of the Last Survivor\" in the style of another Old English poem, \"The Wanderer\", and Beowulf's dealings with the Geats such as his verbal contest with Unferth and his swimming duel with Breca, and the tale of Sigemund and the dragon; history and legend, including the fight at Finnsburg and the tale of Freawaru and Ingeld; and biblical tales such as the creation myth and Cain as ancestor of all monsters. The digressions provide a powerful impression of historical depth, imitated by Tolkien in \"The Lord of the Rings\", a work that embodies many other elements from the poem.\nAuthorship and date.\nThe dating of \"Beowulf\" has attracted considerable scholarly attention; opinion differs as to whether it was first written in the 8th century, whether it was nearly contemporary with its 11th-century manuscript, and whether a proto-version (possibly a version of the \"Bear's Son Tale\") was orally transmitted before being transcribed in its present form. Albert Lord felt strongly that the manuscript represents the transcription of a performance, though likely taken at more than one sitting. J. R. R. Tolkien believed that the poem retains too genuine a memory of Anglo-Saxon paganism to have been composed more than a few generations after the completion of the Christianisation of England around AD 700, and Tolkien's conviction that the poem dates to the 8th century has been defended by scholars including Tom Shippey, Leonard Neidorf, Rafael J. Pascual, and Robert D. Fulk. An analysis of several Old English poems by a team including Neidorf suggests that \"Beowulf\" is the work of a single author, though other scholars disagree.\nThe claim to an early 11th-century date depends in part on scholars who argue that, rather than the transcription of a tale from the oral tradition by an earlier literate monk, \"Beowulf\" reflects an original interpretation of an earlier version of the story by the manuscript's two scribes. On the other hand, some scholars argue that linguistic, palaeographical (handwriting), metrical (poetic structure), and onomastic (naming) considerations align to support a date of composition in the first half of the 8th century; in particular, the poem's apparent observation of etymological vowel-length distinctions in unstressed syllables (described by Kaluza's law) has been thought to demonstrate a date of composition prior to the earlier ninth century. However, scholars disagree about whether the metrical phenomena described by Kaluza's law prove an early date of composition or are evidence of a longer prehistory of the \"Beowulf\" metre; B.R. Hutcheson, for instance, does not believe Kaluza's law can be used to date the poem, while claiming that \"the weight of all the evidence Fulk presents in his book tells strongly in favour of an eighth-century date.\"\nFrom an analysis of creative genealogy and ethnicity, Craig R. Davis suggests a composition date in the AD 890s, when King Alfred of England had secured the submission of Guthrum, leader of a division of the Great Heathen Army of the Danes, and of Aethelred, ealdorman of Mercia. In this thesis, the trend of appropriating Gothic royal ancestry, established in Francia during Charlemagne's reign, influenced the Anglian kingdoms of Britain to attribute to themselves a Geatish descent. The composition of \"Beowulf\" was the fruit of the later adaptation of this trend in Alfred's policy of asserting authority over the \"Angelcynn\", in which Scyldic descent was attributed to the West-Saxon royal pedigree. This date of composition largely agrees with Lapidge's positing of a West-Saxon exemplar .\nThe location of the poem's composition is intensely disputed. In 1914, F.W. Moorman, the first professor of English Language at University of Leeds, claimed that \"Beowulf\" was composed in Yorkshire, but E. Talbot Donaldson claims that it was probably composed during the first half of the eighth century, and that the writer was a native of what was then called West Mercia, located in the Western Midlands of England. However, the late tenth-century manuscript, \"which alone preserves the poem\", originated in the kingdom of the West Saxons \u2014 as it is more commonly known.\nManuscript.\n\"Beowulf\" survived to modern times in a single manuscript, written in ink on parchment, later damaged by fire. The manuscript measures 245 \u00d7 185\u00a0mm.\nProvenance.\nThe poem is known only from a single manuscript, estimated to date from around 975\u20131025, in which it appears with other works. The manuscript therefore dates either to the reign of \u00c6thelred the Unready, characterised by strife with the Danish king Sweyn Forkbeard, or to the beginning of the reign of Sweyn's son Cnut the Great from 1016. The \"Beowulf\" manuscript is known as the Nowell Codex, gaining its name from 16th-century scholar Laurence Nowell. The official designation is \"British Library, Cotton Vitellius A.XV\" because it was one of Sir Robert Bruce Cotton's holdings in the Cotton library in the middle of the 17th century. Many private antiquarians and book collectors, such as Sir Robert Cotton, used their own library classification systems. \"Cotton Vitellius A.XV\" translates as: the 15th book from the left on shelf A (the top shelf) of the bookcase with the bust of Roman Emperor Vitellius standing on top of it, in Cotton's collection. Kevin Kiernan argues that Nowell most likely acquired it through William Cecil, 1st Baron Burghley, in 1563, when Nowell entered Cecil's household as a tutor to his ward, Edward de Vere, 17th Earl of Oxford.\nThe earliest extant reference to the first foliation of the Nowell Codex was made sometime between 1628 and 1650 by Franciscus Junius (the younger). The ownership of the codex before Nowell remains a mystery.\nThe Reverend Thomas Smith (1638\u20131710) and Humfrey Wanley (1672\u20131726) both catalogued the Cotton library (in which the Nowell Codex was held). Smith's catalogue appeared in 1696, and Wanley's in 1705. The \"Beowulf\" manuscript itself is identified by name for the first time in an exchange of letters in 1700 between George Hickes, Wanley's assistant, and Wanley. In the letter to Wanley, Hickes responds to an apparent charge against Smith, made by Wanley, that Smith had failed to mention the \"Beowulf\" script when cataloguing Cotton MS. Vitellius A. XV. Hickes replies to Wanley \"I can find nothing yet of Beowulph.\" Kiernan theorised that Smith failed to mention the \"Beowulf\" manuscript because of his reliance on previous catalogues or because either he had no idea how to describe it or because it was temporarily out of the codex.\nThe manuscript passed to Crown ownership in 1702, on the death of its then owner, Sir John Cotton, who had inherited it from his grandfather, Robert Cotton. It suffered damage in a fire at Ashburnham House in 1731, in which around a quarter of the manuscripts bequeathed by Cotton were destroyed. Since then, parts of the manuscript have crumbled along with many of the letters. Rebinding efforts, though saving the manuscript from much degeneration, have nonetheless covered up other letters of the poem, causing further loss. Kiernan, in preparing his electronic edition of the manuscript, used fibre-optic backlighting and ultraviolet lighting to reveal letters in the manuscript lost from binding, erasure, or ink blotting.\nWriting.\nThe \"Beowulf\" manuscript was transcribed from an original by two scribes, one of whom wrote the prose at the beginning of the manuscript and the first 1939 lines, before breaking off in mid-sentence. The first scribe made a point of carefully regularizing the spelling of the original document into the common West Saxon, removing any archaic or dialectical features. The second scribe, who wrote the remainder, with a difference in handwriting noticeable after line 1939, seems to have written more vigorously and with less interest. As a result, the second scribe's script retains more archaic dialectic features, which allow modern scholars to ascribe the poem a cultural context. While both scribes appear to have proofread their work, there are nevertheless many errors. The second scribe was ultimately the more conservative copyist as he did not modify the spelling of the text as he wrote, but copied what he saw in front of him. In the way that it is currently bound, the \"Beowulf\" manuscript is followed by the Old English poem \"Judith\". \"Judith\" was written by the same scribe that completed \"Beowulf\", as evidenced by similar writing style. Wormholes found in the last leaves of the \"Beowulf\" manuscript that are absent in the \"Judith\" manuscript suggest that at one point \"Beowulf\" ended the volume. The rubbed appearance of some leaves suggests that the manuscript stood on a shelf unbound, as was the case with other Old English manuscripts. Knowledge of books held in the library at Malmesbury Abbey and available as source works, as well as the identification of certain words particular to the local dialect found in the text, suggest that the transcription may have taken place there.\nPerformance.\nThe scholar Roy Liuzza notes that the practice of oral poetry is by its nature invisible to history as evidence is in writing. Comparison with other bodies of verse such as Homer's, coupled with ethnographic observation of early 20th century performers, has provided a vision of how an Anglo-Saxon singer-poet or scop may have practised. The resulting model is that performance was based on traditional stories and a repertoire of word formulae that fitted the traditional metre. The scop moved through the scenes, such as putting on armour or crossing the sea, each one improvised at each telling with differing combinations of the stock phrases, while the basic story and style remained the same. Liuzza notes that \"Beowulf\" itself describes the technique of a court poet in assembling materials, in lines 867\u2013874 in his translation, \"full of grand stories, mindful of songs ... found other words truly bound together; ... to recite with skill the adventure of Beowulf, adeptly tell a tall tale, and (\"wordum wrixlan\") weave his words.\" The poem further mentions (lines 1065\u20131068) that \"the harp was touched, tales often told, when Hrothgar's scop was set to recite among the mead tables his hall-entertainment\".\nDebate over oral tradition.\nThe question of whether \"Beowulf\" was passed down through oral tradition prior to its present manuscript form has been the subject of much debate, and involves more than simply the issue of its composition. Rather, given the implications of the theory of oral-formulaic composition and oral tradition, the question concerns how the poem is to be understood, and what sorts of interpretations are legitimate. In his landmark 1960 work, \"The Singer of Tales\", Albert Lord, citing the work of Francis Peabody Magoun and others, considered it proven that \"Beowulf\" was composed orally. Later scholars have not all been convinced; they agree that \"themes\" like \"arming the hero\" or the \"hero on the beach\" do exist across Germanic works. Some scholars conclude that Anglo-Saxon poetry is a mix of oral-formulaic and literate patterns. Larry Benson proposed that Germanic literature contains \"kernels of tradition\" which \"Beowulf\" expands upon. Ann Watts argued against the imperfect application of one theory to two different traditions: traditional, Homeric, oral-formulaic poetry and Anglo-Saxon poetry. Thomas Gardner agreed with Watts, arguing that the \"Beowulf\" text is too varied to be completely constructed from set formulae and themes. John Miles Foley wrote that comparative work must observe the particularities of a given tradition; in his view, there was a fluid continuum from traditionality to textuality.\nEditions, translations, and adaptations.\nEditions.\nMany editions of the Old English text of \"Beowulf\" have been published; this section lists the most influential.\nThe Icelandic scholar Gr\u00edmur J\u00f3nsson Thorkelin made the first transcriptions of the \"Beowulf\"-manuscript in 1786, working as part of a Danish government historical research commission. He had a copy made by a professional copyist who knew no Old English (and was therefore in some ways more likely to make transcription errors, but in other ways more likely to copy exactly what he saw), and then made a copy himself. Since that time, the manuscript has crumbled further, making these transcripts prized witnesses to the text. While the recovery of at least 2000 letters can be attributed to them, their accuracy has been called into question, and the extent to which the manuscript was actually more readable in Thorkelin's time is uncertain. Thorkelin used these transcriptions as the basis for the first complete edition of \"Beowulf\", in Latin.\nIn 1922, Frederick Klaeber, a German philologist who worked at the University of Minnesota, published his edition of the poem, \"Beowulf and The Fight at Finnsburg\"; it became the \"central source used by graduate students for the study of the poem and by scholars and teachers as the basis of their translations.\" The edition included an extensive glossary of Old English terms. His third edition was published in 1936, with the last version in his lifetime being a revised reprint in 1950. Klaeber's text was re-presented with new introductory material, notes, and glosses, in a fourth edition in 2008.\nAnother widely used edition is Elliott Van Kirk Dobbie's, published in 1953 in the Anglo-Saxon Poetic Records series. The British Library, meanwhile, took a prominent role in supporting Kevin Kiernan's \"Electronic Beowulf\"; the first edition appeared in 1999, and the fourth in 2014.\nTranslations and adaptations.\nThe tightly interwoven structure of Old English poetry makes translating \"Beowulf\" a severe technical challenge. Despite this, a great number of translations and adaptations are available, in poetry and prose. Andy Orchard, in \"A Critical Companion to Beowulf\", lists 33 \"representative\" translations in his bibliography, while the Arizona Center for Medieval and Renaissance Studies published Marijane Osborn's annotated list of over 300 translations and adaptations in 2003. \"Beowulf\" has been translated many times in verse and in prose, and adapted for stage and screen. By 2020, the Beowulf's Afterlives Bibliographic Database listed some 688 translations and other versions of the poem. \"Beowulf\" has been translated into at least 38 other languages.\nIn 1805, the historian Sharon Turner translated selected verses into modern English. This was followed in 1814 by John Josias Conybeare who published an edition \"in English paraphrase and Latin verse translation.\" N. F. S. Grundtvig reviewed Thorkelin's edition in 1815 and created the first complete verse translation in Danish in 1820. In 1837, John Mitchell Kemble created an important literal translation in English. In 1895, William Morris and A. J. Wyatt published the ninth English translation.\nIn 1909, Francis Barton Gummere's full translation in \"English imitative metre\" was published, and was used as the text of Gareth Hinds's 2007 graphic novel based on \"Beowulf\". In 1975, John Porter published the first complete verse translation of the poem entirely accompanied by facing-page Old English. Seamus Heaney's 1999 translation of the poem (\"\", called \"Heaneywulf\" by the \"Beowulf\" translator Howell Chickering and many others) was both praised and criticised. The US publication was commissioned by W. W. Norton &amp; Company, and was included in the \"Norton Anthology of English Literature\". Many retellings of \"Beowulf\" for children appeared in the 20th century.\nIn 2000 (2nd edition 2013), Liuzza published his own version of \"Beowulf\" in a parallel text with the Old English, with his analysis of the poem's historical, oral, religious and linguistic contexts. R. D. Fulk, of Indiana University, published a facing-page edition and translation of the entire Nowell Codex manuscript in 2010. Hugh Magennis's 2011 \"Translating Beowulf: Modern Versions in English Verse\" discusses the challenges and history of translating the poem, as well as the question of how to approach its poetry, and discusses several post-1950 verse translations, paying special attention to those of Edwin Morgan, Burton Raffel, Michael J. Alexander, and Seamus Heaney. Translating \"Beowulf\" is one of the subjects of the 2012 publication \"Beowulf at Kalamazoo\", containing a section with 10 essays on translation, and a section with 22 reviews of Heaney's translation, some of which compare Heaney's work with Liuzza's. Tolkien's long-awaited prose translation (edited by his son Christopher) was published in 2014 as \"\". The book includes Tolkien's own retelling of the story of Beowulf in his tale \"Sellic Spell\", but not his incomplete and unpublished verse translation. \"The Mere Wife\", by Maria Dahvana Headley, was published in 2018. It relocates the action to a wealthy community in 20th-century America and is told primarily from the point of view of Grendel's mother. In 2020, Headley published a translation in which the opening \"Hw\u00e6t!\" is rendered \"Bro!\"; this translation subsequently won the Hugo Award for Best Related Work.\nSources and analogues.\nNeither identified sources nor analogues for \"Beowulf\" can be definitively proven, but many conjectures have been made. These are important in helping historians understand the \"Beowulf\" manuscript, as possible source-texts or influences would suggest time-frames of composition, geographic boundaries within which it could be composed, or range (both spatial and temporal) of influence (i.e. when it was \"popular\" and where its \"popularity\" took it). The poem has been related to Scandinavian, Celtic, and international folkloric sources.\nScandinavian parallels and sources.\n19th-century studies proposed that \"Beowulf\" was translated from a lost original Scandinavian work; surviving Scandinavian works have continued to be studied as possible sources. In 1886 Gregor Sarrazin suggested that an Old Norse original version of \"Beowulf\" must have existed, but in 1914 Carl Wilhelm von Sydow claimed that \"Beowulf\" is fundamentally Christian and was written at a time when any Norse tale would have most likely been pagan. Another proposal was a parallel with the \"Grettis Saga\", but in 1998, Magn\u00fas Fjalldal challenged that, stating that tangential similarities were being overemphasised as analogies. The story of Hrolf Kraki and his servant, the legendary bear-shapeshifter Bodvar Bjarki, has also been suggested as a possible parallel; he survives in \"Hr\u00f3lfs saga kraka\" and Saxo's \"Gesta Danorum\", while Hrolf Kraki, one of the Scyldings, appears as \"Hrothulf\" in \"Beowulf\". New Scandinavian analogues to \"Beowulf\" continue to be proposed regularly, with Hr\u00f3lfs saga Gautrekssonar being the most recently adduced text.\nInternational folktale sources.\n (1910) wrote a thesis that the first part of \"Beowulf\" (the Grendel Story) incorporated preexisting folktale material, and that the folktale in question was of the Bear's Son Tale (\"B\u00e4rensohnm\u00e4rchen\") type, which has surviving examples all over the world. This tale type was later catalogued as international folktale type 301 in the ATU Index, now formally entitled \"The Three Stolen Princesses\" type in Hans Uther's catalogue, although the \"Bear's Son\" is still used in Beowulf criticism, if not so much in folkloristic circles. However, although this folkloristic approach was seen as a step in the right direction, \"The Bear's Son\" tale has later been regarded by many as not a close enough parallel to be a viable choice. Later, Peter A. Jorgensen, looking for a more concise frame of reference, coined a \"two-troll tradition\" that covers both \"Beowulf\" and \"Grettis saga\": \"a Norse 'ecotype' in which a hero enters a cave and kills two giants, usually of different sexes\"; this has emerged as a more attractive folk tale parallel, according to a 1998 assessment by Andersson.\nThe epic's similarity to the Irish folktale \"The Hand and the Child\" was noted in 1899 by Albert S. Cook, and others even earlier. In 1914, the Swedish folklorist Carl Wilhelm von Sydow made a strong argument for parallelism with \"The Hand and the Child\", because the folktale type demonstrated a \"monstrous arm\" motif that corresponded with Beowulf's wrenching off Grendel's arm. No such correspondence could be perceived in the Bear's Son Tale or in the \"Grettis saga\".\nJames Carney and Martin Puhvel agree with this \"Hand and the Child\" contextualisation. Puhvel supported the \"Hand and the Child\" theory through such motifs as (in Andersson's words) \"the more powerful giant mother, the mysterious light in the cave, the melting of the sword in blood, the phenomenon of battle rage, swimming prowess, combat with water monsters, underwater adventures, and the bear-hug style of wrestling.\"\nIn the Mabinogion, Teyrnon discovers the otherworldly boy child Pryderi, the principal character of the cycle, after cutting off the arm of a monstrous beast which is stealing foals from his stables. The medievalist R. Mark Scowcroft notes that the tearing off of the monster's arm without a weapon is found only in \"Beowulf\" and fifteen of the Irish variants of the tale; he identifies twelve parallels between the tale and \"Beowulf\".\nClassical sources.\nAttempts to find classical or Late Latin influence or analogue in \"Beowulf\" are almost exclusively linked with Homer's \"Odyssey\" or Virgil's \"Aeneid\". In 1926, Albert S. Cook suggested a Homeric connection due to equivalent formulas, metonymies, and analogous voyages. In 1930, James A. Work supported the Homeric influence, stating that the encounter between Beowulf and Unferth was parallel to the encounter between Odysseus and Euryalus in Books 7\u20138 of the \"Odyssey,\" even to the point of both characters giving the hero the same gift of a sword upon being proven wrong in their initial assessment of the hero's prowess. This theory of Homer's influence on \"Beowulf\" remained very prevalent in the 1920s, but started to die out in the following decade when a handful of critics stated that the two works were merely \"comparative literature\", although Greek was known in late 7th century England: Bede states that Theodore of Tarsus, a Greek, was appointed Archbishop of Canterbury in 668, and he taught Greek. Several English scholars and churchmen are described by Bede as being fluent in Greek due to being taught by him; Bede claims to be fluent in Greek himself.\nFrederick Klaeber, among others, argued for a connection between \"Beowulf\" and Virgil near the start of the 20th century, claiming that the very act of writing a secular epic in a Germanic world represents Virgilian influence. Virgil was seen as the pinnacle of Latin literature, and Latin was the dominant literary language of England at the time, therefore making Virgilian influence highly likely. Similarly, in 1971, Alistair Campbell stated that the apologue technique used in \"Beowulf\" is so rare in epic poetry aside from Virgil that the poet who composed \"Beowulf\" could not have written the poem in such a manner without first coming across Virgil's writings.\nBiblical influences.\nIt cannot be denied that Biblical parallels occur in the text, whether seen as a pagan work with \"Christian colouring\" added by scribes or as a \"Christian historical novel, with selected bits of paganism deliberately laid on as 'local colour'\", as Margaret E. Goldsmith did in \"The Christian Theme of \"Beowulf\"\". \"Beowulf\" channels the Book of Genesis, the Book of Exodus, and the Book of Daniel in its inclusion of references to the Genesis creation narrative, the story of Cain and Abel, Noah and the flood, the Devil, Hell, and the Last Judgment.\nDialect.\n\"Beowulf\" predominantly uses the West Saxon dialect of Old English, like other Old English poems copied at the time. However, it also uses many other linguistic forms; this leads some scholars to believe that it has endured a long and complicated transmission through all the main dialect areas. It retains a complicated mix of Mercian, Northumbrian, Early West Saxon, Anglian, Kentish and Late West Saxon dialectical forms.\nForm and metre.\nOld English poets typically used alliterative verse, a form of verse in which the first half of the line (the a-verse) is linked to the second half (the b-verse) through similarity in initial sound. That the line consists of two halves is clearly indicated by the caesura: (l. 4). This verse form maps stressed and unstressed syllables onto abstract entities known as metrical positions. There is no fixed number of beats per line: the first one cited has three () whereas the second has two ().\nThe poet had a choice of formulae to assist in fulfilling the alliteration scheme. These were memorised phrases that conveyed a general and commonly-occurring meaning that fitted neatly into a half-line of the chanted poem. Examples are line 8's (\"waxed under welkin\", i.e. \"he grew up under the heavens\"), line 11's (\"pay tribute\"), line 13's (\"young in the yards\", i.e. \"young in the courts\"), and line 14's (\"as a comfort to his people\").\nKennings are a significant technique in \"Beowulf\". They are evocative poetic descriptions of everyday things, often created to fill the alliterative requirements of the metre. For example, a poet might call the sea the \"swan's riding\"; a king might be called a \"ring-giver\". The poem contains many kennings, and the device is typical of much of classic poetry in Old English, which is heavily formulaic. The poem, too, makes extensive use of elided metaphors.\nInterpretation and criticism.\nThe history of modern \"Beowulf\" criticism is often said to begin with Tolkien, author and Merton Professor of Anglo-Saxon at the University of Oxford, who in his 1936 lecture to the British Academy criticised his contemporaries' excessive interest in its historical implications. He noted in \"\" that as a result the poem's literary value had been largely overlooked, and argued that the poem \"is in fact so interesting as poetry, in places poetry so powerful, that this quite overshadows the historical content...\" Tolkien argued that the poem is not an epic; that, while no conventional term exactly fits, the nearest would be elegy; and that its focus is the concluding dirge.\nPaganism and Christianity.\nIn historical terms, the poem's characters were Germanic pagans, yet the poem was recorded by Christian Anglo-Saxons who had mostly converted from their native Anglo-Saxon paganism around the 7th century. \"Beowulf\" thus depicts a Germanic warrior society, in which the relationship between the lord of the region and those who served under him was of paramount importance.\nIn terms of the relationship between characters in \"Beowulf\" and God, one might recall the substantial amount of paganism that is present throughout the work. Literary critics such as Fred C. Robinson argue that the \"Beowulf\" poet tries to send a message to readers during the Anglo-Saxon time period regarding the state of Christianity in their own time. Robinson argues that the intensified religious aspects of the Anglo-Saxon period inherently shape the way in which the poet alludes to paganism as presented in \"Beowulf\". The poet calls on Anglo-Saxon readers to recognize the imperfect aspects of their supposed Christian lifestyles. In other words, the poet is referencing their \"Anglo-Saxon Heathenism\". In terms of the characters of the epic itself, Robinson argues that readers are \"impressed\" by the courageous acts of Beowulf and the speeches of Hrothgar. But one is ultimately left to feel sorry for both men as they are fully detached from supposed \"Christian truth\". The relationship between the characters of \"Beowulf\", and the overall message of the poet, regarding their relationship with God is debated among readers and literary critics alike.\nRichard North argues that the \"Beowulf\" poet interpreted \"Danish myths in Christian form\" (as the poem would have served as a form of entertainment for a Christian audience), and states: \"As yet we are no closer to finding out why the first audience of \"Beowulf\" liked to hear stories about people routinely classified as damned. This question is pressing, given... that Anglo-Saxons saw the Danes as 's' rather than as foreigners.\" Donaldson wrote that \"the poet who put the materials into their present form was a Christian and\u00a0... poem reflects a Christian tradition\".\nOther scholars disagree as to whether \"Beowulf\" is a Christian work set in a Germanic pagan context. The question suggests that the conversion from the Germanic pagan beliefs to Christian ones was a prolonged and gradual process over several centuries, and the poem's message in respect to religious belief at the time it was written remains unclear. Robert F. Yeager describes the basis for these questions:\nUrsula Schaefer's view is that the poem was created, and is interpretable, within both pagan and Christian horizons. Schaefer's concept of \"vocality\" offers neither a compromise nor a synthesis of views that see the poem as on the one hand Germanic, pagan, and oral and on the other Latin-derived, Christian, and literate, but, as stated by Monika Otter: \"a 'tertium quid', a modality that participates in both oral and literate culture yet also has a logic and aesthetic of its own.\"\nPolitics and warfare.\nStanley B. Greenfield has suggested that references to the human body throughout \"Beowulf\" emphasise the relative position of thanes to their lord. He argues that the term \"shoulder-companion\" could refer to both a physical arm as well as a thane (Aeschere) who was very valuable to his lord (Hrothgar). With Aeschere's death, Hrothgar turns to Beowulf as his new \"arm\". Greenfield argues the foot is used for the opposite effect, only appearing four times in the poem. It is used in conjunction with Unfer\u00f0 (a man described by Beowulf as weak, traitorous, and cowardly). Greenfield notes that Unfer\u00f0 is described as \"at the king's feet\" (line 499). Unfer\u00f0 is a member of the foot troops, who, throughout the story, do nothing and \"generally serve as backdrops for more heroic action.\"\nDaniel Podgorski has argued that the work is best understood as an examination of inter-generational vengeance-based conflict, or feuding. In this context, the poem operates as an indictment of feuding conflicts as a function of its conspicuous, circuitous, and lengthy depiction of the Swedish\u2013Geatish wars\u2014coming into contrast with the poem's depiction of the protagonist Beowulf as being disassociated from the ongoing feuds in every way. Francis Leneghan argues that the poem can be understood as a \"dynastic drama\" in which the hero's fights with the monsters unfold against a backdrop of the rise and fall of royal houses, while the monsters themselves serve as portents of disasters affecting dynasties.\nFurther reading.\nThe secondary literature on \"Beowulf\" is immense. The following is a selection."}
{"id": "3836", "revid": "3742946", "url": "https://en.wikipedia.org/wiki?curid=3836", "title": "Barb Wire (character)", "text": "Barb Wire is a fictional character appearing in Comics Greatest World, an imprint of Dark Horse Comics. Created by Chris Warner and Team CGW, the character first appeared in \"Comics' Greatest World: Steel Harbor\" in 1993. The original \"Barb Wire\" series published nine issues between 1994 and 1995 and was followed by a four-issue miniseries in 1996. A reboot was published in 2015 and lasted eight issues. In 1996, the character was adapted into a film starring Pamela Anderson. Unlike the comics, the film takes place in a possible future rather than an alternate version of present-day Earth.\nCreators.\n1\u20134: Chris Warner, script and pencils/Tim Bradstreet, inks\nFictional character biography.\nBarb Wire's stories take place on an alternate version of present-day Earth with superhumans and more advanced technology. In this Earth's history, an alien entity called the Vortex arrived in 1931 and began conducting secret experiments. In 1947, an atom bomb test detonated in a desert nearby the alien's experiments. The result was the creation of a trans-dimensional wormhole referred to as \"the Vortex\" or \"the Maelstrom\", which released energy that gave different people across Earth superpowers for years to come.\nDecades later, Barbara Kopetski grows up in Steel Harbor when it is still a thriving steel industry city. Barbara and her brother Charlie live with their grandmother and parents, their mother being a police officer while their father is a former marine who became a steelworker. Officer Kopetski later dies, after which her husband becomes so ill he is confined to a bed for years, developing Alzheimer's disease as well before passing away. Following the death of her father, Barbara leaves Steel Harbour for a time as the city's economy starts to spiral and crime begins rising. Soon, much of the city is controlled by warring gangs rather than local government. Years later, Barbara returns to Steel Harbor, now an experienced bounty hunter operating under the name Barb Wire. Reuniting with Charlie, she decides to stay in her hometown, becoming the owner of the Hammerhead bar. To help bring in money, she continues moonlighting as a bounty hunter, working with the police directly or bail bondsman Thomas Crashell.\nAs time goes on, Steel Harbor becomes more dangerous, described as \"a city under siege from drugs, crime, pollution and gang warfare\". In 1993, a second American Civil War begins when Golden City announces its secession from the Union. The announcement leads to protests and riots in several cities. The Steel Harbor Riots leave some neighborhoods in literal ruin, with hundreds of buildings destroyed or abandoned in the area known as \"Metal City\". Many are forced to leave the city or take to the streets, and the gangs (all of whom have superhuman members) start moving to take more control. To help contain the chaos and keep her home from descending further, Barb Wire now acts at times as a vigilante, intervening when the police can't or won't. Fighting alongside the Wolf Gang, she defies criminal Mace Blitzkrieg's attempts to bring all gangs under his leadership and control the city.\nGrowing up with a police officer mother and marine father, as well as her life experiences traveling outside of the city, Barb Wire is an excellent hand-to-hand combatant, skilled in various firearms, and an expert driver and motorcycle rider. Her bar has been considered neutral meeting ground by the Steel Harbor gangs. Aiding her bounty hunter activities is her brother Charlie, acting as her mechanic and engineer, and others such as Avram Roman Jr., a cyborg sometimes known simply as \"the Machine\". Though she has loyal allies, including Charlie, Barb Wire is a harsh, guarded person who looks at the world with suspicion and cynicism, considering herself a loner at heart.\nFilm adaptation.\nA film adaptation was released in 1996 starring Pamela Anderson as Barb Wire. The story's premise was that Barb Wire lives in the near future rather than an alternate version of the present day, a world where superhumans and Dark Horse superheroes do not exist. In this version of the story, Steel Harbor is the last neutral \"free city\" during the Second American Civil War, and Barbara Kopetski is a resistance fighter who leaves behind the war after her heart is broken and she loses faith in the cause. Like the comic, she returns home to become a bounty hunter and owner of the Hammerhead."}
{"id": "3837", "revid": "1272244591", "url": "https://en.wikipedia.org/wiki?curid=3837", "title": "Blazing Saddles", "text": "Blazing Saddles is a 1974 American satirical postmodernist Western black comedy film directed by Mel Brooks, who co-wrote the screenplay with Andrew Bergman, Richard Pryor, Norman Steinberg and Alan Uger, based on a story treatment by Bergman. The film stars Cleavon Little and Gene Wilder. Brooks appears in two supporting roles: Governor William J. Le Petomane, and a Yiddish-speaking Native American chief; he also dubs lines for one of Lili Von Shtupp's backing troupe and a cranky moviegoer. The supporting cast includes Slim Pickens, Alex Karras and David Huddleston, as well as Brooks regulars Dom DeLuise, Madeline Kahn and Harvey Korman. Bandleader Count Basie has a cameo as himself, appearing with his orchestra.\nThe film is full of deliberate anachronisms, from the Count Basie Orchestra playing \"April in Paris\" in the Wild West, to Pickens' character mentioning the \"Wide World of Sports\".\nThe film received generally positive reviews from critics and audiences, was nominated for three Academy Awards and is today regarded as a comedy classic. It is ranked number six on the American Film Institute's \"100 Years...100 Laughs\" list, and was deemed \"culturally, historically, or aesthetically significant\" by the Library of Congress and was selected for preservation in the National Film Registry in 2006.\nPlot.\nOn the American frontier of 1874, a new railroad under construction will have to be rerouted through the town of Rock Ridge to avoid quicksand. Realizing this will make Rock Ridge worth millions, territorial attorney general Hedley Lamarr plans to force Rock Ridge's residents out of the town and sends a gang of thugs, led by his flunky Taggart, to shoot the sheriff and trash the town.\nThe townspeople demand that Governor William J. Le Petomane appoint a new sheriff to protect them. Lamarr persuades dim-witted Le Petomane to appoint Bart, a black railroad worker about to be executed for assaulting Taggart. A black sheriff, Lamarr reasons, will offend the townspeople, create chaos and leave Rock Ridge at his mercy.\nAfter an initial hostile reception in which he takes himself \"hostage\" to escape, Bart relies on his quick wits and the assistance of Jim, an alcoholic gunslinger known as the \"Waco Kid\", to overcome the townspeople's hostility. Bart subdues Mongo, an immensely strong and dim-witted, yet philosophical henchman sent to kill him, then outwits German seductress-for-hire Lili Von Shtupp at her own game, with Lili falling in love with him.\nUpon release, Mongo vaguely informs Bart of Lamarr's connection to the railroad, so Bart and Jim visit the railroad worksite and discover from Bart's best friend Charlie that the railway is planned to go through Rock Ridge. Taggart and his men arrive to kill Bart, but Jim outshoots them and forces their retreat. Furious that his schemes have backfired, Lamarr recruits an army of thugs, including common criminals, motorcycle gangsters, Ku Klux Klansmen, Nazi soldiers, and Methodists.\nEast of Rock Ridge, Bart introduces the White townspeople to the black, Chinese, and Irish railroad workers who have all agreed to help them in exchange for acceptance by the community, and explains his plan to defeat Lamarr's army. They labor all night to build a perfect copy of the town as a diversion. When Bart realizes it will not be enough to fool the villains, the townsfolk construct copies of themselves.\nBart, Jim, and Mongo buy time by constructing the \"Gov. William J. Le Petomane Thruway\", forcing the raiding party to send for change to pay the toll. Once through the tollbooth, the raiders attack the fake town and its population of dummies, which have been booby trapped with dynamite. After Jim detonates the bombs with his sharpshooting, launching bad guys and horses skyward, the Rock Ridgers attack the villains with Lili singing with the Nazi soldiers.\nThe resulting brawl between townsfolk, railroad workers, and Lamarr's thugs literally breaks the fourth wall and bursts onto a neighboring movie set where director Buddy Bizarre is filming a Busby Berkeley-style top-hat-and-tails musical number. Then the brawl spreads into the studio commissary for a food fight and spills out of the Warner Bros. film lot onto the streets of Burbank.\nLamarr escapes the brawl and takes a taxi to hide at Mann's Chinese Theatre which is showing the premiere of \"Blazing Saddles\". As he settles into his seat, he sees onscreen Bart arriving on horseback outside the theatre. Bart blocks Lamarr's escape and shoots him in the groin. Bart and Jim then enter the theater to watch the end of the film.\nBack in the film, Bart announces to the townspeople that he is moving on because his work is done (and because he is bored). Riding out of town, he finds Jim, still eating his popcorn, and invites him along to \"nowhere special\". The two friends briefly ride into the desert before dismounting and boarding a limousine which drives off into the sunset.\nCast.\nCast notes:\nProduction.\nDevelopment.\nThe idea came from a story outline written by Andrew Bergman that he originally intended to develop and produce himself. \"I wrote a first draft called \"Tex-X\"\" (a play on Malcolm X's name), he said. \"Alan Arkin was hired to direct and James Earl Jones was going to play the sheriff. That fell apart, as things often do.\" Brooks was taken with the story, which he described as \"hip talk\u20141974 talk and expressions\u2014happening in 1874 in the Old West\", and purchased the film rights from Bergman. Though he had not worked with a writing team since \"Your Show of Shows\", he hired a group of writers (including Bergman) to expand the outline, and posted a large sign: \"Please do not write a polite script.\"\nBrooks described the writing process as chaotic: Bergman remembers the room being just as chaotic, telling \"Creative Screenwriting\", \nTitle.\nThe original title, \"Tex X\", was rejected to avoid it being mistaken for an X-rated film, as were \"Black Bart\" \u2013 a reference to Black Bart, a white highwayman of the 19th century \u2013 and \"Purple Sage\". Brooks said he finally conceived \"Blazing Saddles\" one morning while taking a shower.\nCasting.\nRichard Pryor was Brooks' original choice to play Sheriff Bart, but the studio, claiming his history of drug arrests made him uninsurable, refused to approve financing with Pryor as the star. The role of Sheriff Bart went to Cleavon Little, and Pryor remained as a screenwriter instead. Brooks offered the other leading role, the Waco Kid, to John Wayne. He declined it, deeming the film \"too blue\" for his family-oriented image, but assured Brooks that \"he would be the first one in line to see it.\" After that, Dan Dailey was Brooks' first choice for the role. Gig Young was cast, but he collapsed during his first scene from what was later determined to be alcohol withdrawal syndrome, and Gene Wilder was flown in to replace him.\nJohnny Carson and Wilder both turned down the Hedley Lamarr role before Harvey Korman was cast. Madeline Kahn objected when Brooks asked to see her legs during her audition. \"She said, 'So it's THAT kind of an audition? Brooks recalled. \"I explained that I was a happily married man and that I needed someone who could straddle a chair with her legs like Marlene Dietrich in \"Destry Rides Again.\" So she lifted her skirt and said, 'No touching.\nFilming.\nPrincipal photography began on March 6, 1973, and wrapped in early May 1973. Brooks had numerous conflicts over content with Warner Bros. executives, including frequent use of the word \"nigger\", Lili Von Shtupp's seduction scene, the cacophony of flatulence around the campfire and Mongo punching out a horse. Brooks, whose contract gave him final cut, declined to make any substantive changes, with the exception of cutting Bart's final line during Lili's seduction: \"I hate to disappoint you, ma'am, but you're sucking my arm.\" When asked later about the many \"nigger\" references, Brooks said he received consistent support from Pryor and Little. He added: \"If they did a remake of \"Blazing Saddles\" today [2012], they would leave out the N-word. And then, you've got no movie.\" Brooks said the use of the N-word was to show how despised, hated, and loathed the black sheriff was. Brooks said he received many letters of complaint after the film's release.\nMusic.\nBrooks wrote the music and lyrics for three of \"Blazing Saddles\" songs, \"The Ballad of Rock Ridge\", \"I'm Tired\", and \"The French Mistake\". Brooks also wrote the lyrics to the title song, with music by composer John Morris. To sing the title song, Brooks advertised in the trade papers for a \"Frankie Laine\u2013type\" singer; to his surprise, Laine himself offered his services. \"Frankie sang his heart out ... and we didn't have the heart to tell him it was a spoof. He never heard the whip cracks; we put those in later. We got so lucky with his serious interpretation of the song.\"In an interview with Terry Gross, Laine said that he did not know at the time that \"Blazing Saddles\" was a comedy.\nThe choreographer for \"I'm Tired\" and \"The French Mistake\" was Alan Johnson. \"I'm Tired\" is a homage to and parody of Marlene Dietrich's performance of Cole Porter's song \"I'm the Laziest Gal in Town\" in Alfred Hitchcock's 1950 film \"Stage Fright\", as well as \"Falling in Love Again (Can't Help It)\" from \"The Blue Angel\".\nThe orchestrations were by Morris and Jonathan Tunick.\nLawsuit.\nDuring production, retired longtime film star Hedy Lamarr sued Warner Bros. for $100,000, charging that the film's running parody of her name infringed on her right to privacy. Brooks said that he was flattered and chose to not fight it in court; the studio settled out of court for a small sum and an apology for \"almost using her name\". Brooks said that Lamarr \"never got the joke\". This lawsuit would be referenced by an in-film joke where Brooks' character, the Governor, tells Lamarr that \"This is 1874; you'll be able to sue HER.\"\nRelease.\nThe film was almost unreleased. \"When we screened it for executives, there were few laughs,\" said Brooks. \"The head of distribution said, 'Let's dump it and take a loss.' But [studio president John] Calley insisted they open it in New York, Los Angeles, and Chicago as a test. It became the studio's top moneymaker that summer.\"\nThe world premiere took place on February 7, 1974, at the Pickwick Drive-In Theater in Burbank; 250 invited guests\u2014including Little and Wilder\u2014watched the film on horseback.\nCritical response.\nWhile \"Blazing Saddles\" is now considered a classic, critical reaction was mixed upon initial release. Vincent Canby wrote:\nRoger Ebert gave the film four stars out of four, calling it a \"crazed grab bag of a movie that does everything to keep us laughing except hit us over the head with a rubber chicken. Mostly, it succeeds. It's an audience picture; it doesn't have a lot of classy polish and its structure is a total mess. But of course! What does that matter while Alex Karras is knocking a horse cold with a right cross to the jaw?\" Gene Siskel awarded three stars out of four and called it \"bound to rank with the funniest of the year,\" adding, \"Whenever the laughs begin to run dry, Brooks and his quartet of gag writers splash about in a pool of obscenities that score belly laughs if your ears aren't sensitive and if you're hip to western movie conventions being parodied.\"\nCritics often perceived \"Blazing Saddles\" as inherently \"un-cinematic\", defying some expectations for Hollywood filmmaking in the era, often displaying production style associated with Broadway theater and US television variety shows. This was in part due to its \"simplistic framing\" and the casting of Harvey Korman, known for \"The Carol Burnett Show\" (CBS, 1967\u20131978), which was similarly \"low on characterization and story, instead opting for a high volume of one-liners and visual gags.\" Typical to this perception, \"Variety\" wrote: \"If comedies are measured solely by the number of yocks they generate from audiences, then \"Blazing Saddles\" must be counted a success ... Few viewers will have time between laughs to complain that pic is essentially a raunchy, protracted version of a television comedy skit.\"\nCharles Champlin of the \"Los Angeles Times\" called the film \"irreverent, outrageous, improbable, often as blithely tasteless as a stag night at the Friar's Club and almost continuously funny.\" Gary Arnold of \"The Washington Post\" was negative, writing: \"Mel Brooks squanders a snappy title on a stockpile of stale jokes. To say that this slapdash Western spoof lacks freshness and spontaneity and originality is putting it mildly. \"Blazing Saddles\" is at once a messy and antiquated gag machine.\" Jan Dawson of \"The Monthly Film Bulletin\" wrote: \"Perhaps it is pedantic to complain that the whole is not up to the sum of its parts when, for the curate's egg that it is, \"Blazing Saddles\" contains so many good parts and memorable performances.\" John Simon wrote a negative review of \"Blazing Saddles\", saying: \"All kinds of gags\u2014chiefly anachronisms, irrelevancies, reverse ethnic jokes, and out and out vulgarities\u2014are thrown together pell-mell, batted about insanely in all directions, and usually beaten into the ground.\"\nOn review aggregator Rotten Tomatoes, the film has an approval rating of 88% based on 69 reviews, with an average rating of 8.10/10. The site's critics consensus reads: \"Daring, provocative, and laugh-out-loud funny, \"Blazing Saddles\" is a gleefully vulgar spoof of Westerns that marks a high point in Mel Brooks' storied career.\" On Metacritic it has a score of 73 out of 100 based on 12 critics, indicating \"generally favorable reviews\".\nIshmael Reed's 1969 novel \"Yellow Back Radio Broke-Down\" has been cited as an important precursor or influence for \"Blazing Saddles\", a connection that Reed himself has made.\nBox office.\nThe film earned theatrical rentals of $26.7 million in its initial release in the United States and Canada. In its 1976 reissue, it earned a further $10.5 million and another $8 million in 1979. Its total rentals in the United States and Canada totalled $47.8 million from a gross of $119.5 million, becoming only the tenth film up to that time to pass the $100 million mark.\nAwards and accolades.\nWhile addressing his group of bad guys, Harvey Korman's character reminds them that although they are risking their lives, he is \"risking an almost certain Academy Award nomination for Best Supporting Actor!\" Korman did not receive an Oscar bid, but the film did get three nominations at the 47th Academy Awards, including Best Supporting Actress for Madeline Kahn.\nIn 2006, \"Blazing Saddles\" was deemed \"culturally, historically, or aesthetically significant\" by the Library of Congress and was selected for preservation in the National Film Registry.\nUpon the release of the 30th-anniversary special edition in 2004, \"Today\" said that the movie \"skewer[ed] just about every aspect of racial prejudice while keeping the laughs coming\" and that it was \"at the top of a very short list\" of comedies still funny after 30 years. In 2014, NPR wrote that, four decades after the movie was made, it was \"still as biting a satire\" on racism as ever, although its treatment of gays and women was \"not self-aware at all\".\nThe film is recognized by the American Film Institute in these lists:\nAdaptations.\nTV series.\nA television pilot titled \"Black Bart\" was produced for CBS based on Bergman's original story. It featured Louis Gossett Jr. as Bart and Steve Landesberg as his drunkard sidekick, a former Confederate officer named \"Reb Jordan\". Other cast members included Millie Slavin and Noble Willingham. Bergman is listed as the sole creator. CBS aired the pilot once on April 4, 1975. The pilot episode of \"Black Bart\" was later included as a bonus feature on the \"Blazing Saddles\" 30th Anniversary DVD and the Blu-ray disc.\nPossible stage production.\nIn September 2017, Brooks indicated his desire to do a stage version of \"Blazing Saddles\" in the future.\nIn popular culture.\nThe Rock Ridge standard for CD and DVD media is named after the town in \"Blazing Saddles\".\nThe 1988 animated television film \"The Good, the Bad, and Huckleberry Hound\" is a Western parody. Starring anthropomorphic cartoon dog Huckleberry Hound, the film is set in the California Gold Rush era and has similar spoofs and gags to \"Blazing Saddles\", as well as depiction of Native American stereotypes. Here, much like Bart, Huck is unexpectedly appointed as a sheriff to defend townspeople.\nIn 2011, the fifteenth episode of \"Supernatural\" season 6 was entitled \"The French Mistake\", as a reference to the genre defining Fourth Wall breaking scene.\nThe 2022 animated film \"\", starring Michael Cera, Samuel L. Jackson, Michelle Yeoh and Ricky Gervais, was originally titled \"Blazing Samurai\" and its creators called it \"equally inspired by and an homage to \"Blazing Saddles\".\" Brooks served as an executive producer for the production, voiced one of the characters, and received screenplay credit.\nHome media.\nThe film was released on VHS several times and was first released on DVD in 1997, followed by a 30th Anniversary Special Edition DVD in 2004 and a Blu-ray version in 2006. A 40th anniversary Blu-ray set was released in 2014."}
{"id": "3838", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=3838", "title": "Bruce Sterling", "text": "Michael Bruce Sterling (born April 14, 1954) is an American science fiction author known for his novels and short fiction and editorship of the \"Mirrorshades\" anthology. In particular, he is linked to the cyberpunk subgenre.\nSterling's first science-fiction story, \"Man-Made Self\", was sold in 1976. He is the author of science-fiction novels, including \"Schismatrix\" (1985), \"Islands in the Net\" (1988), and \"Heavy Weather\" (1994). In 1992, he published his first non-fiction book, \"The Hacker Crackdown: Law and Disorder on the Electronic Frontier\".\nHe has been interviewed for documentaries such as \"Freedom Downtime\", \"TechnoCalyps\" and \"Traceroute\".\nWriting.\nSterling is one of the founders of the cyberpunk movement in science fiction, along with William Gibson, Rudy Rucker, John Shirley, Lewis Shiner, and Pat Cadigan. In addition, he is one of the subgenre's chief ideological promulgators. This has earned him the nickname \"Chairman Bruce\". He was also one of the first organizers of the Turkey City Writer's Workshop, and is a frequent attendee at the Sycamore Hill Writer's Workshop. He won Hugo Awards for his novelettes \"Bicycle Repairman\" (1996) and \"Taklamakan\" (1998). His first novel, \"Involution Ocean\" (1977), features the world Nullaqua where all the atmosphere is contained in a single, miles-deep crater. The story concerns a ship sailing on the ocean of dust at the bottom and hunting creatures called dustwhales. It is partially a science-fictional pastiche of \"Moby-Dick\" by Herman Melville.\nIn the early 1980s, Sterling wrote a series of stories set in the Shaper/Mechanist universe: the Solar System is colonized, with two major warring factions. The Mechanists use a great deal of computer-based mechanical technologies; the Shapers do genetic engineering on a massive scale. The situation is complicated by the eventual contact with alien civilizations; humanity eventually splits into many subspecies, with the implication that some of these vanish from the galaxy, reminiscent of the singularity in the works of Vernor Vinge. The Shaper/Mechanist stories can be found in the collections \"Crystal Express\" and \"Schismatrix Plus\", which contains the novel \"Schismatrix\" and all of the stories set in the Shaper/Mechanist universe. Alastair Reynolds identified \"Schismatrix\" and the other Shaper/Mechanist stories as one of the greatest influences on his own work.\nIn the 1980s, Sterling edited the science fiction critical fanzine \"Cheap Truth\" under the alias of Vincent Omniaveritas. He wrote a column called \"Catscan\" for the now-defunct science fiction critical magazine \"SF Eye\". He contributed a chapter to \"Sound Unbound: Sampling Digital Music and Culture\" (MIT Press, 2008) edited by Paul D. Miller, a.k.a. DJ Spooky. From April 2009 through May 2009, he was an editor at \"Cool Tools\".\nFrom October 2003 to May 2020 Sterling blogged at \"Beyond the Beyond\", which was hosted by \"Wired\" until the COVID-19 pandemic led Cond\u00e9 Nast to cut back because of an advertising slump. He also contributed to other print and online platforms, including \"The Magazine of Fantasy &amp; Science Fiction\".\nWriting projects.\nHe has been the instigator of three projects which can be found on the Web:\nNeologisms.\nSterling has coined various neologisms to describe things that he believes will be common in the future, especially items which already exist in limited numbers.\nBibliography.\nSterling's novels include:\nPersonal life.\nIn the beginning of his childhood he lived in Galveston, Texas until his family moved to India. Sterling spent several years in India and has a fondness for Bollywood films. In 1976, he graduated from the University of Texas with a degree in journalism. In 1978, he was the Dungeon Master for a \"Dungeons &amp; Dragons\" game whose players included Warren Spector, who cited Sterling's game as a major inspiration for the game design of \"Deus Ex\". \nIn 2003, he was appointed professor at the European Graduate School where he is teaching summer intensive courses on media and design. In 2005, he became \"visionary in residence\" at ArtCenter College of Design in Pasadena, California. He lived in Belgrade with Serbian author and film-maker Jasmina Te\u0161anovi\u0107 for several years, and married her in 2005. In September 2007 he moved to Turin, Italy. Both Sterling and artist and musician Florian-Ayala Fauna are sponsors for V. Vale's RE/Search newsletter."}
{"id": "3840", "revid": "48351754", "url": "https://en.wikipedia.org/wiki?curid=3840", "title": "Brain abscess", "text": "Brain abscess (or cerebral abscess) is an abscess within the brain tissue caused by inflammation and collection of infected material coming from local (ear infection, dental abscess, infection of paranasal sinuses, infection of the mastoid air cells of the temporal bone, epidural abscess) or remote (lung, heart, kidney etc.) infectious sources. The infection may also be introduced through a skull fracture following a head trauma or surgical procedures. Brain abscess is usually associated with congenital heart disease in young children. It may occur at any age but is most frequent in the third decade of life.\nSigns and symptoms.\nFever, headache, and neurological problems, while classic, only occur in 20% of people with brain abscess.\nThe famous triad of fever, headache and focal neurologic findings are highly suggestive of brain abscess. These symptoms are caused by a combination of increased intracranial pressure due to a space-occupying lesion (headache, vomiting, confusion, coma), infection (fever, fatigue etc.) and focal neurologic brain tissue damage (hemiparesis, aphasia etc.).\nThe most frequent presenting symptoms are headache, drowsiness, confusion, seizures, hemiparesis or speech difficulties together with fever with a rapidly progressive course. Headache is characteristically worse at night and in the morning, as the intracranial pressure naturally increases when in the supine position. This elevation similarly stimulates the medullary vomiting center and area postrema, leading to morning vomiting.\nOther symptoms and findings depend largely on the specific location of the abscess in the brain. An abscess in the cerebellum, for instance, may cause additional complaints as a result of brain stem compression and hydrocephalus. Neurological examination may reveal a stiff neck in occasional cases (erroneously suggesting meningitis).\nPathophysiology.\nBacterial.\nAnaerobic and microaerophilic cocci and gram-negative and gram-positive anaerobic bacilli are the predominant bacterial isolates. Many brain abscesses are polymicrobial. The predominant organisms include: \"Staphylococcus aureus\", aerobic and anaerobic streptococci (especially \"Streptococcus intermedius\"), \"Bacteroides\", \"Prevotella\", and \"Fusobacterium\" species, Enterobacteriaceae, \"Pseudomonas\" species, and other anaerobes. Less common organisms include: \"Haemophillus influenzae\", \"Streptococcus pneumoniae\" and \"Neisseria meningitidis\".\nBacterial abscesses rarely (if ever) arise \"de novo\" within the brain although establishing a cause can be difficult in many cases. There is almost always a primary lesion elsewhere in the body that must be sought assiduously because failure to treat the primary lesion will result in relapse. In cases of trauma, for example in compound skull fractures where fragments of bone are pushed into the substance of the brain, the cause of the abscess is obvious. Similarly, bullets and other foreign bodies may become sources of infection if left in place. The location of the primary lesion may be suggested by the location of the abscess: infections of the middle ear result in lesions in the middle and posterior cranial fossae; congenital heart disease with right-to-left shunts often result in abscesses in the distribution of the middle cerebral artery; and infection of the frontal and ethmoid sinuses usually results in collection in the subdural sinuses.\nOther organisms.\nFungi and parasites may also cause the disease. Fungi and parasites are especially associated with immunocompromised patients. Other causes include: \"Nocardia asteroides\", \"Mycobacterium\", Fungi (e.g. \"Aspergillus\", \"Candida\", \"Cryptococcus\", \"Mucorales\", \"Coccidioides\", \"Histoplasma capsulatum\", \"Blastomyces dermatitidis\", \"Bipolaris\", \"Exophiala dermatitidis\", \"Curvularia pallescens\", \"Ochroconis gallopava\", \"Ramichloridium mackenziei\", \"Pseudallescheria boydii\"), Protozoa (e.g. \"Toxoplasma gondii\", \"Entamoeba histolytica\", \"Trypanosoma cruzi\", \"Schistosoma\", \"Paragonimus\"), and Helminths (e.g. \"Taenia solium\"). Organisms that are most frequently associated with brain abscess in patients with AIDS are poliovirus, \"Toxoplasma gondii\", and \"Cryptococcus neoformans\", though in infection with the latter organism, symptoms of meningitis generally predominate.\nThese organisms are associated with certain predisposing conditions:\nDiagnosis.\nThe diagnosis is established by a computed tomography (CT) (with contrast) examination. At the initial phase of the inflammation (which is referred to as cerebritis), the immature lesion does not have a capsule and it may be difficult to distinguish it from other space-occupying lesions or infarcts of the brain. Within 4\u20135 days the inflammation and the concomitant dead brain tissue are surrounded with a capsule, which gives the lesion the famous ring-enhancing lesion appearance on CT examination with contrast (since intravenously applied contrast material can not pass through the capsule, it is collected around the lesion and looks as a ring surrounding the relatively dark lesion). Lumbar puncture procedure, which is performed in many infectious disorders of the central nervous system is contraindicated in this condition (as it is in all space-occupying lesions of the brain) because removing a certain portion of the cerebrospinal fluid may alter the concrete intracranial pressure balances and causes the brain tissue to move across structures within the skull (brain herniation).\nRing enhancement may also be observed in cerebral hemorrhages (bleeding) and some brain tumors. However, in the presence of the rapidly progressive course with fever, focal neurologic findings (hemiparesis, aphasia etc.) and signs of increased intracranial pressure, the most likely diagnosis should be the brain abscess.\nTreatment.\nThe treatment includes lowering the increased intracranial pressure and starting intravenous antibiotics (and meanwhile identifying the causative organism mainly by blood culture studies).\nHyperbaric oxygen therapy (HBO2 or HBOT) is indicated as a primary and adjunct treatment which provides four primary functions.\nFirstly, HBOT reduces intracranial pressure. Secondly, high partial pressures of oxygen act as a bactericide and thus inhibits the anaerobic and functionally anaerobic flora common in brain abscess. Third, HBOT optimizes the immune function thus enhancing the host defense mechanisms and fourth, HBOT has been found to be of benefit when brain abscess is concomitant with cranial osteomyelitis.\nSecondary functions of HBOT include increased stem cell production and up-regulation of VEGF which aid in the healing and recovery process.\nSurgical drainage of the abscess remains part of the standard management of bacterial brain abscesses. The location and treatment of the primary lesion is also crucial, as is the removal of any foreign material (bone, dirt, bullets, and so forth).\nThere are few exceptions to this rule: \"Haemophilus influenzae\" meningitis is often associated with subdural effusions that are mistaken for subdural empyemas. These effusions resolve with antibiotics and require no surgical treatment. Tuberculosis can produce brain abscesses that look identical to conventional bacterial abscesses on CT imaging. Surgical drainage or aspiration is often necessary to identify \"Mycobacterium tuberculosis\", but once the diagnosis is made no further surgical intervention is necessary.\nCT guided stereotactic aspiration is also indicated in the treatment of brain abscess. The use of pre-operative imaging, intervention with post-operative clinical and biochemical monitoring used to manage brain abscesses today dates back to the Pennybacker system pioneered by Somerset, Kentucky-born neurosurgeon Joseph Buford Pennybacker, director of the neurosurgery department of the Radcliffe Infirmary, Oxford from 1952 to 1971. \nPrognosis.\nWhile death occurs in about 10% of cases, people do well about 70% of the time. This is a large improvement from the 1960s due to improved ability to image the head, more effective neurosurgery and more effective antibiotics."}
{"id": "3845", "revid": "38848426", "url": "https://en.wikipedia.org/wiki?curid=3845", "title": "Brigitte Bardot", "text": "Brigitte Anne-Marie Bardot ( ; ; born 28 September 1934), often referred to by her initials B.B., is a French former actress, singer, and model as well as an animal rights activist. Famous for portraying characters with hedonistic lifestyles, she is one of the best known symbols of the sexual revolution. Although she withdrew from the entertainment industry in 1973, she remains a major pop culture icon. She has acted in 47 films, performed in several musicals, and recorded more than 60 songs. She was awarded the Legion of Honour in 1985.\nBorn and raised in Paris, Bardot was an aspiring ballerina during her childhood. She started her acting career in 1952 and achieved international recognition in 1957 for her role in \"And God Created Woman\" (1956), catching the attention of many French intellectuals and earning her the nickname \"sex kitten\". She was the subject of philosopher Simone de Beauvoir's 1959 essay \"The Lolita Syndrome\", which described her as a \"locomotive of women's history\" and built upon existentialist themes to declare her the most liberated woman of France. She won a 1961 David di Donatello Best Foreign Actress Award for her work in \"The Truth\" (1960)\".\" Bardot later starred in Jean-Luc Godard's film \"Le M\u00e9pris\" (1963). For her role in Louis Malle's film \"Viva Maria!\" (1965), she was nominated for the BAFTA Award for Best Foreign Actress. French President Charles de Gaulle called Bardot \"the French export as important as Renault cars\".\nAfter retiring from acting in 1973, Bardot became an animal rights activist and created the Brigitte Bardot Foundation. She is known for her strong personality, outspokenness, and speeches on animal defense; she has been fined twice for public insults. She has also been a controversial political figure, as of November 2021 having been fined six times for inciting racial hatred when she criticised immigration and Islam in France and called residents of R\u00e9union \"savages\". She is married to Bernard d'Ormale, a former adviser to Jean-Marie Le Pen, a far-right French politician. Bardot is a member of the Global 500 Roll of Honour of the United Nations Environment Programme and has received several awards and accolades from UNESCO and PETA. In 2011, \"Los Angeles Times Magazine\" ranked her second on the \"50 Most Beautiful Women In Film\".\nEarly life.\nBardot was born on 28 September 1934 in the 15th arrondissement of Paris to Louis Bardot (1896\u20131975) and Anne-Marie Mucel (1912\u20131978). Bardot's father, who originated from Ligny-en-Barrois, was an engineer and the proprietor of several industrial factories in Paris. Her mother was the daughter of an insurance company director. She grew up in a conservative Catholic family, as had her father. She suffered from amblyopia as a child, which resulted in decreased vision of her left eye. She has one younger sister, Mijanou Bardot.\nBardot's childhood was prosperous; she lived in her family's seven-bedroom apartment in the luxurious 16th arrondissement. However, she recalled feeling resentful in her early years. Her father demanded that she follow strict behavioural standards, including good table manners, and wear appropriate clothes. Her mother was highly selective in choosing companions for her, so Bardot had very few childhood friends. Bardot cited a personal traumatic incident when she and her sister broke her parents' favourite vase while they were playing in the house; her father whipped the sisters 20 times and subsequently treated them like \"strangers\", demanding that they address their parents by the formal pronoun \"vous\", used in French when speaking to unfamiliar or higher-status persons outside the immediate family. The incident led to Bardot decisively resenting her parents and to her future rebellious lifestyle.\nDuring World War II, when Paris was occupied by Nazi Germany, Bardot spent more time at home due to increasingly strict civilian surveillance. She became engrossed in dancing to records, which her mother saw as a potential for a ballet career. Bardot was admitted at the age of seven to the private school Cours Hattemer. She went to school three days a week, which gave her ample time to take dance lessons at a local studio under her mother's arrangements. In 1949, Bardot was accepted at the Conservatoire de Paris. She attended ballet classes held by Russian choreographer Boris Knyazev for three years. She also studied at the Institut de la Tour, a private Catholic high school near her home.\nH\u00e9l\u00e8ne Gordon-Lazareff, the director of the magazines \"Elle\" and \"Le Jardin des Modes\", hired Bardot in 1949 as a \"junior\" fashion model. On 8 March 1950, 15-year-old Bardot appeared on the cover of \"Elle\", which brought her an acting offer for the film \"Les Lauriers sont coup\u00e9s\" from director Marc All\u00e9gret. Her parents opposed her becoming an actress, but her grandfather was supportive, saying that \"If this little girl is to become a whore, cinema will not be the cause.\" At the audition, Bardot met Roger Vadim, who later notified her that she did not get the role. They subsequently fell in love. Her parents fiercely opposed their relationship; her father announced to her one evening that she would continue her education in England and that he had bought her a train ticket for the following day. Bardot reacted by putting her head into an oven with open fire; her parents stopped her and ultimately accepted the relationship, on condition that she marry Vadim at the age of 18.\nCareer.\nBeginnings: 1952\u20131955.\nBardot appeared on the cover of \"Elle\" again in 1952, which landed her an offer for a small part in the comedy film \"Crazy for Love\" the same year, directed by Jean Boyer and starring Bourvil. She was paid 200,000 francs (about 575 1952 US dollars) for the small role portraying a cousin of the main character. Bardot had her second film role in \"Manina, the Girl in the Bikini\" (1952), directed by Willy Rozier. She also had roles in the 1953 films \"The Long Teeth\" and \"His Father's Portrait\".\nBardot had a small role in a Hollywood-financed film being shot in Paris in 1953, \"Act of Love\", starring Kirk Douglas. She received media attention when she attended the Cannes Film Festival in April 1953.\nBardot had a leading role in 1954 in an Italian melodrama, \"Concert of Intrigue\" and in a French adventure film, \"Caroline and the Rebels\". She had a good part as a flirtatious student in the 1955 \"School for Love\", opposite Jean Marais, for director Marc All\u00e9gret.\nBardot played her first sizeable English-language role in 1955 in \"Doctor at Sea\", as the love interest for Dirk Bogarde. The film was the third-most-popular movie in Britain that year.\nBardot had a small role in \"The Grand Maneuver\" (1955) for director Ren\u00e9 Clair, supporting G\u00e9rard Philipe and Michelle Morgan. The part was bigger in \"The Light Across the Street\" (1956) for director Georges Lacombe. She had another in the Hollywood film \"Helen of Troy\", playing Helen's handmaiden.\nFor the Italian movie \"Mio figlio Nerone\" (1956) brunette Bardot was asked by the director to appear as a blonde. She dyed her hair rather than wear a wig; she was so pleased with the results that she decided to retain the color.\nRise to stardom: 1956\u20131962.\nBardot then appeared in four movies that made her a star. First up was a musical, \"Naughty Girl\" (1956), where Bardot played a troublesome school girl. Directed by Michel Boisrond, it was co-written by Roger Vadim and was a great success, going on to become the 12th most popular film of the year in France. It was followed by a comedy, \"Plucking the Daisy\" (1956), also written by Vadim. This was succeeded by \"The Bride Is Much Too Beautiful\" (1956) with Louis Jourdan.\nFinally, there was the melodrama \"And God Created Woman\" (1956). The movie was Vadim's debut as director, with Bardot starring opposite Jean-Louis Trintignant and Curt Jurgens. The film, about an immoral teenager in an otherwise respectable small-town setting, was an even larger success, not just in France but also around the world, listed among the ten most popular films in Great Britain in 1957. In the United States the film was the highest-grossing foreign film ever released, earning $4\u00a0million, which author Peter Lev describes as \"an astonishing amount for a foreign film at that time.\" It turned Bardot into an international star. From at least 1956, she was hailed as the \"sex kitten\". The film scandalized the United States and some theater managers were even arrested just for screening it.\nPaul O'Neil of \"Life\" (June 1958) in describing Bardot's international popularity, writes:\nIn gaining her present eminence, Brigitte Bardot has had certain advantages beyond those she was born with. Like the European sports car, she has arrived on the American scene at a time when the American public is ready, even hungry, for something racier and more realistic than the familiar domestic product.&lt;br&gt;\nDuring her early career, professional photographer Sam L\u00e9vin's photos contributed to the image of Bardot's sensuality. British photographer Cornel Lucas made images of Bardot in the 1950s and 1960s that have become representative of her public persona.\nBardot followed \"And God Created Woman\" up with \"La Parisienne\" (1957), a comedy co-starring Charles Boyer for director Boisrond. She was reunited with Vadim in another melodrama \"The Night Heaven Fell\" (1958), and played a criminal who seduced Jean Gabin in \"In Case of Adversity\" (1958). The latter was the 13th most seen movie of the year in France. In 1958, Bardot became the highest-paid actress in the country of France.\n\"The Female\" (1959) for director Julien Duvivier was popular, but \"Babette Goes to War\" (1959), a comedy set in World War II, was a huge hit, the fourth biggest movie of the year in France. Also widely seen was \"Come Dance with Me\" (1959) from Boisrond.\nBardot's next film was courtroom drama \"The Truth\" (1960), from Henri-Georges Clouzot. It was a highly publicised production, which resulted in Bardot having an affair and attempting suicide. The film was Bardot's biggest commercial success in France, the third biggest hit of the year, and was nominated for the Academy Award for Best Foreign Film. Bardot was awarded a David di Donatello Award for Best Foreign Actress for her role in the film.\nShe made a comedy with Vadim, \"Please, Not Now!\" (1961), and had a role in the all-star anthology, \"Famous Love Affairs\" (1962).\nBardot starred alongside Marcello Mastroianni in a film inspired by her life in \"A Very Private Affair\" (\"Vie priv\u00e9e\", 1962), directed by Louis Malle. More popular than that was her role in \"Love on a Pillow\" (1962).\nInternational films and singing career: 1962\u20131968.\nIn the mid-1960s, Bardot made films that seemed to be more aimed at the international market. She starred in Jean-Luc Godard's film \"Le M\u00e9pris\" (1963), produced by Joseph E. Levine and starring Jack Palance. The following year she co-starred with Anthony Perkins in the comedy \"Une ravissante idiote\" (1964).\n\"Dear Brigitte\" (1965), Bardot's first Hollywood film, was a comedy starring James Stewart as an academic whose son develops a crush on Bardot. Bardot's appearance was relatively brief in the film, and the movie was not a big success.\nMore successful was the Western buddy comedy \"Viva Maria!\" (1965) for director Louis Malle, appearing opposite Jeanne Moreau. It was a big hit in France and worldwide, although it did not break through in the United States as much as had been hoped.\nAfter a cameo in Godard's \"Masculin F\u00e9minin\" (1966), she had her first outright flop for some years, \"Two Weeks in September\" (1968), a French\u2013English co-production. She had a small role in the all-star \"Spirits of the Dead\" (1968), acting opposite Alain Delon, then tried a Hollywood film again: \"Shalako\" (1968), a Western starring Sean Connery, which was another box-office disappointment.\nShe participated in several musical shows and recorded many popular songs in the 1960s and 1970s, mostly in collaboration with Serge Gainsbourg, Bob Zagury and Sacha Distel, including \"Harley Davidson\"; \"Je Me Donne \u00c0 Qui Me Pla\u00eet\"; \"Bubble gum\"; \"Contact\"; \"Je Reviendrai Toujours Vers Toi\"; \"L'Appareil \u00c0 Sous\"; \"La Madrague\"; \"On D\u00e9m\u00e9nage\"; \"Sidonie\"; \"Tu Veux, Ou Tu Veux Pas?\"; \"Le Soleil De Ma Vie\" (a cover of Stevie Wonder's \"You Are the Sunshine of My Life\"); and \"Je t'aime... moi non-plus\". Bardot pleaded with Gainsbourg not to release this duet and he complied with her wish; the following year, he rerecorded a version with British-born model and actress Jane Birkin that became a massive hit all over Europe. The version with Bardot was issued in 1986 and became a download hit in 2006 when Universal Music made its back catalogue available to purchase online, with this version of the song ranking as the third most popular download.\nFinal films: 1969\u20131973.\nFrom 1969 to 1972, Bardot was the official face of Marianne, who had previously up until then been anonymous, to represent the liberty of France.\n\"Her next film, Les Femmes\" (1969), was a flop, although the screwball comedy \"The Bear and the Doll\" (1970) performed better. Her last few films were mostly comedies: \"Les Novices\" (1970), \"Boulevard du Rhum\" (1971) (with Lino Ventura). \"The Legend of Frenchie King\" (1971) was more popular, helped by Bardot co-starring with Claudia Cardinale.\nShe made one more movie working with Vadim, \"Don Juan, or If Don Juan Were a Woman\" (1973), playing the title role. Vadim said the film marked \"Underneath what people call 'the Bardot myth' was something interesting, even though she was never considered the most professional actress in the world. For years, since she has been growing older, and the Bardot myth has become just a souvenir... I was curious in her as a woman and I had to get to the end of something with her, to get out of her and express many things I felt were in her. Brigitte always gave the impression of sexual freedom \u2013 she is a completely open and free person, without any aggression. So I gave her the part of a man \u2013 that amused me\".\n\"If \"Don Juan\" is not my last movie it will be my next to last\", said Bardot during filming. She kept her word and made only one more film, \"The Edifying and Joyous Story of Colinot\" (1973).\nIn 1973, Bardot announced she was retiring from acting as \"a way to get out elegantly\".\nAnimal rights activism.\nBardot met Paul Watson in 1977, the same year he founded the Sea Shepherd Conservation Society, during an operation to condemn the \"massacre\" of seal pups and seal hunting on the Canadian ice floe. In support of animal protection, Bardot went to the ice floe after being invited by Watson. Bardot posed lying down next to the seal pups; the photos were seen worldwide. Bardot and Watson remained friends.\nAfter appearing in more than 40 motion pictures and recording several music albums, Bardot used her fame to promote animal rights.\nIn 1986, she established the Brigitte Bardot Foundation for the Welfare and Protection of Animals. She became a vegetarian and raised three million francs (about 430,000 1986 US dollars) to fund the foundation by auctioning off jewellery and personal belongings.\nBardot has been a strong animal rights activist and a major opponent of the consumption of horse meat.\nIn 1989, while looking after her neighbour, Jean-Pierre Manivet's donkey, the mare displayed excessive interest in Bardot's older donkey and she subsequently had the neighbour's donkey castrated due to concerns the mating would prove fatal for her mare. The neighbour then sued Bardot, and Bardot later won, with the court ordering Manivet to pay 20,000 francs for creating a \"false scandal\". \nBardot urged French television viewers to boycott horse meat and was soon the target of death threats in January 1994. Not backing off from the threats, she sent a letter to the French Minister of Agriculture, Jean Puech, calling on him to ban the sale of horse meat.\nBardot wrote a 1999 letter to Chinese President Jiang Zemin, published in French magazine \"VSD\", in which she accused the Chinese of \"torturing bears and killing the world's last tigers and rhinos to make aphrodisiacs\".\nShe donated more than US$140,000 over two years in 2001 for a mass sterilization and adoption program for Bucharest's stray dogs, estimated to number 300,000.\nIn August 2010, Bardot addressed a letter to Queen Margrethe II of Denmark, appealing for the sovereign to halt the killing of dolphins in the Faroe Islands. In the letter, Bardot describes the activity as a \"macabre spectacle\" that \"is a shame for Denmark and the Faroe Islands ... This is not a hunt but a mass slaughter ... an outmoded tradition that has no acceptable justification in today's world\".\nOn 22 April 2011, French culture minister Fr\u00e9d\u00e9ric Mitterrand officially included bullfighting in the country's cultural heritage. Bardot wrote him a highly critical letter of protest. On 25 May 2011, the Sea Shepherd Conservation Society renamed its fast interceptor vessel, MV \"Gojira\", as MV \"Brigitte Bardot\" in appreciation of her support.\nFrom 2013, the Brigitte Bardot Foundation, in collaboration with Kagyupa International Monlam Trust of India, operated an annual veterinary care camp. Bardot committed to the cause of animal welfare in Bodhgaya over several years.\nOn 23 July 2015, Bardot condemned Australian politician Greg Hunt's plan to eradicate 2 million cats to save endangered species such as the Warru and night parrot.\nAt the age of 90, Bardot appealed to free Watson, who had been detained in Greenland since 21 July 2024, when Japan requested his extradition. Through a request expressed in mid-October 2024 by her lawyers and Sea Shepherd France, Bardot asked French President Emmanuel Macron to grant Watson political asylum. Bardot asked Macron to show \"a little bit of courage\". During that month, she initiated a demonstration in support of Watson in front of the H\u00f4tel de Ville, Paris. Bardot also wrote a letter to Danish Prime Minister Mette Frederiksen, asking her to \"not choose the camp of the oceans gravediggers\".\nPersonal life.\nMarriages and relationships.\nBardot has been married four times, with her current marriage lasting far longer than the previous three combined. By her own count, she has had a total of 17 romantic relationships. Bardot would characteristically leave for another relationship when \"the present was getting lukewarm\"; she said, \"I have always looked for passion. That's why I was often unfaithful. And when the passion was coming to an end, I was packing my suitcase\".\nOn 20 December 1952, aged 18, Bardot married director Roger Vadim. They separated in 1956 after she became involved with \"And God Created Woman\" co-star Jean-Louis Trintignant, divorcing the next year. Trintignant at the time was married to actress St\u00e9phane Audran. Bardot and Vadim had no children together, but remained in touch for the rest of his life and even collaborated on later projects. Bardot and Trintignant lived together for about two years, spanning the period before and after Bardot's divorce from Vadim, but they never married. Their relationship was complicated by Trintignant's frequent absence due to military service and Bardot's affair with musician Gilbert B\u00e9caud.\nAfter her separation from Vadim, Bardot acquired a historic property dating from the 16th century, called Le Castelet, in Cannes. The fourteen-bedroom villa, surrounded by lush gardens, olive trees, and vineyards, consisted of several buildings.\nIn 1958, she bought a second property called La Madrague, located in Saint-Cyr-sur-Mer. In early 1958, her breakup with Trintignant was followed in quick order by a reported nervous breakdown in Italy, according to newspaper reports. A suicide attempt with sleeping pills two days earlier was also noted but was denied by her public relations manager. She recovered within weeks, began a relationship with actor Jacques Charrier, and became pregnant well before they married on 18 June 1959. Bardot's only child, son Nicolas-Jacques Charrier, was born on 11 January 1960. Bardot had an affair with Glenn Ford in the early 1960s. After she and Charrier divorced in 1962, Nicolas was raised in the Charrier family and had little contact with his biological mother until his adulthood. Sami Frey was mentioned as the reason for her divorce from Charrier. Bardot was enamoured of Frey, but he quickly left her.\nFrom 1963 to 1965, she lived with musician Bob Zagury.\nBardot's third marriage was to German millionaire playboy Gunter Sachs, lasting from 14 July 1966 to 7 October 1969, though they had separated the previous year. While filming \"Shalako\", she rejected Sean Connery's advances; she said, \"It didn't last long because I wasn't a James Bond girl! I have never succumbed to his charm!\" In 1968, she began dating Patrick Gilles, who co-starred with her in \"The Bear and the Doll\" (1970); but she ended their relationship in spring 1971.\nOver the next few years, Bardot dated bartender/ski instructor Christian Kalt, nightclub owner Luigi \"Gigi\" Rizzi, singer-songwriter Serge Gainsbourg, writer John Gilmore, actor Warren Beatty, and Laurent Vergez, her co-star in \"Don Juan, or If Don Juan Were a Woman\".\nIn 1974, Bardot appeared in a nude photo shoot in \"Playboy\" magazine, which celebrated her 40th birthday. In 1975, she entered a relationship with artist Miroslav Brozek and posed for some of his sculptures. Brozek was also an occasional actor; his stage name is . The couple lived together for four years, separating in December 1979.\nFrom 1980 to 1985, Bardot had a live-in relationship with French TV producer . On 27 September 1983, the eve of her 49th birthday, Bardot took an overdose of sleeping pills or tranquilizers with red wine, then wandered out to the beach, where she was later pulled from the surf. She had to be rushed to the hospital, where her life was saved after a stomach pump was used to evacuate the pills from her body. Bardot was diagnosed with breast cancer in 1984. She refused to undergo chemotherapy treatment and decided only to do radiation therapy. She recovered in 1986.\nBardot's fourth and current husband is Bernard d'Ormale; they have been married since 16 August 1992. In 2018, in an interview accorded to \"Le Journal du Dimanche\", she denied rumors of relationships with Johnny Hallyday, Jimi Hendrix, and Mick Jagger.\nPolitics and legal issues.\nBardot expressed support for President Charles de Gaulle in the 1960s.\nIn her 1999 book \"Le Carr\u00e9 de Pluton\" (\"Pluto's Square\"), Bardot criticizes the procedure used in the ritual slaughter of sheep during the Muslim festival of Eid al-Adha. Additionally, in a section in the book entitled \"Open Letter to My Lost France\", she writes that \"my country, France, my homeland, my land is again invaded by an overpopulation of foreigners, especially Muslims\". For this comment, a French court fined her 30,000 francs (about 4,200 US dollars in 2000) in June 2000. She had been fined in 1997 for the original publication of this open letter in \"Le Figaro\" and again in 1998 for making similar remarks.\nIn her 2003 book, \"Un cri dans le silence\" (\"A Scream in the Silence\"), she contrasted her close gay friends with homosexuals who \"jiggle their bottoms, put their little fingers in the air and with their little castrato voices moan about what those ghastly heteros put them through,\" and said some contemporary homosexuals behave like \"fairground freaks\". In her own defence, Bardot wrote in a letter to a French gay magazine: \"Apart from my husband\u2014who maybe will cross over one day as well\u2014I am entirely surrounded by homos. For years, they have been my support, my friends, my adopted children, my confidants.\"\nIn her book, she criticised racial mixing, immigration, the role of women in politics and Islam. The book also contained a section attacking what she called the mixing of genes, and praised previous generations which, she said, had given their lives to push out invaders. On 10 June 2004, Bardot was convicted for a fourth time by a French court for inciting racial hatred and fined \u20ac5,000. Bardot denied the racial hatred charge and apologized in court, saying: \"I never knowingly wanted to hurt anybody. It is not in my character.\" In 2008, Bardot was convicted of inciting racial/religious hatred in regard to a letter she wrote, a copy of which she sent to Nicolas Sarkozy when he was Interior Minister of France. The letter stated her objections to Muslims in France ritually slaughtering sheep by slitting their throats without anesthetizing them first. She also said, in reference to Muslims, that she was \"fed up with being under the thumb of this population which is destroying us, destroying our country and imposing its habits\". The trial concluded on 3 June 2008, with a conviction and fine of \u20ac15,000. The prosecutor stated she was weary of charging Bardot with offences related to racial hatred.\nDuring the 2008 United States presidential election, Bardot branded Republican Party vice-presidential candidate Sarah Palin as \"stupid\" and a \"disgrace to women\". She criticized the former Alaskan governor for her stance on global warming and gun control. She was further offended by Palin's support for Arctic oil exploration and by her lack of consideration in protecting polar bears.\nOn 13 August 2010, Bardot criticised American filmmaker Kyle Newman for his plan to produce a biographical film about her. She told him, \"Wait until I'm dead before you make a movie about my life!\" otherwise \"sparks will fly\".\nIn 2014, Bardot wrote an open letter demanding the ban in France of Jewish ritual slaughter shechita. In response, the European Jewish Congress released a statement saying \"Bardot has once again shown her clear insensitivity for minority groups with the substance and style of her letter...She may well be concerned for the welfare of animals but her longstanding support for the far-right and for discrimination against minorities in France shows a constant disdain for human rights instead.\"\nIn 2015, Bardot threatened to sue a Saint-Tropez boutique for selling items featuring her face. In 2018, she expressed support for the Yellow vests protests.\nOn 19 March 2019, Bardot issued an open letter to R\u00e9union prefect in which she accused inhabitants of the Indian Ocean island of animal cruelty and referred to them as \"autochthones who have kept the genes of savages\". In her letter relating to animal abuse and sent through her foundation, she mentioned the \"beheadings of goats and billy goats\" during festivals, and associated these practices with \"reminiscences of cannibalism from past centuries\". The public prosecutor filed a lawsuit the following day.\nIn June 2021, 86-year-old Bardot was fined \u20ac5,000 by the Arras court for public insults against hunters and their national president . She had published a post at the end of 2019 on her foundation's website, calling hunters \"sub-men\" and \"drunkards\" and carriers of \"genes of cruel barbarism inherited from our primitive ancestors\", and insulting Schraen. At the time of the hearing, she had not removed the comments from the website. Following her letter sent to the prefect of R\u00e9union in 2019, she was convicted on 4 November 2021 by a French court for public insults and fined \u20ac20,000, the largest of her fines to date.\nBardot's husband Bernard d'Ormale is a former adviser to Jean-Marie Le Pen, former leader of the far-right party National Front (which became National Rally), the main far-right party in France. Bardot expressed support for Marine Le Pen, leader of the National Front (National Rally), calling her \"the Joan of Arc of the 21st century\". She endorsed Le Pen in the 2012 and 2017 French presidential elections.\nBardot has been convicted of inciting racial hatred multiple times, having received six separate fines for the offense as of November 2021.\nLegacy.\n\"The Guardian\" named Bardot \"one of the most iconic faces, models, and actors of the 1950s and 1960s\". She has been called a \"style icon\" and a \"muse for Dior, Balmain, and Pierre Cardin\".\nIn fashion, the Bardot neckline (a wide-open neck that exposes both shoulders) is named after her. Bardot popularized this style, which is especially used for knitted sweaters or jumpers, although it is also used for other tops and dresses. Bardot popularized the bikini in her early films such as \"Manina\" (1952) (released in France as \"Manina, la fille sans voiles\"). The following year she was also photographed in a bikini on every beach in southern France during the Cannes Film Festival. She gained additional attention when she filmed \"...And God Created Woman\" (1956) with Jean-Louis Trintignant (released in France as \"Et Dieu Cr\u00e9a La Femme\"). In it Bardot portrays an immoral teenager cavorting in a bikini who seduces men in a respectable small-town setting. The film was an international success. Bardot's image was linked to the shoemaker Repetto, who created a pair of ballerinas for her in 1956. The bikini was in the 1950s relatively well accepted in France but was still considered risqu\u00e9 in the United States. As late as 1959, Anne Cole, one of the United States' largest swimsuit designers, said, \"It's nothing more than a G-string. It's at the razor's edge of decency.\"\nShe also brought into fashion the () hairstyle (a sort of beehive hair style) and gingham clothes after wearing a checkered pink dress, designed by Jacques Esterel, at her wedding to Charrier. French philosopher Simone de Beauvoir described Bardot as \"a locomotive of women's history\".\nIsabella Biedenharn of \"Elle\" wrote that Bardot \"has inspired thousands (millions?) of women to tease their hair or try out winged eyeliner over the past few decades\". A well-known evocative pose describes an iconic modelling portrait shot around 1960 where Bardot is dressed only in a pair of black pantyhose, cross-legged over her front and cross-armed over her breasts; known as the \"Bardot Pose\". This pose has been emulated numerous times by models and celebrities such as Lindsay Lohan, Elle Macpherson, Gisele B\u00fcndchen, and Rihanna. In the late 1960s, Bardot's silhouette was used as a model for designing and modelling the statue's bust of Marianne, a symbol of the French Republic.\nIn addition to popularizing the bikini swimming suit, Bardot has been credited with popularizing the city of St. Tropez and the town of Arma\u00e7\u00e3o dos B\u00fazios in Brazil, which she visited in 1964 with her boyfriend at the time, Brazilian musician Bob Zagury. The place where she stayed in B\u00fazios is today a small hotel, Pousada do Sol, and also a French restaurant, Cigalon. The town hosts a Bardot statue by Christina Motta.\nBardot was idolized by the young John Lennon and Paul McCartney. They made plans to shoot a film featuring The Beatles and Bardot, similar to \"A Hard Day's Night\", but the plans were never fulfilled. Lennon's first wife Cynthia Powell lightened her hair colour to more closely resemble Bardot, while George Harrison made comparisons between Bardot and his first wife Pattie Boyd, as Cynthia wrote later in \"A Twist of Lennon\". Lennon and Bardot met in person once, in 1968 at the May Fair Hotel, introduced by Beatles press agent Derek Taylor; a nervous Lennon took LSD before arriving, and neither star impressed the other. Lennon recalled in a memoir: \"I was on acid, and she was on her way out.\" According to the liner notes of his first (self-titled) album, musician Bob Dylan dedicated the first song he ever wrote to Bardot. He also mentioned her by name in \"I Shall Be Free\", which appeared on his second album, \"The Freewheelin' Bob Dylan\". The first-ever official exhibition spotlighting Bardot's influence and legacy opened in Boulogne-Billancourt on 29 September 2009 \u2013 a day after her 75th birthday. \nBardot was the subject of eight Andy Warhol paintings in 1974.\nThe Australian pop group Bardot was named after her.\nKylie Minogue adopted the Bardot \"sex kitten look\" on the cover of her album \"Body Language\", released in 2003.\nWomen who emulated and were inspired by Bardot include Claudia Schiffer, Emmanuelle B\u00e9art, Elke Sommer, Kate Moss, Faith Hill, Isabelle Adjani, Diane Kruger, Lara Stone, Minogue, Amy Winehouse, Georgia May Jagger, Zahia Dehar, Scarlett Johansson, Louise Bourgoin, and Paris Hilton. Bardot said: \"None have my personality.\" Laetitia Casta embodied Bardot in the 2010 French drama film \"\" by Joann Sfar.\nIn 2011, \"Los Angeles Times Magazine\"s list of \"50 Most Beautiful Women in Film\" ranked her number two.\nA portrait of Bardot by Warhol, commissioned by Sachs in 1974, was sold at Sotheby's in London on 22 and 23 May 2012. The painting, estimated at \u00a0million, was part of Sachs' art collection put on sale a year after his death.\nShe inspired Nicole Kidman, who had \"Bardot-esque\" hair in the 2013 British brand Jimmy Choo campaign.\nIn 2015, Bardot was ranked number six in \"The Top Ten Most Beautiful Women of All Time\", according to a survey carried out by Amway's beauty company in the UK involving 2,000 women.\nIn 2020, \"Vogue\" named Bardot number one of \"The most beautiful French actresses of all time\". In a retrospective retracing women throughout the history of cinema, she was listed among \"the most accomplished, talented and beautiful actresses of all time\" by \"Glamour\".\nThe French drama television series \"Bardot\" was broadcast on France 2 in 2023. It stars Julia de Nunez and is about Bardot's career from her first casting at age 15 and until the filming of \"La V\u00e9rit\u00e9\" ten years later. In 2023, she was mentioned in Olivia Rodrigo's song \"Lacy\" from her album \"Guts\", and Chappell Roan's \"Red Wine Supernova\" from her album \"The Rise and Fall of a Midwest Princess\".\nBooks.\nBardot has also written five books:\nReferences.\nOther sources"}
{"id": "3846", "revid": "48886350", "url": "https://en.wikipedia.org/wiki?curid=3846", "title": "Banjo", "text": "The banjo is a stringed instrument with a thin membrane stretched over a frame or cavity to form a resonator. The membrane is typically circular, in modern forms usually made of plastic, originally of animal skin. \nEarly forms of the instrument were fashioned by African Americans and had African antecedents. In the 19th century, interest in the instrument was spread across the United States and United Kingdom by traveling shows of the 19th-century minstrel show fad, followed by mass-production and mail-order sales, including instruction method books. The inexpensive or home-made banjo remained part of rural folk culture, but 5-string and 4-string banjos also became popular for home parlor music entertainment, college music clubs, and early 20th century jazz bands. By the early 21st century, the banjo was most frequently associated with folk, bluegrass and country music, but was also used in some rock, pop and even hip-hop music. Among rock bands, the Eagles, Led Zeppelin, and the Grateful Dead have used the five-string banjo in some of their songs. Some famous pickers of the banjo are Ralph Stanley and Earl Scruggs.\nHistorically, the banjo occupied a central place in Black American traditional music and rural folk culture before entering the mainstream via the minstrel shows of the 19th century. Along with the fiddle, the banjo is a mainstay of American styles of music, such as bluegrass and old-time music. It is also very frequently used in Dixieland jazz, as well as in Caribbean genres like biguine, calypso, mento and troubadour.\nHistory.\nEarly origins.\nThe modern banjo derives from instruments that have been recorded to be in use in North America and the Caribbean since the 17th\u00a0century by enslaved people taken from West and Central Africa. Their African-style instruments were crafted from split gourds with animal skins stretched across them. Strings, from gut or vegetable fibers, were attached to a wooden neck. Written references to the banjo in North America and the Caribbean appear in the 17th and 18th\u00a0centuries.\nThe earliest written indication of an instrument akin to the banjo is in the 17th\u00a0century: Richard Jobson (1621) in describing The Gambia, wrote about an instrument like the banjo, which he called a \"bandore\".\nThe term \"banjo\" has several etymological claims, one being from the Mandinka language which gives the name of Banjul, capital of The Gambia. Another claim is a connection to the West African \"akonting\": it is made with a long bamboo neck called a \"bangoe\". The material for the neck, called \"ban julo\" in the Mandinka language, again gives \"banjul\". In this interpretation, \"banjul\" became a sort of eponym for the akonting as it crossed the Atlantic. The instrument's name might also derive from the Kimbundu word \"mbanza\", which is a loan word to the Portuguese language resulting in the term \"banza\", which was used by early French travelers in the Americas. Its earliest recorded use was in 1678 by the Sovereign Council of Martinique which reinstated a 1654 decree that placed prohibitions and restrictions on \"dances and assemblies of negroes\" deemed to be \"kalenda\", which was defined as the gathering of enslaved Africans who danced to the sound of a drum and an instrument called the banza.\nThe OED claims that the term \"banjo\" comes from a dialectal pronunciation of Portuguese \"bandore\" or from an early anglicisation of Spanish \"bandurria\". Contrary evidence shows that the terms \"bandore\" and \"bandurria\" were used when Europeans encountered the instrument or its kin varieties in use by people of African descent, who used names for the instrument such as \"banza\", as it was called in places such as Haiti, varieties that were built around a gourd body with a wooden plank for the neck. Fran\u00e7ois Richard de Tussac, a former planter from Saint-Domingue, details its construction in the book \"Le Cri des Colons\", published in 1810, stating:\nAs for the guitars, which the negroes call \"banzas\", this is what they consist of: they cut lengthwise, through the middle, a fresh calabash [the fruit of a tree called the callebassier]. This fruit is sometimes eight inches or more in diameter. The stretch across it the skin of a goat, which they attach on the edges with little nails; they put two or three little holes on this surface, and then a kind of plank or piece of wood that is rudely flattened makes the neck of the instrument; they stretch three strings made of pitre [a kind of string taken from the agave plant, commonly known as pitre] across it; and so the instrument is built. On this instrument they play airs composed of three or four notes, which they repeat constantly.\nMichel \u00c9tienne Descourtilz, a naturalist who visited Haiti in the early 1800s, described it as \"banzas\", a Negro instrument, that the natives prepare by sawing one of the calabashes or a large gourd lengthwise, to which they attach a neck and sonorous strings made from the filament\" of aloe plants. It was played during any occasion, from boredom to joyous parties and calendas to funeral ceremonies. It was the custom to also combine this sound with the more noisy \"bamboula\", a type of drum made from a stick of bamboo covered on both sides with a skin that was played with fingers and knuckles while sitting astride.\nVarious instruments in Africa, chief among them the \"kora\", feature a skin drumhead and gourd (or similar shell) body. These instruments differ from early African-American banjos in that the necks do not possess a Western-style fingerboard and tuning pegs; instead they have stick necks, with strings attached to the neck with loops for tuning.\nAnother likely relative of the banjo is the aforementioned \"akonting\", a spike folk lute which is constructed using a gourd body, a long wooden neck, and three strings played by the Jola tribe of Senegambia, and the \"ubaw-akwala\" of the Igbo. Similar instruments include the \"xalam\" of Senegal and the \"ngoni\" of the Wassoulou region that includes parts of Mali, Guinea, and Ivory Coast, as well as a larger variation of the \"ngoni\", known as the \"gimbri\", developed in Morocco by sub-Saharan Africans (Gnawa or Haratin).\nBanjo-like instruments seem to have been independently invented in several different places, in addition to the many African instruments mentioned above, since instruments similar to the banjo are known from a diverse array of distant countries. For example, the Chinese \"sanxian\", the Japanese \"shamisen\", the Persian \"tar\", and the Moroccan \"sintir\".\nBanjos with fingerboards and tuning pegs are known from the Caribbean as early as the 17th\u00a0century. Some 18th- and early 19th-century writers transcribed the name of these instruments variously as \"bangie\", \"banza\", \"bonjaw\", \"banjer\" and \"banjar\".\nThe instrument became increasingly available commercially from around the second quarter of the 19th century due to minstrel show performances.\nMinstrel era, 1830s\u20131870s.\nIn the antebellum South, many enslaved Africans played the banjo, spreading it to the rest of the population. In his memoir \"With Sabre and Scalpel: The Autobiography of a Soldier and Surgeon\", the Confederate veteran and surgeon John Allan Wyeth recalls learning to play the banjo as a child from an enslaved person on his family plantation. Another man who learned to play from African-Americans, probably in the 1820s, was Joel Walker Sweeney, a minstrel performer from Appomattox Court House, Virginia. Sweeney has been credited with adding a string to the four-string African-American banjo, and popularizing the five-string banjo. Although Robert McAlpin Williamson is the first documented white banjoist, in the 1830s Sweeney became the first white performer to play the banjo on stage. Sweeney's musical performances occurred at the beginning of the minstrel era, as banjos shifted away from being exclusively homemade folk instruments to instruments of a more modern style. Sweeney participated in this transition by encouraging drum maker William Boucher of Baltimore to make banjos commercially for him to sell.\nAccording to Arthur Woodward in 1949, Sweeney replaced the gourd with a sound box made of wood and covered with skin, and added a short fifth string about 1831. However, modern scholar Gene Bluestein pointed out in 1964 that Sweeney may not have originated either the 5th string or sound box. This new banjo was at first tuned d'Gdf\u266fa, though by the 1890s, this had been transposed up to g'cgbd'. Banjos were introduced in Britain by Sweeney's group, the American Virginia Minstrels, in the 1840s, and became very popular in music halls.\nThe instrument grew in popularity during the 1840s after Sweeney began his traveling minstrel show. By the end of the 1840s the instrument had expanded from Caribbean possession to take root in places across America and across the Atlantic in England. It was estimated in 1866 that there were probably 10,000 banjos in New York City, up from only a handful in 1844. People were exposed to banjos not only at minstrel shows, but also medicine shows, Wild-West shows, variety shows, and traveling vaudeville shows. The banjo's popularity also was given a boost by the Civil War, as servicemen on both sides in the Army or Navy were exposed to the banjo played in minstrel shows and by other servicemen. A popular movement of aspiring banjoists began as early as 1861. The enthusiasm for the instrument was labeled a \"banjo craze\" or \"banjo mania.\"\nBy the 1850s, aspiring banjo players had options to help them learn their instrument. There were more teachers teaching banjo basics in the 1850s than there had been in the 1840s. There were also instruction manuals and, for those who could read it, printed music in the manuals. The first book of notated music was \"The Complete Preceptor\" by Elias Howe, published under the pseudonym \"Gumbo Chaff\", consisting mainly of Christy's Minstrels tunes. The first banjo method was the \"Briggs' Banjo instructor\" (1855) by Tom Briggs. Other methods included \"Howe's New American Banjo School\" (1857), and \"Phil Rice's Method for the Banjo, With or Without a Master\" (1858). These books taught the \"stroke style\" or \"banjo style\", similar to modern \"frailing\" or \"clawhammer\" styles.\nBy 1868, music for the banjo was available printed in a magazine, when J. K. Buckley wrote and arranged popular music for \"Buckley's Monthly Banjoist\". Frank B. Converse also published his entire collection of compositions in \"The Complete Banjoist\" in 1868, which included \"polkas, waltzes, marches, and clog hornpipes.\"\nOpportunities to work included the minstrel companies and circuses present in the 1840s, but also floating theaters and variety theaters, forerunners of the variety show and vaudeville.\nClassic era, 1880s\u20131910s.\nThe term \"classic banjo\" is used today to talk about a bare-finger \"guitar style\" that was widely in use among banjo players of the late 19th to early 20th century. It is still used by banjoists today. The term also differentiates that style of playing from the fingerpicking bluegrass banjo styles, such as the Scruggs style and Keith style.\nThe \"Briggs Banjo Method\", considered to be the first banjo method and which taught the \"stroke style\" of playing, also mentioned the existence of another way of playing, the \"guitar style.\" Alternatively known as \"finger style\", the new way of playing the banjo displaced the stroke method, until by 1870 it was the dominant style. Although mentioned by Briggs, it wasn't taught. The first banjo method to teach the technique was \"Frank B. Converse's New and Complete Method for the Banjo with or without a Master\", published in 1865.\nTo play in guitar style, players use the thumb and two or three fingers on their right hand to pick the notes. Samuel Swaim Stewart summarized the style in 1888, saying,\nThe banjo, although popular, carried low-class associations from its role in blackface minstrel shows, medicine shows, tent shows, and variety shows or vaudeville. There was a push in the 19th century to bring the instrument into \"respectability.\" Musicians such as William A. Huntley made an effort to \"elevate\" the instrument or make it more \"artistic,\" by \"bringing it to a more sophisticated level of technique and repertoire based on European standards.\" Huntley may have been the first white performer to successfully make the transition from performing in blackface to being himself on stage, noted by the Boston Herald in November 1884. He was supported by another former blackface performer, Samuel Swaim Stewart, in his corporate magazine that popularized highly talented professionals.\nAs the \"raucous\" imitations of plantation life decreased in minstrelsy, the banjo became more acceptable as an instrument of fashionable society, even to be accepted into women's parlors. Part of that change was a switch from the stroke style to the guitar playing style. An 1888 newspaper said, \"All the maidens and a good many of the women also strum the instrument, banjo classes abound on every side and banjo recitals are among the newest diversions of fashion...Youths and elderly men too have caught the fever...the star strummers among men are in demand at the smartest parties and have the choosing of the society of the most charming girls.\"\nSome of those entertainers, such as Alfred A. Farland, specialized in classical music. However, musicians who wanted to entertain their audiences, and make a living, mixed it in with the popular music that audiences wanted. Farland's pupil Frederick J. Bacon was one of these. A former medicine show entertainer, Bacon performed classical music along with popular songs such as \"Massa's in de cold, cold ground\", a \"Medley of Scotch Airs\", a \"Medley of Southern Airs\", and Thomas Glynn\u2019s \"West Lawn Polka\".\nBanjo innovation which began in the minstrel age continued, with increased use of metal parts, exotic wood, raised metal frets and a tone-ring that improved the sound. Instruments were designed in a variety of sizes and pitch ranges, to play different parts in banjo orchestras. Examples on display in the museum include banjorines and piccolo banjos.\nNew styles of playing, a new look, instruments in a variety of pitch ranges to take the place of different sections in an orchestra \u2013 all helped to separate the instrument from the rough minstrel image of the previous 50\u201360 years. The instrument was modern now, a bright new thing, with polished metal sides.\nRagtime era (1895\u20131919) and Jazz Age era (1910s\u20131930s).\nIn the early 1900s, new banjos began to spread, four-string models, played with a plectrum rather than with the minstrel-banjo clawhammer stroke or the classic-banjo fingerpicking style. The new banjos were a result of changing musical tastes. New music spurred the creation of \"evolutionary variations\" of the banjo, from the five-string model current since the 1830s to newer four-string plectrum and tenor banjos.\nThe instruments became ornately decorated in the 1920s to be visually dynamic to a theater audience. The instruments were increasingly modified or made in a new style \u2013 necks that were shortened to handle the four steel (not fiber as before) strings, strings that were sounded with a pick instead of fingers, four strings instead of five and tuned differently. The changes reflected the nature of post-World-War-I music. The country was turning away from European classics, preferring the \"upbeat and carefree feel\" of jazz, and American soldiers returning from the war helped to drive this change.\nThe change in tastes toward dance music and the need for louder instruments began a few years before the war, however, with ragtime. That music encouraged musicians to alter their 5-string banjos to four, add the louder steel strings and use a pick or plectrum, all in an effort to be heard over the brass and reed instruments that were current in dance-halls. The four string plectrum and tenor banjos did not eliminate the five-string variety. They were products of their times and musical purposes\u2014ragtime and jazz dance music and theater music.\nThe Great Depression is a visible line to mark the end of the Jazz Age. The economic downturn cut into the sales of both four- and five-stringed banjos, and by World War 2, banjos were in sharp decline, the market for them dead.\nModern era.\nIn the years after World War II, the banjo experienced a resurgence, played by music stars such as Earl Scruggs (bluegrass), Bela Fleck (jazz, rock, world music), Gerry O'Connor (Celtic and Irish music), Perry Bechtel (jazz, big band), Pete Seeger (folk), and Otis Taylor (African-American roots, blues, jazz).\nPete Seeger \"was a major force behind a new national interest in folk music.\" Learning to play a fingerstyle in the Appalachians from musicians who never stopped playing the banjo, he wrote the book, \"How to Play the Five-String Banjo\", which was the only banjo method on the market for years. He was followed by a movement of folk musicians, such as Dave Guard of The Kingston Trio and Erik Darling of the Weavers and Tarriers.\nEarl Scruggs was seen both as a legend and a \"contemporary musical innovator\" who gave his name to his style of playing, the \"Scruggs Style\". Scruggs played the banjo \"with heretofore unheard of speed and dexterity,\" using a picking technique for the 5-string banjo that he perfected from 2-finger and 3-finger picking techniques in rural North Carolina. His playing reached Americans through the Grand Ole Opry and into the living rooms of Americans who didn't listen to country or bluegrass music, through the theme music of \"The Beverly Hillbillies\" TV sitcom.\nFor the last one hundred years, the tenor banjo has become an intrinsic part of the world of Irish traditional music. It is a relative newcomer to the genre.\nThe banjo has also been used more recently in the hardcore punk scene, most notably by Show Me the Body on their debut album, \"Body War\".\nTechnique.\nTwo techniques closely associated with the five-string banjo are rolls and drones. Rolls are right hand accompanimental fingering patterns that consist of eight (eighth) notes that subdivide each measure. Drone notes are quick little notes [typically eighth notes], usually played on the 5th (short) string to fill in around the melody notes [typically eighth notes]. These techniques are both idiomatic to the banjo in all styles, and their sound is characteristic of bluegrass.\nHistorically, the banjo was played in the claw-hammer style by the Africans who brought their version of the banjo with them. Several other styles of play were developed from this. Clawhammer consists of downward striking of one or more of the four main strings with the index, middle or both fingers while the drone or fifth string is played with a 'lifting' (as opposed to downward pluck) motion of the thumb. The notes typically sounded by the thumb in this fashion are, usually, on the off beat. Melodies can be quite intricate adding techniques such as double thumbing and drop thumb. In old time Appalachian Mountain music, a style called two-finger up-pick is also used, and a three-finger version that Earl Scruggs developed into the \"Scruggs\" style picking was nationally aired in 1945 on the Grand Ole Opry. In this style the instrument is played by plucking individual notes. Modern fingerstyle is usually played using fingerpicks, though early players and some modern players play either with nails or with a technique known as on the flesh. In this style the strings are played directly with the fingers, rather than any pick or intermediary.\nWhile five-string banjos are traditionally played with either fingerpicks or the fingers themselves, tenor banjos and plectrum banjos are played with a pick, either to strum full chords, or most commonly in Irish traditional music, play single-note melodies.\nModern forms.\nThe modern banjo comes in a variety of forms, including four- and five-string versions. A six-string version, tuned and played similarly to a guitar, has gained popularity. In almost all of its forms, banjo playing is characterized by a fast arpeggiated plucking, though many different playing styles exist.\nThe body, or \"pot\", of a modern banjo typically consists of a circular rim (generally made of wood, though metal was also common on older banjos) and a tensioned head, similar to a drum head. Traditionally, the head was made from animal skin, but today is often made of various synthetic materials. Most modern banjos also have a metal \"tone ring\" assembly that helps further clarify and project the sound, but many older banjos do not include a tone ring.\nThe banjo is usually tuned with friction tuning pegs or planetary gear tuners, rather than the worm gear machine head used on guitars. Frets have become standard since the late 19th century, though fretless banjos are still manufactured and played by those wishing to execute glissando, play quarter tones, or otherwise achieve the sound and feeling of early playing styles.\nModern banjos are typically strung with metal strings. Usually, the fourth string is wound with either steel or bronze-phosphor alloy. Some players may string their banjos with nylon or gut strings to achieve a more mellow, old-time tone.\nSome banjos have a separate resonator plate on the back of the pot to project the sound forward and give the instrument more volume. This type of banjo is usually used in bluegrass music, though resonator banjos are played by players of all styles, and are also used in old-time, sometimes as a substitute for electric amplification when playing in large venues.\nOpen-back banjos generally have a mellower tone and weigh less than resonator banjos. They usually have a different setup than a resonator banjo, often with a higher string action.\nFive-string banjo.\nThe modern five-string banjo is a variation on Sweeney's original design. The fifth string is usually the same gauge as the first, but starts from the fifth fret, three-quarters the length of the other strings. This lets the string be tuned to a higher open pitch than possible for the full-length strings. Because of the short fifth string, the five-string banjo uses a reentrant tuning \u2013 the string pitches do not proceed lowest to highest across the fingerboard. Instead, the fourth string is lowest, then third, second, first, and the fifth string is highest.\nThe short fifth string presents special problems for a capo. For small changes (going up or down one or two semitones, for example), simply retuning the fifth string is possible. Otherwise, various devices called \"fifth-string capos\" effectively shorten the vibrating part of the string. Many banjo players use model-railroad spikes or titanium spikes (usually installed at the seventh fret and sometimes at others), under which they hook the string to press it down on the fret.\nFive-string banjo players use many tunings. (Tunings are given in left-to-right order, as viewed from the front of the instrument with the neck pointing up for a right-handed instrument. Left handed instruments reverse the order of the strings.) Probably the most common, particularly in bluegrass, is the Open-G tuning G4 D3 G3 B3 D4. In earlier times, the tuning G4 C3 G3 B3 D4 was commonly used instead, and this is still the preferred tuning for some types of folk music and for classic banjo. Other tunings found in old-time music include double C (G4 C3 G3 C4 D4), \"sawmill\" (G4 D3 G3 C4 D4) also called \"mountain modal\" and open D (F#4 D3 F#3 A3 D4). These tunings are often taken up a tone, either by tuning up or using a capo. For example, \"double-D\" tuning (A4 D3 A3 D4 E4) \u2013 commonly reached by tuning up from double C \u2013 is often played to accompany fiddle tunes in the key of D, and Open-A (A4 E3 A3 C#4 E4) is usually used for playing tunes in the key of A. Dozens of other banjo tunings are used, mostly in old-time music. These tunings are used to make playing specific tunes easier, usually fiddle tunes or groups of fiddle tunes.\nThe size of the five-string banjo is largely standardized, with a scale length of , but smaller and larger sizes exist, including the long-neck or \"Seeger neck\" variation designed by Pete Seeger. Petite variations on the five-string banjo have been available since the 1890s. S.S. Stewart introduced the banjeaurine, tuned one fourth above a standard five-string. Piccolo banjos are smaller, and tuned one octave above a standard banjo. Between these sizes and standard lies the A-scale banjo, which is two frets shorter and usually tuned one full step above standard tunings. Many makers have produced banjos of other scale lengths, and with various innovations.\nAmerican old-time music typically uses the five-string, open-back banjo. It is played in a number of different styles, the most common being clawhammer or frailing, characterized by the use of a downward rather than upward stroke when striking the strings with a fingernail. Frailing techniques use the thumb to catch the fifth string for a drone after most strums or after each stroke (\"double thumbing\"), or to pick out additional melody notes in what is known as drop-thumb. Pete Seeger popularized a folk style by combining clawhammer with up picking, usually without the use of fingerpicks. Another common style of old-time banjo playing is fingerpicking banjo or classic banjo. This style is based upon parlor-style guitar.\nBluegrass music, which uses the five-string resonator banjo almost exclusively, is played in several common styles. These include Scruggs style, named after Earl Scruggs; melodic, or Keith style, named for Bill Keith; and three-finger style with single-string work, also called Reno style after Don Reno. In these styles, the emphasis is on arpeggiated figures played in a continuous eighth-note rhythm, known as rolls. All of these styles are typically played with fingerpicks.\nThe first five-string, electric, solid-body banjo was developed by Charles Wilburn (Buck) Trent, Harold \"Shot\" Jackson, and David Jackson in 1960.\nThe five-string banjo has been used in classical music since before the turn of the 20th century. Contemporary and modern works have been written or arranged for the instrument by Don Vappie, Jerry Garcia, Buck Trent, B\u00e9la Fleck, Tony Trischka, Ralph Stanley, George Gibson, Steve Martin, Clifton Hicks, George Crumb, Tim Lake, Modest Mouse, Jo Kondo, Paul Elwood, Hans Werner Henze (notably in his \"Sixth Symphony\"), Daniel Mason, Beck, the Water Tower Bucket Boys, Todd Taylor, J.P. Pickens, Peggy Honeywell, Norfolk &amp; Western, Putnam Smith, Iron &amp; Wine, The Avett Brothers, The Well Pennies, Punch Brothers, Julian Koster, Sufjan Stevens, and Sarah Jarosz.\nGeorge Gershwin includes a banjo in his opera \"Porgy and Bess\"\nFrederick Delius wrote for a banjo in his opera \"Koanga\".\nErnst Krenek includes two banjos in his \"Kleine Symphonie\" (\"Little Symphony\").\nKurt Weill has a banjo in his opera \"The Rise and Fall of the City of Mahagonny\".\nViktor Ullmann included a tenor banjo part in his \"Piano Concerto\" (op. 25).\nVirgil Thomson includes a banjo in his orchestral music to accompany the film \"The Plow That Broke the Plains\" (1936).\nFour-string banjos.\nThe four-string plectrum banjo is a standard banjo without the short drone string. It usually has 22 frets on the neck and a scale length of 26 to 28\u00a0inches, and was originally tuned C3 G3 B3 D4. It can also be tuned like the top four strings of a guitar, which is known as \"Chicago tuning\". As the name suggests, it is usually played with a guitar-style pick (that is, a single one held between thumb and forefinger), unlike the five-string banjo, which is either played with a thumbpick and two fingerpicks, or with bare fingers. The plectrum banjo evolved out of the five-string banjo, to cater to styles of music involving strummed chords. The plectrum is also featured in many early jazz recordings and arrangements.\nFour-string banjos can be used for chordal accompaniment (as in early jazz), for single-string melody playing (as in Irish traditional music), in \"chord melody\" style (a succession of chords in which the highest notes carry the melody), in tremolo style (both on chords and single strings), and a mixed technique called duo style that combines single-string tremolo and rhythm chords.\nFour-string banjos are used from time to time in musical theater. Examples include: \"Hello, Dolly!\", \"Mame\", \"Chicago\", \"Cabaret\", \"Oklahoma!\", \"Half a Sixpence\", \"Annie\", \"Barnum\", \"The Threepenny Opera\", \"Monty Python's Spamalot\", and countless others. Joe Raposo had used it variably in the imaginative seven-piece orchestration for the long-running TV show \"Sesame Street\", and has sometimes had it overdubbed with itself or an electric guitar. The banjo is still (albeit rarely) in use in the show's arrangement currently.\nTenor banjo.\nThe shorter-necked, tenor banjo, with 17 (\"short scale\") or 19 frets, is also typically played with a plectrum. It became a popular instrument after about 1910. Early models used for melodic picking typically had 17 frets on the neck and a scale length of 19 to 21\u00a0inches. By the mid-1920s, when the instrument was used primarily for strummed chordal accompaniment, 19-fret necks with a scale length of 21 to 23\u00a0inches became standard. The usual tuning is the all-fifths tuning C3 G3 D4 A4, in which exactly seven semitones (a perfect fifth) occur between the open notes of consecutive strings; this is identical to the tuning of a viola. Other players (particularly in Irish traditional music) tune the banjo G2 D3 A3 E4 like an octave mandolin, which lets the banjoist duplicate fiddle and mandolin fingering. The popularization of this tuning is usually attributed to the late Barney McKenna, banjoist with The Dubliners.\nThe tenor banjo was a common rhythm instrument in early 20th-century dance bands. Its volume and timbre suited early jazz (and jazz-influenced popular music styles) and could both compete with other instruments (such as brass instruments and saxophones) and be heard clearly on acoustic recordings. George Gershwin's \"Rhapsody in Blue\", in Ferde Grofe's original jazz-orchestra arrangement, includes tenor banjo, with widely spaced chords not easily playable on plectrum banjo in its conventional tunings. With development of the archtop and electric guitar, the tenor banjo largely disappeared from jazz and popular music, though keeping its place in traditional \"Dixieland\" jazz.\nSome 1920s Irish banjo players picked out the melodies of jigs, reels, and hornpipes on tenor banjos, decorating the tunes with snappy triplet ornaments. The most important Irish banjo player of this era was Mike Flanagan of the New York-based Flanagan Brothers, one of the most popular Irish-American groups of the day. Other pre-WWII Irish banjo players included Neil Nolan, who recorded with Dan Sullivan's Shamrock Band in Boston, and Jimmy McDade, who recorded with the Four Provinces Orchestra in Philadelphia. Meanwhile, in Ireland, the rise of \"ceili\" bands provided a new market for a loud instrument like the tenor banjo. Use of the tenor banjo in Irish music has increased greatly since the folk revival of the 1960s.\nSix-string banjos.\nThe six-string banjo began as a British innovation by William Temlett, one of England's earliest banjo makers. He opened a shop in London in 1846, and sold seven-string banjos which he marketed as \"zither\" banjos from his 1869 patent. A zither banjo usually has a closed back and sides with the drum body and skin tensioning system suspended inside the wooden rim, the neck and string tailpiece mounted on the outside of the rim, and the drone string led through a tube in the neck so that the tuning peg can be mounted on the head. They were often made by builders who used guitar tuners that came in banks of three, so five-stringed instruments had a redundant tuner; these banjos could be somewhat easily converted over to a six-string banjo.\nAmerican Alfred Davis Cammeyer (1862\u20131949), a young violinist turned concert banjo player, devised the six-string zither banjo around 1880. British opera diva Adelina Patti advised Cammeyer that the zither banjo might be popular with English audiences as it had been invented there, and Cammeyer went to London in 1888. With his virtuoso playing, he helped show that banjos could make more sophisticated music than normally played by blackface minstrels. He was soon performing for London society, where he met Sir Arthur Sullivan, who recommended that Cammeyer progress from arranging the music of others for banjo to composing his own music.\nModern six-string bluegrass banjos have been made. These add a bass string between the lowest string and the drone string on a five-string banjo, and are usually tuned G4 G2 D3 G3 B3 D4. Sonny Osborne played one of these instruments for several years. It was modified by luthier Rual Yarbrough from a Vega five-string model. A picture of Sonny with this banjo appears in Pete Wernick's \"Bluegrass Banjo\" method book.\nSix-string banjos known as banjo guitars basically consist of a six-string guitar neck attached to a bluegrass or plectrum banjo body, which allows players who have learned the guitar to play a banjo sound without having to relearn fingerings. This was the instrument of the early jazz great Johnny St. Cyr, jazzmen Django Reinhardt, Danny Barker, Papa Charlie Jackson and Clancy Hayes, as well as the blues and gospel singer Reverend Gary Davis. Today, musicians as diverse as Keith Urban, Rod Stewart, Taj Mahal, Joe Satriani, David Hidalgo, Larry Lalonde and Doc Watson play the six-string guitar banjo. They have become increasingly popular since the mid-1990s.\nOther banjos.\nLow banjos.\nIn the late 19th and early 20th centuries, in vogue in plucked-string instrument ensembles \u2013 guitar orchestras, mandolin orchestras, banjo orchestras \u2013 was when the instrumentation was made to parallel that of the string section in symphony orchestras. Thus, \"violin, viola, 'cello, bass\" became \"mandolin, mandola, mandocello, mandobass\", or in the case of banjos, \"banjolin, banjola, banjo cello, bass banjo\". Because the range of pluck-stringed instrument generally is not as great as that of comparably sized bowed-string instruments, other instruments were often added to these plucked orchestras to extend the range of the ensemble upwards and downwards.\nThe banjo cello was normally tuned C2-G2-D3-A3, one octave below the tenor banjo like the cello and mandocello. A five-string cello banjo, set up like a bluegrass banjo (with the short fifth string), but tuned one octave lower, has been produced by the Goldtone company.\nBass banjos have been produced in both upright bass formats and with standard, horizontally carried banjo bodies. Contrabass banjos with either three or four strings have also been made; some of these had headstocks similar to those of bass violins. Tuning varies on these large instruments, with four-string models sometimes being tuned in 4ths like a bass violin (E1-A1-D2-G2) and sometimes in 5ths, like a four-string cello banjo, one octave lower (C1-G1-D2-A2).\nLong neck banjos.\nAlso called Seeger banjos for having been invented by Pete Seeger, these banjos feature three extra frets, giving the instrument a longer neck and greater playing versatility. With three extra frets, these banjos can be played one-and-a-half steps lower than a regular banjo, which some players find advantageous for singing or playing along. They are almost always open-backed. Notably, the drone strings on Seeger banjos are not pushed three frets back, so the tuning peg for the 5th string is in line with the 8th fret instead of the 5th fret. \nBanjo hybrids and variants.\nA number of hybrid instruments exist, crossing the banjo with other stringed instruments. Most of these use the body of a banjo, often with a resonator, and the neck of the other instrument. Examples include the banjo mandolin (first patented in 1882) and the banjo ukulele, most famously played by the English comedian George Formby. These were especially popular in the early decades of the 20th century, and were probably a result of a desire either to allow players of other instruments to jump on the banjo bandwagon at the height of its popularity, or to get the natural amplification benefits of the banjo resonator in an age before electric amplification.\nConversely, the tenor and plectrum guitars use the respective banjo necks on guitar bodies. They arose in the early 20th century as a way for banjo players to double on guitar without having to relearn the instrument entirely.\nInstruments that have a five-string banjo neck on a wooden body (for example, a guitar, bouzouki, or dobro body) have also been made, such as the banjola. A 20th-century Turkish instrument similar to the banjo is called the \"c\u00fcmb\u00fc\u015f\", which combines a banjo-like resonator with a neck derived from an oud. At the end of the 20th century, a development of the five-string banjo was the BanSitar. This features a bone bridge, giving the instrument a sitar-like resonance.\nThe Brazilian samba banjo is basically a cavaquinho neck on a banjo body, thereby producing a louder sound than the cavaquinho. It is tuned the same as the top 4 strings of a 5-string banjo up an octave (or any cavaquinho tuning)."}
{"id": "3847", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=3847", "title": "Basic taste", "text": ""}
{"id": "3850", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=3850", "title": "Baseball", "text": "Baseball is a bat-and-ball sport played between two teams of nine players each, taking turns batting and fielding. The game occurs over the course of several plays, with each play generally beginning when a player on the fielding team, called the pitcher, throws a ball that a player on the batting team, called the batter, tries to hit with a bat. The objective of the offensive team (batting team) is to hit the ball into the field of play, away from the other team's players, allowing its players to run the bases, having them advance counter-clockwise around four bases to score what are called \"runs\". The objective of the defensive team (referred to as the fielding team) is to prevent batters from becoming runners, and to prevent runners advancing around the bases. A run is scored when a runner legally advances around the bases in order and touches home plate (the place where the player started as a batter).\nThe initial objective of the batting team is to have a player reach first base safely; this generally occurs either when the batter hits the ball and reaches first base before an opponent retrieves the ball and touches the base, or when the pitcher persists in throwing the ball out of the batter's reach. Players on the batting team who reach first base without being called \"out\" can attempt to advance to subsequent bases as a runner, either immediately or during teammates' turns batting. The fielding team tries to prevent runs by using the ball to get batters or runners \"out\", which forces them out of the field of play. The pitcher can get the batter out by throwing three pitches which result in strikes, while fielders can get the batter out by catching a batted ball before it touches the ground, and can get a runner out by tagging them with the ball while the runner is not touching a base.\nThe opposing teams switch back and forth between batting and fielding; the batting team's turn to bat is over once the fielding team records three outs. One turn batting for each team constitutes an inning. A game is usually composed of nine innings, and the team with the greater number of runs at the end of the game wins. Most games end after the ninth inning, but if scores are tied at that point, extra innings are usually played. Baseball has no game clock, though some competitions feature pace-of-play regulations such as the pitch clock to shorten game time.\nBaseball evolved from older bat-and-ball games already being played in England by the mid-18th century. This game was brought by immigrants to North America, where the modern version developed. Baseball's American origins, as well as its reputation as a source of escapism during troubled points in American history such as the American Civil War and the Great Depression, have led the sport to receive the moniker of \"America's Pastime\"; since the late 19th century, it has been unofficially recognized as the national sport of the United States, though in modern times is considered less popular than other sports, such as American football. In addition to North America, baseball spread throughout the rest of the Americas and the Asia\u2013Pacific in the 19th and 20th centuries, and is now considered the most popular sport in parts of Central and South America, the Caribbean, and East Asia, particularly in Japan, South Korea, and Taiwan.\nIn Major League Baseball (MLB), the highest level of professional baseball in the United States and Canada, teams are divided into the National League (NL) and American League (AL), each with three divisions: East, West, and Central. The MLB champion is determined by playoffs that culminate in the World Series. The top level of play is similarly split in Japan between the Central and Pacific Leagues and in Cuba between the West League and East League. The World Baseball Classic, organized by the World Baseball Softball Confederation, is the major international competition of the sport and attracts the top national teams from around the world. Baseball was played at the Olympic Games from 1992 to 2008, and was reinstated on a one-off basis in 2020.\nRules and gameplay.\nOverview.\nA baseball game is played between two teams, each usually composed of nine players, that take turns playing offense (batting and baserunning) and defense (pitching and fielding). A pair of turns, one at bat and one in the field, by each team constitutes an inning. A game consists of nine innings (seven innings at the high school level and in doubleheaders in college, Minor League Baseball and, since the 2020 season, Major League Baseball; and six innings at the Little League level). One team\u2014customarily the visiting team\u2014bats in the top, or first half, of every inning. The other team\u2014customarily the home team\u2014bats in the bottom, or second half, of every inning. \nThe goal of the game is to score more points (runs) than the other team. The players on the team at bat attempt to score runs by touching all four bases, in order, set at the corners of the square-shaped baseball diamond. A player bats at home plate and must attempt to safely reach a base before proceeding, counterclockwise, from first base, to second base, third base, and back home to score a run. The team in the field attempts to prevent runs from scoring by recording outs, which remove opposing players from offensive action until their next turn at bat comes up again. When three outs are recorded, the teams switch roles for the next half-inning. If the score of the game is tied after nine innings, extra innings are played to resolve the contest. Many amateur games, particularly unorganized ones, involve different numbers of players and innings.\nThe game is played on a field whose primary boundaries, the foul lines, extend forward from home plate at 45-degree angles. The 90-degree area within the foul lines is referred to as fair territory; the 270-degree area outside them is foul territory. The part of the field enclosed by the bases and several yards beyond them is the infield; the area farther beyond the infield is the outfield. In the middle of the infield is a raised pitcher's mound, with a rectangular rubber plate (the rubber) at its center. The outer boundary of the outfield is typically demarcated by a raised fence, which may be of any material and height. The fair territory between home plate and the outfield boundary is baseball's field of play, though significant events can take place in foul territory, as well.\nThere are three basic tools of baseball: the ball, the bat, and the glove or mitt:\nProtective helmets are also standard equipment for all batters.\nFielding positions.\nAt the beginning of each half-inning, the nine players of the fielding team arrange themselves around the field. One of them, the pitcher, stands on the pitcher's mound. The pitcher begins the pitching delivery with one foot on the rubber, pushing off it to gain velocity when throwing toward home plate. Another fielding team player, the catcher, squats on the far side of home plate, facing the pitcher. The rest of the fielding team faces home plate, typically arranged as four infielders\u2014who set up along or within a few yards outside the imaginary lines (basepaths) between first, second, and third base\u2014and three outfielders. In the standard arrangement, there is a first baseman positioned several steps to the left of first base, a second baseman to the right of second base, a shortstop to the left of second base, and a third baseman to the right of third base. The basic outfield positions are left fielder, center fielder, and right fielder. With the exception of the catcher, all fielders are required to be in fair territory when the pitch is delivered. A neutral umpire sets up behind the catcher. Other umpires will be distributed around the field as well.\nOffense.\nPlay starts with a member of the batting team, the batter, standing in either of the two batter's boxes next to home plate, holding a bat. The batter waits for the pitcher to throw a pitch (the ball) toward home plate, and attempts to hit the ball with the bat. The catcher catches pitches that the batter does not hit\u2014as a result of either electing not to swing or failing to connect\u2014and returns them to the pitcher. A batter who hits the ball into the field of play must drop the bat and begin running toward first base, at which point the player is referred to as a \"runner\" (or, until the play is over, a \"batter-runner\"). \nA batter-runner who reaches first base without being put out is said to be \"safe\" and is on base. A batter-runner may choose to remain at first base or attempt to advance to second base or even beyond\u2014however far the player believes can be reached safely. A player who reaches base despite proper play by the fielders has recorded a hit. A player who reaches first base safely on a hit is credited with a single. If a player makes it to second base safely as a direct result of a hit, it is a double; third base, a triple. If the ball is hit in the air within the foul lines over the entire outfield (and outfield fence, if there is one), or if the batter-runner otherwise safely circles all the bases, it is a home run: the batter and any runners on base may all freely circle the bases, each scoring a run. This is the most desirable result for the batter. The ultimate and most desirable result possible for a batter would be to hit a home run while all three bases are occupied or \"loaded\", thus scoring four runs on a single hit. This is called a grand slam. A player who reaches base due to a fielding mistake is not credited with a hit\u2014instead, the responsible fielder is charged with an error.\nAny runners already on base may attempt to advance on batted balls that land, or contact the ground, in fair territory, before or after the ball lands. A runner on first base \"must\" attempt to advance if a ball lands in play, as only one runner may occupy a base at any given time; the same applies for other runners if they are on a base that a teammate is forced to advance to. If a ball hit into play rolls foul before passing through the infield, it becomes dead and any runners must return to the base they occupied when the play began. If the ball is hit in the air and caught before it lands, the batter has flied out and any runners on base may attempt to advance only if they tag up (contact the base they occupied when the play began, as or after the ball is caught). Runners may also attempt to advance to the next base while the pitcher is in the process of delivering the ball to home plate; a successful effort is a stolen base.\nDefense.\nA pitch that is not hit into the field of play is called either a strike or a ball. A batter against whom three strikes are recorded strikes out. A batter against whom four balls are recorded is awarded a base on balls or walk, a free advance to first base. (A batter may also freely advance to first base if the batter's body or uniform is struck by a pitch outside the strike zone, provided the batter does not swing and attempts to avoid being hit.) Crucial to determining balls and strikes is the umpire's judgment as to whether a pitch has passed through the strike zone, a conceptual area above home plate extending from the midpoint between the batter's shoulders and belt down to the hollow of the knee. Any pitch which does not pass through the strike zone is called a ball, unless the batter either swings and misses at the pitch, or hits the pitch into foul territory; an exception generally occurs if the ball is hit into foul territory when the batter already has two strikes, in which case neither a ball nor a strike is called.\nWhile the team at bat is trying to score runs, the team in the field is attempting to record outs. In addition to the strikeout and flyout, common ways a member of the batting team may be put out include the ground out, force out, and tag out. These occur either when a runner is forced to advance to a base, and a fielder with possession of the ball reaches that base before the runner does, or the runner is touched by the ball, held in a fielder's hand, while not on a base. (The batter-runner is always forced to advance to first base, and any other runners must advance to the next base if a teammate is forced to advance to their base.) It is possible to record two outs in the course of the same play. This is called a double play. Three outs in one play, a triple play, is possible, though rare. Players put out or retired must leave the field, returning to their team's dugout or bench. A runner may be stranded on base when a third out is recorded against another player on the team. Stranded runners do not benefit the team in its next turn at bat as every half-inning begins with the bases empty.\nBatting order and substitution.\nAn individual player's turn batting or plate appearance is complete when the player reaches base, hits a home run, makes an out, or hits a ball that results in the team's third out, even if it is recorded against a teammate. On rare occasions, a batter may be at the plate when, without the batter's hitting the ball, a third out is recorded against a teammate\u2014for instance, a runner getting caught stealing (tagged out attempting to steal a base). A batter with this sort of incomplete plate appearance starts off the team's next turn batting; any balls or strikes recorded against the batter the previous inning are erased.\nA runner may circle the bases only once per plate appearance and thus can score at most a single run per batting turn. Once a player has completed a plate appearance, that player may not bat again until the eight other members of the player's team have all taken their turn at bat in the batting order. The batting order is set before the game begins, and may not be altered except for substitutions. Once a player has been removed for a substitute, that player may not reenter the game. Children's games often have more lenient rules, such as Little League rules, which allow players to be substituted back into the same game.\nIf the designated hitter (DH) rule is in effect, each team has a tenth player whose sole responsibility is to bat (and run). The DH takes the place of another player\u2014almost invariably the pitcher\u2014in the batting order, but does not field. Thus, even with the DH, each team still has a batting order of nine players and a fielding arrangement of nine players.\nPersonnel.\nPlayers.\nThe number of players on a baseball roster, or \"squad\", varies by league and by the level of organized play. A Major League Baseball (MLB) team has a roster of 26 players with specific roles. A typical roster features the following players:\nMost baseball leagues worldwide have the DH rule, including MLB, Japan's Pacific League, and Caribbean professional leagues, along with major American amateur organizations. The Central League in Japan does not have the rule and high-level minor league clubs connected to National League teams are not required to field a DH. In leagues that apply the designated hitter rule, a typical team has nine offensive regulars (including the DH), five starting pitchers, seven or eight relievers, a backup catcher, and two or three other reserve players.\nManagers and coaches.\nThe manager, or head coach, oversees the team's major strategic decisions, such as establishing the starting rotation, setting the lineup, or batting order, before each game, and making substitutions during games\u2014in particular, bringing in relief pitchers. Managers are typically assisted by two or more coaches; they may have specialized responsibilities, such as working with players on hitting, fielding, pitching, or strength and conditioning. At most levels of organized play, two coaches are stationed on the field when the team is at bat: the first base coach and third base coach, who occupy designated coaches' boxes, just outside the foul lines. These coaches assist in the direction of baserunners, when the ball is in play, and relay tactical signals from the manager to batters and runners, during pauses in play. In contrast to many other team sports, baseball managers and coaches generally wear their team's uniforms; coaches must be in uniform to be allowed on the field to confer with players during a game.\nUmpires.\nAny baseball game involves one or more umpires, who make rulings on the outcome of each play. At a minimum, one umpire will stand behind the catcher, to have a good view of the strike zone, and call balls and strikes. Additional umpires may be stationed near the other bases, thus making it easier to judge plays such as attempted force outs and tag outs. In MLB, four umpires are used for each game, one near each base. In the playoffs, six umpires are used: one at each base and two in the outfield along the foul lines.\nStrategy.\nMany of the pre-game and in-game strategic decisions in baseball revolve around a fundamental fact: in general, right-handed batters tend to be more successful against left-handed pitchers and, to an even greater degree, left-handed batters tend to be more successful against right-handed pitchers. A manager with several left-handed batters in the regular lineup, who knows the team will be facing a left-handed starting pitcher, may respond by starting one or more of the right-handed backups on the team's roster. During the late innings of a game, as relief pitchers and pinch hitters are brought in, the opposing managers will often go back and forth trying to create favorable matchups with their substitutions. The manager of the fielding team trying to arrange same-handed pitcher-batter matchups and the manager of the batting team trying to arrange opposite-handed matchups. With a team that has the lead in the late innings, a manager may remove a starting position player\u2014especially one whose turn at bat is not likely to come up again\u2014for a more skillful fielder (known as a defensive substitution).\nTactics.\nPitching and fielding.\nThe tactical decision that precedes almost every play in a baseball game involves pitch selection. By gripping and then releasing the baseball in a certain manner, and by throwing it at a certain speed, pitchers can cause the baseball to break to either side, or downward, as it approaches the batter, thus creating differing pitches that can be selected. Among the resulting wide variety of pitches that may be thrown, the four basic types are the fastball, the changeup (or off-speed pitch), and two breaking balls\u2014the curveball and the slider. Pitchers have different repertoires of pitches they are skillful at throwing. Conventionally, before each pitch, the catcher signals the pitcher what type of pitch to throw, as well as its general vertical or horizontal location. If there is disagreement on the selection, the pitcher may shake off the sign and the catcher will call for a different pitch.\nWith a runner on base and taking a lead, the pitcher may attempt a pickoff, a quick throw to a fielder covering the base to keep the runner's lead in check or, optimally, effect a tag out. Pickoff attempts, however, are subject to rules that severely restrict the pitcher's movements before and during the pickoff attempt. Violation of any one of these rules could result in the umpire calling a balk against the pitcher, which permits any runners on base to advance one base with impunity. If an attempted stolen base is anticipated, the catcher may call for a pitchout, a ball thrown deliberately off the plate, allowing the catcher to catch it while standing and throw quickly to a base. Facing a batter with a strong tendency to hit to one side of the field, the fielding team may employ a shift, with most or all of the fielders moving to the left or right of their usual positions. With a runner on third base, the infielders may play in, moving closer to home plate to improve the odds of throwing out the runner on a ground ball, though a sharply hit grounder is more likely to carry through a drawn-in infield.\nBatting and baserunning.\nSeveral basic offensive tactics come into play with a runner on first base, including the fundamental choice of whether to attempt a steal of second base. The hit and run is sometimes employed, with a skillful contact hitter, the runner takes off with the pitch, drawing the shortstop or second baseman over to second base, creating a gap in the infield for the batter to poke the ball through. The sacrifice bunt, calls for the batter to focus on making soft contact with the ball, so that it rolls a short distance into the infield, allowing the runner to advance into scoring position as the batter is thrown out at first. A batter, particularly one who is a fast runner, may also attempt to bunt for a hit. A sacrifice bunt employed with a runner on third base, aimed at bringing that runner home, is known as a squeeze play. With a runner on third and fewer than two outs, a batter may instead concentrate on hitting a fly ball that, even if it is caught, will be deep enough to allow the runner to tag up and score\u2014a successful batter, in this case, gets credit for a sacrifice fly. In order to increase the chance of advancing a batter to first base via a walk, the manager will sometimes signal a batter who is ahead in the count (i.e., has more balls than strikes) to take, or not swing at, the next pitch. The batter's potential reward of reaching base (via a walk) exceeds the disadvantage if the next pitch is a strike.\nHistory.\nThe evolution of baseball from older bat-and-ball games is difficult to trace with precision. Consensus once held that today's baseball is a North American development from the older game rounders, popular among children in Great Britain and Ireland. American baseball historian David Block suggests that the game originated in England; recently uncovered historical evidence supports this position. According to Block and John Thorn, official MLB historian, this earlier version of baseball may have involved hitting the ball with a hand, making it akin to today's punchball. Block argues that rounders and early baseball were actually regional variants of each other, and that the game's most direct antecedents are the English games of stoolball and \"tut-ball\". The earliest known reference to baseball is in a 1744 British publication, \"A Little Pretty Pocket-Book\", by John Newbery. Block discovered that the first recorded game of \"Bass-Ball\" took place in 1749 in Surrey, and featured the Prince of Wales as a player. This early form of the game was apparently brought to Canada by English immigrants.\nBy the early 1830s, there were reports of a variety of uncodified bat-and-ball games recognizable as early forms of baseball being played around North America. The first officially recorded baseball game in North America was played in Beachville, Ontario, Canada, on June 4, 1838. In 1845, Alexander Cartwright, a member of New York City's Knickerbocker Club, led the codification of the so-called Knickerbocker Rules, which in turn were based on rules developed in 1837 by William R. Wheaton of the Gotham Club. While there are reports that the New York Knickerbockers played games in 1845, the contest long recognized as the first officially recorded baseball game in U.S. history took place on June 19, 1846, in Hoboken, New Jersey: the \"New York Nine\" defeated the Knickerbockers, 23\u20131, in four innings. With the Knickerbocker code as the basis, the rules of modern baseball continued to evolve over the next half-century. The game then went on to spread throughout the Pacific Rim and the Americas, with Americans backing the sport as a way to spread American values.\nIn the United States.\nEstablishment of professional leagues.\nIn the mid-1850s, a baseball craze hit the New York metropolitan area, and by 1856, local journals were referring to baseball as the \"national pastime\" or \"national game\". A year later, the sport's first governing body, the National Association of Base Ball Players, was formed. In 1867, it barred participation by African Americans. The more formally structured National League was founded in 1876. Professional Negro leagues formed, but quickly folded. In 1887, softball, under the name of indoor baseball or indoor-outdoor, was invented as a winter version of the parent game. The National League's first successful counterpart, the American League, which evolved from the minor Western League, was established in 1893, and virtually all of the modern baseball rules were in place by then.\nThe National Agreement of 1903 formalized relations both between the two major leagues and between them and the National Association of Professional Base Ball Leagues, representing most of the country's minor professional leagues. The World Series, pitting the two major league champions against each other, was inaugurated that fall. The Black Sox Scandal of the 1919 World Series led to the formation of the office of the Commissioner of Baseball. The first commissioner, Kenesaw Mountain Landis, was elected in 1920. That year also saw the founding of the Negro National League; the first significant Negro league, it would operate until 1931. For part of the 1920s, it was joined by the Eastern Colored League.\nRise of Ruth and racial integration.\nCompared with the present, professional baseball in the early 20th century was lower-scoring, and pitchers were more dominant. This so-called \"dead-ball era\" ended in the early 1920s with several changes in rule and circumstance that were advantageous to hitters. Strict new regulations governed the ball's size, shape and composition, along with a new rule officially banning the spitball and other pitches that depended on the ball being treated or roughed-up with foreign substances, resulted in a ball that traveled farther when hit. The rise of the legendary player Babe Ruth, the first great power hitter of the new era, helped permanently alter the nature of the game. In the late 1920s and early 1930s, St. Louis Cardinals general manager Branch Rickey invested in several minor league clubs and developed the first modern farm system. A new Negro National League was organized in 1933; four years later, it was joined by the Negro American League. The first elections to the National Baseball Hall of Fame took place in 1936. In 1939, Little League Baseball was founded in Pennsylvania.\nMany minor league teams disbanded when World War II led to a player shortage. Chicago Cubs owner Philip K. Wrigley led the formation of the All-American Girls Professional Baseball League to help keep the game in the public eye. The first crack in the unwritten agreement barring blacks from white-controlled professional ball occurred in 1945: Jackie Robinson was signed by the National League's Brooklyn Dodgers and began playing for their minor league team in Montreal. In 1947, Robinson broke the major leagues' color barrier when he debuted with the Dodgers. Latin-American players, largely overlooked before, also started entering the majors in greater numbers. In 1951, two Chicago White Sox, Venezuelan-born Chico Carrasquel and black Cuban-born Minnie Mi\u00f1oso, became the first Hispanic All-Stars. Integration proceeded slowly: by 1953, only six of the 16 major league teams had a black player on the roster.\nAttendance records and the age of steroids.\nIn 1975, the union's power\u2014and players' salaries\u2014began to increase greatly when the reserve clause was effectively struck down, leading to the free agency system. Significant work stoppages occurred in 1981 and 1994, the latter forcing the cancellation of the World Series for the first time in 90 years. Attendance had been growing steadily since the mid-1970s and in 1994, before the stoppage, the majors were setting their all-time record for per-game attendance. After play resumed in 1995, non-division-winning wild card teams became a permanent fixture of the post-season. Regular-season interleague play was introduced in 1997 and the second-highest attendance mark for a full season was set. In 2000, the National and American Leagues were dissolved as legal entities. While their identities were maintained for scheduling purposes (and the designated hitter distinction), the regulations and other functions\u2014such as player discipline and umpire supervision\u2014they had administered separately were consolidated under the rubric of MLB.\nIn 2001, Barry Bonds established the current record of 73 home runs in a single season. There had long been suspicions that the dramatic increase in power hitting was fueled in large part by the abuse of illegal steroids (as well as by the dilution of pitching talent due to expansion), but the issue only began attracting significant media attention in 2002 and there was no penalty for the use of performance-enhancing drugs before 2004. In 2007, Bonds became MLB's all-time home run leader, surpassing Hank Aaron, as total major league and minor league attendance both reached all-time highs.\nAround the world.\nDespite having been called \"America's national pastime\", baseball is well-established in several other countries. As early as 1877, a professional league, the International Association, featured teams from both Canada and the United States. While baseball is widely played in Canada and many minor league teams have been based in the country, the American major leagues did not include a Canadian club until 1969, when the Montreal Expos joined the National League as an expansion team. In 1977, the expansion Toronto Blue Jays joined the American League.\nIn 1847, American soldiers played what may have been the first baseball game in Mexico at Parque Los Berros in Xalapa, Veracruz. The first formal baseball league outside of the United States and Canada was founded in 1878 in Cuba, which maintains a rich baseball tradition. The Dominican Republic held its first islandwide championship tournament in 1912. Professional baseball tournaments and leagues began to form in other countries between the world wars, including the Netherlands (formed in 1922), Australia (1934), Japan (1936), Mexico (1937), and Puerto Rico (1938). The Japanese major leagues have long been considered the highest quality professional circuits outside of the United States.\nAfter World War II, professional leagues were founded in many Latin American countries, most prominently Venezuela (1946) and the Dominican Republic (1955). Since the early 1970s, the annual Caribbean Series has matched the championship clubs from the four leading Latin American winter leagues: the Dominican Professional Baseball League, Mexican Pacific League, Puerto Rican Professional Baseball League, and Venezuelan Professional Baseball League. In Asia, South Korea (1982), Taiwan (1990) and China (2003) all have professional leagues.\nThe English football club, Aston Villa, were the first British baseball champions winning the 1890 National League of Baseball of Great Britain. The 2020 National Champions were the London Mets. Other European countries have seen professional leagues; the most successful, other than the Dutch league, is the Italian league, founded in 1948. In 2004, Australia won a surprise silver medal at the Olympic Games. The Conf\u00e9d\u00e9ration Europ\u00e9ene de Baseball (European Baseball Confederation), founded in 1953, organizes a number of competitions between clubs from different countries. Other competitions between national teams, such as the Baseball World Cup and the Olympic baseball tournament, were administered by the International Baseball Federation (IBAF) from its formation in 1938 until its 2013 merger with the International Softball Federation to create the current joint governing body for both sports, the World Baseball Softball Confederation (WBSC). Women's baseball is played on an organized amateur basis in numerous countries.\nAfter being admitted to the Olympics as a medal sport beginning with the 1992 Games, baseball was dropped from the 2012 Summer Olympic Games at the 2005 International Olympic Committee meeting. It remained part of the 2008 Games. While the sport's lack of a following in much of the world was a factor, more important was MLB's reluctance to allow its players to participate during the major league season. MLB initiated the World Baseball Classic, scheduled to precede its season, partly as a replacement, high-profile international tournament. The inaugural Classic, held in March 2006, was the first tournament involving national teams to feature a significant number of MLB participants. The Baseball World Cup was discontinued after its 2011 edition in favor of an expanded World Baseball Classic.\nDistinctive elements.\nBaseball has certain attributes that set it apart from the other popular team sports in the countries where it has a following. All of these sports use a clock, play is less individual, and the variation between playing fields is not as substantial or important. The comparison between cricket and baseball demonstrates that many of baseball's distinctive elements are shared in various ways with its cousin sports.\nNo clock to kill.\nIn clock-limited sports, games often end with a team that holds the lead killing the clock rather than competing aggressively against the opposing team. In contrast, baseball has no clock, thus a team cannot win without getting the last batter out and rallies are not constrained by time. At almost any turn in any baseball game, the most advantageous strategy is some form of aggressive strategy. Whereas, in the case of multi-day Test and first-class cricket, the possibility of a draw (which occurs because of the restrictions on time, which like in baseball, originally did not exist) often encourages a team that is batting last and well behind, to bat defensively and run out the clock, giving up any faint chance at a win, to avoid an overall loss.\nWhile nine innings has been the standard since the beginning of professional baseball, the duration of the average major league game has increased steadily through the years. At the turn of the 20th century, games typically took an hour and a half to play. In the 1920s, they averaged just less than two hours, which eventually ballooned to 2:38 in 1960. By 1997, the average American League game lasted 2:57 (National League games were about 10 minutes shorter\u2014pitchers at the plate making for quicker outs than designated hitters). In 2004, Major League Baseball declared that its goal was an average game of 2:45. By 2014, though, the average MLB game took over three hours to complete. The lengthening of games is attributed to longer breaks between half-innings for television commercials, increased offense, more pitching changes, and a slower pace of play, with pitchers taking more time between each delivery, and batters stepping out of the box more frequently. Other leagues have experienced similar issues. In 2008, Nippon Professional Baseball took steps aimed at shortening games by 12 minutes from the preceding decade's average of 3:18.\nIn 2016, the average nine-inning playoff game in Major League baseball was 3 hours and 35 minutes. This was up 10 minutes from 2015 and 21 minutes from 2014. In response to the lengthening of the game, MLB decided from the 2023 season onward to institute a pitch clock rule to penalize batters and pitchers who take too much time between pitches; this had the effect of shortening 2023 regular season games by 24 minutes on average.\nIndividual focus.\nAlthough baseball is a team sport, individual players are often placed under scrutiny and pressure. While rewarding, it has sometimes been described as \"ruthless\" due to the pressure on the individual player. In 1915, a baseball instructional manual pointed out that every single pitch, of which there are often more than two hundred in a game, involves an individual, one-on-one contest: \"the pitcher and the batter in a battle of wits\". Pitcher, batter, and fielder all act essentially independent of each other. While coaching staffs can signal pitcher or batter to pursue certain tactics, the execution of the play itself is a series of solitary acts. If the batter hits a line drive, the outfielder is solely responsible for deciding to try to catch it or play it on the bounce and for succeeding or failing. The statistical precision of baseball is both facilitated by this isolation and reinforces it.\nCricket is more similar to baseball than many other team sports in this regard: while the individual focus in cricket is mitigated by the importance of the batting partnership and the practicalities of tandem running, it is enhanced by the fact that a batsman may occupy the wicket for an hour or much more. There is no statistical equivalent in cricket for the fielding error and thus less emphasis on personal responsibility in this area of play.\nUniqueness of parks.\nUnlike those of most sports, baseball playing fields can vary significantly in size and shape. While the dimensions of the infield are specifically regulated, the only constraint on outfield size and shape for professional teams, following the rules of MLB and Minor League Baseball, is that fields built or remodeled since June 1, 1958, must have a minimum distance of from home plate to the fences in left and right field and to center. Major league teams often skirt even this rule. For example, at Daikin Park, which became the home of the Houston Astros in 2000, the Crawford Boxes in left field are only from home plate. There are no rules at all that address the height of fences or other structures at the edge of the outfield. The most famously idiosyncratic outfield boundary is the left-field wall at Boston's Fenway Park, in use since 1912: the Green Monster is from home plate down the line and tall.\nSimilarly, there are no regulations at all concerning the dimensions of foul territory. Thus a foul fly ball may be entirely out of play in a park with little space between the foul lines and the stands, but a foulout in a park with more expansive foul ground. A fence in foul territory that is close to the outfield line will tend to direct balls that strike it back toward the fielders, while one that is farther away may actually prompt more collisions, as outfielders run full speed to field balls deep in the corner. These variations can make the difference between a double and a triple or inside-the-park home run. The surface of the field is also unregulated. While the adjacent image shows a traditional field surfacing arrangement (and the one used by virtually all MLB teams with naturally surfaced fields), teams are free to decide what areas will be grassed or bare. Some fields\u2014including several in MLB\u2014use artificial turf. Surface variations can have a significant effect on how ground balls behave and are fielded as well as on baserunning. Similarly, the presence of a roof (seven major league teams play in stadiums with permanent or retractable roofs) can greatly affect how fly balls are played. While football and soccer players deal with similar variations of field surface and stadium covering, the size and shape of their fields are much more standardized. The area out-of-bounds on a football or soccer field does not affect play the way foul territory in baseball does, so variations in that regard are largely insignificant.\nThese physical variations create a distinctive set of playing conditions at each ballpark. Other local factors, such as altitude and climate, can also significantly affect play. A given stadium may acquire a reputation as a pitcher's park or a hitter's park, if one or the other discipline notably benefits from its unique mix of elements. The most exceptional park in this regard is Coors Field, home of the Colorado Rockies. Its high altitude\u2014 above sea level\u2014is partly responsible for giving it the strongest hitter's park effect in the major leagues due to the low air pressure. Wrigley Field, home of the Chicago Cubs, is known for its fickle disposition: a pitcher's park when the strong winds off Lake Michigan are blowing in, it becomes more of a hitter's park when they are blowing out. The absence of a standardized field affects not only how particular games play out, but the nature of team rosters and players' statistical records. For example, hitting a fly ball into right field might result in an easy catch on the warning track at one park, and a home run at another. A team that plays in a park with a relatively short right field, such as the New York Yankees, will tend to stock its roster with left-handed pull hitters, who can best exploit it. On the individual level, a player who spends most of his career with a team that plays in a hitter's park will gain an advantage in batting statistics over time\u2014even more so if his talents are especially suited to the park.\nStatistics.\nOrganized baseball lends itself to statistics to a greater degree than many other sports. Each play is discrete and has a relatively small number of possible outcomes. In the late 19th century, a former cricket player, English-born Henry Chadwick of Brooklyn, was responsible for the \"development of the box score, tabular standings, the annual baseball guide, the batting average, and most of the common statistics and tables used to describe baseball.\" The statistical record is so central to the game's \"historical essence\" that Chadwick came to be known as Father Baseball. In the 1920s, American newspapers began devoting more and more attention to baseball statistics, initiating what journalist and historian Alan Schwarz describes as a \"tectonic shift in sports, as intrigue that once focused mostly on teams began to go to individual players and their statistics lines.\"\nThe Official Baseball Rules administered by MLB require the official scorer to categorize each baseball play unambiguously. The rules provide detailed criteria to promote consistency. The score report is the official basis for both the box score of the game and the relevant statistical records. General managers, managers, and baseball scouts use statistics to evaluate players and make strategic decisions.\nCertain traditional statistics are familiar to most baseball fans. The basic batting statistics include:\nThe basic baserunning statistics include:\nThe basic pitching statistics include:\nThe basic fielding statistics include:\nAmong the many other statistics that are kept are those collectively known as \"situational statistics\". For example, statistics can indicate which specific pitchers a certain batter performs best against. If a given situation statistically favors a certain batter, the manager of the fielding team may be more likely to change pitchers or have the pitcher intentionally walk the batter in order to face one who is less likely to succeed.\nSabermetrics.\nSabermetrics is the field of baseball statistical study and the development of new statistics and analytical tools. Such new statistics are also called sabermetrics. The term was coined around 1980 by one of the field's leading proponents, Bill James, and derives from the Society for American Baseball Research (SABR).\nThe growing popularity of sabermetrics since the early 1980s has brought more attention to two batting statistics that sabermetricians argue are much better gauges of a batter's skill than batting average:\nSome of the new statistics devised by sabermetricians have gained wide use:\nPopularity and cultural impact.\nWriting in 1919, philosopher Morris Raphael Cohen described baseball as the national religion of the US. In the words of sports columnist Jayson Stark, baseball has long been \"a unique paragon of American culture\"\u2014a status he sees as devastated by the steroid abuse scandal. Baseball has an important place in other national cultures as well: Scholar Peter Bjarkman describes \"how deeply the sport is ingrained in the history and culture of a nation such as Cuba, [and] how thoroughly it was radically reshaped and nativized in Japan.\"\nWestern Hemisphere.\nAmerican influence in the Western Hemisphere has meant that baseball grew significantly in the region.\nIn the United States.\nThe major league game in the United States was originally targeted toward a middle-class, white-collar audience: relative to other spectator pastimes, the National League's set ticket price of 50 cents in 1876 was high, while the location of playing fields outside the inner city and the workweek daytime scheduling of games were also obstacles to a blue-collar audience. A century later, the situation was very different. With the rise in popularity of other team sports with much higher average ticket prices\u2014football, basketball, and hockey\u2014professional baseball had become among the most popular blue-collar-oriented American spectator sports. \nOverall, baseball has a large following in the United States; a 2006 poll found that nearly half of Americans are fans. This led to baseball being granted the title of \"America's favorite pastime\" by many American baseball fans. The game was historically seen as contributing to the melting pot society of the nation, encouraging immigrants to integrate. In the late 1900s and early 2000s, baseball's position compared to football in the United States moved in contradictory directions. In 2008, MLB set a revenue record of $6.5\u00a0billion, matching the NFL's revenue for the first time in decades. A new MLB revenue record of more than $10\u00a0billion was set in 2017. On the other hand, the percentage of American sports fans polled who named baseball as their favorite sport was 9%, compared to pro football at 37%. In 1985, the respective figures were pro football 24%, baseball 23%. Because there are so many more major league games played, there is no comparison in overall attendance. In 2008, total attendance at major league games was the second-highest in history: 78.6\u00a0million, 0.7% off the record set the previous year. The following year, amid the U.S. recession, attendance fell by 6.6% to 73.4\u00a0million. Eight years later, it dropped under 73\u00a0million. Attendance at games held under the Minor League Baseball umbrella set a record in 2008, with 43.3\u00a0million. While MLB games have not drawn the same national TV viewership as football games, MLB games are dominant in teams' local markets and regularly lead all programs in primetime in their markets during the summer.\nLatin America.\nBaseball is very popular in Venezuela; in 2011, 95% of people surveyed claimed it to be the national sport. The sport's overall popularity in Latin America has assisted in integrating Latin American migrants to the United States.\nIn Brazil, baseball fan popularity has grown in last few years, thanks to MLB broadcasts in Brazilian ESPN and the historic silver medal in 2023 Pan-American games. although, it still lags behind Basketball and American Football in the list of most played sports in Brazil.\nCaribbean.\nSince the early 1980s, the Dominican Republic, in particular the city of San Pedro de Macor\u00eds, has been the major leagues' primary source of foreign talent. In 2017, 83 of the 868 players on MLB Opening Day rosters (and disabled lists) were from the country. Among other Caribbean countries and territories, a combined 97 MLB players were born in Venezuela, Cuba, and Puerto Rico. Hall-of-Famer Roberto Clemente remains one of the greatest national heroes in Puerto Rico's history. While baseball has long been the island's primary athletic pastime, its once well-attended professional winter league has declined in popularity since 1990, when young Puerto Rican players began to be included in the major leagues' annual first-year player draft. In Cuba, where baseball is by every reckoning the national sport, the national team overshadows the city and provincial teams that play in the top-level domestic leagues.\nAsia.\nIn East Asia, baseball is among the most popular sports in Japan, Taiwan and South Korea. In Japan, where baseball is inarguably the leading spectator team sport, combined revenue for the twelve teams in Nippon Professional Baseball (NPB), the body that oversees both the Central and Pacific Leagues, was estimated at $1\u00a0billion in 2007. Total NPB attendance for the year was approximately 20\u00a0million. While in the preceding two decades, MLB attendance grew by 50 percent and revenue nearly tripled, the comparable NPB figures were stagnant. There are concerns that MLB's growing interest in acquiring star Japanese players will hurt the game in their home country. Revenue figures are not released for the country's amateur system. Similarly, according to one official pronouncement, the sport's governing authority \"has never taken into account attendance\u00a0... because its greatest interest has always been the development of athletes\". In Taiwan, baseball is one of the most widely spectated sports, in tv and person.\nBaseball has grown significantly in China in recent years, with MLB estimating in 2019 that there are 21 million active fans in the country.\nAmong children.\n, Little League Baseball oversees leagues with close to 2.4\u00a0million participants in over 80 countries. The number of players has fallen since the 1990s, when 3 million children took part in Little League Baseball annually. Babe Ruth League teams have over 1\u00a0million participants. According to the president of the International Baseball Federation, between 300,000 and 500,000 women and girls play baseball around the world, including Little League and the introductory game of Tee Ball.\nA varsity baseball team is an established part of physical education departments at most high schools and colleges in the United States. In 2015, nearly half a million high schoolers and over 34,000 collegians played on their schools' baseball teams. By early in the 20th century, intercollegiate baseball was Japan's leading sport. Today, high school baseball in particular is immensely popular there. The final rounds of the two annual tournaments\u2014the National High School Baseball Invitational Tournament in the spring, and the even more important National High School Baseball Championship in the summer\u2014are broadcast around the country. The tournaments are known, respectively, as Spring Koshien and Summer Koshien after the 55,000-capacity stadium where they are played. In Cuba, baseball is a mandatory part of the state system of physical education, which begins at age six. Talented children as young as seven are sent to special district schools for more intensive training\u2014the first step on a ladder whose acme is the national baseball team.\nIn popular culture.\nBaseball has had a broad impact on popular culture, both in the United States and elsewhere. Dozens of English-language idioms have been derived from baseball; in particular, the game is the source of a number of widely used sexual euphemisms. The first networked radio broadcasts in North America were of the 1922 World Series: famed sportswriter Grantland Rice announced play-by-play from New York City's Polo Grounds on WJZ\u2013Newark, New Jersey, which was connected by wire to WGY\u2013Schenectady, New York, and WBZ\u2013Springfield, Massachusetts. The baseball cap has become a ubiquitous fashion item not only in the United States and Japan, but also in countries where the sport itself is not particularly popular, such as the United Kingdom.\nBaseball has inspired many works of art and entertainment. One of the first major examples, Ernest Thayer's poem \"Casey at the Bat\", appeared in 1888. A wry description of the failure of a star player in what would now be called a \"clutch situation\", the poem became the source of vaudeville and other staged performances, audio recordings, film adaptations, and an opera, as well as a host of sequels and parodies in various media. There have been many baseball movies, including the Academy Award\u2013winning \"The Pride of the Yankees\" (1942) and the Oscar nominees \"The Natural\" (1984) and \"Field of Dreams\" (1989). The American Film Institute's selection of the ten best sports movies includes \"The Pride of the Yankees\" at number 3 and \"Bull Durham\" (1988) at number 5. Baseball has provided thematic material for hits on both stage\u2014the Adler\u2013Ross musical \"Damn Yankees\"\u2014and record\u2014George J. Gaskin's \"Slide, Kelly, Slide\", Simon and Garfunkel's \"Mrs. Robinson\", and John Fogerty's \"Centerfield\". The baseball-inspired comedic sketch \"Who's on First?\", popularized by Abbott and Costello in 1938, quickly became famous. Six decades later, \"Time\" named it the best comedy routine of the 20th century.\nLiterary works connected to the game include the short fiction of Ring Lardner and novels such as Bernard Malamud's \"The Natural\" (the source for the movie), Robert Coover's \"The Universal Baseball Association, Inc., J. Henry Waugh, Prop.\", John Grisham's Calico Joe and W. P. Kinsella's \"Shoeless Joe\" (the source for \"Field of Dreams\"). Baseball's literary canon also includes the beat reportage of Damon Runyon; the columns of Grantland Rice, Red Smith, Dick Young, and Peter Gammons; and the essays of Roger Angell. Among the celebrated nonfiction books in the field are Lawrence S. Ritter's \"The Glory of Their Times\", Roger Kahn's \"The Boys of Summer\", and Michael Lewis's \"\". The 1970 publication of major league pitcher Jim Bouton's tell-all chronicle \"Ball Four\" is considered a turning point in the reporting of professional sports.\nBaseball has also inspired the creation of new cultural forms. Baseball cards were introduced in the late 19th century as trade cards. A typical example featured an image of a baseball player on one side and advertising for a business on the other. In the early 1900s they were produced widely as promotional items by tobacco and confectionery companies. The 1930s saw the popularization of the modern style of baseball card, with a player photograph accompanied on the rear by statistics and biographical data. Baseball cards\u2014many of which are now prized collectibles\u2014are the source of the much broader trading card industry, involving similar products for different sports and non-sports-related fields.\nModern fantasy sports began in 1980 with the invention of Rotisserie League Baseball by New York writer Daniel Okrent and several friends. Participants in a Rotisserie league draft notional teams from the list of active MLB players and play out an entire imaginary season with game outcomes based on the players' latest real-world statistics. Rotisserie-style play quickly became a phenomenon. Now known more generically as fantasy baseball, it has inspired similar games based on an array of different sports. The field boomed with increasing Internet access and new fantasy sports-related websites. By 2008, 29.9 million people in the United States and Canada were playing fantasy sports, spending $800\u00a0million on the hobby. The burgeoning popularity of fantasy baseball is also credited with the increasing attention paid to sabermetrics\u2014first among fans, only later among baseball professionals.\nDerivative games.\nInformal variations of baseball have popped up over time, with games like corkball reflecting local traditions and allowing the game to be played in diverse environments. Two variations of baseball, softball and Baseball5, are internationally governed alongside baseball by the World Baseball Softball Confederation.\nBritish baseball.\nAmerican professional baseball teams toured Britain in 1874 and 1889, and had a great effect on similar sports in Britain. In Wales and Merseyside, a strong community game had already developed with skills and plays more in keeping with the American game and the Welsh began to informally adopt the name \"baseball\" (P\u00eal Fas), to reflect the American style. By the 1890s, calls were made to follow the success of other working class sports (like Rugby in Wales and Soccer in Merseyside) and adopt a distinct set of rules and bureaucracy. During the 1892 season rules for the game of \"baseball\" were agreed and the game was officially codified.\nFinnish baseball.\nFinnish baseball, also known as pes\u00e4pallo, is a combination of traditional ball-batting team games and North American baseball, invented by Lauri \"Tahko\" Pihkala in the 1920s. The basic idea of pes\u00e4pallo is similar to that of baseball: the offense tries to score by hitting the ball successfully and running through the bases, while the defense tries to put the batter and runners out. One of the most important differences between pes\u00e4pallo and baseball is that the ball is pitched vertically, which makes hitting the ball, as well as controlling the power and direction of the hit, much easier. This gives the offensive game more variety, speed, and tactical aspects compared to baseball."}
{"id": "3851", "revid": "46546583", "url": "https://en.wikipedia.org/wiki?curid=3851", "title": "Baseball positions", "text": "In the sport of baseball, each of the nine players on a team is assigned a particular fielding position when it is their turn to play defense. Each position conventionally has an associated number, for use in scorekeeping by the official scorer: 1 (pitcher), 2 (catcher), 3 (first baseman), 4 (second baseman), 5 (third baseman), 6 (shortstop), 7 (left fielder), 8 (center fielder), and 9 (right fielder). Collectively, these positions are usually grouped into three groups: the outfield (left field, center field, and right field), the infield (first base, second base, third base, and shortstop), and the battery (pitcher and catcher). Traditionally, players within each group will often be more able to exchange positions easily (that is, a second baseman can usually play shortstop well, and a center fielder can also be expected to play right field); however, the pitcher and catcher are highly specialized positions and rarely will play at other positions.\nFielding.\nFielders must be able to catch the ball well, as catching batted balls before they bounce is one way they can put the batter out, as well as create opportunities to prevent the advance of, and put out other runners. Additionally, they must be able to throw the ball well, with many plays in the game depending on one fielder collecting the hit ball and then throwing it to another fielder who, while holding the ball in their hand/glove, touches either a runner or the base the runner is forced to run to in order to record an out. Fielders often have to run, dive, and slide a great deal in the act of reaching, stopping, and retrieving a hit ball, and then setting themselves up to transfer the ball, all with the end goal of getting the ball as quickly as possible to another fielder. They also run the risk of colliding with incoming runners during a tag attempt at a base. \nFielders may have different responsibilities depending on the game situation. For example, when an outfielder is attempting to throw the ball from near the fence to one of the bases, an infielder may need to \"cut off\" the throw and then act as a relay thrower to help the ball cover its remaining distance to the target destination. \nAs a group, the outfielders are responsible for preventing home runs by reaching over the fence (and potentially doing a wall climb) for fly balls that are catchable. The infielders are the ones who generally handle plays that involve tagging a base or runner, and also need quick reflexes in order to catch a batted ball before it leaves the infield. The pitcher and catcher have special responsibilities to prevent base stealing, as they are the ones who handle the ball whenever it has not been hit. The catcher will also sometimes attempt to block the plate in order to prevent a run being scored."}
{"id": "3855", "revid": "42316941", "url": "https://en.wikipedia.org/wiki?curid=3855", "title": "Baseball/History of baseball", "text": ""}
{"id": "3856", "revid": "1270066543", "url": "https://en.wikipedia.org/wiki?curid=3856", "title": "History of baseball in the United States", "text": "The history of baseball in the United States dates to the 19th century, when boys and amateur enthusiasts played a baseball-like game by their own informal rules using homemade equipment. The popularity of the sport grew and amateur men's ball clubs were formed in the 1830\u20131850s. Semi-professional baseball clubs followed in the 1860s, and the first professional leagues arrived in the post-American Civil War 1870s.\nEarly history.\nThe earliest known mention of baseball in the United States is either a 1786 diary entry by a Princeton University student who describes playing \"baste ball,\" or a 1791 Pittsfield, Massachusetts, ordinance that barred the playing of baseball within of the town meeting house and its glass windows. Another early reference reports that \"base ball\" was regularly played on Saturdays in 1823 on the outskirts of New York City in an area that today is Greenwich Village. The Olympic Base Ball Club of Philadelphia was organized in 1833.\nIn 1903, the British-born sportswriter Henry Chadwick published an article speculating that baseball was derived from an English game called rounders, which Chadwick had played as a boy in England. Baseball executive Albert Spalding disagreed, asserting that the game was fundamentally American and had hatched on American soil. To settle the matter, the two men appointed a commission, headed by Abraham Mills, the fourth president of the National League of Professional Baseball Clubs. The commission, which also included six other sports executives, labored for three years, finally declaring that Abner Doubleday had invented the national pastime. Doubleday \"...never knew that he had invented baseball. But 15 years after his death, he was anointed as the father of the game,\" writes baseball historian John Thorn. The myth about Doubleday inventing the game of baseball actually came from a Colorado mining engineer who claimed to have been present at the moment of creation. The miner's tale was never corroborated, nonetheless the myth was born and persists to this day. Which does not mean that the Doubleday myth does not continue to be disputed; in fact, it is likely that the parentage of the modern game of baseball will be in some dispute until long after such future time when the game is no longer played.\nThe first team to play baseball under modern rules is believed to be the New York Knickerbockers. The club was founded on September 23, 1845, as a breakaway from the earlier Gotham Club. The new club's by-laws committee, William R. Wheaton and William H. Tucker, formulated the \"Knickerbocker Rules\", which, in large part, dealt with organizational matters but which also laid out some new rules of play. One of these prohibited \"soaking\" or \"plugging\" the runner; under older rules, a fielder could put a runner out by hitting the runner with the thrown ball, as in the common schoolyard game of kickball. The Knickerbocker Rules required fielders to tag or force the runner. The new rules also introduced base paths, foul lines and foul balls; in \"town ball\" every batted ball was fair, as in cricket, and the lack of runner's lanes led to wild chases around the infield.\nInitially, Wheaton and Tucker's innovations did not serve the Knickerbockers well. In the first known competitive game between two clubs under the new rules, played at Elysian Fields in Hoboken, New Jersey, on June 19, 1846, the \"New York nine\" (almost certainly the Gotham Club) humbled the Knickerbockers by a score of 23 to 1. Nevertheless, the Knickerbocker Rules were rapidly adopted by teams in the New York area and their version of baseball became known as the \"New York Game\" (as opposed to the less rule-bound \"Massachusetts Game,\" played by clubs in New England, and \"Philadelphia Town-ball\").\nIn spite of its rapid growth in popularity, baseball had yet to overtake the British import, cricket. As late as 1855, the New York press was still devoting more space to coverage of cricket than to baseball.\nAt a 1857 convention of sixteen New York area clubs, including the Knickerbockers, the National Association of Base Ball Players (NABBP) was formed. It was the first official organization to govern the sport and the first to establish a championship. The convention also formalized three key features of the game: 90 feet distance between the bases, 9-man teams, and 9-inning games (under the Knickerbocker Rules, games were played to 21 runs). During the Civil War, soldiers from different parts of the United States played baseball together, leading to a more unified national version of the sport. Membership in the NABBP grew to almost 100 clubs by 1865 and to over 400 by 1867, including clubs from as far away as California. Beginning in 1869, the league permitted professional play, addressing a growing practice that had not been previously permitted under its rules. The first and most prominent professional club of the NABBP era was the Cincinnati Red Stockings in Ohio, which went undefeated in 1869 and half of 1870. After the Cincy club broke up at the end of that season, four key members including player/manager Harry Wright moved to Boston under owner and businessman Ivers Whitney Adams and became the \"Boston Red Stockings\" and the Boston Base Ball Club.\nIn 1858, at the Fashion Race Course in the Corona neighborhood of Queens (now part of New York City), the first games of baseball to charge admission were played. The All Stars of Brooklyn, including players from the Atlantic, Excelsior, Putnam and Eckford clubs, took on the All Stars of New York (Manhattan), including players from the Knickerbocker, Gotham, Eagle and Empire clubs. These are commonly believed to the first all-star baseball games.\nGrowth.\nBefore the Civil War, baseball competed for public interest with cricket and regional variants of baseball, notably town ball played in Philadelphia and the Massachusetts Game played in New England. In the 1860s, aided by the Civil War, \"New York\" style baseball expanded into a national game. Baseball began to overtake cricket in popularity, impelled by its much shorter duration relative to the form of cricket played at the time. William Humber argues that baseball was also able to grow because there was less of a social taboo against it than in England, where it was strongly perceived as a children's game, and because Americans preferred a sport where the teams alternated offense and defense more frequently.\nAs its first governing body, the National Association of Base Ball Players was formed. The NABBP soon expanded into a truly national organization, although most of the strongest clubs remained those based in the country's northeastern part. In its 12-year history as an amateur league, the Atlantic Club of Brooklyn won seven championships, establishing themselves as the first true dynasty in the sport. However, Mutual of New York was widely considered one of the best teams of the era. By the end of 1865, almost 100 clubs were members of the NABBP. By 1867, it ballooned to over 400 members, including some clubs from as far away as California. One of these western clubs, Chicago (dubbed the \"White Stockings\" by the press for their uniform hosiery), won the championship in 1870. Because of this growth, regional and state organizations began to assume a more prominent role in the governance of the amateur sport at the expense of the NABBP. At the same time, the professionals soon sought a new governing body.\nProfessionalism.\nThe NABBP of America was initially established upon principles of amateurism. However, even early in the Association's history, some star players such as James Creighton of Excelsior received compensation covertly or indirectly. In 1866, the NABBP investigated Athletic of Philadelphia for paying three players including Lip Pike, but ultimately took no action against either the club or the players. In many cases players, quite openly, received a cut of the gate receipts. Clubs playing challenge series were even accused of agreeing beforehand to split the earlier games to guarantee a decisive (and thus more certain to draw a crowd) \"rubber match\". To address this growing practice, and to restore integrity to the game, at its December 1868 meeting the NABBP established a professional category for the 1869 season. Clubs desiring to pay players were now free to declare themselves professional.\nThe Cincinnati Red Stockings were the first to declare themselves openly professional, and were aggressive in recruiting the best available players. Twelve clubs, including most of the strongest clubs in the NABBP, ultimately declared themselves professional for the 1869 season.\nThe first attempt at forming a major league produced the National Association of Professional Base Ball Players, which lasted from 1871 to 1875. The now all-professional Chicago \"White Stockings\" (today the Chicago Cubs), financed by businessman William Hulbert, became a charter member of the league along with a new Red Stockings club (now the Atlanta Braves), formed in Boston with four former Cincinnati players. The Chicagos were close contenders all season, despite the fact that the Great Chicago Fire had destroyed the team's home field and most of their equipment. Chicago finished the season in second place, but were ultimately forced to drop out of the league during the city's recovery period, finally returning to National Association play in 1874. Over the next couple of seasons, the Boston club dominated the league and hoarded many of the game's best players, even those who were under contract with other teams. After Davy Force signed with Chicago, and then breached his contract to play in Boston, Hulbert became discouraged by the \"contract jumping\" as well as the overall disorganization of the N.A. (for example, weaker teams with losing records or inadequate gate receipts would simply decline to play out the season), and thus spearheaded the movement to form a stronger organization. The result of his efforts was the formation of a much more \"ethical\" league, which was named the National League of Professional Base Ball Clubs (NL). After a series of rival leagues were organized but failed (most notably the American Base Ball Association (1882\u20131891), which spawned the clubs which would ultimately become the Cincinnati Reds, Pittsburgh Pirates, St. Louis Cardinals and Brooklyn Dodgers), the current American League (AL), evolving from the minor Western League of 1893, was established in 1901.\nRise of the major leagues.\nIn 1870, a schism developed between professional and amateur ballplayers. The NABBP split into two groups. The National Association of Professional Base Ball Players operated from 1871 through 1875 and is considered by some to have been the first major league. Its amateur counterpart disappeared after only a few years.\nWilliam Hulbert's National League, which was formed after the National Association proved ineffective, put its emphasis on \"clubs\" rather than \"players\". Clubs now had the ability to enforce player contracts and prevent players from jumping to higher-paying clubs. Clubs in turn were required to play their full schedule of games, rather than forfeiting scheduled games once out of the running for the league championship, a practice that had been common under the National Association. A concerted effort was also made to reduce the amount of gambling on games which was leaving the validity of results in doubt.\nAround this time, a gentlemen's agreement was struck between the clubs to exclude non-white players from professional baseball, a \"de facto\" ban that remained in effect until 1947. It is a common misconception that Jackie Robinson was the first African-American major-league ballplayer; he was actually only the first after a long gap (and the first in the modern era). Moses Fleetwood Walker and his brother Weldy Walker were unceremoniously dropped from major and minor-league rosters in the 1880s, as were other African-Americans in baseball. An unknown number of African-Americans played in the major leagues by representing themselves as Indians, or South or Central Americans, and a still larger number played in the minor leagues and on amateur teams. In the majors, however, it was not until the signing of Robinson (in the National League) and Larry Doby (in the American League) that baseball began to relax its ban on African-Americans.\nThe early years of the National League were tumultuous, with threats from rival leagues and a rebellion by players against the hated \"reserve clause\", which restricted the free movement of players between clubs. Competitive leagues formed regularly, and disbanded just as regularly. The most successful of these was the American Association of 1882\u20131891, sometimes called the \"beer and whiskey league\" for its tolerance of the sale of alcoholic beverages to spectators. For several years, the National League and American Association champions met in a postseason World's Championship Series\u2014the first attempt at a World Series.\nThe Union Association survived for only one season (1884), as did the Players' League (1890), which was an attempt to return to the National Association structure of a league controlled by the players themselves. Both leagues are considered major leagues by many baseball researchers because of the perceived high caliber of play and the number of star players featured. However, some researchers have disputed the major league status of the Union Association, pointing out that franchises came and went and contending that the St. Louis club, which was deliberately \"stacked\" by the league's president (who owned that club), was the only club that was anywhere close to major-league caliber.\nIn fact, there were dozens of leagues, large and small, in the late 19th century. What made the National League \"major\" was its dominant position in the major cities, particularly the edgy, emotional nerve center of baseball that was New York City. Large, concentrated populations offered baseball teams national media distribution systems and fan bases that could generate sufficient revenues to afford the best players in the country.\nA number of the other leagues, including the venerable Eastern League, threatened the dominance of the National League. The Western League, founded in 1893, became particularly aggressive. Its fiery leader Ban Johnson railed against the National League and promised to grab the best players and field the best teams. The Western League began play in April 1894 with teams in Detroit (now the American League Detroit Tigers, the only league team that has not moved since), Grand Rapids, Indianapolis, Kansas City, Milwaukee, Minneapolis, Sioux City and Toledo. Prior to the 1900 season, the league changed its name to the American League and moved several franchises to larger, strategic locations. In 1901 the American League declared its intent to operate as a major league.\nThe resulting bidding war for players led to widespread contract-breaking and legal disputes. One of the most famous involved star second baseman Napoleon Lajoie, who in 1901 went across town in Philadelphia from the National League Phillies to the American League Athletics. Barred by a court injunction from playing baseball in the state of Pennsylvania the next year, Lajoie was traded to the Cleveland team, where he played and managed for many years.\nThe war between the American and National leagues caused shock waves across the baseball world. At a meeting in 1901, the other baseball leagues negotiated a plan to maintain their independence. On September 5, 1901, Eastern League president Patrick T. Powers announced the formation of the second National Association of Professional Baseball Leagues, the NABPL (NA).\nThese leagues did not consider themselves \"minor\"\u2014a term that did not come into vogue until St. Louis Cardinals general manager Branch Rickey pioneered the farm system in the 1930s. Nevertheless, these financially troubled leagues, by beginning the practice of selling players to the more affluent National and American leagues, embarked on a path that eventually led to the loss of their independent status.\nBan Johnson had other designs for the NA. While the NA continues to this day, he saw it as a tool to end threats from smaller rivals who might some day want to expand in other territories and threaten his league's dominance.\nAfter 1902 both leagues and the NABPL signed a new National Agreement which achieved three things:\nThe new agreement tied independent contracts to the reserve-clause national league contracts. Baseball players were a commodity, like cars. A player's skill set had a price of $5,000. It set up a rough classification system for independent leagues that regulated the dollar value of contracts, the forerunner of the system refined by Rickey and used today.\nIt also gave the NA great power. Many independents walked away from the 1901 meeting. The deal with the NA punished those other indies who had not joined the NA and submitted to the will of the majors. The NA also agreed to the deal so as to prevent more pilfering of players with little or no compensation for the players' development. Several leagues, seeing the writing on the wall, eventually joined the NA, which grew in size over the next several years.\nIn the very early part of the 20th century, known as the \"dead-ball era\", baseball rules and equipment favored the \"inside game\" and the game was played more violently and aggressively than it is today. This period ended in the 1920s with several changes that gave advantages to hitters. In the largest parks, the outfield fences were brought closer to the infield. In addition, the strict enforcement of new rules governing the construction and regular replacement of the ball caused it to be easier to hit, and be hit harder.\nThe first professional black baseball club, the Cuban Giants, was organized in 1885. Subsequent professional black baseball clubs played each other independently, without an official league to organize the sport. Rube Foster, a former ballplayer, founded the Negro National League in 1920. A second league, the Eastern Colored League, was established in 1923. These became known as the Negro leagues, though these leagues never had any formal overall structure comparable to the Major Leagues. The Negro National League did well until 1930, but folded during the Great Depression.\nFrom 1942 to 1948, the Negro World Series was revived. This was the golden era of Negro league baseball, a time when it produced some of its greatest stars. In 1947, Jackie Robinson signed a contract with the Brooklyn Dodgers, breaking the color barrier that had prevented talented African-American players from entering the white-only major leagues. Although the transformation was not instantaneous, baseball has since become fully integrated. While the Dodgers' signing of Robinson was a key moment in baseball and civil rights history, it prompted the decline of the Negro leagues. The best black players were now recruited for the Major Leagues, and black fans followed. The last Negro league teams folded in the 1960s.\nPitchers dominated the game in the 1960s and early 1970s. In 1973, the designated hitter (DH) rule was adopted by the American League, while in the National League, the DH rule was not adopted until March 2022. The rule had been applied in a variety of ways during the World Series; until the adoption of the DH by the National League, the DH rule applied when Series games were played in an American League stadium, and pitchers would bat during Series games played in National League stadiums. There had been continued disagreement about the future of the DH rule in the World Series until league-wide adoption of the DH rule.\nDuring the late 1960s, the Baseball Players Union became much stronger and conflicts between owners and the players' union led to major work stoppages in 1972, 1981, and 1994. The 1994 baseball strike led to the cancellation of the World Series, and was not settled until the spring of 1995. In the late 1990s, functions that had been administered separately by the two major leagues' administrations were united under the rubric of Major League Baseball (MLB).\nThe dead-ball era: 1901 to 1919.\nThe period 1901\u20131919 is commonly called the \"Dead-ball era\", with low-scoring games dominated by pitchers such as Walter Johnson, Cy Young, Christy Mathewson, and Grover Cleveland Alexander. The term also accurately describes the condition of the baseball itself. Baseballs cost three dollars each in 1901, a unit price which would be equal to $ today. In contrast, modern baseballs purchased in bulk as is the case with professional teams cost about seven dollars each as of 2021 and thus make up a negligible portion of a modern MLB team's operating budget. Due to the much larger relative cost, club owners in the early 20th century were reluctant to spend much money on new balls if not necessary. It was not unusual for a single baseball to last an entire game, nor for a baseball to be reused for the next game especially if it was still in relatively good condition as would likely be the case for a ball introduced late in the game. By the end of the game, the ball would usually be dark with grass, mud, and tobacco juice, and it would be misshapen and lumpy from contact with the bat. Balls were replaced only if they were hit into the crowd and lost, and many clubs employed security guards expressly for the purpose of retrieving balls hit into the stands \u2014 a practice unthinkable today.\nAs a consequence, home runs were rare, and the \"inside game\" dominated\u2014singles, bunts, stolen bases, the hit-and-run play, and other tactics dominated the strategies of the time.\nDespite this, there were also several superstar hitters, the most famous being Honus Wagner, held to be one of the greatest shortstops to ever play the game, and Detroit's Ty Cobb, the \"Georgia Peach.\" His career batting average of .366 has yet to be bested.\nThe Merkle incident.\nThe 1908 pennant races in both the AL and NL were among the most exciting ever witnessed. The conclusion of the National League season, in particular, involved a bizarre chain of events. On September 23, 1908, the New York Giants and Chicago Cubs played a game in the Polo Grounds. Nineteen-year-old rookie first baseman Fred Merkle, later to become one of the best players at his position in the league, was on first base, with teammate Moose McCormick on third with two outs and the game tied. Giants shortstop Al Bridwell socked a single, scoring McCormick and apparently winning the game. However, Merkle, instead of advancing to second base, ran toward the clubhouse to avoid the spectators mobbing the field, which at that time was a common, acceptable practice. The Cubs' second baseman, Johnny Evers, noticed this. In the confusion that followed, Evers claimed to have retrieved the ball and touched second base, forcing Merkle out and nullifying the run scored. Evers brought this to the attention of the umpire that day, Hank O'Day, who after some deliberation called the runner out. Because of the state of the field O'Day thereby called the game. Despite the arguments by the Giants, the league upheld O'Day's decision and ordered the game replayed at the end of the season, if necessary. It turned out that the Cubs and Giants ended the season tied for first place, so the game was indeed replayed, and the Cubs won the game, the pennant, and subsequently the World Series (the last Cubs Series victory until 2016).\nFor his part, Merkle was doomed to endless ridicule throughout his career (and to a lesser extent for the rest of his life) for this lapse, which went down in history as \"Merkle's Boner\". In his defense, some baseball historians have suggested that it was not customary for game-ending hits to be fully \"run out\", it was only Evers's insistence on following the rules strictly that resulted in this unusual play. In fact, earlier in the 1908 season, the identical situation had been brought to the umpires' attention by Evers; the umpire that day was the same Hank O'Day. While the winning run was allowed to stand on that occasion, the dispute raised O'Day's awareness of the rule, and directly set up the Merkle controversy.\nNew places to play.\nTurn-of-the-century baseball attendances were modest by later standards. The average for the 1,110 games in the 1901 season was 3,247. However, the first 20 years of the 20th century saw an unprecedented rise in the popularity of baseball. Large stadiums dedicated to the game were built for many of the larger clubs or existing grounds enlarged, including Tiger Stadium in Detroit, Shibe Park in Philadelphia, Ebbets Field in Brooklyn, the Polo Grounds in Manhattan, Boston's Fenway Park along with Wrigley Field and Comiskey Park in Chicago. Likewise from the Eastern League to the small developing leagues in the West, and the rising Negro leagues professional baseball was being played all across the country. Average major league attendances reached a pre-World War I peak of 5,836 in 1909. Where there weren't professional teams, there were semi-professional teams, traveling teams barnstorming, company clubs and amateur men's leagues that drew small but fervent crowds.\nThe \"Black Sox\".\nThe fix of baseball games by gamblers and players working together had been suspected as early as the 1850s. Hal Chase was particularly notorious for throwing games, but played for a decade after gaining this reputation; he even managed to parlay these accusations into a promotion to manager. Even baseball stars such as Ty Cobb and Tris Speaker have been credibly alleged to have fixed game outcomes. When MLB's complacency during this \"Golden Age\" was eventually exposed after the 1919 World Series, it became known as the Black Sox scandal.\nAfter an excellent regular season (88\u201352, .629 W%), the Chicago White Sox were heavy favorites to win the 1919 World Series. Arguably the best team in baseball, the White Sox had a deep lineup, a strong pitching staff, and a good defense. Even though the National League champion Cincinnati Reds had a superior regular season record (96\u201344, .689 W%,) no one, including gamblers and bookmakers, anticipated the Reds having a chance. When the Reds triumphed 5\u20133, many pundits cried foul.\nAt the time of the scandal, the White Sox were arguably the most successful franchise in baseball, with excellent gate receipts and record attendance. At the time, most baseball players were not paid especially well and had to work other jobs during the winter to survive. Some elite players on the big-city clubs made very good salaries, but Chicago was a notable exception.\nFor many years, the White Sox were owned and operated by Charles Comiskey, who paid the lowest player salaries, on average, in the American League. The White Sox players all intensely disliked Comiskey and his penurious ways, but were powerless to do anything, thanks to baseball's so-called \"reserve clause\" that prevented players from switching teams without their team owner's consent.\nBy late 1919, Comiskey's tyrannical reign over the Sox had sown deep bitterness among the players, and White Sox first baseman Arnold \"Chick\" Gandil decided to conspire to throw the 1919 World Series. He persuaded gambler Joseph \"Sport\" Sullivan, with whom he had had previous dealings, that the fix could be pulled off for $100,000 total (which would be equal to $ today), paid to the players involved. New York gangster Arnold Rothstein supplied the $100,000 that Gandil had requested through his lieutenant Abe Attell, a former featherweight boxing champion.\nAfter the 1919 series, and through the beginning of the 1920 baseball season, rumors swirled that some of the players had conspired to purposefully lose. At last, in 1920, a grand jury was convened to investigate these and other allegations of fixed baseball games. Eight players (Charles \"Swede\" Risberg, Arnold \"Chick\" Gandil, \"Shoeless\" Joe Jackson, Oscar \"Happy\" Felsch, Eddie Cicotte, George \"Buck\" Weaver, Fred McMullin, and Claude \"Lefty\" Williams) were indicted and tried for conspiracy. The players were ultimately acquitted.\nHowever, the damage to the reputation of the sport of baseball led the team owners to appoint Federal judge Kenesaw Mountain Landis to be the first commissioner of baseball. His first act as commissioner was to ban the \"Black Sox\" from professional baseball for life. The White Sox, meanwhile,\nwould not return to the World Series until 1959, and it was not until their next appearance in 2005 they won the World Series.\nThe Negro leagues.\nUntil July 5, 1947, baseball had two histories. One fills libraries, while baseball historians are only just beginning to chronicle the other fully: African Americans have played baseball as long as white Americans. Players of color, both African-American and Hispanic, played for white baseball clubs throughout the very early days of the growing amateur sport. Moses Fleetwood Walker is considered the first African American to play at the major league level, in 1884. But soon, and dating through the first half of the 20th century, an unwritten but iron-clad color line fenced African-Americans and other players of color out of the \"majors\".\nThe Negro leagues were American professional baseball leagues comprising predominantly African-American teams. The term may be used broadly to include professional black teams outside the leagues and it may be used narrowly for the seven relatively successful leagues beginning 1920 that are sometimes termed \"Negro major leagues\".\nThe first professional team, established in 1885, achieved great and lasting success as the Cuban Giants, while the first league, the National Colored Base Ball League, failed in 1887 after only two weeks due to low attendance. The Negro American League of 1951 is considered the last major league season and the last professional club, the Indianapolis Clowns, operated amusingly rather than competitively from the mid-1960s to 1980s.\nThe first international leagues.\nWhile many of the players that made up the black baseball teams were African Americans, many more were Latin Americans (mostly, but not exclusively, black), from nations that deliver some of the greatest talents that make up the Major League rosters of today. Black players moved freely through the rest of baseball, playing in Canadian Baseball, Mexican Baseball, Caribbean Baseball, and Central America and South America, where more than a few achieved a level of fame that was unavailable in the country of their birth.\nBabe Ruth and the end of the dead-ball era.\nIt was not the Black Sox scandal which put an end to the dead-ball era, but a rule change and a single player.\nSome of the increased offensive output can be explained by the 1920 rule change that outlawed tampering with the ball. Pitchers had developed a number of techniques for producing \"spitballs\", \"shine balls\" and other trick pitches which had \"unnatural\" flight through the air. Umpires were now required to put new balls into play whenever the current ball became scuffed or discolored. This rule change was enforced all the more stringently following the death of Ray Chapman, who was struck in the temple by a pitched ball from Carl Mays in a game on August 16, 1920; he died the next day. Discolored balls, harder for batters to see and therefore harder for batters to dodge, have been rigorously removed from play ever since. This meant that batters could now see and hit the ball with less difficulty. With the added prohibition on the ball being purposely wetted or scuffed in any way, pitchers had to rely on pure athletic skill\u2014changes in grip, wrist angle, arm angle and throwing dynamics, plus a new and growing appreciation of the aerodynamic effect of the spinning ball's seams\u2014to pitch with altered trajectories and hopefully confuse or distract batters.\nAt the end of the 1919 season Harry Frazee, then owner of the Boston Red Sox, sold a group of his star players to the New York Yankees. Among them was George Herman Ruth, known affectionately as \"Babe\". Ruth's career mirrors the shift in dominance from pitching to hitting at this time. He started his career as a pitcher in 1914, and by 1916 was considered one of the dominant left-handed pitchers in the game. When Edward Barrow, managing the Red Sox, converted him to an outfielder, ballplayers and sportswriters were shocked. It was apparent, however, that Ruth's bat in the lineup every day was far more valuable than Ruth's arm on the mound every fourth day. Ruth swatted 29 home runs in his last season in Boston. The next year, as a Yankee, he would hit 54 and in 1921 he hit 59. His 1927 mark of 60 home runs would last until 1961.\nRuth's power hitting ability demonstrated a dramatic new way to play the game, one that was extremely popular with fans. Accordingly, ballparks were expanded, sometimes by building outfield \"bleacher\" seating which shrunk the size of the outfield and made home runs more frequent. In addition to Ruth, hitters such as Rogers Hornsby also took advantage, with Hornsby compiling extraordinary figures for both power and average in the early 1920s. By the late 1920s and 1930s all the good teams had their home-run hitting \"sluggers\": the Yankees' Lou Gehrig, Jimmie Foxx in Philadelphia, Hank Greenberg in Detroit and in Chicago Hack Wilson were the most storied. While the American League championship, and to a lesser extent the World Series, would be dominated by the Yankees, there were many other excellent teams in the inter-war years. The National League's St. Louis Cardinals, for example, would win three titles in nine years, the last with a group of players known as the \"Gashouse Gang\".\nThe first radio broadcast of a baseball game was on August 5, 1921, over Westinghouse station KDKA from Forbes Field in Pittsburgh. Harold Arlin announced the Pirates\u2013Phillies game. Attendances in the 1920s were consistently better than they had been before WWI. The interwar peak average attendance was 8,211 in 1930, but baseball was hit hard by the Great Depression and in 1933 the average fell below five thousand for the only time between the wars. At first wary of radio's potential to impact ticket sales at the park, owners began to make broadcast deals and by the late 1930s, all teams' games went out over the air.\n1933 also saw the introduction of the yearly All-Star game, a mid-season break in which the greatest players in each league play against one another in a hard-fought but officially meaningless demonstration game. In 1936 the Baseball Hall of Fame in Cooperstown, New York, was instituted and five players elected: Ty Cobb, Walter Johnson, Christy Mathewson, Babe Ruth and Honus Wagner. The Hall formally opened in 1939 and, of course, remains open to this day.\nThe war years.\nIn 1941, a year which saw the premature death of Lou Gehrig, Boston's great left fielder Ted Williams had a batting average over .400\u2014the last time anyone has achieved that feat. During the same season Joe DiMaggio hit successfully in 56 consecutive games, an accomplishment both unprecedented and unequaled.\nAfter the United States entered World War II after the attack on Pearl Harbor, Landis asked Franklin D. Roosevelt whether professional baseball should continue during the war. In the \"Green Light Letter\", the US president replied that baseball was important to national morale, and asked for more night games so day workers could attend. Thirty-five Hall of Fame members and more than 500 Major League Baseball players served in the war, but with the exception of D-Day, games continued. Both Williams and DiMaggio would miss playing time in the services, with Williams also flying later in the Korean War. During this period Stan Musial led the St. Louis Cardinals to the 1942, 1944 and 1946 World Series titles. The war years also saw the founding of the All-American Girls Professional Baseball League.\nBaseball boomed after World War II. 1945 saw a new attendance record and the following year average crowds leapt nearly 70% to 14,914. Further records followed in 1948 and 1949, when the average reached 16,913. While average attendances slipped to somewhat lower levels through the 1950s, 1960s and the first half of the 1970s, they remained well above pre-war levels, and total seasonal attendance regularly hit new highs from 1962 onward as the number of major league teams\u2014and games\u2014increased.\nRacial integration in baseball.\nThe post-War years in baseball also witnessed the racial integration of the sport. Participation by African Americans in organized baseball had been precluded since the 1890s by formal and informal agreements, with only a few players being surreptitiously included in lineups on a sporadic basis.\nAmerican society as a whole moved toward integration in the post-War years, partially as a result of the distinguished service by African American military units such as the Tuskegee Airmen, 366th Infantry Regiment, and others. During the baseball winter meetings in 1943, noted African-American athlete and actor Paul Robeson campaigned for integration of the sport. After World\u00a0War\u00a0II ended, several team managers considered recruiting members of the Negro leagues for entry into organized baseball. In the early 1920s, New York Giants' manager John McGraw tried to slip a black player, Charlie Grant, into his lineup (reportedly by passing him off to the front office as an Indian), and McGraw's wife reported finding names of dozens of black players that McGraw fantasized about signing, after his death. Pittsburgh Pirates owner Bill Bensawanger reportedly signed Josh Gibson to a contract in 1943, and the Washington Senators were also said to be interested in his services. But those efforts (and others) were opposed by Kenesaw Mountain Landis, baseball's powerful commissioner and a staunch segregationist. Bill Veeck claimed that Landis blocked his purchase of the Philadelphia Phillies because he planned to integrate the team. While this account is disputed, Landis was in fact opposed to integration, and his death in 1944 (and subsequent replacement as Commissioner by Happy Chandler) removed a major obstacle for black players in the Major Leagues.\nThe general manager who would be eventually successful in breaking the color barrier was Branch Rickey of the Brooklyn Dodgers. Rickey himself had experienced the issue of segregation. While playing and coaching for his college team at Ohio Wesleyan University, Rickey had a black teammate named Charles Thomas. On a road trip through southern Ohio, his fellow player was refused a room in a hotel. Although Rickey was able to get the player into his room for that night, he was taken aback when he reached his room to find Thomas upset and crying about this injustice. Rickey related this incident as an example of why he wanted a full desegregation of not only baseball, but the entire nation.\nIn the mid-1940s, Rickey had compiled a list of Negro league ballplayers for possible Major League contracts. Realizing that the first African-American signee would be a magnet for prejudiced sentiment, however, Rickey was intent on finding a player with the distinguished personality and character that would allow him to tolerate the inevitable abuse. Rickey's sights eventually settled on Jackie Robinson, a shortstop with the Kansas City Monarchs. Although probably not the best player in the Negro leagues at the time, Robinson was an exceptional talent, was college-educated, and had the marketable distinction of having served as an officer during World War II. Even more importantly, Rickey judged Robinson to possess the inner strength to withstand the inevitable harsh animosity to come. To prepare him for the task, Rickey played Robinson in 1946 for the Dodgers' minor league team, the Montreal Royals, which proved an arduous emotional challenge, though Robinson enjoyed fervently enthusiastic support from the Montreal fans. On April 15, 1947, Robinson broke the color barrier, which had been tacitly recognized for almost 75 years, with his appearance for the Brooklyn Dodgers at Ebbets Field.\nEleven weeks later, on July 5, 1947, the American League was integrated by the signing of Larry Doby to the Cleveland Indians. Over the next few years, a handful of black baseball players made appearances in the majors, including Roy Campanella (teammate to Robinson in Brooklyn) and Satchel Paige (teammate to Doby in Cleveland). Paige, who had pitched more than 2,400 innings in the Negro leagues, sometimes two and three games a day, was still effective at 42, and still playing at 59. His ERA in the Major Leagues was 3.29.\nHowever, the initial pace of integration was slow. By 1953, only six of the sixteen major league teams had a black player on the roster. The Boston Red Sox became the last major league team to integrate its roster with the addition of Pumpsie Green on July 21, 1959. While limited in numbers, the on-field performance of early black Major League players was outstanding. In the fourteen years from 1947 to 1960, black players won one or more of the Rookie of the Year awards nine times.\nWhile never prohibited in the same fashion as African Americans, Latin American players also benefitted greatly from the integration era. In 1951, two Chicago White Sox, Venezuelan-born Chico Carrasquel and Cuban-born (and black) Minnie Mi\u00f1oso, became the first Hispanic All-Stars.\nAccording to some baseball historians, Jackie Robinson and the other African-American players helped reestablish the importance of baserunning and similar elements of play that were previously de-emphasized by the predominance of power hitting.\nFrom 1947 to the 1970s, African-American participation in baseball rose steadily. By 1974, 27% of baseball players were African American. As a result of this on-field experience, minorities began to experience long-delayed gains in managerial positions within baseball. In 1975, Frank Robinson (who had been the 1956 Rookie of the Year with the Cincinnati Reds) was named player-manager of the Cleveland Indians, making him the first African-American manager in the major leagues.\nAlthough these front-office gains continued, Major League Baseball saw a lengthy slow decline in the percentage of black players after the mid-1970s. By 2007, African Americans made up less than 9% of Major League players. While this trend is largely attributed to an increased emphasis on recruitment of players from Latin America (with the number of Hispanic players in the major leagues rising to 29% by 2007), other factors have been cited as well. Hall of Fame player Dave Winfield, for instance, has pointed out that urban America provides fewer resources for youth baseball than in the past. Despite this continued prevalence of Hispanic players, the percentage of black players rose again in 2008 to 10.2%.\nArturo Moreno became the first Hispanic owner of an MLB franchise when he purchased the Anaheim Angels in 2004.\nIn 2005, a Racial and Gender Report Card on Major League Baseball was issued, which generally found positive results on the inclusion of African Americans and Latinos in baseball, and gave Major League Baseball a grade of \"A\" or better for opportunities for players, managers and coaches as well as for MLB's central office. At that time, 37% of major league players were people of color: Latino (26 percent), African American (9 percent) or Asian (2 percent). Also by 2004, 29% of the professional staff in MLB's central office were people of color, 11% of team vice presidents were people of color, and seven of the league's managers were of color (four African Americans and three Latinos).\nExpansion era.\nBaseball had been in the West for almost as long as the National League and the American League had been around. It evolved into the Pacific Coast League (PCL), which included the Hollywood Stars, Los Angeles Angels, Oakland Oaks, Portland Beavers, Sacramento Solons, San Francisco Seals, San Diego Padres, Seattle Rainiers.\nThe PCL was huge in the West. A member of the National Association of Professional Baseball Leagues, it kept losing great players to the National and the American leagues for less than $8,000 a player.\nThe PCL was far more independent than the other \"minor\" leagues, and rebelled continuously against their Eastern masters. Clarence Pants Rowland, the President of the PCL, took on baseball commissioners Kenesaw Mountain Landis and Happy Chandler at first to get better equity from the major leagues, then to form a third major league. His efforts were rebuffed by both commissioners. Chandler and several of the owners, who saw the value of the markets in the West, started to plot the extermination of the PCL. They had one thing that Rowland did not: The financial power of the Eastern major league baseball establishment.\nNo one was going to back a PCL club building a major-league size stadium if the National or the American League was going to build one too, which discouraged investment in PCL ballparks. PCL games and rivalries still drew fans, but the leagues' days of dominance in the West were numbered.\n1953\u20131955.\nUntil the 1950s, major league baseball franchises had been largely confined to the northeastern United States, with the teams and their locations remaining unchanged from 1903 to 1952. The first team to relocate in fifty years was the Boston Braves, who moved in 1953 to Milwaukee, where the club set attendance records. In 1954, the St. Louis Browns moved to Baltimore and were renamed the Baltimore Orioles. These relocations can be seen as a full-circle ending to the classic era, which began with the moves of teams \"from\" Milwaukee and Baltimore. In 1955, the Philadelphia Athletics moved to Kansas City.\nNational League Baseball leaves New York.\nIn 1958 the New York market ripped apart. The Yankees were becoming the dominant draw, and the cities of the West offered generations of new fans in much more sheltered markets for the other venerable New York clubs, the Brooklyn Dodgers and the New York Giants. Placing these storied, powerhouse clubs in the two biggest cities in the West had the specific design of crushing any attempt by the PCL to form a third major league. Eager to bring these big names to the West, Los Angeles gave Walter O'Malley, owner of the Dodgers, a helicopter tour of the city and asked him to pick his spot. The Giants were given the lease of the PCL San Francisco Seals while Candlestick Park was built for them.\nCalifornia.\nThe logical first candidates for major league \"expansion\" were the same metropolitan areas that had just attracted the Dodgers and Giants. It is said that the Dodgers and Giants\u2014National League rivals in New York City\u2014chose their new cities because Los Angeles (in southern California) and San Francisco (in northern California) already had a fierce rivalry (geographical, economic, cultural and political), dating back to the state's founding. The only California expansion team\u2014and also the first in Major League Baseball in over 70 years\u2014was the Los Angeles Angels (later the California Angels, the Anaheim Angels, Los Angeles Angels of Anaheim, before reverting to Los Angeles Angels in 2016), who brought the American League to southern California in 1961. Northern California, however, would later gain its own American League team, in 1968, when the Athletics would move again, settling in Oakland, across San Francisco Bay from the Giants.\n1961\u20131962.\nAlong with the Angels, the other 1961 expansion team was the Washington Senators, who joined the American League and took over the nation's capital when the moved to Minnesota and became the Twins. 1961 is also noted as being the year in which Roger Maris surpassed Babe Ruth's single season home run record, hitting 61 for the New York Yankees, albeit in a slightly longer season than Ruth's. To keep pace with the American League\u2014which now had ten teams\u2014the National League likewise expanded to ten teams, in 1962, with the addition of the Houston Colt .45s and New York Mets.\n1969.\nIn 1969, the American League expanded when the Kansas City Royals and Seattle Pilots, the latter in a longtime PCL stronghold, were admitted to the league. The Pilots stayed just one season in Seattle before moving to Milwaukee and becoming today's Milwaukee Brewers. The National League also added two teams that year, the Montreal Expos and San Diego Padres. Given the size of the expanded leagues, 12 teams apiece, each split into East and West divisions, with a playoff series to determine the pennant winner and World Series contender\u2014the first post-season baseball instituted since the advent of the World Series itself.\nThe Padres were the last of the core PCL teams to be absorbed. The Coast League did not die, though. After reforming and moving into new markets, it successfully transformed into a Class AAA league.\n1972\u20132013.\nIn 1972, the second Washington Senators moved to the Dallas\u2013Fort Worth area and became the Texas Rangers.\nIn 1977, the American League expanded to fourteen teams, with the newly formed Seattle Mariners and Toronto Blue Jays. Sixteen years later, in 1993, the National League likewise expanded to fourteen teams, with the newly formed Colorado Rockies and Florida Marlins (now Miami Marlins).\nBeginning with the 1994 season, both the AL and the NL were divided into three divisions (East, West, and Central), with the addition of a wild card team (the team with the best record among those finishing in second place) to enable four teams in each league to advance to the preliminary division series. However, due to the 1994\u201395 Major League Baseball strike (which canceled the 1994 World Series), the new rules did not go into effect until the 1995 World Series.\nIn 1998, the AL and the NL each added a fifteenth team, for a total of thirty teams in Major League Baseball. The Arizona Diamondbacks joined the National League, and the Tampa Bay Devil Rays\u2014now called simply the Rays\u2014joined the American League. In order to keep the number of teams in each league at an even number\u2014with 14 in the AL and 16 in the NL\u2014Milwaukee changed leagues and became a member of the National League. Two years later, the NL and AL ended their independent corporate existences and merged into a new legal entity named Major League Baseball; the two leagues remained as playing divisions. In 2001, MLB took over the struggling Montreal Expos franchise and, after the 2004 season, moved it to Washington, DC, which had been clamoring for a team ever since the second Senators' departure in 1972; the club was renamed the Nationals.\nIn 2013, in keeping with Commissioner Bud Selig's desire for expanded interleague play, the Houston Astros were shifted from the National to the American League; with an odd number (15) in each league, an interleague contest was played somewhere almost every day during the season. At this time the divisions within each league were shuffled to create six equal divisions of five teams.\nPitching dominance and rules changes.\nBy the late 1960s, the balance between pitching and hitting had swung back to favor of the pitchers once more. In 1968 Carl Yastrzemski won the American League batting title with an average of just .301, the lowest in history. That same year, Detroit Tigers pitcher Denny McLain won 31 games\u2014making him the last pitcher to win 30 games in a season. St. Louis Cardinals starting pitcher Bob Gibson achieved an equally remarkable feat by allowing an ERA of just 1.12.\nIn response to these events, major league baseball implemented certain rule changes in 1969 to benefit the batters. The pitcher's mound was lowered, and the strike zone was reduced.\nIn 1973 the American League, which had been suffering from much lower attendance than the National League, made a move to increase scoring even further by initiating the designated hitter rule.\nPlayers assert themselves.\nFrom the time of the formation of the Major Leagues to the 1960s, the team owners controlled the game. After the so-called \"Brotherhood Strike\" of 1890 and the failure of the Brotherhood of Professional Base Ball Players and its Players National League, the owners' control of the game seemed absolute. It lasted over 70 years despite a number of short-lived players organizations. In 1966, however, the players enlisted the help of labor union activist Marvin Miller to form the Major League Baseball Players Association (MLBPA). The same year, Sandy Koufax and Don Drysdale\u2014both Cy Young Award winners for the Los Angeles Dodgers\u2014refused to re-sign their contracts, jointly holding out for better contracts. The era of the reserve clause, which held players to one team, was drawing to an end.\nThe first legal challenge came in 1970. Backed by the MLBPA, St. Louis Cardinals outfielder Curt Flood took the leagues to court to negate a player trade, citing the 13th Amendment and antitrust legislation. In 1972, he finally lost his case before the United States Supreme Court by a vote of 5 to 3, but gained large-scale public sympathy, and the damage had been done. The reserve clause survived, but it had been irrevocably weakened. In 1975, Andy Messersmith of the Dodgers and Dave McNally of the Montreal Expos played without contracts, and then declared themselves free agents in response to an arbitrator's ruling. Handcuffed by concessions made in the Flood case, the owners had no choice but to accept the collective bargaining package offered by the MLBPA, and the reserve clause was effectively ended, to be replaced by the current system of free-agency and arbitration.\nWhile the legal challenges were going on, the game continued. In 1969, the \"Miracle Mets\", just seven years after their formation, recorded their first winning season, won the National League East and finally the World Series.\nOn the field, the 1970s saw some of the longest-standing records fall, along with the rise of two powerhouse dynasties. In Oakland, the Swinging A's were overpowering, winning the Series in 1972, 1973 and 1974, and five straight division titles. The strained relationships between teammates, who included Catfish Hunter, Vida Blue and Reggie Jackson, gave the lie to the need for \"chemistry\" between players. The National League, on the other hand, belonged to the Big Red Machine in Cincinnati, where Sparky Anderson's team, which included Pete Rose as well as Hall of Famers Tony P\u00e9rez, Johnny Bench and Joe Morgan, succeeded the A's run in 1975.\nThe decade also contained great individual achievements. On April 8, 1974, Hank Aaron of the Atlanta Braves hit his 715th career home run, surpassing Babe Ruth's all-time record. He would retire in 1976 with 755, and that was just one of numerous records he achieved, many of which, including total bases, still stand today. There was great pitching too: between 1973 and 1975, Nolan Ryan threw four \"no-hit\" games. He would add a record-breaking fifth in 1981 and two more before his retirement in 1993, by which time he had also accumulated 5,714 strikeouts, another record, in a 27-year career.\nThe marketing and hype era.\nFrom the 1980s onward, the major league game changed dramatically, due to the combined effects of free agency, improvements in the science of sports conditioning, changes in the marketing and television broadcasting of sporting events, and the push by brand-name products for greater visibility. These events lead to greater labor difficulties, fan disaffection, rapidly rising prices, changes in game-play, and problems with the use of performance-enhancing substances like steroids tainting the race for records. In spite of all this, stadium crowds generally grew. Average attendances first broke 20,000 in 1979 and 30,000 in 1993. That year total attendance hit 70 million, but baseball was hit hard by a strike in 1994, and as of 2005 it had marginally improved on those 1993 records. (Update: Between 2009 and 2017, average attendance hovered just over the 30,000 mark, with numbers falling into the 28,000s in '18 and '19. The 2019 season saw a million fewer tickets sold than the banner year of 2007, however revenues to major league baseball from media rights fees increased total revenue to $10 billion in 2018, a 70% rise from a decade before.)\nThe science of the sport changes the game.\nDuring the 1980s, significant advances were made in the science of physical conditioning. Weight rooms and training equipment were improved. Trainers and doctors developed better diets and regimens to make athletes bigger, healthier, and stronger than they had ever been.\nAnother major change that had been occurring during this time was the adoption of the pitch count. Starting pitchers who played complete games had not been an unusual thing in baseball's history. Now, pitchers were throwing harder than ever and pitching coaches watched to see how many pitches a player had thrown over the game. At anywhere from 100 to 125, pitchers increasingly would be pulled out to preserve their arms. Bullpens began to specialize more, with more pitchers being trained as middle relievers, and a few hurlers, usually possessing high velocity but not much durability, as closers. The science of maximizing effectiveness and career duration, while attempting to minimize injury and downtime, is an ongoing pursuit by coaches and kinesiologists.\nAlong with the expansion of teams, the addition of more pitchers needed to play a complete game stressed the total number of quality players available in a system that restricted its talent searches at that time to America, Canada, Latin America, and the Caribbean.\nTelevision.\nThe arrival of live televised sports in the 1950s increased attention and revenue for all major league clubs at first. The television programming was extremely regional, hurting the non-televised minor and independent leagues most. People stayed home to watch Maury Wills rather than watch unknowns at their local baseball park. Major League Baseball, as it always did, made sure that it controlled rights and fees charged for the broadcasts of all games, just as it had on radio.\nThe national networks began televising national games of the week, opening the door for a national audience to see particular clubs. While most teams were broadcast in the course of a season, emphasis tended toward the league leaders with famous players and the major market franchises that could draw the largest audience.\nThe rise of cable.\nIn the 1970s the cable revolution began. The Atlanta Braves became a power contender with greater revenues generated by WTBS, Ted Turner's Atlanta-based Super-Station, broadcast as \"America's Team\" to cable households nationwide. The roll-out of ESPN, then regional sports networks (now mostly under the umbrella of Fox Sports Net) changed sports news in general and particularly baseball with its relatively huge number of games-per-season. Now under the microscope of news organizations that needed to fill 24 programming hours per day, the amount of attention\u2014and salary\u2014paid to major league players grew exponentially. Players who would have sought off-season jobs to make ends meet just 20 years earlier were now well-paid professionals at least, and multi-millionaires in many cases. This super-star status often rested on careers that were not as compelling as those of the baseball heroes of a less media-intense time.\nAs player contract values soared, and the number of broadcasters, commentators, columnists, and sports writers also multiplied. The competition for a fresh angle on any story became fierce. Media pundits began questioning the high salaries paid to players when on-field performance was deemed less than deserving. Critical commentary was more of a draw than praise, and coverage began to become intensely negative. Players' personal lives, which had always been off-limits except under extreme circumstances, became the fodder of editorials, insider stories on TV, and features in magazines. When the use of performance-enhancing drugs became an issue, drawing scornful criticism from fans and pundits, the gap between the sports media and the players whom they covered widened further.\nWith the development of satellite television and digital cable, Major League Baseball launched channels with season-subscription fees, making it possible for fans to watch virtually every game played, in both major leagues, everywhere, in real time.\nTeam networks.\nThe next refinement of baseball on cable was the creation of single-team cable networks. YES Network &amp; NESN, the New York Yankees &amp; Boston Red Sox cable television networks, respectively, took in millions to broadcast games not only in New York and Boston but around the country. These networks generated as much revenue as, or more than, revenue annually for large-market teams' baseball operations. By fencing these channels off in separate corporate entities, owners were able to exclude the income from consideration during contract negotiations.\nMerchandise, endorsements and sponsorships.\nThe first merchandise produced in response to the growing popularity of the game was the baseball trading card. The earliest known player cards were produced in 1868 by a pair of New York baseball-equipment purveyors. Since that time, many enterprises, notably tobacco and candy companies, have used trading cards to promote and sell their products. These cards rarely, if ever, provided any benefit directly to the players, but a growing mania for collecting and trading cards helped personalize baseball, giving some fans a more personal connection to their favorite players and introducing them to new ones. Eventually, older cards became \u201cvintage\u201d and rare cards gained in value until the secondary market for trading cards became a billion-dollar industry in itself, with the rarest individuals bringing mid-six-figures to millions of dollars at auction. The advent of the Internet and websites such as eBay provided huge new venues for buyers, sellers and traders, some of whom have made baseball cards their living.\nIn recent years baseball cards have disassociated from unrelated products like tobacco and bubble-gum, to become products in their own right. Following the exit of competitor Donruss from the baseball-card industry, former bubble-gum giants Topps and Fleer came to dominate that market through exclusive contracts with players and Major League Baseball. Fleer, in turn, exited the market in 2007, leaving Topps as the only card manufacturer with an MLB contract.\nOther genuine baseball memorabilia also trades and sells, often at high prices. Much of what is for sale as \"memorabilia\" is manufactured strictly for sale and rarely has a direct connection to teams or players beyond the labeling, unless signed in person by a player. Souvenir balls caught by fans during important games, especially significant home run balls, have great rarity value, and balls signed by players have always been treasured, traded and sold. The high value of autographs has created new businessmen whose sole means of making a living was acquiring autographs and memorabilia from the athletes. Memorabilia hounds fought with fans to get signatures worth $20, $60, or even $100 or more in their inventory.\nOf great value to individual top players are endorsement contracts wherein the player's fame is used to sell anything from sports equipment to automobiles, soda and underwear. Top players can receive as much as a million dollars a year or more directly from the companies.\nIn deals with players, teams and Major League Baseball, large corporations like NIKE and Champion pay big money to make sure that their logos are seen on the clothing and shoes worn by athletes on the field. This \"association branding\" has become a significant revenue stream. In the late 1990s and into the 21st century, the dugout, the backstops behind home plate, and anywhere else that might be seen by a camera, became fair game for the insertion of advertising.\nPlayer wealth.\nBeginning with the 1972 \"Flood v. Kuhn\" Supreme Court case, management's grip on players, as embodied in the reserve clause, began to slip. In 1976, the Messersmith/McNally Arbitration, also known as the Seitz Decision effectively destroyed the reserve clause. Players who had been dramatically underpaid for generations came to be replaced by players who were paid extremely well for their services.\nSports agents.\nA new generation of sports agents arose, hawking the talents of free-agent players who knew baseball but didn't know the business end of the game. The agents broke down what the teams were generating in revenue off of the players' performances. They calculated what their player might be worth to energize a television contract, or provide more merchandise revenue, or put more fans into stadium seats. Management pushed back; the dynamic produced a variety of compromises which ideally left all parties unsatisfied.\nBusiness.\nUnder the Major League Baseball contract, players must play for minimum salary for six years, at which time they become free agents. With players seeking greener pastures when their six years had passed, fewer players remained career members of one ball club. Large-market clubs like the New York Yankees, the Boston Red Sox, and the Los Angeles Dodgers, given big revenues from their cable television operations, signed more and more of the best\u2014and best-known\u2014players away from mid-sized and smaller-market clubs that could not afford to compete on salaries. Major League Baseball, unlike many other sports, does not impose a salary cap on teams. The League does attempt to level the field, as it were, by imposing a luxury tax on teams with very high payrolls, but management is still free to pay players whatever they can afford to attract talent. Some television reporters, commentators, and print sports writers question the kind of money being paid to these players, but just as many on the other side of the debate feel players should bargain for whatever they can get. Still others complain that minor-league players are not fairly compensated by MLB. The tug-of-war between players and management is complex, ongoing, and of great interest to serious students of the professional game.\nOwners and players feud in the 1980s.\nAll was not well with major league baseball. The many contractual disputes between players and owners came to a head in 1981. Previous players' strikes (in 1972, 1973 and 1980) had been held in preseason, with only the 1972 stoppage\u2014over benefits\u2014causing disruption to the regular season from April 1 to April 13. Also, in 1976 the owners had locked the players out of Spring training in a dispute over free agency.\nThe crux of the 1981 dispute was compensation for the loss of players to free agency. After seeing a top-rank player sign with another team, the aggrieved owner wanted a mid-rank player in return, the so-called \"sixteenth player\" (each club was allowed to protect 15 players from this rule). Under this arrangement, losing lower-rated free agents would produce correspondingly smaller compensation. While this seemed reasonable and fair to owners, players only recently freed from the bondage of the reserve clause found it unacceptable, and withdrew their labor, striking on June 12. Immediately, the U.S. Government's National Labor Relations Board ruled that the owners had not been negotiating in good faith, and installed a federal mediator to reach a solution. Seven weeks and 713 games were lost in the middle of the season, before the owners backed down on July 31, settling for proportionally lower-ranked players as compensation. The damaged season was continued as distinct halves starting August 9, with the playoffs reorganized to reflect this.\nThroughout the 1980s then, baseball seemed to prosper. The competitive balance between franchises saw fifteen different teams make the World Series, and produced nine different champions during the decade. Also, every season from 1978 through 1987 saw a different World Series winner, a streak unprecedented in baseball history. Turmoil was, however, just around the corner. In 1986, Pete Rose retired from playing for the Cincinnati Reds, having broken Ty Cobb's record by accumulating 4,256 hits during his career. He continued as Reds manager until, in 1989 it was revealed that he was being investigated for sports gambling, including the possibility that he had bet on teams with which he was involved. While Rose admitted a gambling problem, he denied having bet on baseball. Federal prosecutor John Dowd investigated and, on his recommendation, Rose was banned from organised baseball, a move which precluded his possible inclusion in the Hall of Fame. In a meeting with Commissioner Giamatti, and having failed in a legal action to prevent it, Rose accepted his punishment. It was, essentially, the same fate that had befallen the Black Sox seventy years previously. (Rose, however, would continue to deny that he bet on baseball until he finally confessed to it in his 2004 autobiography.)\n1994\u201395 Major League Baseball strike.\nLabor relations were still strained. There had been a two-day strike in 1985 (over the division of television revenue money), and a 32-day spring training lockout in 1990 (again over salary structure and benefits). By far the worst action would come in 1994. The seeds were sown earlier: in 1992 the owners sought to renegotiate salary and free-agency terms, but little progress was made. The standoff continued until early 1994 when the existing agreement expired, with no agreement on what was to replace it. Adding to the conflict was the perception that \"small market\" teams, such as the struggling Seattle Mariners could not compete with high-spending teams such as those in New York or Los Angeles. Their plan was to institute TV revenue sharing to increase equity among the teams and impose a salary cap to keep expenditures down. Players felt that such a cap would reduce their potential earnings. It wasn't until later, in 2003, that MLB instituted a luxury tax on high-spending teams in an attempt to encourage more equitable player outlays.\nMeanwhile, back in 1994, players officially went on strike on August 12. In September 1994, Major League Baseball announced the cancellation of the World Series for the first time since 1904.\nHome run mania and the second coming of baseball.\nThe cancellation of the 1994 World Series was a severe embarrassment for Major League Baseball. Fans were outraged and frustrated, their love of the game shaken to its core. The strike was declared an act of war, and fought back: attendance figures and broadcast ratings were lower in 1995 than before the strike. It would be a decade before baseball recovered from the disruption.\nOn September 6, 1995, Baltimore Orioles shortstop, Cal Ripken Jr., played his 2,131st consecutive game, breaking Lou Gehrig's 56-year-old record. This was the first celebratory moment in baseball after the strike. Ripken continued his streak for another three years, voluntarily ending it at 2,632 consecutive games played on September 20, 1998.\nIn 1997, the expansion Florida Marlins won the World Series in just their fifth season. This made them the third-youngest team to win the Fall Classic (behind the 1903 Boston Red Sox and later the 2001 Arizona Diamondbacks, who won in their fourth season). Virtually all the key players on the 1997 Marlins team were soon traded or let go to save payroll costs (although the 2003 Marlins did win a second world championship).\nIn 1998, St. Louis Cardinals first baseman Mark McGwire and Chicago Cubs outfielder Sammy Sosa engaged in a home run race for the ages. With both rapidly approaching Roger Maris's record of 61 home runs (set in 1961), the entire nation watched as the two power hitters raced to be the first to break into uncharted territory. McGwire reached 62 first on September 8, 1998, with Sosa right behind. Sosa finished the season with 66 home runs, well behind McGwire's unheard-of 70. However, recent steroid allegations have marred the season in the minds of many fans.\nThat same year, the New York Yankees won a record 125 games, including going 11\u20132 in the postseason, to win the World Series as what many consider to be one of the greatest teams of all time.\nMcGwire's record of 70 would last a mere three years following the meteoric rise of veteran San Francisco Giants left fielder Barry Bonds in 2001. In 2001 Bonds knocked out 73 home runs, breaking the record set by McGwire by hitting his 71st on October 5, 2001. In addition to the home run record, Bonds also set single-season marks for base on balls with 177 (breaking the previous record of 170, set by Babe Ruth in 1923) and slugging percentage with .863 (breaking the mark of .847 set by Ruth in 1920). Bonds continued his torrid home run hitting in the next few seasons, hitting his 660th career home run on April 12, 2004, tying him with his godfather Willie Mays for third place on the all-time career home runs list. He hit his 661st home run the next day, April 13, to take sole possession of third place. Only three years later Bonds surpassed the great Hank Aaron to become baseball's most prolific home run hitter.\nHowever, none of Bonds's accomplishments in the 2000s have been without controversy. During his run, journalists questioned McGwire about his use of the steroid-precursor androstenedione, and in March 2005 he was unforthcoming when questioned as part of a Congressional inquiry into steroids. Bonds has also been dogged by allegations of steroid use and his involvement in the BALCO drugs scandal, as his personal trainer Greg Anderson pleaded guilty to supplying steroids (without naming Bonds as a recipient). Neither Bonds nor McGwire has failed a drug test at any time since there was no steroid-testing until 2003 after the new August 7, 2002, agreement between owners and players was reached. McGwire retired after the 2001 season; in 2010, he admitted to having used steroids throughout his MLB career.\nThe 1990s also saw Major League Baseball expand into new markets as four new teams joined the league. In 1993, the Colorado Rockies and Florida Marlins began play, and in just their fifth year of existence, the Marlins became the first wild card team to win the championship. \nThe year 1998 brought two more teams into the mix, the Tampa Bay Devil Rays and the Arizona Diamondbacks, the latter of which become the youngest expansion franchise to win the championship. \nThe late 1990s were dominated by the New York Yankees, who won four out of five World Series championships from 1996 to 2000.\nThe steroid era.\nDrugs, baseball, and records.\nThe lure of big money pushed players harder and harder to achieve peak performance, while avoiding injury from over-training. The wearying travel schedule and 162-game season meant that amphetamines, usually in the form of pep pills known as \"greenies\", had been widespread in baseball since at least the 1960s. Baseball's drug scene was no particular secret, having been discussed in \"Sports Illustrated\" and in Jim Bouton's groundbreaking book \"Ball Four\", but there was virtually no public backlash. Two decades later, however, some Major League players turned to newer performance-enhancing drugs, including ephedra and improved steroids. The eventual consequences for the game, the players and the fans were substantial.\nA memo circulated in 1991 by baseball commissioner Fay Vincent stated that \"The possession, sale or use of any illegal drug or controlled substance by Major League players and personnel is strictly prohibited ... [and those players involved] are subject to discipline by the commissioner and risk permanent expulsion from the game... This prohibition applies to all illegal drugs and controlled substances, including steroids\u2026\" Some general managers of the time do not remember this memo; it was not emphasized or enforced and, confusingly, Vincent himself has disclaimed any direct responsibility for a ban on steroids, saying, \"I didn't ban steroids...They were banned by Congress\".\nEphedra, an herb used to cure cold symptoms, and also used in some allergy medications, sped up the heart and was considered by some to be a weight-loss short-cut. In 2003, Baltimore Orioles pitcher Steve Bechler had come to training camp 10 pounds overweight. During a workout on February 16, Bechler complained of dizziness and fatigue. His condition worsened while resting in the clubhouse and he was transported to an ambulance on a stretcher. Bechler spent the night in intensive care and died the following morning at the age of 23. The official cause of death was listed as \"multi-organ failure due to heat exhaustion\". The coroner's report stated it was likely that Bechler had taken three ephedra capsules on an empty stomach prior to working out. Many in the media linked Bechler's death to ephedra, raising concerns over the use of performance-enhancing drugs in baseball. Ephedra was banned, and soon the furor died down.\nThe 1998 home run race had generated nearly unbroken positive publicity, but Barry Bonds' run for the all-time home run record provoked a backlash over steroids, which increase a person's testosterone level and subsequently enable that person to bodybuild with much more ease. Some athletes have said that the main advantage to steroids is not so much the additional power or endurance that they can provide, but that they can drastically shorten rehab time from injury.\nCommissioner Bud Selig was criticized, mostly after-the-fact, for a slow response to the rising tide of steroid use in the 1990s. In the early 2000s, as a safe and effective test for anabolic steroids came online and sanctions for their use began to be strictly enforced, some players adopted the use of harder-to-detect human growth hormone (HGH) to increase stamina and strength. Selig, still acting with some caution, imposed a strict anti-drug policy upon its minor league players, who are not part of the Major League Baseball Players Association (the PA). Random drug testing, education and treatment, and strict penalties for those caught became the rule of law. Anyone on a Major League team's forty man roster, including 15 minor leaguers that are on that list, were exempt from that program. Eventually, Selig and MLB had strict rules in place that carried meaningful sanctions against players who \"juiced.\"\nIn a \"Sports Illustrated\" cover story in 2002, a year after his retirement, Ken Caminiti admitted that he had used steroids during his National League MVP-winning 1996 season, and for several seasons afterwards. Caminiti died unexpectedly of an apparent heart attack in The Bronx at the age of 41; he was pronounced dead on October 10, 2004, at New York's Lincoln Memorial Hospital. On November 1, the New York City Medical Examiners Office announced that Caminiti died from \"acute intoxication due to the combined effects of cocaine and opiates\", but possibly-steroid-induced coronary artery disease and cardiac hypertrophy (an enlarged heart) were also contributing factors.\nIn 2005, Jose Canseco published \"\", admitting steroid usage and claiming that it was prevalent throughout major league baseball. When the United States Congress decided to investigate the use of steroids in the sport, some of the game's most prominent players came under scrutiny for possibly using steroids. These include Barry Bonds, Jason Giambi, and Mark McGwire. Other players, such as Canseco and Gary Sheffield, have admitted to have either knowingly (in Canseco's case) or not (Sheffield's) using steroids. In confidential testimony to the BALCO Grand Jury (that was later leaked to the \"San Francisco Chronicle\"), Giambi also admitted steroid use. He later held a press conference in which he appeared to affirm this admission, without actually saying the words. And after an appearance before Congress where he (unlike McGwire) emphatically denied using steroids, \"period\", slugger Rafael Palmeiro became the first major star to be suspended (10 days) on August 1, 2005, for violating Major League Baseball's newly strengthened ban on controlled substances, including steroids, adopted on August 7, 2002, starting in the 2003 season. Many lesser players (mostly from the minor leagues) have tested positive for use, as well.\nIn 2006, Commissioner Selig tasked former United States Senator George J. Mitchell to lead an investigation into the use of performance-enhancing drugs in Major League Baseball (MLB) and on December 13, 2007, the 409-page Mitchell Report was released ('Report to the Commissioner of Baseball of an Independent Investigation into the Illegal Use of Steroids and Other Performance Enhancing Substances by Players in Major League Baseball'). The report described the use of anabolic steroids and human growth hormone (HGH) in MLB and assessed the effectiveness of the MLB Joint Drug Prevention and Treatment Program. Mitchell also advanced certain recommendations regarding the handling of past illegal drug use and future prevention practices. The report names 89 MLB players who are alleged to have used steroids or drugs.\nBaseball has been taken to task for turning a blind eye to its drug problems. It benefited from these drugs in the ever-increasingly competitive fight for airtime and media attention. For example, Commissioner Selig sent a personal representative to the 2007 game where Barry Bonds broke Hank Aaron's career home run record, even though Bonds was widely believed at the time to be a steroid user and had been named in connection with the then-ongoing BALCO scandal; many viewed this as Selig giving wink-and-a-nod tacit approval to the use of PEDs. MLB and its Players Association finally announced tougher measures, but many felt that they did not go far enough. \nIn December 2009, Sports Illustrated named Baseball's Steroid Scandal as the number one sports story of the decade of the 2000s. In 2013, no player from the first \"steroid class\" of players eligible for the Baseball Hall of Fame was elected. Bonds and Clemens received less than half the number of votes needed, and some voters stated that they would not vote for any first-time candidate who played during the steroid era\u2014whether accused of using banned substances or not\u2014because of the effect the substances had on baseball.\nThe BALCO steroids scandal.\nIn 2002, a major scandal arose when it was discovered that the company Bay Area Laboratory Co-operative (BALCO), owned by Victor Conte, had been producing so-called \"designer steroids\", (specifically \"the clear\" and \"the cream\") which are steroids that could not be detected through drug tests at that time. In addition, the company had connections to several San Francisco Bay Area sports trainers and athletes, including the trainers of Jason Giambi and Barry Bonds. This revelation led to a vast criminal investigation into BALCO's connections with athletes from baseball and many other sports. Among the many athletes who have been linked to BALCO are Olympic sprinters Tim Montgomery and Marion Jones, Olympic shot-putter C. J. Hunter, as well as Giambi and Bonds.\nGrand jury testimony in December 2003\u2014which was illegally leaked to the \"San Francisco Chronicle\" and published in December 2004 under the bylines of Mark Fainaru-Wada and Lance Williams\u2014revealed that the Bay Area Laboratory Cooperative did not merely manufacture nutritional supplements, but also distributed exotic steroids. Williams and Fairanu-Wada also provided compelling evidence that Barry Bonds, arguably the greatest player of his generation, was one of BALCO's steroid clients. The paper reported that these substances were probably designer steroids. Bonds said that Greg Anderson gave him a rubbing balm and a liquid substance that at the time he did not believe them to be steroids and thought they were flaxseed oil and other health supplements. Based on the testimony from many of the athletes, Conte and Anderson accepted plea agreements from the government in 2005, on charges they distributed steroids and laundered money, in order to avoid significant time in jail. Conte received a sentence of four months, Anderson received a sentence of three months. Also that year, James Valente, the vice-president of BALCO, and Remi Korchemny, a track coach affiliated with BALCO, pled guilty to distributing banned substances and received probation.\nVarious baseball pundits, fans, and even players have taken this as confirmation that Bonds used illegal steroids. Bonds never tested positive in tests performed in 2003, 2004, and 2005, which may be attributable to successful obfuscation of continued use as documented in the 2006 book \"Game of Shadows\". Before-and-after photos of Bonds, early in his career and late in his career, have led most fans to conclude that he must have used steroids to achieve such startling changes in his physique.\nThe Power Age.\nWhile the introduction of steroids certainly increased the power production of greats, there were other factors that drastically increased the power surge after 1994. The factors cited are: smaller sized ballparks than in the past, the \"juiced balls\" theory claiming that the balls are wound tighter thus travel further following contact with the bat, and \"watered down pitching\" implying that lesser quality pitchers are up in the Major Leagues due to too many teams. Albeit these factors did play a large role in increasing home run thus scoring totals during this time, others that directly impact ballplayers have an equally important role. As noted earlier, one of those factors is the use of anabolic steroids for increasing muscle mass, which enables hitters to not only hit \"mistake\" pitches farther, but it also confers faster bat speed, giving hitters a fraction of a second more to adjust to \"good\" pitches such as a well-placed fastball, slider, changeup, or curveball. A more innocent, but also meaningful factor is better nutrition, as well as scientific training methods and advanced training facilities/equipment which can work without steroids to produce a more potent ballplayer.\nIn today's baseball age, players routinely reach 40 and 50 home runs in a season, a feat that was rare as recently as the 1980s. On the other hand, since the end of the steroid era, the emphasis on swinging for home runs has been accompanied by hitting in general falling off, with batting averages trending downwards towards 1960s levels and strikeouts reaching all-time highs: \"each\" of the eleven seasons from 2006 through 2016 broke the preceding MLB-total record for strikeouts.\nMany modern baseball theorists believe that a new pitch will swing the balance of power back to the pitcher. A pitching revolution would not be unprecedented\u2014several pitches have changed the game of baseball in the past, including the slider in the 1950s and 1960s and the split-fingered fastball in the 1970s to 1990s. Since the 1990s, the changeup has made a resurgence, being thrown masterfully by pitchers such as Tim Lincecum, Pedro Mart\u00ednez, Trevor Hoffman, Greg Maddux, Matt Cain, Tom Glavine, Johan Santana, Marco Estrada, Justin Verlander, and Cole Hamels. Every so often, the time-honored knuckleball puts in another appearance to bedevil batters; pitchers like Phil Niekro, Jesse Haines, and Hoyt Wilhelm have made the Hall of Fame throwing knuckleballs.\nPopularity in recent decades.\nBaseball has declined in popularity as other sports have grown with the help of television broadcasting. MLB games ended up being about 30 minutes longer on average since the 1960s. To combat this problem, in 2023, MLB instituted the pitch clock to make games end quicker, which forces pitchers to throw within a given time limit, with 62% of fans expressing support during that year's season.\nSummary of modern-era major league teams.\nNote: The team names listed below are those currently in use. Some of the franchises have changed their names in the past, in some cases more than once. In the early years of the 20th century, many teams did not have official names, and were referred to by their league and city, or by nicknames created by sportswriters."}
{"id": "3858", "revid": "1272028783", "url": "https://en.wikipedia.org/wiki?curid=3858", "title": "Major League Baseball Most Valuable Player Award", "text": "The Major League Baseball Most Valuable Player Award (MVP) is an annual Major League Baseball (MLB) award given to one outstanding player in the American League and one in the National League. The award has been presented by the Baseball Writers' Association of America (BBWAA) since 1931.\nHistory.\nSince 1931, the Baseball Writers' Association of America (BBWAA) has bestowed a most valuable player award to a player in the National League and a player in the American League. Before 1931, two similar awards were issued\u2014the League Award was issued during 1922\u20131928 in the American League and during 1924\u20131929 in the National League, and during 1911\u20131914, the Chalmers Award was issued to a player in each league. Criteria and a list of winners for these two earlier awards are detailed in below sections.\nMVP voting takes place before the postseason, but the results are not announced until after the World Series. The BBWAA began by polling three writers in each league city in 1938, reducing that number to two per league city in 1961. The BBWAA does not offer a clear-cut definition of what \"most valuable\" means, instead leaving the judgment to the individual voters.\nIn 1944, the award was named after Kenesaw Mountain Landis, the first Commissioner of Baseball, who served from 1920 until his death on November 25, 1944. Formally named the Kenesaw Mountain Landis Memorial Baseball Award, that naming appeared on a plaque given to winning players. Starting in 2020, Landis' name no longer appears on the MVP plaque after the BBWAA received complaints from several former MVP winners about Landis' role against the integration of MLB.\nFirst basemen, with 35 winners, have won the most MVPs among infielders, followed by second basemen (16), third basemen (15), and shortstops (15). Of the 25 pitchers who have won the award, 15 are right-handed while 10 are left-handed. Walter Johnson, Carl Hubbell, and Hal Newhouser are the only pitchers who have won multiple times, with Newhouser winning consecutively in 1944 and 1945.\nHank Greenberg, Stan Musial, Alex Rodriguez, and Robin Yount have won at different positions, while Rodriguez is the only player who has won the award with two different teams at two different positions. Rodriguez and Andre Dawson are the only players to win the award while on a last-place team, the 2003 Texas Rangers and 1987 Chicago Cubs, respectively. Barry Bonds has won the most often (seven times) and the most consecutively (four from 2001 to 2004). Jimmie Foxx was the first player to win multiple times \u2013 10 players have won three times, and 19 have won twice. Frank Robinson and Shohei Ohtani are the only players to win the award in both the American and National Leagues, with Ohtani being the first to win in both leagues in consecutive seasons.\nThe award's only tie occurred in the National League in 1979, when Keith Hernandez and Willie Stargell received an equal number of points. There have been 23 unanimous winners, who received all the first-place votes. The New York Yankees have the most winning players with 24, followed by the St. Louis Cardinals with 21 winners. The award has never been presented to a member of the following three teams: Arizona Diamondbacks, New York Mets, and Tampa Bay Rays.\nIn recent decades, pitchers have rarely won the award. When Shohei Ohtani won the AL award in 2021, he became the first pitcher in either league to be named the MVP since Clayton Kershaw in 2014, and the first in the American League since Justin Verlander in 2011. Ohtani also became the first two-way player to win the award and in 2023, he became the first player in MLB history to win MVP by unanimous vote twice. Since the creation of the Cy Young Award in 1956, he is the only pitcher to win an MVP award without winning a Cy Young in the same year (Don Newcombe, Sandy Koufax, Bob Gibson, Denny McLain, Vida Blue, Rollie Fingers, Willie Hern\u00e1ndez, Roger Clemens, Dennis Eckersley, Justin Verlander, and Clayton Kershaw all won a Cy Young award in their MVP seasons). Ohtani is also the only MVP winner to have played most of his games as a designated hitter, a position that normally does not contribute on defense. In 2024, after winning his third career unanimously MVP award, Ohtani became the first MVP winner to have played exclusively as a designated hitter in a given season. To date, Ohtani is the only player to win both the MVP and the Edgar Mart\u00ednez Award, an award usually given to the top-performing designated hitter in a season.\nIronically, there was no award given by either league in 1930, which meant that one of the single greatest performances ever went unheralded when Hack Wilson of the Chicago Cubs set the (still standing) MLB record for RBI with 191. He also batted .356 and set the NL record with 56 HRs, a record which stood for 68 years until Sammy Sosa (66) and Mark McGwire (70) both eclipsed him.\nChalmers Award (1911\u20131914).\nBefore the 1910 season, Hugh Chalmers of Chalmers Automobile announced he would present a Chalmers Model 30 automobile to the player with the highest batting average in Major League Baseball at the end of the season. The 1910 race for best average in the American League was between the Detroit Tigers' widely disliked Ty Cobb and Nap Lajoie of the Cleveland Indians. On the last day of the season, Lajoie overtook Cobb's batting average with seven bunt hits against the St. Louis Browns. American League President Ban Johnson said a recalculation showed that Cobb had won the race anyway, and Chalmers ended up awarding cars to both players.\nIn the following season, Chalmers created the Chalmers Award. A committee of baseball writers was to convene after the season to determine the \"most important and useful player to the club and the league\". Since the award was not as effective at advertising as Chalmers had hoped, it was discontinued after 1914.\nLeague Awards (1922\u20131929).\nIn 1922, the American League created a new award to honor \"the baseball player who is of the greatest all-around service to his club\". Winners, voted on by a committee of eight baseball writers chaired by James Crusinberry, received a bronze medal and a cash prize. Voters were required to select one player from each team, and player-coaches and prior award winners were ineligible. Famously, these criteria resulted in Babe Ruth winning only a single MVP award before it was dropped after 1928. The National League award, without these restrictions, lasted from 1924 to 1929.\nBBWAA Most Valuable Player (1931\u2013present).\nThe Baseball Writers' Association of America (BBWAA) was first awarded the modern MVP after the 1931 season, adopting the format the National League used to distribute its league award. One writer in each city with a team filled out a ten-place ballot, with ten points for the recipient of a first-place vote, nine for a second-place vote, and so on. In 1938, the BBWAA raised the number of voters to three per city and gave 14 points for a first-place vote. The only significant change since then occurred in 1961 when the number of voters was reduced to two per league city."}
{"id": "3859", "revid": "19037985", "url": "https://en.wikipedia.org/wiki?curid=3859", "title": "Major League Baseball Rookie of the Year Award", "text": "In Major League Baseball, the Rookie of the Year Award is given annually to two outstanding rookie players, one each for the American League (AL) and National League (NL), as voted on by the Baseball Writers' Association of America (BBWAA). The award was established in 1940 by the Chicago chapter of the BBWAA, which selected an annual winner from 1940 through 1946. The award became national in 1947; Jackie Robinson, the Brooklyn Dodgers' second baseman, won the inaugural award. One award was presented for all of MLB in 1947 and 1948; since 1949, the honor has been given to one player each in the NL and AL. Originally, the award was known as the J. Louis Comiskey Memorial Award, named after the Chicago White Sox owner of the 1930s. The award was renamed the Jackie Robinson Award in July 1987, 40 years after Robinson broke the baseball color line.\nNineteen players have been elected to the National Baseball Hall of Fame\u2014Robinson, seven AL players, and eleven others from the NL. The award has been shared twice: once by Butch Metzger and Pat Zachry of the NL in 1976; and once by John Castino and Alfredo Griffin of the AL in 1979. Members of the Brooklyn and Los Angeles Dodgers have won the most awards of any franchise (with 18). Fred Lynn and Ichiro Suzuki are the only two players who have been named Rookie of the Year and Most Valuable Player in the same year, and Fernando Valenzuela is the only player to have won Rookie of the Year and the Cy Young Award in the same year. Sam Jethroe is the oldest player to have won the award, at age 32, 33\u00a0days older than 2000 winner Kazuhiro Sasaki (also 32). Luis Gil of the New York Yankees and Paul Skenes of the Pittsburgh Pirates are the most recent winners.\nQualifications and voting.\nFrom 1947 through 1956, each BBWAA voter used discretion as to who qualified as a rookie. In 1957, the term was first defined as someone with fewer than 75 at-bats or 45 innings pitched in any previous Major League season. This guideline was later amended to 90 at-bats, 45 innings pitched, or 45 days on a Major League roster before September 1 of the previous year. The current standard of 130 at-bats, 50 innings pitched, or 45 days on the active roster of a Major League club (excluding time in military service or on the injury list) before September 1 was adopted in 1971.\nSince 1980, each voter names three rookies: a first-place choice is given five points, a second-place choice three points, and a third-place choice one point. The award goes to the player who receives the most overall points. Edinson V\u00f3lquez received three second-place votes in 2008 balloting despite no longer being a rookie under the award's definition.\nThe award has drawn criticism in recent years because several players with experience in Nippon Professional Baseball (NPB) have won the award, such as Hideo Nomo in 1995, Kazuhiro Sasaki in 2000, Ichiro Suzuki in 2001, and Shohei Ohtani in 2018. The current definition of rookie status for the award is based only on Major League experience, but some feel that past NPB players are not true rookies because of their past professional experience. Others, however, believe it should make no difference since the first recipient and the award's namesake played for the Negro leagues before his MLB career and thus could also not be considered a \"true rookie\". This issue arose in 2003 when Hideki Matsui narrowly lost the AL award to \u00c1ngel Berroa. Jim Souhan of the \"Minneapolis Star Tribune\" said he did not see Matsui as a rookie in 2003 because \"it would be an insult to the Japanese league to pretend that experience didn't count.\" \"The Japan Times\" ran a story in 2007 on the labeling of Daisuke Matsuzaka, Kei Igawa, and Hideki Okajima as rookies, saying \"[t]hese guys aren't rookies.\" Past winners such as Jackie Robinson, Don Newcombe, and Sam Jethroe had professional experience in the Negro leagues.\nWinners.\nWins by team.\nFollowing Corbin Carroll winning the award as a member of the Arizona Diamondbacks, every MLB franchise has had at least one Rookie of the Year winner. The Brooklyn/Los Angeles Dodgers have won more than any other team with 18."}
{"id": "3860", "revid": "5308300", "url": "https://en.wikipedia.org/wiki?curid=3860", "title": "National League Championship Series", "text": "The National League Championship Series (NLCS) is a best-of-seven playoff and one of two League Championship Series comprising the penultimate round of Major League Baseball's (MLB) postseason. It is contested by the winners of the two National League (NL) Division Series. The winner of the NLCS wins the NL pennant and advances to the World Series, MLB's championship series, to play the winner of the American League's (AL) Championship Series. The NLCS began in 1969 as a best-of-five playoff and used this format until 1985, when it changed to a best-of-seven format.\nHistory.\nBefore 1969, the National League champion (the \"pennant winner\") was determined by the best win\u2013loss record at the end of the regular season. There were four \"ad hoc\" three-game playoff series due to ties under this formulation (in 1946, 1951, 1959, and 1962).\nA structured postseason series began in 1969, when both the National and American Leagues were reorganized into two divisions each, East and West. The two division winners within each league played each other in a best-of-five series to determine who would advance to the World Series. In 1985, the format changed to best-of-seven.\nThe NLCS and ALCS, since the expansion to seven games, are always played in a 2\u20133\u20132 format: games 1, 2, 6, and 7 are played in the stadium of the team that has home field advantage, and games 3, 4, and 5 are played in the stadium of the team that does not. Home field advantage is given to the team that has the better record, except a division champion would always get home advantage over a Wild Card team. From 1969 to 1993, home field advantage was alternated between divisions each year regardless of regular season record and from 1995 to 1997 home field advantage was predetermined before the season.\nIn 1981, a one-off division series was held due to a split season caused by a players' strike.\nIn 1994, the league was restructured into three divisions, with the three division winners and a wild card team advancing to a best-of-five postseason round, the now-permanent National League Division Series (NLDS). The winners of that round advance to the best-of-seven NLCS; however, due to the player's strike later that season, no postseason was played and the new format did not formally begin until 1995. The playoffs were expanded in 2012 to include a second Wild Card team and in 2022 to include a third Wild Card team.\nSeven managers have led a team to the NLCS in three consecutive seasons; however, the most consecutive NLCS appearances by one manager is held by Bobby Cox, who led the Atlanta Braves to eight straight from 1991 to 1999. The Braves (1991\u20131999) are also the only team in the National League to have made more than three consecutive National League Championship Series appearances. Tony La Russa and Jim Leyland are the only managers to lead their teams to three consecutive League Championship Series appearances in both leagues.\nThe Milwaukee Brewers, an American League team between 1969 and 1997, and the Houston Astros, a National League team between 1962 and 2012, are the only franchises to play in both the ALCS and NLCS. The Astros are the only team to have won both an NLCS (2005) and an ALCS (2017, 2019, 2021, and 2022). The Astros made four NLCS appearances before moving to the AL in 2013. Every current National League franchise has appeared in the NLCS and all teams except the Brewers have won an NL pennant via the NLCS.\nFor the first time in history, two wild card teams played in the 2022 National League Championship Series.\nChampionship Trophy.\nThe Warren C. Giles Trophy is awarded to the NLCS winner. Warren Giles served as president of the National League from 1951 to 1969.\nMost Valuable Player Award.\nA Most Valuable Player (MVP) award is given to the outstanding player in the NLCS. No MVP award is given for Division Series play.\nThe MVP award has been given to a player on the losing team twice, in 1986 to Mike Scott of the Houston Astros and in 1987 to Jeffrey Leonard of the San Francisco Giants.\nAlthough the National League began its LCS MVP award in 1977, the American League did not begin its LCS MVP award until 1980. The winners are listed in several locations:\nResults.\nYears of appearance.\nIn the sortable table below, teams are ordered first by number of wins, then by number of appearances, and finally by year of first appearance. In the \"Season(s)\" column, bold years indicate winning appearances."}
{"id": "3861", "revid": "37664675", "url": "https://en.wikipedia.org/wiki?curid=3861", "title": "American League Championship Series", "text": "The American League Championship Series (ALCS) is a best-of-seven playoff and one of two League Championship Series comprising the penultimate round of Major League Baseball's (MLB) postseason. The winner of the ALCS wins the AL pennant and advances to the World Series, MLB's championship series, to play the winner of the National League's (NL) Championship Series. The ALCS began in 1969 as a best-of-five playoff and used this format until 1985, when it changed to its current best-of-seven format.\nHistory.\nPrior to 1969, the American League champion (the \"pennant winner\") was determined by the best win\u2013loss record at the end of the regular season. There was one \"ad hoc\" single-game playoff held, in , due to a tie under this formulation.\nThe ALCS started in 1969, when the AL reorganized into two divisions, East and West. The winners of each division played each other in a best-of-five series to determine who would advance to the World Series. In 1985, the format changed to best-of-seven. \nIn 1981, a division series was held due to a split season caused by a players' strike. \nIn 1994, the league was restructured into three divisions, with the three division winners and a Wild Card team advancing to a best-of-five postseason round, known as the American League Division Series (ALDS). The winners of that round then advanced to the best-of-seven ALCS; however, due to the player's strike later that season, no postseason was played and the new format did not formally begin until 1995. The playoffs were expanded in 2012 to include a second Wild Card team and in 2022 to include a third Wild Card team.\nThe ALCS and NLCS, since the expansion to best-of-seven, are always played in a 2\u20133\u20132 format: Games 1, 2, 6, and 7 are played in the stadium of the team that has home field advantage, and Games 3, 4, and 5 are played in the stadium of the team that does not. The series concludes when one team records its fourth win. Since 1998, home field advantage has been given to the team that has the better regular season record, except a division champion would always get home advantage over a Wild Card team. If both teams have identical records in the regular season, then home field advantage goes to the team that has the winning head-to-head record. From 1969 to 1993, home-field advantage alternated between the two divisions, and from 1995 to 1997 home-field advantage was determined before the season.\nNine managers have led a team to the ALCS in three consecutive seasons; the record for most consecutive ALCS appearances by a manager is jointly held by Joe Torre, who led the New York Yankees to four in a row (1998, 1999, 2000, 2001), and Dusty Baker, who led the Houston Astros to four in a row (2020, 2021, 2022, 2023). The Astros (2017\u20132023) are also the only team in the American League to have made seven consecutive American League Championship Series appearances. Tony La Russa and Jim Leyland are the only managers to lead their teams to three consecutive League Championship Series appearances in both leagues.\nThe Milwaukee Brewers, an American League team between 1969 and 1997, and the Houston Astros, a National League team between 1962 and 2012, are the only franchises to play in both the ALCS and NLCS. The Astros are the only team to have won both an NLCS (2005) and an ALCS (2017, 2019, 2021, and 2022). Every current American League franchise has appeared in the ALCS.\nChampionship Trophy.\nThe William Harridge Trophy is awarded to the ALCS champion. Will Harridge served as American League president from 1931 to 1959.\nMost Valuable Player Award.\nThe Lee MacPhail Most Valuable Player (MVP) award is given to the outstanding player in the ALCS. No MVP award is given for Division Series play.\nAlthough the National League began its LCS MVP award in 1977, the American League did not begin its LCS MVP award till 1980. The winners are listed in several locations:\nResults.\nYears of appearance.\nIn the sortable table below, teams are ordered first by number of wins, then by number of appearances, and finally by year of first appearance. In the \"Season(s)\" column, bold years indicate winning appearances."}
{"id": "3862", "revid": "34218277", "url": "https://en.wikipedia.org/wiki?curid=3862", "title": "American League Division Series", "text": "In Major League Baseball, the American League Division Series (ALDS) determines which two teams from the American League will advance to the American League Championship Series. The Division Series consists of two best-of-five series, featuring each of the two division winners with the best records and the winners of the wild-card play-off.\nHistory.\nThe Division Series was implemented in 1981 as a one-off tournament because of a midseason strike, with the first place teams before the strike taking on the teams in first place after the strike. In 1981, a split-season format forced the first ever divisional playoff series, in which the New York Yankees won the Eastern Division series over the Milwaukee Brewers (who were in the American League until 1998) in five games while in the Western Division, the Oakland Athletics swept the Kansas City Royals (the only team with an overall losing record to ever make the postseason).\nIn 1994, it was returned permanently when Major League Baseball (MLB) restructured each league into three divisions, but with a different format than in 1981. Each of the division winners, along with one wild card team, qualify for the Division Series. Despite being planned for the 1994 season, the postseason was cancelled that year due to the 1994\u201395 Major League Baseball strike. In 1995, the first season to feature a division series, the Western Division champion Seattle Mariners defeated the wild card New York Yankees three games to two, while the Central Division champion Cleveland Indians defeated the Eastern Division champion Boston Red Sox in a three-game sweep.\nFrom 1994 to 2011, the wild card was given to the team in the American League with the best overall record that was \"not\" a division champion. Beginning with the 2012 season, a second wild card team was added, and the two wild card teams play a single-game playoff to determine which team would play in the ALDS. For the 2020 Major League Baseball season only, there was an expanded playoff format, owing to an abbreviated 60-game regular season due to the COVID-19 pandemic. Eight teams qualified from the American League: the top two teams in each division plus the next two best records among the remaining teams. These eight teams played a best-of-three-game series to determine placement in the ALDS. The regular format returned for the 2021 season.\nAs of 2022, the Yankees have played in and won the most division series, with thirteen wins in twenty-two appearances. In 2015, the Toronto Blue Jays and Houston Astros were the final American League teams to make their first appearances in the ALDS. The Astros had been in the National League through 2012, and had played in the National League Division Series (NLDS) seven times. The Astros are the only team to win the ALDS in six consecutive seasons. The Yankees record of four consecutive victories was broken by the Astros with their victory in the 2021 ALDS against the Chicago White Sox.\nDetermining the matchups.\nThe ALDS is a best-of-five series where the divisional winner with the best winning percentage in the regular season hosts the winner of the Wild Card Series between the top two wild card teams in one matchup, and the divisional winner with the second best winning percentage hosts the winner of the series between the lowest-seeded divisional winner and the lowest-seeded wild card team. (From 2012 to 2021, the wild card team was assigned to play the divisional winner with the best winning percentage in the regular season in one series, and the other two division winners met in the other series. From 1998 to 2011, if the wild-card team and the division winner with the best record were from the same division, the wild-card team played the division winner with the second-best record, and the remaining two division leaders played each other.) The two series winners move on to the best-of-seven ALCS. According to Nate Silver, the advent of this playoff series, and especially of the wild card, has caused teams to focus more on \"getting to the playoffs\" rather than \"winning the pennant\" as the primary goal of the regular season.\nFrom 2012 to 2021, the wild card team that advances to the Division Series was to face the number 1 seed, regardless whether or not they are in the same division. The two series winners move on to the best-of-seven ALCS. Beginning with the 2022 season, the winner between the lowest-ranked division winner and lowest-ranked wild card team faces the number 2 seed division winner in the Division Series, while the 4 v. 5 wild card winner still faces the number 1 seed, as there is no reseeding even if the 6-seeded wild card advances. Home-field advantage goes to the team with the better regular season record (or head-to-head record if there is a tie between two or more teams), except for the wild-card team, which never receives the home field advantage.\nBeginning in 2003, MLB has implemented a new rule to give the team from the league that wins the All-Star Game with the best regular season record a slightly greater advantage. In order to spread out the Division Series games for broadcast purposes, the two ALDS series follow one of two off-day schedules. Starting in 2007, after consulting the MLBPA, MLB has decided to allow the team with the best record in the league that wins the All-Star Game to choose whether to use the seven-day schedule (1-2-off-3-4-off-5) or the eight-day schedule (1-off-2-off-3-4-off-5). The team only gets to choose the schedule; the opponent is still determined by win\u2013loss records.\nInitially, the best-of-5 series played in a 2\u20133 format, with the first two games set at home for the lower seed team and the last three for the higher seed. Since 1998, the series has followed a 2\u20132\u20131 format, where the higher seed team plays at home in Games 1 and 2, the lower seed plays at home in Game 3 and Game 4 (if necessary), and if a Game 5 is needed, the teams return to the higher seed's field. When MLB added a second wild card team in 2012, the Division Series re-adopted the 2\u20133 format due to scheduling conflicts. However, it reverted to the 2\u20132\u20131 format starting the next season, 2013.\nResults.\nYears of appearance.\nIn the sortable table below, teams are ordered first by number of wins, then by number of appearances, and finally by year of first appearance. In the \"Season(s)\" column, bold years indicate winning appearances."}
{"id": "3863", "revid": "1266241274", "url": "https://en.wikipedia.org/wiki?curid=3863", "title": "National League Division Series", "text": "In Major League Baseball, the National League Division Series (NLDS) determines which two teams from the National League will advance to the National League Championship Series. The Division Series consists of two best-of-five series, featuring each of the two division winners with the best records and the winners of the wild-card play-offs.\nHistory.\nThe Division Series was implemented in 1981 as a one-off tournament because of a midseason strike, with the first place teams before the strike taking on the teams in first place after the strike. In 1981, a split-season format forced the first ever divisional playoff series, in which the Montreal Expos won the Eastern Division series over the Philadelphia Phillies in five games while in the Western Division, the Los Angeles Dodgers defeated the Houston Astros, also in five games (the Astros were members of the National League until 2012).\nIn 1994, it was returned permanently when Major League Baseball (MLB) restructured each league into three divisions, but with a different format than in 1981. Each of the division winners, along with one wild card team, qualify for the Division Series. Despite being planned for the 1994 season, the post-season was cancelled that year due to the 1994\u201395 Major League Baseball strike. In 1995, the first season to feature a division series, the Eastern Division champion Atlanta Braves defeated the wild card Colorado Rockies three games to one, while the Central Division champion Cincinnati Reds defeated the Western Division champion Los Angeles Dodgers in a three-game sweep.\nFrom 1994 to 2011, the wild card was given to the team in the National League with the best overall record that was \"not\" a division champion. Beginning with the 2012 season, a second wild card team was added, and the two wild card teams play a single-game playoff to determine which team would play in the NLDS. For the 2020 Major League Baseball season only, there was an expanded playoff format, owing to an abbreviated 60-game regular season due to the COVID-19 pandemic. Eight teams qualified from the National League: the top two teams in each division plus the next two best records among the remaining teams. These eight teams played a best-of-three-game series to determine placement in the NLDS. The regular format returned for the 2021 season.\nAs of 2021, the Atlanta Braves have currently played in the most NL division series with seventeen appearances. The St. Louis Cardinals have currently won the most NL division series, winning eleven of the fourteen series in which they have played. The Pittsburgh Pirates (who finished with a losing record from 1993 to 2012) were the last team to make their first appearance in the NL division series, making their debut in 2013 after winning the 2013 National League Wild Card Game. In 2008, the Milwaukee Brewers became the first team to play in division series in both leagues when they won the National League wild card, their first postseason berth since winning the American League East Division title in 1982 before switching leagues in 1998. Milwaukee had competed in an American League Division Series in the strike-shortened 1981 season.\nFormat.\nThe NLDS is a best-of-five series where the divisional winner with the best winning percentage in the regular season hosts the winner of the Wild Card Series between the top two wild card teams in one matchup, and the divisional winner with the second best winning percentage hosts the winner of the other Wild Card Series between the lowest-seeded divisional winner and the lowest-seeded wild card team. (From 2012 to 2021, the wild card team was assigned to play the divisional winner with the best winning percentage in the regular season in one series, and the other two division winners met in the other series. From 1998 to 2011, if the wild-card team and the division winner with the best record were from the same division, the wild-card team played the division winner with the second-best record, and the remaining two division leaders played each other.) The two series winners move on to the best-of-seven NLCS. According to Nate Silver, the advent of this playoff series, and especially of the wild card, has caused teams to focus more on \"getting to the playoffs\" rather than \"winning the pennant\" as the primary goal of the regular season.\nFrom 2012 to 2021, the wild card team that advances to the Division Series was to face the number 1 seed, regardless whether or not they are in the same division. The two series winners move on to the best-of-seven NLCS. Beginning with the 2022 season, the winner between the lowest-ranked division winner and lowest-ranked wild card team faces the #2 seed division winner in the Division Series, while the 4 v. 5 wild card winner faces the #1 seed, as there is no reseeding even if the 6 seed wild card advances. Home-field advantage goes to the team with the better regular season record (or head-to-head record if there is a tie between two or more teams), except for the wild-card team, which never receives the home-field advantage.\nBeginning in 2003, MLB has implemented a new rule to give the team with the best regular season record from the league that wins the All-Star Game a slightly greater advantage. In order to spread out the Division Series games for broadcast purposes, the two NLDS series follow one of two off-day schedules. Starting in 2007, after consulting the MLBPA, MLB has decided to allow the team with the best record in the league that wins the All-Star Game to choose whether to use the seven-day schedule (1-2-off-3-4-off-5) or the eight-day schedule (1-off-2-off-3-4-off-5). The team only gets to choose the schedule; the opponent is still determined by win\u2013loss records.\nInitially, the best-of-5 series played in a 2\u20133 format, with the first two games set at home for the lower seed team and the last three for the higher seed. Since 1998, the series has followed a 2\u20132\u20131 format, where the higher seed team plays at home in Games 1 and 2, the lower seed plays at home in Game 3 and Game 4 (if necessary), and if a Game 5 is needed, the teams return to the higher seed's field. When MLB added a second wild card team in 2012, the Division Series re-adopted the 2\u20133 format due to scheduling conflicts. However, it reverted to the 2\u20132\u20131 format starting the next season, 2013.\nResults.\nYears of appearance.\nIn the sortable table below, teams are ordered first by number of wins, then by number of appearances, and finally by year of first appearance. In the \"Season(s)\" column, bold years indicate winning appearances.\nFrequent matchups.\nNOTE: With the Houston Astros move to the American League at the conclusion of the 2012 season, the Braves vs. Astros series is not currently possible."}
{"id": "3864", "revid": "1268836222", "url": "https://en.wikipedia.org/wiki?curid=3864", "title": "2001 World Series", "text": "The 2001 World Series was the championship series of Major League Baseball's (MLB) 2001 season. The 97th edition of the World Series, it was a best-of-seven playoff between the National League (NL) champion Arizona Diamondbacks and the three-time defending World Series champions and American League (AL) champion New York Yankees. The underdog Diamondbacks defeated the heavily favored Yankees, four games to three to win the series. Considered one of the greatest World Series of all time,\nits memorable aspects included two extra-inning games and three late-inning comebacks. Diamondbacks pitchers Randy Johnson and Curt Schilling were both named World Series Most Valuable Players.\nThe Yankees advanced to the World Series by defeating the Oakland Athletics, three games to two, in the AL Division Series, and then the Seattle Mariners in the AL Championship Series, four games to one. It was the Yankees' fourth consecutive World Series appearance, after winning championships in , , and . The Diamondbacks advanced to the World Series by defeating the St. Louis Cardinals, three games to two, in the NL Division Series, and then the Atlanta Braves in the NL Championship Series, four games to one. It was the franchise's first appearance in a World Series.\nThe Series began later than usual as a result of a delay in the regular season after the September 11 attacks and was the first to extend into November. The Diamondbacks won the first two games at home, limiting the Yankees to just one run. The Yankees responded with a close win in Game 3, at which U.S. President George W. Bush threw out the ceremonial first pitch. In Games 4 and 5, the Yankees won in comeback fashion, hitting game-tying home runs off Diamondbacks closer Byung-hyun Kim with one out remaining in consecutive games, before winning in extra innings. The Diamondbacks won Game 6 in a blowout, forcing a decisive Game 7. In the final game, the Yankees led in the ninth inning before the Diamondbacks staged a comeback against closer Mariano Rivera, capped off by a walk-off, bases-loaded bloop single by Luis Gonzalez to clinch Arizona's championship victory. This was the third World Series to end in a bases-loaded, walk-off hit, following and , and to this date, the last Series to end on a walk-off of any kind. This series held the record for the latest date that a Series ended (November 4), until that record was tied during the 2009 World Series and broken during the 2022 World Series.\nAmong several firsts, the 2001 World Series was the first World Series championship for the Diamondbacks; the first World Series played in the state of Arizona or the Mountain Time Zone; the first championship for a Far West state other than California; the first major professional sports team from the state of Arizona to win a championship; and the earliest an MLB franchise had won a World Series (the Diamondbacks had only existed for four years). The home team won every game in the Series, which had only happened twice before, in 1987 and 1991, both won by the Minnesota Twins. The Diamondbacks outscored the Yankees, 37\u201314, as a result of large margins of victory achieved by Arizona at Bank One Ballpark (now known as Chase Field) relative to the one-run margins the Yankees achieved at Yankee Stadium. Arizona's pitching held powerhouse New York to a .183 batting average, the lowest in a seven-game World Series ever, surpassing the St. Louis Cardinals, who hit .185 in the 1985 World Series. This and the 2002 World Series were the last two consecutive World Series to have game sevens until the World Series of 2016 and 2017. The 2001 World Series was the subject of an HBO documentary, \"Nine Innings from Ground Zero\", in 2004. It is often referred to as the greatest World Series of all time.\nBackground.\nArizona Diamondbacks.\nThe Arizona Diamondbacks began play in 1998, along with the Tampa Bay Devil Rays, as the youngest expansion team in Major League Baseball (MLB). After a mediocre debut season, the Diamondbacks finished the following year first in the National League (NL) West with a record, but lost to the New York Mets in the National League Division series. With several All-Star players like Randy Johnson and Matt Williams, the Diamondbacks had high expectations for the 2000 season, but finished third in the NL West with an record. During the offseason, team manager Buck Showalter was fired, and replaced by sportscaster Bob Brenly. The Diamondbacks acquired several notable free agent players during the offseason, including Miguel Batista, Mark Grace, and Reggie Sanders. Most of the Diamondbacks players were above the age of 30, and had already played on a number of teams prior to the 2001 season. In fact, the Diamondbacks starting lineup for the World Series did not include a player under the age of 31, making them the oldest team by player age in World Series history. With several players nearing the age of retirement, Luis Gonzalez noted that the overall team mentality was \"there's too many good guys in here to let this opportunity slip away\".\nAlthough the Diamondbacks were only one game above .500 by the end of April, Gonzalez had a particularly memorable start to the season, in which he tied the MLB record with 13 home runs during the month of April. The Diamondbacks found greater success in May and June, and at one point had a six-game lead in the NL West. During this span, the team won nine consecutive games, and Johnson tied the MLB record with 20 strikeouts in a nine-inning game. The six game lead did not last long however, and by the end of July, the Diamondbacks were a half game behind the Los Angeles Dodgers in the West. A resurgent August pushed the team back into first place, a spot they maintained for the rest of the season. By the end of the season, several Diamondbacks players had put up exceptional statistics: Curt Schilling had the most wins of any pitcher in MLB that year with 22, while Johnson nearly broke the single season strikeout record with 372. Johnson and Schilling also had the two lowest earned run averages (ERA) in the NL, with 2.49 and 2.98 respectively. Gonzalez ended the season with a .325 batting average and 57 home runs, and finished third in voting for the NL Most Valuable Player Award. The Diamondbacks were also one of the best defensive teams in MLB that year, second in fewest errors committed, and tied with the Seattle Mariners for the best fielding percentage.\nThe Diamondbacks entered the postseason as the #2 seed in the National League, and played the #4 seed St. Louis Cardinals in the National League Division Series. Schilling threw a shutout in Game 1 to give the Diamondbacks an early series lead, but the Cardinals won Game 2 thanks to a two-run home run from Albert Pujols. Craig Counsell hit a three-run home run late in Game 3 to give the Diamondbacks a 2\u20131 series lead, but the Cardinals won Game 4 with strong pitching performances from Bud Smith and their relief pitchers. The Diamondbacks clinched the series in Game 5, when Tony Womack hit a game winning single that scored Danny Bautista. They then faced the third seeded Atlanta Braves in the National League Championship Series. Johnson also threw a shutout in Game 1, while the Braves hit three home runs in Game 2 to tie the series at one game apiece. Schilling threw a complete game in Game 3, and the Diamondbacks scored 11 runs in a Game 4 victory to take a 3\u20131 series lead. The Diamondbacks clinched the series in Game 5 with another strong performance from Johnson. With the win, they became the fastest expansion team to reach the World Series, in just their fourth year of play.\nNew York Yankees.\nIn contrast to the Diamondbacks, the New York Yankees were one of the oldest and most recognized teams in all of North American sports. The Yankees had built a dynasty in the late 1990s that extended into 2000, which included winning three consecutive World Series' and four of the last five. These teams were led by a group of talented young players that became known as the Core Four: Derek Jeter, Andy Pettitte, Jorge Posada, and Mariano Rivera. Following the Yankees win over the Braves in the 1999 World Series, sportscaster Bob Costas called the Yankees \"the team of the decade, [and] most successful franchise of the century.\"\nThe Yankees finished the 2001 season in first place in the AL East with a win\u2013loss record of (a winning percentage of ), games ahead of the Boston Red Sox, good enough to secure the #2 seed in the American League playoff bracket. The Yankees then defeated the fourth seeded Oakland Athletics 3 games to 2 in the AL Division Series, after losing 2 games at home, and the top seeded Seattle Mariners 4 games to 1 in the AL Championship Series to advance to their fourth consecutive World Series, and fifth in six years.\nDerek Jeter and Tino Martinez led the Yankees offensively during the 2001 season. Jeter batted .311 with 21 home runs and 74 RBI in 150 games, while Martinez batted .280 with 34 home runs and 113 RBI in 154 games. Roger Clemens and Mike Mussina were the leaders of the Yankees' pitching staff. Clemens who won the Cy Young Award, his sixth of a career total and major league record 7, finished with a win\u2013loss record of 20\u20133, an earned-run average (ERA) of 3.51, and struck out 213 batters in 220.1 innings pitched and was by far the Yankee's best starter in the World Series. Mussina finished with a win\u2013loss record of 17\u201311, an ERA of 3.15, and struck out 214 batters in 228.2 innings pitched.\nSeptember 11 and the month of November.\nAfter MLB games were postponed as a result of the September 11 attacks, the World Series began on October 27, 2001, the latest start date for a World Series until the 2009 World Series, which started on October 28. The last three games were the first major-league games (other than exhibitions) played in the month of November. This was just the fourth time that no World Series champion was decided within the traditional month of October. The previous three occurrences were in (no series), (series held in September because of World War I), and (series cancelled by the players' strike). Game 7 was played on November 4; at the time this was the latest date a World Series game was played, and still tied with Game 6 of the 2009 Series for the second-latest date of a World Series game (only behind 's Game 6, played on November 5).\nAdditionally, the Series took place in New York City only seven weeks after the attacks, representing a remarkable boost in morale for the fatigued city. A tattered and torn American flag recovered from the wreckage at Ground Zero, which had been used at funerals of fallen Port Authority police officers after the attacks, was flown over Yankee Stadium during the series. According to Port Authority sergeant Antonio Scannella, \"We wanted a place America could see this flag so they could see the rips in it, but it still flies.\"\nPresident George W. Bush threw out the ceremonial first pitch before Game 3 at Yankee Stadium. Bush had been counseled by security officials to appear before Game 1 in Phoenix because they believed it would be more secure there, but Bush thought it would be better for the country to do it in New York. Security was extremely tight at Yankee Stadium before the game, with bomb sniffing dogs sweeping the property, snipers positioned around the stadium, and vendors screened by federal agents. A Secret Service agent dressed as an umpire and stood on the field with the other umpires before the game, briefly appearing on the TV broadcast. Bush wore a bulletproof vest underneath an FDNY sweater. Having been counseled by Derek Jeter to throw from the rubber on top of the pitcher's mound rather than the base of the mound, Bush strode to the rubber, gave a thumbs up to the crowd, and fired a strike over home plate as the crowd chanted \"U-S-A\". Bush later reflected, \"I had never had such an adrenaline rush as when I finally made it to the mound. I was saying to the crowd, 'I'm with you, the country's with you' ... And I wound up and fired the pitch. I've been to conventions and rallies and speeches: I've never felt anything so powerful and emotions so strong, and the collective will of the crowd so evident.\"\nMatchups.\nGame 1.\nThe Series commenced on October 27, which was the latest a World Series had started, beating the previous record by four days (1999 World Series, October 23). The Yankees struck first in Game 1 when Derek Jeter was hit by a pitch with one out in the first and scored on Bernie Williams's double two batters later. However, Arizona's Curt Schilling and two relievers, Mike Morgan and Greg Swindell held the Yankees scoreless afterward. They managed to get only two walks and two hits for the rest of the game, Scott Brosius's double in the second and Jorge Posada's single in the fourth, both with two outs.\nMeanwhile, the Diamondbacks tied the game on Craig Counsell's one-out home run in the first off of Mike Mussina. After a scoreless second, Mussina led off the third by hitting Tony Womack with a pitch. He moved to second on Counsell's sacrifice bunt before Luis Gonzalez's home run put the Diamondbacks up 3\u20131. A single and right fielder David Justice's error put runners on second and third before Matt Williams's sacrifice fly put Arizona up 4\u20131. After Mark Grace was intentionally walked, Damian Miller's RBI double gave Arizona a 5\u20131 lead.\nNext inning, Gonzalez hit a two-out double off of Randy Choate. Reggie Sanders was intentionally walked before Gonzalez scored on Steve Finley's single. An error by third baseman Brosius scored Sanders, put Finley at third, and Williams at second. Both men scored on Mark Grace's double, putting Arizona up 9\u20131. Though the Diamondbacks got just one more hit for the rest of the game off of Sterling Hitchcock and Mike Stanton (Williams' leadoff single in the seventh), they went up 1\u20130 in the series.\nThe Diamondbacks' win in Game 1 was the first World Series game won by a non-New York City team since 1997. In every World Series between 1997 and 2001, either both teams were from New York City or a New York City team won in a sweep (1998 and 1999).\nGame 2.\nArizona continued to take control of the Series with the strong pitching performance of Randy Johnson. The Big Unit pitched a complete-game shutout, allowing only four baserunners and three hits while striking out 11 Yankees. Andy Pettitte meanwhile nearly matched him, retiring Arizona in order in five of the seven innings he pitched. In the second, he allowed a leadoff single to Reggie Sanders, who scored on Danny Bautista's double. Bautista was the only Arizona runner stranded for the entire game. In the seventh, Pettitte hit Luis Gonzalez with a pitch before Sanders grounded into a forceout. After Bautista singled, Matt Williams's three-run home run put Arizona up 4\u20130. They won the game with that score and led the series two games to none as it moved to New York City. This was the 1,000th game played in the history of the MLB postseason.\nGame 3.\nThe game was opened in New York City by President George W. Bush, who threw the ceremonial first pitch, a strike to Yankees backup catcher Todd Greene. Bush became the first incumbent U.S. president to throw a World Series first pitch since Jimmy Carter in . He also threw the baseball from the mound where the pitcher would be set (unlike most ceremonial first pitches which are from in front of the mound) and threw it for a strike. Chants of \"U-S-A, U-S-A\" rang throughout Yankee Stadium. Yankees starter Roger Clemens was outstanding allowing only three hits and struck out nine in seven innings of work. Yankees closer Mariano Rivera pitched two innings for the save.\nJorge Posada's leadoff home run off of Brian Anderson in the second put the Yankees up 1\u20130. The Diamondbacks loaded the bases in the fourth on two walks and one hit before Matt Williams's sacrifice fly tied the game. Bernie Williams hit a leadoff single in the sixth and moved to second on a wild pitch one out later before Posada walked. Mike Morgan relieved Anderson and struck out David Justice before Scott Brosius broke the tie with an RBI single. That would be all the scoring as Morgan and Greg Swindell pitched the rest of the game for the Diamondbacks. The Yankees cut Arizona's series lead to 2\u20131 with the win.\nGame 4.\nGame 4 saw the Yankees send Orlando Hern\u00e1ndez to the mound while the Diamondbacks elected to bring back Curt Schilling on three days' rest. Both pitchers gave up home runs, with Schilling doing so to Shane Spencer in the third inning and Hernandez doing so to Mark Grace in the fourth. Hernandez pitched solid innings, giving up four hits while Schilling went seven innings and gave up one.\nWith the game still tied entering the eighth, Arizona struck. After Mike Stanton recorded the first out of the inning, Luis Gonzalez singled and Erubiel Durazo hit a double to bring him in. Matt Williams followed by grounding into a fielder's choice off of Ramiro Mendoza, which scored pinch runner Midre Cummings and gave the team a 3\u20131 lead.\nWith his team on the verge of taking a commanding 3\u20131 series lead, Diamondbacks manager Bob Brenly elected to bring in closer Byung-hyun Kim in the bottom of the eighth for a two-inning save. Kim, at 22, became the first Korean-born player to play in the MLB World Series. Kim struck out the side in the eighth, but ran into trouble in the ninth.\nDerek Jeter led off by trying to bunt for a hit but was thrown out by Williams. Paul O'Neill then lined a single in front of Gonzalez. After Bernie Williams struck out, Kim seemed to be out of trouble with Tino Martinez coming to the plate. However, Martinez drove the first pitch he saw from Kim into the right-center field bleachers, tying the score at 3\u20133. The Yankees were not done, as Jorge Posada walked and David Justice moved him into scoring position with a single. Kim struck Spencer out to end the threat.\nWhen the scoreboard clock in Yankee Stadium passed midnight, World Series play in November began, with the message on the scoreboard \"Welcome to November Baseball\".\nMariano Rivera took the hill for the Yankees in the tenth and retired the Diamondbacks in order. Kim went out for a third inning of work and retired Scott Brosius and Alfonso Soriano, but Jeter hit an opposite field home run on a 3\u20132 pitch count from Kim. This home run gave the Yankees a 4\u20133 victory and tied the Series at two games apiece which guaranteed a return trip to Arizona and made Jeter the first player to hit a November home run and earning him the tongue-in-cheek nickname of \"Mr. November\".\nGame 5.\nGame 5 saw the Yankees return to Mike Mussina for the start while the Diamondbacks sent Miguel Batista, who had not pitched in twelve days, to the mound. Batista pitched a strong scoreless innings, striking out six, and reliever Greg Swindell got the last out of the eighth inning. Mussina bounced back from his poor Game 1 start, recording ten strikeouts, but allowed solo home runs in the fifth inning to Steve Finley and Rod Barajas.\nIn the top of the ninth, Paul O'Neill was honored by Yankee fans who chanted his name to which O'Neill, who was visibly in tears, tipped his hat. With the Diamondbacks leading 2\u20130 in the ninth, Byung-hyun Kim was called upon for the save despite having thrown three innings the night before. Jorge Posada doubled to open the inning, but Kim got Shane Spencer to ground out and then struck out Chuck Knoblauch. As had happened the previous night, Kim could not hold the lead as Scott Brosius hit a 1\u20130 pitch over the left field wall, the second straight game tying home run in the bottom of the ninth for the Yankees. Kim was pulled from the game in favor of Mike Morgan who recorded the final out.\nMorgan retired the Yankees in order in the 10th and 11th innings, while the Diamondbacks got to Mariano Rivera in the 11th. Danny Bautista and Erubiel Durazo opened the inning with hits and Matt Williams advanced them into scoring position with a sacrifice bunt. Rivera then intentionally walked Steve Finley to load the bases, then got Reggie Sanders to line out and Mark Grace grounded out to end the inning.\nArizona went to midseason trade acquisition Albie Lopez in the 12th, and in his first at bat he gave up a single to Knoblauch (who had entered the game as a pinch runner). Brosius moved him over with a bunt, and then Alfonso Soriano ended the game with an RBI single to give the Yankees a 3\u20132 victory and a 3\u20132 series lead as the series went back to Phoenix. Lopez would not pitch again in the series. Sterling Hitchcock got the win for the Yankees after he relieved Rivera for the twelfth.\nGame 6.\nWith Arizona in a must-win situation, Randy Johnson pitched seven innings and struck out seven, giving up just two runs, and Bobby Witt and Troy Brohawn finished the blowout. The Diamondbacks struck first when Tony Womack hit a leadoff double off of Andy Pettitte and scored on Danny Bautista's single in the first. Next inning, Womack's bases-loaded single scored two and Bautista's single scored another. The Yankees loaded the bases in the third on a single and two walks, but Johnson struck out Jorge Posada to end the inning. The Diamondbacks broke the game open with eight runs in the bottom half. Pettitte allowed a leadoff walk to Greg Colbrunn and subsequent double to Matt Williams before being relieved by Jay Witasick, who allowed four straight singles to Reggie Sanders, Jay Bell, Damian Miller, and Johnson that scored three runs. After Womack struck out, Bautista's single scored two more runs and Luis Gonzalez's double scored another, with Bautista being thrown out at home. Colbrunn's single and Williams's double scored a run each before Sanders struck out to end the inning. In the fourth, Bell reached first on a strike-three wild pitch before scoring on Miller's double. Johnson struck out before Womack singled to knock Witasick out of the game. With Randy Choate pitching, Yankees second baseman Alfonso Soriano's error on Bautista's ground ball allowed Miller to score and put runners on first and second before Gonzalez's single scored the Diamondbacks' final run. Choate and Mike Stanton kept them scoreless for the rest of the game. Pettitte was charged with six runs in two innings while Witasick was charged with nine runs in innings, the most runs allowed by any pitcher in a World Series game since Hall of Famer Walter Johnson also allowed nine runs in Game 7 of the 1925 World Series. The Yankees scored their only runs in the sixth on back-to-back one-out singles by Shane Spencer and Luis Sojo with runners on second and third, but by then the score had become so far out of reach that it didn't do the Yankees much good. The Diamondbacks hit six doubles and Danny Bautista batted 3-for-4 with five RBIs. The team set a World Series record with 22 hits and defeated the New York Yankees in its most lopsided postseason loss in 293 postseason games, since surpassed by a 16\u20131 loss to the Boston Red Sox in Game 3 of the 2018 American League Division Series. The 15\u20132 win evened the series at three games apiece and set up a Game 7 for the ages between Roger Clemens and Curt Schilling.\nGame 7.\nIt was a matchup of two 20-game winners in the Series finale. Roger Clemens, at 39 years old, became the oldest Game 7 starter. Curt Schilling had already started two games of the Series and pitched his 300th inning of the season on just three days' rest. The two aces matched each other inning by inning and after seven full innings, the game was tied at 1\u20131. The Diamondbacks scored first in the sixth inning with a Steve Finley single and a Danny Bautista double (Bautista, trying to stretch it into a triple, was called out at third base). The Yankees responded with an RBI single from Tino Martinez, which drove in Derek Jeter who had singled earlier. Brenly stayed with Schilling into the eighth, and the move backfired as Alfonso Soriano hit a home run on an 0\u20132 pitch. After Schilling struck out Scott Brosius, he gave up a single to David Justice, and he left the game trailing 2\u20131. When Brenly came to the mound to remove Schilling, he was heard on the Sounds of the Game microphone telling his clearly upset pitcher, \"love you brother, you're my hero\" and assuring him that \"that ain't gonna beat us, we're gonna get that back and then some.\" He then brought in Game 5 starter Miguel Batista to get Jeter out and then in an unconventional move, brought in the previous night's starter and winner Randy Johnson, who had thrown 104 pitches, in relief to keep it a one-run game. It proved to be a smart move, as Johnson retired pinch hitter Chuck Knoblauch (who batted for the left handed Paul O'Neill) on a fly out to Bautista in right field, then returned to the mound for the top of the ninth where he got Bernie Williams to fly out to Steve Finley in center field and Martinez to ground out to Tony Womack at shortstop, and then struck out catcher Jorge Posada to send the game to the bottom of the ninth inning.\nWith the Yankees ahead 2\u20131 in the bottom of the eighth, manager Joe Torre decided to relieve setup man Mike Stanton, who had got the last two outs, to his ace closer Mariano Rivera for a two-inning save. Rivera struck out the side in the eighth, including Arizona's Luis Gonzalez, Matt Williams, and Bautista. Although he was effective in the eighth, this game would end in the third ninth-inning comeback of the Series.\nMark Grace led off the inning with a single to center on a 1\u20130 pitch. Rivera's errant throw to second base on a bunt attempt by catcher Damian Miller on an 0\u20131 pitch put runners on first and second. Jeter tried to reach for the ball, but got tangled in the legs of pinch-runner David Dellucci, who was sliding in an attempt to break up the double play. During the next at bat, Rivera appeared to regain control when he fielded pinch hitter Jay Bell's (who was hitting for Johnson) bunt and threw out Dellucci at third base, but third baseman Brosius decided to hold onto the baseball instead of throwing to first to complete the double play. Midre Cummings was sent in to pinch-run for Damian Miller, who had reached second base safely. With Cummings at second and Bell at first, the next batter, Womack, hit a double down the right-field line on a 2\u20132 pitch that tied the game and earned Rivera a blown save, his first in a postseason since 1997. Bell advanced to third and the Yankees pulled the infield and outfield in as the potential winning run (Bell) stood at third with fewer than two outs. After Rivera hit Craig Counsell unintentionally with an 0\u20131 pitch, the bases were loaded. On an 0\u20131 pitch, with Williams in the on-deck circle, Gonzalez lofted a soft floater single over the drawn-in Jeter that barely reached the outfield grass, plating Jay Bell with the winning run.\nGonzalez's single ended New York's bid for a fourth consecutive title (and fifth in six seasons) and brought Arizona its first championship in its fourth year of existence, making the Diamondbacks the fastest expansion team to win a World Series (beating out the 1997 Florida Marlins, who had done it in their fifth season at that time). It was also the first, and remains the only, major professional sports championship for the state of Arizona. Randy Johnson picked up his third win of the Series, becoming the first pitcher since Mickey Lolich of the 1968 Tigers to win three games in a World Series. Rivera took the loss, his only postseason loss in his career. Coincidentally, this was also the second World Series in a 5-year span (1997 to 2001) to end with a game-winning RBI single. Edgar Renteria hit the game-winner in the 1997 series, while Gonzalez hit it here, with Craig Counsell being on the basepaths for each. No other World Series has ended with a game-winning hit since 2001.\nIn 2009, Game 7 of the 2001 World Series was chosen by \"Sports Illustrated\" as the Best Postseason Game of the Decade (2000\u20132009).\nIn the years that have followed, many fans regardless of team allegiance consider Game 7 of the 2001 World Series to be one of the greatest games ever played in the history of professional baseball.\nComposite box.\n2001 World Series (4\u20133): Arizona Diamondbacks (N.L.) over New York Yankees (A.L.)\nMedia coverage.\nFor the second consecutive year, Fox carried the World Series with its top broadcast team, Joe Buck and Tim McCarver (himself a Yankees broadcaster). This was the first year of Fox's exclusive rights to the World Series (in the previous contract, Fox only broadcast the World Series in even numbered years while NBC broadcast it in odd numbered years), which it has held since. This particular contract also had given Fox exclusive rights to the entire baseball postseason, which aired over its family of networks, but shortly after the World Series, Fox sold its cable outlet Fox Family Channel, on which it aired Division Series games, shortly after the World Series ended, to Disney, which renamed the channel to ABC Family; since this made the channel a corporate sibling of ESPN, Disney would move those games to ESPN in 2003 after airing them for one more season on ABC Family. \nESPN Radio provided national radio coverage for the fourth consecutive year, with Jon Miller and Joe Morgan calling the action.\nLocally, the Series was broadcast by KTAR-AM in Phoenix with Thom Brennaman, Greg Schulte, Rod Allen and Jim Traber, and by WABC-AM in New York City with John Sterling and Michael Kay. This was WABC's last broadcast of Yankees baseball after twenty-one seasons as the team's flagship, and also the last time Sterling and Kay broadcast together after ten seasons. Sterling and the Yankees joined WCBS-AM the next season on the radio side, while Kay was promoted to television as the YES Network launched for 2002. \nBooks and films.\nBuster Olney, who covered the Yankees for \"The New York Times\" before joining ESPN, would write a book titled \"The Last Night of the Yankee Dynasty\", published in 2004. The book is a play by play account of Game 7 in addition to stories about key players, executives, and moments from the 1996\u20132001 dynasty. In a 2005 reprinting, Olney included a new epilogue covering the aftermath of the 2001 World Series up to the Boston Red Sox epic comeback from down 3\u20130 in the 2004 ALCS.\nThe official MLB Productions documentary film of the series was released in 2001.\nIn 2004, HBO released \"Nine Innings from Ground Zero\", a documentary focusing on the special role that baseball, and particularly the Yankees, played in helping to heal New York after 9/11. The film features interviews with players, fans who lost family members, firefighters, sportswriters, and then United States President George W. Bush.\nIn 2005, A&amp;E Home Video released the \"New York Yankees Fall Classic Collectors Edition (1996\u20132001)\" DVD set. Game 4 of the 2001 World Series is included in the set. In 2008, \"The Arizona Diamondbacks 2001 World Series\" DVD set was released. All seven games are included on this set.\nAftermath.\nThe duo of Curt Schilling and Randy Johnson were awarded the World Series Most Valuable Player, the first players to split the award since Steve Yeager, Ron Cey, and Pedro Guerrero did so for the Dodgers in the 1981 World Series and last to do so to date. This would not be the only award they split as both were named Sports Illustrated Sportsperson of the Year for 2001.\nRivera's blown save and the Yankees' loss proved to be life-saving for Yankees utility player Enrique Wilson. Had the Yankees won, Wilson was planning to fly home to the Dominican Republic on American Airlines Flight 587 on November 12 after what would have been a Yankees victory parade down the Canyon of Heroes. But after the Yankees lost (and thus no parade occurred), Wilson decided to fly home earlier. Flight 587 would crash in Belle Harbor, Queens, killing everyone on board. Rivera later said, \"I am glad we lost the World Series because it means that I still have a friend.\"\nDuring the offseason, several Yankees moved on to other teams or retired, the most notable changes being the free agent departures of Martinez and Knoblauch to the St. Louis Cardinals and Kansas City Royals, and Brosius and O'Neill retiring. Martinez would later return to the Yankees to finish his career in 2005.\nAfter winning the NL West again in 2002 the Diamondbacks were swept 3\u20130 by St. Louis in the NLDS. From here they declined, losing 111 games in 2004 as Bob Brenly was fired during that season. Arizona would not win another NL West title until 2007. Schilling was traded to the Boston Red Sox after the 2003 season and in 2004 helped lead them to their first World Series championship since 1918. He helped them win another championship in 2007 and retired after four years with Boston, missing the entire 2008 season with a shoulder injury. Johnson was traded to the Yankees after the 2004 season, a season that saw him throw a perfect game against the Atlanta Braves, though he would be traded back to the Diamondbacks two years later and finish his career with the San Francisco Giants in 2009. The last player from the 2001 Diamondbacks roster, Lyle Overbay, retired following the 2014 season with the Milwaukee Brewers while the last player from the 2001 Yankees, Randy Choate, retired following the 2016 season.\nFrom 2002 through 2007, the Yankees' misfortune in the postseason continued, with the team losing the ALDS to the Anaheim Angels in 2002, the World Series to the Florida Marlins in 2003, the ALCS to the Boston Red Sox (in the process becoming the first team in postseason history to blow a 3\u20130 series lead) in 2004, the ALDS again to the Angels in 2005, and then losing the ALDS to the Detroit Tigers and the Cleveland Indians in 2006 and 2007, respectively. In addition, including the World Series loss in 2001, every World Series champion from 2001 to 2004 won the title at the Yankees' expense in postseason play, which is an AL record and as of 2023 tied for the MLB record with the Los Angeles Dodgers from 2016 to 2019. Joe Torre's contract was allowed to expire and he was replaced by Joe Girardi in 2008, a season in which the Yankees would miss the playoffs for the first time since 1993. The Yankees won their 27th World Series championship in 2009, defeating the defending 2008 champion Philadelphia Phillies in six games, but could not pull off another dynasty like they did during the late 1990s and early 2000s; in fact, they failed to reach the World Series during the entirety of the 2010s. The Yankees would finally return to the World Series in 2024 only to fall in five games to the Los Angeles Dodgers. Since 2001, the Yankees have played in four World Series and lost three of them ('01, \u00b403, '24)\nThis is the state of Arizona's only championship among the four major North American men's professional sports. However, the WNBA's Phoenix Mercury have won three championships since then (2007, 2009, and 2014).\nThe Diamondbacks and the Baltimore Ravens, who won the Super Bowl earlier in 2001, created the first instance of two major sports teams winning a championship game or series on their first attempts. This would not occur again until 2019, when the Toronto Raptors and Washington Nationals accomplished this feat.\nThe Diamondbacks would not return to the World Series again until 2023; this time, they would go on to lose to the Texas Rangers in five games."}
{"id": "3865", "revid": "8637584", "url": "https://en.wikipedia.org/wiki?curid=3865", "title": "1903 World Series", "text": "The 1903 World Series was the first modern World Series to be played in Major League Baseball. It matched the American League (AL) champion Boston Americans against the National League (NL) champion Pittsburgh Pirates in a best-of-nine series, with Boston prevailing five games to three, winning the last four. The first three games were played in Boston, the next four in Allegheny (home of the Pirates), and the eighth (last) game in Boston.\nPittsburgh pitcher Sam Leever injured his shoulder while trap shooting, so his teammate Deacon Phillippe pitched five complete games. Phillippe won three of his games, but it was not enough to overcome the club from the new American League. Boston pitchers Bill Dinneen and Cy Young led Boston to victory. In Game 1, Phillippe struck out ten Boston batters. The next day, Dinneen bettered that mark, striking out 11 Pittsburgh batters in Game 2.\nHonus Wagner, bothered by injuries, batted only 6-for-27 (.222) in the Series and committed six errors. The shortstop was deeply distraught by his performance. The following spring, Wagner (who in 1903 led the National League in batting average) refused to send his portrait to a \"Hall of Fame\" for batting champions. \"I was too bum last year\", he wrote. \"I was a joke in that Boston-Pittsburgh Series. What does it profit a man to hammer along and make a few hits when they are not needed only to fall down when it comes to a pinch? I would be ashamed to have my picture up now.\"\nDue to overflow crowds at the Exposition Park games in Allegheny City, if a batted ball rolled under a rope in the outfield that held spectators back, a \"ground-rule triple\" would be scored. 17 ground-rule triples were hit in the four games played at the stadium.\nIn the series, Boston came back from a three games to one deficit, winning the final four games to capture the title. Such a large comeback would not happen again until the Pirates came back to defeat the Washington Senators in the 1925 World Series, and has happened only 11 times in baseball history. (The Pirates themselves repeated this feat in against the Baltimore Orioles.) Much was made of the influence of Boston's \"Royal Rooters\", who traveled to Exposition Park and sang their theme song \"Tessie\" to distract the opposing players (especially Wagner). Boston wound up winning three out of four games in Allegheny City.\nPirates owner Barney Dreyfuss added his share of the gate receipts to the players' share, so the losing team's players actually finished with a larger individual share than the winning team's.\nThe Series brought the new American League prestige and proved its best could beat the best of the National League, thus strengthening the demand for future World Series competitions.\nBackground.\nA new league.\nIn 1901, Ban Johnson, president of the Western League, a minor league organization, formed the American League to take advantage of the National League's 1900 contraction from twelve teams to eight. Johnson and fellow owners raided the National League and signed away many star players, including Cy Young and Jimmy Collins. Johnson had a list of 46 National Leaguers he targeted for the American League; by 1902, all but one had made the jump. The constant raiding, however, nixed the idea of a championship between the two leagues. Pirates owner Barney Dreyfuss, whose team ran away with the 1902 National League pennant, was open to a postseason contest and even said he would allow the American League champion to stock its roster with all-stars. However, Johnson had spoken of putting a team in Pittsburgh and even attempted to raid the Pirates' roster in August 1902, which soured Dreyfuss. At the end of the season, however, the Pirates played a group of American League All-Stars in a four-game exhibition series, winning two games to one, with one tie.\nThe leagues finally called a truce in the winter of 1902\u201303 and formed the National Commission to preside over organized baseball. The following season, the Boston Americans and Pittsburgh Pirates had secured their respective championship pennants by September. That August, Dreyfuss challenged the American League to an 11-game championship series. Encouraged by Johnson and National League President Harry Pulliam, Americans owner Henry J. Killilea met with Dreyfuss in Pittsburgh in September and instead agreed to a best-of-nine championship, with the first three games played in Boston, the next four in Allegheny City, and the remaining two (if necessary) in Boston.\nOne significant point about this agreement was that it was an arrangement primarily between the two clubs rather than a formal arrangement between the leagues. In short, it was a voluntary event, a fact which would result in no Series at all for . The formal establishment of the Series as a compulsory event started in .\nThe teams.\nThe Pirates won their third straight pennant in 1903 thanks to a powerful lineup that included legendary shortstop Honus Wagner, who hit .355 and drove in 101 runs, player-manager Fred Clarke, who hit .351, and Ginger Beaumont, who hit .341 and led the league in hits and runs. The Pirates' pitching was weaker than it had been in previous years but boasted 24-game winner Deacon Phillippe and 25-game winner Sam Leever.\nThe Americans had a strong pitching staff, led by Cy Young, who went 28\u20139 in 1903 and became the all-time wins leader that year. Bill Dinneen and Long Tom Hughes, right-handers like Young, had won 21 games and 20 games each. The Boston outfield, featuring Chick Stahl (.274), Buck Freeman (.287, 104 RBI) and Patsy Dougherty (.331, 101 runs scored) was considered excellent.\nAlthough the Pirates had dominated their league for the previous three years, they went into the series riddled with injuries and plagued by bizarre misfortunes. Otto Krueger, the team's only utility player, was beaned on September 19 and never fully played in the series. 16-game winner Ed Doheny left the team three days later, exhibiting signs of paranoia; he was committed to an insane asylum the following month. Leever had been battling an injury to his pitching arm (which he made worse by entering a trapshooting competition). Worst of all, Wagner, who had a sore thumb throughout the season, injured his right leg in September and was never 100 percent for the postseason.\nSome sources say Boston were heavy underdogs. Boston bookies actually gave even odds to the teams (and only because Dreyfuss and other \"sports\" were alleged to have bet on Pittsburgh to bring down the odds). The teams were generally thought to be evenly matched, with the Americans credited with stronger pitching and the Pirates with superior offense and fielding. The outcome, many believed, hinged on Wagner's health. \"If Wagner does not play, bet your money at two to one on Boston\", said the \"Sporting News\", \"but if he does play, place your money at two to one on Pittsburg.\"\nMatchups.\nGame 1.\nThe Pirates started Game 1 strong, scoring six runs in the first four innings, and held on to win the first World Series game in modern baseball history. They extended their lead to 7\u20130 on an inside-the-park home run by Jimmy Sebring in the seventh, the first home run in World Series history. Boston scored a few runs in the last three innings, but it was too little, too late; they ended up losing 7\u20133 in the first ever World Series game. Both Phillippe and Young threw complete games, with Phillippe striking out ten and Young fanning five, but Young also gave up twice as many hits and allowed three earned runs to Phillippe's two.\nGame 2.\nAfter starting out strong in Game 1, the Pirates simply shut down offensively, eking out a mere three hits, all singles. Pittsburgh starter Sam Leever went 1 inning and gave up three hits and two runs, before his ailing arm forced him to leave in favor of Bucky Veil, who finished the game. Bill Dinneen struck out 11 and pitched a complete game for the Americans, while Patsy Dougherty hit home runs in the first and sixth innings for two of the Boston's three runs. The Americans' Patsy Dougherty led off the Boston scoring with an inside-the-park home run, the first time a lead-off batter did just that until Alcides Escobar of the Kansas City Royals duplicated the feat in the 2015 World Series, 112 years later. Dougherty's second home run was the first in World Series history to actually sail over the fence, an incredibly rare feat at the time.\nGame 3.\nPhillippe, pitching after only a single day of rest, started Game 3 for the Pirates and didn't let them down, hurling his second complete-game victory of the Series to put Pittsburgh up two games to one.\nGame 4.\nAfter two days of rest, Phillippe was ready to pitch a second straight game. He threw his third complete-game victory of the series against Bill Dinneen, who was making his second start of the series. But Phillippe's second straight win was almost not to be, as the Americans, down 5\u20131 in the top of the ninth, rallied to narrow the deficit to one run. The comeback attempt failed, as Phillippe managed to put an end to it and give the Pirates a commanding 3\u20131 series lead.\nGame 5.\nGame 5 was a pitcher's duel for the first five innings, with Boston's Cy Young and Pittsburgh's Brickyard Kennedy giving up no runs. That changed in the top of the sixth, however, when the Americans scored a then-record six runs before being retired. Young, on the other hand, managed to keep his shutout intact before finally giving up a pair of runs in the bottom of the eighth. He went the distance and struck out four for his first World Series win.\nGame 6.\nGame 6 was a rematch between the starters of Game 2, Boston's Dinneen and Pittsburgh's Leever. Leever pitched a complete game this time but so did Dinneen, who outmatched him to earn his second complete-game victory of the series. After losing three of the first four games of the World Series, the underdog Americans had tied the series at three games apiece.\nGame 7.\nThe fourth and final game in Allegheny saw Phillippe start his fourth game of the Series for the Pirates. This time, however, he did not fare as well as he did in his first three starts. Cy Young, in his third start of the Series, held the Pirates to three runs and put the Americans ahead for the first time as the Series moved back to Boston.\nGame 8.\nThe final game of this inaugural World Series started out as an intense pitcher's duel, scoreless until the bottom of the fourth when Hobe Ferris hit a two-run single. Phillippe started his fifth and final game of the series and Dinneen his fourth. As he did in Game 2, Dinneen threw a complete-game shutout, striking out seven and leading his Americans to victory, while Phillippe pitched respectably but could not match Dinneen because his arm had been worn out with five starts in the eight games, giving up three runs to give the first 20th-century World Championship to the Boston Americans, Honus Wagner striking out to end the Series.\nComposite line score.\n1903 World Series (5\u20133): Boston Americans (A.L.) over Pittsburgh Pirates (N.L.)\nSeries Statistics.\nBoston Americans.\nBatting.\n\"Note: GP=Games Played; AB=At Bats; R=Runs; H=Hits; 2B=Doubles; 3B=Triples; HR=Home Runs; RBI=Runs Batted In; BB=Walks; AVG=Batting Average; OBP=On Base Percentage; SLG=Slugging Percentage\"\nPitching.\n\"Note: G=Games Played; GS=Games Started; ERA=Earned Run Average; W=Wins; L=Losses; SV=Saves; IP=Innings Pitched; H=Hits; R=Runs; ER= Earned Runs; BB=Walks; SO= Strikeouts\" \nPittsburgh Pirates.\nBatting.\n\"Note: GP=Games Played; AB=At Bats; R=Runs; H=Hits; 2B=Doubles; 3B=Triples; HR=Home Runs; RBI=Runs Batted In; BB=Walks; AVG=Batting Average; OBP=On Base Percentage; SLG=Slugging Percentage\"\nPitching.\n\"Note: G=Games Played; GS=Games Started; ERA=Earned Run Average; W=Wins; L=Losses; SV=Saves; IP=Innings Pitched; H=Hits; R=Runs; ER= Earned Runs; BB=Walks; SO= Strikeouts\""}
{"id": "3866", "revid": "11291818", "url": "https://en.wikipedia.org/wiki?curid=3866", "title": "Bluetongue disease", "text": "Bluetongue (BT) disease is a noncontagious, arthropod-borne viral disease affecting ruminants, primarily sheep and other domestic or wild ruminants, including cattle, yaks, goats, buffalo, deer, dromedaries, and antelope. It is caused by Bluetongue virus (BTV), a non-enveloped, double-stranded RNA virus belongs to the genus \"Orbivirus\" within the family \"Reoviridae\". The virus is mainly transmitted by biting midges, specifically \"Culicoides\" species (e.g. \"Culicoides imicola\", \"Culicoides oxystoma,\" and \"Culicoides variipennis\"). BTV has a widespread geographical distribution, encompassing numerous continents and regions, including Africa, Asia, Australia, Europe, North America, and various tropical and subtropical regions. At present, there are more than 28 recognized serotypes of BTV. Bluetongue outbreaks have had a significant economic impact, with estimated global losses reaching approximately 3 billion USD.\nClinical signs.\nIn sheep, BTV causes an acute disease with high morbidity and mortality. BTV also infects goats, cattle, and other domestic animals, as well as wild ruminants (for example, blesbuck, white-tailed deer, elk, and pronghorn antelope). The clinical signs are summarized under the term FFF (fever, face, feet).\nMajor signs are high fever, excessive salivation, swelling of the face and tongue, and cyanosis (in severe conditions) of the tongue. Swelling of the lips and tongue gives the tongue its typical blue appearance, though this sign is confined to a minority of the animals. Nasal signs may be prominent, with nasal discharge and stertorous respiration.\nSome animals also develop foot lesions, beginning with coronitis, with consequent lameness. In sheep, this can lead to knee-walking. In cattle, constant changing of position of the feet gives bluetongue the nickname the dancing disease. Torsion of the neck (opisthotonos or torticollis) is observed in severely affected animals.\nNot all animals develop signs, but all those that do lose condition rapidly, and the sickest die within a week. For affected animals that do not die, recovery is very slow, lasting several months.\nThe incubation period is 5\u201320 days, and all signs usually develop within a month. The mortality rate is normally low, but it is high in susceptible breeds of sheep. In Africa, local breeds of sheep may have no mortality, but in imported breeds, it may be up to 90%.\nThe manifestation of clinical signs in cattle is contingent upon the strain of virus. BTV-8 has been documented to cause a severe disease state and mortality in cattle. The current circulation of BTV-3 in Northern Europe is epidemiologically noteworthy due to the presentation of clinical signs in cattle and a higher sheep mortality rate than that observed with BTV-8. Other ruminants, such as goats, typically exhibit minimal or no clinical signs despite high virus levels in blood. Therefore, they could serve as potential virus reservoirs of BTV. Red deer are an exception, and in them the disease may be as acute as in sheep.\nLamb infected in utero can develop congenital hydranencephaly. This abnormality is a condition in which the brain's cerebral hemispheres are like swiss cheese, or absent, and replaced by sacs filled with cerebrospinal fluid. Ewes infected with bluetongue virus while pregnant can have lambs with this defect, as well as giving birth to lambs who are small, weak, deformed or blind. These affected lambs die within a few days of birth, or are born dead.\nMicrobiology.\nBluetongue is caused by the pathogenic vector-borne RNA virus, \"Bluetongue virus\" (\"BTV\"), of the genus \"Orbivirus\" within the \"Reoviridae\" family. The virus particle consists of 10 strands of double-stranded RNA surrounded by two protein shells. Unlike other arboviruses, BTV lacks a lipid envelope. The virus exhibits icosahedral symmetry, with a diameter of approximately 80\u201390 nm. The structure of the 70 nm core was determined in 1998 and was at the time the largest atomic structure to be solved. \nThe 10 viral genome segments have been found to encode 7 structural (VP1\u2013VP7) and 5 non-structural (NS1, NS2, NS3/NS3A, NS4 and NS5) proteins. There are currently more than 28 known serotypes of BTV. The sequence of genome Seg-2 and its translated protein VP2, as well as that of Seg-6 and its translated protein VP5, exhibit variations that determine the serotypes.\nThe two outer capsid proteins, VP2 and VP5, mediate attachment and penetration of BTV into the target cell. VP2 and VP5 are the primary antigenic targets for antibody targeting by the host immune system. The virus makes initial contact with the cell with VP2, triggering receptor-mediated endocytosis of the virus. The low pH within the endosome then triggers BTV's membrane penetration protein VP5 to undergo a conformational change that disrupts the endosomal membrane. Uncoating yields a transcriptionally active 470S core particle which is composed of two major proteins VP7 and VP3, and the three minor proteins VP1, VP4 and VP6 in addition to the dsRNA genome. There is no evidence that any trace of the outer capsid remains associated with these cores, as has been described for reovirus. The cores may be further uncoated to form 390S subcore particles that lack VP7, also in contrast to reovirus. Subviral particles are probably akin to cores derived \"in vitro\" from virions by physical or proteolytic treatments that remove the outer capsid and causes activation of the BTV transcriptase. In addition to the seven structural proteins, three non-structural (NS) proteins, NS1, NS2, NS3 (and a related NS3A) are synthesised in BTV-infected cells. Of these, NS3/NS3A is involved in the egress of the progeny virus. The two remaining non-structural proteins, NS1 and NS2, are produced at high levels in the cytoplasm and are believed to be involved in virus replication, assembly and morphogenesis.\nEvolution.\nThe viral genome is replicated via structural protein VP1, an RNA-dependent RNA polymerase. The lack of proof-reading abilities results in high levels of transcription errors, resulting in single nucleotide mutations. Despite this, the BTV genome is quite stable, exhibiting a low rate of variants arising in populations. Evidence suggests this is due to purifying selection across the genome as the virus is transmitted alternately through its insect and animal hosts. However, individual gene segments undergo different selective pressures and some, particularly segments 4 and 5, are subject to positive selection.\nThe BTV genome exhibits rapid evolution through genetic drift, reassortment of genome segments (genetic shift), and intragenic recombination. This evolutionary process, in conjunction with the random fixation of quasispecies variants during transmission between susceptible animals and vectors, is postulated to be the primary driver of the genetic diversity observed in BTV field strains. Reassortment can lead to a rapid shift in phenotypes independent of the slow rate of mutation. During this process, gene segments are not randomly reassorted. Rather, there appears to be a mechanism for selecting for or against certain segments from the parental serotypes present. However, this selective mechanism is still poorly understood.\nTo date, BTV serotypes 25 and above have been identified as the causative agents of infection in small ruminants. The infection is subclinical, which likely explains why these serotypes, which are less or non-virulent, have not been identified earlier through laboratory diagnosis studies. It is noteworthy that BTV serotypes 25 and higher are transmitted without midges, indicating that direct contact between sheep or goats may be a potential vector.\nEpidemiology.\nThe presence of the insect vectors determines the bluetongue disease's global distribution, with regions in Africa, Asia, Australia, Europe, North America, and other tropical/subtropical area being most affected.\nThe virus persists in areas where climatic conditions support the survival of \"Culicoides\" midges during winter. This adaptability allows the disease to establish itself in new regions when conditions become favorable.\nAn outline of the transmission cycle of BTV is illustrated in article Parasitic flies of domestic animals.\nIts occurrence is seasonal in the affected Mediterranean countries, subsiding when temperatures drop and hard frosts kill the adult midge vectors.\nViral survival and vector longevity is seen during milder winters.\nA significant contribution to the northward spread of bluetongue disease has been the ability of \"C. obsoletus\" and \"C.pulicaris\" to acquire and transmit the pathogen, both of which are spread widely throughout Europe. This is in contrast to the original \"C.imicola\" vector, which is limited to North Africa and the Mediterranean. The relatively recent novel vector has facilitated a far more rapid spread than the simple expansion of habitats north through global warming.\nIn August 2006, cases of bluetongue were found in the Netherlands, then Belgium, Germany, and Luxembourg.\nIn 2007, the first case of bluetongue in the Czech Republic was detected in one bull near Cheb at the Czech-German border.\nIn September 2007, the UK reported its first ever suspected case of the disease, in a Highland cow on a rare-breeds farm near Ipswich, Suffolk.\nSince then, the virus has spread from cattle to sheep in Britain.\nBy October 2007, bluetongue had become a serious threat in Scandinavia and Switzerland\nand the first outbreak in Denmark was reported. In autumn 2008, several cases were reported in the southern Swedish provinces of Sm\u00e5land, Halland, and Sk\u00e5ne,\nas well as in areas of the Netherlands bordering Germany, prompting veterinary authorities in Germany to intensify controls.\nNorway had its first finding in February 2009, when cows at two farms in Vest-Agder in the south of Norway showed an immune response to bluetongue. A number of countries, including Norway and Finland, were certified as free of the disease in 2011 and 2021, respectively.\nIn 2023, Europe witnessed a series of notable epizootic occurrences at higher latitudes, partially attributable to the emergence of a novel serotype, BTV-3. The serotype was first identified in the Netherlands in September 2023 and has since been documented in numerous European countries, including Belgium, Germany, the Netherlands, France, Spain, the UK, Norway, and Sweden.\nAlthough the disease is not a threat to humans, the most vulnerable common domestic ruminants are cattle, goats, and especially, sheep.\nOverwintering.\nA puzzling aspect of BTV is its survival between midge seasons in temperate regions. Adults of \"Culicoides\" are killed by cold winter temperatures, and BTV infections typically do not last for more than 60 days, which is not long enough for BTV to survive until the next spring. It is believed that the virus somehow survives in overwintering midges or animals. Multiple mechanisms have been proposed. A few adult \"Culicoides\" midges infected with BTV may survive the mild winters of the temperate zone. Some midges may even move indoors to avoid the cold temperature of the winter. Additionally, BTV could cause a chronic or latent infection in some animals, providing another means for BTV to survive the winter. BTV can also be transmitted from mother to fetus. The outcome is abortion or stillbirth if fetal infection occurs early in gestation and survival if infection occurs late. However infection at an intermediate stage, before the fetal immune system is fully developed, may result in a chronic infection that lingers until the first months after birth of the lamb. Midges then spread the pathogen from the calves to other animals, starting a new season of infection.\nClimate change.\nThe spread of bluetongue to Southern, Central, and Northern Europe provides an illustrative example of the complex interactions between climate change, vector habitat suitability, animal population density, distribution, and movement, which collectively influence the patterns of disease emergence and transmission.\nTreatment and prevention.\nThere are currently no antiviral medications that have been approved for the treatment of bluetongue disease. The standard of care involves the administration of anti-inflammatory drugs and supportive nursing care to alleviate the clinical signs and symptoms. Prevention is effected via quarantine, vaccination, and control of the midges vector, including inspection of aircraft. The recurrent emergence of novel strains and the occurrence of new outbreaks with significant socio-economic impacts highlight the urgent need for effective antiviral strategies. The current vaccines for bluetongue virus (BTV) are serotype-specific, which limits their utility and has led to interest in host-targeted antiviral strategies that offer broader activity against multiple serotypes and a reduced risk of resistance development.\nLivestock management and insect control.\nSome available key measures include vector control, such as the use of insecticides, insect-proof nets, and improved housing to reduce exposure to biting midges. Additionally, the removal of infected animals helps prevent further transmission by reducing the number of viremic hosts, while movement restrictions\u2014including quarantines and health certifications\u2014prevent the introduction of the virus to uninfected regions.\nVaccines.\nVaccination still represents an effective strategy for protecting ruminants against bluetongue. However, this is only possible with a vaccine that is effective against the relevant serotype. The most prevalent vaccines are live attenuated vaccines and killed or inactivated vaccines. Other potential vaccines include subunit vaccines, virus-like particles, DNA vaccines, disabled unfectious single animal vaccines (DISA), and disabled infectious single-cycle vaccines (DISC).\nProtection by live attenuated vaccines (LAVs) are serotype specific. Multiserotype LAV cocktails can induce neutralizing antibodies against unincluded serotypes, and subsequent vaccinations with three different pentavalent LAV cocktails induce broad protection. These pentavalent cocktails contain 15 different serotypes in total: serotypes 1 through 14, as well as 19.\nImmunization with any of the available vaccines, though, precludes later serological monitoring of affected cattle populations, a problem that could be resolved using next-generation subunit vaccines.\nIn January 2015, the vaccine Raksha Blu was launched in India. It is designed to protect livestock against five strains of the bluetongue virus.\nThe vaccine Syvazul BTV was authorized for veterinary use in the European Union in January 2019.\nIn January 2025, the Committee for Veterinary Medicinal Products (CVMP) of the European Medicines Agency adopted a positive opinion, recommending the granting of a marketing authorization for the veterinary medicinal product Bluevac-3, suspension for injection, intended for cattle and sheep. The applicant for this veterinary medicinal product is CZ Vaccines S.A.U. Bluevac-3 is a vaccine containing inactivated bluetongue virus, serotype 3, BTV-3/NET2023 as active substance. The vaccine is intended to stimulate the active immunity of sheep and cattle against bluetongue virus serotype 3. The CVMP also adopted a positive opinion, recommending the granting of a marketing authorization for the veterinary medicinal product Syvazul BTV 3, suspension for injection, intended for sheep. The applicant for this veterinary medicinal product is Laboratorios Syva S.A. Syvazul BTV 3 is a vaccine containing Bluetongue virus, serotype 3, BTV-3/NET2023, inactivated as active substance. It is intended for the active immunization of sheep against bluetongue virus serotype 3.\nHistory.\nIn the early stages of its identification, BT was referred to by a number of different names, including \"epizootic catarrh,\" \"fever,\" \"malarial catarrhal fever of sheep,\" and \"epizootic malignant catarrhal fever of sheep.\" This was due to the prevailing belief at the time that BT was caused by an intraerythrocytic parasite. The English translation \"Bluetongue\" was initially proposed by Spreull and derived from the Afrikaans term \"bloutong,\" which refers to the condition of cyanosis of the tongue in clinically affected sheep. Although bluetongue disease was already recognized in South Africa in the early 19th century, a comprehensive description of the disease was not published until the first decade of the 20th century. In 1906, Arnold Theiler showed that bluetongue was caused by a filterable agent. He also created the first bluetongue vaccine, which was developed from an attenuated BT V strain. For many decades, bluetongue was thought to be confined to Africa. The first confirmed outbreak outside of Africa occurred in Cyprus in 1943.\nIn 2021, a vessel owned by Khalifeh Livestock Trading and managed by Talia Shipping Line, both based in Lebanon, has been denied right to dock in Spain, as it has about 895 male calves suspected to be infected by bluetongue disease.\nRelated diseases.\nAfrican horse sickness is related to bluetongue and is spread by the same midges (\"Culicoides\" species). It can kill the horses it infects and mortality may go as high as 90% of the infected horses during an epidemic.\n\"Epizootic hemorrhagic disease virus\" is closely related and crossreacts with \"Bluetongue virus\" on many blood tests."}
{"id": "3869", "revid": "45789152", "url": "https://en.wikipedia.org/wiki?curid=3869", "title": "Bruce Perens", "text": "Bruce Perens (born around 1958) is an American computer programmer and advocate in the free software movement. He created \"The Open Source Definition\" and published the first formal announcement and manifesto of open source. He co-founded the Open Source Initiative (OSI) with Eric S. Raymond.\nIn 2005, Perens represented Open Source at the United Nations World Summit on the Information Society, at the invitation of the United Nations Development Programme. He has appeared before national legislatures and is often quoted in the press, advocating for open source and the reform of national and international technology policy.\nPerens is also an amateur radio operator, with call sign K6BP. He promotes open radio communications standards and open-source hardware.\nIn 2016 Perens, along with Boalt Hall (Berkeley Law) professor Lothar Determann, co-authored \"Open Cars\" which appeared in the Berkeley Technology Law Journal.\nIn 2018 Perens founded the Open Research Institute (ORI), a non-profit research and development organization to address technologies involving Open Source, Open Hardware, Open Standards, Open Content, and Open Access to Research. In April 2022 he divorced himself from the organization and reported he was starting a new charity, HamOpen.org, to redirect his focus, and align with the ARRL organization for their liability insurance benefit. HamOpen has been most visible supporting the convention exhibitions of projects Perens supports, including M17 and FreeDV.\nCompanies.\nPerens operates two companies: Algoram is a start-up which is creating a web-based control system for radio transmitters and other devices. Legal Engineering is a legal-technical consultancy which specializes in resolving copyright infringement in relation to open source software.\nEarly life.\nPerens grew up in Long Island, New York. He was born with cerebral palsy, which caused him to have slurred speech as a child, a condition that led to a misdiagnosis of him as developmentally disabled in school and led the school to fail to teach him to read. He developed an interest in technology at an early age: besides his interest in amateur radio, he ran a pirate radio station in the town of Lido Beach and briefly engaged in phone phreaking.\nCareer.\nComputer graphics.\nPerens worked for seven years at the New York Institute of Technology Computer Graphics Lab. After that, he worked at Pixar for 12 years, from 1987 to 1999. He is credited as a studio tools engineer on the Pixar films \"A Bug's Life\" (1998) and \"Toy Story 2\" (1999).\nNo-Code International.\nPerens founded No-Code International in 1998 with the goal of ending the Morse Code test then required for an amateur radio license. His rationale was that amateur radio should be a tool for young people to learn advanced technology and networking, rather than something that preserved antiquity and required new hams to master outmoded technology before they were allowed on the air.\nPerens lobbied intensively on the Internet, at amateur radio events in the United States, and during visits to other nations. One of his visits was to Iceland, where he had half of that nation's radio amateurs in the room, and their vote in the International Amateur Radio Union was equivalent to that of the entire United States.\nDebian Social Contract.\nIn 1997, Perens was carbon-copied on an email conversation between Donnie Barnes of Red Hat and Ean Schuessler, who was then working on Debian. Schuessler bemoaned that Red Hat had never stated its social contract with the developer community. Perens took this as inspiration to create a formal social contract for Debian. In a blog posting, Perens claims not to have made use of the Three Freedoms (later the Four Freedoms) published by the Free Software Foundation in composing his document. Perens proposed a draft of the Debian Social Contract to the Debian developers on the debian-private mailing list early in June 1997. Debian developers contributed discussion and changes for the rest of the month while Perens edited, and the completed document was then announced as Debian project policy. Part of the Debian Social Contract was the Debian Free Software Guidelines, a set of 10 guidelines for determining whether a set of software can be described as \"free software\", and thus whether it could be included in Debian.\nOpen Source Definition and The Open Source Initiative.\nOn February 3, 1998, a group of people (not including Perens) met at VA Linux Systems to discuss the promotion of Free Software to business in pragmatic terms, rather than the moral terms preferred by Richard Stallman. Christine Petersen of the nanotechnology organization Foresight Institute, who was present because Foresight took an early interest in Free Software, suggested the term \"Open Source\". The next day, Eric S. Raymond recruited Perens to work with him on the formation of Open Source. Perens modified the Debian Free Software Guidelines into the Open Source Definition by removing Debian references and replacing them with \"Open Source\".\nThe original announcement of The Open Source Definition was made on February 9, 1998, on Slashdot and elsewhere; the definition was given in Linux Gazette on February 10, 1998.\nConcurrently, Perens and Raymond established the Open Source Initiative, an organization intended to promote open source software.\nPerens left OSI in 1999, a year after co-founding it. In February 1999 in an email to the Debian developers mailing list he explained his decision and stated that, though \"most hackers know that Free Software and Open Source are just two words for the same thing\", the success of \"open source\" as a marketing term had \"de-emphasized the importance of the freedoms involved in Free Software\"; he added, \"It's time for us to fix that.\" He stated his regret that OSI co-founder Eric Raymond \"seems to be losing his free software focus.\" But in the following 2000s he spoke about Open source again. Perens presently volunteers as the Open Source Initiative's representative to the European Technical Standards Institute (\"ETSI\"), and is a frequent participant in review of license texts submitted to OSI for certification as Open Source licenses.\nLinux Capital Group.\nIn 1999, Perens left Pixar and became the president of Linux Capital Group, a business incubator and venture capital firm focusing on Linux-based businesses. Their major investment was in Progeny Linux Systems, a company headed by Debian founder Ian Murdock. In 2000, as a result of the economic downturn, Perens shut down Linux Capital Group. (Progeny Linux Systems would end operations in 2007.)\nHewlett-Packard.\nFrom December 2000 to September 2002, Perens served as \"Senior Global Strategist for Linux and Open Source\" at Hewlett-Packard, internally evangelizing for the use of Linux and other open-source software. He was fired as a result of his anti-Microsoft statements, which especially became an issue after HP acquired Compaq, a major manufacturer of Microsoft Windows-based PCs, in 2002.\nUserLinux.\nIn 2003 Perens created UserLinux, a Debian-based distribution whose stated goal was, \"Provide businesses with freely available, high quality Linux operating systems accompanied by certifications, service, and support options designed to encourage productivity and security while reducing overall costs.\" UserLinux was eventually overtaken in popularity by Ubuntu, another Debian-based distribution, which was started in 2004, and UserLinux became unmaintained in 2006.\nSourceLabs.\nPerens was an employee of SourceLabs, a Seattle-based open source software and services company, from June 2005 until December 2007. He produced a video commercial, \"Impending Security Breach\", for SourceLabs in 2007. (SourceLabs was acquired by EMC in 2009.)\nUniversity faculty.\nBetween 1981 and 1986, Perens was on the staff of the New York Institute of Technology Computer Graphics Lab as a Unix kernel programmer.\nIn 2002, Perens was a remote Senior Scientist for Open Source with the Cyber Security Policy Laboratory of George Washington University under the direction of Tony Stanco. Stanco was director of the laboratory for a year, while its regular director was on sabbatical.\nBetween 2006 and 2007, Perens was a visiting lecturer and researcher for the University of Agder under a three-year grant from the Competence Fund of Southern Norway. During this time he consulted the Norwegian Government and other entities on government policy issues related to computers and software. After this time Perens worked remotely on Agder programs, mainly concerning the European Internet Accessibility Observatory.\nOther activities.\nIn 2007, some of Perens's government advisory roles included a meeting with the President of the Chamber of Deputies (the lower house of parliament) in Italy and testimony to the Culture Committee of the Chamber of Deputies; a keynote speech at the foundation of Norway's Open Source Center, following Norway's Minister of Governmental Reform (Perens is on the advisory board of the center); he provided input on the revision of the European Interoperability Framework; and he was keynote speaker at a European Commission conference on \"Digital Business Ecosystems at the Centre Borschette, Brussels, on November 7\".\nIn 2009, Perens acted as an expert witness on open source in the Jacobsen v. Katzer U.S. Federal lawsuit. His report, which was made publicly available by Jacobsen, presented the culture and impact of open-source software development to the federal courts.\nPerens delivered one of the keynote addresses at the 2012 linux.conf.au conference in Ballarat, Australia. He discussed the need for open source software to market itself better to non-technical users. He also discussed some of the latest developments in open-source hardware, such as Papilio and Bus Pirate.\nIn 2013, Perens spoke in South America, as the closing keynote at Latinoware 2013. He was the keynote of CISL \u2013 Conferencia Internacional de Software Libre, in Buenos Aires, Argentina, and keynoted a special event along with the Minister of software and innovation of Chubut Province, in Puerto Madrin, Patagonia, Argentina. He keynoted the Festival de Software Libre 2013, in Puerto Vallarta, Mexico.\nIn 2014\u20132015, Perens took a break from Open Source conferences, having spoken at them often since 1996. In 2016, he returned to the conference circuit, keynoting the Open Source Insight conference in Seoul, sponsored by the Copyright Commission of South Korea. Perens web site presently advertises his availability to keynote conferences as long as travel and lodging expenses are compensated.\nIn 2020, Perens delivered the talk, \"What Comes After Open Source?\" for DebConf 2020. He discussed the future of open source licensing and the need to develop alternative licensing structures so that open source developers could get paid for their work.\nViews.\nPerens poses \"Open Source\" as a means of marketing the free and open-source software idea to business people and mainstream who might be more interested in the practical benefits of an open source development model and ecosystem than abstract ethics. He states that open source and free software are only two ways of talking about the same phenomenon, a point of view not shared by Stallman and his free software movement. Perens postulated in 2004 an economic theory for business use of Open Source in his paper \"The Emerging Economic Paradigm of Open Source\" and his speech \"Innovation Goes Public\". This differs from Raymond's theory in \"The Cathedral and the Bazaar\", which having been written before there was much business involvement in open source, explains open source as a consequence of programmer motivation and leisure.\nIn February 2008, for the 10th anniversary of the phrase \"open source\", Perens published a message to the community called \"State of Open Source Message: A New Decade For Open Source\". Around the same time the ezine RegDeveloper published an interview with Perens where he spoke of the successes of open source, but also warned of dangers, including a proliferation of OSI-approved licenses which had not undergone legal scrutiny. He advocated the use of the GPLv3 license, especially noting Linus Torvalds' refusal to switch away from GPLv2 for the Linux kernel.\nBruce Perens supported Bernie Sanders for President and he claims that his experience with the open source movement influenced that decision. On July 13, 2016, following Sanders's endorsement of Hillary Clinton for president, Perens endorsed Clinton.\nIn January 2013, Perens advocated for abolishment of the Second Amendment to the U.S. constitution, stating that he does \"not believe in private ownership of firearms\" and that he would \"take away guns currently held by individuals, without compensation for their value.\" He reiterated this view in a June 2014 interview in Slashdot, and in November 2017 on his Twitter account.\nAmateur radio and other activities.\nPerens is an avid amateur radio enthusiast (call sign K6BP) and maintained technocrat.net, which he closed in late 2008, because its revenues did not cover its costs.\nMedia appearances.\nPerens is featured in the 2001 documentary film \"Revolution OS\" and the 2006 BBC television documentary \"The Code-Breakers\".\nFrom 2002 to 2006, Prentice Hall PTR published the Bruce Perens' Open Source Series, a set of 24 books covering various open source software tools, for which Perens served as the series editor. It was the first book series to be published under an open license.\nPersonal life.\nPerens lives in Berkeley, California with his wife, Valerie, and son, Stanley, born in 2000."}
{"id": "3870", "revid": "18354904", "url": "https://en.wikipedia.org/wiki?curid=3870", "title": "Bundle theory", "text": "Bundle theory, originated by the 18th century Scottish philosopher David Hume, is the ontological theory about objecthood in which an object consists only of a collection (\"bundle\") of properties, relations or tropes.\nAccording to bundle theory, an object consists of its properties and nothing more; thus, there cannot be an object without properties and one cannot \"conceive\" of such an object. For example, when we think of an apple, we think of its properties: redness, roundness, being a type of fruit, \"etc\". There is nothing above and beyond these properties; the apple is nothing more than the collection of its properties. In particular, there is no \"substance\" in which the properties are \"inherent\".\nBundle theory has been contrasted with the \"ego theory\" of the self, which views the egoic self as a soul-like substance existing in the same manner as the corporeal self.\nArguments in favor.\nThe difficulty in conceiving and or describing an object without also conceiving and or describing its properties is a common justification for bundle theory, especially among current philosophers in the Anglo-American tradition.\nThe inability to comprehend any aspect of the thing other than its properties implies, this argument maintains, that one cannot conceive of a \"bare particular\" (a \"substance\" without properties), an implication that directly opposes substance theory. The conceptual difficulty of \"bare particulars\" was illustrated by John Locke when he described a \"substance\" by itself, apart from its properties as \"something, I know not what. [...] The idea then we have, to which we give the general name substance, being nothing but the supposed, but unknown, support of those qualities we find existing, which we imagine cannot subsist sine re substante, without something to support them, we call that support substantia; which, according to the true import of the word, is, in plain English, standing under or upholding.\"\nWhether a \"relation\" of an object is one of its properties may complicate such an argument. However, the argument concludes that the conceptual challenge of \"bare particulars\" leaves a bundle of properties and nothing more as the only possible conception of an object, thus justifying bundle theory.\nObjections.\nBundle theory maintains that properties are \"bundled\" together in a collection without describing how they are tied together. For example, bundle theory regards an apple as red, four inches (100\u00a0mm) wide, and juicy but lacking an underlying \"substance\". The apple is said to be a \"bundle of properties\" including redness, being four inches (100\u00a0mm) wide, and juiciness.\nHume used the term \"bundle\" in this sense, also referring to the personal identity, in his main work: \"I may venture to affirm of the rest of mankind, that they are nothing but a bundle or collection of different perceptions, which succeed each other with inconceivable rapidity, and are in a perpetual flux and movement\".\nCritics question how bundle theory accounts for the properties' \"compresence\" (the \"togetherness\" relation between those properties) without an underlying \"substance\". Critics also question how any two given properties are determined to be properties of the same object if there is no \"substance\" in which they both \"inhere\". This argument is done away with if one considers spatio-temporal location to be a property as well.\nTraditional bundle theory explains the \"compresence\" of properties by defining an object as a collection of properties \"bound\" together. Thus, different combinations of properties and relations produce different objects. Redness and juiciness, for example, may be found together on top of the table because they are part of a bundle of properties located on the table, one of which is the \"looks like an apple\" property.\nBy contrast, substance theory explains the \"compresence\" of properties by asserting that the properties are found together because it is the \"substance\" that has those properties. In substance theory, a \"substance\" is the thing in which properties \"inhere\". For example, redness and juiciness are found on top of the table because redness and juiciness \"inhere\" in an apple, making the apple red and juicy.\nThe \"bundle theory of substance\" explains \"compresence\". Specifically, it maintains that properties' compresence itself engenders a \"substance\". Thus, it determines \"substancehood\" empirically by the \"togetherness\" of properties rather than by a \"bare particular\" or by any other non-empirical underlying strata. The \"bundle theory of substance\" thus rejects the substance theories of Aristotle, Descartes, Leibniz, and more recently, J. P. Moreland, Jia Hou, Joseph Bridgman, Quentin Smith, and others.\nBuddhism.\nThe Indian Madhyamaka philosopher, Chandrakirti, used the aggregate nature of objects to demonstrate the lack of essence in what is known as the sevenfold reasoning. In his work, \"Guide to the Middle Way\" (Sanskrit: \"Madhyamak\u0101vat\u0101ra\"), he says:\nHe goes on to explain what is meant by each of these seven assertions, but briefly in a subsequent commentary he explains that the conventions of the world do not exist essentially when closely analyzed, but exist only through being taken for granted, without being subject to scrutiny that searches for an essence within them.\nAnother view of the Buddhist theory of the self, especially in early Buddhism, is that the Buddhist theory is essentially an eliminativist theory. According to this understanding, the self can not be reduced to a bundle because there is nothing that answers to the concept of a self. Consequently, the idea of a self must be eliminated."}
{"id": "3871", "revid": "276448", "url": "https://en.wikipedia.org/wiki?curid=3871", "title": "Bare particular", "text": ""}
{"id": "3873", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=3873", "title": "Bernard Montgomery", "text": "Field Marshal Bernard Law Montgomery, 1st Viscount Montgomery of Alamein (; 17 November 1887\u00a0\u2013 24 March 1976), nicknamed \"Monty\", was a senior British Army officer who served in the First World War, the Irish War of Independence and the Second World War.\nMontgomery first saw action in the First World War as a junior officer of the Royal Warwickshire Regiment. At M\u00e9teren, near the Belgian border at Bailleul, he was shot through the right lung by a sniper, during the First Battle of Ypres. On returning to the Western Front as a general staff officer, he took part in the Battle of Arras in AprilMay 1917. He also took part in the Battle of Passchendaele in late 1917 before finishing the war as chief of staff of the 47th (2nd London) Division. In the inter-war years he commanded the 17th (Service) Battalion, Royal Fusiliers and, later, the 1st Battalion, Royal Warwickshire Regiment before becoming commander of the 9th Infantry Brigade and then general officer commanding (GOC), 8th Infantry Division.\nDuring the Western Desert campaign of the Second World War, Montgomery commanded the British Eighth Army from August 1942. He subsequently commanded the British Eighth Army during the Allied invasion of Sicily and the Allied invasion of Italy and was in command of all Allied ground forces during the Battle of Normandy (Operation Overlord), from D-Day on 6 June 1944 until 1 September 1944. He then continued in command of the 21st Army Group for the rest of the , including the failed attempt to cross the Rhine during Operation Market Garden. When German armoured forces broke through the US lines in Belgium during the Battle of the Bulge, Montgomery received command of the northern shoulder of the Bulge. Montgomery's 21st Army Group, including the US Ninth Army and the First Allied Airborne Army, crossed the Rhine in Operation Plunder in March 1945. By the end of the war, troops under Montgomery's command had taken part in the encirclement of the Ruhr Pocket, liberated the Netherlands, and captured much of north-west Germany. On 4 May 1945, Montgomery accepted the surrender of the German forces in north-western Europe at L\u00fcneburg Heath, south of Hamburg, after the surrender of Berlin to the USSR on 2 May.\nAfter the war he became Commander-in-Chief of the British Army of the Rhine (BAOR) in Germany and then Chief of the Imperial General Staff (1946\u20131948). From 1948 to 1951, he served as Chairman of the Commanders-in-Chief Committee of the Western Union. He then served as NATO's Deputy Supreme Allied Commander Europe until his retirement in 1958.\nEarly life.\nMontgomery was born in Kennington, Surrey, in 1887, the fourth child of nine, to a Church of Ireland minister, Henry Montgomery, and his wife Maud (\"n\u00e9e\" Farrar). The Montgomerys, an Ulster Scots 'Ascendancy' gentry family, were the County Donegal branch of the Clan Montgomery. The Rev. Henry Montgomery, at that time Vicar of St Mark's Church, Kennington, was the second son of Sir Robert Montgomery, a native of Inishowen in County Donegal in the north-west of Ulster, and a noted colonial administrator in British India. Sir Robert died a month after his grandson's birth. He was probably a descendant of Colonel Alexander Montgomery. Bernard's mother, Maud, was the daughter of Frederic William Canon Farrar, the famous preacher, and was eighteen years younger than her husband.\nAfter the death of Sir Robert Montgomery, Henry inherited the Montgomery ancestral estate of New Park in Moville, a small town in Inishowen in the north of County Donegal in Ulster, the northern province in Ireland. There was still \u00a313,000 to pay on a mortgage, a large debt in the 1880s (equivalent to \u00a3 in ) and Henry was at the time still only an Anglican vicar. Despite selling off all the farms that were in the townland of Ballynally, on the north-western shores of Lough Foyle, \"there was barely enough to keep up New Park and pay for the blasted summer holiday\" (i.e., at New Park).\nIt was a financial relief of some magnitude when, in 1889, Henry was made Bishop of Tasmania, then still a British colony, and Bernard spent his formative years there. Bishop Montgomery considered it his duty to spend as much time as possible in the rural areas of Tasmania and was away for up to six months at a time. While he was away, his wife, still in her mid-twenties, gave her children \"constant\" beatings, then ignored them most of the time. Of Bernard's siblings, Sibyl died prematurely in Tasmania, and Harold, Donald and Una all emigrated. Maud Montgomery took little active interest in the education of her young children other than to have them taught by tutors brought from Britain, although he briefly attended the then coeducational St Michael's Collegiate School. The loveless environment made Bernard something of a bully, as he himself recalled: \"I was a dreadful little boy. I don't suppose anybody would put up with my sort of behaviour these days.\" Later in life Montgomery refused to allow his son David to have anything to do with his grandmother, and refused to attend her funeral in 1949.\nThe family returned to England once for a Lambeth Conference in 1897, and Bernard and his brother Harold were educated at The King's School, Canterbury. In 1901, Bishop Montgomery became secretary of the Society for the Propagation of the Gospel, and the family returned to London. Montgomery attended St Paul's School and then the Royal Military College, Sandhurst, from which he was almost expelled for rowdiness and violence. On graduation in September 1908 he was commissioned into the 1st Battalion the Royal Warwickshire Regiment as a second lieutenant, and first saw overseas service later that year in India. He was promoted to lieutenant in 1910, and in 1912 became adjutant of the 1st Battalion of his regiment at Shorncliffe Army Camp.\nFirst World War.\nThe Great War began in August 1914 and Montgomery moved to France with his battalion that month, which was at the time part of the 10th Infantry Brigade of the 4th Division of the British Expeditionary Force (BEF). He was promoted to temporary captain on 14 September. He saw action at the Battle of Le Cateau that month and during the retreat from Mons. At M\u00e9teren, near the Belgian border at Bailleul on 13 October 1914, during an Allied counter-offensive, he was shot through the right lung by a sniper. Lying in the open, he remained still and pretended to be dead, in the hope that he would not receive any more enemy attention. One of his men did attempt to rescue him but was shot dead by a hidden enemy sniper and collapsed over Montgomery. The sniper continued to fire and Montgomery was hit once more, in the knee, but the dead soldier, in Montgomery's words, \"received many bullets meant for me.\" Assuming them to both be dead, the officers and men of Montgomery's battalion chose to leave them where they were until darkness arrived and stretcher bearers managed to recover the two bodies, with Montgomery by this time barely clinging on to life. The doctors at the advanced dressing station (ADS), too, had no hope for him and ordered a grave to be dug. Miraculously, however, Montgomery was still alive and, after being placed in an ambulance and then being sent to a hospital, was treated and eventually evacuated to England, where he would remain for well over a year. He was appointed a Companion of the Distinguished Service Order (DSO), for his gallant leadership during this period: the citation for this award, published in \"The London Gazette\" in December 1914 reads:\nAfter recovering in early 1915, he was appointed brigade major, first of the 112th Infantry Brigade, and then with 104th Infantry Brigade, then training in Lancashire. He returned to the Western Front in early 1916 with his brigade, seeing service with it during the Battle of the Somme later in the year. In January 1917 he was assigned as a general staff officer, grade 2 (GSO2) with the 33rd Division and took part in the Battle of Arras in AprilMay. In July he transferred over as a GSO2 to IX Corps, part of General Sir Herbert Plumer's Second Army.\nIt was in this role that Montgomery served at the Battle of Passchendaele which began in late July 1917. He was promoted to the temporary rank of major in February 1918, and brevet major in June. He finished the war in November 1918 as GSO1 (effectively chief of staff) of the 47th (2nd London) Division, with the temporary rank of lieutenant colonel, to which appointment and rank he had been assigned to on 16 July. A photograph from October 1918, reproduced in many biographies, shows the then unknown Lieutenant-Colonel Montgomery standing in front of Winston Churchill (then the Minister of Munitions) at the parade following the liberation of Lille.\nMontgomery was profoundly influenced by his experiences during the war, in particular by the leadership, or rather the lack of it, being displayed by the senior commanders. He later wrote:\nBetween the world wars.\n1920s and Ireland.\nAfter the First World War, Montgomery commanded the 17th (Service) Battalion of the Royal Fusiliers, a battalion in the British Army of the Rhine, before reverting to his substantive rank of captain (brevet major) in November 1919. He had not at first been selected for the Staff College in Camberley, Surrey (his only hope of ever achieving high command). But at a tennis party in Cologne, he was able to persuade the Commander-in-chief (C-in-C) of the British Army of Occupation, Field Marshal Sir William Robertson, to add his name to the list.\nAfter graduating from the Staff College, he was appointed brigade major in the 17th Infantry Brigade in January 1921. The brigade was stationed in County Cork, Ireland, carrying out counter-guerilla operations during the final stages of the Irish War of Independence.\nMontgomery came to the conclusion that the conflict could not be won without harsh measures, and that self-government for Ireland was the only feasible solution; in 1923, after the establishment of the Irish Free State and during the Irish Civil War, Montgomery wrote to Colonel Arthur Ernest Percival of the Essex Regiment:\nIn one noteworthy incident on 2 May 1922, Montgomery led a force of 60 soldiers and 4 armoured cars to the town of Macroom to search for four British officers who were missing in the area. While he had hoped the show of force would assist in finding the men, he was under strict orders not to attack the IRA. On arriving in the town square in front of Macroom Castle, he summoned the IRA commander, Charlie Browne, to parley. At the castle gates Montgomery spoke to Browne explaining what would happen should the officers not be released. Once finished, Browne responded with his own ultimatum to Montgomery to \"leave town within 10 minutes\". Browne then turned heels and returned to the Castle. At this point another IRA officer, Pat O'Sullivan, whistled to Montgomery drawing his attention to scores of IRA volunteers who had quietly taken up firing positions all around the square\u2014surrounding Montgomery's forces. Realising his precarious position, Montgomery led his troops out of the town, a decision which raised hostile questions in the House of Commons but was later approved by Montgomery's own superiors. Unknown to Montgomery at this time, the four missing officers had already been executed.\nIn May 1923, Montgomery was posted to the 49th (West Riding) Infantry Division, a Territorial Army (TA) formation. He returned to the 1st Battalion, Royal Warwickshire Regiment in 1925 as a company commander and was promoted to major in July 1925. From January 1926 to January 1929 he served as Deputy Assistant Adjutant General at the Staff College, Camberley, in the temporary rank of lieutenant-colonel.\nMarriage and family.\nIn 1925, in his first known courtship of a woman, Montgomery, then in his late thirties, proposed to a 17-year-old girl, Betty Anderson. His approach included drawing diagrams in the sand of how he would deploy his tanks and infantry in a future war, a contingency which seemed very remote at that time. She respected his ambition and single-mindedness but declined his proposal.\nIn 1927, he met and married Elizabeth (Betty) Carver, \"n\u00e9e\" Hobart. She was the sister of the future Second World War commander Sir Percy Hobart. Betty Carver had two sons in their early teens, John and Dick, from her first marriage to Oswald Carver. Dick Carver later wrote that it had been \"a very brave thing\" for Montgomery to take on a widow with two children. Montgomery's son, David, was born in August 1928.\nWhile on holiday in Burnham-on-Sea in Somerset in 1937, Betty suffered an insect bite which became infected, and she died in her husband's arms from septicaemia following amputation of her leg. The loss devastated Montgomery, who was then serving as a brigadier, but he insisted on throwing himself back into his work immediately after the funeral. Montgomery's marriage had been extremely happy. Much of his correspondence with his wife was destroyed when his quarters at Portsmouth were bombed during the Second World War. After Montgomery's death, John Carver wrote that his mother had arguably done the country a favour by keeping his personal oddities\u2014his extreme single-mindedness, and his intolerance of and suspicion of the motives of others\u2014within reasonable bounds long enough for him to have a chance of attaining high command.\nBoth of Montgomery's stepsons became army officers in the 1930s (both were serving in India at the time of their mother's death), and both served in the Second World War, each eventually attaining the rank of colonel. While serving as a GSO2 with Eighth Army, Dick Carver was sent forward during the pursuit after El Alamein to help identify a new site for Eighth Army HQ. He was taken prisoner at Mersa Matruh on 7 November 1942. Montgomery wrote to his contacts in England asking that inquiries be made via the Red Cross as to where his stepson was being held, and that parcels be sent to him. Like many British POWs, the most famous being General Richard O'Connor, Dick Carver escaped in September 1943 during the brief hiatus between Italy's departure from the war and the German seizure of the country. He eventually reached British lines on 5 December 1943, to the delight of his stepfather, who sent him home to Britain to recuperate.\n1930s.\nIn January 1929 Montgomery was promoted to brevet lieutenant-colonel. That month he returned to the 1st Battalion, Royal Warwickshire Regiment again, as Commander of Headquarters Company; he went to the War Office to help write the Infantry Training Manual in mid-1929. In 1931 Montgomery was promoted to substantive lieutenant-colonel and became the Commanding officer (CO) of the 1st Battalion, Royal Warwickshire Regiment and saw service in Palestine and British India. He was promoted to colonel in June 1934 (seniority from January 1932). He attended and was then recommended to become an instructor at the Indian Army Staff College (now the Pakistan Command and Staff College) in Quetta, British India.\nOn completion of his tour of duty in India, Montgomery returned to Britain in June 1937 where he took command of the 9th Infantry Brigade with the temporary rank of brigadier. His wife died that year.\nIn 1938, he organised an amphibious combined operations landing exercise that impressed the new C-in-C of Southern Command, General Sir Archibald Percival Wavell. He was promoted to major-general on 14 October 1938 and took command of the 8th Infantry Division in the British mandate of Palestine. In Palestine, Montgomery was involved in suppressing an Arab revolt which had broken out over opposition to Jewish emigration. He returned in July 1939 to Britain, suffering a serious illness on the way, to command the 3rd Infantry Division. Reporting the suppression of the revolt in April 1939, Montgomery wrote, \"I shall be sorry to leave Palestine in many ways, as I have enjoyed the war out here\".\nSecond World War.\nBritish Expeditionary Force.\nPhoney war.\nBritain declared war on Germany on 3 September 1939 and the 3rd Division, together with its new General Officer Commanding (GOC), was deployed to France as part of the British Expeditionary Force (BEF), commanded by General Lord Gort. Shortly after the division's arrival overseas, Montgomery faced serious trouble from his military superiors and the clergy for his frank attitude regarding the sexual health of his soldiers, but was defended from dismissal by his superior Alan Brooke, commander of II Corps, of which Montgomery's division formed a part. Montgomery had issued a circular on the prevention of venereal disease, worded in such \"obscene language\" that both the Church of England and Roman Catholic senior chaplains objected; Brooke told Monty that he did not want any further errors of this kind, though deciding not to get him to formally withdraw it as it would remove any \"vestige of respect\" left for him.\nAlthough Montgomery's new command was a Regular Army formation, comprising the 7th (Guards), and the 8th and 9th Infantry Brigades along with supporting units, he was not impressed with its readiness for battle. As a result, while most of the rest of the BEF set about preparing defences for an expected German attack sometime in the future, Montgomery began training his 3rd Division in offensive tactics, organising several exercises, each of which lasted for several days at a time. Mostly they revolved around the division advancing towards an objective, often a river line, only to come under attack and forced to withdraw to another position, usually behind another river. These exercises usually occurred at night with only very minimal lighting being allowed. By the spring of 1940 Montgomery's division had gained a reputation of being a very agile and flexible formation. By then the Allies had agreed to Plan D, where they would advance deep into Belgium and take up positions on the River Dyle by the time the German forces attacked. Brooke, Montgomery's corps commander, was pessimistic about the plan but Montgomery, in contrast, was not concerned, believing that he and his division would perform well regardless of the circumstances, particularly in a war of movement.\nBattle of France.\nMontgomery's training paid off when the Germans began their invasion of the Low Countries on 10 May 1940 and the 3rd Division advanced to its planned position, near the Belgian city of Louvain. Soon after arrival, the division was fired on by members of the Belgian 10th Infantry Division who mistook them for German paratroopers; Montgomery resolved the incident by approaching them and offering to place himself under Belgian command, although Montgomery himself took control when the Germans arrived. During this time he began to develop a particular habit, which he would keep throughout the war, of going to bed at 21:30 every night without fail and giving only a single order\u2014that he was not to be disturbed\u2014which was only very rarely disobeyed.\nThe 3rd Division saw comparatively little action but, owing to the strict training methods of Montgomery, the division always managed to be in the right place at the right time, especially so during the retreat into France. By 27 May, when the Belgian Army on the left flank of the BEF began to disintegrate, the 3rd Division achieved something very difficult, the movement at night from the right to the left of another division and only 2,000 yards behind it. This was performed with great professionalism and occurred without any incidents and thereby filled a very vulnerable gap in the BEF's defensive line. On 29/30 May, Montgomery temporarily took over from Brooke, who received orders to return to the United Kingdom, as GOC of II Corps for the final stages of the Dunkirk evacuation.\nThe 3rd Division, temporarily commanded by Kenneth Anderson in Montgomery's absence, returned to Britain intact with minimal casualties. Operation Dynamo\u2014codename for the Dunkirk evacuation\u2014saw 330,000 Allied military personnel, including most of the BEF, to Britain, although the BEF was forced to leave behind a significant amount of equipment.\nService in the United Kingdom 1940\u22121942.\nOn his return Montgomery antagonised the War Office with trenchant criticisms of the command of the BEF and was briefly relegated to divisional command of 3rd Division, which was the only fully equipped division in Britain. He was made a Companion of the Order of the Bath.\nMontgomery was ordered to make ready the 3rd Division to invade the neutral Portuguese Azores. Models of the islands were prepared and detailed plans worked out for the invasion. The invasion plans did not go ahead and plans switched to invading Cape Verde island also belonging to neutral Portugal. These invasion plans also did not go ahead. Montgomery was then ordered to prepare plans for the invasion of neutral Ireland and to seize Cork, Cobh and Cork harbour. These invasion plans, like those of the Portuguese islands, also did not go ahead and in July 1940, Montgomery was appointed acting lieutenant-general and after handing over command of his division to James Gammell, he was placed in command of V Corps, responsible for the defence of Hampshire and Dorset and started a long-running feud with the new Commander-in-chief (C-in-C) of Southern Command, Lieutenant-General Claude Auchinleck.\nIn April 1941, he became commander of XII Corps responsible for the defence of Kent. During this period he instituted a regime of continuous training and insisted on high levels of physical fitness for both officers and other ranks. He was ruthless in sacking officers he considered unfit for command in action. Promoted to temporary lieutenant-general in July, overseeing the defence of Kent, Sussex and Surrey. In December Montgomery was given command of South-Eastern Command. He renamed his command the South-Eastern Army to promote offensive spirit. During this time he further developed and rehearsed his ideas and trained his soldiers, culminating in Exercise Tiger in May 1942, a combined forces exercise involving 100,000 troops.\nNorth Africa and Italy.\nMontgomery's early command.\nIn 1942, a new field commander was required in the Middle East, where Auchinleck was fulfilling both the role of C-in-C of Middle East Command and commander Eighth Army. He had stabilised the Allied position at the First Battle of El Alamein, but after a visit in August 1942, the Prime Minister, Winston Churchill, replaced him as C-in-C with General Sir Harold Alexander and William Gott as commander of the Eighth Army in the Western Desert. However, after Gott was killed flying back to Cairo, Churchill was persuaded by Brooke, who by this time was Chief of the Imperial General Staff (CIGS), to appoint Montgomery, who had only just been nominated to replace Alexander, as commander of the British First Army for Operation Torch, the invasion of French North Africa.\nA story, probably apocryphal but popular at the time, is that the appointment caused Montgomery to remark that \"After having an easy war, things have now got much more difficult.\" A colleague is supposed to have told him to cheer up\u2014at which point Montgomery said \"I'm not talking about me, I'm talking about Rommel!\"\nMontgomery's assumption of command transformed the fighting spirit and abilities of the Eighth Army. Taking command on 13 August 1942, he immediately became a whirlwind of activity. He ordered the creation of the X Corps, which contained all armoured divisions, to fight alongside his XXX Corps, which was all infantry divisions. This arrangement differed from the German Panzer Corps: one of Rommel's Panzer Corps combined infantry, armour and artillery units under one corps commander. The only common commander for Montgomery's all-infantry and all-armour corps was the Eighth Army Commander himself. Writing post-war the English historian Correlli Barnett commented that Montgomery's solution \"was in every way opposite to Auchinleck's and in every way wrong, for it carried the existing dangerous separatism still further.\" Montgomery reinforced the long front line at El Alamein, something that would take two months to accomplish. He asked Alexander to send him two new British divisions (51st Highland and 44th Home Counties) that were then arriving in Egypt and were scheduled to be deployed in defence of the Nile Delta. He moved his field HQ to Burg al Arab, close to the Air Force command post in order to better coordinate combined operations.\nMontgomery was determined that the army, navy and air forces should fight their battles in a unified, focused manner according to a detailed plan. He ordered immediate reinforcement of the vital heights of Alam Halfa, just behind his own lines, expecting the German commander, Erwin Rommel, to attack with the heights as his objective, something that Rommel soon did. Montgomery ordered all contingency plans for retreat to be destroyed. \"I have cancelled the plan for withdrawal. If we are attacked, then there will be no retreat. If we cannot stay here alive, then we will stay here dead\", he told his officers at the first meeting he held with them in the desert, though, in fact, Auchinleck had no plans to withdraw from the strong defensive position he had chosen and established at El Alamein.\nMontgomery made a great effort to appear before troops as often as possible, frequently visiting various units and making himself known to the men, often arranging for cigarettes to be distributed. Although he still wore a standard British officer's cap on arrival in the desert, he briefly wore an Australian broad-brimmed hat before switching to wearing the black beret (with the badge of the Royal Tank Regiment and the British General Officer's cap badge) for which he became notable. The black beret was offered to him by Jim Fraser while the latter was driving him on an inspection tour. Both Brooke and Alexander were astonished by the transformation in atmosphere when they visited on 19 August, less than a week after Montgomery had taken command.\nAlan Brooke said that Churchill was always impatient for his generals to attack at once, and he wrote that Montgomery was always \"my Monty\" when Montgomery was out of favour with Churchill. Eden had some late night drinks with Churchill, and Eden said at a meeting of the Chiefs of Staff the next day (29 October 1942) that the Middle East offensive was \"petering out\". Alanbrooke had told Churchill \"fairly plainly\" what he thought of Eden's ability to judge the tactical situation from a distance, and was supported at the Chiefs of Staff meeting by Smuts.\nFirst battles with Rommel.\nRommel attempted to turn the left flank of the Eighth Army at the Battle of Alam el Halfa from 31 August 1942. The German/Italian armoured corps infantry attack was stopped in very heavy fighting. Rommel's forces had to withdraw urgently lest their retreat through the British minefields be cut off. Montgomery was criticised for not counter-attacking the retreating forces immediately, but he felt strongly that his methodical build-up of British forces was not yet ready. A hasty counter-attack risked ruining his strategy for an offensive on his own terms in late October, planning for which had begun soon after he took command. He was confirmed in the permanent rank of lieutenant-general in mid-October.\nThe conquest of Libya was essential for airfields to support Malta and to threaten the rear of Axis forces opposing Operation Torch. Montgomery prepared meticulously for the new offensive after convincing Churchill that the time was not being wasted. (Churchill sent a telegram to Alexander on 23 September 1942 which began, \"We are in your hands and of course a victorious battle makes amends for much delay.\") He was determined not to fight until he thought there had been sufficient preparation for a decisive victory, and put into action his beliefs with the gathering of resources, detailed planning, the training of troops\u2014especially in clearing minefields and fighting at night\u2014and in the use of 252 of the latest American-built Sherman tanks, 90 M7 Priest self-propelled howitzers, and making a personal visit to every unit involved in the offensive. By the time the offensive was ready in late October, Eighth Army had 231,000 men on its ration strength.\nEl Alamein.\nThe Second Battle of El Alamein began on 23 October 1942, and ended 12 days later with one of the first large-scale, decisive Allied land victories of the war. Montgomery correctly predicted both the length of the battle and the number of casualties (13,500).\nHistorian Correlli Barnett has pointed out that the rain also fell on the Germans, and that the weather is therefore an inadequate explanation for the failure to exploit the breakthrough, but nevertheless the Battle of El Alamein had been a great success. Over 30,000 prisoners of war were taken, including the German second-in-command, General von Thoma, as well as eight other general officers.\nTunisia.\nMontgomery was advanced to KCB and promoted to full general. He kept the initiative, applying superior strength when it suited him, forcing Rommel out of each successive defensive position. On 6 March 1943, Rommel's attack on the over-extended Eighth Army at Medenine (Operation Capri) with the largest concentration of German armour in North Africa was successfully repulsed. At the Mareth Line, 20 to 27 March, when Montgomery encountered fiercer frontal opposition than he had anticipated, he switched his major effort into an outflanking inland pincer, backed by low-flying RAF fighter-bomber support. For his role in North Africa he was awarded the Legion of Merit by the United States government in the rank of Chief Commander.\nSicily.\nThe next major Allied attack was the Allied invasion of Sicily (Operation Husky). Montgomery considered the initial plans for the Allied invasion, which had been agreed in principle by General Dwight D. Eisenhower, the Supreme Allied Commander Allied Forces Headquarters, and General Alexander, the 15th Army Group commander, to be unworkable because of the dispersion of effort. He managed to have the plans recast to concentrate the Allied forces, having Lieutenant General George Patton's US Seventh Army land in the Gulf of Gela (on the Eighth Army's left flank, which landed around Syracuse in the south-east of Sicily) rather than near Palermo in the west and north of Sicily. Inter-Allied tensions grew as the American commanders, Patton and Omar Bradley (then commanding US II Corps under Patton), took umbrage at what they saw as Montgomery's attitudes and boastfulness. However, while they were considered three of the greatest soldiers of their time, due to their competitiveness they were renowned for \"squabbling like three schoolgirls\" thanks to their \"bitchiness\", \"whining to their superiors\" and \"showing off\".\nItaly.\nMontgomery's Eighth Army was then fully involved in the Allied invasion of Italy in early September 1943, becoming the first of the Allied forces to land in Western Europe. Led by Lieutenant General Sir Miles Dempsey's XIII Corps, the Eighth Army landed on the toe of Italy in Operation Baytown on 3 September, four years to the day after Britain declared war on Germany. They encountered little enemy resistance. The Germans had made the decision to fall back and did what they could to stall the Eighth Army's advance, including blowing up bridges, laying mines, and setting up booby-traps. All of these slowed the Army's advance north on the awful Italian roads, although it was Montgomery who was later much criticised for the lack of progress. On 9 September the British 1st Airborne Division landed at the key port of Taranto in the heel of Italy as part of Operation Slapstick, capturing the port unopposed. On the same day the U.S. Fifth Army under Lieutenant General Mark W. Clark (which actually contained a large number of British troops) landed at Salerno, near Naples, as part of Operation Avalanche but soon found itself fighting for its very existence with the Germans launching several determined counterattacks to try and push the Allies back into the sea, with Montgomery's men being too far away to provide any real assistance. The situation was tense over the next few days but the two armies (both of which formed the 15th Army Group under General Alexander) finally began to meet on 16 September, by which time the crisis at Salerno was virtually over.\nClark's Fifth Army then began to advance to the west of the Apennine Mountains while Montgomery, with Lieutenant General Charles Allfrey's V Corps having arrived to reinforce Dempsey's XIII Corps, advanced to the east. The Foggia airfields soon fell to Allfrey's V Corps, but the Germans fought hard in the defence of Termoli and Biferno. Movement soon came to an almost complete halt in the early part of November when the Eighth Army came up against a new defensive line established by the Germans on the River Sangro, which was to be the scene of much bitter and heavy fighting for the next month. While some ground was gained, it was often at the expense of heavy casualties and the Germans always managed to retreat to new defensive positions.\nMontgomery abhorred what he considered to be a lack of coordination, a dispersion of effort, a strategic muddle and a lack of opportunism in the Allied campaign in Italy, describing the whole affair as a \"dog's breakfast\".\nNormandy.\nAs a result of his dissatisfaction with Italy, he was delighted to receive the news that he was to return to Britain in January 1944. He was assigned to command the 21st Army Group consisting of all Allied ground forces participating in Operation Overlord, codename for the Allied invasion of Normandy. Overall direction was assigned to the Supreme Allied Commander of the Allied Expeditionary Forces, American General Dwight D. Eisenhower. Both Churchill and Eisenhower had found Montgomery difficult to work with in the past and wanted the position to go to the more affable General Sir Harold Alexander. However Montgomery's patron, General Sir Alan Brooke, firmly argued that Montgomery was a much superior general to Alexander and ensured his appointment. Without Brooke's support, Montgomery would have remained in Italy. At St Paul's School on 7 April and 15 May Montgomery presented his strategy for the invasion. He envisaged a ninety-day battle, with all forces reaching the Seine. The campaign would pivot on an Allied-held Caen in the east of the Normandy bridgehead, with relatively static British and Canadian armies forming a shoulder to attract and defeat German counter-attacks, relieving the US armies who would move and seize the Cotentin Peninsula and Brittany, wheeling south and then east on the right forming a pincer.\nDuring the ten weeks of the Battle of Normandy, unfavourable autumnal weather conditions disrupted the Normandy landing areas. Montgomery's initial plan was for the Anglo-Canadian troops under his command to break out immediately from their beachheads on the Calvados coast towards Caen with the aim of taking the city on either D Day or two days later. Montgomery attempted to take Caen with the 3rd Infantry Division, 50th (Northumbrian) Infantry Division and the 3rd Canadian Division but was stopped from 6\u20138 June by 21st Panzer Division and 12th SS Panzer Division \"Hitlerjugend\", who hit the advancing Anglo-Canadian troops very hard. Rommel followed up this success by ordering the 2nd Panzer Division to Caen while Field Marshal Gerd von Rundstedt received permission from Hitler to have the elite 1st Waffen SS Division \"Leibstandarte Adolf Hitler\" and 2nd Waffen SS Division \"Das Reich\" sent to Caen as well. Montgomery thus had to face what Stephen Badsey called the \"most formidable\" of all the German divisions in France. The 12th Waffen SS Division \"Hitlerjugend\", as its name implies, was drawn entirely from the more fanatical elements of the Hitler Youth and commanded by the ruthless SS-\"Brigadef\u00fchrer\" Kurt Meyer, aka \"Panzer Meyer\".\nThe failure to take Caen immediately has been the source of an immense historiographical dispute with bitter nationalist overtones. Broadly, there has been a \"British school\" which accepts Montgomery's post-war claim that he never intended to take Caen at once, and instead the Anglo-Canadian operations around Caen were a \"holding operation\" intended to attract the bulk of the German forces towards the Caen sector to allow the Americans to stage the \"break out operation\" on the left flank of the German positions, which was all part of Montgomery's \"Master Plan\" that he had conceived long before the Normandy campaign. By contrast, the \"American school\" argued that Montgomery's initial \"master plan\" was for the 21st Army Group to take Caen at once and move his tank divisions into the plains south of Caen, to then stage a breakout that would lead the 21st Army Group into the plains of northern France and hence into Antwerp and finally the Ruhr. Letters written by Eisenhower at the time of the battle make it clear that Eisenhower was expecting from Montgomery \"the early capture of the important focal point of Caen\". Later, when this plan had clearly failed, Eisenhower wrote that Montgomery had \"evolved\" the plan to have the US forces achieve the break-out instead.\nAs the campaign progressed, Montgomery altered his initial plan for the invasion and continued the strategy of attracting and holding German counter-attacks in the area north of Caen rather than to the south, to allow the U.S. First Army in the west to take Cherbourg. A memo summarising Montgomery's operations written by Eisenhower's chief of staff, General Walter Bedell Smith who met with Montgomery in late June 1944 says nothing about Montgomery conducting a \"holding operation\" in the Caen sector, and instead speaks of him seeking a \"breakout\" into the plains south of the Seine. On 12 June, Montgomery ordered the 7th Armoured Division into an attack against the Panzer Lehr Division that made good progress at first but ended when the Panzer Lehr was joined by the 2nd Panzer Division. At Villers Bocage on 14 June, the British lost twenty Cromwell tanks to five Tiger tanks led by SS \"Obersturmf\u00fchrer\" Michael Wittmann, in about five minutes. Despite the setback at Villers Bocage, Montgomery was still optimistic as the Allies were landing more troops and supplies than they were losing in battle, and though the German lines were holding, the \"Wehrmacht\" and \"Waffen SS\" were suffering considerable attrition. Air Marshal Sir Arthur Tedder complained that it was impossible to move fighter squadrons to France until Montgomery had captured some airfields, something he asserted that Montgomery appeared incapable of doing. The first V-1 flying bomb attacks on London, which started on 13 June, further increased the pressure on Montgomery from Whitehall to speed up his advance.\nOn 18 June, Montgomery ordered Bradley to take Cherbourg while the British were to take Caen by 23 June. In Operation Epsom, the British VII Corps commanded by Sir Richard O'Connor attempted to outflank Caen from the west by breaking through the dividing line between the Panzer Lehr and the 12th SS to take the strategic Hill 112. Epsom began well with O'Connor's assault force (the British 15th Scottish Division) breaking through and with the 11th Armoured Division stopping the counter-attacks of the 12th SS Division. General Friedrich Dollmann of Seventh Army had to commit the newly arrived II SS Corps to stop the British offensive. Dollmann, fearing that Epsom would be a success, committed suicide and was replaced by SS \"Oberstegruppenf\u00fchrer\" Paul Hausser. O'Connor, at the cost of about 4,000 men, had won a salient deep and wide but placed the Germans into an unviable long-term position. There was a strong sense of crisis in the Allied command, as the Allies had advanced only about inland, at a time when their plans called for them to have already taken Rennes, Alen\u00e7on and St. Malo. After Epsom, Montgomery had to tell General Harry Crerar that the activation of the First Canadian Army would have to wait as there was only room at present, in the Caen sector, for the newly arrived XII Corps under Lieutenant-General Neil Ritchie, which caused some tension with Crerar, who was anxious to get into the field. Epsom had forced further German forces into Caen but all through June and the first half of July Rommel, Rundstedt, and Hitler were engaged in planning for a great offensive to drive the British into the sea; it was never launched and would have required the commitment of a large number of German forces to the Caen sector.\nIt was only after several failed attempts to break out in the Caen sector that Montgomery devised what he later called his \"master plan\" of having the 21st Army Group hold the bulk of the German forces, thus allowing the Americans to break out. The Canadian historians Terry Copp and Robert Vogel wrote about the dispute between the \"American school\" and \"British school\" after having suffered several setbacks in June 1944:\nHampered by stormy weather and the bocage terrain, Montgomery had to ensure that Rommel focused on the British in the east rather than the Americans in the west, who had to take the Cotentin Peninsula and Brittany before the Germans could be trapped by a general swing east. Montgomery told General Sir Miles Dempsey, the commander of Second British Army: \"Go on hitting, drawing the German strength, especially some of the armour, onto yourself\u2014so as to ease the way for Brad [Bradley].\" The Germans had deployed twelve divisions, of which six were Panzer divisions, against the British while deploying eight divisions, of which three were Panzer divisions, against the Americans. By the middle of July Caen had not been taken, as Rommel continued to prioritise prevention of the break-out by British forces rather than the western territories being taken by the Americans. This was broadly as Montgomery had planned, albeit not with the same speed as he outlined at St Paul's, although as the American historian Carlo D'Este pointed out the actual situation in Normandy was \"vastly different\" from what was envisioned at the St. Paul's conference, as only one of four goals outlined in May had been achieved by 10 July.\nOn 7 July, Montgomery began Operation Charnwood with a carpet bombing offensive that turned much of the French countryside and the city of Caen into a wasteland. The British and Canadians succeeded in advancing into northern Caen before the Germans, who used the ruins to their advantage and stopped the offensive. On 10 July, Montgomery ordered Bradley to take Avranches, after which U.S. Third Army would be activated to drive towards Le Mans and Alen\u00e7on. On 14 July 1944, Montgomery wrote to his patron Brooke, saying he had chosen on a \"real show down on the eastern flanks, and to loose a Corps of three armoured divisions in the open country about the Caen-Falaise road\u00a0... The possibilities are immense; with seven hundred tanks loosed to the South-east of Caen, and the armoured cars operating far ahead, anything can happen.\" The French Resistance had launched Plan Violet in June 1944 to systematically destroy the telephone system of France, which forced the Germans to use their radios more and more to communicate, and as the code-breakers of Bletchley Park had broken many of the German codes, Montgomery had, thanks to \"Ultra\" intelligence, a good idea of the German situation. Montgomery thus knew German Army Group B had lost 96,400 men while receiving 5,200 replacements and the Panzer Lehr Division now based at St. L\u00f4 was down to only 40 tanks. Montgomery later wrote that he knew he had the Normandy campaign won at this point as the Germans had almost no reserves while he had three armoured divisions in reserve.\nAn American break-out was achieved with Operation Cobra and the encirclement of German forces in the Falaise pocket at the cost of British losses with the diversionary Operation Goodwood. On the early morning of 18 July 1944, Operation Goodwood began with British heavy bombers beginning carpet bombing attacks that further devastated what was left of Caen and the surrounding countryside. A British tank crewman from the Guards Armoured Division later recalled: \"At 0500 hours a distant thunder in the air brought all the sleepy-eyed tank crews out of their blankets. 1,000 Lancasters were flying from the sea in groups of three or four at . Ahead of them the pathfinders were scattering their flares and before long the first bombs were dropping.\" A German tankman from the 21st Panzer Division at the receiving end of this bombardment remembered: \"We saw little dots detach themselves from the planes, so many of them that the crazy thought occurred to us: are those leaflets?\u00a0... Among the thunder of the explosions, we could hear the wounded scream and the insane howling of men who had [been] driven mad.\" The British bombing had badly smashed the German front-line units. Initially, the three British armoured divisions assigned to lead the offensive, the 7th, 11th and the Guards, made rapid progress and were soon approaching the Borguebus ridge, which dominated the landscape south of Caen, by noon.\nIf the British could take the Borguebus Ridge, the way to the plains of northern France would be wide open, and potentially Paris could be taken, which explains the ferocity with which the Germans defended the ridge. One German officer, Lieutenant Baron von Rosen, recalled that to motivate a Luftwaffe officer commanding a battery of four 88\u00a0mm guns to fight against the British tanks, he had to hold his handgun to the officer's head \"and asked him whether he would like to be killed immediately or get a high decoration. He decided for the latter.\" The well dug-in 88\u00a0mm guns around the Borguebus Ridge began taking a toll on the British Sherman tanks, and the countryside was soon dotted with dozens of burning Shermans. One British officer reported with worry: \"I see palls of smoke and tanks brewing up with flames belching forth from their turrets. I see men climbing out, on fire like torches, rolling on the ground to try and douse the flames.\" Despite Montgomery's orders to try to press on, fierce German counter-attacks stopped the British offensive.\nThe objectives of Operation Goodwood were all achieved except the complete capture of the Bourgebus Ridge, which was only partially taken. The operation was a strategic Allied success in drawing in the last German reserves in Normandy towards the Caen sector away from the American sector, greatly assisting the American breakout in Operation Cobra. By the end of Goodwood on 25 July 1944, the Canadians had finally taken Caen while the British tanks had reached the plains south of Caen, giving Montgomery the \"hinge\" he had been seeking, while forcing the Germans to commit the last of their reserves to stop the Anglo-Canadian offensive. \"Ultra\" decrypts indicated that the Germans now facing Bradley were seriously understrength, with Operation Cobra about to commence. During Operation Goodwood, the British had 400 tanks knocked out, with many recovered returning to service. The casualties were 5,500 with of ground gained. Bradley recognised Montgomery's plan to pin down German armour and allow U.S. forces to break out:\nThe long-running dispute over what Montgomery's \"master plan\" in Normandy led historians to differ greatly about the purpose of Goodwood. The British journalist Mark Urban wrote that the purpose of Goodwood was to draw German troops to their left flank to allow the American forces to break out on the right flank, arguing that Montgomery had to lie to his soldiers about the purpose of Goodwood, as the average British soldier would not have understood why they were being asked to create a diversion to allow the Americans to have the glory of staging the breakout with Operation Cobra. By contrast, the American historian Stephen Power argued that Goodwood was intended to be the \"breakout\" offensive and not a \"holding operation\", writing: \"It is unrealistic to assert that an operation which called for the use of 4,500 Allied aircraft, 700 artillery pieces and over 8,000 armored vehicles and trucks and that cost the British over 5,500 casualties was conceived and executed for so limited an objective.\" Power noted that Goodwood and Cobra were supposed to take effect on the same day, 18 July 1944, but Cobra was cancelled owing to heavy rain in the American sector, and argued that both operations were meant to be breakout operations to trap the German armies in Normandy. American military writer Drew Middleton wrote that there is no doubt that Montgomery wanted Goodwood to provide a \"shield\" for Bradley, but at the same time Montgomery was clearly hoping for more than merely diverting German attention away from the American sector. British historian John Keegan pointed out that Montgomery made differing statements before Goodwood about the purpose of the operation. Keegan wrote that Montgomery engaged in what he called a \"hedging of his bets\" when drafting his plans for Goodwood, with a plan for a \"break out if the front collapsed, if not, sound documentary evidence that all he had intended in the first place was a battle of attrition\". Again Bradley confirmed Montgomery's plan and that the capture of Caen was only incidental to his mission, not critical. The American magazine \"LIFE\" quoted Bradley in 1951:\n With Goodwood drawing the Wehrmacht towards the British sector, U.S. First Army enjoyed a two-to-one numerical superiority. Bradley accepted Montgomery's advice to begin the offensive by concentrating at one point instead of a \"broad front\" as Eisenhower would have preferred.\nOperation Goodwood almost cost Montgomery his job, as Eisenhower seriously considered sacking him and only chose not to do so because to sack the popular \"Monty\" would have caused such a political backlash in Britain against the Americans at a critical moment in the war that the resulting strains in the Atlantic alliance were not considered worth it. Montgomery expressed his satisfaction at the results of Goodwood when calling the operation off. Eisenhower was under the impression that Goodwood was to be a break-out operation. Either there was a miscommunication between the two men or Eisenhower did not understand the strategy. Bradley fully understood Montgomery's intentions. Both men would not give away to the press the true intentions of their strategy.\nMany American officers had found Montgomery a difficult man to work with, and after Goodwood, pressured Eisenhower to fire Montgomery. Although the Eisenhower\u2013Montgomery dispute is sometimes depicted in nationalist terms as being an Anglo-American struggle, it was the British Air Marshal Arthur Tedder who was pressing Eisenhower most strongly after Goodwood to fire Montgomery. An American officer wrote in his diary that Tedder had come to see Eisenhower to \"pursue his current favourite subject, the sacking of Monty\". With Tedder leading the \"sack Monty\" campaign, it encouraged Montgomery's American enemies to press Eisenhower to fire Montgomery. Brooke was sufficiently worried about the \"sack Monty\" campaign to visit Montgomery at his Tactical Headquarters (TAC) in France and as he wrote in his diary; \"warned [Montgomery] of a tendency in the PM [Churchill] to listen to suggestions that Monty played for safety and was not prepared to take risks\". Brooke advised Montgomery to invite Churchill to Normandy, arguing that if the \"sack Monty\" campaign had won the Prime Minister over, then his career would be over, as having Churchill's backing would give Eisenhower the political \"cover\" to fire Montgomery. On 20 July, Montgomery met Eisenhower and on 21 July, Churchill, at the TAC in France. One of Montgomery's staff officers wrote afterwards that it was \"common knowledge at Tac that Churchill had come to sack Monty\". No notes were taken at the Eisenhower\u2013Montgomery and Churchill\u2013Montgomery meetings, but Montgomery was able to persuade both men not to sack him.\nWith the success of Cobra, which was soon followed by unleashing Patton's Third Army, Eisenhower wrote to Montgomery: \"Am delighted that your basic plan has begun brilliantly to unfold with Bradley's initial success.\" The success of Cobra was aided by Operation Spring, when the II Canadian Corps under General Guy Simonds (the only Canadian general whose skill Montgomery respected) began an offensive south of Caen that made little headway, but which the Germans regarded as the main offensive. Once Third Army arrived, Bradley was promoted to take command of the newly created 12th Army Group, consisting of U.S. First and Third Armies. Following the American breakout, there followed the Battle of Falaise Gap. British, Canadian, and Polish soldiers of 21st Army Group commanded by Montgomery advanced south, while the American and French soldiers of Bradley's 12th Army Group advanced north to encircle the German Army Group B at Falaise, as Montgomery waged what Urban called \"a huge battle of annihilation\" in August 1944. Montgomery began his offensive into the \"Suisse Normande\" region with Operation Bluecoat, with Sir Richard O'Connor's VIII Corps and Gerard Bucknall's XXX Corps heading south. A dissatisfied Montgomery sacked Bucknall for being insufficiently aggressive and replaced him with General Brian Horrocks. At the same time, Montgomery ordered Patton\u2014whose Third Army was supposed to advance into Brittany\u2014to instead capture Nantes, which was soon taken.\nHitler waited too long to order his soldiers to retreat from Normandy, leading Montgomery to write: \"He [Hitler] refused to face the only sound military course. As a result the Allies caused the enemy staggering losses in men and materials.\" Knowing via \"Ultra\" that Hitler was not planning to retreat from Normandy, Montgomery, on 6 August 1944, ordered an envelopment operation against Army Group B\u2014with the First Canadian Army under Harry Crerar to advance towards Falaise, British Second Army under Miles Dempsey to advance towards Argentan, and Patton's Third Army to advance to Alen\u00e7on. On 11 August, Montgomery changed his plan, with the Canadians to take Falaise and to meet the Americans at Argentan. The First Canadian Army launched two operations, Operation Totalize on 7 August, which advanced only in four days in the face of fierce German resistance, and Operation Tractable on 14 August, which finally took Falaise on 17 August. In view of the slow Canadian advance, Patton requested permission to take Falaise, but was refused by Bradley on 13 August. This prompted much controversy, many historians arguing that Bradley lacked aggression and that Montgomery should have overruled Bradley.\nThe so-called Falaise Gap was closed on 22 August 1944, but several American generals, most notably Patton, accused Montgomery of being insufficiently aggressive in closing it. About 60,000 German soldiers were trapped in Normandy, but before 22 August, about 20,000 Germans had escaped through the Falaise Gap. About 10,000 Germans had been killed in the Battle of the Falaise Gap, which led a stunned Eisenhower, who viewed the battlefield on 24 August, to comment with horror that it was impossible to walk without stepping on corpses. The successful conclusion of the Normandy campaign saw the beginning of the debate between the \"American school\" and \"British school\" as both American and British generals started to advance claims about who was most responsible for this victory. Brooke wrote in defence of his prot\u00e9g\u00e9 Montgomery: \"Ike knows nothing about strategy and is 'quite' unsuited to the post of Supreme Commander. It is no wonder that Monty's real high ability is not always realised. Especially so when 'national' spectacles pervert the perspective of the strategic landscape.\" About Montgomery's conduct of the Normandy campaign, Badsey wrote:\nReplaced as Ground Forces Commander.\nEisenhower took over Ground Forces Command on 1 September, while continuing as Supreme Commander, with Montgomery continuing to command the 21st Army Group, now consisting mainly of British and Canadian units. Montgomery bitterly resented this change, although it had been agreed before the D-Day invasion. The British journalist Mark Urban writes that Montgomery seemed unable to grasp that as the majority of the 2.2\u00a0million Allied soldiers fighting against Germany on the Western Front were now American (the ratio was 3:1) that it was politically unacceptable to American public opinion to have Montgomery remain as Land Forces Commander as: \"Politics would not allow him to carry on giving orders to great armies of Americans simply because, in his view, he was better than their generals.\"\nWinston Churchill had Montgomery promoted to field marshal by way of compensation.\nAdvance to the Rhine.\nBy September, ports like Cherbourg were too far away from the front line, causing the Allies great logistical problems. Antwerp was the third largest port in Europe. It was a deep water inland port connected to the North Sea via the river Scheldt. The Scheldt was wide enough and dredged deep enough to allow the passage of ocean-going ships.\nOn 3 September 1944 Hitler ordered Fifteenth Army, which had been stationed in the Pas de Calais region and was withdrawing north into the Low Countries, to hold the mouth of the river Scheldt to deprive the Allies of the use of Antwerp. Von Rundstedt, the German commander of the Western Front, ordered General Gustav-Adolf von Zangen, the commander of 15th Army, that: \"The attempt of the enemy to occupy the West Scheldt in order to obtain the free use of the harbor of Antwerp must be \"resisted to the utmost\"\" (emphasis in the original). Rundstedt argued with Hitler that as long as the Allies could not use the port of Antwerp, the Allies would lack the logistical capacity for an invasion of Germany.\nThe \"Witte Brigade\" (White Brigade) of the Belgian resistance had captured the Port of Antwerp before the Germans could destroy key port facilities, and on 4 September, Antwerp was captured by Horrocks with its harbour mostly intact. The British declined to immediately advance over the Albert Canal, and an opportunity to destroy the German Fifteenth Army was lost. The Germans had mined the river Scheldt, the mouth of the Scheldt was still in German hands making it impossible for the Royal Navy to clear the mines in the river, and therefore the port of Antwerp was still useless to the Allies.\nOn 5 September, SHAEF's naval commander, Admiral Sir Bertram Ramsay, had urged Montgomery to make clearing the mouth of the Scheldt his number-one priority. Alone among the senior commanders, only Ramsay saw opening Antwerp as crucial. Thanks to \"Ultra,\" Montgomery was aware of Hitler's order by 5 September.\nOn 9 September, Montgomery wrote to Brooke that \"one good Pas de Calais port\" would be sufficient to meet all the logistical needs of the 21st Army Group, but only the supply needs of the same formation. At the same time, Montgomery noted that \"one good Pas de Calais port\" would be insufficient for the American armies in France, which would thus force Eisenhower, if for no other reasons than logistics, to favour Montgomery's plans for an invasion of northern Germany by the 21st Army Group, whereas if Antwerp were opened up, then all of the Allied armies could be supplied.\nThe importance of ports closer to Germany was highlighted with the liberation of the city of Le Havre, which was assigned to John Crocker's I Corps. To take Le Havre, two infantry divisions, two tank brigades, most of the artillery of the Second British Army, the specialised armoured \"gadgets\" of Percy Hobart's 79th Armoured Division, the battleship and the monitor were all committed. On 10 September 1944, Bomber Command dropped 4,719 tons of bombs on Le Havre, which was the prelude to Operation Astonia, the assault on Le Havre by Crocker's men, which was taken two days later. The Canadian historian Terry Copp wrote that the commitment of this much firepower and men to take only one French city might \"seem excessive\", but by this point, the Allies desperately needed ports closer to the front line to sustain their advance.\nIn September 1944, Montgomery ordered Crerar and his First Canadian Army to take the French ports on the English Channel, namely Calais, Boulogne and Dunkirk, and to clear the Scheldt, a task that Crerar stated was impossible as he lacked enough troops to perform both operations at once. Montgomery refused Crerar's request to have British XII Corps under Neil Ritchie assigned to help clear the Scheldt as Montgomery stated he needed XII Corps for Operation Market Garden. On 6 September 1944, Montgomery told Crerar that \"I want Boulogne badly\" and that city should be taken no matter what the cost. On 22 September 1944, Simonds's II Canadian Corps took Boulogne, followed up by taking Calais on 1 October 1944. Montgomery was highly impatient with Simonds, complaining that it had taken Crocker's I Corps only two days to take Le Havre while it took Simonds two weeks to take Boulogne and Calais, but Simonds noted that at Le Havre, three divisions and two brigades had been employed, whereas at both Boulogne and Calais, only two brigades were sent in to take both cities. After an attempt to storm the Leopold Canal by the 4th Canadian Division had been badly smashed by the German defenders, Simonds ordered a stop to further attempts to clear the river Scheldt until his mission of capturing the French ports on the English Channel had been accomplished; this allowed the German Fifteenth Army ample time to dig into its new home on the Scheldt. The only port that was not captured by the Canadians was Dunkirk, as Montgomery ordered the 2nd Canadian Division on 15 September to hold his flank at Antwerp as a prelude for an advance up the Scheldt.\nMontgomery pulled away from the First Canadian Army (temporarily commanded now by Simonds as Crerar was ill), the British 51st Highland Division, 1st Polish Division, British 49th (West Riding) Division and 2nd Canadian Armoured Brigade, and sent all of these formations to help the Second British Army to expand the Market Garden salient with Operations Constellation, Aintree, and towards the end of October Pheasant. However, Simonds seems to have regarded the Scheldt campaign as a test of his ability, and he felt he could clear the Scheldt with only three Canadian divisions, despite having to take on the entire Fifteenth Army, which held strongly fortified positions in a landscape that favoured the defence. Simonds never complained about the lack of air support (made worse by the cloudy October weather), shortages of ammunition or having insufficient troops, regarding these problems as challenges for him to overcome, rather than a cause for complaint. As it was, Simonds made only slow progress in October 1944 during the fighting in the Battle of the Scheldt, although he was praised by Copp for imaginative and aggressive leadership who managed to achieve much, despite all of the odds against him. Montgomery had little respect for the Canadian generals, whom he dismissed as mediocre, with the exception of Simonds, whom he consistently praised as Canada's only \"first-rate\" general in the entire war.\nAdmiral Ramsay, who proved to be a far more articulate and forceful champion of the Canadians than their own generals, starting on 9 October demanded of Eisenhower in a meeting that he either order Montgomery to make supporting the First Canadian Army in the Scheldt fighting his number one priority or sack him. Ramsay in very strong language argued to Eisenhower that the Allies could only invade Germany if Antwerp was opened, and that as long as the three Canadian divisions fighting in the Scheldt had shortages of ammunition and artillery shells because Montgomery made the Arnhem salient his first priority, then Antwerp would not be opened anytime soon. Even Brooke wrote in his diary: \"I feel that Monty's strategy for once is at fault. Instead of carrying out the advance to Arnhem he ought to have made certain of Antwerp\". On 9 October 1944, at Ramsay's urging, Eisenhower sent Montgomery a cable that emphasised the \"supreme importance of Antwerp\", that \"the Canadian Army will not, repeat not, be able to attack until November unless immediately supplied with adequate ammunition\", and warned that the Allied advance into Germany would totally stop by mid-November unless Antwerp was opened by October. Montgomery replied by accusing Ramsay of making \"wild statements\" unsupported by the facts, denying the Canadians were having to ration ammunition, and claimed that he would soon take the Ruhr thereby making the Scheldt campaign a sideshow. Montgomery further issued a memo entitled \"Notes on Command in Western Europe\" demanding that he once again be made Land Forces Commander. This led to an exasperated Eisenhower telling Montgomery that the question was not the command arrangement but rather his (Montgomery's) ability and willingness to obey orders. Eisenhower further told Montgomery to either obey orders to immediately clear the mouth of the Scheldt or be sacked.\nA chastised Montgomery told Eisenhower on 15 October 1944 that he was now making clearing the Scheldt his \"top priority\", and the ammunition shortages in the First Canadian Army, a problem which he denied even existed five days earlier, were now over as supplying the Canadians was henceforth his first concern. Simonds, now reinforced with British troops and Royal Marines, cleared the Scheldt by taking Walcheren island, the last of the German \"fortresses\" on the Scheldt, on 8 November 1944. With the Scheldt in Allied hands, Royal Navy minesweepers removed the German mines in the river, and Antwerp was finally opened to shipping on 28 November 1944. Reflecting Antwerp's importance, the Germans spent the winter of 1944\u201345 firing V-1 flying bombs and V-2 rockets at it in an attempt to shut down the port, and the German offensive in December 1944 in the Ardennes had as its ultimate objective the capture of Antwerp. Urban wrote that Montgomery's most \"serious failure\" in the entire war was not the well publicised Battle of Arnhem, but rather his lack of interest in opening up Antwerp, as without it the entire Allied advance from the North Sea to the Swiss Alps stalled in the autumn of 1944 for logistical reasons.\nOperation Market Garden.\nMontgomery was able to persuade Eisenhower to allow him to test his strategy of a single thrust to the Ruhr with Operation Market Garden in September 1944. The offensive was strategically bold. Following the Allied breakout from Normandy, Eisenhower, favored pursuing the German armies northwards and eastwards to the Rhine on a broad front. Eisenhower relied on speed, which in turn depended on logistics, which were \"stretched to the limit\". Supreme Headquarters Allied Expeditionary Force (SHAEF) did provide Montgomery with additional resources, principally additional locomotives and rolling stock, and priority for air supply. Eisenhower's decision to launch Market Garden was influenced by his desire to keep the retreating Germans under pressure, and by the pressure from the United States to use the First Allied Airborne Army as soon as possible.\nMontgomery's plan for Operation Market Garden (17\u201325 September 1944) was to outflank the Siegfried Line and cross the Rhine, setting the stage for later offensives into the Ruhr region. The 21st Army Group would attack north from Belgium, through the Netherlands, across the Rhine and consolidate north of Arnhem on the far side of the Rhine. The risky plan required three Airborne Divisions to capture numerous intact bridges along a single-lane road, on which an entire Corps had to attack and use as its main supply route. The offensive failed to achieve its objectives.\nBoth Churchill and Montgomery claimed that the operation was nearly or 90% successful, although in Montgomery's equivocal acceptance of responsibility he blames lack of support, and also refers to the Battle of the Scheldt which was undertaken by Canadian troops not involved in Market Garden. Montgomery later said:\nIn the aftermath of Market Garden, Montgomery made holding the Arnhem salient his first priority, arguing that the Second British Army might still be able to break through and reach the wide open plains of northern Germany, and that he might be able to take the Ruhr by the end of October. The Germans under Field Marshal Walther Model in early October attempted to retake the Nijmegen salient but were beaten back. In the meantime, the First Canadian Army finally achieved the task of clearing the mouth of the river Scheldt, despite the fact that in the words of Copp and Vogel \"that Montgomery's Directive required the Canadians to continue to fight alone for almost two weeks in a battle which everyone agreed could only be won with the aid of additional divisions\".\nBattle of the Bulge.\nOn 16 December 1944, at the start of the Battle of the Bulge, Montgomery's 21st Army Group was on the northern flank of the allied lines. Bradley's US 12th Army Group was to Montgomery's south, with William Simpson's U.S. Ninth Army adjacent to 21st Army Group, Courtney Hodges' U.S. First Army, holding the Ardennes and Patton's U.S. Third Army further south.\nSHAEF believed the Wehrmacht was no longer capable of launching a major offensive, and that no offensive could be launched through such rugged terrain as the Ardennes Forest. Because of this, the area was held by refitting and newly arrived American formations. The Wehrmacht planned to exploit this by making a surprise attack through the Ardennes Forest whilst bad weather grounded Allied air power, splitting the Allied Armies in two. They would then turn north to recapture the port of Antwerp. If the attack were to succeed in capturing Antwerp, the whole of 21st Army Group, along with U.S. Ninth Army and most of U.S. First Army would be trapped without supplies behind German lines.\nThe attack initially advanced rapidly, splitting U.S. 12th Army Group in two, with all of U.S. Ninth Army and the bulk of U.S. First Army on the northern shoulder of the German 'bulge'. The 12th Army Group commander, Bradley, was located in Luxembourg, making command of the U.S. forces north of the bulge problematic. As Montgomery was the nearest army group commander on the ground, on 20 December, Eisenhower temporarily transferred command of U.S. Ninth Army and U.S. First Army to Montgomery's 21st Army Group. Bradley was \"concerned because it might discredit the American command\" but that it might mean Montgomery would commit more of his reserves to the battle. In practice the change led to \"great resentment on the part of many Americans, particularly at Headquarters, 12th Army Group, and Third Army\".\nWith the British and American forces under Montgomery's command holding the northern flank of the German assault, General Patton's Third Army, which was to the south, turned north and fought its way through the severe weather and German opposition to relieve the besieged American forces in Bastogne. Four days after Montgomery took command of the northern flank, the bad weather cleared and the USAAF and RAF resumed operations, inflicting heavy casualties on German troops and vehicles. Six days after Montgomery took command of the northern flank, Patton's Third Army relieved the besieged American forces in Bastogne. Unable to advance further, and running out of fuel, the Wehrmacht abandoned the offensive.\nMorelock states that Montgomery was preoccupied with leading a \"single thrust offensive\" to Berlin as the overall commander of Allied ground forces, and that he accordingly treated the Ardennes counteroffensive \"as a sideshow, to be finished with the least possible effort and expenditure of resources.\"\nMontgomery subsequently wrote of his actions:\nAfter the war Hasso von Manteuffel, who commanded the 5th Panzer Army in the Ardennes, was imprisoned awaiting trial for war crimes. During this period he was interviewed by B. H. Liddell Hart, a British author who has since been accused of putting words in the mouths of German generals, and attempting to \"rewrite the historical record\". After conducting several interviews via an interpreter, Liddell Hart in a subsequent book attributed to Manteuffel the following statement about Montgomery's contribution to the battle in the Ardennes:\nHowever, American historian Stephen Ambrose, writing in 1997, maintained that \"Putting Monty in command of the northern flank had no effect on the battle\". Ambrose wrote that: \"Far from directing the victory, Montgomery had gotten in everyone's way, and had botched the counter-attack.\" General Omar Bradley blamed Montgomery's \"stagnating conservatism\" for his failure to counter-attack when ordered to do so by Eisenhower.\nCommand of U.S. First Army reverted to 12th Army Group on 17 January 1945, whilst command of U.S. Ninth Army remained with 21st Army Group for the coming operations to cross the Rhine.\nCrossing the Rhine.\nIn February 1945, Montgomery's 21st Army Group advanced to the Rhine in Operation Veritable and Operation Grenade. It crossed the Rhine on 24 March 1945, in Operation Plunder, which took place two weeks after U.S. First Army had crossed the Rhine after capturing the Ludendorff Bridge during the Battle of Remagen.\n21st Army Group's river crossing was followed by the encirclement of the Ruhr Pocket. During this battle, U.S. Ninth Army, which had remained part of 21st Army Group after the Battle of the Bulge, formed the northern arm of the envelopment of German Army Group B, with U.S. First Army forming the southern arm. The two armies linked up on 1 April 1945, encircling 370,000 German troops, and on 4 April 1945, Ninth Army reverted to Omar Bradley's 12th Army Group.\nBy the war's end, the remaining formations of 21st Army group, First Canadian Army and British Second Army, had liberated the northern part of the Netherlands and captured much of north-west Germany, occupied Hamburg and Rostock and sealed off the Danish peninsula.\nOn 4 May 1945, on L\u00fcneburg Heath, Montgomery accepted the surrender of German forces in north-west Germany, Denmark and the Netherlands.\nCasualty conservation policy.\nThe British high command were not only concerned with winning the war and defeating Germany, but also with ensuring that it retained sufficient influence in the post-war world to govern global policy. Suffering heavy losses in Normandy would diminish British leadership and prestige within its empire and in post-war Europe in particular. Many of Montgomery's clashes with Eisenhower were based on his determination to pursue the war \"on lines most suitable to Britain\".\nThe fewer the number of combat-experienced divisions the British had left at the end of the war, the smaller Britain's influence in Europe was likely to be, compared to the emerging superpowers of the United States and the Soviet Union. Montgomery was thus caught in a dilemma\u2014the British Army needed to be seen to be pulling at least half the weight in the liberation of Europe, but without incurring the heavy casualties that such a role would inevitably produce. 21st Army Group scarcely possessed sufficient forces to achieve such a military prominence, and the remaining divisions had to be expended sparingly.\nIn 1944, Britain did not possess the manpower to rebuild shattered divisions and it was imperative for Montgomery to protect the viability of the British army. It was reported to the War Office that \"Montgomery has to be very careful of what he does on his eastern flank because on that flank is the only British Army there is left in this part of the world\". The context of British casualties and the shortage of reinforcements, prompted Montgomery to \"excessive caution\". Dempsey wrote on 13 June, that Caen could only be taken by a \"set piece assault and we did not have the men or the ammunition for that at the time\".\nMontgomery's solution to the dilemma was to attempt to remain Commander of All Land Forces until the end of the war, so that any victory attained on the Western front\u2014although achieved primarily by American formations\u2014would accrue in part to him and thus to Britain. He would also be able to ensure that British units were spared some of the high-attrition actions, but would be most prominent when the final blows were struck. When that strategy failed, he persuaded Eisenhower to occasionally put some American formations under the control of the 21st Army Group, so as to bolster his resources while still maintaining the outward appearance of successful British effort.\nMontgomery initially remained prepared to push Second (British) Army hard to capture the vital strategic town of Caen, and consequently incur heavy losses. In the original Overlord plan, Montgomery was determined to push past Caen to Falaise as quickly as possible. However, after the heavy casualties incurred in capturing Caen, he changed his mind.\nPersonality.\nMontgomery was notorious for his lack of tact and diplomacy. Even his \"patron\", the Chief of the Imperial General Staff, General Sir Alan Brooke, frequently mentions it in his war diaries: \"he is liable to commit untold errors in lack of tact\" and \"I had to haul him over the coals for his usual lack of tact and egotistical outlook which prevented him from appreciating other people's feelings\".\nOne incident that illustrated this occurred during the North African campaign when Montgomery bet Walter Bedell Smith that he could capture Sfax by the middle of April 1943. Smith jokingly replied that if Montgomery could do it he would give him a Flying Fortress complete with crew. Smith promptly forgot all about it, but Montgomery did not, and when Sfax was taken on 10 April, he sent a message to Smith \"claiming his winnings\". Smith tried to laugh it off, but Montgomery was having none of it and insisted on his aircraft. It got as high as Eisenhower who, with his renowned skill in diplomacy, ensured Montgomery did get his Flying Fortress, though at a great cost in ill feeling.\nAntony Beevor, in discussing Montgomery's counterproductive lack of tact in the final months of the war, described him as \"insufferable\". Beevor says that in January 1945 Montgomery had tried to claim far too much credit for the British (and for himself) in defeating the German counter-attack in the Ardennes in December 1944. This \"crass and unpleasant blunder\" helped make it impossible for Churchill and Alan Brooke to persuade Eisenhower of the need for an immediate thrust\u2014to be led by Montgomery\u2014through Germany to Berlin. Eisenhower did not accept the viability of the \"dagger thrust\" approach, it had already been agreed that Berlin would fall into the future Soviet occupation zone, and he was not willing to accept heavy casualties for no gain, so Eisenhower disregarded the British suggestions and continued with his conservative broad front strategy, and the Red Army reached Berlin well ahead of the Western Allies.\nIn August 1945, while Brooke, Sir Andrew Cunningham and Sir Charles Portal were discussing their possible successors as \"Chiefs of Staff\", they concluded that Montgomery would be very efficient as CIGS from the Army's point of view but that he was also very unpopular with a large proportion of the Army. Despite this, Cunningham and Portal were strongly in favour of Montgomery succeeding Brooke after his retirement. Churchill, by all accounts a faithful friend, is quoted as saying of Montgomery, \"In defeat, unbeatable; in victory, unbearable.\"\nMontgomery suffered from \"an overbearing conceit and an uncontrollable urge for self-promotion.\" General Hastings Ismay, who was at the time Winston Churchill's chief staff officer and trusted military adviser, once stated of Montgomery: \"I have come to the conclusion that his love of publicity is a disease, like alcoholism or taking drugs, and that it sends him equally mad.\"\nLater life.\nPost-war military career.\nAfter the war, Montgomery became the C-in-C of the British Army of the Rhine (BAOR), the name given to the British Occupation Forces, and was the British member of the Allied Control Council.\nChief of the Imperial General Staff.\nMontgomery was Chief of the Imperial General Staff (CIGS) from 1946 to 1948, succeeding Alan Brooke.\nAs CIGS, Montgomery toured Africa in 1947 and in a secret 1948 report to Prime Minister Clement Attlee's government proposed a \"master plan\" to amalgamate British Africa territories and to exploit the raw materials of Africa, thereby counteracting the loss of British influence in Asia. Montgomery sought to strengthen white rule to serve as a bulwark against communism. He described Africans as uncivilized, stating \"he is a complete savage and is quite incapable of developing the country himself.\" His statements were publicized in 1999. After learning of Montgomery's remarks, one of his biographers, Lord Chalfont, said his reputation had been \"irredeemably damaged... I find it very disappointing and depressing.\"\nHowever, Montgomery was barely on speaking terms with his fellow service chiefs, sending his deputy Kenneth Crawford to attend their meetings and he clashed particularly with Sir Arthur Tedder, who was by now Chief of the Air Staff (CAS).\nWhen Montgomery's term of office expired, Prime Minister Attlee appointed Sir William Slim from retirement with the rank of field marshal as his successor. When Montgomery protested that he had told his prot\u00e9g\u00e9, General Sir John Crocker, former commander of I Corps in the 1944\u201345 North-West Europe Campaign, that the job was to be his, Attlee is said to have retorted \"Untell him.\"\nWestern Union Defence Organization.\nMontgomery was then appointed Chairman of the Western Union Defence Organization's C-in-C committee. Volume 3 of Nigel Hamilton's \"Life of Montgomery of Alamein\" gives an account of the bickering between Montgomery and his land forces chief, French General Jean de Lattre de Tassigny, which created splits through the Union headquarters.\nNATO.\nOn the creation of the North Atlantic Treaty Organisation's Supreme Headquarters Allied Powers Europe in 1951, Montgomery became Eisenhower's deputy. He would continue to serve under Eisenhower's successors, Generals Matthew Ridgway and Al Gruenther, until his retirement, aged nearly 71, in 1958.\nPersonal.\nMontgomery was created 1st Viscount Montgomery of Alamein in 1946.\nMontgomery's mother, Maude Montgomery, died in 1949. Montgomery did not attend the funeral, claiming he was \"too busy\".\nMontgomery was an Honorary Member of the Winkle Club, a charity in Hastings, East Sussex, and introduced Winston Churchill to the club in 1955.\nHe was chairman of the governing body of St. John's School in Leatherhead, Surrey, from 1951 to 1966, and a generous supporter.\nHe was also President of Portsmouth Football Club between 1944 and 1961.\nIn the mid-1950s, the \"Illustrated London News\" published sets of photographs taken by Montgomery while flying over the Swiss Alps. In February 1957, views of Mount Toedi taken with a Rolleiflex camera were reproduced.\nOpinions.\nMemoirs.\nMontgomery's memoirs (1958) criticised many of his wartime comrades in harsh terms, including Eisenhower. He was threatened with legal action by Field Marshal Auchinleck for suggesting that Auchinleck had intended to retreat from the Alamein position if attacked again, and had to give a radio broadcast (20 November 1958) expressing his gratitude to Auchinleck for having stabilised the front at the First Battle of Alamein.\nThe 1960 paperback edition of Montgomery's memoirs contains a publisher's note drawing attention to that broadcast, and stating that although the reader might assume from Montgomery's text that Auchinleck had been planning to retreat \"into the Nile Delta or beyond\" in the publisher's view it had been Auchinleck's intention to launch an offensive as soon as the Eighth Army was \"rested and regrouped\". Montgomery was stripped of his honorary citizenship of Montgomery, Alabama, and was challenged to a duel by an Italian lawyer.\nMontgomery mentioned to the American journalist John Gunther in April 1944 that (like Alanbrooke) he kept a secret diary. Gunther remarked that it would surely be an essential source for historians. When Montgomery asked whether it would be worth money one day, Gunther suggested \"at least $100,000.\" This was converted into pounds sterling, and he is supposed to have grinned and said \"Well, I guess I won't die in the poor house after all.\"\nMilitary opinions.\nMontgomery twice met Israeli general Moshe Dayan. After an initial meeting in the early 1950s, Montgomery met Dayan again in the 1960s to discuss the Vietnam War, which Dayan was studying. Montgomery was harshly critical of US strategy in Vietnam, which involved deploying large numbers of combat troops, aggressive bombing attacks, and uprooting entire village populations and forcing them into strategic hamlets. Montgomery said that the Americans' most important problem was that they had no clear objective, and allowed local commanders to set military policy. At the end of their meeting, Montgomery asked Dayan to tell the Americans, in his name, that they were \"insane\".\nDuring a visit to the Alamein battlefields in May 1967, he bluntly told high-ranking Egyptian Army officers that they would lose any war with Israel, a warning that was shown to be justified only a few weeks later in the Six-Day War.\nSocial opinions.\nIn retirement, Montgomery publicly supported apartheid after a visit to South Africa in 1962, and after a visit to China declared himself impressed by the Chinese leadership led by Chairman Mao Tse-tung. He spoke out against the legalisation of homosexuality in the United Kingdom, arguing that the Sexual Offences Act 1967 was a \"charter for buggery\" and that \"this sort of thing may be tolerated by the French, but we're British\u2014thank God\".\nMontgomery was a non-smoking teetotaller, a vegetarian, and a Christian.\nDeath.\nMontgomery died in 1976 at his home Isington Mill in Isington, Hampshire, aged 88. After a funeral at St George's Chapel, Windsor, his body was buried in Holy Cross churchyard, in Binsted, Hampshire."}
{"id": "3874", "revid": "25511559", "url": "https://en.wikipedia.org/wiki?curid=3874", "title": "Herman Boerhaave", "text": "Herman Boerhaave (, 31 December 1668 \u2013 23 September 1738) was a Dutch botanist, chemist, Christian humanist, and physician of European fame. He is regarded as the founder of clinical teaching and of the modern academic hospital and is sometimes referred to as \"the father of physiology,\" along with Venetian physician Santorio Santorio (1561\u20131636). Boerhaave introduced the quantitative approach into medicine, along with his pupil Albrecht von Haller (1708\u20131777) and is best known for demonstrating the relation of symptoms to lesions. He was the first to isolate the chemical urea from urine. He was the first physician to put thermometer measurements to clinical practice. His motto was \"Simplex sigillum veri\": 'Simplicity is the sign of the truth'. He is often hailed as the \"Dutch Hippocrates\".\nBiography.\nBoerhaave was born at Voorhout near Leiden. The son of a Protestant pastor, in his youth Boerhaave studied for a divinity degree and wanted to become a preacher. After the death of his father, however, he was offered a scholarship and he entered the University of Leiden, where he took his master's degree in philosophy in 1690, with a dissertation titled \"De distinctione mentis a corpore\" (\"On the Difference of the Mind from the Body\"). There he attacked the doctrines of Epicurus, Thomas Hobbes and Baruch Spinoza. He then turned to the study of medicine. He earned his medical doctorate from the University of Harderwijk (present-day Gelderland) in 1693, with a dissertation titled \"De utilitate explorandorum in aegris excrementorum ut signorum\" (\"The Utility of Examining Signs of Disease in the Excrement of the Sick\").\nIn 1701 he was appointed lecturer on the institutes of medicine at Leiden; in his inaugural discourse, \"De commendando Hippocratis studio\", he recommended to his pupils that great physician as their model. In 1709 he became professor of botany and medicine, and in that capacity he did good service, not only to his own university, but also to botanical science, by his improvements and additions to the botanic garden of Leiden, and by the publication of numerous works descriptive of new species of plants.\nOn 14 September 1710, Boerhaave married Maria Drolenvaux, the daughter of the rich merchant, Alderman Abraham Drolenvaux. They had four children, of whom one daughter, Maria Joanna, lived to adulthood. In 1722, he began to suffer from an extreme case of gout, recovering the next year.\nIn 1714, when he was appointed rector of the university, he succeeded Govert Bidloo in the chair of practical medicine, and in this capacity he introduced the modern system of clinical instruction. Four years later he was appointed to the chair of chemistry as well. In 1728 he was elected into the French Academy of Sciences, and two years later into the Royal Society of London. In 1729 declining health obliged him to resign the chairs of chemistry and botany; and he died, after a lingering and painful illness, at Leiden.\nLegacy.\nHis reputation so increased the fame of the University of Leiden, especially as a school of medicine, that it became popular with visitors from every part of Europe. All the princes of Europe sent him pupils, who found in this skilful professor not only an indefatigable teacher, but an affectionate guardian. When Peter the Great went to Holland in 1716 (he had been in Holland before in 1697 to instruct himself in maritime affairs), he also took lessons from Boerhaave. Voltaire travelled to see him, as did Carl Linnaeus, who became a close friend and named the genus \"Boerhavia\" for him. His reputation was not confined to Europe; a Chinese mandarin sent him a letter addressed to \"the illustrious Boerhaave, physician in Europe,\" and it reached him in due course.\nThe operating theatre of the University of Leiden in which he once worked as an anatomist is now at the centre of a museum named after him; the Boerhaave Museum. Asteroid 8175 Boerhaave is named after Boerhaave. From 1955 to 1961 Boerhaave's image was printed on Dutch 20-guilder banknotes. The Leiden University Medical Centre organises medical trainings called \"Boerhaave-courses\".\nHe had a prodigious influence on the development of medicine and chemistry in Scotland. British medical schools credit Boerhaave for developing the system of medical education upon which their current institutions are based. Every founding member of the Edinburgh Medical School had studied at Leyden and attended Boerhaave's lectures on chemistry including John Rutherford and Francis Home. Boerhaave's \"Elementa Chemiae (1732)\" is recognised as the first text on chemistry. \nBoerhaave first described Boerhaave syndrome, which involves tearing of the oesophagus, usually a consequence of vigorous vomiting. Notoriously, in 1724 he described the case of Baron Jan van Wassenaer, a Dutch admiral who died of this condition following a gluttonous feast and subsequent regurgitation. The condition was uniformly fatal prior to modern surgical techniques allowing repair of the oesophagus.\nBoerhaave was critical of his Dutch contemporary Baruch Spinoza, attacking him in his 1688 dissertation. At the same time, he admired Isaac Newton and was a devout Christian who often wrote about God in his works. A collection of his religious thoughts on medicine, translated from Latin to English, has been compiled by the \"Sir Thomas Browne Instituut Leiden\" under the name \"Boerhaave's Orations\" (meaning \"Boerhaave's Prayers\"). Among other things, he considered nature as God's Creation, and he used to say that the poor were his best patients because God was their paymaster.\nMedical contributions.\nBoerhaave devoted himself intensively to the study of the human body. He was strongly influenced by the mechanistic theories of Ren\u00e9 Descartes, and those of the 17th-century astronomer and mathematician Giovanni Borelli, who described animal movements in terms of mechanical motion. On such premises Boerhaave proposed a hydraulic model of human physiology. His writings refer to simple machines such as levers and pulleys and similar mechanisms, and he saw the bodily organs and members as being assembled from pipe-like structures. The physiology of veins, for example, he compared to the operation of pipes. He asserted the importance of a proper balance of fluid pressure, noting that fluids should be able to move around the body freely, without obstacles. For its well-being the body needed to be self-regulating, so as to maintain a healthy state of equilibrium. Boerhaave's concept of the body as apparatus centred his medical attention on material problems rather than upon ontological or esoteric explanations of illness.\nBoerhaave's teaching of his knowledge and philosophy drew many students to the University of Leiden. He emphasised the importance of anatomical research based on practical observation and scientific experiment. His concept of the bodily system took hold throughout Europe, and helped to transform medical education in the European schools. His insights aroused great interest among other critical medical thinkers, not least in Friedrich Hoffmann, who strongly advocated the importance of physico-mechanical principles for the preservation or indeed the restoration of health. As a professor at Leiden, Boerhaave influenced many students. Some in their experiments upheld and furthered his philosophy, while others rejected it and proposed alternative theories of human physiology. He produced a great many textbooks and writings through which the digested brilliance of his lectures at Leiden was circulated widely in Europe. In 1708 his publication of the \"Institutiones Medicae\" was issued in over five languages, and went into approximately ten editions. His \"Elementa Chemia\", a world-renowned chemistry textbook, was published in 1732.\nThe mechanistic concept of the human body departed from the age-old precepts laid down by Galen and Aristotle. In place of a servile dependence upon teachings handed down from antiquity, Boerhaave understood the importance of establishing definitive findings through his own investigation, and by the direct application of his own methods of testing. This new reasoning expanded the field of Renaissance anatomy: it opened the way to reforms of medical practice and understanding in the field of iatrochemistry."}
{"id": "3875", "revid": "458237", "url": "https://en.wikipedia.org/wiki?curid=3875", "title": "Benjamin Disraeli", "text": "Benjamin Disraeli, 1st Earl of Beaconsfield (21 December 1804\u00a0\u2013 19 April 1881) was a British statesman, Conservative politician and writer who twice served as Prime Minister of the United Kingdom. He played a central role in the creation of the modern Conservative Party, defining its policies and its broad outreach. Disraeli is remembered for his influential voice in world affairs, his political battles with the Liberal Party leader William Ewart Gladstone, and his one-nation conservatism or \"Tory democracy\". He made the Conservatives the party most identified with the British Empire and military action to expand it, both of which were popular among British voters. He is the only British prime minister to have been born Jewish.\nDisraeli was born in Bloomsbury, then a part of Middlesex. His father left Judaism after a dispute at his synagogue; Benjamin became an Anglican at the age of 12. After several unsuccessful attempts, Disraeli entered the House of Commons in 1837. In 1846, Prime Minister Robert Peel split the party over his proposal to repeal the Corn Laws, which involved ending the tariff on imported grain. Disraeli clashed with Peel in the House of Commons, becoming a major figure in the party. When Lord Derby, the party leader, thrice formed governments in the 1850s and 1860s, Disraeli served as Chancellor of the Exchequer and Leader of the House of Commons.\nUpon Derby's retirement in 1868, Disraeli became prime minister briefly before losing that year's general election. He returned to the Opposition before leading the party to a majority in the 1874 general election. He maintained a close friendship with Queen Victoria who, in 1876, elevated him to the peerage as Earl of Beaconsfield. Disraeli's second term was dominated by the Eastern question\u2014the slow decay of the Ottoman Empire and the desire of other European powers, such as Russia, to gain at its expense. Disraeli arranged for the British to purchase a major interest in the Suez Canal Company in Egypt. In 1878, faced with Russian victories against the Ottomans, he worked at the Congress of Berlin to obtain peace in the Balkans at terms favourable to Britain and unfavourable to Russia, its longstanding enemy. This diplomatic victory established Disraeli as one of Europe's leading statesmen.\nWorld events thereafter moved against the Conservatives. Controversial wars in Afghanistan and South Africa undermined his public support. He angered farmers by refusing to reinstitute the Corn Laws in response to poor harvests and cheap imported grain. With Gladstone conducting a massive speaking campaign, the Liberals defeated Disraeli's Conservatives at the 1880 general election. In his final months, Disraeli led the Conservatives in Opposition. Disraeli wrote novels throughout his career, beginning in 1826, and published his last completed novel, \"Endymion\", shortly before he died at the age of 76.\nEarly life.\nChildhood.\nDisraeli was born on 21 December 1804 at 6 King's Road, Bedford Row, Bloomsbury, London, the second child and eldest son of Isaac D'Israeli, a literary critic and historian, and Maria (Miriam), \"n\u00e9e\" Basevi. The family was mostly from Italy, of Sephardic Jewish mercantile background. He also had some Ashkenazi Jewish ancestors. He later romanticised his origins, claiming his father's family was of grand Iberian and Venetian descent; in fact, Isaac's family was of no great distinction, but on Disraeli's mother's side, in which he took no interest, there were some distinguished forebears, including Isaac Cardoso, as well as members of the Goldsmids, the Mocattas and the Montefiores. Historians differ on Disraeli's motives for rewriting his family history: Bernard Glassman argues that it was intended to give him status comparable to that of England's ruling elite; Sarah Bradford believes \"his dislike of the commonplace would not allow him to accept the facts of his birth as being as middle-class and undramatic as they really were\".\nDisraeli's siblings were Sarah, Naphtali (born and died 1807), Ralph and James (\"Jem\"). He was close to his sister and on affectionate but more distant terms with his surviving brothers. Details of his schooling are sketchy. From the age of about six he was a day boy at a dame school in Islington, which one of his biographers described as \"for those days a very high-class establishment\". Two years later or so\u2014the exact date has not been ascertained\u2014he was sent as a boarder to Rev John Potticary's school at Blackheath.\nFollowing a quarrel in 1813 with the Bevis Marks Synagogue, his father renounced Judaism and had the four children baptised into the Church of England in July and August 1817. Isaac D'Israeli had never taken religion very seriously but had remained a conforming member of the synagogue. His father Benjamin was a prominent and devout member; it was probably out of respect for him that Isaac did not leave when he fell out with the synagogue authorities in 1813. After Benjamin senior died in 1816, Isaac felt free to leave the congregation following a second dispute. Isaac's friend Sharon Turner, a solicitor, convinced him that although he could comfortably remain unattached to any formal religion it would be disadvantageous to the children if they did so. Turner stood as godfather when Benjamin was baptised, aged twelve, on 31 July 1817.\nConversion enabled Disraeli to contemplate a career in politics. There had been Members of Parliament (MPs) from Jewish families since Sampson Gideon in 1770. However, until the Jews Relief Act 1858, MPs were required to take the oath of allegiance \"on the true faith of a Christian\", necessitating at least nominal conversion. It is not known whether Disraeli formed any ambition for a parliamentary career at the time of his baptism, but there is no doubt that he bitterly regretted his parents' decision not to send him to Winchester College, one of the great public schools which consistently provided recruits to the political elite. His two younger brothers were sent there, and it is not clear why Isaac chose to send his eldest son to a much less prestigious school. The boy evidently held his mother responsible for the decision; Bradford speculates that \"Benjamin's delicate health and his obviously Jewish appearance may have had something to do with it.\" The school chosen for him was run by Eliezer Cogan at Higham Hill in Walthamstow. He began there in the autumn term of 1817; he later recalled his education:\n1820s.\nIn November 1821, shortly before his seventeenth birthday, Disraeli was articled as a clerk to a firm of solicitors\u2014Swain, Stevens, Maples, Pearse and Hunt\u2014in the City of London. T F Maples was not only the young Disraeli's employer and a friend of his father's, but also his prospective father-in-law: Isaac and Maples considered that the latter's only daughter might be a suitable match for Benjamin. A friendship developed, but there was no romance. The firm had a large and profitable business, and as the biographer R W Davis observes, the clerkship was \"the kind of secure, respectable position that many fathers dream of for their children\". Although biographers including Robert Blake and Bradford comment that such a post was incompatible with Disraeli's romantic and ambitious nature, he reportedly gave his employers satisfactory service, and later professed to have learnt a good deal there. He recalled: I had some scruples, for even then I dreamed of Parliament. My father's refrain always was 'Philip Carteret Webb', who was the most eminent solicitor of his boyhood and who was an MP. It would be a mistake to suppose that the two years and more that I was in the office of our friend were wasted. I have often thought, though I have often regretted the University, that it was much the reverse.\nThe year after joining Maples' firm, Benjamin changed his surname from D'Israeli to Disraeli. His reasons are unknown, but the biographer Bernard Glassman surmises that it was to avoid being confused with his father. Disraeli's sister and brothers adopted the new version of the name; Isaac and his wife retained the older form.\nDisraeli toured Belgium and the Rhine Valley with his father in the summer of 1824. He later wrote that while travelling on the Rhine he decided to abandon his position: \"I determined when descending those magical waters that I would not be a lawyer.\" On their return to England he left the solicitors, at the suggestion of Maples, with the aim of qualifying as a barrister. He enrolled as a student at Lincoln's Inn and joined the chambers of his uncle, Nathaniel Basevy, and then those of Benjamin Austen, who persuaded Isaac that Disraeli would never make a barrister and should be allowed to pursue a literary career. He had made a tentative start: in May 1824 he submitted a manuscript to his father's friend, the publisher John Murray, but withdrew it before Murray could decide whether to publish it.\nReleased from the law, Disraeli did some work for Murray, but turned most of his attention to speculative dealing on the stock exchange. There was at the time a boom in shares in South American mining companies. Spain was losing its South American colonies in the face of rebellions. At the urging of George Canning the British government recognised the new independent governments of Argentina (1824), Colombia and Mexico (both 1825). With no money of his own, Disraeli borrowed money to invest. He became involved with the financier J. D. Powles, who was prominent among those encouraging the mining boom. In 1825, Disraeli wrote three anonymous pamphlets for Powles, promoting the companies. The pamphlets were published by John Murray, who invested heavily in the boom.\nMurray had ambitions to establish a new morning paper to compete with \"The Times\". In 1825 Disraeli convinced him that he should proceed. The new paper, \"The Representative\", promoted the mines and those politicians who supported them, particularly Canning. Disraeli impressed Murray with his energy and commitment to the project, but he failed in his key task of persuading the eminent writer John Gibson Lockhart to edit the paper. After that, Disraeli's influence on Murray waned, and to his resentment he was sidelined in the affairs of \"The Representative\". The paper survived only six months, partly because the mining bubble burst in late 1825, and partly because, according to Blake, the paper was \"atrociously edited\".\nThe bursting of the mining bubble was ruinous for Disraeli. By June 1825 he and his business partners had lost \u00a37,000. Disraeli could not pay off the last of his debts from this debacle until 1849. He turned to writing, motivated partly by his desperate need for money, and partly by a wish for revenge on Murray and others by whom he felt slighted. There was a vogue for what was called \"silver-fork fiction\"\u2014novels depicting aristocratic life, usually by anonymous authors, read by the aspirational middle classes. Disraeli's first novel, \"Vivian Grey\", published anonymously in four volumes in 1826\u201327, was a thinly veiled re-telling of the affair of \"The Representative\". It sold well, but caused much offence in influential circles when the authorship was discovered. Disraeli, then just 23, did not move in high society, as the numerous solecisms in his book made obvious. Reviewers were sharply critical on these grounds of both the author and the book. Murray and Lockhart, men of great influence in literary circles, believed that Disraeli had caricatured them and abused their confidence\u2014an accusation denied by the author but repeated by many of his biographers. In later editions Disraeli made many changes, softening his satire, but the damage to his reputation proved long-lasting.\nDisraeli's biographer Jonathan Parry writes that the financial failure and personal criticism that Disraeli suffered in 1825 and 1826 were probably the trigger for a serious nervous crisis affecting him over the next four years: \"He had always been moody, sensitive, and solitary by nature, but now became seriously depressed and lethargic.\" He was still living with his parents in London, but in search of the \"change of air\" recommended by the family's doctors, Isaac took a succession of houses in the country and on the coast, before Disraeli sought wider horizons.\n1830\u20131837.\nTogether with his sister's fianc\u00e9, William Meredith, Disraeli travelled widely in southern Europe and beyond in 1830\u201331. The trip was financed partly by another high society novel, \"The Young Duke\", written in 1829\u201330. The tour was cut short suddenly by Meredith's death from smallpox in Cairo in July 1831. Despite this tragedy, and the need for treatment for a sexually transmitted disease on his return, Disraeli felt enriched by his experiences. He became, in Parry's words, \"aware of values that seemed denied to his insular countrymen. The journey encouraged his self-consciousness, his moral relativism, and his interest in Eastern racial and religious attitudes.\" Blake regards the tour as one of the formative experiences of Disraeli's career: \"[T]he impressions that it made on him were life-lasting. They conditioned his attitude toward some of the most important political problems which faced him in his later years\u2014especially the Eastern Question; they also coloured many of his novels.\"\nDisraeli wrote two novels in the aftermath of the tour. \"Contarini Fleming\" (1832) was avowedly a self-portrait. It is subtitled \"a psychological autobiography\" and depicts the conflicting elements of its hero's character: the duality of northern and Mediterranean ancestry, the dreaming artist and the bold man of action. As Parry observes, the book ends on a political note, setting out Europe's progress \"from feudal to federal principles\". \"The Wondrous Tale of Alroy\" the following year portrayed the problems of a medieval Jew in deciding between a small, exclusively Jewish state and a large empire embracing all.\nAfter these novels were published, Disraeli declared that he would \"write no more about myself\". He had already turned his attention to politics in 1832, during the great crisis over the Reform Bill. He contributed to an anti-Whig pamphlet edited by John Wilson Croker and published by Murray entitled \"England and France: or a cure for Ministerial Gallomania\". The choice of a Tory publication was regarded as strange by Disraeli's friends and relatives, who thought him more of a Radical. Indeed, he had objected to Murray about Croker's inserting \"high Tory\" sentiment: Disraeli remarked, \"it is quite impossible that anything adverse to the general measure of Reform can issue from my pen.\" Moreover, at the time \"Gallomania\" was published, Disraeli was electioneering in High Wycombe in the Radical interest.\nDisraeli's politics at the time were influenced both by his rebellious streak and his desire to make his mark. At that time, British politics were dominated by the aristocracy, with a few powerful commoners. The Whigs derived from the coalition of Lords who had forced through the Bill of Rights 1689 and in some cases were their descendants. The Tories tended to support King and Church and sought to thwart political change. A small number of Radicals, generally from northern constituencies, were the strongest advocates of continuing reform. In the early 1830s the Tories and the interests they represented appeared to be a lost cause. The other great party, the Whigs, were anathema to Disraeli: \"Toryism is worn out &amp; I cannot condescend to be a Whig.\" There was a by-election and a general election in 1832; Disraeli unsuccessfully stood as a Radical at High Wycombe in each.\nDisraeli's political views embraced certain Radical policies, particularly electoral reform, and also some Tory ones, including protectionism. He began to move in Tory circles. In 1834 he was introduced to the former Lord Chancellor, Lord Lyndhurst, by Henrietta Sykes, wife of Sir Francis Sykes. She was having an affair with Lyndhurst and began another with Disraeli. Disraeli and Lyndhurst took an immediate liking to each other. Lyndhurst was an indiscreet gossip with a fondness for intrigue; this appealed greatly to Disraeli, who became his secretary and go-between. In 1835 Disraeli stood for the last time as a Radical, again unsuccessfully contesting High Wycombe.\nIn April 1835, Disraeli fought a by-election at Taunton as a Tory candidate. The Irish MP Daniel O'Connell, misled by inaccurate press reports, thought Disraeli had slandered him while electioneering at Taunton; he launched an outspoken attack, referring to Disraeli as:\nDisraeli's public exchanges with O'Connell, extensively reproduced in \"The Times\", included a demand for a duel with the 60-year-old O'Connell's son (which resulted in Disraeli's temporary detention by the authorities), a reference to \"the inextinguishable hatred with which [he] shall pursue [O'Connell's] existence\", and the accusation that O'Connell's supporters had a \"princely revenue wrung from a starving race of fanatical slaves\". Disraeli was highly gratified by the dispute, which propelled him to general public notice for the first time. He did not defeat the incumbent Whig member, Henry Labouchere, but the Taunton constituency was regarded as unwinnable by the Tories. Disraeli kept Labouchere's majority down to 170, a good showing that put him in line for a winnable seat in the near future.\nWith Lyndhurst's encouragement Disraeli turned to writing propaganda for his newly adopted party. His \"Vindication of the English Constitution\", was published in December 1835. It was couched in the form of an open letter to Lyndhurst, and in Bradford's view encapsulates a political philosophy that Disraeli adhered to for the rest of his life: the value of benevolent aristocratic government, a loathing of political dogma, and the modernisation of Tory policies. The following year he wrote a series of satires on politicians of the day, which he published in \"The Times\" under the pen-name \"Runnymede\". His targets included the Whigs, collectively and individually, Irish nationalists, and political corruption. One essay ended:\nDisraeli was elected to the exclusively Tory Carlton Club in 1836, and was also taken up by the party's leading hostess, Lady Londonderry. In June 1837 William IV died, the young Queen Victoria succeeded him, and parliament was dissolved. On the recommendation of the Carlton Club, Disraeli was adopted as a Tory parliamentary candidate at the ensuing general election.\nParliament.\nBack-bencher.\nIn the election in July 1837, Disraeli won a seat in the House of Commons as one of two members, both Tory, for the constituency of Maidstone. The other was Wyndham Lewis, who helped finance Disraeli's election campaign, and who died the following year. In the same year Disraeli published a novel, \"Henrietta Temple\", which was a love story and social comedy, drawing on his affair with Henrietta Sykes. He had broken off the relationship in late 1836, distraught that she had taken yet another lover. His other novel of this period is \"Venetia\", a romance based on the characters of Shelley and Byron, written quickly to raise much-needed money.\nDisraeli made his maiden speech in Parliament on 7 December 1837. He followed O'Connell, whom he sharply criticised for the latter's \"long, rambling, jumbling, speech\". He was shouted down by O'Connell's supporters. After this unpromising start Disraeli kept a low profile for the rest of the parliamentary session. He was a loyal supporter of the party leader Sir Robert Peel and his policies, with the exception of a personal sympathy for the Chartist movement that most Tories did not share.\nIn 1839 Disraeli married Mary Anne Lewis, the widow of Wyndham Lewis. Twelve years Disraeli's senior, Mary Lewis had a substantial income of \u00a35,000 a year. His motives were generally assumed to be mercenary, but the couple came to cherish one another, remaining close until she died more than three decades later. \"Dizzy married me for my money\", his wife said later, \"But, if he had the chance again, he would marry me for love.\"\nFinding the financial demands of his Maidstone seat too much, Disraeli secured a Tory nomination for Shrewsbury, winning one of the constituency's two seats at the 1841 general election, despite serious opposition, and heavy debts which opponents seized on. The election was a massive defeat for the Whigs across the country, and Peel became prime minister. Disraeli hoped, unrealistically, for ministerial office. Though disappointed at being left on the back benches, he continued his support for Peel in 1842 and 1843, seeking to establish himself as an expert on foreign affairs and international trade.\nAlthough a Tory (or Conservative, as some in the party now called themselves) Disraeli was sympathetic to some of the aims of Chartism, and argued for an alliance between the landed aristocracy and the working class against the increasing power of the merchants and new industrialists in the middle class. After Disraeli won widespread acclaim in March 1842 for worsting Lord Palmerston in debate, he was taken up by a small group of idealistic new Tory MPs, with whom he formed the Young England group. They held that the landed interests should use their power to protect the poor from exploitation by middle-class businessmen.\nDisraeli hoped to forge a paternalistic Tory-Radical alliance, but he was unsuccessful. Before the Reform Act 1867, the working class did not possess the vote and therefore had little political power. Although Disraeli forged a personal friendship with John Bright, a leading Radical, Disraeli was unable to persuade Bright to sacrifice his distinct position for parliamentary advancement. When Disraeli attempted to secure a Tory-Radical cabinet in 1852, Bright refused.\nDisraeli gradually became a sharp critic of Peel's government, often deliberately taking contrary positions. The young MP attacked his leader as early as 1843. However, the best known of these stances were over the Maynooth Grant in 1845 and the repeal of the Corn Laws in 1846. The President of the Board of Trade, William Gladstone, resigned from the cabinet over the Maynooth Grant. The Corn Laws imposed a tariff on imported wheat, protecting British farmers from foreign competition, but making the cost of bread artificially high. Peel hoped that the repeal of the Corn Laws and the resultant influx of cheaper wheat into Britain would relieve the condition of the poor, and in particular the Great Famine caused by successive failure of potato crops in Ireland.\nThe first months of 1846 were dominated by a battle in Parliament between the free traders and the protectionists over the repeal of the Corn Laws, with the latter rallying around Disraeli and Lord George Bentinck. An alliance of free-trade Conservatives (the \"Peelites\"), Radicals, and Whigs carried repeal, and the Conservative Party split: the Peelites moved towards the Whigs, while a \"new\" Conservative Party formed around the protectionists, led by Disraeli, Bentinck, and Lord Stanley (later Lord Derby).\nThe split in the Tory party over the repeal of the Corn Laws had profound implications for Disraeli's political career: almost every Tory politician with experience of office followed Peel, leaving the rump bereft of leadership. In Blake's words, \"[Disraeli] found himself almost the only figure on his side capable of putting up the oratorical display essential for a parliamentary leader.\" The Duke of Argyll wrote that Disraeli \"was like a subaltern in a great battle where every superior officer was killed or wounded\". If the Tory Party could muster the electoral support necessary to form a government, then Disraeli now seemed to be guaranteed high office, but with a group of men who possessed little or no official experience and who, as a group, remained personally hostile to Disraeli. In the event the Tory split soon had the party out of office, not regaining power until 1852. The Conservatives would not again have a majority in the House of Commons until 1874.\nBentinck and the leadership.\nPeel successfully steered the repeal of the Corn Laws through Parliament and was then defeated by an alliance of his enemies on the issue of Irish law and order; he resigned in June 1846. The Tories remained split, and the Queen sent for Lord John Russell, the Whig leader. In the 1847 general election, Disraeli stood, successfully, for the Buckinghamshire constituency. The new House of Commons had more Conservative than Whig members, but the depth of the Tory schism enabled Russell to continue to govern. The Conservatives were led by Bentinck in the Commons and Stanley in the Lords.\nIn 1847 a small political crisis removed Bentinck from the leadership and highlighted Disraeli's differences with his own party. In that year's general election, Lionel de Rothschild had been returned for the City of London. As a practising Jew he could not take the oath of allegiance in the prescribed Christian form, and therefore could not take his seat. Lord John Russell, the Whig leader who had succeeded Peel as prime minister, proposed in the Commons that the oath should be amended to permit Jews to enter Parliament.\nDisraeli spoke in favour of the measure, arguing that Christianity was \"completed Judaism\", and asking the House of Commons \"Where is your Christianity if you do not believe in their Judaism?\" Russell and Disraeli's future rival Gladstone thought this brave; the speech was badly received by his own party. The Tories and the Anglican establishment were hostile to the bill. With the exception of Disraeli, every member of the future protectionist cabinet then in Parliament voted against the measure. The measure was voted down. In the aftermath of the debate Bentinck resigned the leadership and was succeeded by Lord Granby; Disraeli's speech, thought by many of his own party to be blasphemous, ruled him out for the time being.\nWhile these intrigues played out, Disraeli was working with the Bentinck family to secure the necessary financing to purchase Hughenden Manor, in Buckinghamshire. The possession of a country house and incumbency of a county constituency were regarded as essential for a Tory with leadership ambitions. Disraeli and his wife alternated between Hughenden and several homes in London for the rest of their marriage. The negotiations were complicated by Bentinck's sudden death on 21 September 1848, but Disraeli obtained a loan of \u00a325,000 from Bentinck's brothers Lord Henry Bentinck and Lord Titchfield.\nWithin a month of his appointment Granby resigned the leadership in the Commons and the party functioned without a leader in the Commons for the rest of the session. At the start of the next session, affairs were handled by a triumvirate of Granby, Disraeli, and John Charles Herries\u2014indicative of the tension between Disraeli and the rest of the party, who needed his talents but mistrusted him. This confused arrangement ended with Granby's resignation in 1851; Disraeli effectively ignored the two men regardless.\nChancellor of the Exchequer.\nFirst Derby government.\nIn March 1851, Lord John Russell's government was defeated over a bill to equalise the county and borough franchises, mostly because of divisions among his supporters. He resigned, and the Queen sent for Stanley, who felt that a minority government could do little and would not last long, so Russell remained in office. Disraeli regretted this, hoping for an opportunity, however brief, to show himself capable in office. Stanley, in contrast, deprecated his inexperienced followers as a reason for not assuming office: \"These are not names I can put before the Queen.\"\nAt the end of June 1851, Stanley succeeded to the title of Earl of Derby. The Whigs were wracked by internal dissensions during the second half of 1851, much of which Parliament spent in recess. Russell dismissed Lord Palmerston from the cabinet, leaving the latter determined to deprive the Prime Minister of office. Palmerston did so within weeks of Parliament's reassembly on 4 February 1852, his followers combining with Disraeli's Tories to defeat the government on a Militia Bill, and Russell resigned. Derby had either to take office or risk damage to his reputation, and he accepted the Queen's commission as prime minister. Palmerston declined any office; Derby had hoped to have him as Chancellor of the Exchequer. Disraeli, his closest ally, was his second choice and accepted, though disclaiming any great knowledge in the financial field. Gladstone refused to join the government. Disraeli may have been attracted to the office by the \u00a35,000 annual salary, which would help pay his debts. Few of the new cabinet had held office before; when Derby tried to inform the Duke of Wellington of the names of the ministers, the old Duke, who was somewhat deaf, inadvertently branded the new government by incredulously repeating \"Who? Who?\"\nIn the following weeks, Disraeli served as Leader of the House (with Derby as prime minister in the Lords) and as Chancellor. He wrote regular reports on proceedings in the Commons to Victoria, who described them as \"very curious\" and \"much in the style of his books\". Parliament was prorogued on 1 July 1852 as the Tories could not govern for long as a minority; Disraeli hoped that they would gain a majority of about 40. Instead, the election later that month had no clear winner, and the Derby government held to power pending the meeting of Parliament.\nBudget.\nDisraeli's task as Chancellor was to devise a budget which would satisfy the protectionist elements who supported the Tories, without uniting the free-traders against it. His proposed budget, which he presented to the Commons on 3 December, lowered the taxes on malt and tea, provisions designed to appeal to the working class. To make his budget revenue-neutral, as funds were needed to provide defences against the French, he doubled the house tax and continued the income tax. Disraeli's overall purpose was to enact policies which would benefit the working classes, making his party more attractive to them. Although the budget did not contain protectionist features, the Opposition was prepared to destroy it\u2014and Disraeli's career as Chancellor\u2014in part out of revenge for his actions against Peel in 1846. MP Sidney Herbert predicted that the budget would fail because \"Jews make no converts\".\nDisraeli delivered the budget on 3 December 1852, and prepared to wind up the debate for the government on 16 December\u2014it was customary for the Chancellor to have the last word. A massive defeat for the government was predicted. Disraeli attacked his opponents individually, and then as a force: \"I face a Coalition\u00a0... This, too, I know, that England does not love coalitions.\" His speech of three hours was quickly seen as a parliamentary masterpiece. As MPs prepared to divide, Gladstone rose to his feet and began an angry speech, despite the efforts of Tory MPs to shout him down. The interruptions were fewer, as Gladstone gained control of the House, and in the next two hours painted a picture of Disraeli as frivolous and his budget as subversive. The government was defeated by 19 votes, and Derby resigned four days later. He was replaced by the Peelite Earl of Aberdeen, with Gladstone as his Chancellor. Because of Disraeli's unpopularity among the Peelites, no party reconciliation was possible while he remained Tory leader in the Commons.\nOpposition.\nWith the fall of the government, Disraeli and the Conservatives returned to the Opposition benches. Disraeli would spend three-quarters of his 44-year parliamentary career in Opposition. Derby was reluctant to seek to unseat the government, fearing a repetition of the Who? Who? Ministry and knowing that shared dislike of Disraeli was part of what had formed the governing coalition. Disraeli, on the other hand, was anxious to return to office. In the interim, Disraeli, as Conservative leader in the Commons, opposed the government on all major measures.\nIn June 1853 Disraeli was awarded an honorary degree by the University of Oxford. He had been recommended for it by Lord Derby, the university's Chancellor. The start of the Crimean War in 1854 caused a lull in party politics; Disraeli spoke patriotically in support. The British military efforts were marked by bungling, and in 1855 a restive Parliament considered a resolution to establish a committee on the conduct of the war. The Aberdeen government made this a motion of confidence; Disraeli led the Opposition to defeat the government, 305 to 148. Aberdeen resigned, and the Queen sent for Derby, who to Disraeli's frustration refused to take office. Palmerston was deemed essential to any Whig ministry, and he would not join any he did not head. The Queen reluctantly asked Palmerston to form a government. Under Palmerston, the war went better, and was ended by the Treaty of Paris in early 1856. Disraeli was early to call for peace but had little influence on events.\nWhen a rebellion broke out in India in 1857, Disraeli took a keen interest, having been a member of a select committee in 1852 which considered how best to rule the subcontinent, and had proposed eliminating the governing role of the British East India Company. After peace was restored, and Palmerston in early 1858 brought in legislation for direct rule of India by the Crown, Disraeli opposed it. Many Conservative MPs refused to follow him, and the bill passed the Commons easily.\nPalmerston's grip on the premiership was weakened by his response to the Orsini affair, in which an attempt was made to assassinate the French Emperor Napoleon III by an Italian revolutionary with a bomb made in Birmingham. At the request of the French ambassador, Palmerston proposed amending the conspiracy to murder statute to make creating an infernal device a felony. He was defeated by 19 votes on the second reading, with many Liberals crossing the aisle against him. He immediately resigned, and Lord Derby returned to office.\nSecond Derby government.\nDerby took office at the head of a purely \"Conservative\" administration, not in coalition. He again offered a place to Gladstone, who declined. Disraeli was once more leader of the House of Commons and returned to the Exchequer. As in 1852, Derby led a minority government, dependent on the division of its opponents for survival. As Leader of the House, Disraeli resumed his regular reports to Queen Victoria, who had requested that he include what she \"could not meet in newspapers\".\nDuring its brief life of just over a year, the Derby government proved moderately progressive. The Government of India Act 1858 ended the role of the East India Company in governing the subcontinent. The Thames Purification Bill funded the construction of much larger sewers for London. Disraeli had supported efforts to allow Jews to sit in Parliament with a bill passed through the Commons allowing each house of Parliament to determine what oaths its members should take. This was grudgingly agreed to by the House of Lords, with a minority of Conservatives joining with the Opposition to pass it. In 1858, Baron Lionel de Rothschild became the first MP to profess the Jewish faith.\nFaced with a vacancy, Disraeli and Derby tried yet again to bring Gladstone, still nominally a Conservative MP, into the government, hoping to strengthen it. Disraeli wrote a personal letter to Gladstone, asking him to place the good of the party above personal animosity: \"Every man performs his office, and there is a Power, greater than ourselves, that disposes of all this.\" In response, Gladstone denied that personal feelings played any role in his decisions then and previously whether to accept office, while acknowledging that there were differences between him and Derby \"broader than you may have supposed\".\nThe Tories pursued a Reform Bill in 1859, which would have resulted in a modest increase to the franchise. The Liberals were healing the breaches between those who favoured Russell and the Palmerston loyalists, and in late March 1859, the government was defeated on a Russell-sponsored amendment. Derby dissolved Parliament, and the ensuing general election resulted in modest Tory gains, but not enough to control the Commons. When Parliament assembled, Derby's government was defeated by 13 votes on an amendment to the Address from the Throne. He resigned, and the Queen reluctantly sent for Palmerston again.\nOpposition and third term as Chancellor.\nAfter Derby's second ejection from office, Disraeli faced dissension within Conservative ranks from those who blamed him for the defeat, or who felt he was disloyal to Derby\u2014the former prime minister warned Disraeli of some MPs seeking his removal from the front bench. Among the conspirators were Lord Robert Cecil, a Conservative MP who would a quarter century later become prime minister as Lord Salisbury; he wrote that having Disraeli as leader in the Commons decreased the Conservatives' chance of holding office. When Cecil's father objected, Lord Robert stated, \"I have merely put into print what all the country gentlemen were saying in private.\"\nDisraeli led a toothless Opposition in the Commons\u2014seeing no way of unseating Palmerston, Derby privately agreed not to seek the government's defeat. Disraeli kept himself informed on foreign affairs, and on what was going on in cabinet, thanks to a source within it. When the American Civil War began in 1861, Disraeli said little publicly, but like most Englishmen expected the South to win. Less reticent were Palmerston, Gladstone, and Russell, whose statements in support of the South contributed to years of hard feelings in the United States. In 1862, Disraeli met Prussian Count Otto von Bismarck and said of him, \"be careful about that man, he means what he says\".\nThe party truce ended in 1864, with Tories outraged over Palmerston's handling of the territorial dispute between the German Confederation and Denmark known as the Schleswig-Holstein Question. Disraeli had little help from Derby, who was ill, but he united the party enough on a no-confidence vote to limit the government to a majority of 18\u2014Tory defections and absentees kept Palmerston in office. Despite rumours about Palmerston's health as he turned 80, he remained personally popular, and the Liberals increased their margin in the July 1865 general election. In the wake of the poor election results, Derby predicted to Disraeli that neither of them would ever hold office again.\nPolitical plans were thrown into disarray by Palmerston's death on 18 October 1865. Russell became prime minister again, with Gladstone clearly the Liberal Party's leader-in-waiting, and as Leader of the House Disraeli's direct opponent. One of Russell's early priorities was a Reform Bill, but the proposed legislation that Gladstone announced on 12 March 1866 divided his party. The Conservatives and the dissident Liberals repeatedly attacked Gladstone's bill, and in June finally defeated the government; Russell resigned on 26 June. The dissidents were unwilling to serve under Disraeli in the House of Commons, and Derby formed a third Conservative minority government, with Disraeli again as Chancellor.\nTory Democrat: the 1867 Reform Act.\nIt was Disraeli's belief that if given the vote British people would use it instinctively to put their natural and traditional rulers, the gentlemen of the Conservative Party, into power. Responding to renewed agitation for popular suffrage, Disraeli persuaded a majority of the cabinet to agree to a Reform bill. With what Derby cautioned was \"a leap in the dark\", Disraeli had outflanked the Liberals who, as the supposed champions of Reform, dared not oppose him. In the absence of a credible party rival and for fear of having an election called on the issue, Conservatives felt obliged to support Disraeli despite their misgivings.\nThere were Tory dissenters, most notably Lord Cranborne (as Robert Cecil was by then known) who resigned from the government and spoke against the bill, accusing Disraeli of \"a political betrayal which has no parallel in our Parliamentary annals\". Even as Disraeli accepted Liberal amendments (although pointedly refusing those moved by Gladstone) that further lowered the property qualification, Cranborne was unable to lead an effective rebellion. Disraeli gained wide acclaim and became a hero to his party for the \"marvellous parliamentary skill\" with which he secured the passage of Reform in the Commons.\nFrom the Liberal benches too there was admiration. MP for Nottingham Bernal Ostborne declared:\nThe Reform Act 1867 passed that August. It extended the franchise by 938,427 men\u2014an increase of 88%\u2014by giving the vote to male householders and male lodgers paying at least \u00a310 for rooms. It eliminated rotten boroughs with fewer than 10,000 inhabitants, and granted constituencies to 15 unrepresented towns, with extra representation to large municipalities such as Liverpool and Manchester.\nPrime Minister (1868).\nFirst term.\nDerby had long had attacks of gout which left him bedbound, unable to deal with politics. As the new session of Parliament approached in February 1868, he was unable to leave his home but was reluctant to resign, as at 68 he was much younger than either Palmerston or Russell at the end of their premierships. Derby knew that his \"attacks of illness would, at no distant period, incapacitate me from the discharge of my public duties\"; doctors had warned him that his health required his resignation. In late February, with Parliament in session and Derby absent, he wrote to Disraeli asking for confirmation that \"you will not shrink from the additional heavy responsibility\". Reassured, he wrote to the Queen, resigning and recommending Disraeli as \"only he could command the cordial support, en masse, of his present colleagues\". Disraeli went to Osborne House on the Isle of Wight, where the Queen asked him to form a government. The monarch wrote to her daughter, Prussian Crown Princess Victoria, \"Mr. Disraeli is Prime Minister! A proud thing for a man 'risen from the people' to have obtained!\" The new prime minister told those who came to congratulate him, \"I have climbed to the top of the greasy pole.\"\nFirst government, February\u2013December 1868.\nThe Conservatives remained a minority in the House of Commons and the passage of the Reform Bill required the calling of a new election once the new voting register had been compiled. Disraeli's term as prime minister, which began in February 1868, would therefore be short unless the Conservatives won the general election. He made only two major changes in the cabinet: he replaced Lord Chelmsford as Lord Chancellor with Lord Cairns and brought in George Ward Hunt as Chancellor of the Exchequer. Derby had intended to replace Chelmsford once a vacancy in a suitable sinecure developed. Disraeli was unwilling to wait, and Cairns, in his view, was a far stronger minister.\nDisraeli's first premiership was dominated by the heated debate over the Church of Ireland. Although Ireland was largely Roman Catholic, the Church of England represented most landowners. It remained the established church and was funded by direct taxation, which was greatly resented by the Catholics and Presbyterians. An initial attempt by Disraeli to negotiate with Archbishop Manning the establishment of a Catholic university in Dublin foundered in March when Gladstone moved resolutions to disestablish the Irish Church altogether. The proposal united the Liberals under Gladstone's leadership, while causing divisions among the Conservatives.\nThe Conservatives remained in office because the new electoral register was not yet ready; neither party wished a poll under the old roll. Gladstone began using the Liberal majority in the Commons to push through resolutions and legislation. Disraeli's government survived until the December general election, at which the Liberals were returned to power with a majority.\nIn its short life, the first Disraeli government passed noncontroversial laws. It ended public executions, and the Corrupt Practices Act did much to end electoral bribery. It authorised an early version of nationalisation, having the Post Office buy up the telegraph companies. Amendments to the school law, the Scottish legal system, and the railway laws were passed. In addition, the Public Health (Scotland) Act instituted sanitary inspectors and medical officers. According to one study, \"better sanitation was enforced throughout Scotland.\" Disraeli sent the successful expedition against Tewodros II of Ethiopia under Sir Robert Napier.\nOpposition leader; 1874 election.\nGiven Gladstone's majority in the Commons, Disraeli could do little but protest as the government advanced legislation; he chose to await Liberal mistakes. He used this leisure time to write a new novel, \"Lothair\" (1870). A work of fiction by a former prime minister was a novelty for Britain, and the book became a bestseller.\nBy 1872 there was dissent in the Conservative ranks over the failure to challenge Gladstone. This was quieted as Disraeli took steps to assert his leadership, and as divisions among the Liberals became clear. Public support for Disraeli was shown by cheering at a thanksgiving service in 1872 on the recovery of the Prince of Wales from illness, while Gladstone was met with silence. Disraeli had supported the efforts of party manager John Eldon Gorst to put the administration of the Conservative Party on a modern basis. On Gorst's advice, Disraeli gave a speech to a mass meeting in Manchester that year. To roaring approval, he compared the Liberal front bench to \"a range of exhausted volcanoes... But the situation is still dangerous. There are occasional earthquakes and ever and again the dark rumbling of the sea.\" Gladstone, Disraeli stated, dominated the scene and \"alternated between a menace and a sigh\".\nAt his first departure from 10 Downing Street in 1868, Disraeli had Victoria make his wife Mary Anne Viscountess Beaconsfield in her own right in lieu of a peerage for himself. Through 1872 the eighty-year-old peeress had stomach cancer. She died on 15 December. Urged by a clergyman to turn her thoughts to Jesus Christ in her final days, she said she could not: \"You know Dizzy is my J.C.\"\nIn 1873, Gladstone brought forward legislation to establish a Catholic university in Dublin. This divided the Liberals, and on 12 March an alliance of Conservatives and Irish Catholics defeated the government by three votes. Gladstone resigned, and the Queen sent for Disraeli, who refused to take office. Without a general election, a Conservative government would be another minority; Disraeli wanted the power a majority would bring and felt he could gain it later by leaving the Liberals in office now. Gladstone's government struggled on, beset by scandal and unimproved by a reshuffle. As part of that change, Gladstone took on the office of Chancellor, leading to questions as to whether he had to stand for re-election on taking on a second ministry\u2014until the 1920s, MPs becoming ministers had to seek re-election.\nIn January 1874, Gladstone called a general election, convinced that if he waited longer, he would do worse at the polls. Balloting was spread over two weeks, beginning on 1 February. As the constituencies voted, it became clear that the result would be a Conservative majority, the first since 1841. In Scotland, where the Conservatives were perennially weak, they increased from seven seats to nineteen. Overall, they won 350 seats to 245 for the Liberals and 57 for the Irish Home Rule League. Disraeli became prime minister for the second time.\nPrime Minister (1874\u20131880).\nSecond term.\nDisraeli's cabinet of twelve, with six peers and six commoners, was the smallest since Reform. Of the peers, five of them had been in Disraeli's 1868 cabinet; the sixth, Lord Salisbury, was reconciled to Disraeli after negotiation and became Secretary of State for India. Lord Stanley (who had succeeded his father, the former prime minister, as Earl of Derby) became Foreign Secretary and Sir Stafford Northcote the Chancellor.\nIn August 1876, Disraeli was elevated to the House of Lords as Earl of Beaconsfield and Viscount Hughenden. The Queen had offered to ennoble him as early as 1868; he had then declined. She did so again in 1874, when he fell ill at Balmoral, but he was reluctant to leave the Commons for a house in which he had no experience. Continued ill health during his second premiership caused him to contemplate resignation, but his lieutenant, Derby, was unwilling, feeling that he could not manage the Queen. For Disraeli, the Lords, where the debate was less intense, was the alternative to resignation. Five days before the end of the 1876 session of Parliament, on 11 August, Disraeli was seen to linger and look around the chamber before departing. Newspapers reported his ennoblement the following morning.\nIn addition to the viscounty bestowed on Mary Anne Disraeli, the earldom of Beaconsfield was to have been bestowed on Edmund Burke in 1797, but he had died before receiving it. The name Beaconsfield, a town near Hughenden, was given to a minor character in \"Vivian Grey\". Disraeli made various statements about his elevation, writing to Selina, Lady Bradford on 8 August 1876, \"I am quite tired of that place [the Commons]\" but when asked by a friend how he liked the Lords, replied, \"I am dead; dead but in the Elysian fields.\"\nDomestic policy.\nLegislation.\nUnder the stewardship of Richard Assheton Cross, the Home Secretary, Disraeli's new government enacted many reforms, including the Artisans' and Labourers' Dwellings Improvement Act 1875 (38 &amp; 39 Vict. c. 36), which made inexpensive loans available to towns and cities to construct working-class housing. Also enacted were the Public Health Act 1875 (38 &amp; 39 Vict. c. 55), modernising sanitary codes, the Sale of Food and Drugs Act 1875 (38 &amp; 39 Vict. c. 63), and the Elementary Education Act 1876 (39 &amp; 40 Vict. c. 70). Disraeli's government introduced a new Factory Act meant to protect workers, the Conspiracy, and Protection of Property Act 1875 (38 &amp; 39 Vict. c. 86), which allowed peaceful picketing, and the Employers and Workmen Act 1875 (38 &amp; 39 Vict. c. 90) to enable workers to sue employers in the civil courts if they broke legal contracts.\nThe Sale of Food and Drugs Act 1875 prohibited the mixing of injurious ingredients with articles of food or with drugs, and provision was made for the appointment of analysts; all tea \"had to be examined by a customs official on importation, and when in the opinion of the analyst it was unfit for food, the tea had to be destroyed\". The Employers and Workmen Act 1875, according to one study, \"finally placed employers and employed on an equal footing before the law\". The Conspiracy, and Protection of Property Act 1875 established the right to strike by providing that \"an agreement or combination by one or more persons to do, or procure to be done, any act in contemplation or furtherance of a trade dispute between employers and workmen, shall not be indictable as a conspiracy if such act committed by one person would not be punishable as a crime\".\nAs a result of these social reforms the Liberal-Labour MP Alexander Macdonald told his constituents in 1879, \"The Conservative party have done more for the working classes in five years than the Liberals have in fifty.\"\nCivil Service.\nGladstone in 1870 had sponsored an Order in Council, introducing competitive examination into the Civil Service, diminishing the political aspects of government hiring. Disraeli did not agree, and while he did not seek to reverse the order, his actions often frustrated its intent. For example, Disraeli made political appointments to positions previously given to career civil servants. He was backed by his party, hungry for office and its emoluments after almost thirty years with only brief spells in government. Disraeli gave positions to hard-up Conservative leaders, even\u2014to Gladstone's outrage\u2014creating one office at \u00a32,000 per year. Nevertheless, Disraeli made fewer peers (only 22, including one of Victoria's sons) than had Gladstone (37 during his just over five years in office).\nAs he had in government posts, Disraeli rewarded old friends with clerical positions, making Sydney Turner, son of a good friend of Isaac D'Israeli, Dean of Ripon. He favoured Low church clergymen in promotion, disliking other movements in Anglicanism for political reasons. In this, he came into disagreement with the Queen, who out of loyalty to her late husband Albert preferred Broad church teachings. One controversial appointment had occurred shortly before the 1868 election. When the position of Archbishop of Canterbury fell vacant, Disraeli reluctantly agreed to the Queen's preferred candidate, Archibald Tait, the Bishop of London. To fill Tait's vacant see, Disraeli was urged by many people to appoint Samuel Wilberforce, the former Bishop of Winchester. Disraeli disliked Wilberforce and instead appointed John Jackson, the Bishop of Lincoln. Blake suggested that, on balance, these appointments cost Disraeli more votes than they gained him.\nForeign policy.\nDisraeli always considered foreign affairs to be the most critical and interesting part of statesmanship. Nevertheless, his biographer Robert Blake doubts that his subject had specific ideas about foreign policy when he took office in 1874. He had rarely travelled abroad; since his youthful tour of the Middle East in 1830\u20131831, he had left Britain only for his honeymoon and three visits to Paris, the last of which was in 1856. As he had criticised Gladstone for a do-nothing foreign policy, he most probably contemplated what actions would reassert Britain's place in Europe. His brief first premiership, and the first year of his second, gave him little opportunity to make his mark in foreign affairs.\nSuez.\nThe Suez Canal, opened in 1869, cut weeks and thousands of miles off the sea journey between Britain and India; in 1875, approximately 80% of the ships using the canal were British. In the event of another rebellion in India or a Russian invasion, the time saved at Suez might be crucial. Built by French interests, 56% of the stocks in the canal remained in their hands, while 44% of the stock belonged to Isma'il Pasha, the Khedive of Egypt. He was notorious for his profligate spending. The canal was losing money, and an attempt by Ferdinand de Lesseps, builder of the canal, to raise the tolls had fallen through when the Khedive had threatened military force to prevent it, and had also attracted Disraeli's attention. The Khedive governed Egypt under the Ottoman Empire; as in the Crimea, the issue of the Canal raised the Eastern Question of what to do about the decaying empire governed from Constantinople. With much of the pre-canal trade and communications between Britain and India passing through the Ottoman Empire, Britain had done its best to prop up the Ottomans against the threat that Russia would take Constantinople, cutting those communications, and giving Russian ships unfettered access to the Mediterranean. The French might also threaten those lines. Britain had had the opportunity to purchase shares in the canal but had declined to do so.\nDisraeli sent the Liberal MP Nathan Rothschild to Paris to enquire about buying de Lesseps's shares. On 14 November 1875, the editor of the \"Pall Mall Gazette\", Frederick Greenwood, learnt from London banker Henry Oppenheim that the Khedive was seeking to sell his shares in the Suez Canal Company to a French firm. Greenwood quickly told Lord Derby, the Foreign Secretary, who notified Disraeli. The Prime Minister moved immediately to secure the shares. On 23 November, the Khedive offered to sell the shares for 100,000,000 francs. Rather than seek the aid of the Bank of England, Disraeli borrowed funds from Lionel de Rothschild, who took a commission on the deal. The banker's capital was at risk as Parliament could have refused to ratify the transaction. The contract for purchase was signed at Cairo on 25 November and the shares deposited at the British consulate the following day.\nDisraeli told the Queen, \"it is settled; you have it, madam!\" The public saw the venture as a daring statement of British dominance of the seas. Sir Ian Malcolm described the Suez Canal share purchase as \"the greatest romance of Mr. Disraeli's romantic career\". In the following decades, the security of the Suez Canal became a major concern of British foreign policy. Under Gladstone, Britain took control of Egypt in 1882. A later Foreign Secretary, Lord Curzon, described the canal in 1909 as \"the determining influence of every considerable movement of British power to the east and south of the Mediterranean\".\nRoyal Titles Act.\nAlthough initially curious about Disraeli when he entered Parliament in 1837, Victoria came to detest him over his treatment of Peel. Over time, her dislike softened, especially as Disraeli took pains to cultivate her. He told Matthew Arnold, \"Everybody likes flattery; and, when you come to royalty, you should lay it on with a trowel\". Disraeli's biographer, Adam Kirsch, suggests that Disraeli's obsequious treatment of his queen was part flattery, part belief that this was how a queen should be addressed by a loyal subject, and part awe that a middle-class man of Jewish birth should be the companion of a monarch. By the time of his second premiership, Disraeli had built a strong relationship with Victoria, probably closer to her than any of her prime ministers except her first, Lord Melbourne. When Disraeli returned as prime minister in 1874 and went to kiss hands, he did so literally, on one knee; according to Richard Aldous in his book on the rivalry between Disraeli and Gladstone, \"Victoria and Disraeli would exploit their closeness for mutual advantage.\"\nVictoria had long wished to have an imperial title, reflecting Britain's expanding domain. She was irked when Tsar Alexander II held a higher rank than her as an emperor, and was appalled that her daughter, the Prussian Crown Princess, would outrank her when her husband came to the throne. She also saw an imperial title as proclaiming Britain's increased stature in the world. The title \"Empress of India\" had been used informally for some time and she wished to have that title formally bestowed on her. The Queen prevailed upon Disraeli to introduce a Royal Titles Bill, and also told of her intent to open Parliament in person, which during this time she did only when she wanted something from legislators. Disraeli was cautious in response, as careful soundings of MPs brought a negative reaction, and he declined to place such a proposal in the Queen's Speech.\nOnce the desired bill was finally prepared, Disraeli's handling of it was not adept. He neglected to notify either the Prince of Wales or the Opposition and was met by irritation from the prince and a full-scale attack from the Liberals. An old enemy of Disraeli, former Liberal Chancellor Robert Lowe, alleged during the debate in the Commons that two previous prime ministers had refused to introduce such legislation for the Queen. Gladstone immediately stated that he was not one of them, and the Queen gave Disraeli leave to quote her saying she had never approached a prime minister with such a proposal. According to Blake, Disraeli \"in a brilliant oration of withering invective proceeded to destroy Lowe\", who apologised and never held office again. Disraeli said of Lowe that he was the only person in London with whom he would not shake hands: \"he is in the mud and there I leave him.\"\nFearful of losing, Disraeli was reluctant to bring the bill to a vote in the Commons, but when he did it passed with a majority of 75. Once the bill was formally enacted, Victoria began signing her letters \"Victoria R &amp; I\" (, Queen and Empress). According to Aldous, the bill \"shattered Disraeli's authority in the House of Commons\".\nBalkans and Bulgaria.\nIn July 1875 Serb populations in Bosnia and Herzegovina, then provinces of the Ottoman Empire, revolted against the Turks, alleging religious persecution and poor administration. The following January, Sultan Abd\u00fclaziz agreed to reforms proposed by Hungarian statesman Julius Andr\u00e1ssy, but the rebels, suspecting they might win their freedom, continued their uprising, joined by militants in Serbia and Bulgaria. The Turks suppressed the Bulgarian uprising harshly, and when reports of these actions escaped, Disraeli and Derby stated in Parliament that they did not believe them. Disraeli called them \"coffee-house babble\" and dismissed allegations of torture by the Ottomans since \"Oriental people usually terminate their connections with culprits in a more expeditious fashion\".\nGladstone, who had left the Liberal leadership and retired from public life, was appalled by reports of atrocities in Bulgaria, and in August 1876, penned a hastily written pamphlet arguing that the Turks should be deprived of Bulgaria because of what they had done there. He sent a copy to Disraeli, who called it \"vindictive and ill-written\u00a0... of all the Bulgarian horrors perhaps the greatest\". Gladstone's pamphlet became an immense best-seller and rallied the Liberals to urge that the Ottoman Empire should no longer be a British ally. Disraeli wrote to Lord Salisbury on 3 September, \"Had it not been for these unhappy 'atrocities', we should have settled a peace very honourable to England and satisfactory to Europe. Now we are obliged to work from a new point of departure, and dictate to Turkey, who has forfeited all sympathy.\" In spite of this, Disraeli's policy favoured Constantinople and Ottoman territorial integrity.\nDisraeli and the cabinet sent Salisbury as lead British representative to the Constantinople Conference, which met in December 1876 and January 1877. In advance of the conference, Disraeli sent Salisbury private word to seek British military occupation of Bulgaria and Bosnia, and British control of the Ottoman Army. Salisbury ignored these instructions, which his biographer, Andrew Roberts deemed \"ludicrous\". The conference failed to reach agreement with the Turks.\nParliament opened in February 1877, with Disraeli now in the Lords as Earl of Beaconsfield. He spoke only once there in the 1877 session on the Eastern Question, stating on 20 February that there was a need for stability in the Balkans, and that forcing Turkey into territorial concessions would not secure it. The Prime Minister wanted a deal with the Ottomans whereby Britain would temporarily occupy strategic areas to deter the Russians from war, to be returned on the signing of a peace treaty, but found little support in his cabinet, which favoured partition of the Ottoman Empire. As Disraeli, by then in poor health, continued to battle within the cabinet, Russia invaded Turkey on 21 April, beginning the Russo-Turkish War.\nCongress of Berlin.\nThe Russians pushed through Ottoman territory and by December 1877 had captured the strategic Bulgarian town of Plevna. The war divided the British, but the Russian success caused some to forget the atrocities and call for intervention on the Turkish side. Others hoped for further Russian successes. The fall of Plevna was a major story for weeks, and Disraeli's warnings that Russia was a threat to British interests in the eastern Mediterranean were deemed prophetic. The jingoistic attitude of many Britons increased Disraeli's political support, and the Queen showed her favour by visiting him at Hughenden\u2014the first time she had visited the country home of her prime minister since the Melbourne ministry. At the end of January 1878, the Ottoman Sultan appealed to Britain to save Constantinople. Amid war fever in Britain, the government asked Parliament to vote \u00a36,000,000 to prepare the Army and Navy for war. Gladstone opposed the measure, but less than half his party voted with him. Popular opinion was with Disraeli, though some thought him too soft for not immediately declaring war on Russia.\nWith the Russians close to Constantinople, the Turks yielded and in March 1878, signed the Treaty of San Stefano, conceding a Bulgarian state covering a large part of the Balkans. It would be initially Russian-occupied and many feared that it would give them a client state close to Constantinople. Other Ottoman possessions in Europe would become independent; additional territory was to be ceded directly to Russia. This was unacceptable to the British, who protested, hoping to get the Russians to agree to attend an international conference which German Chancellor Bismarck proposed to hold at Berlin. The cabinet discussed Disraeli's proposal to position Indian troops at Malta for possible transit to the Balkans and call out reserves. Derby resigned in protest, and Disraeli appointed Salisbury as Foreign Secretary. Amid British preparations for war, the Russians and Turks agreed to discussions at Berlin.\nIn advance of the meeting, confidential negotiations took place between Britain and Russia in April and May 1878. The Russians were willing to make changes to the big Bulgaria, but were determined to retain their new possessions, Bessarabia in Europe and Batum and Kars on the east coast of the Black Sea. To counterbalance this, Britain required a possession in the Eastern Mediterranean where it might base ships and troops and negotiated with the Ottomans for the cession of Cyprus. Once this was secretly agreed, Disraeli was prepared to allow Russia's territorial gains.\nThe Congress of Berlin was held in June and July 1878, the central relationship in it that between Disraeli and Bismarck. In later years, the German chancellor would show visitors to his office three pictures on the wall: \"the portrait of my Sovereign, there on the right that of my wife, and on the left, there, that of Lord Beaconsfield\". Disraeli caused an uproar in the congress by making his opening address in English, rather than in French, hitherto accepted as the international language of diplomacy. By one account, the British ambassador in Berlin, Lord Odo Russell, hoping to spare the delegates Disraeli's awful French accent, told Disraeli that the congress was hoping to hear a speech in English by one of its masters.\nDisraeli left much of the detailed work to Salisbury, concentrating his efforts on making it as difficult as possible for the broken-up big Bulgaria to reunite. Disraeli intended that Batum be demilitarised, but the Russians obtained their preferred language, and in 1886, fortified the town. Nevertheless, the Cyprus Convention ceding the island to Britain was announced during the congress, and again made Disraeli a sensation.\nDisraeli gained agreement that Turkey should retain enough of its European possessions to safeguard the Dardanelles. By one account, when met with Russian intransigence, Disraeli told his secretary to order a special train to return them home to begin the war. Czar Alexander II later described the congress as \"a European coalition against Russia, under Bismarck\".\nThe Treaty of Berlin was signed on 13 July 1878 at the Radziwill Palace in Berlin. Disraeli and Salisbury returned home to heroes' receptions. At the door of 10 Downing Street, Disraeli received flowers sent by the Queen. There, he told the gathered crowd, \"Lord Salisbury and I have brought you back peace\u2014but a peace I hope with honour.\" The Queen offered him a dukedom, which he declined, though accepting the Garter, as long as Salisbury also received it. In Berlin, word spread of Bismarck's admiring description of Disraeli, \"\"Der alte Jude, das ist der Mann!\"\u00a0\"\nIn the weeks after Berlin, Disraeli and the cabinet considered calling a general election to capitalise on the public applause he and Salisbury had received. Parliaments were then for a seven-year term, and it was the custom not to go to the country until the sixth year unless forced to by events. Only four and a half years had passed and they did not see any clouds on the horizon that might forecast Conservative defeat if they waited. This decision not to seek re-election has often been cited as a great mistake by Disraeli. Blake, however, pointed out that results in local elections had been moving against the Conservatives, and doubted if Disraeli missed any great opportunity by waiting.\nAfghanistan to Zululand.\nAs successful invasions of India generally came through Afghanistan, the British had observed and sometimes intervened there since the 1830s, hoping to keep the Russians out. In 1878 the Russians sent a mission to Kabul; it was not rejected by the Afghans, as the British had hoped. The British proposed to send their own mission, insisting that the Russians be sent away. The Viceroy of India Lord Lytton concealed his plans to issue this ultimatum from Disraeli, and when the Prime Minister insisted he take no action, went ahead anyway. When the Afghans made no answer, Lord Cranbrook as Secretary of State for War, ordered the advance against them in the Second Anglo-Afghan War. Under Lord Roberts, the British easily defeated them and installed a new ruler, leaving a mission and garrison in Kabul.\nBritish policy in South Africa was to encourage federation between the British-run Cape Colony and Natal, and the Boer republics, the Transvaal (annexed by Britain in 1877) and the Orange Free State. The governor of Cape Colony, Sir Bartle Frere, believing that the federation could not be accomplished until the native tribes acknowledged British rule, made demands on the Zulu and their king, Cetewayo, which they were certain to reject. As Zulu troops could not marry until they had washed their spears in blood, they were eager for combat. Frere did not send word to the cabinet of what he had done until the ultimatum was about to expire. Disraeli and the cabinet reluctantly backed him, and in early January 1879 resolved to send reinforcements. Before they could arrive, on 22 January, a Zulu \"impi\" (army), moving with great speed and endurance, destroyed a British encampment in South Africa in the Battle of Isandlwana. Over a thousand British and colonial troops were killed. Word of the defeat did not reach London until 12 February. Disraeli wrote the next day, \"the terrible disaster has shaken me to the centre\". He reprimanded Frere, but left him in charge, attracting fire from all sides. Disraeli sent General Sir Garnet Wolseley as High Commissioner and Commander in Chief, and Cetewayo and the Zulus were crushed at the Battle of Ulundi on 4 July 1879.\nOn 8 September 1879 Sir Louis Cavagnari, in charge of the mission in Kabul, was killed with his entire staff by rebelling Afghan soldiers. Roberts undertook a successful punitive expedition against the Afghans over the next six weeks.\n1880 election.\nIn December 1878, Gladstone was offered the Liberal nomination for Edinburghshire, a constituency popularly known as Midlothian. The small Scottish electorate was dominated by two noblemen, the Conservative Duke of Buccleuch and the Liberal Earl of Rosebery. The Earl, a friend of both Disraeli and Gladstone who would succeed the latter after his final term as prime minister, had journeyed to the United States to view politics there, and was convinced that aspects of American electioneering techniques could be translated to Britain. On his advice, Gladstone accepted the offer in January 1879, and later that year began his Midlothian campaign, speaking not only in Edinburgh, but across Britain, attacking Disraeli, to huge crowds.\nConservative chances of re-election were damaged by the poor weather, and consequent effects on agriculture. Four consecutive wet summers through 1879 had led to poor harvests. In the past, the farmer had the consolation of higher prices at such times, but with bumper crops cheaply transported from the United States, grain prices remained low. Other European nations, faced with similar circumstances, opted for protection, and Disraeli was urged to reinstitute the Corn Laws. He declined, stating that he regarded the matter as settled. Protection would have been highly unpopular among the newly enfranchised urban working classes, as it would raise their cost of living. Amid an economic slump generally, the Conservatives lost support among farmers.\nDisraeli's health continued to fail through 1879. Owing to his infirmities, Disraeli was 45 minutes late for the Lord Mayor's Dinner at the Guildhall in November, at which it is customary that the Prime Minister speaks. Though many commented on how healthy he looked, it took him great effort to appear so, and when he told the audience he expected to speak to the dinner again the following year, attendees chuckled. Gladstone was then in the midst of his campaign. Despite his public confidence, Disraeli recognised that the Conservatives would probably lose the next election and was already contemplating his Resignation Honours.\nDespite this pessimism, Conservatives hopes were buoyed in early 1880 with successes in by-elections the Liberals had expected to win, concluding with victory in Southwark, normally a Liberal stronghold. The cabinet had resolved to wait before dissolving Parliament; in early March they reconsidered, agreeing to go to the country as soon as possible. Parliament was dissolved on 24 March; the first borough constituencies began voting a week later.\nDisraeli took no public part in the electioneering, it being deemed improper for peers to make speeches to influence Commons elections. This meant that the chief Conservatives\u2014Disraeli, Salisbury, and India Secretary Lord Cranbrook\u2014would not be heard from. The election was thought likely to be close. Once returns began to be announced, it became clear that the Conservatives were decisively beaten. The final result gave the Liberals an absolute majority of about 50.\nFinal months, death, and memorials.\nDisraeli refused to cast blame for the defeat, which he understood was likely to be final for him. He wrote to Lady Bradford that it was just as much work to end a government as to form one, without any of the fun. Queen Victoria was bitter at his departure. Among the honours he arranged before resigning as Prime Minister on 21 April 1880 was one for his private secretary, Montagu Corry, who became Baron Rowton.\nReturning to Hughenden, Disraeli brooded over his electoral dismissal, but also resumed work on \"Endymion\", which he had begun in 1872 and laid aside before the 1874 election. The work was rapidly completed and published by November 1880. He carried on a correspondence with Victoria, with letters passed through intermediaries. When Parliament met in January 1881, he served as Conservative leader in the Lords, attempting to serve as a moderating influence on Gladstone's legislation.\nBecause of his asthma and gout, Disraeli went out as little as possible, fearing more serious episodes of illness. In March, he fell ill with bronchitis, and emerged from bed only for a meeting with Salisbury and other Conservative leaders on the 26th. As it became clear that this might be his final sickness, friends and opponents alike came to call. Disraeli declined a visit from the Queen, saying, \"She would only ask me to take a message to Albert.\" Almost blind, when he received the last letter from Victoria of which he was aware on 5 April, he held it momentarily, then had it read to him by Lord Barrington, a Privy Councillor. One card, signed \"A Workman\", delighted its recipient: \"Don't die yet, we can't do without you.\"\nDespite the gravity of Disraeli's condition, the doctors concocted optimistic bulletins for public consumption. Prime Minister Gladstone called several times to enquire about his rival's condition, and wrote in his diary, \"May the Almighty be near his pillow.\" There was intense public interest in Disraeli's struggles for life. Disraeli had customarily taken the sacrament at Easter; when this day was observed on 17 April, there was discussion among his friends and family if he should be given the opportunity, but those against, fearing that he would lose hope, prevailed. On the morning of the following day, Easter Monday, he became incoherent, then comatose. Disraeli's last confirmed words before dying at his home at 19 Curzon Street in the early morning of 19 April were \"I had rather live but I am not afraid to die\". The anniversary of Disraeli's death was for some years commemorated in the United Kingdom as Primrose Day.\nDespite having been offered a state funeral by Queen Victoria, Disraeli's executors decided against a public procession and funeral, fearing that too large crowds would gather to do him honour. The chief mourners at the service at Hughenden on 26 April were his brother Ralph and nephew Coningsby, to whom Hughenden would eventually pass; Gathorne Gathorne-Hardy, Viscount Cranbrook, despite most of Disraeli's former cabinet being present, was notably absent in Italy. Queen Victoria was prostrated with grief, and considered ennobling Ralph or Coningsby as a memorial to Disraeli (without children, his titles became extinct with his death), but decided against it on the ground that their means were too small for a peerage. Protocol forbade her attending Disraeli's funeral (this would not be changed until 1965, when Elizabeth II attended the rites for the former prime minister Sir Winston Churchill) but she sent primroses (\"his favourite flowers\") to the funeral and visited the burial vault to place a wreath four days later.\nDisraeli is buried with his wife in a vault beneath the Church of St Michael and All Angels which stands in the grounds of his home, Hughenden Manor. There is also a memorial to him in the chancel in the church, erected in his honour by Queen Victoria. His literary executor was his private secretary, Lord Rowton. The Disraeli vault also contains the body of Sarah Brydges Willyams, the wife of James Brydges Willyams of St Mawgan. Disraeli carried on a long correspondence with Mrs. Willyams, writing frankly about political affairs. At her death in 1865, she left him a large legacy, which helped clear his debts. His will was proved in April 1882 at \u00a384,019 18 s. 7 d. (roughly equivalent to \u00a3 in ).\nDisraeli has a memorial in Westminster Abbey, erected by the nation on the motion of Gladstone in his memorial speech on Disraeli in the House of Commons. Gladstone had absented himself from the funeral, with his plea of the press of public business met with public mockery. His speech was widely anticipated, if only because his dislike for Disraeli was well known. In the event, the speech was a model of its kind, in which he avoided comment on Disraeli's politics while praising his personal qualities.\nLegacy.\nDisraeli's literary and political career interacted over his lifetime and fascinated Victorian Britain, making him \"one of the most eminent figures in Victorian public life\", and occasioned a large output of commentary. Critic Shane Leslie noted three decades after his death that \"Disraeli's career was a romance such as no Eastern vizier or Western plutocrat could tell. He began as a pioneer in dress and an aesthete of words\u00a0... Disraeli actually made his novels come true.\"\nLiterary.\nDisraeli's novels are his main literary achievement. They have from the outset divided critical opinion. The writer R. W. Stewart observed that there have always been two criteria for judging Disraeli's novels\u2014political and artistic. The critic Robert O'Kell, concurring, writes, \"It is after all, even if you are a Tory of the staunchest blue, impossible to make Disraeli into a first-rate novelist. And it is equally impossible, no matter how much you deplore the extravagances and improprieties of his works, to make him into an insignificant one.\"\nDisraeli's early \"silver fork\" novels \"Vivian Grey\" (1826) and \"The Young Duke\" (1831) featured romanticised depictions of aristocratic life (despite his ignorance of it) with character sketches of well-known public figures lightly disguised. In some of his early fiction Disraeli also portrayed himself and what he felt to be his Byronic dual nature: the poet and the man of action. His most autobiographical novel was \"Contarini Fleming\" (1832), an avowedly serious work that did not sell well. The critic William Kuhn suggests that Disraeli's fiction can be read as \"the memoirs he never wrote\", revealing the inner life of a politician for whom the norms of Victorian public life appeared to represent a social straitjacket\u2014particularly with regard to what Kuhn sees as the author's \"ambiguous sexuality\".\nOf the other novels of the early 1830s, \"Alroy\" is described by Blake as \"profitable but unreadable\", and \"The Rise of Iskander\" (1833), \"The Infernal Marriage\" and \"Ixion in Heaven\" (1834) made little impact. \"Henrietta Temple\" (1837) was Disraeli's next major success. It draws on the events of his affair with Henrietta Sykes to tell the story of a debt-ridden young man torn between a mercenary loveless marriage and a passionate love at first sight for the eponymous heroine. \"Venetia\" (1837) was a minor work, written to raise much-needed cash.\nIn the 1840s Disraeli wrote a trilogy of novels with political themes. \"Coningsby\" attacks the evils of the Whig Reform Bill of 1832 and castigates the leaderless conservatives for not responding. \"Sybil; or, The Two Nations\" (1845) reveals Peel's betrayal over the Corn Laws. These themes are expanded in \"Tancred\" (1847). With \"Coningsby; or, The New Generation\" (1844), Disraeli, in Blake's view, \"infused the novel genre with political sensibility, espousing the belief that England's future as a world power depended not on the complacent old guard, but on youthful, idealistic politicians.\" \"Sybil; or, The Two Nations\" was less idealistic than \"Coningsby\"; the \"two nations\" of its sub-title referred to the huge economic and social gap between the privileged few and the deprived working classes. The last was \"Tancred; or, The New Crusade\" (1847), promoting the Church of England's role in reviving Britain's flagging spirituality. Disraeli often wrote about religion, for he was a strong promoter of the Church of England. He was troubled by the growth of elaborate rituals in the late 19th century, such as the use of incense and vestments, and heard warnings to the effect that the ritualists were going to turn control of the Church of England over to the Pope. He consequently was a strong supporter of the Public Worship Regulation Act 1874 which allowed the archbishops to go to court to stop the ritualists.\n\"Lothair\" was \"Disraeli's ideological \"Pilgrim's Progress\"\", It tells a story of political life with particular regard to the roles of the Anglican and Roman Catholic churches. It reflected anti-Catholicism of the sort that was popular in Britain, and which fueled support for Italian unification (\"Risorgimento\"). \"Endymion\", despite having a Whig as hero, is a last exposition of the author's economic policies and political beliefs. Disraeli continued to the last to pillory his enemies in barely disguised caricatures: the character St Barbe in \"Endymion\" is widely seen as a parody of Thackeray, who had offended Disraeli more than thirty years earlier by lampooning him in \"Punch\" as \"Codlingsby\". Disraeli left an unfinished novel in which the priggish central character, Falconet, is unmistakably a caricature of Gladstone.\nBlake commented that Disraeli \"produced an epic poem, unbelievably bad, and a five-act blank verse tragedy, if possible worse. Further he wrote a discourse on political theory and a political biography, the \"Life of Lord George Bentinck\", which is excellent\u00a0... remarkably fair and accurate.\"\nPolitical.\nIn the years after Disraeli's death, as Salisbury began his reign of more than twenty years over the Conservatives, the party emphasised the late leader's \"One Nation\" views, that the Conservatives at root shared the beliefs of the working classes, with the Liberals the party of the urban \u00e9lite. The memory of Disraeli was used by the Conservatives to appeal to the working classes, with whom he was said to have had a rapport. This aspect of his policies has been re-evaluated by historians in the 20th and 21st centuries. In 1972 B. H. Abbott stressed that it was not Disraeli but Lord Randolph Churchill who invented the term \"Tory democracy\", though it was Disraeli who made it an essential part of Conservative policy and philosophy. In 2007 Parry wrote, \"The tory democrat myth did not survive detailed scrutiny by professional historical writing of the 1960s [which] demonstrated that Disraeli had very little interest in a programme of social legislation and was very flexible in handling parliamentary reform in 1867.\" Despite this, Parry sees Disraeli, rather than Peel, as the founder of the modern Conservative party. The Conservative politician and writer Douglas Hurd wrote in 2013, \"[Disraeli] was not a one-nation Conservative\u2014and this was not simply because he never used the phrase. He rejected the concept in its entirety.\"\nDisraeli's enthusiastic propagation of the British Empire has also been seen as appealing to working-class voters. Before his leadership of the Conservative Party, imperialism was the province of the Liberals, most notably Palmerston. Disraeli made the Conservatives the party that most loudly supported both the Empire and military action to assert its primacy. This came about in part because Disraeli's own views stemmed that way, in part because he saw advantage for the Conservatives, and partially in reaction against Gladstone, who disliked the expense of empire. Blake argued that Disraeli's imperialism \"decisively orientated the Conservative party for many years to come, and the tradition which he started was probably a bigger electoral asset in winning working-class support during the last quarter of the century than anything else\". Some historians have commented on a romantic impulse behind Disraeli's approach to Empire and foreign affairs: Abbott writes, \"To the mystical Tory concepts of Throne, Church, Aristocracy and People, Disraeli added Empire.\" Others have identified a strongly pragmatic aspect to his policies. Gladstone's biographer Philip Magnus contrasted Disraeli's grasp of foreign affairs with that of Gladstone, who \"never understood that high moral principles, in their application to foreign policy, are more often destructive of political stability than motives of national self-interest.\" In Parry's view, Disraeli's foreign policy \"can be seen as a gigantic castle in the air (as it was by Gladstone), or as an overdue attempt to force the British commercial classes to awaken to the realities of European politics.\"\nDuring his lifetime Disraeli's opponents, and sometimes even his friends and allies, questioned whether he sincerely held the views he propounded, or whether they were adopted by him as politically essential and lacked conviction. Lord John Manners, in 1843 at the time of Young England, wrote, \"could I only satisfy myself that D'Israeli believed all that he said, I should be more happy: his historical views are quite mine, but does he believe them?\" Paul Smith, in his journal article on Disraeli's politics, argues that Disraeli's ideas were coherently argued over a political career of nearly half a century, and \"it is impossible to sweep them aside as a mere bag of burglar's tools for effecting felonious entry to the British political pantheon.\"\nStanley Weintraub, in his biography of Disraeli, points out that his subject did much to advance Britain towards the 20th century, carrying one of the two great Reform Acts of the 19th despite the opposition of his Liberal rival, Gladstone. He helped preserve constitutional monarchy by drawing the Queen out of mourning into a new symbolic national role and created the climate for what became 'Tory democracy'. He articulated an imperial role for Britain that would last into World War II and brought an intermittently self-isolated Britain into the concert of Europe.\nFrances Walsh comments on Disraeli's multifaceted public life:\nHistorian Llewellyn Woodward has evaluated Disraeli:\nHistorical writers have often played Disraeli and Gladstone against each other as great rivals. Roland Quinault, however, cautions not to exaggerate the confrontation:\nRole of Jewishness.\nBy 1882, 46,000 Jews lived in England and, by 1890, Jewish emancipation was complete. Since 1858, Parliament has never been without practising Jewish members. The first Jewish Lord Mayor of London, Sir David Salomons, was elected in 1855, followed by the 1858 emancipation of the Jews. On 26 July 1858, Lionel de Rothschild was allowed to sit in the House of Commons when the hitherto specifically Christian oath of office was changed. Disraeli, a baptised Christian of Jewish parentage, was already an MP, as the mandated oath of office presented no barrier to him. In 1884 Nathan Mayer Rothschild, 1st Baron Rothschild became the first Jewish member of the British House of Lords; Disraeli was already a member.\nDisraeli as a leader of the Conservative Party, with its ties to the landed aristocracy, used his Jewish ancestry to claim an aristocratic heritage of his own. His biographer Jonathan Parry argues:\nTodd Endelman points out that, \"The link between Jews and old clothes was so fixed in the popular imagination that Victorian political cartoonists regularly drew Benjamin Disraeli as an old clothes man in order to stress his Jewishness.\" He adds, \"Before the 1990s...few biographers of Disraeli or historians of Victorian politics acknowledged the prominence of the antisemitism that accompanied his climb up the greasy pole or its role in shaping his own singular sense of Jewishness.\"\nAccording to Michael Ragussis: \nPopular culture.\nHistorian Michael Diamond asserts that for British music hall patrons in the 1880s and 1890s, \"xenophobia and pride in empire\" were reflected in the halls' most popular political heroes: all were Conservatives and Disraeli stood out above all, even decades after his death, while Gladstone was used as a villain. Film historian Roy Armes has argued that historical films helped maintain the political status quo in Britain in the 1920s and 1930s by imposing an establishment viewpoint that emphasized the greatness of monarchy, empire, and tradition. The films created \"a facsimile world where existing values were invariably validated by events in the film and where all discord could be turned into harmony by an acceptance of the status quo.\"\nSteven Fielding has argued that Disraeli was an especially popular film hero: \"historical dramas favoured Disraeli over Gladstone and, more substantively, promulgated an essentially deferential view of democratic leadership.\" Stage and screen actor George Arliss was known for his portrayals of Disraeli, winning the Academy Award for Best Actor for 1929's \"Disraeli\". Fielding says Arliss \"personified the kind of paternalistic, kindly, homely statesmanship that appealed to a significant proportion of the cinema audience\u00a0... Even workers attending Labour party meetings deferred to leaders with an elevated social background who showed they cared.\"\nJohn Gielgud portrayed Disraeli in 1941, in Thorold Dickinson's morale-boosting film \"The Prime Minister\", which followed the politician from age 30 to 70.\nAlec Guinness portrayed him in \"The Mudlark\" (1950).\nIan McShane starred in the four-part 1978 ATV miniseries \"\", written by David Butler. Presented in the U.S. on PBS's \"Masterpiece Theatre\" in 1980, it was nominated for the Emmy Award for Outstanding Limited Series.\nRichard Pasco played Disraeli in the ITV series \"Number 10\" in 1983.\nIn the 1997 film \"Mrs Brown\", Disraeli was played by Antony Sher.\nNotes and references.\nNotes\nReferences"}
{"id": "3876", "revid": "41987223", "url": "https://en.wikipedia.org/wiki?curid=3876", "title": "Binomial distribution", "text": "&lt;/math&gt;\n | kurtosis = formula_1\n | entropy = formula_2 in shannons. For nats, use the natural log in the log.\n | mgf = formula_3\n | char = formula_4\n | pgf = formula_5\n | fisher = formula_6(for fixed formula_7)\nIn probability theory and statistics, the binomial distribution with parameters and is the discrete probability distribution of the number of successes in a sequence of independent experiments, each asking a yes\u2013no question, and each with its own Boolean-valued outcome: \"success\" (with probability ) or \"failure\" (with probability ). A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment, and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., , the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the binomial test of statistical significance.\nThe binomial distribution is frequently used to model the number of successes in a sample of size drawn with replacement from a population of size . If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for much larger than , the binomial distribution remains a good approximation, and is widely used.\nDefinitions.\nProbability mass function.\nIf the random variable follows the binomial distribution with parameters and , we write . The probability of getting exactly successes in independent Bernoulli trials (with the same rate ) is given by the probability mass function:\nfor , where\nis the binomial coefficient. The formula can be understood as follows: is the probability of obtaining the sequence of independent Bernoulli trials in which trials are \"successes\" and the remaining trials result in \"failure\". Since the trials are independent with probabilities remaining constant between them, any sequence of trials with successes (and failures) has the same probability of being achieved (regardless of positions of successes within the sequence). There are formula_10 such sequences, since the binomial coefficient formula_10 counts the number of ways to choose the positions of the successes among the trials. The binomial distribution is concerned with the probability of obtaining \"any\" of these sequences, meaning the probability of obtaining one of them () must be added formula_10 times, hence formula_13.\nIn creating reference tables for binomial distribution probability, usually, the table is filled in up to values. This is because for , the probability can be calculated by its complement as\nLooking at the expression as a function of , there is a value that maximizes it. This value can be found by calculating\nand comparing it to 1. There is always an integer that satisfies\n is monotone increasing for and monotone decreasing for , with the exception of the case where is an integer. In this case, there are two values for which is maximal: and . is the \"most probable\" outcome (that is, the most likely, although this can still be unlikely overall) of the Bernoulli trials and is called the mode.\nEquivalently, . Taking the floor function, we obtain .\nExample.\nSuppose a biased coin comes up heads with probability 0.3 when tossed. The probability of seeing exactly 4 heads in 6 tosses is\nCumulative distribution function.\nThe cumulative distribution function can be expressed as:\nwhere formula_19 is the \"floor\" under , i.e. the greatest integer less than or equal to .\nIt can also be represented in terms of the regularized incomplete beta function, as follows:\nwhich is equivalent to the cumulative distribution functions of the beta distribution and of the -distribution:\nSome closed-form bounds for the cumulative distribution function are given below.\nProperties.\nExpected value and variance.\nIf , that is, is a binomially distributed random variable, being the total number of experiments and \"p\" the probability of each experiment yielding a successful result, then the expected value of is:\nThis follows from the linearity of the expected value along with the fact that is the sum of identical Bernoulli random variables, each with expected value . In other words, if formula_24 are identical (and independent) Bernoulli random variables with parameter , then and\nThe variance is:\nThis similarly follows from the fact that the variance of a sum of independent random variables is the sum of the variances.\nHigher moments.\nThe first 6 central moments, defined as formula_27, are given by \nThe non-central moments satisfy\nand in general\nwhere formula_31 is the formula_32th falling power of formula_7.\nA simple bound\n follows by bounding the Binomial moments via the higher Poisson moments: \nThis shows that if formula_35, then formula_36 is at most a constant factor away from formula_37\nMode.\nUsually the mode of a binomial distribution is equal to formula_38, where formula_39 is the floor function. However, when is an integer and is neither 0 nor 1, then the distribution has two modes: and . When is equal to 0 or 1, the mode will be 0 and correspondingly. These cases can be summarized as follows:\nProof: Let\nFor formula_42 only formula_43 has a nonzero value with formula_44. For formula_45 we find formula_46 and formula_47 for formula_48. This proves that the mode is 0 for formula_42 and formula_7 for formula_45.\nLet formula_52. We find\nFrom this follows\nSo when formula_55 is an integer, then formula_55 and formula_57 is a mode. In the case that formula_58, then only formula_59 is a mode.\nMedian.\nIn general, there is no single formula to find the median for a binomial distribution, and it may even be non-unique. However, several special results have been established:\nwhich implies the simpler but looser bound\nFor and for even , it is possible to make the denominator constant:\nStatistical inference.\nEstimation of parameters.\nWhen is known, the parameter can be estimated using the proportion of successes:\nThis estimator is found using maximum likelihood estimator and also the method of moments. This estimator is unbiased and uniformly with minimum variance, proven using Lehmann\u2013Scheff\u00e9 theorem, since it is based on a minimal sufficient and complete statistic (i.e.: ). It is also consistent both in probability and in MSE. This statistic is asymptotically normal thanks to the central limit theorem, because it is the same as taking the mean over Bernoulli samples. It has a variance of formula_65, a property which is used in various ways, such as in Wald's confidence intervals.\nA closed form Bayes estimator for also exists when using the Beta distribution as a conjugate prior distribution. When using a general formula_66 as a prior, the posterior mean estimator is:\nThe Bayes estimator is asymptotically efficient and as the sample size approaches infinity (), it approaches the MLE solution. The Bayes estimator is biased (how much depends on the priors), admissible and consistent in probability. Using the Bayesian estimator with the Beta distribution can be used with Thompson sampling.\nFor the special case of using the standard uniform distribution as a non-informative prior, formula_68, the posterior mean estimator becomes:\n(A posterior mode should just lead to the standard estimator.) This method is called the rule of succession, which was introduced in the 18th century by Pierre-Simon Laplace.\nWhen relying on Jeffreys prior, the prior is formula_70, which leads to the estimator:\nWhen estimating with very rare events and a small (e.g.: if ), then using the standard estimator leads to formula_72 which sometimes is unrealistic and undesirable. In such cases there are various alternative estimators. One way is to use the Bayes estimator formula_73, leading to:\nAnother method is to use the upper bound of the confidence interval obtained using the rule of three:\nConfidence intervals for the parameter p.\nEven for quite large values of \"n\", the actual distribution of the mean is significantly nonnormal. Because of this problem several methods to estimate confidence intervals have been proposed.\nIn the equations for confidence intervals below, the variables have the following meaning:\nWald method.\nA continuity correction of may be added.\nAgresti\u2013Coull method.\nHere the estimate of is modified to\nThis method works well for and . See here for formula_86. For use the Wilson (score) method below.\nWilson (score) method.\nThe notation in the formula below differs from the previous formulas in two respects:\nComparison.\nThe so-called \"exact\" (Clopper\u2013Pearson) method is the most conservative. (\"Exact\" does not mean perfectly accurate; rather, it indicates that the estimates will not be less conservative than the true value.)\nThe Wald method, although commonly recommended in textbooks, is the most biased.\nRelated distributions.\nSums of binomials.\nIf and are independent binomial variables with the same probability , then is again a binomial variable; its distribution is :\nA Binomial distributed random variable can be considered as the sum of Bernoulli distributed random variables. So the sum of two Binomial distributed random variables and is equivalent to the sum of Bernoulli distributed random variables, which means . This can also be proven directly using the addition rule.\nHowever, if and do not have the same probability , then the variance of the sum will be smaller than the variance of a binomial variable distributed as .\nPoisson binomial distribution.\nThe binomial distribution is a special case of the Poisson binomial distribution, which is the distribution of a sum of independent non-identical Bernoulli trials .\nRatio of two binomial distributions.\nThis result was first derived by Katz and coauthors in 1978.\nLet and be independent. Let .\nThen log(\"T\") is approximately normally distributed with mean log(\"p\"1/\"p\"2) and variance .\nConditional binomials.\nIf \"X\"\u00a0~\u00a0B(\"n\",\u00a0\"p\") and \"Y\"\u00a0|\u00a0\"X\"\u00a0~\u00a0B(\"X\",\u00a0\"q\") (the conditional distribution of \"Y\", given\u00a0\"X\"), then \"Y\" is a simple binomial random variable with distribution \"Y\"\u00a0~\u00a0B(\"n\",\u00a0\"pq\").\nFor example, imagine throwing \"n\" balls to a basket \"UX\" and taking the balls that hit and throwing them to another basket \"UY\". If \"p\" is the probability to hit \"UX\" then \"X\"\u00a0~\u00a0B(\"n\",\u00a0\"p\") is the number of balls that hit \"UX\". If \"q\" is the probability to hit \"UY\" then the number of balls that hit \"UY\" is \"Y\"\u00a0~\u00a0B(\"X\",\u00a0\"q\") and therefore \"Y\"\u00a0~\u00a0B(\"n\",\u00a0\"pq\").\nSince formula_95 and formula_96, by the law of total probability,\nSince formula_98 the equation above can be expressed as\nFactoring formula_100 and pulling all the terms that don't depend on formula_101 out of the sum now yields\nAfter substituting formula_103 in the expression above, we get\nNotice that the sum (in the parentheses) above equals formula_105 by the binomial theorem. Substituting this in finally yields\nand thus formula_107 as desired.\nBernoulli distribution.\nThe Bernoulli distribution is a special case of the binomial distribution, where . Symbolically, has the same meaning as . Conversely, any binomial distribution, , is the distribution of the sum of independent Bernoulli trials, , each with the same probability .\nNormal approximation.\nIf is large enough, then the skew of the distribution is not too great. In this case a reasonable approximation to is given by the normal distribution\nand this basic approximation can be improved in a simple way by using a suitable continuity correction.\nThe basic approximation generally improves as increases (at least 20) and is better when is not near to 0 or 1. Various rules of thumb may be used to decide whether is large enough, and is far enough from the extremes of zero or one:\nThis can be made precise using the Berry\u2013Esseen theorem.\nThe rule formula_112 is totally equivalent to request that\nMoving terms around yields:\nSince formula_115 and formula_116, to obtain the desired conditions:\nNotice that these conditions automatically imply that formula_118. On the other hand, apply again the square root and divide by 3,\nSubtracting the second set of inequalities from the first one yields:\nand so, the desired first rule is satisfied,\nAssume that both values formula_122 and formula_123 are greater than\u00a09. Since formula_124, we easily have that \nWe only have to divide now by the respective factors formula_126 and formula_127, to deduce the alternative form of the 3-standard-deviation rule:\nThe following is an example of applying a continuity correction. Suppose one wishes to calculate for a binomial random variable . If has a distribution given by the normal approximation, then is approximated by . The addition of 0.5 is the continuity correction; the uncorrected normal approximation gives considerably less accurate results.\nThis approximation, known as de Moivre\u2013Laplace theorem, is a huge time-saver when undertaking calculations by hand (exact calculations with large are very onerous); historically, it was the first use of the normal distribution, introduced in Abraham de Moivre's book \"The Doctrine of Chances\" in 1738. Nowadays, it can be seen as a consequence of the central limit theorem since is a sum of independent, identically distributed Bernoulli variables with parameter\u00a0. This fact is the basis of a hypothesis test, a \"proportion z-test\", for the value of using , the sample proportion and estimator of , in a common test statistic.\nFor example, suppose one randomly samples people out of a large population and ask them whether they agree with a certain statement. The proportion of people who agree will of course depend on the sample. If groups of \"n\" people were sampled repeatedly and truly randomly, the proportions would follow an approximate normal distribution with mean equal to the true proportion \"p\" of agreement in the population and with standard deviation\nPoisson approximation.\nThe binomial distribution converges towards the Poisson distribution as the number of trials goes to infinity while the product converges to a finite limit. Therefore, the Poisson distribution with parameter can be used as an approximation to of the binomial distribution if is sufficiently large and is sufficiently small. According to rules of thumb, this approximation is good if and such that , or if and such that , or if and .\nConcerning the accuracy of Poisson approximation, see Novak, ch. 4, and references therein.\nBeta distribution.\nThe binomial distribution and beta distribution are different views of the same model of repeated Bernoulli trials. The binomial distribution is the PMF of successes given independent events each with a probability of success. \nMathematically, when and , the beta distribution and the binomial distribution are related by a factor of :\nBeta distributions also provide a family of prior probability distributions for binomial distributions in Bayesian inference: \nGiven a uniform prior, the posterior distribution for the probability of success given independent events with observed successes is a beta distribution.\nComputational methods.\nRandom number generation.\nMethods for random number generation where the marginal distribution is a binomial distribution are well-established.\nOne way to generate random variates samples from a binomial distribution is to use an inversion algorithm. To do so, one must calculate the probability that for all values from through . (These probabilities should sum to a value close to one, in order to encompass the entire sample space.) Then by using a pseudorandom number generator to generate samples uniformly between 0 and 1, one can transform the calculated samples into discrete numbers by using the probabilities calculated in the first step.\nHistory.\nThis distribution was derived by Jacob Bernoulli. He considered the case where where is the probability of success and and are positive integers. Blaise Pascal had earlier considered the case where , tabulating the corresponding binomial coefficients in what is now recognized as Pascal's triangle."}
{"id": "3878", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=3878", "title": "Biostatistics", "text": "Biostatistics (also known as biometry) is a branch of statistics that applies statistical methods to a wide range of topics in biology. It encompasses the design of biological experiments, the collection and analysis of data from those experiments and the interpretation of the results.\nHistory.\nBiostatistics and genetics.\nBiostatistical modeling forms an important part of numerous modern biological theories. Genetics studies, since its beginning, used statistical concepts to understand observed experimental results. Some genetics scientists even contributed with statistical advances with the development of methods and tools. Gregor Mendel started the genetics studies investigating genetics segregation patterns in families of peas and used statistics to explain the collected data. In the early 1900s, after the rediscovery of Mendel's Mendelian inheritance work, there were gaps in understanding between genetics and evolutionary Darwinism. Francis Galton tried to expand Mendel's discoveries with human data and proposed a different model with fractions of the heredity coming from each ancestral composing an infinite series. He called this the theory of \"Law of Ancestral Heredity\". His ideas were strongly disagreed by William Bateson, who followed Mendel's conclusions, that genetic inheritance were exclusively from the parents, half from each of them. This led to a vigorous debate between the biometricians, who supported Galton's ideas, as Raphael Weldon, Arthur Dukinfield Darbishire and Karl Pearson, and Mendelians, who supported Bateson's (and Mendel's) ideas, such as Charles Davenport and Wilhelm Johannsen. Later, biometricians could not reproduce Galton conclusions in different experiments, and Mendel's ideas prevailed. By the 1930s, models built on statistical reasoning had helped to resolve these differences and to produce the neo-Darwinian modern evolutionary synthesis.\nSolving these differences also allowed to define the concept of population genetics and brought together genetics and evolution. The three leading figures in the establishment of population genetics and this synthesis all relied on statistics and developed its use in biology.\nThese and other biostatisticians, mathematical biologists, and statistically inclined geneticists helped bring together evolutionary biology and genetics into a consistent, coherent whole that could begin to be quantitatively modeled.\nIn parallel to this overall development, the pioneering work of D'Arcy Thompson in \"On Growth and Form\" also helped to add quantitative discipline to biological study.\nDespite the fundamental importance and frequent necessity of statistical reasoning, there may nonetheless have been a tendency among biologists to distrust or deprecate results which are not qualitatively apparent. One anecdote describes Thomas Hunt Morgan banning the Friden calculator from his department at Caltech, saying \"Well, I am like a guy who is prospecting for gold along the banks of the Sacramento River in 1849. With a little intelligence, I can reach down and pick up big nuggets of gold. And as long as I can do that, I'm not going to let any people in my department waste scarce resources in placer mining.\"\nResearch planning.\nAny research in life sciences is proposed to answer a scientific question we might have. To answer this question with a high certainty, we need accurate results. The correct definition of the main hypothesis and the research plan will reduce errors while taking a decision in understanding a phenomenon. The research plan might include the research question, the hypothesis to be tested, the experimental design, data collection methods, data analysis perspectives and costs involved. It is essential to carry the study based on the three basic principles of experimental statistics: randomization, replication, and local control.\nResearch question.\nThe research question will define the objective of a study. The research will be headed by the question, so it needs to be concise, at the same time it is focused on interesting and novel topics that may improve science and knowledge and that field. To define the way to ask the scientific question, an exhaustive literature review might be necessary. So the research can be useful to add value to the scientific community.\nHypothesis definition.\nOnce the aim of the study is defined, the possible answers to the research question can be proposed, transforming this question into a hypothesis. The main propose is called null hypothesis (H0) and is usually based on a permanent knowledge about the topic or an obvious occurrence of the phenomena, sustained by a deep literature review. We can say it is the standard expected answer for the data under the situation in test. In general, HO assumes no association between treatments. On the other hand, the alternative hypothesis is the denial of HO. It assumes some degree of association between the treatment and the outcome. Although, the hypothesis is sustained by question research and its expected and unexpected answers.\nAs an example, consider groups of similar animals (mice, for example) under two different diet systems. The research question would be: what is the best diet? In this case, H0 would be that there is no difference between the two diets in mice metabolism (H0: \u03bc1 = \u03bc2) and the alternative hypothesis would be that the diets have different effects over animals metabolism (H1: \u03bc1 \u2260 \u03bc2).\nThe hypothesis is defined by the researcher, according to his/her interests in answering the main question. Besides that, the alternative hypothesis can be more than one hypothesis. It can assume not only differences across observed parameters, but their degree of differences (\"i.e.\" higher or shorter).\nSampling.\nUsually, a study aims to understand an effect of a phenomenon over a population. In biology, a population is defined as all the individuals of a given species, in a specific area at a given time. In biostatistics, this concept is extended to a variety of collections possible of study. Although, in biostatistics, a population is not only the individuals, but the total of one specific component of their organisms, as the whole genome, or all the sperm cells, for animals, or the total leaf area, for a plant, for example.\nIt is not possible to take the measures from all the elements of a population. Because of that, the sampling process is very important for statistical inference. Sampling is defined as to randomly get a representative part of the entire population, to make posterior inferences about the population. So, the sample might catch the most variability across a population. The sample size is determined by several things, since the scope of the research to the resources available. In clinical research, the trial type, as inferiority, equivalence, and superiority is a key in determining sample size.\nExperimental design.\nExperimental designs sustain those basic principles of experimental statistics. There are three basic experimental designs to randomly allocate treatments in all plots of the experiment. They are completely randomized design, randomized block design, and factorial designs. Treatments can be arranged in many ways inside the experiment. In agriculture, the correct experimental design is the root of a good study and the arrangement of treatments within the study is essential because environment largely affects the plots (plants, livestock, microorganisms). These main arrangements can be found in the literature under the names of \"lattices\", \"incomplete blocks\", \"split plot\", \"augmented blocks\", and many others. All of the designs might include control plots, determined by the researcher, to provide an error estimation during inference.\nIn clinical studies, the samples are usually smaller than in other biological studies, and in most cases, the environment effect can be controlled or measured. It is common to use randomized controlled clinical trials, where results are usually compared with observational study designs such as case\u2013control or cohort.\nData collection.\nData collection methods must be considered in research planning, because it highly influences the sample size and experimental design.\nData collection varies according to type of data. For qualitative data, collection can be done with structured questionnaires or by observation, considering presence or intensity of disease, using score criterion to categorize levels of occurrence. For quantitative data, collection is done by measuring numerical information using instruments.\nIn agriculture and biology studies, yield data and its components can be obtained by metric measures. However, pest and disease injuries in plats are obtained by observation, considering score scales for levels of damage. Especially, in genetic studies, modern methods for data collection in field and laboratory should be considered, as high-throughput platforms for phenotyping and genotyping. These tools allow bigger experiments, while turn possible evaluate many plots in lower time than a human-based only method for data collection.\nFinally, all data collected of interest must be stored in an organized data frame for further analysis.\nAnalysis and data interpretation.\nDescriptive tools.\nData can be represented through tables or graphical representation, such as line charts, bar charts, histograms, scatter plot. Also, measures of central tendency and variability can be very useful to describe an overview of the data. Follow some examples:\nFrequency tables.\nOne type of table is the frequency table, which consists of data arranged in rows and columns, where the frequency is the number of occurrences or repetitions of data. Frequency can be:\nAbsolute: represents the number of times that a determined value appear;\nformula_1\nRelative: obtained by the division of the absolute frequency by the total number;\nformula_2\nIn the next example, we have the number of genes in ten operons of the same organism.\nLine graph.\nLine graphs represent the variation of a value over another metric, such as time. In general, values are represented in the vertical axis, while the time variation is represented in the horizontal axis.\nBar chart.\nA bar chart is a graph that shows categorical data as bars presenting heights (vertical bar) or widths (horizontal bar) proportional to represent values. Bar charts provide an image that could also be represented in a tabular format.\nIn the bar chart example, we have the birth rate in Brazil for the December months from 2010 to 2016. The sharp fall in December 2016 reflects the outbreak of Zika virus in the birth rate in Brazil.\nHistograms.\nThe histogram (or frequency distribution) is a graphical representation of a dataset tabulated and divided into uniform or non-uniform classes. It was first introduced by Karl Pearson.\nScatter plot.\nA scatter plot is a mathematical diagram that uses Cartesian coordinates to display values of a dataset. A scatter plot shows the data as a set of points, each one presenting the value of one variable determining the position on the horizontal axis and another variable on the vertical axis. They are also called scatter graph, scatter chart, scattergram, or scatter diagram.\nMean.\nThe arithmetic mean is the sum of a collection of values (formula_3) divided by the number of items of this collection (formula_4).\nMedian.\nThe median is the value in the middle of a dataset.\nMode.\nThe mode is the value of a set of data that appears most often.\nBox plot.\nBox plot is a method for graphically depicting groups of numerical data. The maximum and minimum values are represented by the lines, and the interquartile range (IQR) represent 25\u201375% of the data. Outliers may be plotted as circles.\nCorrelation coefficients.\nAlthough correlations between two different kinds of data could be inferred by graphs, such as scatter plot, it is necessary validate this though numerical information. For this reason, correlation coefficients are required. They provide a numerical value that reflects the strength of an association.\nPearson correlation coefficient.\n Pearson correlation coefficient is a measure of association between two variables, X and Y. This coefficient, usually represented by \"\u03c1\" (rho) for the population and \"r\" for the sample, assumes values between \u22121 and 1, where \"\u03c1\" = 1 represents a perfect positive correlation, \"\u03c1\" = \u22121 represents a perfect negative correlation, and \"\u03c1\" = 0 is no linear correlation.\nInferential statistics.\nIt is used to make inferences about an unknown population, by estimation and/or hypothesis testing. In other words, it is desirable to obtain parameters to describe the population of interest, but since the data is limited, it is necessary to make use of a representative sample in order to estimate them. With that, it is possible to test previously defined hypotheses and apply the conclusions to the entire population. The standard error of the mean is a measure of variability that is crucial to do inferences.\nHypothesis testing is essential to make inferences about populations aiming to answer research questions, as settled in \"Research planning\" section. Authors defined four steps to be set:\nA confidence interval is a range of values that can contain the true real parameter value in given a certain level of confidence. The first step is to estimate the best-unbiased estimate of the population parameter. The upper value of the interval is obtained by the sum of this estimate with the multiplication between the standard error of the mean and the confidence level. The calculation of lower value is similar, but instead of a sum, a subtraction must be applied.\nStatistical considerations.\nPower and statistical error.\nWhen testing a hypothesis, there are two types of statistic errors possible: Type I error and Type II error. \nThe significance level denoted by \u03b1 is the type I error rate and should be chosen before performing the test. The type II error rate is denoted by \u03b2 and statistical power of the test is 1 \u2212 \u03b2.\np-value.\nThe p-value is the probability of obtaining results as extreme as or more extreme than those observed, assuming the null hypothesis (H0) is true. It is also called the calculated probability. It is common to confuse the p-value with the significance level (\u03b1), but, the \u03b1 is a predefined threshold for calling significant results. If p is less than \u03b1, the null hypothesis (H0) is rejected.\nMultiple testing.\nIn multiple tests of the same hypothesis, the probability of the occurrence of falses positives (familywise error rate) increase and some strategy are used to control this occurrence. This is commonly achieved by using a more stringent threshold to reject null hypotheses. The Bonferroni correction defines an acceptable global significance level, denoted by \u03b1* and each test is individually compared with a value of \u03b1 = \u03b1*/m. This ensures that the familywise error rate in all m tests, is less than or equal to \u03b1*. When m is large, the Bonferroni correction may be overly conservative. An alternative to the Bonferroni correction is to control the false discovery rate (FDR). The FDR controls the expected proportion of the rejected null hypotheses (the so-called discoveries) that are false (incorrect rejections). This procedure ensures that, for independent tests, the false discovery rate is at most q*. Thus, the FDR is less conservative than the Bonferroni correction and have more power, at the cost of more false positives.\nMis-specification and robustness checks.\nThe main hypothesis being tested (e.g., no association between treatments and outcomes) is often accompanied by other technical assumptions (e.g., about the form of the probability distribution of the outcomes) that are also part of the null hypothesis. When the technical assumptions are violated in practice, then the null may be frequently rejected even if the main hypothesis is true. Such rejections are said to be due to model mis-specification. Verifying whether the outcome of a statistical test does not change when the technical assumptions are slightly altered (so-called robustness checks) is the main way of combating mis-specification.\nModel selection criteria.\nModel criteria selection will select or model that more approximate true model. The Akaike's Information Criterion (AIC) and The Bayesian Information Criterion (BIC) are examples of asymptotically efficient criteria.\nDevelopments and big data.\nRecent developments have made a large impact on biostatistics. Two important changes have been the ability to collect data on a high-throughput scale, and the ability to perform much more complex analysis using computational techniques. This comes from the development in areas as sequencing technologies, Bioinformatics and Machine learning (Machine learning in bioinformatics).\nUse in high-throughput data.\nNew biomedical technologies like microarrays, next-generation sequencers (for genomics) and mass spectrometry (for proteomics) generate enormous amounts of data, allowing many tests to be performed simultaneously. Careful analysis with biostatistical methods is required to separate the signal from the noise. For example, a microarray could be used to measure many thousands of genes simultaneously, determining which of them have different expression in diseased cells compared to normal cells. However, only a fraction of genes will be differentially expressed.\nMulticollinearity often occurs in high-throughput biostatistical settings. Due to high intercorrelation between the predictors (such as gene expression levels), the information of one predictor might be contained in another one. It could be that only 5% of the predictors are responsible for 90% of the variability of the response. In such a case, one could apply the biostatistical technique of dimension reduction (for example via principal component analysis). Classical statistical techniques like linear or logistic regression and linear discriminant analysis do not work well for high dimensional data (i.e. when the number of observations n is smaller than the number of features or predictors p: n &lt; p). As a matter of fact, one can get quite high R2-values despite very low predictive power of the statistical model. These classical statistical techniques (esp. least squares linear regression) were developed for low dimensional data (i.e. where the number of observations n is much larger than the number of predictors p: n \u00bb p). In cases of high dimensionality, one should always consider an independent validation test set and the corresponding residual sum of squares (RSS) and R2 of the validation test set, not those of the training set.\nOften, it is useful to pool information from multiple predictors together. For example, Gene Set Enrichment Analysis (GSEA) considers the perturbation of whole (functionally related) gene sets rather than of single genes. These gene sets might be known biochemical pathways or otherwise functionally related genes. The advantage of this approach is that it is more robust: It is more likely that a single gene is found to be falsely perturbed than it is that a whole pathway is falsely perturbed. Furthermore, one can integrate the accumulated knowledge about biochemical pathways (like the JAK-STAT signaling pathway) using this approach.\nBioinformatics advances in databases, data mining, and biological interpretation.\nThe development of biological databases enables storage and management of biological data with the possibility of ensuring access for users around the world. They are useful for researchers depositing data, retrieve information and files (raw or processed) originated from other experiments or indexing scientific articles, as PubMed. Another possibility is search for the desired term (a gene, a protein, a disease, an organism, and so on) and check all results related to this search. There are databases dedicated to SNPs (dbSNP), the knowledge on genes characterization and their pathways (KEGG) and the description of gene function classifying it by cellular component, molecular function and biological process (Gene Ontology). In addition to databases that contain specific molecular information, there are others that are ample in the sense that they store information about an organism or group of organisms. As an example of a database directed towards just one organism, but that contains much data about it, is the \"Arabidopsis thaliana\" genetic and molecular database \u2013 TAIR. Phytozome, in turn, stores the assemblies and annotation files of dozen of plant genomes, also containing visualization and analysis tools. Moreover, there is an interconnection between some databases in the information exchange/sharing and a major initiative was the International Nucleotide Sequence Database Collaboration (INSDC) which relates data from DDBJ, EMBL-EBI, and NCBI.\nNowadays, increase in size and complexity of molecular datasets leads to use of powerful statistical methods provided by computer science algorithms which are developed by machine learning area. Therefore, data mining and machine learning allow detection of patterns in data with a complex structure, as biological ones, by using methods of supervised and unsupervised learning, regression, detection of clusters and association rule mining, among others. To indicate some of them, self-organizing maps and \"k\"-means are examples of cluster algorithms; neural networks implementation and support vector machines models are examples of common machine learning algorithms.\nCollaborative work among molecular biologists, bioinformaticians, statisticians and computer scientists is important to perform an experiment correctly, going from planning, passing through data generation and analysis, and ending with biological interpretation of the results.\nUse of computationally intensive methods.\nOn the other hand, the advent of modern computer technology and relatively cheap computing resources have enabled computer-intensive biostatistical methods like bootstrapping and re-sampling methods.\nIn recent times, random forests have gained popularity as a method for performing statistical classification. Random forest techniques generate a panel of decision trees. Decision trees have the advantage that you can draw them and interpret them (even with a basic understanding of mathematics and statistics). Random Forests have thus been used for clinical decision support systems.\nApplications.\nPublic health.\nPublic health, including epidemiology, health services research, nutrition, environmental health and health care policy &amp; management. In these medicine contents, it's important to consider the design and analysis of the clinical trials. As one example, there is the assessment of severity state of a patient with a prognosis of an outcome of a disease.\nWith new technologies and genetics knowledge, biostatistics are now also used for Systems medicine, which consists in a more personalized medicine. For this, is made an integration of data from different sources, including conventional patient data, clinico-pathological parameters, molecular and genetic data as well as data generated by additional new-omics technologies.\nQuantitative genetics.\nThe study of population genetics and statistical genetics in order to link variation in genotype with a variation in phenotype. In other words, it is desirable to discover the genetic basis of a measurable trait, a quantitative trait, that is under polygenic control. A genome region that is responsible for a continuous trait is called a quantitative trait locus (QTL). The study of QTLs become feasible by using molecular markers and measuring traits in populations, but their mapping needs the obtaining of a population from an experimental crossing, like an F2 or recombinant inbred strains/lines (RILs). To scan for QTLs regions in a genome, a gene map based on linkage have to be built. Some of the best-known QTL mapping algorithms are Interval Mapping, Composite Interval Mapping, and Multiple Interval Mapping.\nHowever, QTL mapping resolution is impaired by the amount of recombination assayed, a problem for species in which it is difficult to obtain large offspring. Furthermore, allele diversity is restricted to individuals originated from contrasting parents, which limit studies of allele diversity when we have a panel of individuals representing a natural population. For this reason, the genome-wide association study was proposed in order to identify QTLs based on linkage disequilibrium, that is the non-random association between traits and molecular markers. It was leveraged by the development of high-throughput SNP genotyping.\nIn animal and plant breeding, the use of markers in selection aiming for breeding, mainly the molecular ones, collaborated to the development of marker-assisted selection. While QTL mapping is limited due resolution, GWAS does not have enough power when rare variants of small effect that are also influenced by environment. So, the concept of Genomic Selection (GS) arises in order to use all molecular markers in the selection and allow the prediction of the performance of candidates in this selection. The proposal is to genotype and phenotype a training population, develop a model that can obtain the genomic estimated breeding values (GEBVs) of individuals belonging to a genotype and but not phenotype population, called testing population. This kind of study could also include a validation population, thinking in the concept of cross-validation, in which the real phenotype results measured in this population are compared with the phenotype results based on the prediction, what used to check the accuracy of the model.\nAs a summary, some points about the application of quantitative genetics are:\nExpression data.\nStudies for differential expression of genes from RNA-Seq data, as for RT-qPCR and microarrays, demands comparison of conditions. The goal is to identify genes which have a significant change in abundance between different conditions. Then, experiments are designed appropriately, with replicates for each condition/treatment, randomization and blocking, when necessary. In RNA-Seq, the quantification of expression uses the information of mapped reads that are summarized in some genetic unit, as exons that are part of a gene sequence. As microarray results can be approximated by a normal distribution, RNA-Seq counts data are better explained by other distributions. The first used distribution was the Poisson one, but it underestimate the sample error, leading to false positives. Currently, biological variation is considered by methods that estimate a dispersion parameter of a negative binomial distribution. Generalized linear models are used to perform the tests for statistical significance and as the number of genes is high, multiple tests correction have to be considered. Some examples of other analysis on genomics data comes from microarray or proteomics experiments. Often concerning diseases or disease stages.\nTools.\nThere are a lot of tools that can be used to do statistical analysis in biological data. Most of them are useful in other areas of knowledge, covering a large number of applications (alphabetical). Here are brief descriptions of some of them:\nScope and training programs.\nAlmost all educational programmes in biostatistics are at postgraduate level. They are most often found in schools of public health, affiliated with schools of medicine, forestry, or agriculture, or as a focus of application in departments of statistics.\nIn the United States, where several universities have dedicated biostatistics departments, many other top-tier universities integrate biostatistics faculty into statistics or other departments, such as epidemiology. Thus, departments carrying the name \"biostatistics\" may exist under quite different structures. For instance, relatively new biostatistics departments have been founded with a focus on bioinformatics and computational biology, whereas older departments, typically affiliated with schools of public health, will have more traditional lines of research involving epidemiological studies and clinical trials as well as bioinformatics. In larger universities around the world, where both a statistics and a biostatistics department exist, the degree of integration between the two departments may range from the bare minimum to very close collaboration. In general, the difference between a statistics program and a biostatistics program is twofold: (i) statistics departments will often host theoretical/methodological research which are less common in biostatistics programs and (ii) statistics departments have lines of research that may include biomedical applications but also other areas such as industry (quality control), business and economics and biological areas other than medicine."}
{"id": "3879", "revid": "44473045", "url": "https://en.wikipedia.org/wiki?curid=3879", "title": "Business statistics", "text": ""}
{"id": "3912", "revid": "1238511094", "url": "https://en.wikipedia.org/wiki?curid=3912", "title": "List of major biblical figures", "text": "The Bible is a collection of canonical sacred texts of Judaism and Christianity. Different religious groups include different books within their canons, in different orders, and sometimes divide or combine books, or incorporate additional material into canonical books. Christian Bibles range from the sixty-six books of the Protestant canon to the eighty-one books of the Ethiopian Orthodox Church canon.\nHebrew Bible.\nTribes of Israel.\nAccording to the Book of Genesis, the Israelites were descendants of the sons of Jacob, who was renamed Israel after wrestling with an angel. His twelve male children become the ancestors of the Twelve Tribes of Israel.\nNew Testament.\nApostles of Jesus.\nThe Thirteen:\nOthers:"}
{"id": "3913", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=3913", "title": "BinaryOperation", "text": ""}
{"id": "3914", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=3914", "title": "British &amp; Irish Lions", "text": "The British &amp; Irish Lions is a rugby union team selected from players eligible for the national teams of England, Ireland, Scotland, and Wales. The Lions are a test side and most often select players who have already played for their national team, although they can pick uncapped players who are eligible for any of the four unions. The team tours every four years, with these rotating between Australia, New Zealand and South Africa in order. The most recent test series, the 2021 series against South Africa, was won 2\u20131 by South Africa.\nFrom 1888 onwards, combined British rugby sides toured the Southern Hemisphere. The first tour was a commercial venture, undertaken without official backing. The six subsequent visits enjoyed a growing degree of support from the authorities, before the 1910 South Africa tour, which was the first tour representative of the four Home Unions. In 1949 the four Home Unions formally created a Tours Committee and for the first time, every player of the 1950 Lions squad had played internationally before the tour. The 1950s tours saw high win rates in provincial games, but the Test series were typically lost or drawn. The series wins in 1971 (New Zealand) and 1974 (South Africa) interrupted this pattern. The last tour of the amateur age took place in 1993. The Lions have also played occasional matches in the Northern Hemisphere either as one-off exhibitions or before a Southern Hemisphere tour.\nNaming and symbols.\nName.\nThe Shaw and Shrewsbury team first played in 1888 and is considered the precursor of the British &amp; Irish Lions. It was then primarily English in composition but also contained players from Scotland and Wales. Later the team used the name British Isles. On their 1950 tour of New Zealand and Australia they officially adopted the name British Lions, the nickname first used by British and South African journalists on the 1924 South African tour after the lion emblem on their ties, the emblem on their jerseys having been dropped in favour of the four-quartered badge with the symbols of the four represented unions.\nWhen the team first emerged in the 19th century, the United Kingdom of Great Britain and Ireland was one single state. The team continued after the Irish Free State was set up in 1922, but was still known as the British Lions or British Isles. The name \"British &amp; Irish Lions\" has been used since the 2001 tour of Australia. The team is often referred to simply as the Lions.\nAnthem.\nAs the Lions represent four rugby unions, which cover two sovereign states, they do not currently have a national anthem. For the 1989 tour, the British national anthem \"God Save the Queen\" was used. For the 2005 tour to New Zealand, the Lions management commissioned a song, \"The Power of Four\", although it was met with little support among Lions fans at the matches and has not been used since.\nColours and strip.\nFor more than half a century, the Lions have worn a red jersey that sports the amalgamated crests of the four unions. Prior to 1950 the strip went through a number of significantly different formats.\nUnsanctioned tours.\nIn 1888, the promoter of the first expedition to Australia and New Zealand, Arthur Shrewsbury, demanded \"something that would be good material and yet take them by storm out here\". The result was a jersey in thick red, white and blue hoops, worn above white shorts and dark socks. The tours to South Africa in 1891 and 1896 retained the red, white and blue theme but this time as red and white hooped jerseys and dark blue shorts and socks. The 1899 trip to Australia saw a reversion to red, white and blue jerseys, but with the blue used in thick hoops and the red and white in thin bands. The shorts remained blue, as did the socks although a white flash was added to the latter. The one-off test in 1999 between England and Australia that was played to commemorate Australia's first test against Reverend Matthew Mullineux's British side saw England wear an updated version of this jersey. In 1903, the South Africa tour followed on from the 1896 tour, with red and white hooped jerseys. The slight differences were that the red hoops were slightly thicker than the white (the opposite was true in 1896), and the white flash on the socks introduced in 1899 was partially retained. The Australia tour of 1904 saw exactly the same kit as in 1899. In 1908, with the Scottish and Irish unions not taking part, the Anglo-Welsh side sported red jerseys with a thick white band on tour to Australia and New Zealand. Blue shorts were retained, but the socks were for the first time red, with a white flash.\nBlue jerseys, the Lions named and the crest adopted.\nThe Scots were once again involved in Tom Smyth's 1910 team to South Africa. Thus, dark blue jerseys were introduced with white shorts and the red socks of 1908. The jerseys also had a single lion-rampant crest. The 1924 tour returned to South Africa, retaining the blue jerseys but now with shorts to match. It is the 1924 tour that is credited as being the first in which the team were referred to as \"the Lions\", the irony being that it was on this tour that the single lion-rampant crest was replaced with the forerunner of the four-quartered badge with the symbols of the four represented unions, that is still worn today. Although the lion had been dropped from the jersey, the players had worn the lion motif on their ties as they arrived in South Africa, which led the press and public referring to them as \"the Lions\".\nThe unofficial 1927 Argentina tour used the same kit and badge, and three heraldic lions returned as the jersey badge in 1930. This was the tour to New Zealand where the tourists' now standard blue jerseys caused some controversy. The convention in rugby is for the home side to accommodate its guests when there is a clash of kit. The New Zealand side, by then already synonymous with the appellation \"All Blacks\", had an all black kit that clashed with the Lions' blue. After much reluctance and debate New Zealand agreed to change for the Tests and New Zealand played in all white for the first time. On the 1930 tour a delegation led by the Irish lock George Beamish expressed their displeasure at the fact that while the blue of Scotland, white of England and red of Wales were represented in the strip there was no green for Ireland. A green flash was added to the socks, which from 1938 became a green turnover (although on blue socks thus eliminating red from the kit), and that has remained a feature of the strip ever since. In 1936, the four-quartered badge returned for the tour to Argentina and has remained on the kits ever since, but other than that the strip remained the same.\nRed jerseys.\nThe adoption of the red jersey happened in the 1950 tour. A return to New Zealand was accompanied by a desire to avoid the controversy of 1930 and so red replaced blue for the jersey with the resultant kit being that which is still worn today, the combination of red jersey, white shorts and green and blue socks, representing the four unions. The only additions to the strip since 1950 began appearing in 1993, with the addition of kit suppliers logos in prominent positions. Umbro had in 1989 asked for \"maximum brand exposure whenever possible\" but this did not affect the kit's appearance. Since then, Nike, Adidas and Canterbury have had more overt branding on the shirts, with sponsors Scottish Provident (1997), (2001), Zurich (2005), HSBC (2009 and 2013), Standard Life Investments (2017) and Vodafone (2021).\nHistory.\n1888\u20131909.\nThe earliest tours date back to 1888, when a 21-man squad visited Australia and New Zealand. The squad drew players from England, Scotland and Wales, though English players predominated. The 35-match tour of two host nations included no tests, but the side played provincial, city and academic sides, winning 27 matches. They played 19 games of Australian rules football, against prominent clubs in Victoria and South Australia, winning six and drawing one of these (see Australian rules football in England).\nThe first tour, although unsanctioned by rugby bodies, established the concept of Northern Hemisphere sporting sides touring to the Southern Hemisphere. Three years after the first tour, the Western Province union invited rugby bodies in Britain to tour South Africa. Some saw the 1891 team \u2013 the first sanctioned by the Rugby Football Union \u2013 as the England national team, though others referred to it as \"the British Isles\". The tourists played a total of twenty matches, three of them tests. The team also played the regional side of South Africa (South Africa did not exist as a political unit in 1891), winning all three matches. In a notable event of the tour, the touring side presented the Currie Cup to Griqualand West, the province they thought produced the best performance on the tour.\nFive years later a British Isles side returned to South Africa. They played one extra match on this tour, making the total of 21 games, including four tests against South Africa, with the British Isles winning three of them. The squad had a notable Irish orientation, with the Ireland national team contributing six players to the 21-man squad.\nIn 1899 the British Isles touring side returned to Australia for the first time since the unofficial tour of 1888. The squad of 23 for the first time ever had players from each of the home nations. The team again participated in 21 matches, playing state teams as well as northern Queensland sides and Victorian teams. A four-test series took place against Australia, the tourists winning three out of the four. The team returned via Hawaii and Canada playing additional games en route.\nFour years later, in 1903, the British Isles team returned to South Africa. The opening performance of the side proved disappointing from the tourists' point of view, with defeats in its opening three matches by Western Province sides in Cape Town. From then on the team experienced mixed results, though more wins than losses. The side lost the test series to South Africa, drawing twice, but with the South Africans winning the decider 8 to nil.\nNo more than twelve months passed before the British Isles team ventured to Australia and New Zealand in 1904. The tourists devastated the Australian teams, winning every single game. Australia also lost all three tests to the visitors, even getting held to a standstill in two of the three games. Though the New Zealand leg of the tour did not take long in comparison to the number of Australian games, the British Isles experienced considerable difficulty across the Tasman after whitewashing the Australians. The team managed two early wins before losing the test to New Zealand and only winning one more game as well as drawing once. Despite their difficulties in New Zealand, the tour proved a raging success on-field for the British Isles.\nIn 1908, another tour took place to Australia and New Zealand. In a reversal of previous practice, the planners allocated more matches in New Zealand rather than in Australia: perhaps the strength of the New Zealand teams and the heavy defeats of all Australian teams on the previous tour influenced this decision. Some commentators thought that this tour hoped to reach out to rugby communities in Australia, as rugby league (infamously) started in Australia in 1908. The Anglo-Welsh side (Irish and Scottish unions did not participate) performed well in all the non-test matches, but drew a test against New Zealand and lost the other two.\n1910\u20131949.\nVisits that took place before the 1910 South Africa tour (the first selected by a committee from the four Home Unions) had enjoyed a growing degree of support from the authorities, although only one of these included representatives of all four nations. The 1910 tour to South Africa marked the official beginning of British Isles rugby tours: the inaugural tour operating under all four unions. The team performed moderately against the non-test teams, claiming victories in just over half their matches, and the test series went to South Africa, who won two of the three games. A side managed by Oxford University \u2014 supposedly the England rugby team, but actually including three Scottish players \u2014 toured Argentina at the time: the people of Argentina termed it the \"Combined British\".\nThe next British Isles team tour did not take place until 1924, again in South Africa. The team, led by Ronald Cove-Smith, struggled with injuries and lost three of the four test matches, drawing the other 3\u20133. In total, 21 games were played, with the touring side winning 9, drawing 3 and losing 9.\nIn 1927 a short, nine-game series took place in Argentina, with the British isles winning all nine encounters, and the tour was a financial success for Argentine rugby. The Lions returned to New Zealand in 1930 with some success. The Lions won all of their games that did not have test status except for the matches against Auckland, Wellington and Canterbury, but they lost three of their four test matches against New Zealand, winning the first test 6\u20133. The side also visited Australia, losing a test but winning five out of the six non-test games.\nIn 1936 the British Isles visited Argentina for the third time, winning all ten of their matches and only conceding nine points in the whole tour. Two years later in 1938 the British Isles toured in South Africa, winning more than half of their normal matches. Despite having lost the test series to South Africa by game three, they won the final test. This is when they were named THE LIONS by their then Captain Sam Walker.\n1950\u20131969.\nThe first post-war tour went to New Zealand and Australia in 1950. The Lions, sporting newly redesigned jerseys and displaying a fresh style of play, managed to win 22 and draw one of 29 matches over the two nations. The Lions won the opening four fixtures before losing to Otago and Southland, but succeeded in holding New Zealand to a 9\u20139 draw. The Lions performed well in the remaining All Black tests though they lost all three, the team did not lose another non-test in the New Zealand leg of the tour. The Lions won all their games in Australia except for their final fixture against a New South Wales XV in Newcastle. They won both tests against Australia, in Brisbane, Queensland and in Sydney.\nIn 1955 the Lions toured South Africa and left with another imposing record, one draw and 19 wins from the 25 fixtures. The four-test series against South Africa, a thrilling affair, ended in a drawn series.\nThe 1959 tour to Australia and New Zealand marked once again a very successful tour for the Lions, who only lost six of their 35 fixtures. The Lions easily won both tests against Australia and lost the first three tests against New Zealand, but did find victory (9\u20136) in the final test.\nAfter the glittering decade of the 1950s, the first tour of the 1960s proved not nearly as successful as previous ones. The 1962 tour to South Africa saw the Lions still win 16 of their 25 games, but did not fare well against the Springboks, losing three of the four tests. For the 1966 tour to Australia and New Zealand John Robins became the first Lions coach, and the trip started off very well for the Lions, who stormed through Australia, winning five non-tests and drawing one, and defeating Australia in two tests. The Lions experienced mixed results during the New Zealand leg of the tour, as well as losing all of the tests against New Zealand. The Lions also played a test against Canada on their way home, winning 19 to 8 in Toronto. The 1968 tour of South Africa saw the Lions win 15 of their 16 provincial matches, but the team actually lost three tests against the Springboks and drew one.\n1970\u20131979.\nThe 1970s saw a renaissance for the Lions. The 1971 British Lions tour to New Zealand and Australia, centred around the skilled Welsh half-back pairing of Gareth Edwards and Barry John, secured a series win over New Zealand. The tour started with a loss to Queensland but proceeded to storm through the next provincial fixtures, winning 11 games in a row. The Lions then went on to defeat New Zealand in Dunedin. The Lions only lost one match on the rest of the tour and won the test series against New Zealand, winning and drawing the last two games, to take the series two wins to one.\nThe 1974 British Lions tour to South Africa was one of the best-known and most successful Lions teams. Apartheid concerns meant some players declined the tour. Nonetheless, led by the esteemed Irish forward Willie John McBride, the tour went through 22 games unbeaten and triumphed 3\u20130 (with one drawn) in the test series. The series featured a lot of violence. The management of the Lions concluded that the Springboks dominated their opponents with physical aggression. At that time, test match referees came from the home nation, substitutions took place only if a doctor found a player unable to continue and there were no video cameras or sideline officials to prevent violent play. The Lions decided \"to get their retaliation in first\" with the infamous \"99 call\". The Lions postulated that a South African referee would probably not send off all of the Lions if they all retaliated against \"blatant thuggery\". Famous video footage of the 'battle of Boet Erasmus Stadium' shows JPR Williams running over half of the pitch and launching himself at Van Heerden after such a call.\nThe 1977 British Lions tour to New Zealand saw the Lions drop only one non-test out of 21 games, a loss to a Universities side. The team did not win the test series though, winning one game but losing the other three.\nIn August 1977 the British Lions made a stopover in Fiji on the way home from their tour of New Zealand. Fiji beat them 25\u201321 at Buckhurst Park, Suva.\n1980\u20131989.\nThe Lions toured South Africa in 1980, and completed a flawless non-test record, winning 14 out of 14 matches. The Lions lost the first three tests to South Africa, only winning the last one once the Springboks were guaranteed to win the series.\nThe 1983 tour to New Zealand saw the team successful in the non-test games, winning all but two games, but being whitewashed in the test series against New Zealand.\nA tour to South Africa by the Lions was anticipated in 1986, but the invitation for the Lions to tour was never accepted because of controversy surrounding Apartheid and the tour did not go ahead. The Lions did not return to South Africa until 1997, after the Apartheid era. A Lions team was selected in April 1986 for the International Rugby Board centenary match against 'The Rest'. The team was organised by the Four Home Unions Committee and the players were given the status of official British Lions.\nThe Lions tour to Australia in 1989 was a shorter affair, being only 12 matches in total. The tour was very successful for the Lions, who won all eight non-test matches and won the test series against Australia, two to one.\n1990\u20131999.\nThe tour to New Zealand in 1993 was the last of the amateur era. The Lions won six and lost four non-test matches, and lost the test series 2\u20131. The tour to South Africa in 1997 was a success for the Lions, who completed the tour with only two losses, and won the test series 2\u20131.\n2000\u20132009.\nIn 2001, the ten-game tour to Australia saw the Wallabies win the test series 2\u20131. This series saw the first award of the Tom Richards Trophy. In the Lions' 2005 tour to New Zealand, coached by Clive Woodward, the Lions won seven games against provincial teams, were defeated by the New Zealand Maori team, and suffered heavy defeats in all three tests.\nIn 2009, the Lions toured South Africa. There they faced the World Cup winners South Africa, with Ian McGeechan leading a coaching team including Warren Gatland, Shaun Edwards and Rob Howley. The Lions were captained by Irish lock Paul O'Connell. The initial Lions selection consisted of fourteen Irish players, thirteen Welsh, eight English and two Scots in the 37-man squad. In the first Test on 20 June, they lost 26\u201321, and lost the series in the second 28\u201325 in a tightly fought game at Loftus Versfeld on 27 June. The Lions won the third Test 28\u20139 at Ellis Park, and the series finished 2\u20131 to South Africa.\n2010\u20132019.\nDuring June 2013 the British &amp; Irish Lions toured Australia. Former Scotland and Lions full-back Andy Irvine was appointed as tour manager in 2010. Wales head coach Warren Gatland was the Lions' head coach, and their tour captain was Sam Warburton. The tour started in Hong Kong with a match against the Barbarians before moving on to Australia for the main tour featuring six provincial matches and three tests. The Lions won all but one non-test matches, losing to the Brumbies 14\u201312 on 18 June. The first test was followed shortly after this, which saw the Lions go 1-up over Australia winning 23\u201321. Australia did have a chance to take the win in the final moments of the game, but a missed penalty by Kurtley Beale saw the Lions take the win. The Wallabies drew the series in the second test winning 16\u201315, though the Lions had a chance to steal the win had it not been because of a missed penalty by Leigh Halfpenny. With tour captain Warburton out of the final test due to injury, Alun Wyn Jones took over the captaincy in the final test in Sydney. The final test was won by the Lions in what was a record win, winning 41\u201316 to earn their first series win since 1997 and their first over Australia since 1989.\nFollowing his winning tour of Australia in 2013, Warren Gatland was reappointed as Lions Head Coach for the tour to New Zealand in June and July 2017. In April 2016, it was announced that the side would again be captained again by Sam Warburton. The touring schedule included 10 games: an opening game against the Provincial Barbarians, challenge matches against all five of New Zealand's Super Rugby sides, a match against the M\u0101ori All Blacks and three tests against . The Lions defeated the Provincial Barbarians in the first game of the tour, before being beaten by the Blues three days later. The team recovered to beat the Crusaders but this was followed up with another midweek loss, this time against the Highlanders. The Lions then faced the M\u0101ori All Blacks, winning comfortably to restore optimism and followed up with their first midweek victory of the tour against the Chiefs. On 24 June, the Lions, captained by Peter O'Mahony, faced New Zealand in Eden Park in the first Test and were beaten 30\u201315. This was followed by the final midweek game of the tour, a draw against the Hurricanes. For the second Test, Gatland recalled Warburton to the starting team as captain. In Wellington Regional Stadium, the Lions beat a 14-man New Zealand side 24\u201321 after Sonny Bill Williams was red-carded at the 24-minute mark after a shoulder charge on Anthony Watson. This tied the series going into the final game, ending the side's 47-game winning run at home. In the final test at Eden Park the following week, the teams were tied at 15 points apiece with 78 minutes gone. Romain Poite signaled a penalty to New Zealand for an offside infringement after Ken Owens received the ball in front of his teammate Liam Williams, giving New Zealand the opportunity to kick for goal and potentially win the series. Poite, however, decided to downgrade the penalty to a free-kick after discussing with assistant referee J\u00e9r\u00f4me Garc\u00e8s and Lions captain Sam Warburton. The match finished as a draw and the series was tied.\n2020\u2013present.\nWarren Gatland was Lions head coach again for the tour to South Africa in 2021. In December 2019, the Lions' Test venues were announced, but the tour was significantly disrupted by the COVID-19 pandemic, and all the games were played behind closed doors. South Africa won the test series by two games to one. In the deciding third test, Morne Steyn again kicked a late penalty to win the series. In 2024, it was announced that Andy Farrell would succeed Gatland as the Lions head coach. A women's Lions team was established in 2024, with their inaugural tour to New Zealand to take place in 2027.\nOverall test match record.\nOverall test series results\nTours.\nFormat.\nThe Lions now regularly tour three Southern Hemisphere countries; Australia, South Africa and New Zealand. They also toured Argentina three times before the Second World War. Since 1989 tours have been held every four years. The most recent tour was to South Africa in 2021.\nIn a break with tradition, the 2005 tour of New Zealand was preceded by a \"home\" fixture against Argentina at the Millennium Stadium in Cardiff on 23 May 2005. It finished in a 25\u201325 draw. A similar fixture was held against Japan before the 2021 tour of South Africa at Murrayfield, with the Lions winning 28\u201310.\nOn tour, games take place against local provinces, clubs or representative sides as well as test matches against the host's national team.\nThe Lions and their predecessor teams have also played games against other nearby countries on tour. For example, they played Rhodesia in 1910, 1924, 1938, 1955, 1962, 1968 and 1974 during their tours to South Africa. They were also beaten by Fiji on their 1977 tour to New Zealand. In addition, they visited pre-independence Namibia (then South West Africa), in 1955, 1962, 1968 and 1974.\nThere have also been games in other countries on the way home. These include games in in 1959 and 1966, East Africa (then mostly Kenya, and held in Nairobi), and an unofficial game against Ceylon (future Sri Lanka) in 1950.\nLions non-tour and home matches.\nThe Lions have played a number of other matches against international opposition. With the exception of the matches against Argentina in 2005 and Japan in 2021, which were preparation matches for Lions tours, these matches have been one-offs to mark special occasions.\nThe Lions played an unofficial international match in 1955 at Cardiff Arms Park against a Welsh XV to mark the 75th anniversary of the Welsh Rugby Union. The Lions won 20\u201317 but did not include all the big names of the 1955 tour, such as Tony O'Reilly, Jeff Butterfield, Phil Davies, Dickie Jeeps, Bryn Meredith and Jim Greenwood.\nIn 1977, the Lions played their first official home game, against the Barbarians as a charity fund-raiser held as part of the Queen's silver jubilee celebrations. The Baa-Baas line-up featured JPR Williams, Gerald Davies, Gareth Edwards, Jean-Pierre Rives and Jean-Claude Skrela. The Lions included 13 of the team who played in the fourth test against New Zealand three weeks before and won 23\u201314.\nIn 1986, a match was organised against The Rest as a warm-up to the 1986 South Africa tour, and as a celebration to mark the International Rugby Board's centenary. The Lions lost 15\u20137 and the planned tour was subsequently cancelled.\nIn 1989, the Lions played against France in Paris. The game formed part of the celebrations of the bi-centennial of the French Revolution. The Lions, captained by Rob Andrew, won 29\u201327.\nIn 1990, a Four Home Unions team played against the Rest of Europe in a match to raise money for the rebuilding of Romania following the overthrow of Nicolae Ceau\u0219escu in December 1989. The team used the Lions' logo, while the Rest of Europe played under the symbol of the Romanian Rugby Federation.\nPlayer records.\nMost caps.\n\"Updated 7 August 2021\"\nMost points.\n\"Updated 31 July 2021\"\nMost tries.\n\"Updated 31 July 2021\""}
{"id": "3916", "revid": "48767858", "url": "https://en.wikipedia.org/wiki?curid=3916", "title": "Bass guitar", "text": "The bass guitar, electric bass or simply bass () is the lowest-pitched member of the guitar family. It is a plucked string instrument similar in appearance and construction to an electric or acoustic guitar, but with a longer neck and scale length. The bass guitar most commonly has four strings, though five- and six-stringed models are also relatively popular, and bass guitars with even more (or fewer) strings or courses have been built. Since the mid-1950s, the bass guitar has largely come to replace the double bass in popular music due to its lighter weight, the inclusion of frets (for easier intonation) in most models, and, most importantly, its design for electric amplification. This is also because the double bass is acoustically compromised for its range (like the viola) in that it is scaled down from the optimal size that would be appropriate for those low notes.\nThe four-string bass guitar is usually tuned the same as the double bass, which corresponds to pitches one octave lower than the four lowest-pitched strings of a guitar (typically E, A, D, and G). It is played primarily with the fingers or thumb, or with a pick. \nThe electric bass guitar is acoustically a relatively quiet instrument, so to be heard at a practical performance volume, it requires external amplification. It can also be used in conjunction with direct input boxes, audio interfaces, mixing consoles, computers, or bass effects processors that offer headphone jacks. The majority of bass pickup systems are electromagnetic in nature.\nTerminology.\nThe \"New Grove Dictionary of Music and Musicians\" refers to this instrument as an \"Electric bass guitar, usually with four heavy strings tuned E1'\u2013A1'\u2013D2\u2013G2.\" It also defines \"bass\" as \"Bass (iv). A contraction of Double bass or Electric bass guitar.\" \"Mottola's Cyclopedic Dictionary of Lutherie Terms\" begins its definition of the instrument as \"A bass guitar that produces sound primarily with the aid of electronic devices.\" According to some authors the proper term is \"electric bass\". Common names for the instrument are \"bass guitar\", \"electric bass guitar\", and \"electric bass\" and some authors claim that they are historically accurate. A bass guitar whose neck lacks frets is termed a fretless bass.\nScale.\nThe scale of a bass is defined as the length of the freely oscillating strings between the nut and the bridge saddles. On a modern 4-string bass guitar, 30\" (76 cm) or less is considered short scale, 32\" (81 cm) medium scale, 34\" (86 cm) standard or long scale and 35\" (89 cm) extra-long scale.\nPickup.\nBass pickups are generally attached to the body of the guitar and located beneath the strings. They are responsible for converting the vibrations of the strings into analogous electrical signals, which are in turn passed as input to an instrument amplifier.\nStrings.\nBass guitar strings are composed of a \"core\" and \"winding\". The core is a wire which runs through the center of the string and is generally made of steel, nickel, or an alloy. The winding is an additional wire wrapped around the core. Bass guitar strings vary by the material and cross-sectional shape of the winding. \nCommon variants include roundwound, flatwound, halfwound (groundwound), coated, tapewound and taperwound (not to be confused with tapewound) strings. Roundwound and flatwound strings feature windings with circular and rounded-square cross-sections, respectively, with halfround (also referred to as halfwound, ground wound, pressure wound) strings being a hybrid between the two. Coated strings have their surface coated with a synthetic layer while tapewound strings feature a metal core with a non-metallic winding. Taperwound strings have a tapered end where the exposed core sits on the bridge saddle without windings. The choice of winding has considerable impact on the sound of the instrument, with certain winding styles often being preferred for certain musical genres.\nHistory.\n1930s.\nIn the 1930s, musician and inventor Paul Tutmarc of Seattle, Washington, developed the first electric bass guitar in its modern form, a fretted instrument designed to be played horizontally. The 1935 sales catalog for Tutmarc's company Audiovox featured his \"Model 736 Bass Fiddle\", a solid-bodied electric bass guitar with four strings, a scale length, and a single pickup. Around 100 were made during this period.\nAudiovox also sold their \"Model 236\" bass amplifier.\n1950s.\nIn the 1950s, Leo Fender and George Fullerton developed the first mass-produced electric bass guitar. The Fender Electric Instrument Manufacturing Company began producing the Precision Bass, or P-Bass, in October 1951. The design featured a simple uncontoured \"slab\" body design and a single coil pickup similar to that of a Telecaster. By 1957 the Precision more closely resembled the Fender Stratocaster with the body edges beveled for comfort, and the pickup was changed to a split coil design.\nThe Fender Bass was a revolutionary instrument for gigging musicians. In comparison with the large, heavy upright bass, which had been the main bass instrument in popular music from the early 20th century to the 1940s, the bass guitar could be easily transported to shows. When amplified, the bass guitar was also less prone than acoustic basses to unwanted audio feedback. The addition of frets enabled bassists to play in tune more easily than on fretless acoustic or electric upright basses, and allowed guitarists to more easily transition to the instrument.\nIn 1953, Monk Montgomery became the first bassist to tour with the Fender bass, in Lionel Hampton's postwar big band. Montgomery was also possibly the first to record with the electric bass, on July 2, 1953, with the Art Farmer Septet. Roy Johnson (with Lionel Hampton), and Shifty Henry (with Louis Jordan and His Tympany Five), were other early Fender bass pioneers. Bill Black, who played with Elvis Presley, switched from upright bass to the Fender Precision Bass around 1957. The bass guitar was intended to appeal to guitarists as well as upright bass players, and many early pioneers of the instrument, such as Carol Kaye, Joe Osborn, and Paul McCartney were originally guitarists.\nAlso in 1953, Gibson released the first short-scale violin-shaped electric bass, the EB-1, with an extendable end pin so a bassist could play it upright or horizontally. In 1958, Gibson released the maple arched-top EB-2 described in the Gibson catalog as a \"hollow-body electric bass that features a Bass/Baritone pushbutton for two different tonal characteristics\". In 1959, these were followed by the more conventional-looking EB-0 Bass. The EB-0 was very similar to a Gibson SG in appearance (although the earliest examples have a slab-sided body shape closer to that of the double-cutaway Les Paul Special). The Fender and Gibson versions used bolt-on and set necks.\nSeveral other companies also began manufacturing bass guitars during the 1950s. Kay Musical Instrument Company began production of the K162 in 1952, while Danelectro released the Longhorn in 1956. Also in 1956, at the German trade fair \"Musikmesse Frankfurt\" the distinctive H\u00f6fner 500/1 violin-shaped bass first appeared, constructed using violin techniques by Walter H\u00f6fner, a second-generation violin luthier. Due to its use by Paul McCartney, it became known as the \"Beatle bass\". In 1957, Rickenbacker introduced the model 4000, the first bass to feature a neck-through-body design in which the neck is part of the body wood. The Burns London Supersound was introduced in 1958.\n1960s.\nWith the explosion in popularity of rock music in the 1960s, many more manufacturers began making electric basses, including Yamaha, Teisco and Guyatone. Introduced in 1960, the Fender Jazz Bass, initially known as the \"Deluxe Bass\", used a body design known as an offset waist which was first seen on the Jazzmaster guitar in an effort to improve comfort while playing seated. The Jazz bass, or J-Bass, features two single-coil pickups.\nProviding a more \"Gibson-scale\" instrument, rather than the Jazz and Precision, Fender produced the Mustang Bass, a scale-length instrument. The Fender VI, a 6-string bass, was tuned one octave lower than standard guitar tuning. It was released in 1961, and was briefly favored by Jack Bruce of Cream.\nGibson introduced its short-scale EB-3 in 1961, also used by Bruce. The EB-3 had a \"mini-humbucker\" at the bridge position. Gibson basses tended to be instruments with a shorter 30.5\" scale length than the Precision. Gibson did not produce a -scale bass until 1963 with the release of the Thunderbird.\nThe first commercial fretless bass guitar was the Ampeg AUB-1, introduced in 1966. In the late 1960s, eight-string basses, with four octave paired courses (similar to a 12 string guitar), were introduced, such as the Hagstr\u00f6m H8.\n1970s.\nIn 1972, Alembic established what became known as \"boutique\" or \"high-end\" electric bass guitars. These expensive, custom-tailored instruments, as used by Phil Lesh, Jack Casady, and Stanley Clarke, featured unique designs, premium hand-finished wood bodies, and innovative construction techniques such as multi-laminate neck-through-body construction and graphite necks. Alembic also pioneered the use of onboard electronics for pre-amplification and equalization. \nActive electronics increase the output of the instrument, and allow more options for controlling tonal flexibility, giving the player the ability to amplify as well as to attenuate certain frequency ranges while improving the overall frequency response (including more low-register and high-register sounds). 1976 saw the UK company Wal begin production of their own range of active basses. In 1974 Music Man Instruments, founded by Tom Walker, Forrest White and Leo Fender, introduced the StingRay, the first widely produced bass with active (powered) electronics built into the instrument. Basses with active electronics can include a preamplifier and knobs for boosting and cutting the low and high frequencies.\nIn the mid-1970s, five-string basses, with a very low \"B\" string, were introduced. In 1975, bassist Anthony Jackson commissioned luthier Carl Thompson to build a six-string bass tuned (low to high) B0, E1, A1, D2, G2, C3, adding a low B string and a high C string."}
{"id": "3920", "revid": "43676619", "url": "https://en.wikipedia.org/wiki?curid=3920", "title": "Beatles", "text": ""}
{"id": "3921", "revid": "48376748", "url": "https://en.wikipedia.org/wiki?curid=3921", "title": "Basketball", "text": "Basketball is a team sport in which two teams, most commonly of five players each, opposing one another on a rectangular court, compete with the primary objective of shooting a basketball (approximately in diameter) through the defender's hoop (a basket in diameter mounted high to a backboard at each end of the court), while preventing the opposing team from shooting through their own hoop. A field goal is worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops and the player fouled or designated to shoot a technical foul is given one, two or three one-point free throws. The team with the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period of play (overtime) is mandated.\nPlayers advance the ball by bouncing it while walking or running (dribbling) or by passing it to a teammate, both of which require considerable skill. On offense, players may use a variety of shotsthe layup, the jump shot, or a dunk; on defense, they may steal the ball from a dribbler, intercept passes, or block shots; either offense or defense may collect a rebound, that is, a missed shot that bounces from rim or backboard. It is a violation to lift or drag one's pivot foot without dribbling the ball, to carry it, or to hold the ball with both hands then resume dribbling.\nThe five players on each side fall into five playing positions. The tallest player is usually the center, the second-tallest and strongest is the power forward, a slightly shorter but more agile player is the small forward, and the shortest players or the best ball handlers are the shooting guard and the point guard, who implement the coach's game plan by managing the execution of offensive and defensive plays (player positioning). Informally, players may play three-on-three, two-on-two, and one-on-one.\nInvented in 1891 by Canadian-American gym teacher James Naismith in Springfield, Massachusetts, in the United States, basketball has evolved to become one of the world's most popular and widely viewed sports. The National Basketball Association (NBA) is the most significant professional basketball league in the world in terms of popularity, salaries, talent, and level of competition (drawing most of its talent from U.S. college basketball). Outside North America, the top clubs from national leagues qualify to continental championships such as the EuroLeague and the Basketball Champions League Americas. The FIBA Basketball World Cup and Men's Olympic Basketball Tournament are the major international events of the sport and attract top national teams from around the world. Each continent hosts regional competitions for national teams, like EuroBasket and FIBA AmeriCup.\nThe FIBA Women's Basketball World Cup and Women's Olympic Basketball Tournament feature top national teams from continental championships. The main North American league is the WNBA (NCAA Women's Division I Basketball Championship is also popular), whereas the strongest European clubs participate in the EuroLeague Women.\nHistory.\nEarly history.\nA game similar to basketball is mentioned in a 1591 book published in Frankfurt am Main that reports on the lifestyles and customs of coastal North American residents, (German; translates as \"Truthful Depictions of the Savages\": \"Among other things, a game of skill is described in which balls must be thrown against a target woven from twigs, mounted high on a pole. There's a small reward for the player if the target is being hit.\"\nCreation.\nIn December 1891, James Naismith, a Canadian-American professor of physical education and instructor at the International Young Men's Christian Association Training School (now Springfield College) in Springfield, Massachusetts, was trying to keep his gym class active on a rainy day. He sought a vigorous indoor game to keep his students occupied and at proper levels of fitness during the long New England winters. After rejecting other ideas as either too rough or poorly suited to walled-in gymnasiums, he invented a new game in which players would pass a ball to teammates and try to score points by tossing the ball into a basket mounted on a wall. Naismith wrote the basic rules and nailed a peach basket onto an elevated track. Naismith initially set up the peach basket with its bottom intact, which meant that the ball had to be retrieved manually after each \"basket\" or point scored. This quickly proved tedious, so Naismith removed the bottom of the basket to allow the balls to be poked out with a long dowel after each scored basket.\nShortly after, Senda Berenson, instructor of physical culture at the nearby Smith College, went to Naismith to learn more about the game. Fascinated by the new sport and the values it could teach, she started to organize games with her pupils, following adjusted rules. The first official women's interinstitutional game was played barely 11 months later, between the University of California and the Miss Head's School. In 1899, a committee was established at the Conference of Physical Training in Springfield to draw up general rules for women's basketball. Thus, the sport quickly spread throughout America's schools, colleges and universities with uniform rules for both sexes.\nBasketball was originally played with a soccer ball. These round balls from \"association football\" were made, at the time, with a set of laces to close off the hole needed for inserting the inflatable bladder after the other sewn-together segments of the ball's cover had been flipped outside-in. These laces could cause bounce passes and dribbling to be unpredictable. Eventually a lace-free ball construction method was invented, and this change to the game was endorsed by Naismith (whereas in American football, the lace construction proved to be advantageous for gripping and remains to this day). The first balls made specifically for basketball were brown, and it was only in the late 1950s that Tony Hinkle, searching for a ball that would be more visible to players and spectators alike, introduced the orange ball that is now in common use. Dribbling was not part of the original game except for the \"bounce pass\" to teammates. Passing the ball was the primary means of ball movement. Dribbling was eventually introduced but limited by the asymmetric shape of early balls. Dribbling was common by 1896, with a rule against the double dribble by 1898.\nThe peach baskets were used until 1906 when they were finally replaced by metal hoops with backboards. A further change was soon made, so the ball merely passed through. Whenever a person got the ball in the basket, their team would gain a point. Whichever team got the most points won the game. The baskets were originally nailed to the mezzanine balcony of the playing court, but this proved impractical when spectators in the balcony began to interfere with shots. The backboard was introduced to prevent this interference; it had the additional effect of allowing rebound shots. Naismith's handwritten diaries, discovered by his granddaughter in early 2006, indicate that he was nervous about the new game he had invented, which incorporated rules from a children's game called duck on a rock, as many had failed before it.\nFrank Mahan, one of the players from the original first game, approached Naismith after the Christmas break, in early 1892, asking him what he intended to call his new game. Naismith replied that he had not thought of it because he had been focused on just getting the game started. Mahan suggested that it be called \"Naismith ball\", at which he laughed, saying that a name like that would kill any game. Mahan then said, \"Why not call it basketball?\" Naismith replied, \"We have a basket and a ball, and it seems to me that would be a good name for it.\" The first official game was played in the YMCA gymnasium in Albany, New York, on January 20, 1892, with nine players. The game ended at 1\u20130; the shot was made from , on a court just half the size of a present-day Streetball or National Basketball Association (NBA) court.\nAt the time, soccer was being played with 10 to a team (which was increased to 11). When winter weather got too icy to play soccer, teams were taken indoors, and it was convenient to have them split in half and play basketball with five on each side. By 1897\u201398, teams of five became standard.\nCollege basketball.\nBasketball's early adherents were dispatched to YMCAs throughout the United States, and it quickly spread through the United States and Canada. By 1895, it was well established at several women's high schools. While YMCA was responsible for initially developing and spreading the game, within a decade it discouraged the new sport, as rough play and rowdy crowds began to detract from YMCA's primary mission. However, other amateur sports clubs, colleges, and professional clubs quickly filled the void. In the years before World War I, the Amateur Athletic Union and the Intercollegiate Athletic Association of the United States (forerunner of the NCAA) vied for control over the rules for the game. The first pro league, the National Basketball League, was formed in 1898 to protect players from exploitation and to promote a less rough game. This league only lasted five years.\nJames Naismith was instrumental in establishing college basketball. His colleague C. O. Beamis fielded the first college basketball team just a year after the Springfield YMCA game at the suburban Pittsburgh Geneva College. Naismith himself later coached at the University of Kansas for six years, before handing the reins to renowned coach Forrest \"Phog\" Allen. Naismith's disciple Amos Alonzo Stagg brought basketball to the University of Chicago, while Adolph Rupp, a student of Naismith's at Kansas, enjoyed great success as coach at the University of Kentucky. On February 9, 1895, the first intercollegiate 5-on-5 game was played at Hamline University between Hamline and the School of Agriculture, which was affiliated with the University of Minnesota. The School of Agriculture won in a 9\u20133 game.\nIn 1901, colleges, including the University of Chicago, Columbia University, Cornell University, Dartmouth College, the University of Minnesota, the U.S. Naval Academy, the University of Colorado and Yale University began sponsoring men's games. In 1905, frequent injuries on the football field prompted President Theodore Roosevelt to suggest that colleges form a governing body, resulting in the creation of the Intercollegiate Athletic Association of the United States (IAAUS). In 1910, that body changed its name to the National Collegiate Athletic Association (NCAA). The first Canadian interuniversity basketball game was played at YMCA in Kingston, Ontario on February 6, 1904, when McGill UniversityNaismith's alma matervisited Queen's University. McGill won 9\u20137 in overtime; the score was 7\u20137 at the end of regulation play, and a ten-minute overtime period settled the outcome. A good turnout of spectators watched the game.\nThe first men's national championship tournament, the National Association of Intercollegiate Basketball tournament, which still exists as the National Association of Intercollegiate Athletics (NAIA) tournament, was organized in 1937. The first national championship for NCAA teams, the National Invitation Tournament (NIT) in New York, was organized in 1938; the NCAA national tournament began one year later. College basketball was rocked by gambling scandals from 1948 to 1951, when dozens of players from top teams were implicated in game-fixing and point shaving. Partially spurred by an association with cheating, the NIT lost support to the NCAA tournament.\nHigh school basketball.\nBefore widespread school district consolidation, most American high schools were far smaller than their present-day counterparts. During the first decades of the 20th century, basketball quickly became the ideal interscholastic sport due to its modest equipment and personnel requirements. In the days before widespread television coverage of professional and college sports, the popularity of high school basketball was unrivaled in many parts of America. Perhaps the most legendary of high school teams was Indiana's Franklin Wonder Five, which took the nation by storm during the 1920s, dominating Indiana basketball and earning national recognition.\nToday virtually every high school in the United States fields a basketball team in varsity competition. Basketball's popularity remains high, both in rural areas where they carry the identification of the entire community, as well as at some larger schools known for their basketball teams where many players go on to participate at higher levels of competition after graduation. In the 2016\u201317 season, 980,673 boys and girls represented their schools in interscholastic basketball competition, according to the National Federation of State High School Associations. The states of Illinois, Indiana and Kentucky are particularly well known for their residents' devotion to high school basketball, commonly called Hoosier Hysteria in Indiana; the critically acclaimed film \"Hoosiers\" shows high school basketball's depth of meaning to these communities.\n\u2063There is currently no tournament to determine a national high school champion. The most serious effort was the National Interscholastic Basketball Tournament at the University of Chicago from 1917 to 1930. The event was organized by Amos Alonzo Stagg and sent invitations to state champion teams. The tournament started out as a mostly Midwest affair but grew. In 1929 it had 29 state champions. Faced with opposition from the National Federation of State High School Associations and North Central Association of Colleges and Schools that bore a threat of the schools losing their accreditation the last tournament was in 1930. The organizations said they were concerned that the tournament was being used to recruit professional players from the prep ranks. The tournament did not invite minority schools or private/parochial schools.\nThe National Catholic Interscholastic Basketball Tournament ran from 1924 to 1941 at Loyola University. The National Catholic Invitational Basketball Tournament from 1954 to 1978 played at a series of venues, including Catholic University, Georgetown and George Mason. The National Interscholastic Basketball Tournament for Black High Schools was held from 1929 to 1942 at Hampton Institute. The National Invitational Interscholastic Basketball Tournament was held from 1941 to 1967 starting out at Tuskegee Institute. Following a pause during World War II it resumed at Tennessee State College in Nashville. The basis for the champion dwindled after 1954 when \"Brown v. Board of Education\" began an integration of schools. The last tournaments were held at Alabama State College from 1964 to 1967.\nProfessional basketball.\nTeams abounded throughout the 1920s. There were hundreds of men's professional basketball teams in towns and cities all over the United States, and little organization of the professional game. Players jumped from team to team and teams played in armories and smoky dance halls. Leagues came and went. Barnstorming squads such as the Original Celtics and two all-African American teams, the New York Renaissance Five (\"Rens\") and the (still existing) Harlem Globetrotters played up to two hundred games a year on their national tours.\nIn 1946, the Basketball Association of America (BAA) was formed. The first game was played in Toronto, Ontario, Canada between the Toronto Huskies and New York Knickerbockers on November 1, 1946. Three seasons later, in 1949, the BAA merged with the National Basketball League (NBL) to form the National Basketball Association (NBA). By the 1950s, basketball had become a major college sport, thus paving the way for a growth of interest in professional basketball. In 1959, a basketball hall of fame was founded in Springfield, Massachusetts, site of the first game. Its rosters include the names of great players, coaches, referees and people who have contributed significantly to the development of the game. The hall of fame has people who have accomplished many goals in their career in basketball. An upstart organization, the American Basketball Association, emerged in 1967 and briefly threatened the NBA's dominance until the ABA-NBA merger in 1976. Today the NBA is the top professional basketball league in the world in terms of popularity, salaries, talent, and level of competition.\nThe NBA has featured many famous players, including George Mikan, the first dominating \"big man\"; ball-handling wizard Bob Cousy and defensive genius Bill Russell of the Boston Celtics; charismatic center Wilt Chamberlain, who originally played for the barnstorming Harlem Globetrotters; all-around stars Oscar Robertson and Jerry West; more recent big men Kareem Abdul-Jabbar, Shaquille O'Neal, Hakeem Olajuwon and Karl Malone; playmakers John Stockton, Isiah Thomas and Steve Nash; crowd-pleasing forwards Julius Erving and Charles Barkley; European stars Dirk Nowitzki, Pau Gasol and Tony Parker; Latin American stars Manu Ginobili, more recent superstars, Allen Iverson, Kobe Bryant, Tim Duncan, LeBron James, Stephen Curry, Giannis Antetokounmpo, etc.; and the three players who many credit with ushering the professional game to its highest level of popularity during the 1980s and 1990s: Larry Bird, Earvin \"Magic\" Johnson, and Michael Jordan.\nIn 2001, the NBA formed a developmental league, the National Basketball Development League (later known as the NBA D-League and then the NBA G League after a branding deal with Gatorade). As of the 2023\u201324 season, the G League has 31 teams.\nInternational basketball.\nFIBA (International Basketball Federation) was formed in 1932 by eight founding nations: Argentina, Czechoslovakia, Greece, Italy, Latvia, Portugal, Romania and Switzerland. At this time, the organization only oversaw amateur players. Its acronym, derived from the French \"F\u00e9d\u00e9ration Internationale de Basket-ball Amateur\", was thus \"FIBA\". Men's basketball was first included at the Berlin 1936 Summer Olympics, although a demonstration tournament was held in 1904. The United States defeated Canada in the first final, played outdoors. This competition has usually been dominated by the United States, whose team has won all but three titles. The first of these came in a controversial final game in Munich in 1972 against the Soviet Union, in which the ending of the game was replayed three times until the Soviet Union finally came out on top. In 1950 the first FIBA World Championship for men, now known as the FIBA Basketball World Cup, was held in Argentina. Three years later, the first FIBA World Championship for women, now known as the FIBA Women's Basketball World Cup, was held in Chile. Women's basketball was added to the Olympics in 1976, which were held in Montreal, Quebec, Canada with teams such as the Soviet Union, Brazil and Australia rivaling the American squads.\nIn 1989, FIBA allowed professional NBA players to participate in the Olympics for the first time. Prior to the 1992 Summer Olympics, only European and South American teams were allowed to field professionals in the Olympics. The United States' dominance continued with the introduction of the original Dream Team. In the 2004 Athens Olympics, the United States suffered its first Olympic loss while using professional players, falling to Puerto Rico (in a 19-point loss) and Lithuania in group games, and being eliminated in the semifinals by Argentina. It eventually won the bronze medal defeating Lithuania, finishing behind Argentina and Italy. The Redeem Team, won gold at the 2008 Olympics, and the B-Team, won gold at the 2010 FIBA World Championship in Turkey despite featuring no players from the 2008 squad. The United States continued its dominance as they won gold at the 2012 Olympics, 2014 FIBA World Cup and the 2016 Olympics.\nWorldwide, basketball tournaments are held for boys and girls of all age levels. The global popularity of the sport is reflected in the nationalities represented in the NBA. Players from all six inhabited continents currently play in the NBA. Top international players began coming into the NBA in the mid-1990s, including Croatians Dra\u017een Petrovi\u0107 and Toni Kuko\u010d, Serbian Vlade Divac, Lithuanians Arvydas Sabonis and \u0160ar\u016bnas Mar\u010diulionis, Dutchman Rik Smits and German Detlef Schrempf.\nIn the Philippines, the Philippine Basketball Association's first game was played on April 9, 1975, at the Araneta Coliseum in Cubao, Quezon City, Philippines. It was founded as a \"rebellion\" of several teams from the now-defunct Manila Industrial and Commercial Athletic Association, which was tightly controlled by the Basketball Association of the Philippines (now defunct), the then-FIBA recognized national association. Nine teams from the MICAA participated in the league's first season that opened on April 9, 1975. The NBL is Australia's pre-eminent men's professional basketball league. The league commenced in 1979, playing a winter season (April\u2013September) and did so until the completion of the 20th season in 1998. The 1998\u201399 season, which commenced only months later, was the first season after the shift to the current summer season format (October\u2013April). This shift was an attempt to avoid competing directly against Australia's various football codes. It features 8 teams from around Australia and one in New Zealand. A few players including Luc Longley, Andrew Gaze, Shane Heal, Chris Anstey and Andrew Bogut made it big internationally, becoming poster figures for the sport in Australia. The Women's National Basketball League began in 1981.\nWomen's basketball.\nWomen began to play basketball in the fall of 1892 at Smith College through Senda Berenson, substitute director of the newly opened gymnasium and physical education teacher, after having modified the rules for women. Shortly after Berenson was hired at Smith, she visited Naismith to learn more about the game. Fascinated by the new sport and the values it could teach, she instantly introduced the game as a class exercise and soon after teams were organized. The first women's collegiate basketball game was played on March 21, 1893, when her Smith freshmen and sophomores played against one another. The first official women's interinstitutional game was played later that year between the University of California and the Miss Head's School. In 1899, a committee was established at the Conference of Physical Training in Springfield to draw up general rules for women's basketball. These rules, designed by Berenson, were published in 1899. In 1902 Berenson became the editor of A. G. Spalding's first Women's Basketball Guide. The same year women of Mount Holyoke and Sophie Newcomb College (coached by Clara Gregory Baer), began playing basketball. By 1895, the game had spread to colleges across the country, including Wellesley, Vassar, and Bryn Mawr. The first intercollegiate women's game was on April 4, 1896. Stanford women played Berkeley, 9-on-9, ending in a 2\u20131 Stanford victory.\nWomen's basketball development was more structured than that for men in the early years. In 1905, the executive committee on Basket Ball Rules (National Women's Basketball Committee) was created by the American Physical Education Association. These rules called for six to nine players per team and 11 officials. The International Women's Sports Federation (1924) included a women's basketball competition. 37 women's high school varsity basketball or state tournaments were held by 1925. And in 1926, the Amateur Athletic Union backed the first national women's basketball championship, complete with men's rules. The Edmonton Grads, a touring Canadian women's team based in Edmonton, Alberta, operated between 1915 and 1940. The Grads toured all over North America, and were exceptionally successful. They posted a record of 522 wins and only 20 losses over that span, as they met any team that wanted to challenge them, funding their tours from gate receipts. The Grads also shone on several exhibition trips to Europe, and won four consecutive exhibition Olympics tournaments, in 1924, 1928, 1932, and 1936; however, women's basketball was not an official Olympic sport until 1976. The Grads' players were unpaid, and had to remain single. The Grads' style focused on team play, without overly emphasizing skills of individual players. The first women's AAU All-America team was chosen in 1929. Women's industrial leagues sprang up throughout the United States, producing famous athletes, including Babe Didrikson of the Golden Cyclones, and the All American Red Heads Team, which competed against men's teams, using men's rules. By 1938, the women's national championship changed from a three-court game to two-court game with six players per team.\nThe NBA-backed Women's National Basketball Association (WNBA) began in 1997. Though it had shaky attendance figures, several marquee players (Lisa Leslie, Diana Taurasi, and Candace Parker among others) have helped the league's popularity and level of competition. Other professional women's basketball leagues in the United States, such as the American Basketball League (1996\u201398), have folded in part because of the popularity of the WNBA. The WNBA has been looked at by many as a niche league. However, the league has recently taken steps forward. In June 2007, the WNBA signed a contract extension with ESPN. The new television deal ran from 2009 to 2016. Along with this deal, came the first-ever rights fees to be paid to a women's professional sports league. Over the eight years of the contract, \"millions and millions of dollars\" were \"dispersed to the league's teams.\" In a March 12, 2009, article, NBA commissioner David Stern said that in the bad economy, \"the NBA is far less profitable than the WNBA. We're losing a lot of money among a large number of teams. We're budgeting the WNBA to break even this year.\"\nRules and regulations.\nMeasurements and time limits discussed in this section often vary among tournaments and organizations; international and NBA rules are used in this section.\nThe object of the game is to outscore one's opponents by throwing the ball through the opponents' basket from above while preventing the opponents from doing so on their own. An attempt to score in this way is called a shot. A successful shot is worth two points, or three points if it is taken from beyond the three-point arc from the basket in international games and in NBA games. A one-point shot can be earned when shooting from the foul line after a foul is made. After a team has scored from a field goal or free throw, play is resumed with a throw-in awarded to the non-scoring team taken from a point beyond the endline of the court where the points were scored.\nPlaying regulations.\nGames are played in four quarters of 10 (FIBA) or 12 minutes (NBA). College men's games use two 20-minute halves, college women's games use 10-minute quarters, and most United States high school varsity games use 8-minute quarters; however, this varies from state to state. 15 minutes are allowed for a half-time break under FIBA, NBA, and NCAA rules and 10 minutes in United States high schools. Overtime periods are five minutes in length except for high school, which is four minutes in length. Teams exchange baskets for the second half. The time allowed is actual playing time; the clock is stopped while the play is not active. Therefore, games generally take much longer to complete than the allotted game time, typically about two hours.\nFive players from each team may be on the court at one time. Substitutions are unlimited but can only be done when play is stopped. Teams also have a coach, who oversees the development and strategies of the team, and other team personnel such as assistant coaches, managers, statisticians, doctors and trainers.\nFor both men's and women's teams, a standard uniform consists of a pair of shorts and a jersey with a clearly visible number, unique within the team, printed on both the front and back. Players wear high-top sneakers that provide extra ankle support. Typically, team names, players' names and, outside of North America, sponsors are printed on the uniforms.\nA limited number of time-outs, clock stoppages requested by a coach (or sometimes mandated in the NBA) for a short meeting with the players, are allowed. They generally last no longer than one minute (100 seconds in the NBA) unless, for televised games, a commercial break is needed.\nThe game is controlled by the officials consisting of the referee (referred to as crew chief in the NBA), one or two umpires (referred to as referees in the NBA) and the table officials. For college, the NBA, and many high schools, there are a total of three referees on the court. The table officials are responsible for keeping track of each team's scoring, timekeeping, individual and team fouls, player substitutions, team possession arrow, and the shot clock.\nEquipment.\nThe only essential equipment in a basketball game is the ball and the court: a flat, rectangular surface with baskets at opposite ends. Competitive levels require the use of more equipment such as clocks, score sheets, scoreboards, alternating possession arrows, and whistle-operated stop-clock systems.\nA regulation basketball court in international games is long and wide. In the NBA and NCAA the court is . Most courts have wood flooring, usually constructed from maple planks running in the same direction as the longer court dimension. The name and logo of the home team is usually painted on or around the center circle.\nThe basket is a steel rim diameter with an attached net affixed to a backboard that measures and one basket is at each end of the court. The white outlined box on the backboard is high and wide. At almost all levels of competition, the top of the rim is exactly above the court and inside the baseline. While variation is possible in the dimensions of the court and backboard, it is considered important for the basket to be of the correct height \u2013 a rim that is off by just a few inches can have an adverse effect on shooting. The net must \"check the ball momentarily as it passes through the basket\" to aid the visual confirmation that the ball went through. The act of checking the ball has the further advantage of slowing down the ball so the rebound does not go as far.\nThe size of the basketball is also regulated. For men, the official ball is in circumference (size 7, or a \"295 ball\") and weighs . If women are playing, the official basketball size is in circumference (size 6, or a \"285 ball\") with a weight of . In 3x3, a formalized version of the halfcourt 3-on-3 game, a dedicated ball with the circumference of a size 6 ball but the weight of a size 7 ball is used in all competitions (men's, women's, and mixed teams).\nViolations.\nThe ball may be advanced toward the basket by being shot, passed between players, thrown, tapped, rolled or dribbled (bouncing the ball while running).\nThe ball must stay within the court; the last team to touch the ball before it travels out of bounds forfeits possession. The ball is out of bounds if it touches a boundary line, or touches any player or object that is out of bounds.\nThere are limits placed on the steps a player may take without dribbling, which commonly results in an infraction known as traveling. Nor may a player stop their dribble and then resume dribbling. A dribble that touches both hands is considered stopping the dribble, giving this infraction the name double dribble. Within a dribble, the player cannot carry the ball by placing their hand on the bottom of the ball; doing so is known as carrying the ball. A team, once having established ball control in the front half of their court, may not return the ball to the backcourt and be the first to touch it. A violation of these rules results in loss of possession.\nThe ball may not be kicked, nor be struck with the fist. For the offense, a violation of these rules results in loss of possession; for the defense, most leagues reset the shot clock and the offensive team is given possession of the ball out of bounds.\nThere are limits imposed on the time taken before progressing the ball past halfway (8 seconds in FIBA and the NBA; 10 seconds in NCAA and high school for both sexes), before attempting a shot (24 seconds in FIBA, the NBA, and U Sports (Canadian universities) play for both sexes, and 30 seconds in NCAA play for both sexes), holding the ball while closely guarded (5 seconds), and remaining in the restricted area known as the free-throw lane, (or the \"key\") (3 seconds). These rules are designed to promote more offense.\nThere are also limits on how players may block an opponent's field goal attempt or help a teammate's field goal attempt. Goaltending is a defender's touching of a ball that is on a downward flight toward the basket, while the related violation of basket interference is the touching of a ball that is on the rim or above the basket, or by a player reaching through the basket from below. Goaltending and basket interference committed by a defender result in awarding the basket to the offense, while basket interference committed by an offensive player results in cancelling the basket if one is scored. The defense gains possession in all cases of goaltending or basket interference.\nFouls.\nAn attempt to unfairly disadvantage an opponent through certain types of physical contact is illegal and is called a personal foul. These are most commonly committed by defensive players; however, they can be committed by offensive players as well. Players who are fouled either receive the ball to pass inbounds again, or receive one or more free throws if they are fouled in the act of shooting, depending on whether the shot was successful. One point is awarded for making a free throw, which is attempted from a line from the basket.\nThe referee is responsible for judging whether contact is illegal, sometimes resulting in controversy. The calling of fouls can vary between games, leagues and referees.\nThere is a second category of fouls called technical fouls, which may be charged for various rules violations including failure to properly record a player in the scorebook, or for unsportsmanlike conduct. These infractions result in one or two free throws, which may be taken by any of the five players on the court at the time. Repeated incidents can result in disqualification. A blatant foul involving physical contact that is either excessive or unnecessary is called an intentional foul (flagrant foul in the NBA). In FIBA and NCAA women's basketball, a foul resulting in ejection is called a disqualifying foul, while in leagues other than the NBA, such a foul is referred to as flagrant.\nIf a team exceeds a certain limit of team fouls in a given period (quarter or half) \u2013 four for NBA, NCAA women's, and international games \u2013 the opposing team is awarded one or two free throws on all subsequent non-shooting fouls for that period, the number depending on the league. In the US college men's game and high school games for both sexes, if a team reaches 7 fouls in a half, the opposing team is awarded one free throw, along with a second shot if the first is made. This is called shooting \"one-and-one\". If a team exceeds 10 fouls in the half, the opposing team is awarded two free throws on all subsequent fouls for the half.\nWhen a team shoots foul shots, the opponents may not interfere with the shooter, nor may they try to regain possession until the last or potentially last free throw is in the air.\nAfter a team has committed a specified number of fouls, the other team is said to be \"in the bonus\". On scoreboards, this is usually signified with an indicator light reading \"Bonus\" or \"Penalty\" with an illuminated directional arrow or dot indicating that team is to receive free throws when fouled by the opposing team. (Some scoreboards also indicate the number of fouls committed.)\nIf a team misses the first shot of a two-shot situation, the opposing team must wait for the completion of the second shot before attempting to reclaim possession of the ball and continuing play.\nIf a player is fouled while attempting a shot and the shot is unsuccessful, the player is awarded a number of free throws equal to the value of the attempted shot. A player fouled while attempting a regular two-point shot thus receives two shots, and a player fouled while attempting a three-point shot receives three shots.\nIf a player is fouled while attempting a shot and the shot is successful, typically the player will be awarded one additional free throw for one point. In combination with a regular shot, this is called a \"three-point play\" or \"four-point play\" (or more colloquially, an \"and one\") because of the basket made at the time of the foul (2 or 3 points) and the additional free throw (1 point).\nCommon techniques and practices.\nPositions.\nAlthough the rules do not specify any positions whatsoever, they have evolved as part of basketball. During the early years of basketball's evolution, two guards, two forwards, and one center were used. In more recent times specific positions evolved, but the current trend, advocated by many top coaches including Mike Krzyzewski, is towards positionless basketball, where big players are free to shoot from outside and dribble if their skill allows it. Popular descriptions of positions include:\nPoint guard (often called the \"1\") : usually the fastest player on the team, organizes the team's offense by controlling the ball and making sure that it gets to the right player at the right time.\nShooting guard (the \"2\") : creates a high volume of shots on offense, mainly long-ranged; and guards the opponent's best perimeter player on defense.\nSmall forward (the \"3\") : often primarily responsible for scoring points via cuts to the basket and dribble penetration; on defense seeks rebounds and steals, but sometimes plays more actively.\nPower forward (the \"4\"): plays offensively often with their back to the basket; on defense, plays under the basket (in a zone defense) or against the opposing power forward (in man-to-man defense).\nCenter (the \"5\"): uses height and size to score (on offense), to protect the basket closely (on defense), or to rebound.\nThe above descriptions are flexible. For most teams today, the shooting guard and small forward have very similar responsibilities and are often called the wings, as do the power forward and center, who are often called post players. While most teams describe two players as guards, two as forwards, and one as a center, on some occasions teams choose to call them by different designations.\nStrategy.\nThere are two main defensive strategies: \"zone defense\" and \"man-to-man defense\". In a zone defense, each player is assigned to guard a specific area of the court. Zone defenses often allow the defense to double team the ball, a manoeuver known as a trap. In a man-to-man defense, each defensive player guards a specific opponent.\nOffensive plays are more varied, normally involving planned passes and movement by players without the ball. A quick movement by an offensive player without the ball to gain an advantageous position is known as a \"cut\". A legal attempt by an offensive player to stop an opponent from guarding a teammate, by standing in the defender's way such that the teammate cuts next to him, is a \"screen\" or \"pick\". The two plays are combined in the \"pick and roll\", in which a player sets a pick and then \"rolls\" away from the pick towards the basket. Screens and cuts are very important in offensive plays; these allow the quick passes and teamwork, which can lead to a successful basket. Teams almost always have several offensive plays planned to ensure their movement is not predictable. On court, the point guard is usually responsible for indicating which play will occur.\nShooting.\nShooting is the act of attempting to score points by throwing the ball through the basket, methods varying with players and situations.\nTypically, a player faces the basket with both feet facing the basket. A player will rest the ball on the fingertips of the dominant hand (the shooting arm) slightly above the head, with the other hand supporting the side of the ball. The ball is usually shot by jumping (though not always) and extending the shooting arm. The shooting arm, fully extended with the wrist fully bent, is held stationary for a moment following the release of the ball, known as a \"follow-through\". Players often try to put a steady backspin on the ball to absorb its impact with the rim. The ideal trajectory of the shot is somewhat controversial, but generally a proper arc is recommended. Players may shoot directly into the basket or may use the backboard to redirect the ball into the basket.\nThe two most common shots that use the above described setup are the \"set shot\" and the \"jump shot\". Both are preceded by a crouching action which preloads the muscles and increases the power of the shot. In a set shot, the shooter straightens up and throws from a standing position with neither foot leaving the floor; this is typically used for free throws. For a jump shot, the throw is taken in mid-air with the ball being released near the top of the jump. This provides much greater power and range, and it also allows the player to elevate over the defender. Failure to release the ball before the feet return to the floor is considered a traveling violation.\nAnother common shot is called the \"layup\". This shot requires the player to be in motion toward the basket, and to \"lay\" the ball \"up\" and into the basket, typically off the backboard (the backboard-free, underhand version is called a \"finger roll\"). The most crowd-pleasing and typically highest-percentage accuracy shot is the \"slam dunk\", in which the player jumps very high and throws the ball downward, through the basket while touching it.\nAnother shot that is less common than the layup, is the \"circus shot\". The circus shot is a low-percentage shot that is flipped, heaved, scooped, or flung toward the hoop while the shooter is off-balance, airborne, falling down or facing away from the basket. A back-shot is a shot taken when the player is facing away from the basket, and may be shot with the dominant hand, or both; but there is a very low chance that the shot will be successful.\nA shot that misses both the rim and the backboard completely is referred to as an \"air ball\". A particularly bad shot, or one that only hits the backboard, is jocularly called a brick. The \"hang time\" is the length of time a player stays in the air after jumping, either to make a slam dunk, layup or jump shot.\nRebounding.\nThe objective of rebounding is to successfully gain possession of the basketball after a missed field goal or free throw, as it rebounds from the hoop or backboard. This plays a major role in the game, as most possessions end when a team misses a shot. There are two categories of rebounds: offensive rebounds, in which the ball is recovered by the offensive side and does not change possession, and defensive rebounds, in which the defending team gains possession of the loose ball. The majority of rebounds are defensive, as the team on defense tends to be in better position to recover missed shots; for example, about 75% of rebounds in the NBA are defensive.\nPassing.\nA pass is a method of moving the ball between players. Most passes are accompanied by a step forward to increase power and are followed through with the hands to ensure accuracy.\nA staple pass is the \"chest pass\". The ball is passed directly from the passer's chest to the receiver's chest. A proper chest pass involves an outward snap of the thumbs to add velocity and leaves the defence little time to react.\nAnother type of pass is the \"bounce pass\". Here, the passer bounces the ball crisply about two-thirds of the way from his own chest to the receiver. The ball strikes the court and bounces up toward the receiver. The bounce pass takes longer to complete than the chest pass, but it is also harder for the opposing team to intercept (kicking the ball deliberately is a violation). Thus, players often use the bounce pass in crowded moments, or to pass around a defender.\nThe \"overhead pass\" is used to pass the ball over a defender. The ball is released while over the passer's head.\nThe \"outlet pass\" occurs after a team gets a defensive rebound. The next pass after the rebound is the \"outlet pass\".\nThe crucial aspect of any good pass is it being difficult to intercept. Good passers can pass the ball with great accuracy and they know exactly where each of their other teammates prefers to receive the ball. A special way of doing this is passing the ball without looking at the receiving teammate. This is called a \"no-look pass\".\nAnother advanced style of passing is the \"behind-the-back pass\", which, as the description implies, involves throwing the ball behind the passer's back to a teammate. Although some players can perform such a pass effectively, many coaches discourage no-look or behind-the-back passes, believing them to be difficult to control and more likely to result in turnovers or violations.\nDribbling.\nDribbling is the act of bouncing the ball continuously with one hand and is a requirement for a player to take steps with the ball. To dribble, a player pushes the ball down towards the ground with the fingertips rather than patting it; this ensures greater control.\nWhen dribbling past an opponent, the dribbler should dribble with the hand farthest from the opponent, making it more difficult for the defensive player to get to the ball. It is therefore important for a player to be able to dribble competently with both hands.\nGood dribblers (or \"ball handlers\") tend to keep their dribbling hand low to the ground, reducing the distance of travel of the ball from the floor to the hand, making it more difficult for the defender to \"steal\" the ball. Good ball handlers frequently dribble behind their backs, between their legs, and switch directions suddenly, making a less predictable dribbling pattern that is more difficult to defend against. This is called a crossover, which is the most effective way to move past defenders while dribbling.\nA skilled player can dribble without watching the ball, using the dribbling motion or peripheral vision to keep track of the ball's location. By not having to focus on the ball, a player can look for teammates or scoring opportunities, as well as avoid the danger of having someone steal the ball away from him/her.\nBlocking.\nA block is performed when, after a shot is attempted, a defender succeeds in altering the shot by touching the ball. In almost all variants of play, it is illegal to touch the ball after it is in the downward path of its arc; this is known as \"goaltending\". It is also illegal under NBA and Men's NCAA basketball to block a shot after it has touched the backboard, or when any part of the ball is directly above the rim. Under international rules it is illegal to block a shot that is in the downward path of its arc or one that has touched the backboard until the ball has hit the rim. After the ball hits the rim, it is again legal to touch it even though it is no longer considered as a block performed.\nTo block a shot, a player has to be able to reach a point higher than where the shot is released. Thus, height can be an advantage in blocking. Players who are taller and playing the power forward or center positions generally record more blocks than players who are shorter and playing the guard positions. However, with good timing and a sufficiently high vertical leap, even shorter players can be effective shot blockers.\nHeight.\nAt the professional level, most male players are above and most women above . Guards, for whom physical coordination and ball-handling skills are crucial, tend to be the smallest players. Almost all forwards in the top men's pro leagues are or taller. Most centers are over tall. According to a survey given to all NBA teams, the average height of all NBA players is just under , with the average weight being close to . The tallest players ever in the NBA were Manute Bol and Gheorghe Mure\u0219an, who were both tall. At , Margo Dydek was the tallest player in the history of the WNBA.\nThe shortest player ever to play in the NBA is Muggsy Bogues at . Other average-height or relatively short players have thrived at the pro level, including Anthony \"Spud\" Webb, who was tall, but had a vertical leap, giving him significant height when jumping, and Temeka Johnson, who won the WNBA Rookie of the Year Award and a championship with the Phoenix Mercury while standing only . While shorter players are often at a disadvantage in certain aspects of the game, their ability to navigate quickly through crowded areas of the court and steal the ball by reaching low are strengths.\nPlayers regularly inflate their height in high school or college. Many prospects exaggerate their height while in high school or college to make themselves more appealing to coaches and scouts, who prefer taller players. Charles Barkley stated; \"I've been measured at 6\u20135, 6-. But I started in college at 6\u20136.\" Sam Smith, a former writer from the \"Chicago Tribune\", said: \"We sort of know the heights, because after camp, the sheet comes out. But you use that height, and the player gets mad. And then you hear from his agent. Or you file your story with the right height, and the copy desk changes it because they have the 'official' N.B.A. media guide, which is wrong. So you sort of go along with the joke.\"\nSince the 2019\u201320 NBA season heights of NBA players are recorded definitively by measuring players with their shoes off.\nVariations and similar games.\nVariations of basketball are activities based on the game of basketball, using common basketball skills and equipment (primarily the ball and basket). Some variations only have superficial rule changes, while others are distinct games with varying degrees of influence from basketball. Other variations include children's games, contests or activities meant to help players reinforce skills.\nAn earlier version of basketball, played primarily by women and girls, was six-on-six basketball. Horseball is a game played on horseback where a ball is handled and points are scored by shooting it through a high net (approximately 1.5m\u00d71.5m). The sport is like a combination of polo, rugby, and basketball. There is even a form played on donkeys known as Donkey basketball, which has attracted criticism from animal rights groups.\nHalf-court.\nPerhaps the single most common variation of basketball is the half-court game, played in informal settings without referees or strict rules. Only one basket is used, and the ball must be \"taken back\" or \"cleared\" \u2013 passed or dribbled outside the three-point line each time possession of the ball changes from one team to the other. Half-court games require less cardiovascular stamina, since players need not run back and forth a full court. Half-court raises the number of players that can use a court or, conversely, can be played if there is an insufficient number to form full 5-on-5 teams.\nHalf-court basketball is usually played 1-on-1, 2-on-2 or 3-on-3. The last of these variations is gradually gaining official recognition as 3x3, originally known as FIBA 33. It was first tested at the 2007 Asian Indoor Games in Macau and the first official tournaments were held at the 2009 Asian Youth Games and the 2010 Youth Olympics, both in Singapore. The first FIBA 3x3 Youth World Championships were held in Rimini, Italy in 2011, with the first FIBA 3x3 World Championships for senior teams following a year later in Athens. The sport is highly tipped to become an Olympic sport as early as 2016. In the summer of 2017, the BIG3 basketball league, a professional 3x3 half court basketball league that features former NBA players, began. The BIG3 features several rule variants including a four-point field goal.\nOther variations.\nVariations of basketball with their own page or subsection include:\nSpin-offs from basketball that are now separate sports include:\nSocial forms of basketball.\nBasketball as a social and communal sport features environments, rules and demographics different from those seen in professional and televised basketball.\nRecreational basketball.\nBasketball is played widely as an extracurricular, intramural or amateur sport in schools and colleges. Notable institutions of recreational basketball include:\nFantasy basketball.\nFantasy basketball was popularized during the 1990s by ESPN Fantasy Sports, NBA.com, and Yahoo! Fantasy Sports. On the model of fantasy baseball and football, players create fictional teams, select professional basketball players to \"play\" on these teams through a mock draft or trades, then calculate points based on the players' real-world performance."}
{"id": "3922", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=3922", "title": "Bicycling/Road", "text": ""}
{"id": "3926", "revid": "100426", "url": "https://en.wikipedia.org/wiki?curid=3926", "title": "Blowfish (disambiguation)", "text": "Blowfish are species of fish in the family Tetraodontidae.\nBlowfish may also refer to:"}
{"id": "3928", "revid": "48535834", "url": "https://en.wikipedia.org/wiki?curid=3928", "title": "Ball", "text": "A ball is a round object (usually spherical, but can sometimes be ovoid) with several uses. It is used in ball games, where the play of the game follows the state of the ball as it is hit, kicked or thrown by players. Balls can also be used for simpler activities, such as catch or juggling. Balls made from hard-wearing materials are used in engineering applications to provide very low friction bearings, known as ball bearings. Black-powder weapons use stone and metal balls as projectiles.\nAlthough many types of balls are today made from rubber, this form was unknown outside the Americas until after the voyages of Columbus. The Spanish were the first Europeans to see the bouncing rubber balls (although solid and not inflated) which were employed most notably in the Mesoamerican ballgame. Balls used in various sports in other parts of the world prior to Columbus were made from other materials such as animal bladders or skins, stuffed with various materials.\nAs balls are one of the most familiar spherical objects to humans, the word \"ball\" may refer to or describe spherical or near-spherical objects.\n\"Ball\" is used metaphorically sometimes to denote something spherical or spheroid, e.g., armadillos and human beings curl up into a ball, making a fist into a ball.\nEtymology.\nThe first known use of the word \"ball\" in English in the sense of a globular body that is played with was in 1205 in \"Layamon's Brut, or Chronicle of Britain\" in the phrase, \"\" (\"Some of them drove balls far across the fields.\") The word came from the Middle English \"bal\" (inflected as \"ball-e, -es\"), in turn from Old Norse \"b\u00f6llr\" (pronounced ; compare Old Swedish \"baller\", and Swedish \"boll\") from Proto-Germanic \"ballu-z\" (whence probably Middle High German \"bal, ball-es\", Middle Dutch \"bal\"), a cognate with Old High German \"ballo, pallo\", Middle High German balle from Proto-Germanic \"*ballon\" (weak masculine), and Old High German \"ball\u00e2, pall\u00e2\", Middle High German \"balle\", Proto-Germanic \"*ball\u00f4n\" (weak feminine). No Old English representative of any of these is known. (The answering forms in Old English would have been \"beallu, -a, -e\"\u2014compare \"bealluc, ballock\".) If \"ball-\" was native in Germanic, it may have been a cognate with the Latin \"foll-is\" in sense of a \"thing blown up or inflated.\" In the later Middle English spelling \"balle\" the word coincided graphically with the French \"balle\" \"ball\" and \"bale\" which has hence been erroneously assumed to be its source. French \"balle\" (but not \"boule\") is assumed to be of Germanic origin, itself, however. In Ancient Greek the word \u03c0\u03ac\u03bb\u03bb\u03b1 (\"palla\") for \"ball\" is attested besides the word \u03c3\u03c6\u03b1\u03af\u03c1\u03b1 (\"sfa\u00edra\"), \"sphere\".\nHistory.\nSome form of game with a ball is found portrayed on Egyptian monuments. In Homer, Nausicaa was playing at ball with her maidens when Odysseus first saw her in the land of the Phaeacians (Od. vi. 100). And Halios and Laodamas performed before Alcinous and Odysseus with ball play, accompanied with dancing (Od. viii. 370). The most ancient balls in Eurasia have been discovered in Karasahr, China and are 3000 years old. They were made of hair-filled leather.\nAncient Greeks.\nAmong the ancient Greeks, games with balls (\u03c3\u03c6\u03b1\u1fd6\u03c1\u03b1\u03b9) were regarded as a useful subsidiary to the more violent athletic exercises, as a means of keeping the body supple, and rendering it graceful, but were generally left to boys and girls. Of regular rules for the playing of ball games, little trace remains, if there were any such. The names in Greek for various forms, which have come down to us in such works as the \u1f48\u03bd\u03bf\u03bc\u03b1\u03c3\u03c4\u03b9\u03ba\u03cc\u03bd of Julius Pollux, imply little or nothing of such; thus, \u1f00\u03c0\u03cc\u03c1\u03c1\u03b1\u03be\u03b9\u03c2 (\"aporraxis\") only means the putting of the ball on the ground with the open hand, \u03bf\u1f50\u03c1\u03b1\u03bd\u03af\u03b1 (\"ourania\"), the flinging of the ball in the air to be caught by two or more players; \u03c6\u03b1\u03b9\u03bd\u03af\u03bd\u03b4\u03b1 (\"phaininda\") would seem to be a game of catch played by two or more, where feinting is used as a test of quickness and skill. Pollux (i. x. 104) mentions a game called episkyros (\u1f10\u03c0\u03af\u03c3\u03ba\u03c5\u03c1\u03bf\u03c2), which has often been looked on as the origin of football. It seems to have been played by two sides, arranged in lines; how far there was any form of \"goal\" seems uncertain. It was impossible to produce a ball that was perfectly spherical; children usually made their own balls by inflating pig's bladders and heating them in the ashes of a fire to make them rounder, although Plato (fl. 420s BC \u2013 340s BC) described \"balls which have leather coverings in twelve pieces\".\nAncient Romans.\nAmong the Romans, ball games were looked upon as an adjunct to the bath, and were graduated to the age and health of the bathers, and usually a place (sphaeristerium) was set apart for them in the baths (thermae). There appear to have been three types or sizes of ball, the pila, or small ball, used in catching games, the paganica, a heavy ball stuffed with feathers, and the follis, a leather ball filled with air, the largest of the three. This was struck from player to player, who wore a kind of gauntlet on the arm. There was a game known as trigon, played by three players standing in the form of a triangle, and played with the follis, and also one known as harpastum, which seems to imply a \"scrimmage\" among several players for the ball. These games are known to us through the Romans, though the names are Greek.\nModern ball games.\nThe various modern games played with a ball or balls and subject to rules are treated under their various names, such as polo, cricket, football, etc.\nPhysics.\nIn sports, many modern balls are pressurized. Some are pressurized at the factory (e.g. tennis, squash (sport)) and others are pressurized by users (e.g. volleyball, basketball, football). Almost all pressurized balls gradually leak air. If the ball is factory pressurized, there is usually a rule about whether the ball retains sufficient pressure to remain playable. Depressurized balls lack bounce and are often termed \"dead\". In extreme cases, a dead ball becomes flaccid. If the ball is pressured on use, there are generally rules about how the ball is pressurized before the match, and when (or whether) the ball can be repressurized or replaced.\nDue to the ideal gas law, ball pressure is a function of temperature, generally tracking ambient conditions. Softer balls that are struck hard (especially squash balls) increase in temperature due to inelastic collision.\nIn outdoor sports, wet balls play differently than dry balls. In indoor sports, balls may become damp due to hand sweat. Any form of humidity or dampness will affect a ball's surface friction, which will alter a player's ability to impart spin on the ball. The action required to apply spin to a ball is governed by the physics of angular momentum. Spinning balls travelling through air (technically a fluid) will experience the Magnus effect, which can produce lateral deflections in addition to the normal up-down curvature induced by a combination of wind resistance and gravity."}
{"id": "3931", "revid": "42223875", "url": "https://en.wikipedia.org/wiki?curid=3931", "title": "Binary relation", "text": "In mathematics, a binary relation associates elements of one set called the \"domain\" with elements of another set called the \"codomain\". Precisely, a binary relation over sets formula_1 and formula_2 is a set of ordered pairs formula_3 where formula_4 is in formula_1 and formula_6 is in formula_2. It encodes the common concept of relation: an element formula_4 is \"related\" to an element formula_6, if and only if the pair formula_3 belongs to the set of ordered pairs that defines the binary relation.\nAn example of a binary relation is the \"divides\" relation over the set of prime numbers formula_11 and the set of integers formula_12, in which each prime formula_13 is related to each integer formula_14 that is a multiple of formula_13, but not to an integer that is not a multiple of formula_13. In this relation, for instance, the prime number formula_17 is related to numbers such as formula_18, formula_19, formula_20, formula_21, but not to formula_22 or formula_23, just as the prime number formula_24 is related to formula_19, formula_20, and formula_23, but not to formula_28 or formula_29.\nBinary relations, and especially homogeneous relations, are used in many branches of mathematics to model a wide variety of concepts. These include, among others:\nA function may be defined as a binary relation that meets additional constraints. Binary relations are also heavily used in computer science.\nA binary relation over sets formula_1 and formula_2 is an element of the power set of formula_32 Since the latter set is ordered by inclusion (formula_33), each relation has a place in the lattice of subsets of formula_32 A binary relation is called a \"homogeneous relation\" when formula_35. A binary relation is also called a \"heterogeneous relation\" when it is not necessary that formula_35.\nSince relations are sets, they can be manipulated using set operations, including union, intersection, and complementation, and satisfying the laws of an algebra of sets. Beyond that, operations like the converse of a relation and the composition of relations are available, satisfying the laws of a calculus of relations, for which there are textbooks by Ernst Schr\u00f6der, Clarence Lewis, and Gunther Schmidt. A deeper analysis of relations involves decomposing them into subsets called \"concepts\", and placing them in a complete lattice.\nIn some systems of axiomatic set theory, relations are extended to classes, which are generalizations of sets. This extension is needed for, among other things, modeling the concepts of \"is an element of\" or \"is a subset of\" in set theory, without running into logical inconsistencies such as Russell's paradox.\nA binary relation is the most studied special case formula_37 of an formula_38-ary relation over sets formula_39, which is a subset of the Cartesian product formula_40\nDefinition.\nGiven sets formula_1 and formula_2, the Cartesian product formula_43 is defined as formula_44 and its elements are called \"ordered pairs\".\nA formula_45 over sets formula_1 and formula_2 is a subset of formula_32 The set formula_1 is called the or of formula_45, and the set formula_2 the or of formula_45. In order to specify the choices of the sets formula_1 and formula_2, some authors define a or as an ordered triple formula_55, where formula_56 is a subset of formula_43 called the of the binary relation. The statement formula_58 reads \"formula_4 is formula_45-related to formula_6\" and is denoted by formula_62. The or of formula_45 is the set of all formula_4 such that formula_62 for at least one formula_6. The \"codomain of definition\", , or of formula_45 is the set of all formula_6 such that formula_62 for at least one formula_4. The of formula_45 is the union of its domain of definition and its codomain of definition.\nWhen formula_72 a binary relation is called a (or ). To emphasize the fact that formula_1 and formula_2 are allowed to be different, a binary relation is also called a heterogeneous relation. The prefix \"hetero\" is from the Greek \u1f15\u03c4\u03b5\u03c1\u03bf\u03c2 (\"heteros\", \"other, another, different\").\nA heterogeneous relation has been called a rectangular relation, suggesting that it does not have the square-like symmetry of a homogeneous relation on a set where formula_75 Commenting on the development of binary relations beyond homogeneous relations, researchers wrote, \"...\u00a0a variant of the theory has evolved that treats relations from the very beginning as or , i.e. as relations where the normal case is that they are relations between different sets.\"\nThe terms \"correspondence\", dyadic relation and two-place relation are synonyms for binary relation, though some authors use the term \"binary relation\" for any subset of a Cartesian product formula_43 without reference to formula_1 and formula_2, and reserve the term \"correspondence\" for a binary relation with reference to formula_1 and formula_2.\nIn a binary relation, the order of the elements is important; if formula_81 then formula_82 can be true or false independently of formula_62. For example, formula_24 divides formula_23, but formula_23 does not divide formula_24.\nOperations.\nUnion.\nIf formula_45 and formula_89 are binary relations over sets formula_1 and formula_2 then formula_92 is the of formula_45 and formula_89 over formula_1 and formula_2.\nThe identity element is the empty relation. For example, formula_97 is the union of &lt; and =, and formula_98 is the union of &gt; and =.\nIntersection.\nIf formula_45 and formula_89 are binary relations over sets formula_1 and formula_2 then formula_103 is the of formula_45 and formula_89 over formula_1 and formula_2.\nThe identity element is the universal relation. For example, the relation \"is divisible by 6\" is the intersection of the relations \"is divisible by 3\" and \"is divisible by 2\".\nComposition.\nIf formula_45 is a binary relation over sets formula_1 and formula_2, and formula_89 is a binary relation over sets formula_2 and formula_113 then formula_114 (also denoted by formula_115) is the of formula_45 and formula_89 over formula_1 and formula_113.\nThe identity element is the identity relation. The order of formula_45 and formula_89 in the notation formula_122 used here agrees with the standard notational order for composition of functions. For example, the composition (is parent of)formula_123(is mother of) yields (is maternal grandparent of), while the composition (is mother of)formula_123(is parent of) yields (is grandmother of). For the former case, if formula_4 is the parent of formula_6 and formula_6 is the mother of formula_14, then formula_4 is the maternal grandparent of formula_14.\nConverse.\nIf formula_45 is a binary relation over sets formula_1 and formula_2 then formula_134 is the , also called , of formula_45 over formula_2 and formula_1.\nFor example, formula_138 is the converse of itself, as is formula_139, and formula_140 and formula_141 are each other's converse, as are formula_97 and formula_98. A binary relation is equal to its converse if and only if it is symmetric.\nComplement.\nIf formula_45 is a binary relation over sets formula_1 and formula_2 then formula_147 (also denoted by formula_148) is the of formula_45 over formula_1 and formula_2.\nFor example, formula_138 and formula_139 are each other's complement, as are formula_33 and formula_155, formula_156 and formula_157, formula_158 and formula_159, and for total orders also formula_140 and formula_98, and formula_141 and formula_97.\nThe complement of the converse relation formula_164 is the converse of the complement: formula_165\nIf formula_72 the complement has the following properties:\nRestriction.\nIf formula_45 is a binary homogeneous relation over a set formula_1 and formula_89 is a subset of formula_1 then formula_171 is the of formula_45 to formula_89 over formula_1.\nIf formula_45 is a binary relation over sets formula_1 and formula_2 and if formula_89 is a subset of formula_1 then formula_180 is the of formula_45 to formula_89 over formula_1 and formula_2.\nIf a relation is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), or an equivalence relation, then so too are its restrictions.\nHowever, the transitive closure of a restriction is a subset of the restriction of the transitive closure, i.e., in general not equal. For example, restricting the relation \"formula_4 is parent of formula_6\" to females yields the relation \"formula_4 is mother of the woman formula_6\"; its transitive closure does not relate a woman with her paternal grandmother. On the other hand, the transitive closure of \"is parent of\" is \"is ancestor of\"; its restriction to females does relate a woman with her paternal grandmother.\nAlso, the various concepts of completeness (not to be confused with being \"total\") do not carry over to restrictions. For example, over the real numbers a property of the relation formula_97 is that every non-empty subset formula_190 with an upper bound in formula_191 has a least upper bound (also called supremum) in formula_192 However, for the rational numbers this supremum is not necessarily rational, so the same property does not hold on the restriction of the relation formula_97 to the rational numbers.\nA binary relation formula_45 over sets formula_1 and formula_2 is said to be a relation formula_89 over formula_1 and formula_2, written formula_200 if formula_45 is a subset of formula_89, that is, for all formula_203 and formula_204 if formula_62, then formula_206. If formula_45 is contained in formula_89 and formula_89 is contained in formula_45, then formula_45 and formula_89 are called written formula_213. If formula_45 is contained in formula_89 but formula_89 is not contained in formula_45, then formula_45 is said to be than formula_89, written formula_220 For example, on the rational numbers, the relation formula_141 is smaller than formula_98, and equal to the composition formula_223.\nMatrix representation.\nBinary relations over sets formula_1 and formula_2 can be represented algebraically by logical matrices indexed by formula_1 and formula_2 with entries in the Boolean semiring (addition corresponds to OR and multiplication to AND) where matrix addition corresponds to union of relations, matrix multiplication corresponds to composition of relations (of a relation over formula_1 and formula_2 and a relation over formula_2 and formula_113), the Hadamard product corresponds to intersection of relations, the zero matrix corresponds to the empty relation, and the matrix of ones corresponds to the universal relation. Homogeneous relations (when formula_35) form a matrix semiring (indeed, a matrix semialgebra over the Boolean semiring) where the identity matrix corresponds to the identity relation.\nTypes of binary relations.\nSome important types of binary relations formula_45 over sets formula_1 and formula_2 are listed below.\nUniqueness properties:\nTotality properties (only definable if the domain formula_1 and codomain formula_2 are specified):\nUniqueness and totality properties (only definable if the domain formula_1 and codomain formula_2 are specified):\nIf relations over proper classes are allowed:\nSets versus classes.\nCertain mathematical \"relations\", such as \"equal to\", \"subset of\", and \"member of\", cannot be understood to be binary relations as defined above, because their domains and codomains cannot be taken to be sets in the usual systems of axiomatic set theory. For example, to model the general concept of \"equality\" as a binary relation formula_138, take the domain and codomain to be the \"class of all sets\", which is not a set in the usual set theory.\nIn most mathematical contexts, references to the relations of equality, membership and subset are harmless because they can be understood implicitly to be restricted to some set in the context. The usual work-around to this problem is to select a \"large enough\" set formula_292, that contains all the objects of interest, and work with the restriction formula_293 instead of formula_138. Similarly, the \"subset of\" relation formula_33 needs to be restricted to have domain and codomain formula_296 (the power set of a specific set formula_292): the resulting set relation can be denoted by formula_298 Also, the \"member of\" relation needs to be restricted to have domain formula_292 and codomain formula_296 to obtain a binary relation formula_301 that is a set. Bertrand Russell has shown that assuming formula_158 to be defined over all sets leads to a contradiction in naive set theory, see \"Russell's paradox\".\nAnother solution to this problem is to use a set theory with proper classes, such as NBG or Morse\u2013Kelley set theory, and allow the domain and codomain (and so the graph) to be proper classes: in such a theory, equality, membership, and subset are binary relations without special comment. (A minor modification needs to be made to the concept of the ordered triple formula_55, as normally a proper class cannot be a member of an ordered tuple; or of course one can identify the binary relation with its graph in this context.) With this definition one can for instance define a binary relation over every set and its power set.\nHomogeneous relation.\nA homogeneous relation over a set formula_1 is a binary relation over formula_1 and itself, i.e. it is a subset of the Cartesian product formula_306 It is also simply called a (binary) relation over formula_1.\nA homogeneous relation formula_45 over a set formula_1 may be identified with a directed simple graph permitting loops, where formula_1 is the vertex set and formula_45 is the edge set (there is an edge from a vertex formula_4 to a vertex formula_6 if and only if formula_62).\nThe set of all homogeneous relations formula_315 over a set formula_1 is the power set formula_317 which is a Boolean algebra augmented with the involution of mapping of a relation to its converse relation. Considering composition of relations as a binary operation on formula_315, it forms a semigroup with involution.\nSome important properties that a homogeneous relation formula_45 over a set formula_1 may have are:\nA is a relation that is reflexive, antisymmetric, and transitive. A is a relation that is irreflexive, asymmetric, and transitive. A is a relation that is reflexive, antisymmetric, transitive and connected. A is a relation that is irreflexive, asymmetric, transitive and connected.\nAn is a relation that is reflexive, symmetric, and transitive.\nFor example, \"formula_4 divides formula_6\" is a partial, but not a total order on natural numbers formula_358 \"formula_359\" is a strict total order on formula_358 and \"formula_4 is parallel to formula_6\" is an equivalence relation on the set of all lines in the Euclidean plane.\nAll operations defined in section also apply to homogeneous relations.\nBeyond that, a homogeneous relation over a set formula_1 may be subjected to closure operations like:\nCalculus of relations.\nDevelopments in algebraic logic have facilitated usage of binary relations. The calculus of relations includes the algebra of sets, extended by composition of relations and the use of converse relations. The inclusion formula_200 meaning that formula_371 implies formula_372, sets the scene in a lattice of relations. But since formula_373 the inclusion symbol is superfluous. Nevertheless, composition of relations and manipulation of the operators according to Schr\u00f6der rules, provides a calculus to work in the power set of formula_374\nIn contrast to homogeneous relations, the composition of relations operation is only a partial function. The necessity of matching target to source of composed relations has led to the suggestion that the study of heterogeneous relations is a chapter of category theory as in the category of sets, except that the morphisms of this category are relations. The of the category Rel are sets, and the relation-morphisms compose as required in a category.\nInduced concept lattice.\nBinary relations have been described through their induced concept lattices:\nA concept formula_375 satisfies two properties:\nFor a given relation formula_380 the set of concepts, enlarged by their joins and meets, forms an \"induced lattice of concepts\", with inclusion formula_381 forming a preorder.\nThe MacNeille completion theorem (1937) (that any partial order may be embedded in a complete lattice) is cited in a 2013 survey article \"Decomposition of relations on concept lattices\". The decomposition is\nParticular cases are considered below: formula_385 total order corresponds to Ferrers type, and formula_385 identity corresponds to difunctional, a generalization of equivalence relation on a set.\nRelations may be ranked by the Schein rank which counts the number of concepts necessary to cover a relation. Structural analysis of relations with concepts provides an approach for data mining.\nParticular relations.\nDifunctional.\nThe idea of a difunctional relation is to partition objects by distinguishing attributes, as a generalization of the concept of an equivalence relation. One way this can be done is with an intervening set formula_399 of indicators. The partitioning relation formula_400 is a composition of relations using relations formula_401 Jacques Riguet named these relations difunctional since the composition formula_402 involves functional relations, commonly called \"partial functions\".\nIn 1950 Riguet showed that such relations satisfy the inclusion:\nIn automata theory, the term rectangular relation has also been used to denote a difunctional relation. This terminology recalls the fact that, when represented as a logical matrix, the columns and rows of a difunctional relation can be arranged as a block matrix with rectangular blocks of ones on the (asymmetric) main diagonal. More formally, a relation formula_45 on formula_43 is difunctional if and only if it can be written as the union of Cartesian products formula_406, where the formula_407 are a partition of a subset of formula_1 and the formula_409 likewise a partition of a subset of formula_2.\nUsing the notation formula_411, a difunctional relation can also be characterized as a relation formula_45 such that wherever formula_413 and formula_414 have a non-empty intersection, then these two sets coincide; formally formula_415 implies formula_416\nIn 1997 researchers found \"utility of binary decomposition based on difunctional dependencies in database management.\" Furthermore, difunctional relations are fundamental in the study of bisimulations.\nIn the context of homogeneous relations, a partial equivalence relation is difunctional.\nFerrers type.\nA strict order on a set is a homogeneous relation arising in order theory.\nIn 1951 Jacques Riguet adopted the ordering of an integer partition, called a Ferrers diagram, to extend ordering to binary relations in general.\nThe corresponding logical matrix of a general binary relation has rows which finish with a sequence of ones. Thus the dots of a Ferrer's diagram are changed to ones and aligned on the right in the matrix.\nAn algebraic statement required for a Ferrers type relation R is\nformula_417\nIf any one of the relations formula_418 is of Ferrers type, then all of them are.\nContact.\nSuppose formula_419 is the power set of formula_292, the set of all subsets of formula_292. Then a relation formula_384 is a contact relation if it satisfies three properties: \nThe set membership relation, formula_426 \"is an element of\", satisfies these properties so formula_427 is a contact relation. The notion of a general contact relation was introduced by Georg Aumann in 1970.\nIn terms of the calculus of relations, sufficient conditions for a contact relation include\nformula_428 \nwhere formula_429 is the converse of set membership (formula_158).\nPreorder R\\R.\nEvery relation formula_45 generates a preorder formula_432 which is the left residual. In terms of converse and complements, formula_433 Forming the diagonal of formula_434, the corresponding row of formula_435 and column of formula_436 will be of opposite logical values, so the diagonal is all zeros. Then \nTo show transitivity, one requires that formula_439 Recall that formula_440 is the largest relation such that formula_441 Then \nThe inclusion relation \u03a9 on the power set of formula_447 can be obtained in this way from the membership relation formula_158 on subsets of formula_447:\nFringe of a relation.\nGiven a relation formula_45, its fringe is the sub-relation defined as\nformula_452\nWhen formula_45 is a partial identity relation, difunctional, or a block diagonal relation, then formula_454. Otherwise the formula_455 operator selects a boundary sub-relation described in terms of its logical matrix: formula_456 is the side diagonal if formula_45 is an upper right triangular linear order or strict order. formula_456 is the block fringe if formula_45 is irreflexive (formula_460) or upper right block triangular. formula_456 is a sequence of boundary rectangles when formula_45 is of Ferrers type.\nOn the other hand, formula_463 when formula_45 is a dense, linear, strict order.\nMathematical heaps.\nGiven two sets formula_292 and formula_419, the set of binary relations between them formula_467 can be equipped with a ternary operation formula_468 where formula_469 denotes the converse relation of formula_470. In 1953 Viktor Wagner used properties of this ternary operation to define semiheaps, heaps, and generalized heaps. The contrast of heterogeneous and homogeneous relations is highlighted by these definitions:"}
{"id": "3933", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=3933", "title": "Braille", "text": "Braille ( , ) is a tactile writing system used by people who are visually impaired. It can be read either on embossed paper or by using refreshable braille displays that connect to computers and smartphone devices. Braille can be written using a slate and stylus, a braille writer, an electronic braille notetaker or with the use of a computer connected to a braille embosser.\nBraille is named after its creator, Louis Braille, a Frenchman who lost his sight as a result of a childhood accident. In 1824, at the age of fifteen, he developed the braille code based on the French alphabet as an improvement on night writing. He published his system, which subsequently included musical notation, in 1829. The second revision, published in 1837, was the first binary form of writing developed in the modern era.\nBraille characters are formed using a combination of six raised dots arranged in a 3\u00a0\u00d7\u00a02 matrix, called the braille cell. The number and arrangement of these dots distinguishes one character from another. Since the various braille alphabets originated as transcription codes for printed writing, the mappings (sets of character designations) vary from language to language, and even within one; in English braille there are three levels: \"uncontracted\"a letter-by-letter transcription used for basic literacy; \"contracted\"an addition of abbreviations and contractions used as a space-saving mechanism; and \"grade\u00a03\" various non-standardized personal stenographies that are less commonly used.\nIn addition to braille text (letters, punctuation, contractions), it is also possible to create embossed illustrations and graphs, with the lines either solid or made of series of dots, arrows, and bullets that are larger than braille dots. A full braille cell includes six raised dots arranged in two columns, each column having three dots. The dot positions are identified by numbers from one to six. There are 64 possible combinations, including no dots at all for a word space. Dot configurations can be used to represent a letter, digit, punctuation mark, or even a word.\nEarly braille education is crucial to literacy, education and employment among the blind. Despite the evolution of new technologies, including screen reader software that reads information aloud, braille provides blind people with access to spelling, punctuation and other aspects of written language less accessible through audio alone.\nWhile some have suggested that audio-based technologies will decrease the need for braille, technological advancements such as braille displays have continued to make braille more accessible and available. Braille users highlight that braille remains as essential as print is to the sighted.\nHistory.\nBraille was based on a tactile code, now known as night writing, developed by Charles Barbier. (The name \"night writing\" was later given to it when it was considered as a means for soldiers to communicate silently at night and without a light source, but Barbier's writings do not use this term and suggest that it was originally designed as a simpler form of writing and for the visually impaired.) In Barbier's system, sets of 12 embossed dots were used to encode 36 different sounds. Braille identified three major defects of the code: first, the symbols represented phonetic sounds and not letters of the alphabetthus the code was unable to render the orthography of the words. Second, the 12-dot symbols could not easily fit beneath the pad of the reading finger. This required the reading finger to move in order to perceive the whole symbol, which slowed the reading process. (This was because Barbier's system was based only on the number of dots in each of two 6-dot columns, not the pattern of the dots.) Third, the code did not include symbols for numerals or punctuation. Braille's solution was to use 6-dot cells and to assign a specific pattern to each letter of the alphabet. Braille also developed symbols for representing numerals and punctuation.\nAt first, braille was a one-to-one transliteration of the French alphabet, but soon various abbreviations (contractions) and even logograms were developed, creating a system much more like shorthand.\nToday, there are braille codes for over 133 languages.\nIn English, some variations in the braille codes have traditionally existed among English-speaking countries. In 1991, work to standardize the braille codes used in the English-speaking world began. Unified English Braille (UEB) has been adopted in all seven member countries of the International Council on English Braille (ICEB) as well as Nigeria.\nFor blind readers, braille is an independent writing system, rather than a code of printed orthography.\nDerivation.\nBraille is derived from the Latin alphabet, albeit indirectly. In Braille's original system, the dot patterns were assigned to letters according to their position within the alphabetic order of the French alphabet of the time, with accented letters and \"w\" sorted at the end.\nUnlike print, which consists of mostly arbitrary symbols, the braille alphabet follows a logical sequence. The first ten letters of the alphabet, \"a\"\u2013\"j\", use the upper four dot positions: (black dots in the table below). These stand for the ten digits \"1\"\u2013\"9\" and \"0\" in an alphabetic numeral system similar to Greek numerals (as well as derivations of it, including Hebrew numerals, Cyrillic numerals, Abjad numerals, also Hebrew gematria and Greek isopsephy).\nThough the dots are assigned in no obvious order, the cells with the fewest dots are assigned to the first three letters (and lowest digits), \"abc\" = \"123\" (), and to the three vowels in this part of the alphabet, \"aei\" (), whereas the even digits \"4\", \"6\", \"8\", \"0\" () are right angles.\nThe next ten letters, \"k\"\u2013\"t\", are identical to \"a\"\u2013\"j\" respectively, apart from the addition of a dot at position 3 (red dots in the bottom left corners of the cells in the table below): :\nThe next ten letters (the next \"decade\") are the same again, but with dots also at both position 3 and position 6 (green dots in the bottom rows of the cells in the table above). Here \"w\" was left out as it was not part of the official French alphabet in Braille's time; the French order of the decade was \"u v x y z \u00e7 \u00e9 \u00e0 \u00e8 \u00f9\" ().\nThe next ten letters, ending in \"w\", are the same again, except that for this series position 6 (purple dot in the bottom right corner of the cell in the table above) is used without a dot at position 3. In French braille these are the letters \"\u00e2 \u00ea \u00ee \u00f4 \u00fb \u00eb \u00ef \u00fc \u0153 w\" (). \"W\" had been tacked onto the end of 39 letters of the French alphabet to accommodate English.\nThe \"a\"\u2013\"j\" series shifted down by one dot space () is used for punctuation. Letters \"a\" and \"c\" , which only use dots in the top row, were shifted two places for the apostrophe and hyphen: . (These are also the decade diacritics, on the left in the table below, of the second and third decade.)\nIn addition, there are ten patterns that are based on the first two letters () with their dots shifted to the right; these were assigned to non-French letters (\"\u00ec \u00e4 \u00f2\" ), or serve non-letter functions: (superscript; in English the accent mark), (currency prefix), (capital, in English the decimal point), (number sign), (emphasis mark), (symbol prefix).\nThe first four decades are similar in that the numeric sequence is extended by adding the decade dots, whereas in the fifth decade it is extended by shifting it downward.\nOriginally there had been nine decades. The fifth through ninth used dashes as well as dots, but they proved to be impractical to distinguish by touch under normal conditions and were soon abandoned. From the beginning, these additional decades could be substituted with what we now know as the number sign () applied to the earlier decades, though that only caught on for the digits (the old 5th decade being replaced by applied to the 1st decade). The dash occupying the top row of the original sixth decade was simply omitted, producing the modern fifth decade. (See 1829 braille.)\nAssignment.\nHistorically, there have been three principles in assigning the values of a linear script (print) to Braille: Using Louis Braille's original French letter values; reassigning the braille letters according to the sort order of the print alphabet being transcribed; and reassigning the letters to improve the efficiency of writing in braille.\nUnder international consensus, most braille alphabets follow the French sorting order for the 26 letters of the basic Latin alphabet, and there have been attempts at unifying the letters beyond these 26 (see international braille), though differences remain, for example, in German Braille. This unification avoids the chaos of each nation reordering the braille code to match the sorting order of its print alphabet, as happened in Algerian Braille, where braille codes were numerically reassigned to match the order of the Arabic alphabet and bear little relation to the values used in other countries (compare modern Arabic Braille, which uses the French sorting order), and as happened in an early American version of English Braille, where the letters \"w\", \"x\", \"y\", \"z\" were reassigned to match English alphabetical order. A convention sometimes seen for letters beyond the basic 26 is to exploit the physical symmetry of braille patterns iconically, for example, by assigning a reversed \"n\" to \"\u00f1\" or an inverted \"s\" to \"sh\". (See Hungarian Braille and Bharati Braille, which do this to some extent.)\nA third principle was to assign braille codes according to frequency, with the simplest patterns (quickest ones to write with a stylus) assigned to the most frequent letters of the alphabet. Such frequency-based alphabets were used in Germany and the United States in the 19th century (see American Braille), but with the invention of the braille typewriter their advantage disappeared, and none are attested in modern use they had the disadvantage that the resulting small number of dots in a text interfered with following the alignment of the letters, and consequently made texts more difficult to read than Braille's more arbitrary letter assignment. Finally, there are braille scripts that do not order the codes numerically at all, such as Japanese Braille and Korean Braille, which are based on more abstract principles of syllable composition.\nTexts are sometimes written in a script of eight dots per cell rather than six, enabling them to encode a greater number of symbols. (See Gardner\u2013Salinas braille codes.) Luxembourgish Braille has adopted eight-dot cells for general use; for example, accented letters take the unaccented versions plus dot 8.\nForm.\nBraille was the first writing system with binary encoding. The system as devised by Braille consists of two parts:\nWithin an individual cell, the dot positions are arranged in two columns of three positions. A raised dot can appear in any of the six positions, producing 64 (26) possible patterns, including one in which there are no raised dots. For reference purposes, a pattern is commonly described by listing the positions where dots are raised, the positions being universally numbered, from top to bottom, as 1 to 3 on the left and 4 to 6 on the right. For example, dot pattern 1-3-4 describes a cell with three dots raised, at the top and bottom in the left column and at the top of the right column: that is, the letter \"m\". The lines of horizontal braille text are separated by a space, much like visible printed text, so that the dots of one line can be differentiated from the braille text above and below. Different assignments of braille codes (or code pages) are used to map the character sets of different printed scripts to the six-bit cells. Braille assignments have also been created for mathematical and musical notation. However, because the six-dot braille cell allows only 64 (26) patterns, including space, the characters of a braille script commonly have multiple values, depending on their context. That is, character mapping between print and braille is not one-to-one. For example, the character corresponds in print to both the letter \"d\" and the digit \"4\".\nIn addition to simple encoding, many braille alphabets use contractions to reduce the size of braille texts and to increase reading speed. (See Contracted braille.)\nWriting braille.\nBraille may be produced by hand using a slate and stylus in which each dot is created from the back of the page, writing in mirror image, or it may be produced on a braille typewriter or Perkins Brailler, or an electronic Brailler or braille notetaker. Braille users with access to smartphones may also activate the on-screen braille input keyboard, to type braille symbols on to their device by placing their fingers on to the screen according to the dot configuration of the symbols they wish to form. These symbols are automatically translated into print on the screen. The different tools that exist for writing braille allow the braille user to select the method that is best for a given task. For example, the slate and stylus is a portable writing tool, much like the pen and paper for the sighted. Errors can be erased using a braille eraser or can be overwritten with all six dots (). \"Interpoint\" refers to braille printing that is offset, so that the paper can be embossed on both sides, with the dots on one side appearing between the divots that form the dots on the other.\nUsing a computer or other electronic device, Braille may be produced with a braille embosser (printer) or a refreshable braille display (screen).\nEight-dot braille.\nBraille has been extended to an 8-dot code, particularly for use with braille embossers and refreshable braille displays. In 8-dot braille the additional dots are added at the bottom of the cell, giving a matrix 4 dots high by 2 dots wide. The additional dots are given the numbers 7 (for the lower-left dot) and 8 (for the lower-right dot). Eight-dot braille has the advantages that the casing of each letter is coded in the cell and that every printable ASCII character can be encoded in a single cell. All 256 (28) possible combinations of 8 dots are encoded by the Unicode standard. Braille with six dots is frequently stored as Braille ASCII.\nLetters.\nThe first 25 braille letters, up through the first half of the 3rd decade, transcribe \"a\u2013z\" (skipping \"w\"). In English Braille, the rest of that decade is rounded out with the ligatures \"and, for, of, the,\" and \"with\". Omitting dot 3 from these forms the 4th decade, the ligatures \"ch, gh, sh, th, wh, ed, er, ou, ow\" and the letter \"w\".\nFormatting.\nVarious formatting marks affect the values of the letters that follow them. They have no direct equivalent in print. The most important in English Braille are:\nThat is, is read as capital 'A', and as the digit '1'.\nPunctuation.\nBasic punctuation marks in English Braille include:\n is both the question mark and the opening quotation mark. Its reading depends on whether it occurs before a word or after.\n is used for both opening and closing parentheses. Its placement relative to spaces and other characters determines its interpretation.\nPunctuation varies from language to language. For example, French Braille uses for its question mark and swaps the quotation marks and parentheses (to and ); it uses () for both the period and the decimal point, and the English decimal point () to mark capitalization.\nContractions.\nBraille contractions are words and affixes that are shortened so that they take up fewer cells. In English Braille, for example, the word \"afternoon\" is written with just three letters, , much like stenoscript. There are also several abbreviation marks that create what are effectively logograms. The most common of these is dot 5, which combines with the first letter of words. With the letter \"m\", the resulting word is \"mother\". There are also ligatures (\"contracted\" letters), which are single letters in braille but correspond to more than one letter in print. The letter \"and\", for example, is used to write words with the sequence \"a-n-d\" in them, such as \"grand\".\nPage dimensions.\nMost braille embossers support between 34 and 40 cells per line, and 25 lines per page.\nA manually operated Perkins braille typewriter supports a maximum of 42 cells per line (its margins are adjustable), and typical paper allows 25 lines per page.\nA large interlining Stainsby has 36 cells per line and 18 lines per page.\nAn A4-sized Marburg braille frame, which allows interpoint braille (dots on both sides of the page, offset so they do not interfere with each other), has 30 cells per line and 27 lines per page.\nBraille writing machine.\nA Braille writing machine is a typewriter with six keys that allows the user to write braille on a regular hard copy page.\nThe first Braille typewriter to gain general acceptance was invented by Frank Haven Hall (Superintendent of the Illinois School for the Blind), and was presented to the public in 1892.\nThe Stainsby Brailler, developed by Henry Stainsby in 1903, is a mechanical writer with a sliding carriage that moves over an aluminium plate as it embosses Braille characters. An improved version was introduced around 1933.\nIn 1951 David Abraham, a woodworking teacher at the Perkins School for the Blind, produced a more advanced Braille typewriter, the Perkins Brailler.\nBraille printers or embossers were produced in the 1950s.\nIn 1960 Robert Mann, a teacher in MIT, wrote DOTSYS, a software that allowed automatic braille translation, and another group created an embossing device called \"M.I.T. Braillemboss\". The Mitre Corporation team of Robert Gildea, Jonathan Millen, Reid Gerhart and Joseph Sullivan (now president of Duxbury Systems) developed DOTSYS III, the first braille translator written in a portable programming language. DOTSYS III was developed for the Atlanta Public Schools as a public domain program.\nIn 1991 Ernest Bate developed the Mountbatten Brailler, an electronic machine used to type braille on braille paper, giving it a number of additional features such as word processing, audio feedback and embossing. This version was improved in 2008 with a quiet writer that had an erase key.\nIn 2011 David S. Morgan produced the first SMART Brailler machine, with added text to speech function and allowed digital capture of data entered.\nBraille reading.\nBraille is traditionally read in hardcopy form, such as with paper books written in braille, documents produced in paper braille (such as restaurant menus), and braille labels or public signage. It can also be read on a refreshable braille display either as a stand-alone electronic device or connected to a computer or smartphone. Refreshable braille displays convert what is visually shown on a computer or smartphone screen into braille through a series of pins that rise and fall to form braille symbols. Currently more than 1% of all printed books have been translated into hardcopy braille.\nThe fastest braille readers apply a light touch and read braille with two hands, although reading braille with one hand is also possible. Although the finger can read only one braille character at a time, the brain chunks braille at a higher level, processing words a digraph, root or suffix at a time. The processing largely takes place in the visual cortex.\nLiteracy.\nChildren who are blind miss out on fundamental parts of early and advanced education if not provided with the necessary tools, such as access to educational materials in braille. Children who are blind or visually impaired can begin learning foundational braille skills from a very young age to become fluent braille readers as they get older. Sighted children are naturally exposed to written language on signs, on TV and in the books they see. Blind children require the same early exposure to literacy, through access to braille rich environments and opportunities to explore the world around them. Print-braille books, for example, present text in both print and braille and can be read by sighted parents to blind children (and vice versa), allowing blind children to develop an early love for reading even before formal reading instruction begins.\nAdults who experience sight loss later in life or who did not have the opportunity to learn it when they were younger can also learn braille. In most cases, adults who learn braille were already literate in print before vision loss and so instruction focuses more on developing the tactile and motor skills needed to read braille.\nWhile different countries publish statistics on how many readers in a given organization request braille, these numbers only provide a partial picture of braille literacy statistics. For example, this data does not survey the entire population of braille readers or always include readers who are no longer in the school system (adults) or readers who request electronic braille materials. Therefore, there are currently no reliable statistics on braille literacy rates, as described in a publication in the \"Journal of Visual Impairment and Blindness\". Regardless of the precise percentage of braille readers, there is consensus that braille should be provided to all those who benefit from it.\nNumerous factors influence access to braille literacy, including school budget constraints, technology advancements such as screen-reader software, access to qualified instruction, and different philosophical views over how blind children should be educated.\nIn the US, a key turning point for braille literacy was the passage of the Rehabilitation Act of 1973, an act of Congress that moved thousands of children from specialized schools for the blind into mainstream public schools. Because only a small percentage of public schools could afford to train and hire braille-qualified teachers, braille literacy has declined since the law took effect. Braille literacy rates have improved slightly since the bill was passed, in part because of pressure from consumers and advocacy groups that has led 27 states to pass legislation mandating that children who are legally blind be given the opportunity to learn braille.\nIn 1998 there were 57,425 legally blind students registered in the United States, but only 10% (5,461) of them used braille as their primary reading medium.\nEarly Braille education is crucial to literacy for a blind or low-vision child. A study conducted in the state of Washington found that people who learned braille at an early age did just as well as, if not better than, their sighted peers in several areas, including vocabulary and comprehension. In the preliminary adult study, while evaluating the correlation between adult literacy skills and employment, it was found that 44% of the participants who had learned to read in braille were unemployed, compared to the 77% unemployment rate of those who had learned to read using print. Currently, among the estimated 85,000 blind adults in the United States, 90% of those who are braille-literate are employed. Among adults who do not know braille, only 33% are employed. Statistically, history has proven that braille reading proficiency provides an essential skill set that allows blind or low-vision children to compete with their sighted peers in a school environment and later in life as they enter the workforce.\nRegardless of the specific percentage of braille readers, proponents point out the importance of increasing access to braille for all those who can benefit from it.\nBraille transcription.\nAlthough it is possible to transcribe print by simply substituting the equivalent braille character for its printed equivalent, in English such a character-by-character transcription (known as \"uncontracted braille\") is typically used by beginners or those who only engage in short reading tasks (such as reading household labels).\nBraille characters are much larger than their printed equivalents, and the standard page has room for only 25 lines of 43 characters. To reduce space and increase reading speed, most braille alphabets and orthographies use ligatures, abbreviations, and contractions. Virtually all English braille books in hardcopy (paper) format are transcribed in contracted braille: The Library of Congress's \"Instruction Manual for Braille Transcribing\" runs to over 300 pages, and braille transcribers must pass certification tests.\nUncontracted braille was previously known as grade 1 braille, and contracted braille was previously known as grade\u00a02 braille. Uncontracted braille is a direct transliteration of print words (one-to-one correspondence); hence, the word \"about\" would contain all the same letters in uncontracted braille as it does in inkprint. Contracted braille includes short forms to save space; hence, for example, the letters \"ab\" when standing alone represent the word \"about\" in English contracted braille. In English, some braille users only learn uncontracted braille, particularly if braille is being used for shorter reading tasks such as reading household labels. However, those who plan to use braille for educational and employment purposes and longer reading texts often go on to contracted braille.\nThe system of contractions in English Braille begins with a set of 23 words contracted to single characters. Thus the word \"but\" is contracted to the single letter \"b\", \"can\" to \"c\", \"do\" to \"d\", and so on. Even this simple rule creates issues requiring special cases; for example, \"d\" is, specifically, an abbreviation of the verb \"do\"; the noun \"do\" representing the note of the musical scale is a different word and must be spelled out.\nPortions of words may be contracted, and many rules govern this process. For example, the character with dots 2-3-5 (the letter \"f\" lowered in the Braille cell) stands for \"ff\" when used in the middle of a word. At the beginning of a word, this same character stands for the word \"to\"; the character is written in braille with no space following it. (This contraction was removed in the Unified English Braille Code.) At the end of a word, the same character represents an exclamation point.\nSome contractions are more similar than their print equivalents. For example, the contraction , meaning \"letter\", differs from , meaning \"little\", only by one dot in the second letter: \"little\", \"letter\". This causes greater confusion between the braille spellings of these words and can hinder the learning process of contracted braille.\nThe contraction rules take into account the linguistic structure of the word; thus, contractions are generally not to be used when their use would alter the usual braille form of a base word to which a prefix or suffix has been added. Some portions of the transcription rules are not fully codified and rely on the judgment of the transcriber. Thus, when the contraction rules permit the same word in more than one way, preference is given to \"the contraction that more nearly approximates correct pronunciation\".\n\"Grade 3 braille\" is a variety of non-standardized systems that include many additional shorthand-like contractions. They are not used for publication, but by individuals for their personal convenience.\nBraille translation software.\nWhen people produce braille, this is called braille transcription. When computer software produces braille, this is called a braille translator. Braille translation software exists to handle almost all of the common languages of the world, and many technical areas, such as mathematics (mathematical notation), for example WIMATS, music (musical notation), and tactile graphics.\nBraille reading techniques.\nSince Braille is one of the few writing systems where tactile perception is used, as opposed to visual perception, a braille reader must develop new skills. One skill important for Braille readers is the ability to create smooth and even pressures when running one's fingers along the words. There are many different styles and techniques used for the understanding and development of braille, even though a study by B. F. Holland suggests that there is no specific technique that is superior to any other.\nAnother study by Lowenfield &amp; Abel shows that braille can be read \"the fastest and best... by students who read using the index fingers of both hands\". Another important reading skill emphasized in this study is to finish reading the end of a line with the right hand and to find the beginning of the next line with the left hand simultaneously.\nInternational uniformity.\nWhen Braille was first adapted to languages other than French, many schemes were adopted, including mapping the native alphabet to the alphabetical order of French \u2013 e.g. in English W, which was not in the French alphabet at the time, is mapped to braille X, X to Y, Y to Z, and Z to the first French-accented letter \u2013 or completely rearranging the alphabet such that common letters are represented by the simplest braille patterns. Consequently, mutual intelligibility was greatly hindered by this state of affairs. In 1878, the International Congress on Work for the Blind, held in Paris, proposed an international braille standard, where braille codes for different languages and scripts would be based, not on the order of a particular alphabet, but on phonetic correspondence and transliteration to Latin.\nThis unified braille has been applied to the languages of India and Africa, Arabic, Vietnamese, Hebrew, Russian, and Armenian, as well as nearly all Latin-script languages. In Greek, for example, \u03b3 (g) is written as Latin \"g\", despite the fact that it has the alphabetic position of \"c\"; Hebrew \u05d1 (b), the second letter of the alphabet and cognate with the Latin letter \"b\", is sometimes pronounced /b/ and sometimes /v/, and is written \"b\" or \"v\" accordingly; Russian \u0446 (ts) is written as \"c\", which is the usual letter for /ts/ in those Slavic languages that use the Latin alphabet; and Arabic \u0641 (f) is written as \"f\", despite being historically \"p\" and occurring in that part of the Arabic alphabet (between historic \"o\" and \"q\").\nOther braille conventions.\nOther systems for assigning values to braille patterns are also followed beside the simple mapping of the alphabetical order onto the original French order. Some braille alphabets start with unified braille, and then diverge significantly based on the phonology of the target languages, while others diverge even further.\nIn the various Chinese systems, traditional braille values are used for initial consonants and the simple vowels. In both Mandarin and Cantonese Braille, however, characters have different readings depending on whether they are placed in syllable-initial (onset) or syllable-final (rime) position. For instance, the cell for Latin \"k\", , represents Cantonese \"k\" (\"g\" in Yale and other modern romanizations) when initial, but \"aak\" when final, while Latin \"j\", , represents Cantonese initial \"j\" but final \"oei\".\nNovel systems of braille mapping include Korean, which adopts separate syllable-initial and syllable-final forms for its consonants, explicitly grouping braille cells into syllabic groups in the same way as hangul. Japanese, meanwhile, combines independent vowel dot patterns and modifier consonant dot patterns into a single braille cell \u2013 an abugida representation of each Japanese mora.\nUses.\nBraille is read by people who are blind, deafblind or who have low vision, and by both those born with a visual impairment and those who experience sight loss later in life. Braille may also be used by print impaired people, who although may be fully sighted, due to a physical disability are unable to read print. Even individuals with low vision will find that they benefit from braille, depending on level of vision or context (for example, when lighting or colour contrast is poor). Braille is used for both short and long reading tasks. Examples of short reading tasks include braille labels for identifying household items (or cards in a wallet), reading elevator buttons, accessing phone numbers, recipes, grocery lists and other personal notes. Examples of longer reading tasks include using braille to access educational materials, novels and magazines. People with access to a refreshable braille display can also use braille for reading email and ebooks, browsing the internet and accessing other electronic documents. It is also possible to adapt or purchase playing cards and board games in braille.\nIn India there are instances where the parliament acts have been published in braille, such as \"The Right to Information Act\". Sylheti Braille is used in Northeast India.\nIn Canada, passenger safety information in braille and tactile seat row markers are required aboard planes, trains, large ferries, and interprovincial busses pursuant to the Canadian Transportation Agency's regulations.\nIn the United States, the Americans with Disabilities Act of 1990 requires various building signage to be in braille.\nIn the United Kingdom, it is required that medicines have the name of the medicine in Braille on the labeling.\nCurrency.\nThe current series of Canadian banknotes has a tactile feature consisting of raised dots that indicate the denomination, allowing bills to be easily identified by blind or low vision people. It does not use standard braille numbers to identify the value. Instead, the number of full braille cells, which can be simply counted by both braille readers and non-braille readers alike, is an indicator of the value of the bill.\nMexican bank notes, Australian bank notes, Indian rupee notes, Israeli new shekel notes and Russian ruble notes also have special raised symbols to make them identifiable by persons who are blind or have low vision.\nEuro coins were designed in cooperation with organisations representing blind people, and as a result they incorporate many features allowing them to be distinguished by touch alone. In addition, their visual appearance is designed to make them easy to tell apart for persons who cannot read the inscriptions on the coins. \"A good design for the blind and partially sighted is a good design for everybody\" was the principle behind the cooperation of the European Central Bank and the European Blind Union during the design phase of the first series Euro banknotes in the 1990s. As a result, the design of the first euro banknotes included several characteristics which aid both the blind and partially sighted to confidently use the notes.\nAustralia introduced the tactile feature onto their five-dollar banknote in 2016\nIn the United Kingdom, the front of the \u00a310 polymer note (the side with raised print), has two clusters of raised dots in the top left hand corner, and the \u00a320 note has three. This tactile feature helps blind and partially sighted people identify the value of the note.\nIn 2003 the US Mint introduced the commemorative Alabama State Quarter, which recognized State Daughter Helen Keller on the Obverse, including the name Helen Keller in both English script and Braille inscription. This appears to be the first known use of Braille on US Coin Currency, though not standard on all coins of this type.\nUnicode.\nThe Braille set was added to the Unicode Standard in version 3.0 (1999).\nMost braille embossers and refreshable braille displays do not use the Unicode code points, but instead reuse the 8-bit code points that are assigned to standard ASCII for braille ASCII. (Thus, for simple material, the same bitstream may be interpreted equally as visual letter forms for sighted readers or their exact semantic equivalent in tactile patterns for blind readers. However some codes have quite different tactile versus visual interpretations and most are not even defined in Braille ASCII.)\nSome embossers have proprietary control codes for 8-dot braille or for full graphics mode, where dots may be placed anywhere on the page without leaving any space between braille cells so that continuous lines can be drawn in diagrams, but these are rarely used and are not standard.\nThe Unicode standard encodes 6-dot and 8-dot braille glyphs according to their binary appearance, rather than following their assigned numeric order. Dot 1 corresponds to the least significant bit of the low byte of the Unicode scalar value, and dot 8 to the high bit of that byte.\nThe Unicode block for braille is U+2800 ... U+28FF. The mapping of patterns to characters etc. is language dependent: even for English for example, see American Braille and English Braille.\nObservation.\nEvery year on 4 January, World Braille Day is observed internationally to commemorate the birth of Louis Braille and to recognize his efforts. Although the event is not considered a public holiday, it has been recognized by the United Nations as an official day of celebration since 2019.\nBraille devices.\nThere is a variety of contemporary electronic devices that serve the needs of blind people that operate in Braille, such as refreshable braille displays and Braille e-books that use different technologies for transmitting graphic information of different types (pictures, maps, graphs, texts, etc.)."}
{"id": "3934", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=3934", "title": "Baby Boom", "text": ""}
{"id": "3935", "revid": "19382112", "url": "https://en.wikipedia.org/wiki?curid=3935", "title": "BuddHism", "text": ""}
{"id": "3936", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=3936", "title": "Bastille Day", "text": "Bastille Day is the common name given in English-speaking countries to the national day of France, which is celebrated on 14 July each year. It is referred to, both legally and commonly, as () in French, though \"la f\u00eate nationale\" is also used in the press.\nFrench National Day is the anniversary of the Storming of the Bastille on 14 July 1789, a major event of the French Revolution, as well as the that celebrated the unity of the French people on 14 July 1790. Celebrations are held throughout France. One that has been reported as \"the oldest and largest military parade in Europe\" is held on 14 July on the Champs-\u00c9lys\u00e9es in Paris in front of the President of France, along with other French officials and foreign guests.\nHistory.\nIn 1789, tensions rose in France between reformist and conservative factions as the country struggled to resolve an economic crisis. In May, the Estates General legislative assembly was revived, but members of the Third Estate broke ranks, declaring themselves to be the National Assembly of the country, and on 20 June, vowed to write a constitution for the kingdom. \nOn 11 July, Jacques Necker, the finance minister of Louis XVI, who was sympathetic to the Third Estate, was dismissed by the King, provoking an angry reaction among Parisians. Crowds formed, fearful of an attack by the royal army or by foreign regiments of mercenaries in the King's service and seeking to arm themselves. Early on 14 July, a crowd besieged the H\u00f4tel des Invalides for firearms, muskets, and cannons stored in its cellars. That same day, another crowd stormed the Bastille, a fortress-prison in Paris that had historically held people jailed on the basis of \"lettres de cachet\" (literally \"signet letters\"), arbitrary royal indictments that could not be appealed and did not indicate the reason for the imprisonment, and was believed to hold a cache of ammunition and gunpowder. As it happened, at the time of the attack, the Bastille held only seven inmates, none of great political significance. \nThe crowd was eventually reinforced by the mutinous R\u00e9giment des Gardes Fran\u00e7aises (\"Regiment of French Guards\"), whose usual role was to protect public buildings. They proved a fair match for the fort's defenders, and Governor de Launay, the commander of the Bastille, capitulated and opened the gates to avoid a mutual massacre. According to the official documents, about 200 attackers and just one defender died before the capitulation. However, possibly because of a misunderstanding, fighting resumed. In this second round of fighting, de Launay and seven other defenders were killed, as was Jacques de Flesselles, the \"pr\u00e9v\u00f4t des marchands\" (\"provost of the merchants\"), the elected head of the city's guilds, who under the French monarchy had the responsibilities of a present-day mayor.\nShortly after the storming of the Bastille, late in the evening of 4 August, after a very stormy session of the \"Assembl\u00e9e constituante\", feudalism was abolished. On 26 August, the Declaration of the Rights of Man and of the Citizen (\"D\u00e9claration des Droits de l'Homme et du Citoyen\") was proclaimed.\n\"F\u00eate de la F\u00e9d\u00e9ration\".\nAs early as 1789, the year of the storming of the Bastille, preliminary designs for a national festival were underway. These designs were intended to strengthen the country's national identity through the celebration of the events of 14 July 1789. One of the first designs was proposed by Cl\u00e9ment Gonchon, a French textile worker, who presented his design for a festival celebrating the anniversary of the storming of the Bastille to the French city administration and the public on 9 December 1789. There were other proposals and unofficial celebrations of 14 July 1789, but the official festival sponsored by the National Assembly was called the F\u00eate de la F\u00e9d\u00e9ration.\nThe \"F\u00eate de la F\u00e9d\u00e9ration\" on 14 July 1790 was a celebration of the unity of the French nation during the French Revolution. The aim of this celebration, one year after the Storming of the Bastille, was to symbolize peace. The event took place on the Champ de Mars, which was located far outside of Paris at the time. The work needed to transform the Champ de Mars into a suitable location for the celebration was not on schedule to be completed in time. On the day recalled as the Journ\u00e9e des brouettes (\"The Day of the Wheelbarrow\"), thousands of Parisian citizens gathered together to finish the construction needed for the celebration.\nThe day of the festival, the National Guard assembled and proceeded along the boulevard du Temple in the pouring rain, and were met by an estimated 260,000 Parisian citizens at the Champ de Mars. A mass was celebrated by Talleyrand, bishop of Autun. The popular General Lafayette, as captain of the National Guard of Paris and a confidant of the king, took his oath to the constitution, followed by King Louis XVI. After the end of the official celebration, the day ended in a huge four-day popular feast, and people celebrated with fireworks, as well as fine wine and running nude through the streets in order to display their freedom.\nOrigin of the current celebration.\nOn 30 June 1878, a feast was officially arranged in Paris to honour the French Republic (the event was commemorated in a painting by Claude Monet). On 14 July 1879, there was another feast, with a semi-official aspect. The day's events included a reception in the Chamber of Deputies, organised and presided over by L\u00e9on Gambetta (a military reviewer at Longchamp), and a Republican Feast in the Pr\u00e9 Catelan. All throughout France, \"Le Figaro\" wrote, \"people feasted much to honour the storming of the Bastille\".\nIn 1880, the government of the Third Republic wanted to revive the 14 July festival. The campaign for the reinstatement of the festival was sponsored by the notable politician L\u00e9on Gambetta and scholar Henri Baudrillant. On 21 May 1880, Benjamin Raspail proposed a law, signed by sixty-four members of government, to have \"the Republic adopt 14 July as the day of an annual national festival\". There were many disputes over which date to be remembered as the national holiday, including 4 August (the commemoration of the end of the feudal system), 5 May (when the Estates-General first assembled), 27 July (the fall of Robespierre), and 21 January (the date of Louis XVI's execution). The government decided that the date of the holiday would be 14 July, but that was still somewhat problematic. The events of 14 July 1789 were illegal under the previous government, which contradicted the Third Republic's need to establish legal legitimacy. French politicians also did not want the sole foundation of their national holiday to be rooted in a day of bloodshed and class-hatred as the day of storming the Bastille was. Instead, they based the establishment of the holiday as both the celebration of the F\u00eate de la F\u00e9d\u00e9ration, a festival celebrating the anniversary of the Republic of France on 14 July 1789, and the storming of the Bastille. The Assembly voted in favor of the proposal on 21 May, and 8 June. The law was approved on 27 and 29 June. The celebration was made official on 6 July 1880.\nIn the debate leading up to the adoption of the holiday, Senator Henri Martin, who wrote the National Day law, addressed the chamber on 29 June 1880:\nBastille Day military parade.\nThe Bastille Day military parade is the French military parade that has been held in the morning, every year in Paris, since 1880. While previously held elsewhere within or near the capital city, since 1918 it has been held on the Champs-\u00c9lys\u00e9es, with the participation of the Allies as represented in the Versailles Peace Conference, and with the exception of the period of German occupation from 1940 to 1944 (when the ceremony took place in London under the command of General Charles de Gaulle); and 2020 when the COVID-19 pandemic forced its cancellation. The parade passes down the Champs-\u00c9lys\u00e9es from the Arc de Triomphe to the Place de la Concorde, where the President of the French Republic, his government and foreign ambassadors to France stand. This is a popular event in France, broadcast on French TV, and is the oldest and largest regular military parade in Europe.\nSmaller military parades are held in French garrison towns, including Toulon and Belfort, with local troops.\nBastille Day celebrations in other countries.\nBelgium.\nLi\u00e8ge celebrates Bastille Day each year since the end of the First World War, as Li\u00e8ge was decorated by the L\u00e9gion d'Honneur for its unexpected resistance during the Battle of Li\u00e8ge. The city also hosts a fireworks show outside of Congress Hall. Specifically in Li\u00e8ge, celebrations of Bastille Day have been known to be bigger than the celebrations of the Belgian National holiday. Around 35,000 people gather to celebrate Bastille Day. There is a traditional festival dance of the French consul that draws large crowds, and many unofficial events over the city celebrate the relationship between France and the city of Li\u00e8ge.\nCanada.\nVancouver, British Columbia holds a celebration featuring exhibits, food and entertainment. The Toronto Bastille Day festival is also celebrated in Toronto, Ontario. The festival is organized by the French-Canadian community in Toronto and sponsored by the Consulate General of France. The celebration includes music, performances, sport competitions, and a French Market. At the end of the festival, there is also a traditional French bal populaire.\nCzech Republic.\nSince 2008, Prague has hosted a French market \"\" (\"Fourteenth of July Market\") offering traditional French food and wine as well as music. The market takes place on Kampa Island, it is usually between 11 and 14 July. It acts as an event that marks the relinquish of the EU presidency from France to the Czech Republic. Traditional selections of French produce, including cheese, wine, meat, bread and pastries, are provided by the market. Throughout the event, live music is played in the evenings, with lanterns lighting up the square at night.\nDenmark.\nThe amusement park Tivoli celebrates Bastille Day.\nHungary.\nBudapest's two-day celebration is sponsored by the Institut de France. The festival is hosted along the Danube River, with streets filled with music and dancing. There are also local markets dedicated to French foods and wine, mixed with some traditional Hungarian specialties. At the end of the celebration, a fireworks show is held on the river banks.\nIndia.\nBastille Day is celebrated with great festivity in Pondicherry, a former French colony.\nIreland.\nThe Embassy of France in Ireland organizes several events around Dublin, Cork and Limerick for Bastille Day; including evenings of French music and tasting of French food. Many members of the French community in Ireland take part in the festivities. Events in Dublin include live entertainment, speciality menus on French cuisine, and screenings of popular French films.\nNew Zealand.\nThe Auckland suburb of Remuera hosts an annual French-themed Bastille Day street festival. Visitors enjoy mimes, dancers, music, as well as French foods and drinks. The budding relationship between the two countries, with the establishment of a Maori garden in France and exchange of their analyses of cave art, resulted in the creation of an official reception at the Residence of France. There is also an event in Wellington for the French community held at the Residence of France.\nSouth Africa.\nFranschhoek's weekend festival has been celebrated since 1993. (Franschhoek, or 'French Corner,' is situated in the Western Cape.) As South Africa's gourmet capital, French food, wine and other entertainment is provided throughout the festival. The French Consulate in South Africa also celebrates their national holiday with a party for the French community. Activities also include dressing up in different items of French clothing.\nFrench Polynesia.\nFollowing colonial rule, France annexed a large portion of what is now French Polynesia. Under French rule, Tahitians were permitted to participate in sport, singing, and dancing competitions one day a year: Bastille Day. The single day of celebration evolved into the major Heiva i Tahiti festival in Papeete Tahiti, where traditional events such as canoe races, tattooing, and fire walks are held. The singing and dancing competitions continue with music composed with traditional instruments such as the nasal flute and ukulele.\nUnited Kingdom.\nWithin the UK, London has a large French contingent, and celebrates Bastille Day at various locations across the city including Battersea Park, Camden Town and Kentish Town. Live entertainment is performed at Canary Wharf, with weeklong performances of French theatre at the Lion and Unicorn Theatre in Kentish Town. Restaurants feature cabarets and special menus across the city, and other celebrations include garden parties and sports tournaments. There is also a large event at the Bankside and Borough Market, where there is live music, street performers, and traditional French games played.\nUnited States.\nThe United States has over 20 cities that conduct annual celebrations of Bastille Day. The different cities celebrate with many French staples such as food, music, games, and sometimes the recreation of famous French landmarks.\nBaltimore, Maryland, has a large Bastille Day celebration each year at Petit Louis in the Roland Park area of Baltimore. Boston has a celebration annually, hosted by the French Cultural Center for 40 years. The street festival occurs in Boston's Back Bay neighborhood, near the Cultural Center's headquarters. The celebration includes francophone musical performers, dancing, and French cuisine. New York City has numerous Bastille Day celebrations each July, including \"Bastille Day on 60th Street\" hosted by the French Institute Alliance Fran\u00e7aise between Fifth and Lexington Avenues on the Upper East Side of Manhattan, Bastille Day on Smith Street in Brooklyn, and Bastille Day in Tribeca. There is also the annual Bastille Day Ball, taking place since 1924. Philadelphia's Bastille Day, held at Eastern State Penitentiary, involves Marie Antoinette throwing locally manufactured Tastykakes at the Parisian militia, as well as a re-enactment of the storming of the Bastille. (This Philadelphia tradition ended in 2018.) In Newport, Rhode Island, the annual Bastille Day celebration is organized by the local chapter of the Alliance Fran\u00e7aise. It takes place at King Park in Newport at the monument memorializing the accomplishments of the General Comte de Rochambeau whose 6,000 to 7,000 French forces landed in Newport on 11 July 1780. Their assistance in the defeat of the English in the War of Independence is well documented and is proof of the special relationship between France and the United States. In Washington D.C., food, music, and auction events are sponsored by the Embassy of France. There is also a French Festival within the city, where families can meet period entertainment groups set during the time of the French Revolution. Restaurants host parties serving traditional French food. \nIn Dallas, Texas, the Bastille Day celebration, \"Bastille On Bishop\", began in 2010 and is held annually in the Bishop Arts District of the North Oak Cliff neighborhood, southwest of downtown just across the Trinity River. Dallas' French roots are tied to the short lived socialist Utopian community La R\u00e9union, formed in 1855 and incorporated into the City of Dallas in 1860. Miami's celebration is organized by \"French &amp; Famous\" in partnership with the French American Chamber of Commerce, the Union des Fran\u00e7ais de l'Etranger and many French brands. The event gathers over 1,000 attendees to celebrate \"La F\u00eate Nationale\". The location and theme change every year. In 2017, the theme was \"Guinguette Party\" and attracted 1,200 francophiles at The River Yacht Club. New Orleans, Louisiana, has multiple celebrations, the largest in the historic French Quarter. In Austin, Texas, the Alliance Fran\u00e7aise d\u2019Austin usually conducts a family-friendly Bastille Day party at the French Legation, the home of the French representative to the Republic of Texas from 1841 to 1845.\nChicago, Illinois, has hosted a variety of Bastille Day celebrations in a number of locations in the city, including Navy Pier and Oz Park. The recent incarnations have been sponsored in part by the Chicago branch of the French-American Chamber of Commerce and by the French Consulate-General in Chicago. Milwaukee's four-day street festival begins with a \"Storming of the Bastille\" with a 43-foot replica of the Eiffel Tower. Minneapolis, Minnesota, has a celebration with wine, French food, pastries, a flea market, circus performers and bands. Also in the Twin Cities area, the local chapter of the Alliance Fran\u00e7aise has hosted an annual event for years at varying locations with a competition for the \"Best Baguette of the Twin Cities.\" Montgomery, Ohio, has a celebration with wine, beer, local restaurants' fare, pastries, games and bands. St. Louis, Missouri, has annual festivals in the Soulard neighborhood, the former French village of Carondelet, Missouri, and in the Benton Park neighborhood. The Chatillon-DeMenil Mansion in the Benton Park neighborhood, holds an annual Bastille Day festival with reenactments of the beheading of Marie Antoinette and Louis XVI, traditional dancing, and artillery demonstrations. Carondelet also began hosting an annual saloon crawl to celebrate Bastille Day in 2017. The Soulard neighborhood in St. Louis, Missouri celebrates its unique French heritage with special events including a parade, which honors the peasants who rejected the monarchy. The parade includes a 'gathering of the mob,' a walking and golf cart parade, and a mock beheading of the King and Queen.\nPortland, Oregon, has celebrated Bastille Day with crowds up to 8,000, in public festivals at various public parks, since 2001. The event is coordinated by the Alliance Fran\u00e7aise of Portland. Seattle's Bastille Day celebration, held at the Seattle Center, involves performances, picnics, wine and shopping. Sacramento, California, conducts annual \"waiter races\" in the midtown restaurant and shopping district, with a street festival."}
{"id": "3940", "revid": "10120461", "url": "https://en.wikipedia.org/wiki?curid=3940", "title": "Blowfish (cipher)", "text": "Blowfish is a symmetric-key block cipher, designed in 1993 by Bruce Schneier and included in many cipher suites and encryption products. Blowfish provides a good encryption rate in software, and no effective cryptanalysis of it has been found to date for smaller files. It is recommended Blowfish should not be used to encrypt files larger than 4GB in size, Twofish should be used instead. \nBlowfish has a 64-bit block size and therefore it could be vulnerable to Sweet32 birthday attacks.\nSchneier designed Blowfish as a general-purpose algorithm, intended as an alternative to the aging DES and free of the problems and constraints associated with other algorithms. At the time Blowfish was released, many other designs were proprietary, encumbered by patents, or were commercial or government secrets. Schneier has stated that \"Blowfish is unpatented, and will remain so in all countries. The algorithm is hereby placed in the public domain, and can be freely used by anyone.\"\nNotable features of the design include key-dependent S-boxes and a highly complex key schedule.\nThe algorithm.\nBlowfish has a 64-bit block size and a variable key length from 32 bits up to 448 bits. It is a 16-round Feistel cipher and uses large key-dependent S-boxes. In structure it resembles CAST-128, which uses fixed S-boxes.\nThe adjacent diagram shows Blowfish's encryption routine. Each line represents 32\u00a0bits. There are five subkey-arrays: one 18-entry P-array (denoted as K in the diagram, to avoid confusion with the Plaintext) and four 256-entry S-boxes (S0, S1, S2 and S3).\nEvery round \"r\" consists of 4 actions: \nThe F-function splits the 32-bit input into four 8-bit quarters and uses the quarters as input to the S-boxes. The S-boxes accept 8-bit input and produce 32-bit output. The outputs are added modulo 232 and XORed to produce the final 32-bit output (see image in the upper right corner).\nAfter the 16th round, undo the last swap, and XOR L with K18 and R with K17 (output whitening).\nDecryption is exactly the same as encryption, except that P1, P2, ..., P18 are used in the reverse order. This is not so obvious because xor is commutative and associative. A common misconception is to use inverse order of encryption as decryption algorithm (i.e. first XORing P17 and P18 to the ciphertext block, then using the P-entries in reverse order).\nBlowfish's key schedule starts by initializing the P-array and S-boxes with values derived from the hexadecimal digits of pi, which contain no obvious pattern (see nothing up my sleeve number). The secret key is then, byte by byte, cycling the key if necessary, XORed with all the P-entries in order. A 64-bit all-zero block is then encrypted with the algorithm as it stands. The resultant ciphertext replaces P1 and P2. The same ciphertext is then encrypted again with the new subkeys, and the new ciphertext replaces P3 and P4. This continues, replacing the entire P-array and all the S-box entries. In all, the Blowfish encryption algorithm will run 521 times to generate all the subkeys about 4\u00a0KB of data is processed.\nBecause the P-array is 576\u00a0bits long, and the key bytes are XORed through all these 576\u00a0bits during the initialization, many implementations support key sizes up to 576\u00a0bits. The reason for that is a discrepancy between the original Blowfish description, which uses 448-bit keys, and its reference implementation, which uses 576-bit keys. The test vectors for verifying third-party implementations were also produced with 576-bit keys. When asked which Blowfish version is the correct one, Bruce Schneier answered: \"The test vectors should be used to determine the one true Blowfish\".\nAnother opinion is that the 448\u00a0bits limit is present to ensure that every bit of every subkey depends on every bit of the key, as the last four values of the P-array don't affect every bit of the ciphertext. This point should be taken in consideration for implementations with a different number of rounds, as even though it increases security against an exhaustive attack, it weakens the security guaranteed by the algorithm. And given the slow initialization of the cipher with each change of key, it is granted a natural protection against brute-force attacks, which doesn't really justify key sizes longer than 448\u00a0bits.\nBlowfish in pseudocode.\n P[18] // \"P-array of 18 elements\"\n S[4][256] // \"S-boxes: 4 arrays of 256 elements\"\n function f(x):\n // \"Calculates a function f on a 32-bit input x, using S-boxes and bit manipulation\"\n high_byte := (x shifted right by 24 bits)\n second_byte := (x shifted right by 16 bits) AND 0xff\n third_byte := (x shifted right by 8 bits) AND 0xff\n low_byte := x AND 0xff\n h := S[0][high_byte] + S[1][second_byte]\n return (h XOR S[2][third_byte]) + S[3][low_byte]\n procedure blowfish_encrypt(L, R):\n // \"Encrypts two 32-bit halves L and R using the P-array and function f over 16 rounds\"\n for round := 0 to 15:\n L := L XOR P[round]\n R := f(L) XOR R\n swap values of L and R\n swap values of L and R\n R := R XOR P[16]\n L := L XOR P[17]\n procedure blowfish_decrypt(L, R):\n // \"Decrypts two 32-bit halves L and R using the P-array and function f over 16 rounds in reverse\"\n for round := 17 down to 2:\n L := L XOR P[round]\n R := f(L) XOR R\n swap values of L and R\n swap values of L and R\n R := R XOR P[1]\n L := L XOR P[0]\n // \"Initializes the P-array and S-boxes using the provided key, followed by key expansion\"\n //\" Initialize P-array with the key values\"\n key_position := 0\n for i := 0 to 17:\n k := 0\n for j := 0 to 3:\n k := (k shifted left by 8 bits) OR key[key_position]\n key_position := (key_position + 1) mod key_length\n P[i] := P[i] XOR k\n //\" Blowfish key expansion (521 iterations)\"\n L := 0, R := 0\n for i := 0 to 17 by 2:\n blowfish_encrypt(L, R)\n P[i] := L\n P[i + 1] := R\n //\" Fill S-boxes by encrypting L and R\"\n for i := 0 to 3:\n for j := 0 to 255 by 2:\n blowfish_encrypt(L, R)\n S[i][j] := L\n S[i][j + 1] := R\nBlowfish in practice.\nBlowfish is a fast block cipher, except when changing keys. Each new key requires the pre-processing equivalent of encrypting about 4 kilobytes of text, which is very slow compared to other block ciphers. This prevents its use in certain applications, but is not a problem in others.\nBlowfish must be initialized with a key. It is good practice to have this key hashed with a hash function before use.\nIn one application Blowfish's slow key changing is actually a benefit: the password-hashing method (crypt $2, i.e. bcrypt) used in OpenBSD uses an algorithm derived from Blowfish that makes use of the slow key schedule; the idea is that the extra computational effort required gives protection against dictionary attacks. \"See\" key stretching.\nBlowfish has a memory footprint of just over 4 kilobytes of RAM. This constraint is not a problem even for older desktop and laptop computers, though it does prevent use in the smallest embedded systems such as early smartcards.\nBlowfish was one of the first secure block ciphers not subject to any patents and therefore freely available for anyone to use. This benefit has contributed to its popularity in cryptographic software.\nbcrypt is a password hashing function which, combined with a variable number of iterations (work \"cost\"), exploits the expensive key setup phase of Blowfish to increase the workload and duration of hash calculations, further reducing threats from brute force attacks.\nbcrypt is also the name of a cross-platform file encryption utility developed in 2002 that implements Blowfish.\nWeakness and successors.\nBlowfish's use of a 64-bit block size (as opposed to e.g. AES's 128-bit block size) makes it vulnerable to birthday attacks, particularly in contexts like HTTPS. In 2016, the SWEET32 attack demonstrated how to leverage birthday attacks to perform plaintext recovery (i.e. decrypting ciphertext) against ciphers with a 64-bit block size. The GnuPG project recommends that Blowfish not be used to encrypt files larger than 4 GB due to its small block size.\nA reduced-round variant of Blowfish is known to be susceptible to known-plaintext attacks on reflectively weak keys. Blowfish implementations use 16 rounds of encryption, and are not susceptible to this attack.\nBruce Schneier has recommended migrating to his Blowfish successor, Twofish.\nBlowfish2 was released in 2005, developed by Alexander Pukall. It has exactly the same design but has twice as many S tables and uses 64-bit integers instead of 32-bit integers. It no longer works on 64-bit blocks but on 128-bit blocks like AES. Blowfish2 is used for example, in FreePascal."}
{"id": "3942", "revid": "1257887902", "url": "https://en.wikipedia.org/wiki?curid=3942", "title": "Bijection", "text": "A bijection, bijective function, or one-to-one correspondence between two mathematical sets is a function such that each element of the second set (the codomain) is the image of exactly one element of the first set (the domain). Equivalently, a bijection is a relation between two sets such that each element of either set is paired with exactly one element of the other set.\nA function is bijective if and only if it is invertible; that is, a function formula_1 is bijective if and only if there is a function formula_2 the \"inverse\" of , such that each of the two ways for composing the two functions produces an identity function: formula_3 for each formula_4 in formula_5 and formula_6 for each formula_7 in formula_8\nFor example, the \"multiplication by two\" defines a bijection from the integers to the even numbers, which has the \"division by two\" as its inverse function.\nA function is bijective if and only if it is both injective (or \"one-to-one\")\u2014meaning that each element in the codomain is mapped from at most one element of the domain\u2014and surjective (or \"onto\")\u2014meaning that each element of the codomain is mapped from at least one element of the domain. The term \"one-to-one correspondence\" must not be confused with \"one-to-one function\", which means injective but not necessarily surjective.\nThe elementary operation of counting establishes a bijection from some finite set to the first natural numbers , up to the number of elements in the counted set. It results that two finite sets have the same number of elements if and only if there exists a bijection between them. More generally, two sets are said to have the same cardinal number if there exists a bijection between them.\nA bijective function from a set to itself is also called a permutation, and the set of all permutations of a set forms its symmetric group.\nSome bijections with further properties have received specific names, which include automorphisms, isomorphisms, homeomorphisms, diffeomorphisms, permutation groups, and most geometric transformations. Galois correspondences are bijections between sets of mathematical objects of apparently very different nature.\nDefinition.\nFor a binary relation pairing elements of set \"X\" with elements of set \"Y\" to be a bijection, four properties must hold:\nSatisfying properties (1) and (2) means that a pairing is a function with domain \"X\". It is more common to see properties (1) and (2) written as a single statement: Every element of \"X\" is paired with exactly one element of \"Y\". Functions which satisfy property (3) are said to be \"onto \"Y\" \" and are called surjections (or \"surjective functions\"). Functions which satisfy property (4) are said to be \"one-to-one functions\" and are called injections (or \"injective functions\"). With this terminology, a bijection is a function which is both a surjection and an injection, or using other words, a bijection is a function which is both \"one-to-one\" and \"onto\".\nExamples.\nBatting line-up of a baseball or cricket team.\nConsider the batting line-up of a baseball or cricket team (or any list of all the players of any sports team where every player holds a specific spot in a line-up). The set \"X\" will be the players on the team (of size nine in the case of baseball) and the set \"Y\" will be the positions in the batting order (1st, 2nd, 3rd, etc.) The \"pairing\" is given by which player is in what position in this order. Property (1) is satisfied since each player is somewhere in the list. Property (2) is satisfied since no player bats in two (or more) positions in the order. Property (3) says that for each position in the order, there is some player batting in that position and property (4) states that two or more players are never batting in the same position in the list.\nSeats and students of a classroom.\nIn a classroom there are a certain number of seats. A group of students enter the room and the instructor asks them to be seated. After a quick look around the room, the instructor declares that there is a bijection between the set of students and the set of seats, where each student is paired with the seat they are sitting in. What the instructor observed in order to reach this conclusion was that:\nThe instructor was able to conclude that there were just as many seats as there were students, without having to count either set.\nInverses.\nA bijection \"f\" with domain \"X\" (indicated by \"f\": \"X \u2192 Y\" in functional notation) also defines a converse relation starting in \"Y\" and going to \"X\" (by turning the arrows around). The process of \"turning the arrows around\" for an arbitrary function does not, \"in general\", yield a function, but properties (3) and (4) of a bijection say that this inverse relation is a function with domain \"Y\". Moreover, properties (1) and (2) then say that this inverse \"function\" is a surjection and an injection, that is, the inverse function exists and is also a bijection. Functions that have inverse functions are said to be invertible. A function is invertible if and only if it is a bijection.\nStated in concise mathematical notation, a function \"f\": \"X \u2192 Y\" is bijective if and only if it satisfies the condition\nContinuing with the baseball batting line-up example, the function that is being defined takes as input the name of one of the players and outputs the position of that player in the batting order. Since this function is a bijection, it has an inverse function which takes as input a position in the batting order and outputs the player who will be batting in that position.\nComposition.\nThe composition formula_11 of two bijections \"f\": \"X \u2192 Y\" and \"g\": \"Y \u2192 Z\" is a bijection, whose inverse is given by formula_11 is formula_13.\nConversely, if the composition formula_14 of two functions is bijective, it only follows that \"f\" is injective and \"g\" is surjective.\nCardinality.\nIf \"X\" and \"Y\" are finite sets, then there exists a bijection between the two sets \"X\" and \"Y\" if and only if \"X\" and \"Y\" have the same number of elements. Indeed, in axiomatic set theory, this is taken as the definition of \"same number of elements\" (equinumerosity), and generalising this definition to infinite sets leads to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.\nCategory theory.\nBijections are precisely the isomorphisms in the category \"Set\" of sets and set functions. However, the bijections are not always the isomorphisms for more complex categories. For example, in the category \"Grp\" of groups, the morphisms must be homomorphisms since they must preserve the group structure, so the isomorphisms are \"group isomorphisms\" which are bijective homomorphisms.\nGeneralization to partial functions.\nThe notion of one-to-one correspondence generalizes to partial functions, where they are called \"partial bijections\", although partial bijections are only required to be injective. The reason for this relaxation is that a (proper) partial function is already undefined for a portion of its domain; thus there is no compelling reason to constrain its inverse to be a total function, i.e. defined everywhere on its domain. The set of all partial bijections on a given base set is called the symmetric inverse semigroup.\nAnother way of defining the same notion is to say that a partial bijection from \"A\" to \"B\" is any relation \n\"R\" (which turns out to be a partial function) with the property that \"R\" is the graph of a bijection \"f\":\"A\u2032\"\u2192\"B\u2032\", where \"A\u2032\" is a subset of \"A\" and \"B\u2032\" is a subset of \"B\".\nWhen the partial bijection is on the same set, it is sometimes called a \"one-to-one partial transformation\". An example is the M\u00f6bius transformation simply defined on the complex plane, rather than its completion to the extended complex plane.\nReferences.\nThis topic is a basic concept in set theory and can be found in any text which includes an introduction to set theory. Almost all texts that deal with an introduction to writing proofs will include a section on set theory, so the topic may be found in any of these:"}
{"id": "3943", "revid": "48546149", "url": "https://en.wikipedia.org/wiki?curid=3943", "title": "Binary function", "text": "In mathematics, a binary function (also called bivariate function, or function of two variables) is a function that takes two inputs.\nPrecisely stated, a function formula_1 is binary if there exists sets formula_2 such that\nwhere formula_4 is the Cartesian product of formula_5 and formula_6\nAlternative definitions.\nSet-theoretically, a binary function can be represented as a subset of the Cartesian product formula_7, where formula_8 belongs to the subset if and only if formula_9.\nConversely, a subset formula_10 defines a binary function if and only if for any formula_11 and formula_12, there exists a unique formula_13 such that formula_8 belongs to formula_10.\nformula_16 is then defined to be this formula_17.\nAlternatively, a binary function may be interpreted as simply a function from formula_4 to formula_19.\nEven when thought of this way, however, one generally writes formula_16 instead of formula_21.\nExamples.\nDivision of whole numbers can be thought of as a function. If formula_22 is the set of integers, formula_23 is the set of natural numbers (except for zero), and formula_24 is the set of rational numbers, then division is a binary function formula_25.\nIn a vector space \"V\" over a field \"F\", scalar multiplication is a binary function. A scalar \"a\" \u2208 \"F\" is combined with a vector \"v\" \u2208 \"V\" to produce a new vector \"av\" \u2208 \"V\".\nAnother example is that of inner products, or more generally functions of the form formula_26, where , are real-valued vectors of appropriate size and is a matrix. If is a positive definite matrix, this yields an inner product.\nFunctions of two real variables.\nFunctions whose domain is a subset of formula_27 are often also called functions of two variables even if their domain does not form a rectangle and thus the cartesian product of two sets.\nRestrictions to ordinary functions.\nIn turn, one can also derive ordinary functions of one variable from a binary function.\nGiven any element formula_11, there is a function formula_29, or formula_30, from formula_31 to formula_19, given by formula_33.\nSimilarly, given any element formula_12, there is a function formula_35, or formula_36, from formula_5 to formula_19, given by formula_39. In computer science, this identification between a function from formula_4 to formula_19 and a function from formula_5 to formula_43, where formula_43 is the set of all functions from formula_31 to formula_19, is called \"currying\".\nGeneralisations.\nThe various concepts relating to functions can also be generalised to binary functions.\nFor example, the division example above is \"surjective\" (or \"onto\") because every rational number may be expressed as a quotient of an integer and a natural number.\nThis example is \"injective\" in each input separately, because the functions \"f\" \"x\" and \"f\" \"y\" are always injective.\nHowever, it's not injective in both variables simultaneously, because (for example) \"f\" (2,4) = \"f\" (1,2).\nOne can also consider \"partial\" binary functions, which may be defined only for certain values of the inputs.\nFor example, the division example above may also be interpreted as a partial binary function from Z and N to Q, where N is the set of all natural numbers, including zero.\nBut this function is undefined when the second input is zero.\nA binary operation is a binary function where the sets \"X\", \"Y\", and \"Z\" are all equal; binary operations are often used to define algebraic structures.\nIn linear algebra, a bilinear transformation is a binary function where the sets \"X\", \"Y\", and \"Z\" are all vector spaces and the derived functions \"f\" \"x\" and \"f\"\"y\" are all linear transformations.\nA bilinear transformation, like any binary function, can be interpreted as a function from \"X\" \u00d7 \"Y\" to \"Z\", but this function in general won't be linear.\nHowever, the bilinear transformation can also be interpreted as a single linear transformation from the tensor product formula_47 to \"Z\".\nGeneralisations to ternary and other functions.\nThe concept of binary function generalises to \"ternary\" (or \"3-ary\") \"function\", \"quaternary\" (or \"4-ary\") \"function\", or more generally to \"n-ary function\" for any natural number \"n\".\nA \"0-ary function\" to \"Z\" is simply given by an element of \"Z\".\nOne can also define an \"A-ary function\" where \"A\" is any set; there is one input for each element of \"A\".\nCategory theory.\nIn category theory, \"n\"-ary functions generalise to \"n\"-ary morphisms in a multicategory.\nThe interpretation of an \"n\"-ary morphism as an ordinary morphisms whose domain is some sort of product of the domains of the original \"n\"-ary morphism will work in a monoidal category.\nThe construction of the derived morphisms of one variable will work in a closed monoidal category.\nThe category of sets is closed monoidal, but so is the category of vector spaces, giving the notion of bilinear transformation above."}
