{"id": "5170", "revid": "46812063", "url": "https://en.wikipedia.org/wiki?curid=5170", "title": "Combinatorics", "text": "Combinatorics is an area of mathematics primarily concerned with counting, both as a means and as an end to obtaining results, and certain properties of finite structures. It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics and from evolutionary biology to computer science.\nCombinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics, notably in algebra, probability theory, topology, and geometry, as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an \"ad hoc\" solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right. One of the oldest and most accessible parts of combinatorics is graph theory, which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms.\nDefinition.\nThe full scope of combinatorics is not universally agreed upon. According to H.J. Ryser, a definition of the subject is difficult because it crosses so many mathematical subdivisions. Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with:\nLeon Mirsky has said: \"combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.\" One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella. Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable) but discrete setting.\nHistory.\nBasic combinatorial concepts and enumerative results appeared throughout the ancient world. Indian physician Sushruta asserts in Sushruta Samhita that 63 combinations can be made out of 6 different tastes, taken one at a time, two at a time, etc., thus computing all 26\u00a0\u2212\u00a01 possibilities. Greek historian Plutarch discusses an argument between Chrysippus (3rd century BCE) and Hipparchus (2nd century BCE) of a rather delicate enumerative problem, which was later shown to be related to Schr\u00f6der\u2013Hipparchus numbers. Earlier, in the \"Ostomachion\", Archimedes (3rd century BCE) may have considered the number of configurations of a tiling puzzle, while combinatorial interests possibly were present in lost works by Apollonius.\nIn the Middle Ages, combinatorics continued to be studied, largely outside of the European civilization. The Indian mathematician Mah\u0101v\u012bra () provided formulae for the number of permutations and combinations, and these formulas may have been familiar to Indian mathematicians as early as the 6th century CE. The philosopher and astronomer Rabbi Abraham ibn Ezra () established the symmetry of binomial coefficients, while a closed formula was obtained later by the talmudist and mathematician Levi ben Gerson (better known as Gersonides), in 1321.\nThe arithmetical triangle\u2014a graphical diagram showing relationships among the binomial coefficients\u2014was presented by mathematicians in treatises dating as far back as the 10th century, and would eventually become known as Pascal's triangle. Later, in Medieval England, campanology provided examples of what is now known as Hamiltonian cycles in certain Cayley graphs on permutations.\nDuring the Renaissance, together with the rest of mathematics and the sciences, combinatorics enjoyed a rebirth. Works of Pascal, Newton, Jacob Bernoulli and Euler became foundational in the emerging field. In modern times, the works of J.J. Sylvester (late 19th century) and Percy MacMahon (early 20th century) helped lay the foundation for enumerative and algebraic combinatorics. Graph theory also enjoyed an increase of interest at the same time, especially in connection with the four color problem.\nIn the second half of the 20th century, combinatorics enjoyed a rapid growth, which led to establishment of dozens of new journals and conferences in the subject. In part, the growth was spurred by new connections and applications to other fields, ranging from algebra to probability, from functional analysis to number theory, etc. These connections shed the boundaries between combinatorics and parts of mathematics and theoretical computer science, but at the same time led to a partial fragmentation of the field.\nApproaches and subfields of combinatorics.\nEnumerative combinatorics.\nEnumerative combinatorics is the most classical area of combinatorics and concentrates on counting the number of certain combinatorial objects. Although counting the number of elements in a set is a rather broad mathematical problem, many of the problems that arise in applications have a relatively simple combinatorial description. Fibonacci numbers is the basic example of a problem in enumerative combinatorics. The twelvefold way provides a unified framework for counting permutations, combinations and partitions.\nAnalytic combinatorics.\nAnalytic combinatorics concerns the enumeration of combinatorial structures using tools from complex analysis and probability theory. In contrast with enumerative combinatorics, which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.\nPartition theory.\nPartition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials. Originally a part of number theory and analysis, it is now considered a part of combinatorics or an independent field. It incorporates the bijective approach and various tools in analysis and analytic number theory and has connections with statistical mechanics. Partitions can be graphically visualized with Young diagrams or Ferrers diagrams. They occur in a number of branches of mathematics and physics, including the study of symmetric polynomials and of the symmetric group and in group representation theory in general.\nGraph theory.\nGraphs are fundamental objects in combinatorics. Considerations of graph theory range from enumeration (e.g., the number of graphs on \"n\" vertices with \"k\" edges) to existing structures (e.g., Hamiltonian cycles) to algebraic representations (e.g., given a graph \"G\" and two numbers \"x\" and \"y\", does the Tutte polynomial \"T\"\"G\"(\"x\",\"y\") have a combinatorial interpretation?). Although there are very strong connections between graph theory and combinatorics, they are sometimes thought of as separate subjects. While combinatorial methods apply to many graph theory problems, the two disciplines are generally used to seek solutions to different types of problems.\nDesign theory.\nDesign theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties. Block designs are combinatorial designs of a special type. This area is one of the oldest parts of combinatorics, such as in Kirkman's schoolgirl problem proposed in 1850. The solution of the problem is a special case of a Steiner system, which play an important role in the classification of finite simple groups. The area has further connections to coding theory and geometric combinatorics.\nCombinatorial design theory can be applied to the area of design of experiments. Some of the basic theory of combinatorial designs originated in the statistician Ronald Fisher's work on the design of biological experiments. Modern applications are also found in a wide gamut of areas including finite geometry, tournament scheduling, lotteries, mathematical chemistry, mathematical biology, algorithm design and analysis, networking, group testing and cryptography.\nFinite geometry.\nFinite geometry is the study of geometric systems having only a finite number of points. Structures analogous to those found in continuous geometries (Euclidean plane, real projective space, etc.) but defined combinatorially are the main items studied. This area provides a rich source of examples for design theory. It should not be confused with discrete geometry (combinatorial geometry).\nOrder theory.\nOrder theory is the study of partially ordered sets, both finite and infinite. It provides a formal framework for describing statements such as \"this is less than that\" or \"this precedes that\". Various examples of partial orders appear in algebra, geometry, number theory and throughout combinatorics and graph theory. Notable classes and examples of partial orders include lattices and Boolean algebras.\nMatroid theory.\nMatroid theory abstracts part of geometry. It studies the properties of sets (usually, finite sets) of vectors in a vector space that do not depend on the particular coefficients in a linear dependence relation. Not only the structure but also enumerative properties belong to matroid theory. Matroid theory was introduced by Hassler Whitney and studied as a part of order theory. It is now an independent field of study with a number of connections with other parts of combinatorics.\nExtremal combinatorics.\nExtremal combinatorics studies how large or how small a collection of finite objects (numbers, graphs, vectors, sets, etc.) can be, if it has to satisfy certain restrictions. Much of extremal combinatorics concerns classes of set systems; this is called extremal set theory. For instance, in an \"n\"-element set, what is the largest number of \"k\"-element subsets that can pairwise intersect one another? What is the largest number of subsets of which none contains any other? The latter question is answered by Sperner's theorem, which gave rise to much of extremal set theory.\nThe types of questions addressed in this case are about the largest possible graph which satisfies certain properties. For example, the largest triangle-free graph on \"2n\" vertices is a complete bipartite graph \"Kn,n\". Often it is too hard even to find the extremal answer \"f\"(\"n\") exactly and one can only give an asymptotic estimate.\nRamsey theory is another part of extremal combinatorics. It states that any sufficiently large configuration will contain some sort of order. It is an advanced generalization of the pigeonhole principle.\nProbabilistic combinatorics.\nIn probabilistic combinatorics, the questions are of the following type: what is the probability of a certain property for a random discrete object, such as a random graph? For instance, what is the average number of triangles in a random graph? Probabilistic methods are also used to determine the existence of combinatorial objects with certain prescribed properties (for which explicit examples might be difficult to find) by observing that the probability of randomly selecting an object with those properties is greater than 0. This approach (often referred to as \"the\" probabilistic method) proved highly effective in applications to extremal combinatorics and graph theory. A closely related area is the study of finite Markov chains, especially on combinatorial objects. Here again probabilistic tools are used to estimate the mixing time.\nOften associated with Paul Erd\u0151s, who did the pioneering work on the subject, probabilistic combinatorics was traditionally viewed as a set of tools to study problems in other parts of combinatorics. The area recently grew to become an independent field of combinatorics.\nAlgebraic combinatorics.\nAlgebraic combinatorics is an area of mathematics that employs methods of abstract algebra, notably group theory and representation theory, in various combinatorial contexts and, conversely, applies combinatorial techniques to problems in algebra. Algebraic combinatorics has come to be seen more expansively as an area of mathematics where the interaction of combinatorial and algebraic methods is particularly strong and significant. Thus the combinatorial topics may be enumerative in nature or involve matroids, polytopes, partially ordered sets, or finite geometries. On the algebraic side, besides group and representation theory, lattice theory and commutative algebra are common.\nCombinatorics on words.\nCombinatorics on words deals with formal languages. It arose independently within several branches of mathematics, including number theory, group theory and probability. It has applications to enumerative combinatorics, fractal analysis, theoretical computer science, automata theory, and linguistics. While many applications are new, the classical Chomsky\u2013Sch\u00fctzenberger hierarchy of classes of formal grammars is perhaps the best-known result in the field.\nGeometric combinatorics.\nGeometric combinatorics is related to convex and discrete geometry. It asks, for example, how many faces of each dimension a convex polytope can have. Metric properties of polytopes play an important role as well, e.g. the Cauchy theorem on the rigidity of convex polytopes. Special polytopes are also considered, such as permutohedra, associahedra and Birkhoff polytopes. Combinatorial geometry is a historical name for discrete geometry.\nIt includes a number of subareas such as polyhedral combinatorics (the study of faces of convex polyhedra), convex geometry (the study of convex sets, in particular combinatorics of their intersections), and discrete geometry, which in turn has many applications to computational geometry. The study of regular polytopes, Archimedean solids, and kissing numbers is also a part of geometric combinatorics. Special polytopes are also considered, such as the permutohedron, associahedron and Birkhoff polytope.\nTopological combinatorics.\nCombinatorial analogs of concepts and methods in topology are used to study graph coloring, fair division, partitions, partially ordered sets, decision trees, necklace problems and discrete Morse theory. It should not be confused with combinatorial topology which is an older name for algebraic topology.\nArithmetic combinatorics.\nArithmetic combinatorics arose out of the interplay between number theory, combinatorics, ergodic theory, and harmonic analysis. It is about combinatorial estimates associated with arithmetic operations (addition, subtraction, multiplication, and division). Additive number theory (sometimes also called additive combinatorics) refers to the special case when only the operations of addition and subtraction are involved. One important technique in arithmetic combinatorics is the ergodic theory of dynamical systems.\nInfinitary combinatorics.\nInfinitary combinatorics, or combinatorial set theory, is an extension of ideas in combinatorics to infinite sets. It is a part of set theory, an area of mathematical logic, but uses tools and ideas from both set theory and extremal combinatorics. Some of the things studied include continuous graphs and trees, extensions of Ramsey's theorem, and Martin's axiom. Recent developments concern combinatorics of the continuum and combinatorics on successors of singular cardinals.\nGian-Carlo Rota used the name \"continuous combinatorics\" to describe geometric probability, since there are many analogies between \"counting\" and \"measure\".\nRelated fields.\nCombinatorial optimization.\nCombinatorial optimization is the study of optimization on discrete and combinatorial objects. It started as a part of combinatorics and graph theory, but is now viewed as a branch of applied mathematics and computer science, related to operations research, algorithm theory and computational complexity theory.\nCoding theory.\nCoding theory started as a part of design theory with early combinatorial constructions of error-correcting codes. The main idea of the subject is to design efficient and reliable methods of data transmission. It is now a large field of study, part of information theory.\nDiscrete and computational geometry.\nDiscrete geometry (also called combinatorial geometry) also began as a part of combinatorics, with early results on convex polytopes and kissing numbers. With the emergence of applications of discrete geometry to computational geometry, these two fields partially merged and became a separate field of study. There remain many connections with geometric and topological combinatorics, which themselves can be viewed as outgrowths of the early discrete geometry.\nCombinatorics and dynamical systems.\nCombinatorial aspects of dynamical systems is another emerging field. Here dynamical systems can be defined on combinatorial objects. See for example\ngraph dynamical system.\nCombinatorics and physics.\nThere are increasing interactions between combinatorics and physics, particularly statistical physics. Examples include an exact solution of the Ising model, and a connection between the Potts model on one hand, and the chromatic and Tutte polynomials on the other hand."}
{"id": "5173", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=5173", "title": "Continuous Random Variable", "text": ""}
{"id": "5176", "revid": "38005489", "url": "https://en.wikipedia.org/wiki?curid=5176", "title": "Calculus", "text": "Calculus is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations.\nOriginally called infinitesimal calculus or \"the calculus of infinitesimals\", it has two major branches, differential calculus and integral calculus. The former concerns instantaneous rates of change, and the slopes of curves, while the latter concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus. They make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit. It is the \"mathematical backbone\" for dealing with problems where variables change with time or another reference variable.\nInfinitesimal calculus was formulated separately in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. Today, calculus is widely used in science, engineering, biology, and even has applications in social science and other branches of math.\nEtymology.\nIn mathematics education, \"calculus\" is an abbreviation of both infinitesimal calculus and integral calculus, which denotes courses of elementary mathematical analysis. \nIn Latin, the word \"calculus\" means \u201csmall pebble\u201d, (the diminutive of \"calx,\" meaning \"stone\"), a meaning which still persists in medicine. Because such pebbles were used for counting out distances, tallying votes, and doing abacus arithmetic, the word came to be the Latin word for \"calculation\". In this sense, it was used in English at least as early as 1672, several years before the publications of Leibniz and Newton, who wrote their mathematical texts in Latin.\nIn addition to differential calculus and integral calculus, the term is also used for naming specific methods of computation or theories that imply some sort of computation. Examples of this usage include propositional calculus, Ricci calculus, calculus of variations, lambda calculus, sequent calculus, and process calculus. Furthermore, the term \"calculus\" has variously been applied in ethics and philosophy, for such systems as Bentham's felicific calculus, and the ethical calculus.\nHistory.\nModern calculus was developed in 17th-century Europe by Isaac Newton and Gottfried Wilhelm Leibniz (independently of each other, first publishing around the same time) but elements of it first appeared in ancient Egypt and later Greece, then in China and the Middle East, and still later again in medieval Europe and India.\nAncient precursors.\nEgypt.\nCalculations of volume and area, one goal of integral calculus, can be found in the Egyptian Moscow papyrus (), but the formulae are simple instructions, with no indication as to how they were obtained.\nGreece.\nLaying the foundations for integral calculus and foreshadowing the concept of the limit, ancient Greek mathematician Eudoxus of Cnidus () developed the method of exhaustion to prove the formulas for cone and pyramid volumes.\nDuring the Hellenistic period, this method was further developed by Archimedes (BC), who combined it with a concept of the indivisibles\u2014a precursor to infinitesimals\u2014allowing him to solve several problems now treated by integral calculus. In \"The Method of Mechanical Theorems\" he describes, for example, calculating the center of gravity of a solid hemisphere, the center of gravity of a frustum of a circular paraboloid, and the area of a region bounded by a parabola and one of its secant lines.\nChina.\nThe method of exhaustion was later discovered independently in China by Liu Hui in the 3rd century AD to find the area of a circle. In the 5th century AD, Zu Gengzhi, son of Zu Chongzhi, established a method that would later be called Cavalieri's principle to find the volume of a sphere.\nMedieval.\nMiddle East.\nIn the Middle East, Hasan Ibn al-Haytham, Latinized as Alhazen (AD) derived a formula for the sum of fourth powers. He used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid.\nIndia.\nBh\u0101skara II () was acquainted with some ideas of differential calculus and suggested that the \"differential coefficient\" vanishes at an extremum value of the function. In his astronomical work, he gave a procedure that looked like a precursor to infinitesimal methods. Namely, if formula_1 then formula_2 This can be interpreted as the discovery that cosine is the derivative of sine. In the 14th century, Indian mathematicians gave a non-rigorous method, resembling differentiation, applicable to some trigonometric functions. Madhava of Sangamagrama and the Kerala School of Astronomy and Mathematics stated components of calculus, but according to Victor J. Katz they were not able to \"combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today\".\nModern.\nJohannes Kepler's work \"Stereometria Doliorum\" (1615) formed the basis of integral calculus. Kepler developed a method to calculate the area of an ellipse by adding up the lengths of many radii drawn from a focus of the ellipse.\nSignificant work was a treatise, the origin being Kepler's methods, written by Bonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes' in \"The Method\", but this treatise is believed to have been lost in the 13th century and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri's work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.\nThe formal study of calculus brought together Cavalieri's infinitesimals with the calculus of finite differences developed in Europe at around the same time. Pierre de Fermat, claiming that he borrowed from Diophantus, introduced the concept of adequality, which represented equality up to an infinitesimal error term. The combination was achieved by John Wallis, Isaac Barrow, and James Gregory, the latter two proving predecessors to the second fundamental theorem of calculus around 1670.\nThe product rule and chain rule, the notions of higher derivatives and Taylor series, and of analytic functions were used by Isaac Newton in an idiosyncratic notation which he applied to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his \"Principia Mathematica\" (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.\nThese ideas were arranged into a true calculus of infinitesimals by Gottfried Wilhelm Leibniz, who was originally accused of plagiarism by Newton. He is now regarded as an independent inventor of and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing the product rule and chain rule, in their differential and integral forms. Unlike Newton, Leibniz put painstaking effort into his choices of notation.\nToday, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to general physics. Leibniz developed much of the notation used in calculus today. The basic insights that both Newton and Leibniz provided were the laws of differentiation and integration, emphasizing that differentiation and integration are inverse processes, second and higher derivatives, and the notion of an approximating polynomial series.\nWhen Newton and Leibniz first published their results, there was great controversy over which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in his \"Method of Fluxions\"), but Leibniz published his \"Nova Methodus pro Maximis et Minimis\" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of the Royal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics. A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation. It is Leibniz, however, who gave the new discipline its name. Newton called his calculus \"the science of fluxions\", a term that endured in English schools into the 19th century. The first complete treatise on calculus to be written in English and use the Leibniz notation was not published until 1815.\nSince the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi.\nFoundations.\nIn calculus, \"foundations\" refers to the rigorous development of the subject from axioms and definitions. In early calculus, the use of infinitesimal quantities was thought unrigorous and was fiercely criticized by several authors, most notably Michel Rolle and Bishop Berkeley. Berkeley famously described infinitesimals as the ghosts of departed quantities in his book \"The Analyst\" in 1734. Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz, and is still to some extent an active area of research today.\nSeveral mathematicians, including Maclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work of Cauchy and Weierstrass, a way was finally found to avoid mere \"notions\" of infinitely small quantities. The foundations of differential and integral calculus had been laid. In Cauchy's \"Cours d'Analyse\", we find a broad range of foundational approaches, including a definition of continuity in terms of infinitesimals, and a (somewhat imprecise) prototype of an (\u03b5, \u03b4)-definition of limit in the definition of differentiation. In his work, Weierstrass formalized the concept of limit and eliminated infinitesimals (although his definition can validate nilsquare infinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called \"infinitesimal calculus\". Bernhard Riemann used these ideas to give a precise definition of the integral. It was also during this period that the ideas of calculus were generalized to the complex plane with the development of complex analysis.\nIn modern mathematics, the foundations of calculus are included in the field of real analysis, which contains full definitions and proofs of the theorems of calculus. The reach of calculus has also been greatly extended. Henri Lebesgue invented measure theory, based on earlier developments by \u00c9mile Borel, and used it to define integrals of all but the most pathological functions. Laurent Schwartz introduced distributions, which can be used to take the derivative of any function whatsoever.\nLimits are not the only rigorous approach to the foundation of calculus. Another way is to use Abraham Robinson's non-standard analysis. Robinson's approach, developed in the 1960s, uses technical machinery from mathematical logic to augment the real number system with infinitesimal and infinite numbers, as in the original Newton-Leibniz conception. The resulting numbers are called hyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus. There is also smooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher-power infinitesimals during derivations. Based on the ideas of F. W. Lawvere and employing the methods of category theory, smooth infinitesimal analysis views all functions as being continuous and incapable of being expressed in terms of discrete entities. One aspect of this formulation is that the law of excluded middle does not hold. The law of excluded middle is also rejected in constructive mathematics, a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. Reformulations of calculus in a constructive framework are generally part of the subject of constructive analysis.\nSignificance.\nWhile many of the ideas of calculus had been developed earlier in Greece, China, India, Iraq, Persia, and Japan, the use of calculus began in Europe, during the 17th century, when Newton and Leibniz built on the work of earlier mathematicians to introduce its basic principles. The Hungarian polymath John von Neumann wrote of this work,\nApplications of differential calculus include computations involving velocity and acceleration, the slope of a curve, and optimization. Applications of integral calculus include computations involving area, volume, arc length, center of mass, work, and pressure. More advanced applications include power series and Fourier series.\nCalculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involving division by zero or sums of infinitely many numbers. These questions arise in the study of motion and area. The ancient Greek philosopher Zeno of Elea gave several famous examples of such paradoxes. Calculus provides tools, especially the limit and the infinite series, that resolve the paradoxes.\nPrinciples.\nLimits and infinitesimals.\nCalculus is usually developed by working with very small quantities. Historically, the first method of doing so was by infinitesimals. These are objects which can be treated like real numbers but which are, in some sense, \"infinitely small\". For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positive real number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals. The symbols formula_3 and formula_4 were taken to be infinitesimal, and the derivative formula_5 was their ratio.\nThe infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. In the late 19th century, infinitesimals were replaced within academia by the epsilon, delta approach to limits. Limits describe the behavior of a function at a certain input in terms of its values at nearby inputs. They capture small-scale behavior using the intrinsic structure of the real number system (as a metric space with the least-upper-bound property). In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by sequences of smaller and smaller numbers, and the infinitely small behavior of a function is found by taking the limiting behavior for these sequences. Limits were thought to provide a more rigorous foundation for calculus, and for this reason, they became the standard approach during the 20th century. However, the infinitesimal concept was revived in the 20th century with the introduction of non-standard analysis and smooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals.\nDifferential calculus.\nDifferential calculus is the study of the definition, properties, and applications of the derivative of a function. The process of finding the derivative is called \"differentiation\". Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called the \"derivative function\" or just the \"derivative\" of the original function. In formal terms, the derivative is a linear operator which takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function\u2014such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on\u2014and uses this information to produce another function. The function produced by differentiating the squaring function turns out to be the doubling function.\nIn more explicit terms the \"doubling function\" may be denoted by and the \"squaring function\" by . The \"derivative\" now takes the function , defined by the expression \"\", as an input, that is all the information\u2014such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on\u2014and uses this information to output another function, the function , as will turn out.\nIn Lagrange's notation, the symbol for a derivative is an apostrophe-like mark called a prime. Thus, the derivative of a function called is denoted by , pronounced \"f prime\" or \"f dash\". For instance, if is the squaring function, then is its derivative (the doubling function from above).\nIf the input of the function represents time, then the derivative represents change concerning time. For example, if is a function that takes time as input and gives the position of a ball at that time as output, then the derivative of is how the position is changing in time, that is, it is the velocity of the ball.\nIf a function is linear (that is if the graph of the function is a straight line), then the function can be written as , where is the independent variable, is the dependent variable, is the \"y\"-intercept, and:\nThis gives an exact value for the slope of a straight line. If the graph of the function is not a straight line, however, then the change in divided by the change in varies. Derivatives give an exact meaning to the notion of change in output concerning change in input. To be concrete, let be a function, and fix a point in the domain of . is a point on the graph of the function. If is a number close to zero, then is a number close to . Therefore, is close to . The slope between these two points is\nThis expression is called a \"difference quotient\". A line through two points on a curve is called a \"secant line\", so is the slope of the secant line between and . The second line is only an approximation to the behavior of the function at the point because it does not account for what happens between and . It is not possible to discover the behavior at by setting to zero because this would require dividing by zero, which is undefined. The derivative is defined by taking the limit as tends to zero, meaning that it considers the behavior of for all small values of and extracts a consistent value for the case when equals zero:\nGeometrically, the derivative is the slope of the tangent line to the graph of at . The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the function .\nHere is a particular example, the derivative of the squaring function at the input 3. Let be the squaring function.\nThe slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines the \"derivative function\" of the squaring function or just the \"derivative\" of the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.\nLeibniz notation.\nA common notation, introduced by Leibniz, for the derivative in the example above is\nIn an approach based on limits, the symbol is to be interpreted not as the quotient of two numbers but as a shorthand for the limit computed above. Leibniz, however, did intend it to represent the quotient of two infinitesimally small numbers, being the infinitesimally small change in caused by an infinitesimally small change applied to . We can also think of as a differentiation operator, which takes a function as an input and gives another function, the derivative, as the output. For example:\nIn this usage, the in the denominator is read as \"with respect to \". Another example of correct notation could be:\nEven when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols like and as if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as the total derivative.\nIntegral calculus.\n\"Integral calculus\" is the study of the definitions, properties, and applications of two related concepts, the \"indefinite integral\" and the \"definite integral\". The process of finding the value of an integral is called \"integration\". The indefinite integral, also known as the \"antiderivative\", is the inverse operation to the derivative. is an indefinite integral of when is a derivative of . (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.) The definite integral inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and the x-axis. The technical definition of the definite integral involves the limit of a sum of areas of rectangles, called a Riemann sum.\nA motivating example is the distance traveled in a given time. If the speed is constant, only multiplication is needed:\nBut if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (a Riemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.\nWhen velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time. For example, traveling a steady 50\u00a0mph for 3 hours results in a total distance of 150 miles. Plotting the velocity as a function of time yields a rectangle with a height equal to the velocity and a width equal to the time elapsed. Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve. This connection between the area under a curve and the distance traveled can be extended to \"any\" irregularly shaped region exhibiting a fluctuating velocity over a given period. If represents speed as it varies over time, the distance traveled between the times represented by and is the area of the region between and the -axis, between and .\nTo approximate that area, an intuitive method would be to divide up the distance between and into several equal segments, the length of each segment represented by the symbol . For each small segment, we can choose one value of the function . Call that value . Then the area of the rectangle with base and height gives the distance (time multiplied by speed ) traveled in that segment. Associated with each segment is the average value of the function above it, . The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value for will give more rectangles and in most cases a better approximation, but for an exact answer, we need to take a limit as approaches zero.\nThe symbol of integration is formula_14, an elongated \"S\" chosen to suggest summation. The definite integral is written as:\nand is read \"the integral from \"a\" to \"b\" of \"f\"-of-\"x\" with respect to \"x\".\" The Leibniz notation is intended to suggest dividing the area under the curve into an infinite number of rectangles so that their width becomes the infinitesimally small .\nThe indefinite integral, or antiderivative, is written:\nFunctions differing by only a constant have the same derivative, and it can be shown that the antiderivative of a given function is a family of functions differing only by a constant. Since the derivative of the function , where is any constant, is , the antiderivative of the latter is given by:\nThe unspecified constant present in the indefinite integral or antiderivative is known as the constant of integration.\nFundamental theorem.\nThe fundamental theorem of calculus states that differentiation and integration are inverse operations. More precisely, it relates the values of antiderivatives to definite integrals. Because it is usually easier to compute an antiderivative than to apply the definition of a definite integral, the fundamental theorem of calculus provides a practical way of computing definite integrals. It can also be interpreted as a precise statement of the fact that differentiation is the inverse of integration.\nThe fundamental theorem of calculus states: If a function is continuous on the interval and if is a function whose derivative is on the interval , then\nFurthermore, for every in the interval ,\nThis realization, made by both Newton and Leibniz, was key to the proliferation of analytic results after their work became known. (The extent to which Newton and Leibniz were influenced by immediate predecessors, and particularly what Leibniz may have learned from the work of Isaac Barrow, is difficult to determine because of the priority dispute between them.) The fundamental theorem provides an algebraic method of computing many definite integrals\u2014without performing limit processes\u2014by finding formulae for antiderivatives. It is also a prototype solution of a differential equation. Differential equations relate an unknown function to its derivatives and are ubiquitous in the sciences.\nApplications.\nCalculus is used in every branch of the physical sciences, actuarial science, computer science, statistics, engineering, economics, business, medicine, demography, and in other fields wherever a problem can be mathematically modeled and an optimal solution is desired. It allows one to go from (non-constant) rates of change to the total change or vice versa, and many times in studying a problem we know one and are trying to find the other. Calculus can be used in conjunction with other mathematical disciplines. For example, it can be used with linear algebra to find the \"best fit\" linear approximation for a set of points in a domain. Or, it can be used in probability theory to determine the expectation value of a continuous random variable given a probability density function. In analytic geometry, the study of graphs of functions, calculus is used to find high points and low points (maxima and minima), slope, concavity and inflection points. Calculus is also used to find approximate solutions to equations; in practice, it is the standard way to solve differential equations and do root finding in most applications. Examples are methods such as Newton's method, fixed point iteration, and linear approximation. For instance, spacecraft use a variation of the Euler method to approximate curved courses within zero-gravity environments.\nPhysics makes particular use of calculus; all concepts in classical mechanics and electromagnetism are related through calculus. The mass of an object of known density, the moment of inertia of objects, and the potential energies due to gravitational and electromagnetic forces can all be found by the use of calculus. An example of the use of calculus in mechanics is Newton's second law of motion, which states that the derivative of an object's momentum concerning time equals the net force upon it. Alternatively, Newton's second law can be expressed by saying that the net force equals the object's mass times its acceleration, which is the time derivative of velocity and thus the second time derivative of spatial position. Starting from knowing how an object is accelerating, we use calculus to derive its path.\nMaxwell's theory of electromagnetism and Einstein's theory of general relativity are also expressed in the language of differential calculus. Chemistry also uses calculus in determining reaction rates and in studying radioactive decay. In biology, population dynamics starts with reproduction and death rates to model population changes.\nGreen's theorem, which gives the relationship between a line integral around a simple closed curve C and a double integral over the plane region D bounded by C, is applied in an instrument known as a planimeter, which is used to calculate the area of a flat surface on a drawing. For example, it can be used to calculate the amount of area taken up by an irregularly shaped flower bed or swimming pool when designing the layout of a piece of property.\nIn the realm of medicine, calculus can be used to find the optimal branching angle of a blood vessel to maximize flow. Calculus can be applied to understand how quickly a drug is eliminated from a body or how quickly a cancerous tumor grows.\nIn economics, calculus allows for the determination of maximal profit by providing a way to easily calculate both marginal cost and marginal revenue."}
{"id": "5177", "revid": "41790710", "url": "https://en.wikipedia.org/wiki?curid=5177", "title": "Communication", "text": "Communication is commonly defined as the transmission of information. Its precise definition is disputed and there are disagreements about whether unintentional or failed transmissions are included and whether communication not only transmits meaning but also creates it. Models of communication are simplified overviews of its main components and their interactions. Many models include the idea that a source uses a coding system to express information in the form of a message. The message is sent through a channel to a receiver who has to decode it to understand it. The main field of inquiry investigating communication is called communication studies.\nA common way to classify communication is by whether information is exchanged between humans, members of other species, or non-living entities such as computers. For human communication, a central contrast is between verbal and non-verbal communication. Verbal communication involves the exchange of messages in linguistic form, including spoken and written messages as well as sign language. Non-verbal communication happens without the use of a linguistic system, for example, using body language, touch, and facial expressions. Another distinction is between interpersonal communication, which happens between distinct persons, and intrapersonal communication, which is communication with oneself. Communicative competence is the ability to communicate well and applies to the skills of formulating messages and understanding them.\nNon-human forms of communication include animal and plant communication. Researchers in this field often refine their definition of communicative behavior by including the criteria that observable responses are present and that the participants benefit from the exchange. Animal communication is used in areas like courtship and mating, parent\u2013offspring relations, navigation, and self-defense. Communication through chemicals is particularly important for the relatively immobile plants. For example, maple trees release so-called volatile organic compounds into the air to warn other plants of a herbivore attack. Most communication takes place between members of the same species. The reason is that its purpose is usually some form of cooperation, which is not as common between different species. Interspecies communication happens mainly in cases of symbiotic relationships. For instance, many flowers use symmetrical shapes and distinctive colors to signal to insects where nectar is located. Humans engage in interspecies communication when interacting with pets and working animals.\nHuman communication has a long history and how people exchange information has changed over time. These changes were usually triggered by the development of new communication technologies. Examples are the invention of writing systems, the development of mass printing, the use of radio and television, and the invention of the internet. The technological advances also led to new forms of communication, such as the exchange of data between computers.\nDefinitions.\nThe word \"\" has its root in the Latin verb , which means or . Communication is usually understood as the transmission of information: a message is conveyed from a sender to a receiver using some medium, such as sound, written signs, bodily movements, or electricity. Sender and receiver are often distinct individuals but it is also possible for an individual to communicate with themselves. In some cases, sender and receiver are not individuals but groups like organizations, social classes, or nations. In a different sense, the term \"communication\" refers to the message that is being communicated or to the field of inquiry studying communicational phenomena.\nThe precise characterization of communication is disputed. Many scholars have raised doubts that any single definition can capture the term accurately. These difficulties come from the fact that the term is applied to diverse phenomena in different contexts, often with slightly different meanings. The issue of the right definition affects the research process on many levels. This includes issues like which empirical phenomena are observed, how they are categorized, which hypotheses and laws are formulated as well as how systematic theories based on these steps are articulated.\nSome definitions are broad and encompass unconscious and non-human behavior. Under a broad definition, many animals communicate within their own species and flowers communicate by signaling the location of nectar to bees through their colors and shapes. Other definitions restrict communication to conscious interactions among human beings. Some approaches focus on the use of symbols and signs while others stress the role of understanding, interaction, power, or transmission of ideas. Various characterizations see the communicator's intent to send a message as a central component. In this view, the transmission of information is not sufficient for communication if it happens unintentionally. A version of this view is given by philosopher Paul Grice, who identifies communication with actions that aim to make the recipient aware of the communicator's intention. One question in this regard is whether only successful transmissions of information should be regarded as communication. For example, distortion may interfere with and change the actual message from what was originally intended. A closely related problem is whether acts of deliberate deception constitute communication.\nAccording to a broad definition by literary critic I.\u00a0A. Richards, communication happens when one mind acts upon its environment to transmit its own experience to another mind. Another interpretation is given by communication theorists Claude Shannon and Warren Weaver, who characterize communication as a transmission of information brought about by the interaction of several components, such as a source, a message, an encoder, a channel, a decoder, and a receiver. The transmission view is rejected by transactional and constitutive views, which hold that communication is not just about the transmission of information but also about the creation of meaning. Transactional and constitutive perspectives hold that communication shapes the participant's experience by conceptualizing the world and making sense of their environment and themselves. Researchers studying animal and plant communication focus less on meaning-making. Instead, they often define communicative behavior as having other features, such as playing a beneficial role in survival and reproduction, or having an observable response.\nModels of communication.\nModels of communication are conceptual representations of the process of communication. Their goal is to provide a simplified overview of its main components. This makes it easier for researchers to formulate hypotheses, apply communication-related concepts to real-world cases, and test predictions. Due to their simplified presentation, they may lack the conceptual complexity needed for a comprehensive understanding of all the essential aspects of communication. They are usually presented visually in the form of diagrams showing the basic components and their interaction.\nModels of communication are often categorized based on their intended applications and how they conceptualize communication. Some models are general in the sense that they are intended for all forms of communication. Specialized models aim to describe specific forms, such as models of mass communication.\nOne influential way to classify communication is to distinguish between linear transmission, interaction, and transaction models. Linear transmission models focus on how a sender transmits information to a receiver. They are \"linear\" because this flow of information only goes in a single direction. This view is rejected by interaction models, which include a feedback loop. Feedback is needed to describe many forms of communication, such as a conversation, where the listener may respond to a speaker by expressing their opinion or by asking for clarification. Interaction models represent the process as a form of two-way communication in which the communicators take turns sending and receiving messages. Transaction models further refine this picture by allowing representations of sending and responding at the same time. This modification is needed to describe how the listener can give feedback in a face-to-face conversation while the other person is talking. Examples are non-verbal feedback through body posture and facial expression. Transaction models also hold that meaning is produced during communication and does not exist independently of it.\nAll the early models, developed in the middle of the 20th century, are linear transmission models. Lasswell's model, for example, is based on five fundamental questions: \"Who?\", \"Says what?\", \"In which channel?\", \"To whom?\", and \"With what effect?\". The goal of these questions is to identify the basic components involved in the communicative process: the sender, the message, the channel, the receiver, and the effect. Lasswell's model was initially only conceived as a model of mass communication, but it has been applied to other fields as well. Some communication theorists, like Richard Braddock, have expanded it by including additional questions, like \"Under what circumstances?\" and \"For what purpose?\".\nThe Shannon\u2013Weaver model is another influential linear transmission model. It is based on the idea that a source creates a message, which is then translated into a signal by a transmitter. Noise may interfere with and distort the signal. Once the signal reaches the receiver, it is translated back into a message and made available to the destination. For a landline telephone call, the person calling is the source and their telephone is the transmitter. The transmitter translates the message into an electrical signal that travels through the wire, which acts as the channel. The person taking the call is the destination and their telephone is the receiver. The Shannon\u2013Weaver model includes an in-depth discussion of how noise can distort the signal and how successful communication can be achieved despite noise. This can happen by making the message partially redundant so that decoding is possible nonetheless. Other influential linear transmission models include Gerbner's model and Berlo's model.\nThe earliest interaction model was developed by communication theorist Wilbur Schramm. He states that communication starts when a source has an idea and expresses it in the form of a message. This process is called \"encoding\" and happens using a code, i.e. a sign system that is able to express the idea, for instance, through visual or auditory signs. The message is sent to a destination, who has to decode and interpret it to understand it. In response, they formulate their own idea, encode it into a message, and send it back as a form of feedback. Another innovation of Schramm's model is that previous experience is necessary to be able to encode and decode messages. For communication to be successful, the fields of experience of source and destination have to overlap.\nThe first transactional model was proposed by communication theorist Dean Barnlund in 1970. He understands communication as \"the production of meaning, rather than the production of messages\". Its goal is to decrease uncertainty and arrive at a shared understanding. This happens in response to external and internal cues. Decoding is the process of ascribing meaning to them and encoding consists in producing new behavioral cues as a response.\nHuman.\nThere are many forms of human communication. A central distinction is whether language is used, as in the contrast between verbal and non-verbal communication. A further distinction concerns whether one communicates with others or with oneself, as in the contrast between interpersonal and intrapersonal communication. Forms of human communication are also categorized by their channel or the medium used to transmit messages. The field studying human communication is known as anthroposemiotics.\nVerbal.\nVerbal communication is the exchange of messages in linguistic form, i.e., by means of language. In colloquial usage, verbal communication is sometimes restricted to oral communication and may exclude writing and sign language. However, in academic discourse, the term is usually used in a wider sense, encompassing any form of linguistic communication, whether through speech, writing, or gestures. Some of the challenges in distinguishing verbal from non-verbal communication come from the difficulties in defining what exactly \"language\" means. Language is usually understood as a conventional system of symbols and rules used for communication. Such systems are based on a set of simple units of meaning that can be combined to express more complex ideas. The rules for combining the units into compound expressions are called grammar. Words are combined to form sentences.\nOne hallmark of human language, in contrast to animal communication, lies in its complexity and expressive power. Human language can be used to refer not just to concrete objects in the here-and-now but also to spatially and temporally distant objects and to abstract ideas. Humans have a natural tendency to acquire their native language in childhood. They are also able to learn other languages later in life as second languages. However, this process is less intuitive and often does not result in the same level of linguistic competence. The academic discipline studying language is called \"linguistics\". Its subfields include semantics (the study of meaning), morphology (the study of word formation), syntax (the study of sentence structure), pragmatics (the study of language use), and phonetics (the study of basic sounds).\nA central contrast among languages is between natural and artificial or constructed languages. Natural languages, like English, Spanish, and Japanese, developed naturally and for the most part unplanned in the course of history. Artificial languages, like Esperanto, Quenya, C++, and the language of first-order logic, are purposefully designed from the ground up. Most everyday verbal communication happens using natural languages. Central forms of verbal communication are speech and writing together with their counterparts of listening and reading. Spoken languages use sounds to produce signs and transmit meaning while for writing, the signs are physically inscribed on a surface. Sign languages, like American Sign Language and Nicaraguan Sign Language, are another form of verbal communication. They rely on visual means, mostly by using gestures with hands and arms, to form sentences and convey meaning.\nVerbal communication serves various functions. One key function is to exchange information, i.e. an attempt by the speaker to make the audience aware of something, usually of an external event. But language can also be used to express the speaker's feelings and attitudes. A closely related role is to establish and maintain social relations with other people. Verbal communication is also utilized to coordinate one's behavior with others and influence them. In some cases, language is not employed for an external purpose but only for entertainment or personal enjoyment. Verbal communication further helps individuals conceptualize the world around them and themselves. This affects how perceptions of external events are interpreted, how things are categorized, and how ideas are organized and related to each other.\nNon-verbal.\nNon-verbal communication is the exchange of information through non-linguistic modes, like facial expressions, gestures, and postures. However, not every form of non-verbal behavior constitutes non-verbal communication. Some theorists, like Judee Burgoon, hold that it depends on the existence of a socially shared coding system that is used to interpret the meaning of non-verbal behavior. Non-verbal communication has many functions. It frequently contains information about emotions, attitudes, personality, interpersonal relations, and private thoughts.\nNon-verbal communication often happens unintentionally and unconsciously, like sweating or blushing, but there are also conscious intentional forms, like shaking hands or raising a thumb. It often happens simultaneously with verbal communication and helps optimize the exchange through emphasis and illustration or by adding additional information. Non-verbal cues can clarify the intent behind a verbal message. Using multiple modalities of communication in this way usually makes communication more effective if the messages of each modality are consistent. However, in some cases different modalities can contain conflicting messages. For example, a person may verbally agree with a statement but press their lips together, thereby indicating disagreement non-verbally.\nThere are many forms of non-verbal communication. They include kinesics, proxemics, haptics, paralanguage, chronemics, and physical appearance. Kinesics studies the role of bodily behavior in conveying information. It is commonly referred to as body language, even though it is, strictly speaking, not a language but rather non-verbal communication. It includes many forms, like gestures, postures, walking styles, and dance. Facial expressions, like laughing, smiling, and frowning, all belong to kinesics and are expressive and flexible forms of communication. Oculesics is another subcategory of kinesics in regard to the eyes. It covers questions like how eye contact, gaze, blink rate, and pupil dilation form part of communication. Some kinesic patterns are inborn and involuntary, like blinking, while others are learned and voluntary, like giving a military salute.\nProxemics studies how personal space is used in communication. The distance between the speakers reflects their degree of familiarity and intimacy with each other as well as their social status. Haptics examines how information is conveyed using touching behavior, like handshakes, holding hands, kissing, or slapping. Meanings linked to haptics include care, concern, anger, and violence. For instance, handshaking is often seen as a symbol of equality and fairness, while refusing to shake hands can indicate aggressiveness. Kissing is another form often used to show affection and erotic closeness.\nParalanguage, also known as vocalics, encompasses non-verbal elements in speech that convey information. Paralanguage is often used to express the feelings and emotions that the speaker has but does not explicitly stated in the verbal part of the message. It is not concerned with the words used but with how they are expressed. This includes elements like articulation, lip control, rhythm, intensity, pitch, fluency, and loudness. For example, saying something loudly and in a high pitch conveys a different meaning on the non-verbal level than whispering the same words. Paralanguage is mainly concerned with spoken language but also includes aspects of written language, like the use of colors and fonts as well as spatial arrangement in paragraphs and tables. Non-linguistic sounds may also convey information; crying indicates that an infant is distressed, and babbling conveys information about infant health and well-being.\nChronemics concerns the use of time, such as what messages are sent by being on time versus late for a meeting. The physical appearance of the communicator, such as height, weight, hair, skin color, gender, clothing, tattooing, and piercing, also carries information. Appearance is an important factor for first impressions but is more limited as a mode of communication since it is less changeable. Some forms of non-verbal communication happen using such artifacts as drums, smoke, batons, traffic lights, and flags.\nNon-verbal communication can also happen through visual media like paintings and drawings. They can express what a person or an object looks like and can also convey other ideas and emotions. In some cases, this type of non-verbal communication is used in combination with verbal communication, for example, when diagrams or maps employ labels to include additional linguistic information.\nTraditionally, most research focused on verbal communication. However, this paradigm began to shift in the 1950s when research interest in non-verbal communication increased and emphasized its influence. For example, many judgments about the nature and behavior of other people are based on non-verbal cues. It is further present in almost every communicative act to some extent and certain parts of it are universally understood. These considerations have prompted some communication theorists, like Ray Birdwhistell, to claim that the majority of ideas and information is conveyed this way. It has also been suggested that human communication is at its core non-verbal and that words can only acquire meaning because of non-verbal communication. The earliest forms of human communication, such as crying and babbling, are non-verbal. Some basic forms of communication happen even before birth between mother and embryo and include information about nutrition and emotions. Non-verbal communication is studied in various fields besides communication studies, like linguistics, semiotics, anthropology, and social psychology.\nInterpersonal.\nInterpersonal communication is communication between distinct people. Its typical form is dyadic communication, i.e. between two people, but it can also refer to communication within groups. It can be planned or unplanned and occurs in many forms, like when greeting someone, during salary negotiations, or when making a phone call. Some communication theorists, like Virginia M. McDermott, understand interpersonal communication as a fuzzy concept that manifests in degrees. In this view, an exchange varies in how interpersonal it is based on several factors. It depends on how many people are present, and whether it happens face-to-face rather than through telephone or email. A further factor concerns the relation between the communicators: group communication and mass communication are less typical forms of interpersonal communication and some theorists treat them as distinct types.\nInterpersonal communication can be synchronous or asynchronous. For asynchronous communication, the parties take turns in sending and receiving messages. This occurs when exchanging letters or emails. For synchronous communication, both parties send messages at the same time. This happens when one person is talking while the other person sends non-verbal messages in response signaling whether they agree with what is being said. Some communication theorists, like Sarah Trenholm and Arthur Jensen, distinguish between content messages and relational messages. Content messages express the speaker's feelings toward the topic of discussion. Relational messages, on the other hand, demonstrate the speaker's feelings toward their relation with the other participants.\nVarious theories of the function of interpersonal communication have been proposed. Some focus on how it helps people make sense of their world and create society. Others hold that its primary purpose is to understand why other people act the way they do and to adjust one's behavior accordingly. A closely related approach is to focus on information and see interpersonal communication as an attempt to reduce uncertainty about others and external events. Other explanations understand it in terms of the needs it satisfies. This includes the needs of belonging somewhere, being included, being liked, maintaining relationships, and influencing the behavior of others. On a practical level, interpersonal communication is used to coordinate one's actions with the actions of others to get things done. Research on interpersonal communication includes topics like how people build, maintain, and dissolve relationships through communication. Other questions are why people choose one message rather than another and what effects these messages have on the communicators and their relation. A further topic is how to predict whether two people would like each other.\nIntrapersonal.\nIntrapersonal communication is communication with oneself. In some cases this manifests externally, like when engaged in a monologue, taking notes, highlighting a passage, and writing a diary or a shopping list. But many forms of intrapersonal communication happen internally in the form of an inner exchange with oneself, like when thinking about something or daydreaming. Closely related to intrapersonal communication is communication that takes place within an organism below the personal level, such as exchange of information between organs or cells.\nIntrapersonal communication can be triggered by internal and external stimuli. It may happen in the form of articulating a phrase before expressing it externally. Other forms are to make plans for the future and to attempt to process emotions to calm oneself down in stressful situations. It can help regulate one's own mental activity and outward behavior as well as internalize cultural norms and ways of thinking. External forms of intrapersonal communication can aid one's memory. This happens, for example, when making a shopping list. Another use is to unravel difficult problems, as when solving a complex mathematical equation line by line. New knowledge can also be internalized this way, like when repeating new vocabulary to oneself. Because of these functions, intrapersonal communication can be understood as \"an exceptionally powerful and pervasive tool for thinking.\"\nBased on its role in self-regulation, some theorists have suggested that intrapersonal communication is more basic than interpersonal communication. Young children sometimes use egocentric speech while playing in an attempt to direct their own behavior. In this view, interpersonal communication only develops later when the child moves from their early egocentric perspective to a more social perspective. A different explanation holds that interpersonal communication is more basic since it is first used by parents to regulate what their child does. Once the child has learned this, they can apply the same technique to themselves to get more control over their own behavior.\nChannels.\nFor communication to be successful, the message has to travel from the sender to the receiver. The \"channel\" is the way this is accomplished. It is not concerned with the meaning of the message but only with the technical means of how the meaning is conveyed. Channels are often understood in terms of the senses used to perceive the message, i.e. hearing, seeing, smelling, touching, and tasting. But in the widest sense, channels encompass any form of transmission, including technological means like books, cables, radio waves, telephones, or television. Naturally transmitted messages usually fade rapidly whereas some messages using artificial channels have a much longer lifespan, as in the case of books or sculptures.\nThe physical characteristics of a channel have an impact on the code and cues that can be used to express information. For example, typical telephone calls are restricted to the use of verbal language and paralanguage but exclude facial expressions. It is often possible to translate messages from one code into another to make them available to a different channel. An example is writing down a spoken message or expressing it using sign language.\nThe transmission of information can occur through multiple channels at once. For example, face-to-face communication often combines the auditory channel to convey verbal information with the visual channel to transmit non-verbal information using gestures and facial expressions. Employing multiple channels can enhance the effectiveness of communication by helping the receiver better understand the subject matter. The choice of channels often matters since the receiver's ability to understand may vary depending on the chosen channel. For instance, a teacher may decide to present some information orally and other information visually, depending on the content and the student's preferred learning style. This underlines the role of a media-adequate approach.\nCommunicative competence.\nCommunicative competence is the ability to communicate effectively or to choose the appropriate communicative behavior in a given situation. It concerns what to say, when to say it, and how to say it. It further includes the ability to receive and understand messages. Competence is often contrasted with performance since competence can be present even if it is not exercised, while performance consists in the realization of this competence. However, some theorists reject a stark contrast and hold that performance is the observable part and is used to infer competence in relation to future performances.\nTwo central components of communicative competence are effectiveness and appropriateness. Effectiveness is the degree to which the speaker achieves their desired outcomes or the degree to which preferred alternatives are realized. This means that whether a communicative behavior is effective does not just depend on the actual outcome but also on the speaker's intention, i.e. whether this outcome was what they intended to achieve. Because of this, some theorists additionally require that the speaker be able to give an explanation of why they engaged in one behavior rather than another. Effectiveness is closely related to efficiency, the difference being that effectiveness is about achieving goals while efficiency is about using few resources (such as time, effort, and money) in the process.\nAppropriateness means that the communicative behavior meets social standards and expectations. Communication theorist Brian H. Spitzberg defines it as \"the perceived legitimacy or acceptability of behavior or enactments in a given context\". This means that the speaker is aware of the social and cultural context in order to adapt and express the message in a way that is considered acceptable in the given situation. For example, to bid farewell to their teacher, a student may use the expression \"Goodbye, sir\" but not the expression \"I gotta split, man\", which they may use when talking to a peer. To be both effective and appropriate means to achieve one's preferred outcomes in a way that follows social standards and expectations. Some definitions of communicative competence put their main emphasis on either effectiveness or appropriateness while others combine both features.\nMany additional components of communicative competence have been suggested, such as empathy, control, flexibility, sensitivity, and knowledge. It is often discussed in terms of the individual skills employed in the process, i.e. the specific behavioral components that make up communicative competence. Message production skills include reading and writing. They are correlated with the reception skills of listening and reading. There are both verbal and non-verbal communication skills. For example, verbal communication skills involve the proper understanding of a language, including its phonology, orthography, syntax, lexicon, and semantics.\nMany aspects of human life depend on successful communication, from ensuring basic necessities of survival to building and maintaining relationships. Communicative competence is a key factor regarding whether a person is able to reach their goals in social life, like having a successful career and finding a suitable spouse. Because of this, it can have a large impact on the individual's well-being. The lack of communicative competence can cause problems both on the individual and the societal level, including professional, academic, and health problems.\nBarriers to effective communication can distort the message. They may result in failed communication and cause undesirable effects. This can happen if the message is poorly expressed because it uses terms with which the receiver is not familiar, or because it is not relevant to the receiver's needs, or because it contains too little or too much information. Distraction, selective perception, and lack of attention to feedback may also be responsible. Noise is another negative factor. It concerns influences that interfere with the message on its way to the receiver and distort it. Crackling sounds during a telephone call are one form of noise. Ambiguous expressions can also inhibit effective communication and make it necessary to disambiguate between possible interpretations to discern the sender's intention. These interpretations depend also on the cultural background of the participants. Significant cultural differences constitute an additional obstacle and make it more likely that messages are misinterpreted.\nOther species.\nBesides human communication, there are many other forms of communication found in the animal kingdom and among plants. They are studied in fields like biocommunication and biosemiotics. There are additional obstacles in this area for judging whether communication has taken place between two individuals. Acoustic signals are often easy to notice and analyze for scientists, but it is more difficult to judge whether tactile or chemical changes should be understood as communicative signals rather than as other biological processes.\nFor this reason, researchers often use slightly altered definitions of communication to facilitate their work. A common assumption in this regard comes from evolutionary biology and holds that communication should somehow benefit the communicators in terms of natural selection. The biologists Rumsa\u00efs Blatrix and Veronika Mayer define communication as \"the exchange of information between individuals, wherein both the signaller and receiver may expect to benefit from the exchange\". According to this view, the sender benefits by influencing the receiver's behavior and the receiver benefits by responding to the signal. These benefits should exist on average but not necessarily in every single case. This way, deceptive signaling can also be understood as a form of communication. One problem with the evolutionary approach is that it is often difficult to assess the impact of such behavior on natural selection. Another common pragmatic constraint is to hold that it is necessary to observe a response by the receiver following the signal when judging whether communication has occurred.\nAnimals.\nAnimal communication is the process of giving and taking information among animals. The field studying animal communication is called zoosemiotics. There are many parallels to human communication. One is that humans and many animals express sympathy by synchronizing their movements and postures. Nonetheless, there are also significant differences, like the fact that humans also engage in verbal communication, which uses language, while animal communication is restricted to non-verbal (i.e. non-linguistic) communication. Some theorists have tried to distinguish human from animal communication based on the claim that animal communication lacks a referential function and is thus not able to refer to external phenomena. However, various observations seem to contradict this view, such as the warning signals in response to different types of predators used by vervet monkeys, Gunnison's prairie dogs, and red squirrels. A further approach is to draw the distinction based on the complexity of human language, especially its almost limitless ability to combine basic units of meaning into more complex meaning structures. One view states that recursion sets human language apart from all non-human communicative systems. Another difference is that human communication is frequently linked to the conscious intention to send information, which is often not discernable for animal communication. Despite these differences, some theorists use the term \"animal language\" to refer to certain communicative patterns in animal behavior that have similarities with human language.\nAnimal communication can take a variety of forms, including visual, auditory, tactile, olfactory, and gustatory communication. Visual communication happens in the form of movements, gestures, facial expressions, and colors. Examples are movements seen during mating rituals, the colors of birds, and the rhythmic light of fireflies. Auditory communication takes place through vocalizations by species like birds, primates, and dogs. Auditory signals are frequently used to alert and warn. Lower-order living systems often have simple response patterns to auditory messages, reacting either by approach or avoidance. More complex response patterns are observed for higher animals, which may use different signals for different types of predators and responses. For example, some primates use one set of signals for airborne predators and another for land predators. Tactile communication occurs through touch, vibration, stroking, rubbing, and pressure. It is especially relevant for parent-young relations, courtship, social greetings, and defense. Olfactory and gustatory communication happen chemically through smells and tastes, respectively.\nThere are large differences between species concerning what functions communication plays, how much it is realized, and the behavior used to communicate. Common functions include the fields of courtship and mating, parent-offspring relations, social relations, navigation, self-defense, and territoriality. One part of courtship and mating consists in identifying and attracting potential mates. This can happen through various means. Grasshoppers and crickets communicate acoustically by using songs, moths rely on chemical means by releasing pheromones, and fireflies send visual messages by flashing light. For some species, the offspring depends on the parent for its survival. One central function of parent-offspring communication is to recognize each other. In some cases, the parents are also able to guide the offspring's behavior.\nSocial animals, like chimpanzees, bonobos, wolves, and dogs, engage in various forms of communication to express their feelings and build relations. Communication can aid navigation by helping animals move through their environment in a purposeful way, e.g. to locate food, avoid enemies, and follow other animals. In bats, this happens through echolocation, i.e. by sending auditory signals and processing the information from the echoes. Bees are another often-discussed case in this respect since they perform a type of dance to indicate to other bees where flowers are located. In regard to self-defense, communication is used to warn others and to assess whether a costly fight can be avoided. Another function of communication is to mark and claim territories used for food and mating. For example, some male birds claim a hedge or part of a meadow by using songs to keep other males away and attract females.\nTwo competing theories in the study of animal communication are nature theory and nurture theory. Their conflict concerns to what extent animal communication is programmed into the genes as a form of adaptation rather than learned from previous experience as a form of conditioning. To the degree that it is learned, it usually happens through imprinting, i.e. as a form of learning that only occurs in a certain phase and is then mostly irreversible.\nPlants, fungi, and bacteria.\nPlant communication refers to plant processes involving the sending and receiving of information. The field studying plant communication is called phytosemiotics. This field poses additional difficulties for researchers since plants are different from humans and other animals in that they lack a central nervous system and have rigid cell walls. These walls restrict movement and usually prevent plants from sending and receiving signals that depend on rapid movement. However, there are some similarities since plants face many of the same challenges as animals. For example, they need to find resources, avoid predators and pathogens, find mates, and ensure that their offspring survive. Many of the evolutionary responses to these challenges are analogous to those in animals but are implemented using different means. One crucial difference is that chemical communication is much more prominent in the plant kingdom in contrast to the importance of visual and auditory communication for animals.\nIn plants, the term \"behavior\" is usually not defined in terms of physical movement, as is the case for animals, but as a biochemical response to a stimulus. This response has to be short relative to the plant's lifespan. Communication is a special form of behavior that involves conveying information from a sender to a receiver. It is distinguished from other types of behavior, like defensive reactions and mere sensing. Like in the field of animal communication, plant communication researchers often require as additional criteria that there is some form of response in the receiver and that the communicative behavior is beneficial to sender and receiver. Biologist Richard Karban distinguishes three steps of plant communication: the emission of a cue by a sender, the perception of the cue by a receiver, and the receiver's response. For plant communication, it is not relevant to what extent the emission of a cue is intentional. However, it should be possible for the receiver to ignore the signal. This criterion can be used to distinguish a response to a signal from a defense mechanism against an unwanted change like intense heat.\nPlant communication happens in various forms. It includes communication within plants, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. A prominent form of communication is airborne and happens through volatile organic compounds (VOCs). For example, maple trees release VOCs when they are attacked by a herbivore to warn neighboring plants, which then react accordingly by adjusting their defenses. Another form of plant-to-plant communication happens through mycorrhizal fungi. These fungi form underground networks, colloquially referred to as the Wood-Wide Web, and connect the roots of different plants. The plants use the network to send messages to each other, specifically to warn other plants of a pest attack and to help prepare their defenses.\nCommunication can also be observed for fungi and bacteria. Some fungal species communicate by releasing pheromones into the external environment. For instance, they are used to promote sexual interaction in several aquatic fungal species. One form of communication between bacteria is called quorum sensing. It happens by releasing hormone-like molecules, which other bacteria detect and respond to. This process is used to monitor the environment for other bacteria and to coordinate population-wide responses, for example, by sensing the density of bacteria and regulating gene expression accordingly. Other possible responses include the induction of bioluminescence and the formation of biofilms.\nInterspecies.\nMost communication happens between members within a species as intraspecies communication. This is because the purpose of communication is usually some form of cooperation. Cooperation happens mostly within a species while different species are often in conflict with each other by competing over resources. However, there are also some forms of interspecies communication. This occurs especially for symbiotic relations and significantly less for parasitic or predator-prey relations.\nInterspecies communication plays a key role for plants that depend on external agents for reproduction. For example, flowers need insects for pollination and provide resources like nectar and other rewards in return. They use communication to signal their benefits and attract visitors by using distinctive colors and symmetrical shapes to stand out from their surroundings. This form of advertisement is necessary since flowers compete with each other for visitors. Many fruit-bearing plants rely on plant-to-animal communication to disperse their seeds and move them to a favorable location. This happens by providing nutritious fruits to animals. The seeds are eaten together with the fruit and are later excreted at a different location. Communication makes animals aware of where the fruits are and whether they are ripe. For many fruits, this happens through their color: they have an inconspicuous green color until they ripen and take on a new color that stands in visual contrast to the environment. Another example of interspecies communication is found in the ant-plant relation. It concerns, for instance, the selection of seeds by ants for their ant gardens and the pruning of exogenous vegetation as well as plant protection by ants.\nSome animal species also engage in interspecies communication, like apes, whales, dolphins, elephants, and dogs. For example, different species of monkeys use common signals to cooperate when threatened by a common predator. Humans engage in interspecies communication when interacting with pets and working animals. For instance, acoustic signals play a central role in communication with dogs. Dogs can learn to react to various commands, like \"sit\" and \"come\". They can even be trained to respond to short syntactic combinations, like \"bring X\" or \"put X in a box\". They also react to the pitch and frequency of the human voice to detect emotions, dominance, and uncertainty. Dogs use a range of behavioral patterns to convey their emotions to humans, for example, in regard to aggressiveness, fearfulness, and playfulness.\nComputer.\nComputer communication concerns the exchange of data between computers and similar devices. For this to be possible, the devices have to be connected through a transmission system that forms a network between them. A transmitter is needed to send messages and a receiver is needed to receive them. A personal computer may use a modem as a transmitter to send information to a server through the public telephone network as the transmission system. The server may use a modem as its receiver. To transmit the data, it has to be converted into an electric signal. Communication channels used for transmission are either analog or digital and are characterized by features like bandwidth and latency.\nThere are many forms of computer networks. The most commonly discussed ones are LANs and WANs. \"LAN\" stands for \"local area network\", which is a computer network within a limited area, usually with a distance of less than one kilometer. This is the case when connecting two computers within a home or an office building. LANs can be set up using a wired connection, like Ethernet, or a wireless connection, like Wi-Fi. \"WANs\", on the other hand, are \"wide area networks\" that span large geographical regions, like the internet. Their networks are more complex and may use several intermediate connection nodes to transfer information between endpoints. Further types of computer networks include PANs (personal area networks), CANs (campus area networks), and MANs (metropolitan area networks).\nFor computer communication to be successful, the involved devices have to follow a common set of conventions governing their exchange. These conventions are known as the communication protocol. They concern various aspects of the exchange, like the format of messages and how to respond to transmission errors. They also cover how the two systems are synchronized, for example, how the receiver identifies the start and end of a signal. Based on the flow of informations, systems are categorized as simplex, half-duplex, and full-duplex. For simplex systems, signals flow only in one direction from the sender to the receiver, like in radio, cable television, and screens displaying arrivals and departures at airports. Half-duplex systems allow two-way exchanges but signals can only flow in one direction at a time, like walkie-talkies and police radios. In the case of full-duplex systems, signals can flow in both directions at the same time, like regular telephone and internet. In either case, it is often important for successful communication that the connection is secure to ensure that the transmitted data reaches only the intended destination and is not intercepted by an unauthorized third party. This can be achieved by using cryptography, which changes the format of the transmitted information to make it unintelligible to potential interceptors.\nHuman-computer communication is a closely related field that concerns topics like how humans interact with computers and how data in the form of inputs and outputs is exchanged. This happens through a user interface, which includes the hardware used to interact with the computer, like a mouse, a keyboard, and a monitor, as well as the software used in the process. On the software side, most early user interfaces were command-line interfaces in which the user must type a command to interact with the computer. Most modern user interfaces are graphical user interfaces, like Microsoft Windows and macOS, which are usually much easier to use for non-experts. They involve graphical elements through which the user can interact with the computer, commonly using a design concept known as skeumorphism to make a new concept feel familiar and speed up understanding by mimicking the real-world equivalent of the interface object. Examples include the typical computer folder icon and recycle bin used for discarding files. One aim when designing user interfaces is to simplify the interaction with computers. This helps make them more user-friendly and accessible to a wider audience while also increasing productivity.\nCommunication studies.\nCommunication studies, also referred to as \"communication science\", is the academic discipline studying communication. It is closely related to semiotics, with one difference being that communication studies focuses more on technical questions of how messages are sent, received, and processed. Semiotics, on the other hand, tackles more abstract questions in relation to meaning and how signs acquire it. Communication studies covers a wide area overlapping with many other disciplines, such as biology, anthropology, psychology, sociology, linguistics, media studies, and journalism.\nMany contributions in the field of communication studies focus on developing models and theories of communication. Models of communication aim to give a simplified overview of the main components involved in communication. Theories of communication try to provide conceptual frameworks to accurately present communication in all its complexity. Some theories focus on communication as a practical art of discourse while others explore the roles of signs, experience, information processing, and the goal of building a social order through coordinated interaction. Communication studies is also interested in the functions and effects of communication. It covers issues like how communication satisfies physiological and psychological needs, helps build relationships, and assists in gathering information about the environment, other individuals, and oneself. A further topic concerns the question of how communication systems change over time and how these changes correlate with other societal changes. A related topic focuses on psychological principles underlying those changes and the effects they have on how people exchange ideas.\nCommunication was studied as early as Ancient Greece. Early influential theories were created by Plato and Aristotle, who stressed public speaking and the understanding of rhetoric. According to Aristotle, for example, the goal of communication is to persuade the audience. The field of communication studies only became a separate research discipline in the 20th century, especially starting in the 1940s. The development of new communication technologies, such as telephone, radio, newspapers, television, and the internet, has had a big impact on communication and communication studies.\nToday, communication studies is a wide discipline. Some works in it try to provide a general characterization of communication in the widest sense. Others attempt to give a precise analysis of one specific form of communication. Communication studies includes many subfields. Some focus on wide topics like interpersonal communication, intrapersonal communication, verbal communication, and non-verbal communication. Others investigate communication within a specific area. Organizational communication concerns communication between members of organizations such as corporations, nonprofits, or small businesses. Central in this regard is the coordination of the behavior of the different members as well as the interaction with customers and the general public. Closely related terms are business communication, corporate communication, and professional communication. The main element of marketing communication is advertising but it also encompasses other communication activities aimed at advancing the organization's objective to its audiences, like public relations. Political communication covers topics like electoral campaigns to influence voters and legislative communication, like letters to a congress or committee documents. Specific emphasis is often given to propaganda and the role of mass media.\nIntercultural communication is relevant to both organizational and political communication since they often involve attempts to exchange messages between communicators from different cultural backgrounds. The cultural background affects how messages are formulated and interpreted and can be the cause of misunderstandings. It is also relevant for development communication, which is about the use of communication for assisting in development, like aid given by first-world countries to third-world countries. Health communication concerns communication in the field of healthcare and health promotion efforts. One of its topics is how healthcare providers, like doctors and nurses, should communicate with their patients.\nHistory.\nCommunication history studies how communicative processes evolved and interacted with society, culture, and technology. Human communication has a long history and the way people communicate has changed considerably over time. Many of these changes were triggered by the development of new communication technologies and had various effects on how people exchanged ideas. New communication technologies usually require new skills that people need to learn to use them effectively.\nIn the academic literature, the history of communication is usually divided into ages based on the dominant form of communication in that age. The number of ages and the precise periodization are disputed. They usually include ages for speaking, writing, and print as well as electronic mass communication and the internet. According to communication theorist Marshall Poe, the dominant media for each age can be characterized in relation to several factors. They include the amount of information a medium can store, how long it persists, how much time it takes to transmit it, and how costly it is to use the medium. Poe argues that subsequent ages usually involve some form of improvement of one or more of the factors.\nAccording to some scientific estimates, language developed around 40,000 years ago while others consider it to be much older. Before this development, human communication resembled animal communication and happened through a combination of grunts, cries, gestures, and facial expressions. Language helped early humans to organize themselves and plan ahead more efficiently. In early societies, spoken language was the primary form of communication. Most knowledge was passed on through it, often in the form of stories or wise sayings. This form does not produce stable knowledge since it depends on imperfect human memory. Because of this, many details differ from one telling to the next and are presented differently by distinct storytellers. As people started to settle and form agricultural communities, societies grew and there was an increased need for stable records of ownership of land and commercial transactions. This triggered the invention of writing, which is able to solve many problems that arose from using exclusively oral communication. It is much more efficient at preserving knowledge and passing it on between generations since it does not depend on human memory. Before the invention of writing, certain forms of proto-writing had already developed. Proto-writing encompasses long-lasting visible marks used to store information, like decorations on pottery items, knots in a cord to track goods, or seals to mark property.\nMost early written communication happened through pictograms. Pictograms are graphical symbols that convey meaning by visually resembling real-world objects. The use of basic pictographic symbols to represent things like farming produce was common in ancient cultures and began around 9000 BCE. The first complex writing system including pictograms was developed around 3500 BCE by the Sumerians and is called cuneiform. Pictograms are still in use today, like no-smoking signs and the symbols of male and female figures on bathroom doors. A significant disadvantage of pictographic writing systems is that they need a large amount of symbols to refer to all the objects one wants to talk about. This problem was solved by the development of other writing systems. For example, the symbols of alphabetic writing systems do not stand for regular objects. Instead, they relate to the sounds used in spoken language. Other types of early writing systems include logographic and ideographic writing systems. A drawback of many early forms of writing, like the clay tablets used for cuneiform, was that they were not very portable. This made it difficult to transport the texts from one location to another to share information. This changed with the invention of papyrus by the Egyptians around 2500 BCE and was further improved later by the development of parchment and paper.\nUntil the 1400s, almost all written communication was hand-written, which limited the spread of written media within society since copying texts by hand was costly. The introduction and popularization of mass printing in the middle of the 15th century by Johann Gutenberg resulted in rapid changes. Mass printing quickly increased the circulation of written media and also led to the dissemination of new forms of written documents, like newspapers and pamphlets. One side effect was that the augmented availability of written documents significantly improved the general literacy of the population. This development served as the foundation for revolutions in various fields, including science, politics, and religion.\nScientific discoveries in the 19th and 20th centuries caused many further developments in the history of communication. They include the invention of telegraphs and telephones, which made it even easier and faster to transmit information from one location to another without the need to transport written documents. These communication forms were initially limited to cable connections, which had to be established first. Later developments found ways of wireless transmission using radio signals. They made it possible to reach wide audiences and radio soon became one of the central forms of mass communication. Various innovations in the field of photography enabled the recording of images on film, which led to the development of cinema and television. The reach of wireless communication was further enhanced with the development of satellites, which made it possible to broadcast radio and television signals to stations all over the world. This way, information could be shared almost instantly everywhere around the globe. The development of the internet constitutes a further milestone in the history of communication. It made it easier than ever before for people to exchange ideas, collaborate, and access information from anywhere in the world by using a variety of means, such as websites, e-mail, social media, and video conferences."}
{"id": "5178", "revid": "1269403232", "url": "https://en.wikipedia.org/wiki?curid=5178", "title": "Classics", "text": "Classics or classical studies is the study of classical antiquity. In the Western world, \"classics\" traditionally refers to the study of Ancient Greek and Roman literature and their original languages, Ancient Greek and Latin. Classics may also include as secondary subjects Greco-Roman philosophy, history, archaeology, anthropology, art, mythology, and society.\nIn Western civilization, the study of the Ancient Greek and Roman classics was considered the foundation of the humanities, and they traditionally have been the cornerstone of an elite higher education.\nEtymology.\nThe word \"classics\" is derived from the Latin adjective \"classicus\", meaning \"belonging to the highest class of citizens.\" The word was originally used to describe the members of the Patricians, the highest class in ancient Rome. By the 2nd century AD the word was used in literary criticism to describe writers of the highest quality. For example, Aulus Gellius, in his \"Attic Nights\", contrasts \"classicus\" and \"proletarius\" writers. By the 6th century AD, the word had acquired a second meaning, referring to pupils at a school. Thus, the two modern meanings of the word, referring both to literature considered to be of the highest quality and the standard texts used as part of a curriculum, were both derived from Roman use.\nHistory.\nMiddle Ages.\nIn the Middle Ages, classics and education were tightly intertwined; according to Jan Ziolkowski, there is no era in history in which the link was tighter. Medieval education taught students to imitate earlier classical models, and Latin continued to be the language of scholarship and culture, despite the increasing difference between literary Latin and the vernacular languages of Europe during the period.\nWhile Latin was hugely influential, according to thirteenth-century English philosopher Roger Bacon, \"there are not four men in Latin Christendom who are acquainted with the Greek, Hebrew, and Arabic grammars.\" Greek was rarely studied in the West, and Greek literature was known almost solely in Latin translation. The works of even major Greek authors such as Hesiod, whose names continued to be known by educated Europeans, along with most of Plato, were unavailable in Christian Europe. Some were rediscovered through Arabic translations; a School of Translators was set up in the border city of Toledo, Spain, to translate from Arabic into Latin.\nAlong with the unavailability of Greek authors, there were other differences between the classical canon known today and the works valued in the Middle Ages. Catullus, for instance, was almost entirely unknown in the medieval period. The popularity of different authors also waxed and waned throughout the period: Lucretius, popular during the Carolingian Renaissance, was barely read in the twelfth century, while for Quintilian the reverse is true.\nRenaissance.\nThe Renaissance led to the increasing study of both ancient literature and ancient history, as well as a revival of classical styles of Latin. From the 14th century, first in Italy and then increasingly across Europe, Renaissance Humanism, an intellectual movement that \"advocated the study and imitation of classical antiquity\", developed. Humanism saw a reform in education in Europe, introducing a wider range of Latin authors as well as bringing back the study of Greek language and literature to Western Europe. This reintroduction was initiated by Petrarch (1304\u20131374) and Boccaccio (1313\u20131375) who commissioned a Calabrian scholar to translate the Homeric poems. This humanist educational reform spread from Italy, in Catholic countries as it was adopted by the Jesuits, and in countries that became Protestant such as England, Germany, and the Low Countries, in order to ensure that future clerics were able to study the New Testament in the original language.\nNeoclassicism.\nThe late 17th and 18th centuries are the period in Western European literary history which is most associated with the classical tradition, as writers consciously adapted classical models. Classical models were so highly prized that the plays of William Shakespeare were rewritten along neoclassical lines, and these \"improved\" versions were performed throughout the 18th century. In the United States, the nation's Founders were strongly influenced by the classics, and they looked in particular to the Roman Republic for their form of government.\nFrom the beginning of the 18th century, the study of Greek became increasingly important relative to that of Latin.\nIn this period Johann Winckelmann's claims for the superiority of the Greek visual arts influenced a shift in aesthetic judgements, while in the literary sphere, G. E. Lessing \"returned Homer to the centre of artistic achievement\".\nIn the United Kingdom, the study of Greek in schools began in the late 18th century. The poet Walter Savage Landor claimed to have been one of the first English schoolboys to write in Greek during his time at Rugby School. In the United States, philhellenism began to emerge in the 1830s, with a turn \"from a love of Rome and a focus on classical grammar to a new focus on Greece and the totality of its society, art, and culture.\"\n19th century.\nThe 19th century saw the influence of the classical world, and the value of a classical education, decline, especially in the United States, where the subject was often criticised for its elitism. By the 19th century, little new literature was still being written in Latin \u2013 a practice which had continued as late as the 18th century \u2013 and a command of Latin declined in importance. Correspondingly, classical education from the 19th century onwards began to increasingly de-emphasise the importance of the ability to write and speak Latin. In the United Kingdom this process took longer than elsewhere. Composition continued to be the dominant classical skill in England until the 1870s, when new areas within the discipline began to increase in popularity.\nIn the same decade came the first challenges to the requirement of Greek at the universities of Oxford and Cambridge, though it would not be finally abolished for another 50 years.\nThough the influence of classics as the dominant mode of education in Europe and North America was in decline in the 19th century, the discipline was rapidly evolving in the same period. Classical scholarship was becoming more systematic and scientific, especially with the \"new philology\" created at the end of the 18th and beginning of the 19th century. Its scope was also broadening: it was during the 19th century that ancient history and classical archaeology began to be seen as part of classics, rather than separate disciplines.\n20th century to present.\nDuring the 20th century, the study of classics became less common. In England, for instance, Oxford and Cambridge universities stopped requiring students to have qualifications in Greek in 1920, and in Latin at the end of the 1950s. When the National Curriculum was introduced in England, Wales, and Northern Ireland in 1988, it did not mention the classics. By 2003, only about 10% of state schools in Britain offered any classical subjects to their students at all. In 2016, AQA, the largest exam board for A-Levels and GCSEs in England, Wales and Northern Ireland, announced that it would be scrapping A-Level subjects in Classical Civilisation, Archaeology, and Art History. This left just one out of five exam boards in England which still offered Classical Civilisation as a subject. The decision was immediately denounced by archaeologists and historians, with Natalie Haynes of \"The Guardian\" stating that the loss of the A-Level would deprive state school students, 93% of all students, the opportunity to study classics while making it once again the exclusive purview of wealthy private-school students.\nHowever, the study of classics has not declined as fast elsewhere in Europe. In 2009, a review of \"Meeting the Challenge\", a collection of conference papers about the teaching of Latin in Europe, noted that though there is opposition to the teaching of Latin in Italy, it is nonetheless still compulsory in most secondary schools. The same may also be said in the case of France or Greece. Indeed, Ancient Greek is one of the compulsory subjects in Greek secondary education, whereas in France, Latin is one of the optional subjects that can be chosen in a majority of middle schools and high schools. Ancient Greek is also still being taught, but not as much as Latin.\nSubdisciplines.\nOne of the most notable characteristics of the modern study of classics is the diversity of the field. Although traditionally focused on ancient Greece and Rome, the study now encompasses the entire ancient Mediterranean world, thus expanding the studies to Northern Africa and parts of the Middle East.\nPhilology.\nPhilology is the study of language preserved in written sources; classical philology is thus concerned with understanding any texts from the classical period written in the classical languages of Latin and Greek.\nThe roots of classical philology lie in the Renaissance, as humanist intellectuals attempted to return to the Latin of the classical period, especially of Cicero, and as scholars attempted to produce more accurate editions of ancient texts. Some of the principles of philology still used today were developed during this period; for instance, the observation that if a manuscript could be shown to be a copy of an earlier extant manuscript, then it provides no further evidence of the original text, was made as early as 1489 by Angelo Poliziano. Other philological tools took longer to be developed: the first statement, for instance, of the principle that a more difficult reading should be preferred over a simpler one, was in 1697 by Jean Le Clerc.\nThe modern discipline of classical philology began in Germany at the turn of the nineteenth century. It was during this period that scientific principles of philology began to be put together into a coherent whole, in order to provide a set of rules by which scholars could determine which manuscripts were most accurate. This \"new philology\", as it was known, centered around the construction of a genealogy of manuscripts, with which a hypothetical common ancestor, closer to the original text than any existing manuscript, could be reconstructed.\nArchaeology.\nClassical archaeology is the oldest branch of archaeology, with its roots going back to J. J. Winckelmann's work on Herculaneum in the 1760s. It was not until the last decades of the 19th century, however, that classical archaeology became part of the tradition of Western classical scholarship. It was included as part of Cambridge University's Classical Tripos for the first time after the reforms of the 1880s, though it did not become part of Oxford's Greats until much later.\nThe second half of the 19th century saw Schliemann's excavations of Troy and Mycenae; the first excavations at Olympia and Delos; and Arthur Evans' work in Crete, particularly on Knossos. This period also saw the foundation of important archaeological associations (e.g. the Archaeological Institute of America in 1879), including many foreign archaeological institutes in Athens and Rome (the American School of Classical Studies at Athens in 1881, British School at Athens in 1886, American Academy in Rome in 1895, and British School at Rome in 1900).\nMore recently, classical archaeology has taken little part in the theoretical changes in the rest of the discipline, largely ignoring the popularity of \"New Archaeology\", which emphasized the development of general laws derived from studying material culture, in the 1960s. New Archaeology is still criticized by traditional minded scholars of classical archaeology despite a wide acceptance of its basic techniques.\nArt history.\nSome art historians focus their study on the development of art in the classical world. Indeed, the art and architecture of ancient Rome and Greece is very well regarded and remains at the heart of much of our art today. For example, ancient Greek architecture gave us the classical orders: Doric, Ionic, and Corinthian. The Parthenon is still the architectural symbol of the classical world.\nGreek sculpture is well known and we know the names of several ancient Greek artists: for example, Phidias.\nAncient history.\nWith philology, archaeology, and art history, scholars seek understanding of the history and culture of a civilization, through critical study of the extant literary and physical artefacts, in order to compose and establish a continual historic narrative of the Ancient World and its peoples. The task is difficult due to a dearth of physical evidence: for example, Sparta was a leading Greek city-state, yet little evidence of it survives to study, and what is available comes from Athens, Sparta's principal rival; likewise, the Roman Empire destroyed most evidence (cultural artefacts) of earlier, conquered civilizations, such as that of the Etruscans.\nPhilosophy.\nThe English word \"philosophy\" comes from the Greek word \u03c6\u03b9\u03bb\u03bf\u03c3\u03bf\u03c6\u03af\u03b1, meaning \"love of wisdom\", probably coined by Pythagoras. Along with the word itself, the discipline of philosophy as we know it today has its roots in ancient Greek thought, and according to Martin West \"philosophy as we understand it is a Greek creation\". Ancient philosophy was traditionally divided into three branches: logic, physics, and ethics. However, not all of the works of ancient philosophers fit neatly into one of these three branches. For instance, Aristotle's \"Rhetoric\" and \"Poetics\" have been traditionally classified in the West as \"ethics\", but in the Arabic world were grouped with logic; in reality, they do not fit neatly into either category.\nFrom the last decade of the eighteenth century, scholars of ancient philosophy began to study the discipline historically. Previously, works on ancient philosophy had been unconcerned with chronological sequence and with reconstructing the reasoning of ancient thinkers; with what Wolfgang-Ranier Mann calls \"New Philosophy\", this changed.\nReception studies.\nAnother discipline within the classics is \"reception studies\", which developed in the 1960s at the University of Konstanz.\nReception studies is concerned with how students of classical texts have understood and interpreted them.\nAs such, reception studies is interested in a two-way interaction between reader and text, taking place within a historical context.\nThough the idea of an \"aesthetics of reception\" was first put forward by Hans Robert Jauss in 1967, the principles of reception theory go back much earlier than this.\nAs early as 1920, T. S. Eliot wrote that \"the past [is] altered by the present as much as the present is directed by the past\"; Charles Martindale describes this as a \"cardinal principle\" for many versions of modern reception theory.\nClassical Greece.\nAncient Greece was the civilization belonging to the period of Greek history lasting from the Archaic period, beginning in the eighth century BC, to the Roman conquest of Greece after the Battle of Corinth in 146 BC. The Classical period, during the fifth and fourth centuries BC, has traditionally been considered the height of Greek civilisation. The Classical period of Greek history is generally considered to have begun with the first and second Persian invasions of Greece at the start of the Greco-Persian wars, and to have ended with the death of Alexander the Great.\nClassical Greek culture had a powerful influence on the Roman Empire, which carried a version of it to many parts of the Mediterranean region and Europe; thus Classical Greece is generally considered to be the seminal culture which provided the foundation of Western civilization.\nLanguage.\nAncient Greek is the historical stage in the development of the Greek language spanning the Archaic ( to 6th centuries BC), Classical ( to 4th centuries BC), and Hellenistic ( century BC to 6th century AD) periods of ancient Greece and the ancient world. It is predated in the 2nd millennium BC by Mycenaean Greek. Its Hellenistic phase is known as Koine (\"common\") or Biblical Greek, and its late period mutates imperceptibly into Medieval Greek. Koine is regarded as a separate historical stage of its own, although in its earlier form it closely resembles Classical Greek. Prior to the Koine period, Greek of the classical and earlier periods included several regional dialects.\nAncient Greek was the language of Homer and of classical Athenian historians, playwrights, and philosophers. It has contributed many words to the vocabulary of English and many other European languages, and has been a standard subject of study in Western educational institutions since the Renaissance. Latinized forms of Ancient Greek roots are used in many of the scientific names of species and in other scientific terminology.\nLiterature.\nThe earliest surviving works of Greek literature are epic poetry. Homer's \"Iliad\" and \"Odyssey\" are the earliest to survive to us today, probably composed in the eighth century BC. These early epics were oral compositions, created without the use of writing.\nAround the same time that the Homeric epics were composed, the Greek alphabet was introduced; the earliest surviving inscriptions date from around 750 BC.\nEuropean drama was invented in ancient Greece. Traditionally this was attributed to Thespis, around the middle of the sixth century BC, though the earliest surviving work of Greek drama is Aeschylus' tragedy \"The Persians\", which dates to 472 BC. Early Greek tragedy was performed by a chorus and two actors, but by the end of Aeschylus' life, a third actor had been introduced, either by him or by Sophocles. The last surviving Greek tragedies are the \"Bacchae\" of Euripides and Sophocles' Oedipus at Colonus, both from the end of the fifth century BC.\nSurviving Greek comedy begins later than tragedy; the earliest surviving work, Aristophanes' \"Acharnians\", comes from 425 BC. However, comedy dates back as early as 486 BC, when the Dionysia added a competition for comedy to the much earlier competition for tragedy. The comedy of the fifth century is known as Old Comedy, and it comes down to us solely in the eleven surviving plays of Aristophanes, along with a few fragments. Sixty years after the end of Aristophanes' career, the next author of comedies to have any substantial body of work survive is Menander, whose style is known as New Comedy.\nMythology and religion.\nGreek mythology is the body of myths and legends belonging to the ancient Greeks concerning their gods and heroes, the nature of the world, and the origins and significance of their own cult and ritual practices. They were a part of religion in ancient Greece. Modern scholars refer to the myths and study them in an attempt to throw light on the religious and political institutions of ancient Greece and its civilization, and to gain understanding of the nature of myth-making itself.\nGreek religion encompassed the collection of beliefs and rituals practiced in ancient Greece in the form of both popular public religion and cult practices. These different groups varied enough for it to be possible to speak of Greek religions or \"cults\" in the plural, though most of them shared similarities. Also, the Greek religion extended out of Greece and out to neighbouring islands.\nMany Greek people recognized the major gods and goddesses: Zeus, Poseidon, Hades, Apollo, Artemis, Aphrodite, Ares, Dionysus, Hephaestus, Athena, Hermes, Demeter, Hestia and Hera; though philosophies such as Stoicism and some forms of Platonism used language that seems to posit a transcendent single deity. Different cities often worshipped the same deities, sometimes with epithets that distinguished them and specified their local nature.\nPhilosophy.\nThe earliest surviving philosophy from ancient Greece dates back to the 6th century BC, when according to Aristotle Thales of Miletus was considered to have been the first Greek philosopher. Other influential pre-Socratic philosophers include Pythagoras and Heraclitus. The most famous and significant figures in classical Athenian philosophy, from the 5th to the 3rd centuries BC, are Socrates, his student Plato, and Aristotle, who studied at Plato's Academy before founding his own school, known as the Lyceum. Later Greek schools of philosophy, including the Cynics, Stoics, and Epicureans, continued to be influential after the Roman annexation of Greece, and into the post-Classical world.\nGreek philosophy dealt with a wide variety of subjects, including political philosophy, ethics, metaphysics, ontology, and logic, as well as disciplines which are not today thought of as part of philosophy, such as biology and rhetoric.\nClassical Rome.\nLanguage.\nThe language of ancient Rome was Latin, a member of the Italic family of languages. The earliest surviving inscription in Latin comes from the 7th century BC, on a brooch from Palestrina. Latin from between this point and the early 1st century BC is known as Old Latin. Most surviving Latin literature is Classical Latin, from the 1st century BC to the 2nd century AD. Latin then evolved into Late Latin, in use during the late antique period. Late Latin survived long after the end of classical antiquity, and was finally replaced by written Romance languages around the 9th century AD. Along with literary forms of Latin, there existed various vernacular dialects, generally known as Vulgar Latin, in use throughout antiquity. These are mainly preserved in sources such as graffiti and the Vindolanda tablets.\nLiterature.\nLatin literature seems to have started in 240 BC, when a Roman audience saw a play adapted from the Greek by Livius Andronicus. Andronicus also translated Homer's \"Odyssey\" into Saturnian verse. The poets Ennius, Accius, and Patruvius followed. Their work survives only in fragments; the earliest Latin authors whose work we have full examples of are the playwrights Plautus and Terence. Much of the best known and most highly thought of Latin literature comes from the classical period, with poets such as Virgil, Horace, and Ovid; historians such as Julius Caesar and Tacitus; orators such as Cicero; and philosophers such as Seneca the Younger and Lucretius. Late Latin authors include many Christian writers such as Lactantius, Tertullian and Ambrose; non-Christian authors, such as the historian Ammianus Marcellinus, are also preserved.\nHistory.\nAccording to legend, the city of Rome was founded in 753 BC; in reality, there had been a settlement on the site since around 1000 BC, when the Palatine Hill was settled. The city was originally ruled by kings, first Roman, and then Etruscanaccording to Roman tradition, the first Etruscan king of Rome, Tarquinius Priscus, ruled from 616 BC. Over the course of the 6th century BC, the city expanded its influence over the entirety of Latium. Around the end of the 6th century \u2013 traditionally in 510 BCthe kings of Rome were driven out, and the city became a republic.\nAround 387 BC, Rome was sacked by the Gauls following the Battle of the Allia. It soon recovered from this humiliating defeat, however, and in 381 the inhabitants of Tusculum in Latium were made Roman citizens. This was the first time Roman citizenship was extended in this way. Rome went on to expand its area of influence, until by 269 the entirety of the Italian peninsula was under Roman rule. Soon afterwards, in 264, the First Punic War began; it lasted until 241. The Second Punic War began in 218, and by the end of that year, the Carthaginian general Hannibal had invaded Italy. The war saw Rome's worst defeat to that point at Cannae; the largest army Rome had yet put into the field was wiped out, and one of the two consuls leading it was killed. However, Rome continued to fight, annexing much of Spain and eventually defeating Carthage, ending her position as a major power and securing Roman preeminence in the Western Mediterranean.\nLegacy of the classical world.\nThe classical languages of the ancient Mediterranean world influenced every European language, imparting to each a learned vocabulary of international application. Thus, Latin grew from a highly developed cultural product of the Golden and Silver eras of Latin literature to become the \"international lingua franca\" in matters diplomatic, scientific, philosophic and religious, until the 17th century. Long before this, Latin had evolved into the Romance languages and Ancient Greek into Modern Greek and its dialects. In the specialised science and technology vocabularies, the influence of Latin and Greek is notable. Ecclesiastical Latin, the Roman Catholic Church's official language, remains a living legacy of the classical world in the contemporary world.\nLatin had an impact far beyond the classical world. It continued to be the pre-eminent language for serious writings in Europe long after the fall of the Roman Empire. The modern Romance languages (French, Italian, Portuguese, Romanian, Spanish, Galician, Catalan) all derive from Latin. Latin is still seen as a foundational aspect of European culture.\nThe legacy of the classical world is not confined to the influence of classical languages. The Roman Empire was taken as a model by later European empires, such as the Spanish and British empires. Classical art has been taken as a model in later periods \u2013 medieval Romanesque architecture and Enlightenment-era neoclassical literature were both influenced by classical models, to take but two examples, while James Joyce's \"Ulysses\" is one of the most influential works of twentieth-century literature."}
{"id": "5180", "revid": "1398", "url": "https://en.wikipedia.org/wiki?curid=5180", "title": "Chemistry", "text": "Chemistry is the scientific study of the properties and behavior of matter. It is a physical science within the natural sciences that studies the chemical elements that make up matter and compounds made of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during reactions with other substances. Chemistry also addresses the nature of chemical bonds in chemical compounds.\nIn the scope of its subject, chemistry occupies an intermediate position between physics and biology. It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level. For example, chemistry explains aspects of plant growth (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the Moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).\nChemistry has existed under various names since ancient times. It has evolved, and now chemistry encompasses various areas of specialisation, or subdisciplines, that continue to increase in number and interrelate to create further interdisciplinary fields of study. The applications of various fields of chemistry are used frequently for economic purposes in the chemical industry.\nEtymology.\nThe word \"chemistry\" comes from a modification during the Renaissance of the word \"alchemy,\" which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism, and medicine. Alchemy is often associated with the quest to turn lead or other base metals into gold, though alchemists were also interested in many of the questions of modern chemistry.\nThe modern word \"alchemy\" in turn is derived from the Arabic word (). This may have Egyptian origins since is derived from the Ancient Greek , which is in turn derived from the word , which is the ancient name of Egypt in the Egyptian language. Alternately, may derive from 'cast together'.\nModern principles.\nThe current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. Matter can be studied in solid, liquid, gas and plasma states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory.\nThe chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.\nA chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws.\nEnergy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:\nMatter.\nIn chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well \u2013 not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances.\nAtom.\nThe atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space occupied by an electron cloud. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus. In a neutral atom, the negatively charged electrons balance out the positive charge of the protons. The nucleus is dense; the mass of a nucleon is approximately 1,836 times that of an electron, yet the radius of an atom is about 10,000 times that of its nucleus.\nThe atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential, preferred oxidation state(s), coordination number, and preferred types of bonds to form (e.g., metallic, ionic, covalent).\nElement.\nA chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol \"Z\". The mass number is the sum of the number of protons and neutrons in a nucleus. Although all the nuclei of all atoms belonging to one element will have the same atomic number, they may not necessarily have the same mass number; atoms of an element which have different mass numbers are known as isotopes. For example, all atoms with 6 protons in their nuclei are atoms of the chemical element carbon, but atoms of carbon may have mass numbers of 12 or 13.\nThe standard presentation of the chemical elements is in the periodic table, which orders elements by atomic number. The periodic table is arranged in groups, or columns, and periods, or rows. The periodic table is useful in identifying periodic trends.\nCompound.\nA \"compound\" is a pure chemical substance composed of more than one element. The properties of a compound bear little similarity to those of its elements. The standard nomenclature of compounds is set by the International Union of Pure and Applied Chemistry (IUPAC). Organic compounds are named according to the organic nomenclature system. The names for inorganic compounds are created according to the inorganic nomenclature system. When a compound has more than one component, then they are divided into two classes, the electropositive and the electronegative components. In addition the Chemical Abstracts Service has devised a method to index chemical substances. In this scheme each chemical substance is identifiable by a number known as its CAS registry number.\nMolecule.\nA \"molecule\" is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties, that is, its potential to undergo a certain set of chemical reactions with other substances. However, this definition only works well for substances that are composed of molecules, which is not true of many substances (see below). Molecules are typically a set of atoms bound together by covalent bonds, such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs.\nThus, molecules exist as electrically neutral units, unlike ions. When this rule is broken, giving the \"molecule\" a charge, the result is sometimes named a molecular ion or a polyatomic ion. However, the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well-separated form, such as a directed beam in a vacuum in a mass spectrometer. Charged polyatomic collections residing in solids (for example, common sulfate or nitrate ions) are generally not considered \"molecules\" in chemistry. Some molecules contain one or more unpaired electrons, creating radicals. Most radicals are comparatively reactive, but some, such as nitric oxide (NO) can be stable.\nThe \"inert\" or noble gas elements (helium, neon, argon, krypton, xenon and radon) are composed of lone atoms as their smallest discrete unit, but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way. Identifiable molecules compose familiar substances such as water, air, and many organic compounds like alcohol, sugar, gasoline, and the various pharmaceuticals.\nHowever, not all substances or chemical compounds consist of discrete molecules, and indeed most of the solid substances that make up the solid crust, mantle, and core of the Earth are chemical compounds without molecules. These other types of substances, such as ionic compounds and network solids, are organized in such a way as to lack the existence of identifiable molecules \"per se\". Instead, these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance. Examples of such substances are mineral salts (such as table salt), solids like carbon and diamond, metals, and familiar silica and silicate minerals such as quartz and granite.\nOne of the main characteristics of a molecule is its geometry often called its structure. While the structure of diatomic, triatomic or tetra-atomic molecules may be trivial, (linear, angular pyramidal etc.) the structure of polyatomic molecules, that are constituted of more than six atoms (of several elements) can be crucial for its chemical nature.\nSubstance and mixture.\nA chemical substance is a kind of matter with a definite composition and set of properties. A collection of substances is called a mixture. Examples of mixtures are air and alloys.\nMole and amount of substance.\nThe mole is a unit of measurement that denotes an amount of substance (also called chemical amount). One mole is defined to contain exactly particles (atoms, molecules, ions, or electrons), where the number of particles per mole is known as the Avogadro constant. Molar concentration is the amount of a particular substance per volume of solution, and is commonly reported in mol/dm3.\nPhase.\nIn addition to the specific chemical properties that distinguish different chemical classifications, chemicals can exist in several phases. For the most part, the chemical classifications are independent of these bulk phase classifications; however, some more exotic phases are incompatible with certain chemical properties. A \"phase\" is a set of states of a chemical system that have similar bulk structural properties, over a range of conditions, such as pressure or temperature.\nPhysical properties, such as density and refractive index tend to fall within values characteristic of the phase. The phase of matter is defined by the \"phase transition\", which is when energy put into or taken out of the system goes into rearranging the structure of the system, instead of changing the bulk conditions.\nSometimes the distinction between phases can be continuous instead of having a discrete boundary; in this case the matter is considered to be in a supercritical state. When three states meet based on the conditions, it is known as a triple point and since this is invariant, it is a convenient way to define a set of conditions.\nThe most familiar examples of phases are solids, liquids, and gases. Many substances exhibit multiple solid phases. For example, there are three phases of solid iron (alpha, gamma, and delta) that vary based on temperature and pressure. A principal difference between solid phases is the crystal structure, or arrangement, of the atoms. Another phase commonly encountered in the study of chemistry is the \"aqueous\" phase, which is the state of substances dissolved in aqueous solution (that is, in water).\nLess familiar phases include plasmas, Bose\u2013Einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials. While most familiar phases deal with three-dimensional systems, it is also possible to define analogs in two-dimensional systems, which has received attention for its relevance to systems in biology.\nBonding.\nAtoms sticking together in molecules or crystals are said to be bonded with one another. A chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them. More than simple attraction and repulsion, the energies and distributions characterize the availability of an electron to bond to another atom.\nThe chemical bond can be a covalent bond, an ionic bond, a hydrogen bond or just because of Van der Waals force. Each of these kinds of bonds is ascribed to some potential. These potentials create the interactions which hold atoms together in molecules or crystals. In many simple compounds, valence bond theory, the Valence Shell Electron Pair Repulsion model (VSEPR), and the concept of oxidation number can be used to explain molecular structure and composition.\nAn ionic bond is formed when a metal loses one or more of its electrons, becoming a positively charged cation, and the electrons are then gained by the non-metal atom, becoming a negatively charged anion. The two oppositely charged ions attract one another, and the ionic bond is the electrostatic force of attraction between them. For example, sodium (Na), a metal, loses one electron to become an Na+ cation while chlorine (Cl), a non-metal, gains this electron to become Cl\u2212. The ions are held together due to electrostatic attraction, and that compound sodium chloride (NaCl), or common table salt, is formed.\nIn a covalent bond, one or more pairs of valence electrons are shared by two atoms: the resulting electrically neutral group of bonded atoms is termed a molecule. Atoms will share valence electrons in such a way as to create a noble gas electron configuration (eight electrons in their outermost shell) for each atom. Atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule. However, some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration; these atoms are said to follow the \"duet rule\", and in this way they are reaching the electron configuration of the noble gas helium, which has two electrons in its outer shell.\nSimilarly, theories from classical physics can be used to predict many ionic structures. With more complicated compounds, such as metal complexes, valence bond theory is less applicable and alternative approaches, such as the molecular orbital theory, are generally used.\nEnergy.\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structures, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants.\nA reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. A reaction is said to be exothermic if the reaction releases heat to the surroundings; in the case of endothermic reactions, the reaction absorbs heat from the surroundings.\nChemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The \"speed\" of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor formula_1 \u2013 that is the probability of a molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.\nThe activation energy necessary for a chemical reaction to occur can be in the form of heat, light, electricity or mechanical force in the form of ultrasound.\nA related concept free energy, which also incorporates entropy considerations, is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction, in chemical thermodynamics. A reaction is feasible only if the total change in the Gibbs free energy is negative, formula_2; if it is equal to zero the chemical reaction is said to be at equilibrium.\nThere exist only limited possible states of energy for electrons, atoms and molecules. These are determined by the rules of quantum mechanics, which require quantization of energy of a bound system. The atoms/molecules in a higher energy state are said to be excited. The molecules/atoms of substance in an excited energy state are often much more reactive; that is, more amenable to chemical reactions.\nThe phase of a substance is invariably determined by its energy and the energy of its surroundings. When the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them, it occurs in a more ordered phase like liquid or solid as is the case with water (H2O); a liquid at room temperature because its molecules are bound by hydrogen bonds. Whereas hydrogen sulfide (H2S) is a gas at room temperature and standard pressure, as its molecules are bound by weaker dipole\u2013dipole interactions.\nThe transfer of energy from one chemical substance to another depends on the \"size\" of energy quanta emitted from one substance. However, heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer. Thus, because vibrational and rotational energy levels are more closely spaced than electronic energy levels, heat is more easily transferred between substances relative to light or other forms of electronic energy. For example, ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy.\nThe existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines. Different kinds of spectra are often used in chemical spectroscopy, e.g. IR, microwave, NMR, ESR, etc. Spectroscopy is also used to identify the composition of remote objects \u2013 like stars and distant galaxies \u2013 by analyzing their radiation spectra.\nThe term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances.\nReaction.\nWhen a chemical substance is transformed as a result of its interaction with another substance or with energy, a chemical reaction is said to have occurred. A \"chemical reaction\" is therefore a concept related to the \"reaction\" of a substance when it comes in close contact with another, whether as a mixture or a solution; exposure to some form of energy, or both. It results in some energy exchange between the constituents of the reaction as well as with the system environment, which may be designed vessels\u2014often laboratory glassware.\nChemical reactions can result in the formation or dissociation of molecules, that is, molecules breaking apart to form two or more molecules or rearrangement of atoms within or across molecules. Chemical reactions usually involve the making or breaking of chemical bonds. Oxidation, reduction, dissociation, acid\u2013base neutralization and molecular rearrangement are some examples of common chemical reactions.\nA chemical reaction can be symbolically depicted through a chemical equation. While in a non-nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal, for a nuclear reaction this holds true only for the nuclear particles viz. protons and neutrons.\nThe sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism. A chemical reaction can be envisioned to take place in a number of steps, each of which may have a different speed. Many reaction intermediates with variable stability can thus be envisaged during the course of a reaction. Reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction. Many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions. Several empirical rules, like the Woodward\u2013Hoffmann rules often come in handy while proposing a mechanism for a chemical reaction.\nAccording to the IUPAC gold book, a chemical reaction is \"a process that results in the interconversion of chemical species.\" Accordingly, a chemical reaction may be an elementary reaction or a stepwise reaction. An additional caveat is made, in that this definition includes cases where the interconversion of conformers is experimentally observable. Such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition, but it is often conceptually convenient to use the term also for changes involving single molecular entities (i.e. 'microscopic chemical events').\nIons and salts.\nAn \"ion\" is a charged species, an atom or a molecule, that has lost or gained one or more electrons. When an atom loses an electron and thus has more protons than electrons, the atom is a positively charged ion or cation. When an atom gains an electron and thus has more electrons than protons, the atom is a negatively charged ion or anion. Cations and anions can form a crystalline lattice of neutral salts, such as the Na+ and Cl\u2212 ions forming sodium chloride, or NaCl. Examples of polyatomic ions that do not split up during acid\u2013base reactions are hydroxide (OH\u2212) and phosphate (PO43\u2212).\nPlasma is composed of gaseous matter that has been completely ionized, usually through high temperature.\nAcidity and basicity.\nA substance can often be classified as an acid or a base. There are several different theories which explain acid\u2013base behavior. The simplest is Arrhenius theory, which states that an acid is a substance that produces hydronium ions when it is dissolved in water, and a base is one that produces hydroxide ions when dissolved in water. According to Br\u00f8nsted\u2013Lowry acid\u2013base theory, acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction; by extension, a base is the substance which receives that hydrogen ion.\nA third common theory is Lewis acid\u2013base theory, which is based on the formation of new chemical bonds. Lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation, while a base is a substance which can provide a pair of electrons to form a new bond. There are several other ways in which a substance may be classified as an acid or a base, as is evident in the history of this concept.\nAcid strength is commonly measured by two methods. One measurement, based on the Arrhenius definition of acidity, is pH, which is a measurement of the hydronium ion concentration in a solution, as expressed on a negative logarithmic scale. Thus, solutions that have a low pH have a high hydronium ion concentration and can be said to be more acidic. The other measurement, based on the Br\u00f8nsted\u2013Lowry definition, is the acid dissociation constant (Ka), which measures the relative ability of a substance to act as an acid under the Br\u00f8nsted\u2013Lowry definition of an acid. That is, substances with a higher Ka are more likely to donate hydrogen ions in chemical reactions than those with lower Ka values.\nRedox.\nRedox (-) reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons (reduction) or losing electrons (oxidation). Substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents, oxidants or oxidizers. An oxidant removes electrons from another substance. Similarly, substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents, reductants, or reducers.\nA reductant transfers electrons to another substance and is thus oxidized itself. And because it \"donates\" electrons it is also called an electron donor. Oxidation and reduction properly refer to a change in oxidation number\u2014the actual transfer of electrons may never occur. Thus, oxidation is better defined as an increase in oxidation number, and reduction as a decrease in oxidation number.\nEquilibrium.\nAlthough the concept of equilibrium is widely used across sciences, in the context of chemistry, it arises whenever a number of different states of the chemical composition are possible, as for example, in a mixture of several chemical compounds that can react with one another, or when a substance can be present in more than one kind of phase.\nA system of chemical substances at equilibrium, even though having an unchanging composition, is most often not static; molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium. Thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time.\nChemical laws.\nChemical reactions are governed by certain laws, which have become fundamental concepts in chemistry. Some of them are:\nHistory.\nThe history of chemistry spans a period from the ancient past to the present. Since several millennia BC, civilizations were using technologies that would eventually form the basis of the various branches of chemistry. Examples include extracting metals from ores, making pottery and glazes, fermenting beer and wine, extracting chemicals from plants for medicine and perfume, rendering fat into soap, making glass, and making alloys like bronze.\nChemistry was preceded by its protoscience, alchemy, which operated a non-scientific approach to understanding the constituents of matter and their interactions. Despite being unsuccessful in explaining the nature of matter and its transformations, alchemists set the stage for modern chemistry by performing experiments and recording the results. Robert Boyle, although skeptical of elements and convinced of alchemy, played a key part in elevating the \"sacred art\" as an independent, fundamental and philosophical discipline in his work \"The Sceptical Chymist\" (1661).\nWhile both alchemy and chemistry are concerned with matter and its transformations, the crucial difference was given by the scientific method that chemists employed in their work. Chemistry, as a body of knowledge distinct from alchemy, became an established science with the work of Antoine Lavoisier, who developed a law of conservation of mass that demanded careful measurement and quantitative observations of chemical phenomena. The history of chemistry afterwards is intertwined with the history of thermodynamics, especially through the work of Willard Gibbs.\nDefinition.\nThe definition of chemistry has changed over time, as new discoveries and theories add to the functionality of the science. The term \"chymistry\", in the view of noted scientist Robert Boyle in 1661, meant the subject of the material principles of mixed bodies. In 1663, the chemist Christopher Glaser described \"chymistry\" as a scientific art, by which one learns to dissolve bodies, and draw from them the different substances on their composition, and how to unite them again, and exalt them to a higher perfection.\nThe 1730 definition of the word \"chemistry\", as used by Georg Ernst Stahl, meant the art of resolving mixed, compound, or aggregate bodies into their principles; and of composing such bodies from those principles. In 1837, Jean-Baptiste Dumas considered the word \"chemistry\" to refer to the science concerned with the laws and effects of molecular forces. This definition further evolved until, in 1947, it came to mean the science of substances: their structure, their properties, and the reactions that change them into other substances\u2014a characterization accepted by Linus Pauling. More recently, in 1998, Professor Raymond Chang broadened the definition of \"chemistry\" to mean the study of matter and the changes it undergoes.\nBackground.\nEarly civilizations, such as the Egyptians, Babylonians, and Indians, amassed practical knowledge concerning the arts of metallurgy, pottery and dyes, but did not develop a systematic theory.\nA basic chemical hypothesis first emerged in Classical Greece with the theory of four elements as propounded definitively by Aristotle stating that fire, air, earth and water were the fundamental elements from which everything is formed as a combination. Greek atomism dates back to 440 BC, arising in works by philosophers such as Democritus and Epicurus. In 50 BCE, the Roman philosopher Lucretius expanded upon the theory in his poem \"De rerum natura\" (On The Nature of Things). Unlike modern concepts of science, Greek atomism was purely philosophical in nature, with little concern for empirical observations and no concern for chemical experiments.\nAn early form of the idea of conservation of mass is the notion that \"Nothing comes from nothing\" in Ancient Greek philosophy, which can be found in Empedocles (approx. 4th century BC): \"For it is impossible for anything to come to be from what is not, and it cannot be brought about or heard of that what is should be utterly destroyed.\" and Epicurus (3rd century BC), who, describing the nature of the Universe, wrote that \"the totality of things was always such as it is now, and always will be\".\nIn the Hellenistic world the art of alchemy first proliferated, mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life. Work, particularly the development of distillation, continued in the early Byzantine period with the most famous practitioner being the 4th century Greek-Egyptian Zosimos of Panopolis. Alchemy continued to be developed and practised throughout the Arab world after the Muslim conquests, and from there, and from the Byzantine remnants, diffused into medieval and Renaissance Europe through Latin translations.\nThe Arabic works attributed to Jabir ibn Hayyan introduced a systematic classification of chemical substances, and provided instructions for deriving an inorganic compound (sal ammoniac or ammonium chloride) from organic substances (such as plants, blood, and hair) by chemical means. Some Arabic Jabirian works (e.g., the \"Book of Mercy\", and the \"Book of Seventy\") were later translated into Latin under the Latinized name \"Geber\", and in 13th-century Europe an anonymous writer, usually referred to as pseudo-Geber, started to produce alchemical and metallurgical writings under this name. Later influential Muslim philosophers, such as Ab\u016b al-Rayh\u0101n al-B\u012br\u016bn\u012b and Avicenna disputed the theories of alchemy, particularly the theory of the transmutation of metals.\nImprovements of the refining of ores and their extractions to smelt metals was widely used source of information for early chemists in the 16th century, among them Georg Agricola (1494\u20131555), who published his major work \"De re metallica\" in 1556. His work, describing highly developed and complex processes of mining metal ores and metal extraction, were the pinnacle of metallurgy during that time. His approach removed all mysticism associated with the subject, creating the practical base upon which others could and would build. The work describes the many kinds of furnace used to smelt ore, and stimulated interest in minerals and their composition. Agricola has been described as the \"father of metallurgy\" and the founder of geology as a scientific discipline.\nUnder the influence of the new empirical methods propounded by Sir Francis Bacon and others, a group of chemists at Oxford, Robert Boyle, Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline. Boyle in particular questioned some commonly held chemical theories and argued for chemical practitioners to be more \"philosophical\" and less commercially focused in \"The Sceptical Chemyst\". He formulated Boyle's law, rejected the classical \"four elements\" and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment.\nIn the following decades, many important discoveries were made, such as the nature of 'air' which was discovered to be composed of many different gases. The Scottish chemist Joseph Black and the Flemish Jan Baptist van Helmont discovered carbon dioxide, or what Black called 'fixed air' in 1754; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and, independently, Carl Wilhelm Scheele isolated pure oxygen. The theory of phlogiston (a substance at the root of all combustion) was propounded by the German Georg Ernst Stahl in the early 18th century and was only overturned by the end of the century by the French chemist Antoine Lavoisier, the chemical analogue of Newton in physics. Lavoisier did more than any other to establish the new science on proper theoretical footing, by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day.\nEnglish scientist John Dalton proposed the modern theory of atoms; that all substances are composed of indivisible 'atoms' of matter and that different atoms have varying atomic weights.\nThe development of the electrochemical theory of chemical combinations occurred in the early 19th century as the result of the work of two scientists in particular, J\u00f6ns Jacob Berzelius and Humphry Davy, made possible by the prior invention of the voltaic pile by Alessandro Volta. Davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current.\nBritish William Prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen. J.A.R. Newlands devised an early table of elements, which was then developed into the modern periodic table of elements in the 1860s by Dmitri Mendeleev and independently by several other scientists including Julius Lothar Meyer. The inert gases, later called the noble gases were discovered by William Ramsay in collaboration with Lord Rayleigh at the end of the century, thereby filling in the basic structure of the table.\nOrganic chemistry was developed by Justus von Liebig and others, following Friedrich W\u00f6hler's synthesis of urea. Other crucial 19th century advances were; an understanding of valence bonding (Edward Frankland in 1852) and the application of thermodynamics to chemistry (J. W. Gibbs and Svante Arrhenius in the 1870s).\nAt the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms. In 1897, J.J. Thomson of the University of Cambridge discovered the electron and soon after the French scientist Becquerel as well as the couple Pierre and Marie Curie investigated the phenomenon of radioactivity. In a series of pioneering scattering experiments Ernest Rutherford at the University of Manchester discovered the internal structure of the atom and the existence of the proton, classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles.\nHis work on atomic structure was improved on by his students, the Danish physicist Niels Bohr, the Englishman Henry Moseley and the German Otto Hahn, who went on to father the emerging nuclear chemistry and discovered nuclear fission. The electronic theory of chemical bonds and molecular orbitals was developed by the American scientists Linus Pauling and Gilbert N. Lewis.\nThe year 2011 was declared by the United Nations as the International Year of Chemistry. It was an initiative of the International Union of Pure and Applied Chemistry, and of the United Nations Educational, Scientific, and Cultural Organization and involves chemical societies, academics, and institutions worldwide and relied on individual initiatives to organize local and regional activities.\nPractice.\nIn the practice of chemistry, pure chemistry is the study of the fundamental principles of chemistry, while applied chemistry applies that knowledge to develop technology and solve real-world problems.\nSubdisciplines.\nChemistry is typically divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry.\nOther subdivisions include electrochemistry, femtochemistry, flavor chemistry, flow chemistry, immunohistochemistry, hydrogenation chemistry, mathematical chemistry, molecular mechanics, natural product chemistry, organometallic chemistry, petrochemistry, photochemistry, physical organic chemistry, polymer chemistry, radiochemistry, sonochemistry, supramolecular chemistry, synthetic chemistry, and many others.\nInterdisciplinary.\nInterdisciplinary fields include agrochemistry, astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemical biology, chemo-informatics, environmental chemistry, geochemistry, green chemistry, immunochemistry, marine chemistry, materials science, mechanochemistry, medicinal chemistry, molecular biology, nanotechnology, oenology, pharmacology, phytochemistry, solid-state chemistry, surface science, thermochemistry, and many others.\nIndustry.\nThe chemical industry represents an important economic activity worldwide. The global top 50 chemical producers in 2013 had sales of US$980.5 billion with a profit margin of 10.3%.\nFurther reading.\nPopular reading\nIntroductory undergraduate textbooks\nAdvanced undergraduate-level or graduate textbooks"}
{"id": "5181", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5181", "title": "Continents", "text": ""}
{"id": "5182", "revid": "29291527", "url": "https://en.wikipedia.org/wiki?curid=5182", "title": "ConstitutioN", "text": ""}
{"id": "5184", "revid": "1269301063", "url": "https://en.wikipedia.org/wiki?curid=5184", "title": "Cytoplasm", "text": "The cytoplasm describes all the material within a eukaryotic or prokaryotic cell, enclosed by the cell membrane, including the organelles and excluding the nucleus in eukaryotic cells. The material inside the nucleus of a eukaryotic cell and contained within the nuclear membrane is termed the nucleoplasm. The main components of the cytoplasm are the cytosol (a gel-like substance), the cell's internal sub-structures, and various cytoplasmic inclusions. In eukaryotes the cytoplasm also includes the nucleus, and other membrane-bound organelles.The cytoplasm is about 80% water and is usually colorless.\nThe submicroscopic ground cell substance, or cytoplasmic matrix, that remains after the exclusion of the cell organelles and particles is groundplasm. It is the hyaloplasm of light microscopy, a highly complex, polyphasic system in which all resolvable cytoplasmic elements are suspended, including the larger organelles such as the ribosomes, mitochondria, plant plastids, lipid droplets, and vacuoles.\nMany cellular activities take place within the cytoplasm, such as many metabolic pathways, including glycolysis, photosynthesis, and processes such as cell division. The concentrated inner area is called the endoplasm and the outer layer is called the cell cortex, or ectoplasm.\nMovement of calcium ions in and out of the cytoplasm is a signaling activity for metabolic processes.\nIn plants, movement of the cytoplasm around vacuoles is known as cytoplasmic streaming.\nHistory.\nThe term was introduced by Rudolf von K\u00f6lliker in 1863, originally as a synonym for protoplasm, but later it has come to mean the cell substance and organelles outside the nucleus.\nThere has been certain disagreement on the definition of cytoplasm, as some authors prefer to exclude from it some organelles, especially the vacuoles and sometimes the plastids.\nPhysical nature.\nIt remains uncertain how the various components of the cytoplasm interact to allow movement of organelles while maintaining the cell's structure. The flow of cytoplasmic components plays an important role in many cellular functions which are dependent on the permeability of the cytoplasm. An example of such function is cell signalling, a process which is dependent on the manner in which signaling molecules are allowed to diffuse across the cell. While small signaling molecules like calcium ions are able to diffuse with ease, larger molecules and subcellular structures often require aid in moving through the cytoplasm. The irregular dynamics of such particles have given rise to various theories on the nature of the cytoplasm.\nAs a sol-gel.\nThere has long been evidence that the cytoplasm behaves like a sol-gel. It is thought that the component molecules and structures of the cytoplasm behave at times like a disordered colloidal solution (sol) and at other times like an integrated network, forming a solid mass (gel). This theory thus proposes that the cytoplasm exists in distinct fluid and solid phases depending on the level of interaction between cytoplasmic components, which may explain the differential dynamics of different particles observed moving through the cytoplasm. A papers suggested that at length scale smaller than 100\u00a0nm, the cytoplasm acts like a liquid, while in a larger length scale, it acts like a gel.\nAs a glass.\nIt has been proposed that the cytoplasm behaves like a glass-forming liquid approaching the glass transition. In this theory, the greater the concentration of cytoplasmic components, the less the cytoplasm behaves like a liquid and the more it behaves as a solid glass, freezing more significant cytoplasmic components in place (it is thought that the cell's metabolic activity can fluidize the cytoplasm to allow the movement of such more significant cytoplasmic components). A cell's ability to vitrify in the absence of metabolic activity, as in dormant periods, may be beneficial as a defense strategy. A solid glass cytoplasm would freeze subcellular structures in place, preventing damage, while allowing the transmission of tiny proteins and metabolites, helping to kickstart growth upon the cell's revival from dormancy.\nOther perspectives.\nResearch has examined the motion of cytoplasmic particles independent of the nature of the cytoplasm. In such an alternative approach, the aggregate random forces within the cell caused by motor proteins explain the non-Brownian motion of cytoplasmic constituents.\nConstituents.\nThe three major elements of the cytoplasm are the cytosol, organelles and inclusions.\nCytosol.\nThe cytosol is the portion of the cytoplasm not contained within membrane-bound organelles. Cytosol makes up about 70% of the cell volume and is a complex mixture of cytoskeleton filaments, dissolved molecules, and water. The cytosol's filaments include the protein filaments such as actin filaments and microtubules that make up the cytoskeleton, as well as soluble proteins and small structures such as ribosomes, proteasomes, and the mysterious vault complexes. The inner, granular and more fluid portion of the cytoplasm is referred to as endoplasm.\nDue to this network of fibres and high concentrations of dissolved macromolecules, such as proteins, an effect called macromolecular crowding occurs and the cytosol does not act as an ideal solution. This crowding effect alters how the components of the cytosol interact with each other.\nOrganelles.\nOrganelles (literally \"little organs\") are usually membrane-bound structures inside the cell that have specific functions. Some major organelles that are suspended in the cytosol are the mitochondria, the endoplasmic reticulum, the Golgi apparatus, vacuoles, lysosomes, and in plant cells, chloroplasts.\nCytoplasmic inclusions.\nThe inclusions are small particles of insoluble substances suspended in the cytosol. A huge range of inclusions exist in different cell types, and range from crystals of calcium oxalate or silicon dioxide in plants, to granules of energy-storage materials such as starch, glycogen, or polyhydroxybutyrate. A particularly widespread example are lipid droplets, which are spherical droplets composed of lipids and proteins that are used in both prokaryotes and eukaryotes as a way of storing lipids such as fatty acids and sterols. Lipid droplets make up much of the volume of adipocytes, which are specialized lipid-storage cells, but they are also found in a range of other cell types.\nControversy and research.\nThe cytoplasm, mitochondria, and most organelles are contributions to the cell from the maternal gamete. Contrary to the older information that disregards any notion of the cytoplasm being active, new research has shown it to be in control of movement and flow of nutrients in and out of the cell by viscoplastic behavior and a measure of the reciprocal rate of bond breakage within the cytoplasmic network.\nThe material properties of the cytoplasm remain an ongoing investigation. A method of determining the mechanical behaviour of living cell mammalian cytoplasm with the aid of optical tweezers has been described."}
{"id": "5185", "revid": "1272632080", "url": "https://en.wikipedia.org/wiki?curid=5185", "title": "Christ (title)", "text": "Christ, used by Christians as both a name and a title, unambiguously refers to Jesus. It is also used as a title, in the reciprocal usage \"Christ Jesus\", meaning \"the Messiah Jesus\" or \"Jesus the Anointed\", and independently as \"the Christ\". The Pauline epistles, the earliest texts of the New Testament, often call Jesus \"Christ Jesus\" or just \"Christ\".\nThe concept of the Christ in Christianity originated from the concept of the messiah in Judaism. Christians believe that Jesus is the messiah foretold in the Hebrew Bible and the Christian Old Testament. Although the conceptions of the messiah in each religion are similar, for the most part they are distinct from one another due to the split of early Christianity and Judaism in the 1st century.\nAlthough the original followers of Jesus believed Jesus to be the Jewish messiah, e.g. in the Confession of Peter, he was usually called \"Jesus of Nazareth\" or \"Jesus, son of Joseph\". Jesus came to be called \"Jesus Christ\" (meaning \"Jesus the \"Khrist\u00f3s\"\", i.e. \"Jesus the Messiah\" or \"Jesus the Anointed\") by Christians, who believe that his crucifixion and resurrection fulfill the messianic prophecies of the Old Testament, especially the prophecies outlined in Isaiah 53 and Psalm 22.\nEtymology.\n\"Christ\" derives from the Greek word (), meaning literally \"anointed one\". The word is derived from the Greek verb (), meaning literally \"to anoint.\" In the Greek Septuagint, \"\u03c7\u03c1\u03b9\u03c3\u03c4\u03cc\u03c2\" was a semantic loan used to translate the Hebrew (, messiah), meaning \"[one who is] anointed\".\nUsage.\nThe word \"Christ\" (and similar spellings) appears in English and in most European languages. English speakers now often use \"Christ\" as if it were a name, one part of the name \"Jesus Christ\", though it was originally a title (\"the Messiah\"). Its usage in \"Christ Jesus\" emphasizes its nature as a title. Compare the usage \"the Christ\".\nThe spelling \"Christ\" in English became standardized in the 18th century, when, in the spirit of the Enlightenment, certain words' spelling changed to fit their Greek or Latin origins. Before that, scribes writing in Old and Middle English usually used the spelling \"Crist\"\u2014the \"i\" being pronounced either as , preserved in the names of churches such as St Katherine Cree, or as a short , preserved in the modern pronunciation of \"Christmas\". The spelling \"Christ\" in English is attested from the 14th century.\nIn modern and ancient usage, even in secular terminology, \"Christ\" usually refers to Jesus, based on the centuries-old tradition of such usage. Since the Apostolic Age, the use of the definite article before the word \"Christ\" and its gradual development into a proper name show the Christians identified the bearer with the promised Messias of the Jews.\nBackground and New Testament references.\nPre\u2013New-Testament references.\nIn the Old Testament, anointing was a ceremonial ritual reserved to:\nIn the Septuagint text of the deuterocanonical books, the term \"Christ\" (\u03a7\u03c1\u03b9\u03c3\u03c4\u03cc\u03c2, translit. Christ\u00f3s) is found in 2 Maccabees 1:10 (referring to the anointed High Priest of Israel) and in the Book of Sirach 46:19, in relation to Samuel, prophet and institutor of the kingdom under Saul.\nAt the time of Jesus, there was no single form of Second Temple Judaism, and there were significant political, social, and religious differences among the various Jewish groups. But for centuries the Jews had used the term \"moshiach\" (\"anointed\") to refer to their expected deliverer.\nOpening lines of Mark and Matthew.\nMark (\"The beginning of the gospel of Jesus Christ, the Son of God\") identifies Jesus as both Christ and the Son of God. uses Christ as a name and Matthew explains it again with: \"Jesus, who is called Christ\". The use of the definite article before the word \"Christ\" and its gradual development into a proper name show that the Christians identified Jesus with the promised messiah of the Jews who fulfilled all the messianic predictions in a fuller and a higher sense than had been given them by the rabbis.\nConfession of Peter (Matthew, Mark and Luke).\nThe so-called Confession of Peter, recorded in the Synoptic Gospels as Jesus's foremost apostle Peter saying that Jesus was the Messiah, has become a famous proclamation of faith among Christians since the first century.\nMartha's statement (John).\nIn Martha told Jesus, \"you are the Christ, the Son of God, who is coming into the world\", signifying that both titles were generally accepted (yet considered distinct) among the followers of Jesus before the raising of Lazarus.\nSanhedrin trial of Jesus (Matthew, Mark and Luke).\nDuring the Sanhedrin trial of Jesus, it might appear from the narrative of Matthew that Jesus at first refused a direct reply to the high priest Caiaphas's question: \"Are you the Messiah, the Son of God?\", where his answer is given merely as \u03a3\u1f7a \u03b5\u1f36\u03c0\u03b1\u03c2 (\"Su eipas\", \"You [singular] have said it\"). Similarly but differently in Luke, all those present are said to ask Jesus: 'Are you then the Son of God?', to which Jesus reportedly answered: \u1f59\u03bc\u03b5\u1fd6\u03c2 \u03bb\u03ad\u03b3\u03b5\u03c4\u03b5 \u1f45\u03c4\u03b9 \u1f10\u03b3\u03ce \u03b5\u1f30\u03bc\u03b9 (\"Hymeis legete hoti ego eimi\", \"You [plural] say that I am\". In the Gospel of Mark, however, when asked by Caiaphas 'Are you the Messiah, the Son of the Blessed One?', Jesus tells the Sanhedrin: \u1f18\u03b3\u03ce \u03b5\u1f30\u03bc\u03b9 (\"ego eimi\", \"I am\"). There are instances from Jewish literature in which the expression \"you have said it\" is equivalent to \"you are right\". The Messianic claim was less significant than the claim to divinity, which caused the high priest's horrified accusation of blasphemy and the subsequent call for the death sentence. Before Pilate, on the other hand, it was merely the assertion of his royal dignity which gave grounds for his condemnation.\nPauline epistles.\nThe word \"Christ\" is closely associated with Jesus in the Pauline epistles, which suggests that there was no need for the early Christians to claim that Jesus is Christ because it was considered widely accepted among them. Hence Paul can use the term \"Khrist\u00f3s\" with no confusion as to whom it refers, and he can use expressions such as \"in Christ\" to refer to the followers of Jesus, as in and . Paul proclaimed him as the Last Adam, who restored through obedience what Adam lost through disobedience. The Pauline epistles are a source of some key Christological connections; e.g., relates the love of Christ to the knowledge of Christ, and considers the love of Christ as a necessity for knowing him.\nThere are also implicit claims to him being the Christ in the words and actions of Jesus.\nUse of \"Messias\" in John.\nThe Hellenization \u039c\u03b5\u03c3\u03c3\u03af\u03b1\u03c2 (Mess\u00edas) is used twice to mean \"Messiah\" in the New Testament: by the disciple Andrew at , and by the Samaritan woman at the well at John 4:25. In both cases, the Greek text specifies immediately after that this means \"the Christ.\"\nChristology.\nChristology, literally \"the understanding of Christ\", is the study of the nature (person) and work (role in salvation) of Jesus in Christianity. It studies Jesus Christ's humanity and divinity, and the relation between these two aspects; and the role he plays in salvation.\nFrom the second to the fifth centuries, the relation of the human and divine nature of Christ was a major focus of debates in the early church and at the first seven ecumenical councils. The Council of Chalcedon in 451 issued a formulation of the hypostatic union of the two natures of Christ, one human and one divine, \"united with neither confusion nor division\". Most of the major branches of Western Christianity and Eastern Orthodoxy subscribe to this formulation, while many branches of Oriental Orthodox Churches reject it, subscribing to miaphysitism.\nAccording to the \"Summa Theologica\" of Thomas Aquinas, in the singular case of Jesus, the word \"Christ\" has a twofold meaning, which stands for \"both the Godhead anointing and the manhood anointed\". It derives from the twofold human-divine nature of Christ (dyophysitism): the Son of man is anointed in consequence of His incarnated flesh, as well as the Son of God is anointing in consequence of the \"Godhead which He has with the Father\" (ST \"III\", q. 16, a. 5).\nSymbols.\nThe use of \"\u03a7\" as an abbreviation for \"Christ\" derives from the Greek letter Chi (\u03c7), in the word (). An early Christogram is the \"Chi Rho\" symbol, formed by superimposing the first two Greek letters in Christ, chi (\u03a7) and rho (\u03a1), to produce \u2627.\nThe centuries-old English word \"\u03a7mas\" (or, in earlier form, \"XPmas\") is an English form of \u03c7-mas, itself an abbreviation for Christ-mas. The \"Oxford English Dictionary\" (\"OED\") and the \"OED Supplement\" have cited usages of \"X-\" or \"Xp-\" for \"Christ-\" as early as 1485. The terms \"Xpian\" and \"Xren\" have been used for \"Christian\", \"Xst\" for \"Christ's\" \"X\u03c1ofer\" for (Saint) Christopher and Xmas, Xstmas, and Xtmas for Christmas. The \"OED\" further cites usage of \"Xtianity\" for \"Christianity\" from 1634. According to \"Merriam-Webster's Dictionary of English Usage\", most of the evidence for these words comes from \"educated Englishmen who knew their Greek\".\nThe December 1957 \"News and Views\" published by the Church League of America, a conservative organization founded in 1937, attacked the use of \"Xmas\" in an article titled \"X=The Unknown Quantity\". Gerald L. K. Smith picked up the statements later, in December 1966, saying that Xmas was a \"blasphemous omission of the name of Christ\" and that \"'X' is referred to as being symbolical of the unknown quantity.\" More recently, American evangelist Franklin Graham and former CNN contributor Roland S. Martin publicly raised concerns. Graham stated in an interview that the use of \"Xmas\" is taking \"Christ out of Christmas\" and called it a \"war against the name of Jesus Christ.\" Roland Martin relates the use of \"Xmas\" to his growing concerns of increasing commercialization and secularization of what he says is one of Christianity's highest holy days."}
{"id": "5186", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5186", "title": "CountriesV", "text": ""}
{"id": "5187", "revid": "29421082", "url": "https://en.wikipedia.org/wiki?curid=5187", "title": "Capital", "text": "Capital and its variations may refer to:"}
{"id": "5188", "revid": "1272885629", "url": "https://en.wikipedia.org/wiki?curid=5188", "title": "Central Europe", "text": "Central Europe is a geographical region of Europe between Eastern, Southern, Western and Northern Europe. Central Europe is known for its cultural diversity; however, countries in this region also shares historical and cultural similarities.\nWhilst the region is variously defined, it often includes Austria, Croatia, the Czech Republic, Germany, Hungary, Liechtenstein, Lithuania, Poland, Slovakia, Slovenia, Switzerland and Romania's western region of Transylvania. From the early 16th century until the early 18th century, parts of present-day Croatia and Hungary were under Ottoman rule. During the 17th century, the empire also occupied southern parts of present-day Slovakia. During the Early Modern period, the territories of Poland and Lithuania were part of the Polish\u2013Lithuanian Commonwealth. Meanwhile, the Archduchy of Austria, the Kingdom of Bohemia (Czech Republic), the Duchy of Carniola (part of present-day Slovenia), the various German Principalities and the Old Swiss Confederacy were within the Holy Roman Empire. By the end of the 18th century, the Habsburg monarchy, a prominent power within the Holy Roman Empire, came to reign over the territories of Austria, Bosnia and Herzegovina, Croatia, the Czech Republic, Hungary, Slovakia and Slovenia, alongside parts of Serbia, Germany, Italy, Poland and Switzerland.\nSince the Cold War the countries that make up Central Europe have historically been, and in some cases continue to be, divided into either Eastern or Western Europe. After World War II, Europe was divided by the Iron Curtain into two parts, the capitalist Western Bloc and the socialist Eastern Bloc, although Austria, Switzerland and Yugoslavia (encompassing the territories of present-day Croatia, Slovenia and various other Balkans nations) declared neutrality. The Berlin Wall was one of the most visible symbols of this division. Respectively, countries in Central Europe have historical, cultural and geopolitical ties with these wider regions of Europe.\nCentral Europe began a \"strategic awakening\" in the late 20th and early 21st century, with initiatives such as the Central European Defence Cooperation, the Central European Initiative, Centrope, and the Visegr\u00e1d Four Group. This awakening was accelerated by writers and other intellectuals, who recognized the societal paralysis of decaying dictatorships and felt compelled to speak up against Soviet oppression.\nHistorical perspective.\nMiddle Ages and early modern period.\nIn the early Middle Ages, Central Europe had a diverse landscape, with various ethnic groups inhabiting the region. Germanic tribes, among them the Franks, Alemans and Bavarians, were predominantly situated in the west, while Slavic tribes were predominantly in the east. However, the region encompassed a wide spectrum of additional tribes and communities.\nFrom the late 6th century to the early 9th century, the area roughly corresponding to the Carpathian Basin was part of the Avar Khaganate, the realm of the Pannonian Avars. While the Avars dominated the east of what is now Austria, its north and south were under Germanic and Slavic influence, respectively. Meanwhile, the territories now comprising Germany and Switzerland were under the influence of the Merovingian dynasty, and later the Carolingian dynasty. Various Slavic tribes that inhabited eastern Central Europe established settlements during this period, primarily in present-day Croatia, Czech Republic, Poland, Slovakia and Slovenia. The territory of Lithuania was inhabited by Baltic tribes. Amongst them were the Samogitians, Auk\u0161taitians and Curonians.\nThe Holy Roman Empire was founded at the turn of the 9th century, following the coronation of Charlemagne by Pope Leo III. At its inception, it incorporated present-day Germany and nearby regions, including parts of what is now Austria, the Czech Republic, Slovenia and Switzerland. Three decades later, Great Moravia, centred in present-day Czech Republic and Slovakia, became one of the first West Slavic states to be founded in Central Europe. In the late 9th Century, the Hungarian tribes, originating in the Ural Mountains and Western Siberia, settled in the Carpathian Basin and established the Principality of Hungary.\nThe earliest recorded concept of Europe as a cultural sphere (instead of simply a geographic term) was formed by Alcuin of York in the late 8th century during the Carolingian Renaissance, limited to the territories that practised Western Christianity at the time. \"European\" as a cultural term did not include much of the territories where the Orthodox Church represented the dominant religion until the 19th century.\nFollowing the Christianization of various Central European countries, elements of cultural unity emerged within the region, specifically Catholicism and Latin. Eastern Europe remained Eastern Orthodox, and was dominated by Byzantine cultural influence. After the East\u2013West Schism in 1054, significant parts of Eastern Europe developed cultural unity and resistance to Catholic Western and Central Europe within the framework of the Eastern Orthodox Church, Church Slavonic language and the Cyrillic alphabet.\nAccording to historian Jen\u0151 Sz\u0171cs, at the end of the first millennium Central Europe became influenced by Western European developments. Sz\u0171cs argued that between the 11th and 15th centuries, not only did Christianization influence the cultures within Central Europe, but well-defined social features were also implemented in the region based on Western characteristics. The keyword of Western social development after the turn of the millennium was the spread of Magdeburg rights in some cities and towns of Western Europe. These began to spread in the middle of the 13th century in Central European countries, bringing about self-governments of towns and counties.\nIn 1335, the Kings of Poland, Bohemia and Hungary and Croatia met in the castle of Visegr\u00e1d and agreed to cooperate closely in the field of politics and commerce, inspiring the post-Cold War Visegr\u00e1d Group.\nIn 1386, Jogaila, the Grand Duke of Lithuania, converted to Christianity (specifically Catholicism) and subsequently became King of Poland through marriage to Queen Jadwiga of Poland. This initiated the Christianization of Lithuania. It also resulted in the Union of Krewo, signifying a personal union between the Grand Duchy of Lithuania and the Kingdom of Poland. The union commenced an enduring political alliance between the two entities and laid the foundations for the later establishment of the Polish-Lithuanian Commonwealth in 1569.\nBetween the 15th and early 16th centuries, the Kingdom of Croatia, which was at the time in personal union with the Kingdom of Hungary, served as a significant maritime gateway of Central Europe, with its ports facilitating key trade routes between Central Europe and the Mediterranean. The Republic of Ragusa emerged as a prominent hub for cultural exchange during this time. Following the Ottoman and Habsburg wars of the 16th and 17th centuries, the Kingdom of Croatia, under Habsburg rule, began to regain its position as a significant trade route, restoring ports and revitalising commercial activity.\nBefore World War I.\nBefore 1870, the industrialization that had started to develop in Northwestern and Central Europe and the United States did not extend in any significant way to the rest of the world. Even in Eastern Europe, industrialization lagged far behind. Russia, for example, remained largely rural and agricultural, and its autocratic rulers kept the peasants in serfdom.\nThe concept of Central Europe was already known at the beginning of the 19th century, but it developed further and became an object of intensive interest towards the 20th century. However, the first concept mixed science, politics, and economy \u2013 it was strictly connected with the aspirations of German states to dominate a part of European continent called \"Mitteleuropa\". At the Frankfurt Parliament, which was established in the wake of the March Revolution of 1848, there were multiple competing ideas for the integration of German-speaking areas, including the \"mitteleurop\u00e4ische L\u00f6sung\" (Central European Solution) propagated by Austria, which sought to merge the smaller German-speaking states with the multi-ethnic Habsburg Empire, but was opposed by Prussia and others. An imperialistic idea of \"Mitteleuropa\" also became popular in the German Empire established in 1871, which experienced intensive economic growth. The term was used when the Union of German Railway Administrations established the \"Mitteleurop\u00e4ische Eisenbahn-Zeit\" (Central European Railway Time) time zone, which was applied by the railways from 1 June 1891 and was later widely adopted in civilian life, thus the time zone name shortened to the present-day Central European Time.\nThe German term denoting Central Europe was so fashionable that other languages started referring to it when indicating territories from Rhine to Vistula, or even Dnieper, and from the Baltic Sea to the Balkans. An example of this vision of Central Europe may be seen in Joseph Partsch's book of 1903.\nOn 21 January 1904, \"Mitteleurop\u00e4ischer Wirtschaftsverein\" (Central European Economic Association) was established in Berlin with economic integration of Germany and Austria (with eventual extension to Switzerland, Belgium and the Netherlands) as its main aim. Another time, the term Central Europe became connected to the German plans of political, economic, and cultural domination. The \"bible\" of the concept was Friedrich Naumann's book \"Mitteleuropa\" in which he called for an economic federation to be established after World War I. Naumann's proposed a federation with Germany and the Habsburg empire as its centre, eventually uniting all external European nations through economic prosperity. The concept failed after the German defeat in World War I. The revival of the idea may be observed during the Hitler era.\nInterwar period.\nThe interwar period (1918\u20131938) brought a new geopolitical system, as well as economic and political problems, and the concept of Central Europe took on a different character. The centre of interest was moved to its eastern part \u2013 particularly to the countries that had (re)appeared on the map of Europe. Central Europe ceased to be the area of German aspiration to lead or dominate and became a territory of various integration movements aiming at resolving political, economic, and national problems of \"new\" states, being a way to face German and Soviet pressures. However, the conflict of interests was too big and neither Little Entente nor Intermarium (\"Mi\u0119dzymorze\") ideas succeeded. Hungarian historian \u00c1d\u00e1m Magda wrote in her study \"Versailles System and Central Europe\" (2006): \"Today we know that the bane of Central Europe was the Little Entente, military alliance of Czechoslovakia, Romania and Kingdom of Serbs, Croats and Slovenes (later Yugoslavia), created in 1921 not for Central Europe's cooperation nor to fight German expansion, but in a wrong perceived notion that a completely powerless Hungary must be kept down\". The events preceding World War II in Europe\u2014including the so-called Western betrayal/ Munich Agreement were very much enabled by the rising nationalism and ethnocentrism that typified that period.\nThe interwar period brought new elements to the concept of Central Europe. Before World War I, it embraced mainly German-speaking states, with non-German speaking territories being an area of intended German penetration and domination \u2013 German leadership was to be the 'natural' result of economic dominance. Post-war, the Eastern part of Central Europe was placed at the centre of the concept. At that time the scientists took an interest in the idea: the International Historical Congress in Brussels in 1923 was committed to Central Europe, and the 1933 Congress continued the discussions.\nAccording to Emmanuel de Martonne, in 1927, Central Europe encompassed Austria, Czechoslovakia, Germany, Hungary, Poland, Romania and Switzerland, northern Italy and northern Yugoslavia. The author uses both Human and Physical Geographical features to define Central Europe, but he doesn't take into account the legal development or the social, cultural, economic, and infrastructural developments in these countries.\nThe avant-garde movements of Central Europe contributed to the evolution of modernism, reaching its peak throughout the continent during the 1920s. The \"Sourcebook of Central European avantgards\" (Los Angeles County Museum of Art) contains primary documents of the avant-gardes in the territories of Austria, Germany, Poland, Czechoslovakia, Hungary, Romania and Yugoslavia from 1910 to 1930.\nMitteleuropa.\nWith the dissolution of the Holy Roman Empire around 1800, there was a consolidation of power among the Habsburgs and Hohenzollerns as the two major states in the area. They had much in common and occasionally cooperated in various channels, but more often competed. One approach in the various attempts at cooperation, was the conception of a set of supposed common features and interests, and this idea led to the first discussions of a \"Mitteleuropa\" in the mid-nineteenth century, as espoused by Friedrich List and Karl Ludwig Bruck. These were mostly based on economic issues.\n\"Mitteleuropa\" may refer to a historical concept or a contemporary German definition of Central Europe. As a historical concept, the German term \"Mitteleuropa\" (or alternatively its literal translation into English, \"Middle Europe\") is an ambiguous German concept. It is sometimes used in English to refer to an area somewhat larger than most conceptions of 'Central Europe'. According to Fritz Fischer \"Mitteleuropa\" was a scheme in the era of the Reich of 1871\u20131918 by which the old imperial elites had allegedly sought to build a system of German economic, military and political domination from the northern seas to the Near East and from the Low Countries through the steppes of Russia to the Caucasus. Later on, professor Fritz Epstein argued the threat of a Slavic \"Drang nach Westen\" (Western expansion) had been a major factor in the emergence of a \"Mitteleuropa\" ideology before the Reich of 1871 ever came into being.\nIn Germany the connotation was also sometimes linked to the pre-war German provinces east of the Oder-Neisse line.\nThe term \"Mitteleuropa\" conjures up negative historical associations among some people, although the Germans have not played an exclusively negative role in the region. Most Central European Jews embraced the enlightened German humanistic culture of the 19th century. Jews of turn of the 20th century Central Europe became representatives of what many consider to be Central European culture at its best, though the Nazi conceptualisation of \"Mitteleuropa\" sought to destroy this culture. The term \"Mitteleuropa\" is widely used in German education and media without negative meaning, especially since the end of communism. Many people from the new states of Germany do not identify themselves as being part of Western Europe and therefore prefer the term \"Mitteleuropa\".\nCentral Europe during World War II.\nDuring World War II, Central Europe was largely occupied by Nazi Germany. Many areas were a battle area and were devastated. The mass murder of the Jews depopulated many of their centuries-old settlement areas or settled other people there and their culture was wiped out. Both Adolf Hitler and Joseph Stalin diametrically opposed the centuries-old Habsburg principles of \"live and let live\" with regard to ethnic groups, peoples, minorities, religions, cultures and languages and tried to assert their own ideologies and power interests in Central Europe. There were various Allied plans for state order in Central Europe for post-war. While Stalin tried to get as many states under his control as possible, Winston Churchill preferred a Central European Danube Confederation to counter these countries against Germany and Russia. There were also plans to add Bavaria and W\u00fcrttemberg to an enlarged Austria. There were also various resistance movements around Otto von Habsburg that pursued this goal. The group around the Austrian priest Heinrich Maier also planned in this direction, which also successfully helped the Allies to wage war by, among other things, forwarding production sites and plans for V-2 rockets, Tiger tanks and aircraft to the USA. Otto von Habsburg tried to relieve Austria, Czechoslovakia, Hungary and northern Yugoslavia (particularly the territories of present-day Croatia and Slovenia) from Nazi German, and Soviet, influence and control. There were various considerations to prevent German and Soviet power in Europe after the war. Churchill's idea of reaching the area around Vienna before the Russians via an operation from the Adriatic had not been approved by the Western Allied chiefs of staff. As a result of the military situation at the end of the war, Stalin's plans prevailed and much of Central Europe came under Russian control.\nCentral Europe behind the Iron Curtain.\nFollowing World War II, parts of Central Europe became part of the Eastern Bloc. The boundary between the two blocks was called the Iron Curtain. Austria, Switzerland and Yugoslavia remained neutral.\nThe post-World War II period brought blocking of research on Central Europe in the Eastern Bloc countries, as its every result proved the dissimilarity of Central Europe, which was inconsistent with the Stalinist doctrine. On the other hand, the topic became popular in Western Europe and the United States, much of the research being carried out by immigrants from Central Europe. Following the Fall of Communism, publicists and historians in Central Europe, especially the anti-communist opposition, returned to their research.\nAccording to Karl A. Sinnhuber (\"Central Europe: Mitteleuropa: Europe Centrale: An Analysis of a Geographical Term\") most Central European states were unable to preserve their political independence and became Soviet satellites. Besides Austria, Switzerland and Yugoslavia, only the marginal European states of Cyprus, Finland, Malta and Sweden preserved their political sovereignty to a certain degree, being left out of any military alliances in Europe.\nThe opening of the Iron Curtain between Austria and Hungary at the Pan-European Picnic on 19 August 1989 then set in motion a peaceful chain reaction, at the end of which there was no longer an East Germany and the Eastern Bloc had disintegrated. It was the largest escape movement from East Germany since the Berlin Wall was built in 1961. After the picnic, which was based on an idea by Otto von Habsburg to test the reaction of the USSR and Mikhail Gorbachev to an opening of the border, tens of thousands of media-informed East Germans set off for Hungary. The leadership of the GDR in East Berlin did not dare to completely block the borders of their own country and the USSR did not respond at all. \nThis broke the bracket of the Eastern Bloc and Central Europe subsequently became free from communism.\nRoles.\nAccording to American professor Ronald Tiersky, the 1991 summit held in Visegr\u00e1d attended by the Czechoslovak, Hungarian and Polish presidents was hailed at the time as a major breakthrough in Central European cooperation, but the Visegr\u00e1d Group became a vehicle for coordinating Central Europe's road to the European Union, while development of closer ties within the region languished.\nAmerican professor Peter J. Katzenstein described Central Europe as a way station in a Europeanization process that marks the transformation process of the Visegr\u00e1d Group countries in different, though comparable ways. According to him, in Germany's contemporary public discourse \"Central European identity\" refers to the civilizational divide between Catholicism and Eastern Orthodoxy. He argued that there is no precise way to define Central Europe and that the region may even include Bulgaria, Estonia, Latvia and Serbia.\nDefinitions.\nThe issue of how to name and define the Central European area is subject to debates. Very often, the definition depends on the nationality and historical perspective of its author. The concept of \"Central Europe\" appeared in the 19th century. It was understood as a contact zone between the Southern and Northern areas, and later the Eastern and Western areas of Europe. Thinkers portrayed \"Central Europe\" either as a separate region, or a buffer zone between these regions.\nIn the early nineteenth century, the terms \"Middle\" or \"Central\" Europe (known as \"Mitteleuropa\" in German and \"Europe centrale\" in French) were introduced in geographical scholarship in both German and French languages. At first, these terms were linked to the regions spanning from the Pyrenees to the Danube, which, according to German authors, could be united under German authority. However, after the Franco-Prussian war of 1870, the French began to exclude France from this area, and later the Germans also adopted this perspective by the end of World War I.\nThe concept of \"Central\" or \"Middle Europe\", understood as a region with German influence, lost a significant part of its popularity after WWI and was completely dismissed after WWII. Two defeats of Germany in the world wars, combined with the division of Germany, an almost complete disappearance of German-speaking communities in these countries, and the Communist-led isolation of Czechoslovakia, Hungary, Lithuania, Poland and Yugoslavia from the Western world, turned the concept of \"Central/Middle Europe\" into an anachronism. On the other side, the non-German areas of Central Europe were almost universally regarded as \"Eastern European\" primarily associated with the Soviet sphere of influence in the late 1940s\u20131980s.\nFor the most part, this geographical framework lost its attraction after the end of the Cold War. A number of Post-Communist countries rather re-branded themselves in the 1990s as \"Central European.\", while avoiding the stained wording of \"Middle Europe,\" which they associated with German influence in the region. This reinvented concept of \"Central Europe\" excluded Germany, Austria and Switzerland, reducing its coverage chiefly to Poland, the Czech Republic, Slovakia, Hungary, Lithuania and Yugoslavia.\nAcademic.\nThe main proposed regional definitions, gathered by Polish historian Jerzy K\u0142oczowski, include:\nFormer University of Vienna professor Lonnie R. Johnson points out criteria to distinguish Central Europe from Western, Northern, Eastern and Southern Europe:\nHe also thinks that Central Europe is a dynamic historical concept, not a static spatial one. For example, a fair share of Belarus and Right-bank Ukraine are in Eastern Europe today, but years ago they were in the Polish\u2013Lithuanian Commonwealth. Johnson's study on Central Europe received acclaim and positive reviews in the scientific community. However, according to Romanian researcher Maria Bucur, this very ambitious project suffers from the weaknesses imposed by its scope (almost 1600 years of history).\nEncyclopedias, gazetteers, dictionaries.\nThe World Factbook defines Central Europe as: Austria, the Czech Republic, Germany, Hungary, Liechtenstein, Poland, Slovakia, Slovenia and Switzerland. \"The Columbia Encyclopedia\" includes: Austria, the Czech Republic, Germany, Hungary, Latvia, Lithuania, Poland, Slovakia and Switzerland. While it does not have a single article defining Central Europe, Encyclop\u00e6dia Britannica includes the following countries in Central Europe in one or more of its articles: Austria, Bosnia and Herzegovina, Croatia, the Czech Republic, Germany, Hungary, Poland, Slovakia, Slovenia and Switzerland.\nThe German Encyclopaedia \"Meyers Grosses Taschenlexikon\" (\"Meyers Big Pocket Encyclopedia\"), 1999, defines Central Europe as the central part of Europe with no precise borders to the East and West. The term is mostly used to denominate the territory between the Schelde to Vistula and from the Danube to the Moravian Gate.\nAccording to \"Meyers Enzyklop\u00e4disches Lexikon\", Central Europe is a part of Europe composed of Austria, Belgium, the Czech Republic, Slovakia, Germany, Hungary, Luxembourg, Netherlands, Poland, Romania and Switzerland, and northern marginal regions of Italy and Yugoslavia (northern states \u2013 Croatia and Slovenia), as well as northeastern France.\nThe German (Standing Committee on Geographical Names), which develops and recommends rules for the uniform use of geographical names, proposes two sets of boundaries. The first follows international borders of current countries. The second subdivides and includes some countries based on cultural criteria. In comparison to some other definitions, it is broader, including Luxembourg, Estonia, Latvia, and in the second sense, parts of Russia, Belarus, Ukraine, Romania, Serbia, Italy, and France.\nGeographical.\nThere is no general agreement either on what geographic area constitutes Central Europe, nor on how to further subdivide it geographically.\nAt times, the term \"Central Europe\" denotes a geographic definition as the Danube region in the heart of the continent, including the language and culture areas which are today included in the states of Bulgaria, Croatia, the Czech Republic, Hungary, Moldova, Poland, Romania, Serbia, Slovakia, Slovenia and usually also Austria and Germany.\nGovernmental and standards organisations.\nThe terminology EU11 countries refer the Central, Eastern and Baltic European member states which accessed in 2004 and after: in 2004 Czech Republic, Estonia, Latvia, Lithuania, Hungary, Poland, Slovenia, and Slovakia; in 2007 Bulgaria, Romania; and in 2013 Croatia.\nThe EU-funded Interreg region \"Central Europe\" includes the following countries and regions:\nStates.\nThe choice of states that make up \"Central Europe\" is an ongoing source of controversy. Although views on which countries belong to Central Europe are vastly varied, according to many sources (see section Definitions) the region includes some or all of the states listed in the sections below:\nDepending on the context, Central European countries are sometimes not seen as a specific group, but sorted as either Eastern or Western European countries. In this case Austria, Germany, Liechtenstein and Switzerland are often placed in Western Europe, while Croatia, the Czech Republic, Hungary, Lithuania, Poland, Slovakia and Slovenia are placed in Eastern Europe.\nCroatia is alternatively placed in Southeastern Europe. Additionally, Hungary and Slovenia are sometimes included in the region.\nLithuania is alternatively placed in Northeastern Europe.\nOther countries and regions.\nSome sources also add regions of neighbouring countries for historical reasons, or based on geographical and/or cultural reasons:\nGeography.\nGeography defines Central Europe's natural borders with the neighbouring regions to the north across the Baltic Sea, namely Northern Europe (or Scandinavia), and to the south across the Alps, the Apennine peninsula (or Italy), and the Balkan peninsula across the So\u010da\u2013Krka\u2013Sava\u2013Danube line. The borders to Western Europe and Eastern Europe are geographically less defined, and for this reason the cultural and historical boundaries migrate more easily west\u2013east than south\u2013north.\nSouthwards, the Pannonian Plain is bounded by the rivers Sava and Danube \u2013 and their respective floodplains. The Pannonian Plain stretches over the following countries: Austria, Croatia, Hungary, Romania, Serbia, Slovakia and Slovenia, and touches borders of Bosnia and Herzegovina and Ukraine (\"peri- Pannonian states\").\nSouth of the Eastern Alps (spanning Austria, Germany, Italy, Liechtenstein, Slovenia and Switzerland), the Dinaric Alps extend for 650 kilometres along the coast of the Adriatic Sea (northwest-southeast), from the Julian Alps in the northwest down to the \u0160ar-Korab massif, north\u2013south. According to the Freie Universit\u00e4t Berlin, this mountain chain is classified as South Central European. The city of Trieste in this area, for example, expressly sees itself as a \"citt\u00e0 mitteleuropea\". This is particularly because it lies at the interface between the Latin, Slavic, Germanic, Greek and Jewish culture on the one hand and the geographical area of the Mediterranean and the Alps on the other. A geographical and cultural assignment is made.\nThe Central European flora region stretches from Central France (the Massif Central) to the Northern Balkans, Central Romania (Carpathians) and Southern Scandinavia.\nDemography.\nCentral Europe is one of the continent's most populous regions. It includes countries of varied sizes, ranging from tiny Liechtenstein to Germany, the second largest European country by population. Demographic figures for countries entirely located within notion of Central Europe (\"the core countries\") number around 173 million people, out of which around 82\u00a0million are residents of Germany. Other populations include: Poland with around 38.5\u00a0million residents, Czech Republic at 10.5\u00a0million, Hungary at 10 million, Austria with 8.8\u00a0million, Switzerland with 8.5\u00a0million, Slovakia at 5.4\u00a0million, Croatia with 4.3\u00a0million, Lithuania with 2.9 million, Slovenia with 2.1\u00a0million and Liechtenstein at a bit less than 40,000.\nIf the countries which are sometimes also included in Central Europe were counted in, partially or in whole \u2013 Romania (20\u00a0million), Latvia (2\u00a0million), Estonia (1.3\u00a0million), Serbia (7.1\u00a0million) \u2013 this would contribute around an additional 30.4 million, although this figure would vary depending on whether a regional or integral approach is used. If smaller, western and eastern historical parts of Central Europe would be included in the demographic corpus, a further 20 million people of different nationalities would also be added in the overall count, surpassing a total of 200 million people.\nEconomy.\nCurrencies.\nCurrently, the members of the Eurozone include Austria, Croatia, Germany, Lithuania, Slovakia, and Slovenia. The Czech Republic, Hungary and Poland use their own currencies (koruna, forint, Polish z\u0142oty, respectively), but are obliged to adopt the Euro. Switzerland uses its own currency (Swiss franc), as does Serbia (dinar) and Romania (Romanian leu).\nHuman Development Index.\nIn 2018, Switzerland topped the HDI list among Central European countries, also ranking No. 2 in the world. Serbia rounded out the list at No. 11 (67 world).\nGlobalisation.\nThe index of globalization in Central European countries (2016 data): Switzerland topped this list as well (#1 world).\nProsperity Index.\nLegatum Prosperity Index demonstrates an average and high level of prosperity in Central Europe (2018 data). Switzerland topped the index (#4 world).\nCorruption.\nMost countries in Central Europe tend to score above the average in the Corruption Perceptions Index (2018 data), led by Switzerland, Germany, and Austria.\nRail.\nCentral Europe contains the continent's earliest railway systems, whose greatest expansion was recorded in Austrian, Czech, German, Hungarian and Swiss territories between 1860-1870s. By the mid-19th century Berlin, Vienna, Zurich, Pest and Prague were focal points for network lines connecting industrial areas of Saxony, Silesia, Bohemia, Moravia and Lower Austria with the Baltic (Kiel, Szczecin) and Adriatic (Rijeka, Trieste). By 1913, the combined length of the railway tracks of Austria and Hungary reached . By 1936, 70% of the Swiss Federal Railway network had undergone electrification.\nRail infrastructure in Central Europe remains the densest in the world. Railway density as of 2022, with total length of lines operated (km) per 1,000 km2, from highest to lowest is Switzerland (129.2), the Czech Republic (120.7), Germany (108.8), Hungary (85.0), Slovakia (74.0), Austria (66.5), Poland (61.9), Slovenia (59.6), Serbia (49.2), Croatia (46.3) and Lithuania (29.4).\nRiver transport and canals.\nBefore the first railroads appeared in the 1840s, river transport constituted the main means of communication and trade. Earliest canals included Plauen Canal (1745), Finow Canal, and also Bega Canal (1710) which connected Timi\u0219oara to Novi Sad and Belgrade via the Danube. The most significant achievement in this regard was the facilitation of navigability on the Danube from the Black sea to Ulm in the 19th century.\nThe economies of Austria, Croatia, the Czech Republic, Germany, Hungary, Lithuania, Poland, Slovakia, Slovenia and Switzerland tend to demonstrate high complexity. Industrialisation reached Central Europe relatively early beginning with Germany and the Czech lands near the end of the 18th century.\nThe industrialization of the cities of Romania and Serbia started in the interwar period, and did not make significant progress until the post ww2 era.\nAgriculture.\nCentral European countries are some of the most significant food producers in the world. Germany is the world's largest hops producer with 34.27% share in 2010, 3rd largest producer of rye and barley, 5th rapeseed producer, 6th largest milk producer, and 5th largest potato producer. Poland is the world's largest triticale producer, 2nd largest producer of raspberries, currants, 3rd largest of rye, the 5th apple and buckwheat producer, and 7th largest producer of potatoes. The Czech Republic is the world's 4th largest hops producer and 8th producer of triticale. Slovenia is one of the world's leading producers of honey and the world's 6th largest hops producer. Hungary is world's 5th hops and 7th largest triticale producer. Serbia is the world's 2nd largest producer of plums and 2nd largest producer of raspberries.\nBusiness.\nCentral European business has a regional organisation, Central European Business Association (CEBA), founded in 1996 in New York as a non-profit organization dedicated to promoting business opportunities within Central Europe and supporting the advancement of professionals in America with a Central European background.\nTourism.\nCentral European countries, especially Austria, the Czech Republic, Germany and Switzerland are some of the most competitive tourism destinations.\nEducation.\nEducation performance.\nStudent performance has varied across Central Europe, according to the Programme for International Student Assessment. In the 2012 study, countries scored medium, below or over the average scores in three fields studied.\nHigher education.\nUniversities.\nThe first university established east of France and north of the Alps was in Prague in 1348 by Charles IV, Holy Roman Emperor. The Charles University was modeled upon the University of Paris and initially included the faculty of law, medicine, philosophy, and theology.\nCentral European University.\nIn 1991, Ernest Gellner proposed the establishment of a truly Central European institution of higher learning in Prague (1991\u20131995). Eventually, the Central European University (CEU) project was taken on and financially supported by the Hungarian philanthropist George Soros, who had provided an endowment of US$880\u00a0million, making the university one of the wealthiest in Europe. Over its 30-year history CEU has become one of the most internationally diverse and recognisable universities in the world. For example, as of 2019, 1217 students were enrolled in the university, of which 962 were international students, making the student body the fourth most international in the world. CEU offers highly selective programs with a student to faculty ratio of 7:1. In 2021, the admission rate into its programs was 13%. CEU has thus become a leading global university in Europe promoting a distinctively Central European perspective while emphasizing academic rigor, applied research, and academic honesty and integrity. CEU is a founding member of CIVICCA, a group of prestigious European higher education institutions in the social sciences, humanities, business management and public policy, such as Sciences Po (France), The London School of Economics and Political Science (UK), Bocconi University (Italy) and the Stockholm School of Economics (Sweden). In 2019, Central European University leadership announced their preparatory work on moving CEU to Vienna due to legal constraints against academic freedom in Hungary.\nCulture and society.\nResearch.\nResearch centres of Central European literature include Harvard University (Cambridge, MA), Purdue University, and Central European Studies Programme (CESP), Masaryk University, Brno, Czech Republic.\nReligion.\nCentral European countries are mostly Catholic (Austria, Croatia, Liechtenstein, Lithuania, Poland, Slovakia and Slovenia) or historically both Catholic and Protestant (the Czech Republic, Germany, Hungary and Switzerland). Large Protestant groups include Lutheran, Calvinist, and the Unity of the Brethren affiliates. Significant populations of Eastern Catholicism and Old Catholicism are also prevalent throughout Central Europe. Orthodox Christianity is a minority denomination observed to varying extents across Central Europe.\nCentral Europe has been the center of the Protestant movement for centuries, with the majority of Protestants suppressed and annihilated during the Counterreformation.\nHistorically, people in Bohemia in today's Czech Republic were some of the first Protestants in Europe. As a result of the Thirty Years' War following the Bohemian Revolt, many Czechs were either killed, executed (see for Old Town Square execution), forcibly turned into Roman Catholics, or emigrated to Scandinavia and the Low Countries. In the aftermath of the Thirty Years' War, the number of inhabitants in the Kingdom of Bohemia decreased from three million to only 800,000 due to multiple factors, including devastating ongoing battles such as the significant Battle of White Mountain and the Battle of Prague (1648). However, in recent years, most Czechs report as overwhelmingly non-religious, with some describing themselves as Catholic (10.3%).\nBefore the Holocaust (1941\u201345), there was also a sizeable Ashkenazi Jewish community in the region, numbering approximately 16.7\u00a0million people. Poland, Lithuania and Hungary had the largest Jewish populations in Europe as a percentage of their total populations, with Jews constituting 9.5% of the Polish population in 1933.\nCertain countries in Central Europe, particularly the Czech Republic, Germany and Switzerland have sizeable atheist and non-religious populations. In 2021, 48% of the Czech population declared that they had no religion. In 2022, 43.8% of the German population declared that they had no religion. Meanwhile, 33.5% of the Swiss population stated that they were not affiliated with any religion.\nCuisine.\nCentral European cuisine has evolved over centuries due to social and political change and is generally diverse. However, the national cuisines of western Central Europe share notable similarities, as do the cuisines of eastern Central Europe. Sausages, salamis and cheeses are popular in most of Central Europe, with the earliest evidence of cheesemaking in the archaeological record dates back to 5,500 BCE (Kuyavia region, Poland). Other popular food items in Central Europe include soups, stews, pickled and fermented vegetables. Schnitzel, goulash and cabbage rolls are popular in the region.\nAnother common feature among Central European cuisines, particularly Austrian, Croatian, Lithuanian, Slovenian and Swiss cuisine, is the use of wild ingredients in traditional dishes, spanning from wild herbs to mushrooms and berries. Beer consumption is also prominent in parts of Central Europe, where the Czech Republic has the highest beer consumption per capita globally, followed by Austria, with Germany coming 4th. The cuisines of Central European countries that are included in broader definitions of Eastern Europe share similarities and traditions with other Eastern European cuisines. This is particularly evident in the cuisines of Lithuania and Poland, which feature dishes like borscht, pierogi and sour rye soup. \nHuman rights.\nGenerally, the countries in the region have been progressive on the issue of human rights: death penalty is illegal in all of them, corporal punishment is outlawed in most of them and people of both genders can vote in elections. However, Central European countries are divided on the subject of same-sex marriage and abortion. Austria, the Czech Republic, Germany, and Poland also have a history of participation in the CIA's extraordinary rendition and detention program, according to the Open Society Foundations.\nLiterature.\nRegional writing tradition revolves around the turbulent history of the region, as well as its cultural diversity. Its existence is sometimes challenged. Specific courses on Central European literature are taught at Stanford University, Harvard University and Jagiellonian University as well as cultural magazines dedicated to regional literature. Angelus Central European Literature Award is an award worth 150,000.00 PLN (about $50,000 or \u00a330,000) for writers originating from the region. Likewise, the Vilenica International Literary Prize is awarded to a Central European author for \"outstanding achievements in the field of literature and essay writing\".\nSport.\nThere is a number of Central European Sport events and leagues. They include:\nFootball is one of the most popular sports. Countries of Central Europe hosted several major competitions. Germany hosted two FIFA World Cups (1974 and 2006) and two UEFA European Championships (1988 and 2024). Yugoslavia hosted the UEFA Euro 1976 before the competition expanded to 8 teams. Recently, the 2008 and 2012 UEFA European Championships were held in Austria &amp; Switzerland and Poland &amp; Ukraine respectively.\nPolitics.\nOrganisations.\nCentral Europe is a birthplace of regional political organisations:\nDemocracy Index.\nCentral Europe is a home to some of world's oldest democracies. However, most of them have been impacted by totalitarianism, particularly Fascism and Nazism. Germany and Italy occupied all Central European countries, except Switzerland. In all occupied countries, the Axis powers suspended democracy and installed puppet regimes loyal to the occupation forces. Also, they forced conquered countries to apply racial laws and formed military forces for helping German and Italian struggle against Communists. After World War II, almost the whole of Central Europe (the Eastern and Middle part) had been transformed into communist states, most of which had been occupied and later allied with the Soviet Union, often against their will through forged referendum (e.g., Polish people's referendum in 1946) or force (northeast Germany, Poland, Hungary et alia). Nevertheless, these experiences have been dealt in most of them. Most of Central European countries score very highly in the Democracy Index.\nGlobal Peace Index.\nIn spite of its turbulent history, Central Europe is currently one of world's safest regions. Most Central European countries are in top 20%.\nCentral European Time.\nThe time zone is a standard time which is 1 hour ahead of Coordinated Universal Time. Countries using CET include:\nIn popular culture.\nCentral Europe is mentioned in the 35th episode of \"Lovejoy\", entitled \"The Prague Sun\", filmed in 1992. While walking over the well-regarded and renowned Charles Bridge in Prague, the main character, Lovejoy, says: \"I've never been to Prague before. Well, it is one of the great unspoiled cities in Central Europe. Notice: I said: 'Central', not 'Eastern'! The Czechs are a bit funny about that, they think of Eastern Europeans as \"turnip heads\".\"\nWes Anderson's Oscar-winning film \"The Grand Budapest Hotel\" depicts a fictional grand hotel located somewhere in Central Europe which is in actuality modeled on the Grandhotel Pupp in Karlovy Vary in the Czech Republic. The film is a celebration of the 1920s and 1930s Central Europe with its artistic splendor and societal sensibilities."}
{"id": "5192", "revid": "287161", "url": "https://en.wikipedia.org/wiki?curid=5192", "title": "Geography of Canada", "text": " \nCanada has a vast geography that occupies much of the continent of North America, sharing a land border with the contiguous United States to the south and the U.S. state of Alaska to the northwest. Canada stretches from the Atlantic Ocean in the east to the Pacific Ocean in the west; to the north lies the Arctic Ocean. Greenland is to the northeast with a shared border on Hans Island. To the southeast Canada shares a maritime boundary with France's overseas collectivity of Saint Pierre and Miquelon, the last vestige of New France. By total area (including its waters), Canada is the second-largest country in the world, after Russia. By land area alone, however, Canada ranks fourth, the difference being due to it having the world's largest proportion of fresh water lakes. Of Canada's thirteen provinces and territories, only two are landlocked (Alberta and Saskatchewan) while the other eleven all directly border one of three oceans.\nCanada is home to the world's northernmost settlement, Canadian Forces Station Alert, on the northern tip of Ellesmere Island\u2014latitude 82.5\u00b0N\u2014which lies from the North Pole. Much of the Canadian Arctic is covered by ice and permafrost. Canada has the longest coastline in the world, with a total length of ; additionally, its border with the United States is the world's longest land border, stretching . Three of Canada's Arctic islands, Baffin Island, Victoria Island and Ellesmere Island, are among the ten largest in the world.\nCanada can be divided into seven physiographic regions: the Canadian Shield, the interior plains, the Great Lakes-St. Lawrence Lowlands, the Appalachian region, the Western Cordillera, Hudson Bay Lowlands and the Arctic Archipelago. Canada is also divided into fifteen terrestrial and five marine ecozones, encompassing over 80,000 classified species of life. Since the end of the last glacial period, Canada has consisted of eight distinct forest regions, including extensive boreal forest on the Canadian Shield; 42 percent of the land acreage of Canada is covered by forests (approximately 8 percent of the world's forested land), made up mostly of spruce, poplar and pine. Canada has over 2,000,000 lakes\u2014563 greater than \u2014which is more than any other country, containing much of the world's fresh water. There are also freshwater glaciers in the Canadian Rockies, the Coast Mountains and the Arctic Cordillera. A recent global remote sensing analysis also suggested that there were 6,477\u00a0km2 of tidal flats in Canada, making it the 5th ranked country in terms of how much tidal flat occurs there. Protected areas of Canada and National Wildlife Areas have been established to preserve ecosystems.\nCanada is geologically active, having many earthquakes and potentially active volcanoes, notably the Mount Meager massif, Mount Garibaldi, Mount Cayley, and the Mount Edziza volcanic complex. Average winter and summer high temperatures across Canada range from Arctic weather in the north, to hot summers in the southern regions, with four distinct seasons.\nPhysiography.\nCanada covers and a panoply of various geoclimatic regions, of which there are seven main regions. Canada also encompasses vast maritime terrain, with the world's longest coastline of . The physical geography of Canada is widely varied. Boreal forests prevail throughout the country, ice is prominent in northerly Arctic regions and through the Canadian Rocky Mountains, and the relatively flat Canadian Prairies in the southwest facilitate productive agriculture. The Great Lakes feed the St. Lawrence River (in the southeast) where lowlands host much of Canada's population.\nThe National Topographic System is used by Natural Resources Canada for providing general purpose topographic maps of the country. The maps provide details on landforms and terrain, lakes and rivers, forested areas, administrative zones, populated areas, roads and railways, as well as other man-made features. These maps are used by all levels of government and industry for forest fire and flood control (as well as other environmental issues), depiction of crop areas, right-of-way, real estate planning, development of natural resources and highway planning.\nAppalachian Mountains.\nThe Appalachian mountain range extends from Alabama in southern United States through the Gasp\u00e9 Peninsula and the Atlantic Provinces, creating rolling hills indented by river valleys. It also runs through parts of southern Quebec.\nThe Appalachian Mountains (more specifically the Chic-Choc, Notre Dame, and Long Range Mountains) are an old and eroded range of mountains, approximately 380 million years in age. Notable mountains in the Appalachians include Mount Jacques-Cartier (Quebec, ), Mount Carleton (New Brunswick, ), The Cabox (Newfoundland, ). Parts of the Appalachians are home to a rich endemic flora and fauna and are considered to have been nunataks during the last glaciation era.\nCanadian Shield.\nThe northeastern part of Alberta, northern parts of Saskatchewan, Manitoba, Ontario and Quebec, all of Labrador and the Great Northern Peninsula of Newfoundland, eastern mainland Northwest Territories, most of Nunavut's mainland and, of its Arctic Archipelago, Baffin Island and significant bands through Somerset, Southampton, Devon and Ellesmere islands are located on a vast rock base known as the Canadian Shield. The Shield mostly consists of eroded hilly terrain and contains many lakes and important rivers used for hydroelectric production, particularly in northern Quebec and Ontario. The Shield also encloses an area of wetlands around the Hudson Bay. Some particular regions of the Shield are referred to as mountain ranges, including the Torngat and Laurentian Mountains.\nThe Shield cannot support intensive agriculture, although there is subsistence agriculture and small dairy farms in many of the river valleys and around the abundant lakes, particularly in the southern regions. Boreal forest covers much of the shield, with a mix of conifers that provide valuable timber resources in areas such as the Central Canadian Shield forests ecoregion that covers much of Northern Ontario.\nThe Canadian Shield is known for its vast mineral reserves such as emeralds, diamonds and copper, and is there also called the \"mineral house\".\nCanadian Arctic.\nWhile the largest part of the Canadian Arctic is composed of seemingly endless permafrost and tundra north of the tree line, it encompasses geological regions of varying types: the Arctic Cordillera (with the British Empire Range and the United States Range on Ellesmere Island) contains the northernmost mountain system in the world. The Arctic Lowlands and Hudson Bay lowlands comprise a substantial part of the geographic region often designated as the Canadian Shield (in contrast to the sole geologic area). The ground in the Arctic is mostly composed of permafrost, making construction difficult and often hazardous, and agriculture virtually impossible.\nThe Arctic, when defined as everything north of the tree line, covers most of Nunavut and the northernmost parts of Northwest Territories, Yukon, Manitoba, Ontario, Quebec and Labrador. The archipelago consists of 36,563 islands, of which 94 are classified as major islands, being larger than , and cover a total area of .\nWestern Cordillera.\nThe Coast Mountains in British Columbia run from the lower Fraser River and the Fraser Canyon northwestward, separating the Interior Plateau from the Pacific Ocean. Its southeastern end is separated from the North Cascades by the Fraser Lowland, where nearly a third of Western Canada's population reside.\nThe coastal flank of the Coast Mountains is characterized by an intense network of fjords and associated islands, very similar to the Norwegian coastline in Northern Europe; while their inland side transitions to the high plateau with dryland valleys notable for a series of large alpine lakes similar to those in southern Switzerland, beginning in deep mountains and ending in flatland. They are subdivided in three main groups, the Pacific Ranges between the Fraser River and Bella Coola, the Kitimat Ranges from there northwards to the Nass River, and the Boundary Ranges from there to the mountain terminus in Yukon at Champagne Pass and Chilkat Pass northwest of Haines, Alaska. The Saint Elias Mountains lie to their west and northwest, while the Yukon Ranges and Yukon Basin lie to their north. On the inland side of the Boundary Ranges are the Tahltan and Tagish Highlands and also the Skeena Mountains, part of the Interior Mountains system, which also extend southwards on the inland side of the Kitimat Ranges.\nThe terrain of the main spine of the Coast Mountains is typified by heavy glaciation, including several very large icefields of varying elevation. Of the three subdivisions, the Pacific Ranges are the highest and are crowned by Mount Waddington, while the Boundary Ranges contain the largest icefields, the Juneau Icefield being the largest. The Kitimat Ranges are lower and less glacier-covered than either of the other two groupings, but are extremely rugged and dense.\nThe Coast Mountains are made of igneous and metamorphic rock from an episode of arc volcanism related to subduction of the Kula and Farallon Plates during the Laramide orogeny about 100 million years ago. The widespread granite forming the Coast Mountains formed when magma intruded and cooled at depth beneath volcanoes of the Coast Range Arc whereas the metamorphic formed when intruding magma heated the surrounding rock to produce schist.\nThe Insular Mountains extend from Vancouver Island in the south to the Haida Gwaii in the north on the British Columbia Coast. It contains two main mountain ranges, the Vancouver Island Ranges on Vancouver Island and the Queen Charlotte Mountains on Haida Gwaii.\nHudson Bay Lowlands.\nExtreme points.\nThe northernmost point of land within the boundaries of Canada is Cape Columbia, Ellesmere Island, Nunavut . The northernmost point of the Canadian mainland is Zenith Point on Boothia Peninsula, Nunavut . The southernmost point is Middle Island, in Lake Erie, Ontario (41\u00b041\u2032N 82\u00b040\u2032W); the southernmost water point lies just south of the island, on the Ontario\u2013Ohio border (41\u00b040\u203235\u2033N). The southernmost point of the Canadian mainland is Point Pelee, Ontario . The lowest point is sea level at 0 m, whilst the highest point is Mount Logan, Yukon, at 5,959 m\u00a0/\u00a019,550\u00a0ft .\nThe westernmost point is Boundary Peak 187 (60\u00b018\u203222.929\u2033N 141\u00b000\u20327.128\u2033W) at the southern end of the Yukon\u2013Alaska border, which roughly follows 141\u00b0W but leans very slightly east as it goes North . The easternmost point is Cape Spear, Newfoundland (47\u00b031\u2032N 52\u00b037\u2032W) . The easternmost point of the Canadian mainland is Elijah Point, Cape St. Charles, Labrador (52\u00b013\u2032N 55\u00b037\u2032W) .\nThe Canadian pole of inaccessibility is allegedly near Jackfish River, Alberta (59\u00b02\u2032N 112\u00b049\u2032W). The furthest straight-line distance that can be travelled to Canadian points of land is between the southwest tip of Kluane National Park and Reserve (next to Mount Saint Elias) and Cripple Cove, Newfoundland (near Cape Race) at a distance of .\nClimatology.\nClimate varies widely from region to region. Winters can be harsh in many parts of the country, particularly in the interior and Prairie provinces, which experience a continental climate, where daily average temperatures are near , but can drop below with severe wind chills. In non-coastal regions, snow can cover the ground for almost six months of the year, while in parts of the north snow can persist year-round. Coastal British Columbia has a temperate climate, with a mild and rainy winter. On the east and west coasts, average high temperatures are generally in the low 20s \u00b0C (70s \u00b0F), while between the coasts, the average summer high temperature ranges from , with temperatures in some interior locations occasionally exceeding .\nMuch of Northern Canada is covered by ice and permafrost; however, the future of the permafrost is uncertain because the Arctic has been warming at three times the global average as a result of climate change in Canada. Canada's annual average temperature over land has warmed by , with changes ranging from in various regions, since 1948. The rate of warming has been higher across the North and in the Prairies. In the southern regions of Canada, air pollution from both Canada and the United States\u2014caused by metal smelting, burning coal to power utilities, and vehicle emissions\u2014has resulted in acid rain, which has severely impacted waterways, forest growth and agricultural productivity in Canada.\nBiogeography.\nCanada is divided into fifteen major terrestrial and five marine ecozones, that are further subdivided into 53 ecoprovinces, 194 ecoregions, and 1,027 ecodistricts. These eco-areas encompass over 80,000 classified species of Canadian wildlife, with an equal number yet to be formally recognized or discovered. Due to pollution, loss of biodiversity, over-exploitation of commercial species, invasive species, and habitat loss, there are currently more than 800 wild life species at risk of being lost.\nCanada's major biomes are the tundra, boreal forest, grassland, and temperate deciduous forest. British Columbia contains several smaller biomes, including; mountain forest which extends to Alberta, and a small temperate rainforest along the Pacific coast, the semi arid desert located in the Okanagan and alpine tundra in the higher mountainous regions.\nOver half of Canada's landscape is intact and relatively free of human development. Approximately half of Canada is covered by forest, totaling around . The boreal forest of Canada is considered to be the largest intact forest on Earth, with around undisturbed by roads, cities or industry. The Canadian Arctic tundra is the second-largest vegetation region in the country consisting of dwarf shrubs, sedges and grasses, mosses and lichens.\nApproximately 12.1 percent of the nation's landmass and freshwater are conservation areas, including 11.4 percent designated as protected areas. Approximately 13.8 percent of its territorial waters are conserved, including 8.9 percent designated as protected areas.\nHydrography.\nCanada holds vast reserves of water: its rivers discharge nearly 7% of the world's renewable water supply, Canada has over 2,000,000 lakes\u2014563 greater than \u2014which is more than any other country and has the third largest amount of glacier water. Canada is also home to about twenty five percent (134.6 million ha) of the world's wetlands that support a vast array of local ecosystems.\nCanada's waterways host forty-seven rivers of at least in length, with the two longest being the Mackenzie River, that begins at Great Slave Lake and ends in the Arctic Ocean, with its drainage basin covering a large part of northwestern Canada, and the Saint Lawrence River, which drains the Great Lakes into the Gulf of St. Lawrence ending in the Atlantic Ocean. The Mackenzie, including its tributaries is over in length and lies within the second largest drainage basin of North America, while the St. Lawrence in length, drains the world's largest system of freshwater lakes.\nThe Atlantic watershed drains the entirety of the Atlantic provinces (parts of the Quebec-Labrador border are fixed at the Atlantic Ocean-Arctic Ocean continental divide), most of inhabited Quebec and large parts of southern Ontario. It is mostly drained by the economically important St. Lawrence River and its tributaries, notably the Saguenay, Manicouagan, and Ottawa rivers. The Great Lakes and Lake Nipigon are also drained by the St. Lawrence. The Churchill River and Saint John River are other important elements of the Atlantic watershed in Canada.\nThe Hudson Bay watershed drains over a third of Canada. It covers Manitoba, northern Ontario and Quebec, most of Saskatchewan, southern Alberta, southwestern Nunavut, and the southern half of Baffin Island. This basin is most important in fighting drought in the prairies and producing hydroelectricity, especially in Manitoba, northern Ontario and Quebec. Major elements of this watershed include Lake Winnipeg, Nelson River, the North Saskatchewan and South Saskatchewan Rivers, Assiniboine River, and Nettilling Lake on Baffin Island. Wollaston Lake lies on the boundary between the Hudson Bay and Arctic Ocean watersheds and drains into both. It is the largest lake in the world that naturally drains in two directions.\nThe continental divide in the Rockies separates the Pacific watershed in British Columbia and Yukon from the Arctic and Hudson Bay watersheds. This watershed irrigates the agriculturally important areas of inner British Columbia (such as the Okanagan and Kootenay valleys), and is used to produce hydroelectricity. Major elements are the Yukon, Columbia and Fraser rivers.\nThe northern parts of Alberta, Manitoba, and British Columbia, most of Northwest Territories and Nunavut, and parts of Yukon are drained by the Arctic watershed. This watershed has been little used for hydroelectricity, with the exception of the Mackenzie River. The Peace, Athabasca and Liard Rivers, as well as Great Bear Lake and Great Slave Lake (respectively the largest and second largest lakes wholly enclosed by Canada) are significant elements of the Arctic watershed. Each of these elements eventually merges with the Mackenzie, thereby draining the vast majority of the Arctic watershed.\nThe southernmost part of Alberta drains into the Gulf of Mexico through the Milk River and its tributaries. The Milk River originates in the Rocky Mountains of Montana, then flows into Alberta, then returns into the United States, where it is drained by the Missouri River. A small area of southwestern Saskatchewan is drained by Battle Creek, which empties into the Milk River.\nNatural resources.\nCanada's abundance of natural resources is reflected in their continued importance in the economy of Canada. Major resource-based industries are fisheries, forestry, agriculture, petroleum products and mining.\nThe fisheries industry has historically been one of Canada's strongest. Unmatched cod stocks on the Grand Banks of Newfoundland launched this industry in the 16th century. Today these stocks are nearly depleted, and their conservation has become a preoccupation of the Atlantic Provinces. On the West Coast, tuna stocks are now restricted. The less depleted (but still greatly diminished) salmon population continues to drive a strong fisheries industry. Canada claims of territorial sea, a contiguous zone of , an exclusive economic zone of with and a continental shelf of or to the edge of the continental margin.\nFive per cent of Canada's land area is arable, none of which is for permanent crops. Three per cent of Canada's land area is covered by permanent pastures. Canada has of irrigated land (1993 estimate). Agricultural regions in Canada include the Canadian Prairies, the Lower Mainland and various regions within the Interior of British Columbia, the St. Lawrence Basin and the Canadian Maritimes. Main crops in Canada include flax, oats, wheat, maize, barley, sugar beets and rye in the prairies; flax and maize in Western Ontario; Oats and potatoes in the Maritimes. Fruit and vegetables are grown primarily in the Annapolis Valley of Nova Scotia, Southwestern Ontario, the Golden Horseshoe region of Ontario, along the south coast of Georgian Bay and in the Okanagan Valley of British Columbia. Cattle and sheep are raised in the valleys and plateaus of British Columbia. Cattle, sheep and hogs are raised on the prairies, cattle and hogs in Western Ontario, sheep and hogs in Quebec, and sheep in the Maritimes. There are significant dairy regions in central Nova Scotia, southern New Brunswick, the St. Lawrence Valley, northeastern Ontario, southwestern Ontario, the Red River valley of Manitoba and the valleys in the British Columbia Interior, on Vancouver Island and in the Lower Mainland.\nFossil fuels are a more recently developed resource in Canada, with oil and gas being extracted from deposits in the Western Canadian Sedimentary Basin since the mid-1900s. While Canada's crude oil deposits are fewer, technological developments in recent decades have opened up oil production in Alberta's Oil Sands to the point where Canada now has some of the largest reserves of oil in the world. In other forms, Canadian industry has a long history of extracting large coal and natural gas reserves.\nCanada's mineral resources are diverse and extensive. Across the Canadian Shield and in the north there are large iron, nickel, zinc, copper, gold, lead, molybdenum, and uranium reserves. Large diamond concentrations have been recently developed in the Arctic, making Canada one of the world's largest producers. Throughout the Shield there are many mining towns extracting these minerals. The largest, and best known, is Sudbury, Ontario. Sudbury is an exception to the normal process of forming minerals in the Shield since there is significant evidence that the Sudbury Basin is an ancient meteorite impact crater. The nearby, but less known Temagami Magnetic Anomaly has striking similarities to the Sudbury Basin. Its magnetic anomalies are very similar to the Sudbury Basin, and so it could be a second metal-rich impact crater. The Shield is also covered by vast boreal forests that support an important logging industry.\nCanada's many rivers have afforded extensive development of hydroelectric power. Extensively developed in British Columbia, Ontario, Quebec and Labrador, the many dams have long provided a clean, dependable source of energy.\nEnvironmental issues.\nAir pollution and resulting acid rain severely affects lakes and damages forests. Metal smelting, coal-burning utilities, and vehicle emissions impact agricultural and forest productivity. Ocean waters are also becoming contaminated by agricultural, industrial, mining, and forestry activities.\nGlobal climate change and the warming of the polar region will likely cause significant changes to the environment, including loss of the polar bear, the exploration for resource then the extraction of these resources and an alternative transport route to the Panama Canal through the Northwest Passage.\nCanada is currently warming at twice the global average, and this is effectively irreversible.\nPolitical geography.\nCanada is divided into ten provinces and three territories. According to Statistics Canada, 72.0 percent of the population is concentrated within of the nation's southern border with the United States, 70.0% live south of the 49th parallel, and over 60 percent of the population lives along the Great Lakes and St. Lawrence River between Windsor, Ontario, and Quebec City. This leaves the vast majority of Canada's territory as sparsely populated wilderness; Canada's population density is , among the lowest in the world. Despite this, 79.7 percent of Canada's population resides in urban areas, where population densities are increasing.\nCanada shares with the U.S. the world's longest binational border at ; are with Alaska. The Danish island dependency of Greenland lies to Canada's northeast, separated from the Canadian Arctic islands by Baffin Bay and Davis Strait. As of June 14, 2022, Canada shares a land border with Greenland on Hans Island. The French islands of Saint Pierre and Miquelon lie off the southern coast of Newfoundland in the Gulf of St. Lawrence and have a maritime territorial enclave within Canada's exclusive economic zone.\nCanada's geographic proximity to the United States has historically bound the two countries together in the political world as well. Canada's position between the Soviet Union (now Russia) and the U.S. was strategically important during the Cold War since the route over the North Pole and Canada was the fastest route by air between the two countries and the most direct route for intercontinental ballistic missiles. Since the end of the Cold War, there has been growing speculation that Canada's Arctic maritime claims may become increasingly important if global warming melts the ice enough to open the Northwest Passage."}
{"id": "5193", "revid": "8729451", "url": "https://en.wikipedia.org/wiki?curid=5193", "title": "Demographics of Canada", "text": "Statistics Canada conducts a country-wide census that collects demographic data every five years on the first and sixth year of each decade. The 2021 Canadian census enumerated a total population of 36,991,981, an increase of around 5.2 percent over the 2016 figure. It is estimated that Canada's population surpassed 40 million in 2023 and 41 million in 2024. Between 1990 and 2008, the population increased by 5.6\u00a0million, equivalent to 20.4 percent overall growth. The main driver of population growth is immigration, with 6.2% of the country's population being made up of temporary residents as of 2023, or about 2.5 million people. Between 2011 and May 2016, Canada's population grew by 1.7\u00a0million people, with immigrants accounting for two-thirds of the increase.\nCanada has one of the highest per-capita immigration rates in the world, driven mainly by economic policy and, to a lesser extent, family reunification. In 2021, a total of 405,330 immigrants were admitted to Canada. New immigrants settle mostly in major urban areas such as Toronto, Montreal, and Vancouver. Canada also accepts large numbers of refugees, accounting for over 10 percent of annual global refugee resettlements.\nPopulation.\nThe 2021 Canadian census had a total population count of 36,991,981 individuals, making up approximately 0.5% of the world's total population. A population estimate for 2024 put the total number of people in Canada at 41,012,563.\nProvinces and territories.\n&lt;onlyinclude&gt;\nPopulation distribution.\nThe vast majority of Canadians are positioned in a discontinuous band within approximately 300\u00a0km of the southern border with the United States; the most populated province is Ontario, followed by Quebec and British Columbia.\nFertility rate.\nThe total fertility rate is the number of children born in a specific year cohort to the total number of women who can give birth in the country.\nIn 1971, the birth rate for the first time dipped below replacement and since then has not rebounded.\nCanada's fertility rate hit a record low of 1.4 children born per woman in 2020, below the population replacement level, which stands at 2.1 births per woman. In 2020, Canada also experienced the country's lowest number of births in 15 years, also seeing the largest annual drop in childbirths (\u22123.6%) in a quarter of a century. The total birth rate is 10.17 births/1,000 population in 2022.\nMother's mean age at first birth.\nCanada is among late-childbearing countries, with the average age of mothers at the first birth being 31.3 years in 2020.\nPopulation projection.\nAccording to Organisation for Economic Co-operation and Development (OECD)/World Bank, the population in Canada increased from 1990 to 2008 with 5.6\u00a0million and 20.4% growth in population, compared to 21.7% growth in the United States and 31.2% growth in Mexico. According to the OECD/World Bank population statistics, for the same period the world population growth was 27%, a total of 1,423\u00a0million people. However, over the same period, the population of France grew by 8.0%. And from 1991 to 2011, the population of the UK increased by 10.0%.\nThe current population growth rate for Canada in 2022 was 0.75%.\nLife expectancy.\nLife expectancy in Canada has consistently risen since the country's formation. \nSchool life expectancy (primary to tertiary education)\nInfant mortality rate\nAge characteristics.\nPopulation by Sex and Age Group (Census 10.V.2016) (To ensure confidentiality, the values, including totals are randomly rounded either up or down to a multiple of '5' or '10.' As a result, when these data are summed or grouped, the total value may not match the individual values since totals and sub-totals are independently rounded. Similarly, percentages, which are calculated on rounded data, may not necessarily add up to 100%.):\nAge structure\nMedian age\nSex ratio.\nat birth: 1.05 male(s)/female\n0\u201314 years: 1.06 male(s)/female\n15\u201324 years: 1.06 male(s)/female\n25\u201354 years: 1.01 male(s)/female\n55\u201364 years: 0.98 male(s)/female\n65 years and over: 0.75 male(s)/female\ntotal population: 0.98 male(s)/female (2022 est).\nDependency ratios\nVital statistics.\nCurrent vital statistics.\n \nNote: all numbers in this table are provisional. While data for at least two years ago may be final, newer data for recent days are subject to change in the future. For example, as of September 25 2024, The numbers are final up to December 2021, updated from January 2022 to March 2024 and preliminary from April 2024.\nEmployment.\nUnemployment, youth ages 15\u201324\nEthnicity and visible minorities.\nCanadians as ethnic group by province.\nAll citizens of Canada are classified as \"Canadians\" as defined by Canada's nationality laws. \"Canadian\" as an ethnic group has since 1996 been added to census questionnaires for possible ancestral origin or descent. \"Canadian\" was included as an example on the English questionnaire and \"Canadien\" as an example on the French questionnaire. The majority of respondents to this selection are from the eastern part of the country that was first settled. Respondents generally are visibly European (Anglophones and Francophones) and no longer self-identify directly with their ethnic ancestral origins. This response is attributed to a multitude of reasons such as generational distance from ancestral lineage.\nEthnic origin.\nAccording to the 2021 Canadian census, over 450 \"ethnic or cultural origins\" were self-reported by Canadians. The major panethnic groups chosen were; European (), North American (), Asian (), North American Indigenous (), African (), Latin, Central and South American (), Caribbean (), Oceanian (), and Other (). Statistics Canada reports that 35.5% of the population reported multiple ethnic origins, thus the overall total is greater than 100%.\nThe country's ten largest self-reported specific ethnic or cultural origins in 2021 were Canadian (accounting for 15.6\u00a0percent of the population), followed by English (14.7\u00a0percent), Irish (12.1\u00a0percent), Scottish (12.1\u00a0percent), French (11.0\u00a0percent), German (8.1\u00a0percent), Indian (5.1\u00a0percent),\nChinese (4.7\u00a0percent), Italian (4.3\u00a0percent), and Ukrainian (3.5\u00a0percent).\nOf the 36.3 million people enumerated in 2021 approximately 24.5 million reported being \"White\", representing 67.4\u00a0percent of the population. The Indigenous population representing 5\u00a0percent or 1.8 million individuals, grew by 9.4\u00a0percent compared to the non-Indigenous population, which grew by 5.3\u00a0percent from 2016 to 2021. One out of every four Canadians or 26.5\u00a0percent of the population belonged to a non-White and non-Indigenous visible minority, the largest of which in 2021 were South Asian (2.6 million people; 7.1\u00a0percent), Chinese (1.7 million; 4.7\u00a0percent) and Black (1.5 million; 4.3\u00a0percent).\nAs data is completely self-reported, and reporting individuals may have varying definitions of \"Ethnic origin\" (or may not know their ethnic origin), these figures should not be considered an exact record of the relative prevalence of different ethno-cultural ancestries but rather how Canadians self-identify.\nData from this section from Statistics Canada, 2021.\nThe most common ethnic origins per province are as follows in 2006 (total responses; only percentages 10% or higher shown; ordered by percentage of \"Canadian\"):\n\"Italics\" indicates either that this response is dominant within this province, or that this province has the highest ratio (percentage) of this response among provinces.\nVisible minority population.\nNote: Indigenous population decline between 1991 and 1996 censuses attributed to change in criteria in census count; \"the 1996 Royal Commission on Aboriginal Peoples used a more restrictive definition of Aboriginal\".\nIndigenous population.\n\"Note: Other Indigenous and mixed Indigenous groups are not listed as their own, but they are all accounted for in total Indigenous\"\nFuture projections.\nStatistics Canada projects that visible minorities will make up between 38.2% and 43.0% of the total Canadian population by 2041, compared with 26.5% in 2021. Among the working-age population (15 to 64 years), meanwhile, visible minorities are projected to represent between 42.1% and 47.3% of Canada's total population, compared to 28.5% in 2021.\nLanguages.\nKnowledge of language.\nThe question on knowledge of languages allows for multiple responses, and first appeared on the 1991 Canadian census. The following figures are from the 1991 Canadian census, 2001 Canadian census, 2011 Canadian census, and the 2021 Canadian census. \nImmigration.\nAccording to the 2021 Canadian census, immigrants in Canada number 8.3 million persons and make up approximately 23\u00a0percent of Canada's total population. This represents the eighth-largest immigrant population in the world, while the proportion represents one of the highest ratios for industrialized Western countries.\nImmigrants from specific countries are divided into several ethnic groups. For example, there are both Punjabis and Muhajirs from Pakistan, both Turks and Kurds from Turkey and both Sinhalese and Tamil from Sri Lanka. Immigrants from Iran are divided into Mazandaranians, Azeris, Persians, Kurds, Gilaks and Lurs.\nSince confederation in 1867 through to the contemporary era, decadal and demi-decadal census reports have detailed immigration statistics. During this period, the highest annual immigration rate in Canada occurred in 1913, when 400,900 new immigrants accounted for 5.3\u00a0percent of the total population, while the greatest number of foreign-born individuals admitted to Canada in single year occurred in 2021, with 405,330 new immigrants accounting for 1.1\u00a0percent of the total population.\nStatistics Canada projects that immigrants will represent between 29.1% and 34.0% of Canada's population in 2041, compared with 23.0% in 2021, while the Canadian population with at least one foreign born parent (first and second generation persons) could rise to between 49.8% and 54.3%, up from 44.0% in 2021.\nReligion.\nIn 2021, 53.3% of Canadians were Christians, down from 67.3% in 2011. 29.9% were Catholic while 11.4% were Protestant (all other listed denominations excluding Christian Orthodox, Latter Day Saints and Jehovah's Witnesses). 7.6% were Christian not otherwise specified, 2.1% were \"other Christian and Christian-related traditions\", 1.7% were Christian Orthodox, 0.4% were Jehovah's Witnesses and 0.2% were Latter Day Saints adherents.\n34.6% of Canadians were non-religious or secular, up from 23.9% in 2011. Of the non-Christian religions listed, 4.9% of Canadians were Muslim (3.2% in 2011), 2.3% were Hindu (1.5% in 2011), 2.1% were Sikh (1.4% in 2011), 1.0% were Buddhist (1.1% in 2011), 0.9% were Jewish (1.0% in 2011), 0.2% were believers of traditional (North American Indigenous) spirituality (same as 2011), and 0.6% were believers of other religions and spiritual traditions (0.4% in 2011)."}
{"id": "5194", "revid": "8729451", "url": "https://en.wikipedia.org/wiki?curid=5194", "title": "Politics of Canada", "text": " \nThe politics of Canada functions within a framework of parliamentary democracy and a federal system of parliamentary government with strong democratic traditions. Canada is a constitutional monarchy where the monarch is the ceremonial head of state. In practice, executive authority is entrusted to the Cabinet, a committee of ministers of the Crown chaired by the prime minister of Canada that act as the executive committee of the King's Privy Council for Canada and are responsible to the democratically elected House of Commons.\nCanada is described as a \"full democracy\", with a tradition of secular liberalism, and an egalitarian, moderate political ideology. Extremism has never been prominent in Canadian politics. The traditional \"brokerage\" model of Canadian politics leaves little room for ideology. Peace, order, and good government, alongside an Implied Bill of Rights, are founding principles of the Canadian government. An emphasis on multiculturalism and social justice has been a distinguishing element of Canada's political culture. Canada has placed emphasis on diversity, equity and inclusion for all its people.\nThe country has a multi-party system in which many of its legislative practices derive from the unwritten conventions of and precedents set by the Westminster parliament of the United Kingdom. The two dominant political parties in Canada have historically been the Liberal Party of Canada and the current Conservative Party of Canada (as well as its numerous predecessors). Parties like the New Democratic Party, the Quebec nationalist Bloc Qu\u00e9b\u00e9cois and the Green Party of Canada have grown in prominence, exerting their own influence to the political process.\nCanada has evolved variations: party discipline in Canada is stronger than in the United States and United Kingdom, and more parliamentary votes are considered motions of confidence, which tends to diminish the role of non-Cabinet members of parliament (MPs). Such members, in the government caucus, and junior or lower-profile members of opposition caucuses, are known as backbenchers. Backbenchers can, however, exert their influence by sitting in parliamentary committees, like the Public Accounts Committee or the National Defence Committee.\nContext.\nCanada's governmental structure was originally established by the British Parliament through the \"British North America Act, 1867\" (now the \"Constitution Act, 1867\"), but the federal model and division of powers were devised by Canadian politicians. Particularly after World War I, citizens of the self-governing Dominions, such as Canada, began to develop a strong sense of identity, and, in the Balfour Declaration of 1926, the British government and the governments of the six Dominions jointly agreed that the Dominions had full autonomy within the British Commonwealth.\nIn 1931, after further consultations and agreements between the British government and the governments of the Dominions, the British Parliament passed the Statute of Westminster, giving legal recognition to the autonomy of Canada and other Dominions. However, Canadian politicians were unable to obtain consensus on a process for amending the constitution, which was therefore not affected by the Statute of Westminster, meaning amendments to Canada's constitution continued to require the approval of the British parliament until that date. Similarly, the Judicial Committee of the Privy Council in Britain continued to make the final decision on criminal appeals until 1933 and on civil appeals until 1949. It was not until 1982, with the Patriation of the Constitution, that the role of the British Parliament was ended.\nPolitical culture.\nCanada's egalitarian approach to governance has emphasized social welfare, economic freedom, and multiculturalism, which is based on selective economic migrants, social integration, and suppression of far-right politics, that has wide public and political support. Its broad range of constituent nationalities and policies that promote a \"just society\" are constitutionally protected. Individual rights, equality and inclusiveness (social equality) have risen to the forefront of political and legal importance for most Canadians, as demonstrated through support for the Charter of Rights and Freedoms, a relatively free economy, and social liberal attitudes toward women's rights (like pregnancy termination), divorce, homosexuality, same-sex marriage, birth control, euthanasia or cannabis use. There is also a sense of collective responsibility in Canadian political culture, as is demonstrated in general support for universal health care, multiculturalism, evolution, gun control, foreign aid, and other social programs.\nAt the federal level, Canada has been dominated by two relatively centrist parties practising \"brokerage politics\", the centre-left leaning Liberal Party of Canada and the centre-right leaning Conservative Party of Canada (or its predecessors). \"The traditional \"brokerage\" model of Canadian politics leaves little\nroom for ideology\" as the Canadian catch-all party system requires support from a broad spectrum of voters. The historically predominant Liberals position themselves at the centre of the political scale, with the Conservatives sitting on the right and the New Democratic Party occupying the left. Five parties had representatives elected to the federal parliament in the 2021 election: the Liberal Party who currently form the government, the Conservative Party who are the Official Opposition, the New Democratic Party, the Bloc Qu\u00e9b\u00e9cois, and the Green Party of Canada.\nPolls have suggested that Canadians generally do not have a solid understanding of civics. This has been theorized to be a result of less attention being given to the subject in provincial education curricula, beginning in the 1960s. By 2008, a poll showed only 24 per cent of respondents could name the monarch as head of state. Likewise, Senator Lowell Murray wrote five years earlier that \"the Crown has become irrelevant to most Canadians' understanding of our system of Government.\" As John Robson of the \"National Post\" opined in 2015: \"Intellectually, voters and commentators succumb to the mistaken notion that we elect 'governments' of prime ministers and cabinets with untrammelled authority, that indeed ideal 'democracy' consists precisely in this kind of plebiscitary autocracy.\"\nGovernmental organization.\nMonarchy.\nCanada is a constitutional monarchy, wherein the role of the reigning sovereign is both legal and practical, but not political. The monarch is vested with all powers of state and sits at the centre of a construct in which the power of the whole is shared by multiple institutions of government acting under the sovereign's authority. The executive is thus formally referred to as the \"King-in-Council\", the legislature as the \"King-in-Parliament\", and the courts as the \"King-on-the-Bench\".\nThough the person who is monarch of Canada (currently ) is also the monarch of 14 other countries in the Commonwealth of Nations, he nevertheless reigns separately as King of Canada, an office that is \"truly Canadian\" and \"totally independent from that of the monarch of the United Kingdom or the other Commonwealth realms.\" On the advice of the Canadian prime minister, the sovereign appoints a federal viceregal representative\u2014the governor general(currently Mary Simon)\u2014who, since 1947, is permitted to exercise almost all of the monarch's royal prerogative; though, there are some duties which must be specifically performed by the monarch themselves (such as assent of certain bills). In case of the governor general's absence or incapacitation, the administrator of Canada performs the Crown's most basic functions.\nRoyal assent is required to enact laws. As part of the royal prerogative, the royal sign-manual gives authority to letters patent and orders-in-Council. Much of the royal prerogative is only exercised in-council, on the advice of the Cabinet; within the conventional stipulations of a constitutional monarchy, the sovereign's direct participation in any of these areas of governance is limited. The royal prerogative also includes summoning, proroguing, and dissolving Parliament in order to call an election and extends to foreign affairs, which include the negotiation and ratification of treaties, alliances, international agreements, and declarations of war; the accreditation of Canadian diplomats and receipt of foreign diplomats; and the issuance of passports.\nLegislative power.\nThe bicameral Parliament of Canada consists of three parts: the monarch, the Senate, and the House of Commons.\nCurrently, the Senate, which is frequently described as providing regional representation, has 105 members appointed by the Governor-General on the advice of the Prime Minister to serve until age 75. It was created with equal representation from the three regions of Ontario, Quebec, and the Maritimes (originally New Brunswick and Nova Scotia, expanded in 1873 to include Prince Edward Island). In 1915, a new Western division was created, with six senators from each of the four western provinces, so that each of the four regions had 24 seats in the Senate. When Newfoundland and Labrador joined Confederation in 1949, it was not included in an existing region and was assigned six seats. Each of the three territories has one seat. It is not based on representation-by-population. The normal number of senators can be exceeded by the monarch on the advice of the Prime Minister, as long as the additional senators are distributed equally with regard to region (up to a total of eight additional Senators). This power of additional appointment has only been used once, when Prime Minister Brian Mulroney petitioned Queen Elizabeth II to add eight seats to the Senate so as to ensure the passage of the Goods and Services Tax legislation.\nThe House of Commons currently has 338 members elected in single-member districts in a plurality voting system (first past the post), meaning that members must attain only a plurality (the most votes of any candidate) rather than a majority. The electoral districts are also known as ridings.\nMandates cannot exceed five years; an election must occur by the end of this time. This fixed mandate has been exceeded only once, when Prime Minister Robert Borden perceived the need to do so during World War I. A constitutional amendment was passed, extending the life of the Parliament by one year, by the unanimous consent of the House of Commons. The size of the House and apportionment of seats to each province is revised after every census, conducted every five years, and is based on population changes and approximately on representation-by-population.\nElections and government formation.\nCanadians vote for the election of their local member of parliament (MP) only. A vote is cast directly for a candidate. The candidate in each riding who receives a plurality of votes (first-past-the-post system) is elected. An MP need not be a member of any political party: such MPs are known as independents. When a number of MPs share political opinions they may form a body known as a political party.\nThe \"Canada Elections Act\" defines a political party as \"an organization one of whose fundamental purposes is to participate in public affairs by endorsing one or more of its members as candidates and supporting their election.\" Forming and registering a federal political party are two different things. There is no legislation regulating the formation of federal political parties. Elections Canada cannot dictate how a federal political party should be formed or how its legal, internal and financial structures should be established.\nMost parties elect their leaders in instant-runoff elections to ensure that the winner receives more than 50% of the votes. Normally the party leader stands as a candidate to be an MP during an election. This happens at leadership conventions. Canada's parliamentary system empowers political parties and their party leaders. Where one party gets a majority of the seats in the House of Commons, that party is said to have a \"majority government.\" Through party discipline, the party leader, who is elected in only one riding, exercises a great deal of control over the cabinet and the parliament.\nHistorically, the prime minister and senators are selected by the Governor General as a representative of the King, though in modern practice the monarch's duties are ceremonial. Consequently, the prime minister, while technically selected by the Governor General, is for all practical purposes selected by the party with the majority of seats. That is, the party that gets the most seats normally forms the government, with that party's leader becoming prime minister. The prime minister is not directly elected by the general population, although the prime minister is almost always directly elected as an MP within his or her constituency.\nOften the most popular party in an election takes a majority of the seats, even if it did not receive a majority of the vote. However, as there are usually three or more political parties represented in parliament, often no party takes a majority of the seats. A minority government occurs when the party that holds the most seats in the House of Commons holds fewer seats than the opposition parties combined. Where no party is given a majority, the defeated Ministry may choose to stay in office until defeated on a vote of confidence in the House, or it may resign. If it resigns, the Governor General will ask the leader of the opposition party most likely to enjoy the confidence of the House to form a government; however, for the government to survive and to pass laws, the leader chosen must have the support of the majority of the House, meaning they need the support of the elected members of at least one other party. This can be done on a case-by-case basis, through a coalition government (which has only occurred once at the federal level, the Unionist government formed during World War I) or through a confidence-and-supply agreement (such as the one the Liberals and the NDP signed in 2022).\nFederal-provincial relations.\nAs a federation, the existence and powers of the federal government and the ten provinces are guaranteed by the Constitution. The \"Constitution Act, 1867\" sets out the basic constitutional structure of the federal government and the provinces. The powers of the federal Parliament and the provinces can only be changed by constitutional amendments passed by the federal and provincial governments. The Crown is the formal head of state of the federal government and each of the ten provinces, but rarely has any political role. The governments are led by the representatives of the people: elected by all Canadians, at the federal level, and by the Canadian citizens of each provinces, at the provincial level.\nFederal-provincial (or intergovernmental, formerly Dominion-provincial) relations is a regular issue in Canadian politics: Quebec wishes to preserve and strengthen its distinctive nature, western provinces desire more control over their abundant natural resources, especially energy reserves; industrialized Central Canada is concerned with its manufacturing base, and the Atlantic provinces strive to escape from being less affluent than the rest of the country.\nIn order to ensure that social programs such as health care and education are funded consistently throughout Canada, the \"have-not\" (poorer) provinces receive a proportionately greater share of federal \"transfer (equalization) payments\" than the richer, or \"have\", provinces do; this has been somewhat controversial. The richer provinces often favour freezing transfer payments, or rebalancing the system in their favour, based on the claim that they already pay more in taxes than they receive in federal government services, and the poorer provinces often favour an increase on the basis that the amount of money they receive is not sufficient for their existing needs.\nParticularly in the past decade, critics have argued that the federal government's exercise of its unlimited constitutional spending power has contributed to strained federal-provincial relations. This power allows the federal government to influence provincial policies, by offering funding in areas that the federal government cannot itself regulate. The federal spending power is not expressly set out in the \"Constitution Act, 1867\"; however, in the words of the Court of Appeal for Ontario the power \"can be inferred\" from s. 91(1A), \"the public debt and property\".\nA prime example of an exercise of the spending power is the \"Canada Health Act\", which is a conditional grant of money to the provinces. Regulation of health services is, under the Constitution, a provincial responsibility. However, by making the funding available to the provinces under the \"Canada Health Act\" contingent upon delivery of services according to federal standards, the federal government has the ability to influence health care delivery.\nQuebec and Canadian politics.\nExcept for three short-lived transitional or minority governments, prime ministers from Quebec led Canada continuously from 1968 to early 2006. People from Quebec led both Liberal and Progressive Conservative governments in this period.\nMonarchs, governors general, and prime ministers are now expected to be at least functional, if not fluent, in both English and French. In selecting leaders, political parties give preference to candidates who are fluently bilingual.\nBy law, three of the nine positions on the Supreme Court of Canada must be held by judges from Quebec. This representation makes sure that at least three judges have sufficient experience with the civil law system to treat cases involving Quebec laws.\nNational unity.\nCanada has a long and storied history of secessionist movements (see Secessionist movements of Canada). National unity has been a major issue in Canada since the forced union of Upper and Lower Canada in 1840.\nThe predominant and lingering issue concerning Canadian national unity has been the ongoing conflict between the French-speaking majority in Quebec and the English-speaking majority in the rest of Canada. Quebec's continued demands for recognition of its \"distinct society\" through special political status has led to attempts for constitutional reform, most notably with the failed attempts to amend the constitution through the Meech Lake Accord and the Charlottetown Accord (the latter of which was rejected through a national referendum).\nSince the Quiet Revolution, sovereigntist sentiments in Quebec have been variably stoked by the patriation of the Canadian constitution in 1982 (without Quebec's consent) and by the failed attempts at constitutional reform. Two provincial referendums, in 1980 and 1995, rejected proposals for sovereignty with majorities of 60% and 50.6% respectively. Given the narrow federalist victory in 1995, a reference was made by the Chr\u00e9tien government to the Supreme Court of Canada in 1998 regarding the legality of unilateral provincial secession. The court decided that a unilateral declaration of secession would be unconstitutional. This resulted in the passage of the \"Clarity Act\" in 2000.\nThe Bloc Qu\u00e9b\u00e9cois, a sovereigntist party which runs candidates exclusively in Quebec, was started by a group of MPs who left the Progressive Conservative (PC) party (along with several disaffected Liberal MPs), and first put forward candidates in the 1993 federal election. With the collapse of the PCs in that election, the Bloc and Liberals were seen as the only two viable parties in Quebec. Thus, prior to the 2006 election, any gain by one party came at the expense of the other, regardless of whether national unity was really at issue. The Bloc, then, benefited (with a significant increase in seat total) from the impressions of corruption that surrounded the Liberal Party in the lead-up to the 2004 election. However, the newly unified Conservative party re-emerged as a viable party in Quebec by winning 10 seats in the 2006 election. In the 2011 election, the New Democratic Party succeeded in winning 59 of Quebec's 75 seats, successfully reducing the number of seats of every other party substantially. The NDP surge nearly destroyed the Bloc, reducing them to 4 seats, far below the minimum requirement of 12 seats for Official party status.\nNewfoundland and Labrador is also a problem regarding national unity. As the Dominion of Newfoundland was a self-governing country equal to Canada until 1949, there are large, though unco-ordinated, feelings of Newfoundland nationalism and anti-Canadian sentiment among much of the population. This is due in part to the perception of chronic federal mismanagement of the fisheries, forced resettlement away from isolated settlements in the 1960s, the government of Quebec still drawing inaccurate political maps whereby they take parts of Labrador, and to the perception that mainland Canadians look down upon Newfoundlanders. In 2004, the Newfoundland and Labrador First Party contested provincial elections and in 2008 in federal ridings within the province. In 2004, then-premier Danny Williams ordered all federal flags removed from government buildings as a result of lost offshore revenues to equalization clawbacks. On December 23, 2004, premier Williams made this statement to reporters in St. John's,\nWestern alienation is another national-unity-related concept that enters into Canadian politics. Residents of the four western provinces, particularly Alberta, have often been unhappy with a lack of influence and a perceived lack of understanding when residents of Central Canada consider \"national\" issues. While this is seen to play itself out through many avenues (media, commerce, and so on.), in politics, it has given rise to a number of political parties whose base constituency is in western Canada. These include the United Farmers of Alberta, who first won federal seats in 1917, the Progressives (1921), the Social Credit Party (1935), the Co-operative Commonwealth Federation (1935), the Reconstruction Party (1935), New Democracy (1940) and most recently the Reform Party (1989).\nThe Reform Party's slogan \"The West Wants In\" was echoed by commentators when, after a successful merger with the PCs, the successor party to both parties, the Conservative Party won the 2006 election. Led by Stephen Harper, who is an MP from Alberta, the electoral victory was said to have made \"The West IS In\" a reality. However, regardless of specific electoral successes or failures, the concept of western alienation continues to be important in Canadian politics, particularly on a provincial level, where opposing the federal government is a common tactic for provincial politicians. For example, in 2001, a group of prominent Albertans produced the Alberta Agenda, urging Alberta to take steps to make full use of its constitutional powers, much as Quebec has done.\nPolitical conditions.\nCanada is considered by most sources to be a very stable democracy. In 2006, \"The Economist\" ranked Canada the third-most democratic nation in its Democracy Index, ahead of all other nations in the Americas and ahead of every nation more populous than itself. According to the V-Dem Democracy indices, in 2023 Canada was the 19th most electoral democratic country in the world.\nMore recently, with the existence of strong third parties and first-past-the-post elections amongst other factors, Canada on a federal and provincial level has experienced huge swings in seat shares, where third parties (e.g. NDP, Reform) end up (usually briefly) replacing the Liberals, the Progressive Conservatives or the Conservatives as the main opposition or even the government and leaving them as a rump. Such examples federally include the 1993 federal election with the collapse of the Progressive Conservatives, and the 2011 election leaving the Liberal Party a (temporary) rump along with Bloc Qu\u00e9b\u00e9cois. Other examples include the changes of fortune for the Alberta NDP during the province's 2015 and 2019 elections, and possibly the 2018 Quebec elections with the rise of Coalition Avenir Qu\u00e9bec taking government from the Liberals and Parti Qu\u00e9b\u00e9cois.\nOn a provincial level, in the legislatures of western provinces the NDP often is the left-leaning main party instead of that province's Liberal Party branch, the latter generally being a rump or smaller than the NDP. The other main party (right of the NDP) is either the Progressive Conservatives or their successor, or the Saskatchewan Party in Saskatchewan.\nParty systems.\nAccording to recent scholars, there have been four party systems in [Canada] at the federal level since Confederation, each with its own distinctive pattern of social support, patronage relationships, leadership styles, and electoral strategies. Political scientists disagree on the names and precise boundaries of the eras, however. Steve Patten identifies four party systems in Canada's political history\nClarkson (2005) shows how the Liberal Party has dominated all the party systems, using different approaches. It began with a \"clientelistic approach\" under Laurier, which evolved into a \"brokerage\" system of the 1920s, 1930s and 1940s under Mackenzie King. The 1950s saw the emergence of a \"pan-Canadian system\", which lasted until the 1990s. The 1993 election \u2014 categorized by Clarkson as an electoral \"earthquake\" which \"fragmented\" the party system, saw the emergence of regional politics within a four party-system, whereby various groups championed regional issues and concerns. Clarkson concludes that the inherent bias built into the first-past-the-post system, has chiefly benefited the Liberals.\nParty funding.\nThe rules governing the funding of parties are designed to ensure reliance on personal contributions. Personal donations to federal parties and campaigns benefit from tax credits, although the amount of tax relief depends on the amount given. Also only people paying income taxes receive any benefit from this.\nThe rules are based on the belief that union or business funding should not be allowed to have as much impact on federal election funding as these are not contributions from citizens and are not evenly spread out between parties. The new rules stated that a party had to receive 2% of the vote nationwide in order to receive the general federal funding for parties. Each vote garnered a certain dollar amount for a party (approximately $1.75) in future funding. For the initial disbursement, approximations were made based on previous elections. The NDP received more votes than expected (its national share of the vote went up) while the new Conservative Party of Canada received fewer votes than had been estimated and was asked to refund the difference. Quebec was the first province to implement a similar system of funding many years before the changes to funding of federal parties.\nFederal funds are disbursed quarterly to parties, beginning at the start of 2005. For the moment, this disbursement delay leaves the NDP and the Green Party in a better position to fight an election, since they rely more on individual contributors than federal funds. The Green Party now receives federal funds, since it for the first time received a sufficient share of the vote in the 2004 election.\nIn 2007, news emerged of a funding loophole that \"could cumulatively exceed the legal limit by more than $60,000\", through anonymous recurrent donations of $200 to every riding of a party from corporations or unions. At the time, for each individual, the legal annual donation limit was $1,100 for each party, $1,100 combined total for each party's associations, and in an election year, an additional $1,100 combined total for each party's candidates. All three limits increase on 1 April every year based on the inflation rate.\nTwo of the biggest federal political parties in Canada experienced a drop in donations in 2020, in light of the COVID-19 pandemic impact on the global economy.\nLeaders' debates.\nLeaders' debates in Canada consist of two debates, one English and one French, both produced by a consortium of Canada's five major television broadcasters (CBC/SRC, CTV, Global and TVA) and usually consist of the leaders of all parties with representation in the House of Commons.\nThese debates air on the networks of the producing consortium as well as the public affairs and parliamentary channel CPAC and the American public affairs network C-SPAN.\nGovernment departments and structure.\nThe Canadian government operates the public service using departments, smaller agencies (for example, commissions, tribunals, and boards), and crown corporations. There are two types of departments: central agencies such as Finance, Privy Council Office, and Treasury Board Secretariat have an organizing and oversight role for the entire public service; line departments are departments that perform tasks in a specific area or field, such as the departments of Agriculture, Environment, or Defence.\nScholar Peter Aucoin, writing about the Canadian Westminster system, raised concerns in the early 2000s about the centralization of power; an increased number, role and influence of partisan-political staff; personal-politicization of appointments to the senior public service; and the assumption that the public service is promiscuously partisan for the government of the day."}
{"id": "5195", "revid": "48129973", "url": "https://en.wikipedia.org/wiki?curid=5195", "title": "Economy of Canada", "text": "The economy of Canada is a highly developed mixed economy, the world's ninth-largest , and a nominal GDP of approximately . Canada is one of the world's largest trading nations, with a highly globalized economy. In 2021, Canadian trade in goods and services reached $2.016\u00a0trillion. Canada's exports totalled over $637\u00a0billion, while its imported goods were worth over $631\u00a0billion, of which approximately $391\u00a0billion originated from the United States. In 2018, Canada had a trade deficit in goods of $22\u00a0billion and a trade deficit in services of $25\u00a0billion. The Toronto Stock Exchange is the tenth-largest stock exchange in the world by market capitalization, listing over 1,500 companies with a combined market capitalization of over .\nCanada has a strong cooperative banking sector, with the world's highest per-capita membership in credit unions. It ranks low in the Corruption Perceptions Index (14th in 2023) and \"is widely regarded as among the least corrupt countries of the world\". It ranks high in the Global Competitiveness Report (14th in 2019) and Global Innovation Indexes (15th in 2022). Canada's economy ranks above most Western nations on The Heritage Foundation's Index of Economic Freedom and experiences a relatively low level of income disparity. The country's average household disposable income per capita is \"well above\" the OECD average. Canada ranks among the lowest of the most developed countries for housing affordability and foreign direct investment. Among OECD members, Canada has a highly efficient and strong social security system; social expenditure stood at roughly 23.1% of GDP.\nSince the early 20th century, the growth of Canada's manufacturing, mining, and service sectors has transformed the nation from a largely rural economy to an urbanized, industrial one. Like many other developed countries, the Canadian economy is dominated by the service industry, which employs about three-quarters of the country's workforce. Among developed countries, Canada has an unusually important primary sector, of which the forestry and petroleum industries are the most prominent components. Many towns in northern Canada, where agriculture is difficult, are sustained by nearby mines or sources of timber. Canada spends around 1.70% of GDP on advance research and development across various sectors of the economy.\nCanada's economic integration with the United States has increased significantly since World War II. The Automotive Products Trade Agreement of 1965 opened Canada's borders to trade in the automobile manufacturing industry. In the 1970s, concerns over energy self-sufficiency and foreign ownership in the manufacturing sectors prompted the federal government to enact the National Energy Program (NEP) and the Foreign Investment Review Agency (FIRA). The government abolished the NEP in the 1980s and changed the name of FIRA to Investment Canada to encourage foreign investment. The Canada\u00a0\u2013 United States Free Trade Agreement (FTA) of 1988 eliminated tariffs between the two countries, while the North American Free Trade Agreement (NAFTA) expanded the free-trade zone to include Mexico in 1994 (later replaced by the Canada\u2013United States\u2013Mexico Agreement). As of 2023, Canada is a signatory to 15 free trade agreements with 51 countries.\nCanada is one of the few developed nations that are net exporters of energy. Atlantic Canada possess vast offshore deposits of natural gas, and Alberta hosts the fourth-largest oil reserves in the world. The vast Athabasca oil sands and other oil reserves give Canada 13\u00a0percent of global oil reserves, constituting the world's third or fourth-largest. Canada is additionally one of the world's largest suppliers of agricultural products; the Canadian Prairies are one of the most important global producers of wheat, canola, and other grains. The country is a leading exporter of zinc, uranium, gold, nickel, platinoids, aluminum, steel, iron ore, coking coal, lead, copper, molybdenum, cobalt, and cadmium. Canada has a sizeable manufacturing sector centred in southern Ontario and Quebec, with automobiles and aeronautics representing particularly important industries. The fishing industry is also a key contributor to the economy.\nOverview.\nWith the exception of a few island nations in the Caribbean, Canada is the only North American country to use the parliamentary system of government. As a result, Canada has developed its own social and political institutions, distinct from most other countries in the world. Though the Canadian economy is closely integrated with the American economy, it has developed unique economic institutions.\nThe Canadian economic system generally combines elements of private enterprise and public enterprise. Many aspects of public enterprise, most notably the development of an extensive welfare spending system to redress social and economic inequities, were adopted after the end of World War II in 1945.\nApproximately 89% of Canada's land is Crown land. Canada has one of the highest levels of economic freedom in the world. Today Canada closely resembles the U.S. in its market-oriented economic system and pattern of production. As of 2019, Canada has 56 companies in the Forbes Global 2000 list, ranking ninth just behind South Korea and ahead of Saudi Arabia.\nInternational trade makes up a large part of the Canadian economy, particularly of its natural resources. In 2009, agriculture, energy, forestry and mining exports accounted for about 58% of Canada's total exports. Machinery, equipment, automotive products and other manufactures accounted for a further 38% of exports in 2009. In 2009, exports accounted for about 30% of Canada's GDP. The United States is by far its largest trading partner, accounting for about 73% of exports and 63% of imports as of 2009. Canada's combined exports and imports ranked 8th among all nations in 2006.\nAbout 4% of Canadians are directly employed in primary resource fields, and they account for 6.2% of GDP. They are still paramount in many parts of the country. Many, if not most, towns in northern Canada, where agriculture is difficult, exist because of a nearby mine or source of timber. Canada is a world leader in the production of many natural resources such as gold, nickel, uranium, diamonds, lead, and in recent years, crude petroleum, which, with the world's second-largest oil reserves, is taking an increasingly prominent position in natural resources extraction. Several of Canada's largest companies are based in natural resource industries, such as Encana, Cameco, Goldcorp, and Barrick Gold. The vast majority of these products are exported, mainly to the United States. There are also many secondary and service industries that are directly linked to primary ones. For instance one of Canada's largest manufacturing industries is the pulp and paper sector, which is directly linked to the logging business.\nThe reliance on natural resources has several effects on the Canadian economy and Canadian society. While manufacturing and service industries are easy to standardize, natural resources vary greatly by region. This ensures that differing economic structures developed in each region of Canada, contributing to Canada's strong regionalism. At the same time the vast majority of these resources are exported, integrating Canada closely into the international economy. Howlett and Ramesh argue that the inherent instability of such industries also contributes to greater government intervention in the economy, to reduce the social impact of market changes.\nNatural resource industries also raise important questions of sustainability. Despite many decades as a leading producer, there is little risk of depletion. Large discoveries continue to be made, such as the massive nickel find at Voisey's Bay. Moreover, the far north remains largely undeveloped as producers await higher prices or new technologies as many operations in this region are not yet cost effective. In recent decades Canadians have become less willing to accept the environmental destruction associated with exploiting natural resources. High wages and Aboriginal land claims have also curbed expansion. Instead, many Canadian companies have focused their exploration, exploitation and expansion activities overseas where prices are lower and governments more amenable. Canadian companies are increasingly playing important roles in Latin America, Southeast Asia, and Africa.\nThe depletion of renewable resources has raised concerns in recent years. After decades of escalating overutilization the cod fishery all but collapsed in the 1990s, and the Pacific salmon industry also suffered greatly. The logging industry, after many years of activism, has in recent years moved to a more sustainable model, or to other countries.\nMeasuring productivity.\nProductivity measures are key indicators of economic performance and a key source of economic growth and competitiveness. OECD's \"Compendium of Productivity Indicators\", published annually, presents a broad overview of productivity levels and growth in member nations, highlighting key measurement issues. It analyses the role of \"productivity as the main driver of economic growth and convergence\" and the \"contributions of labour, capital and MFP in driving economic growth\". According to the definition above \"MFP is often interpreted as the contribution to economic growth made by factors such as technical and organisational innovation\". Measures of productivity include the gross domestic product (GDP) and total factor productivity.\nMultifactor productivity.\nAnother productivity measure, used by OECD, is the long-term trend in multifactor productivity (MFP) also known as total factor productivity (TFP). This indicator assesses an economy's \"underlying productive capacity ('potential output'), itself an important measure of the growth possibilities of economies and of inflationary pressures\". MFP measures the residual growth that cannot be explained by the rate of change in the services of labour, capital and intermediate outputs, and is often interpreted as the contribution to economic growth made by factors such as technical and organisational innovation.\nAccording to OECD's annual economic survey of Canada in June 2012, Canada has experienced weak growth of multi-factor productivity (MFP) and has been declining further since 2002. One of the ways MFP growth is raised is by boosting innovation and Canada's innovation indicators such as business R&amp;D and patenting rates were poor. Raising MFP growth is \"needed to sustain rising living standards, especially as the population ages\".\nSince 2010 productivity growth has picked up, almost entirely driven by above average multifactor productivity growth. However, productivity on the whole still lags behind the upper half of OECD countries such as the United States. Canada's productivity is now around the median OECD productivity, close to that of Australia. More can be done to increase productivity, such as increasing the productivity of capital through improving the capital stock to output ratio and capital quality. This could be achieved through the liberalization of internal trade barriers, as suggested in the OECD's latest Canadian economic survey.\nBank of Canada.\nThe mandate of the central bank\u2014the Bank of Canada is to conduct monetary policy that \"preserves the value of money by keeping inflation low and stable\".\nMonetary Policy Report.\nThe Bank of Canada issues its bank rate announcement through its Monetary Policy Report which is released eight times a year. The Bank of Canada, a federal crown corporation, has the responsibility of Canada's monetary system. Under the inflation-targeting monetary policy that has been the cornerstone of Canada's monetary and fiscal policy since the early 1990s, the Bank of Canada sets an inflation target The inflation target was set at 2 per cent, which is the midpoint of an inflation range of 1 to 3 per cent. They established a set of inflation-reduction targets to keep inflation \"low, stable and predictable\" and to foster \"confidence in the value of money\", contribute to Canada's sustained growth, employment gains and improved standard of living.\nIn a January 9, 2019 statement on the release of the Monetary Policy Report, Bank of Canada Governor Stephen S. Poloz summarized major events since the October report, such as \"negative economic consequences\" of the US-led trade war with China. In response to the ongoing trade war \"bond yields have fallen, yield curves have flattened even more and stock markets have repriced significantly\" in \"global financial markets\". In Canada, low oil prices will impact Canada's \"macroeconomic outlook\". Canada's housing sector is not stabilizing as quickly as anticipated.\nInflation targeting.\nDuring the period that John Crow was Governor of the Bank of Canada\u20141987 to 1994\u2014there was a worldwide recession and the bank rate rose to around 14% and unemployment topped 11%. Although since that time inflation-targeting has been adopted by \"most advanced-world central banks\", in 1991 it was innovative and Canada was an early adopter when the then-Finance Minister Michael Wilson approved the Bank of Canada's first inflation-targeting in the 1991 federal budget. The inflation target was set at 2 per cent. Inflation is measured by the total consumer price index (CPI). In 2011 the Government of Canada and the Bank of Canada extended Canada's inflation-control target to December 31, 2016. The Bank of Canada uses three unconventional instruments to achieve the inflation target: \"a conditional statement on the future path of the policy rate\", quantitative easing, and credit easing.\nAs a result, interest rates and inflation eventually came down along with the value of the Canadian dollar. From 1991 to 2011 the inflation-targeting regime kept \"price gains fairly reliable\".\nFollowing the Great Recession, the narrow focus of inflation-targeting as a means of providing stable growth in the Canadian economy was questioned. By 2011, the then-Bank of Canada Governor Mark Carney argued that the central bank's mandate would allow for a more flexible inflation-targeting in specific situations where he would consider taking longer \"than the typical six to eight quarters to return inflation to 2 per cent\".\nOn July 15, 2015, the Bank of Canada announced that it was lowering its target for the overnight rate by another one-quarter percentage point, to 0.5 per cent \"to try to stimulate an economy that appears to have failed to rebound meaningfully from the oil shock woes that dragged it into decline in the first quarter\". According to the Bank of Canada announcement, in the first quarter of 2015, the total Consumer price index (CPI) inflation was about 1 per cent. This reflects \"year-over-year price declines for consumer energy products\". Core inflation in the first quarter of 2015 was about 2 per cent with an underlying trend in inflation at about 1.5 to 1.7 per cent.\nIn response to the Bank of Canada's July 15, 2015 rate adjustment, Prime Minister Stephen Harper explained that the economy was \"being dragged down by forces beyond Canadian borders such as global oil prices, the European debt crisis, and China's economic slowdown\" which has made the global economy \"fragile\".\nThe Chinese stock market had lost about US$3 trillion of wealth by July 2015 when panicked investors sold stocks, which created declines in the commodities markets, which in turn negatively impacted resource-producing countries like Canada.\nThe Bank's main priority has been to keep inflation at a moderate level. As part of that strategy, interest rates were kept at a low level for almost seven years. Since September 2010, the key interest rate (overnight rate) was 0.5%. In mid 2017, inflation remained below the Bank's 2% target, (at 1.6%) mostly because of reductions in the cost of energy, food and automobiles; as well, the economy was in a continuing spurt with a predicted GDP growth of 2.8 percent by year end. Early on July 12, 2017, the bank issued a statement that the benchmark rate would be increased to 0.75%.\nFollowing the COVID-19 pandemic, critics have pointed out that the Bank of Canada's inflation-targeting has had unintended consequences, such as fuelling an increase in home prices and contributing to wealth inequalities by supporting higher equity values.\nKey industries.\nIn 2020, the Canadian economy had the following relative weighting by the industry as a percentage value of GDP:\nService sector.\nThe service sector in Canada is vast and multifaceted, employing about three quarters of Canadians and accounting for 70% of GDP. The largest employer is the retail sector, employing almost 12% of Canadians. The retail industry is concentrated mainly in a small number of chain stores clustered together in shopping malls. In recent years, there has been an increase in the number of big-box stores, such as Walmart (of the United States), Real Canadian Superstore, and Best Buy (of the United States). This has led to fewer workers in this sector and the migration of retail jobs to the suburbs.\nThe second-largest portion of the service sector is the business service, and it employs only a slightly smaller percentage of the population. This includes the financial services, real estate, and communications industries. This portion of the economy has been rapidly growing in recent years. It is largely concentrated in the major urban centres, especially Toronto, Montreal and Vancouver (see Banking in Canada).\nThe education and health sectors are two of Canada's largest, but both are primarily under the influence of the government. The health care industry has been quickly growing and is the third-largest in Canada. Its rapid growth has led to problems for governments who must find money to fund it.\nCanada has an important high tech industry, and a burgeoning film, television, and entertainment industry creating content for local and international consumption (see Media in Canada). Tourism is of ever increasing importance, with the vast majority of international visitors coming from the United States. Casino gaming is currently the fastest-growing component of the Canadian tourism industry, contributing $5 billion in profits for Canadian governments and employing 41,000 Canadians as of 2001.\nManufacturing.\nThe general pattern of development for wealthy nations was a transition from a raw material production-based economy to a manufacturing-based economy and then to a service-based economy. At its World War II peak in 1944, Canada's manufacturing sector accounted for 29% of GDP, declining to 10.37% in 2017. Canada has not suffered as greatly as most other rich, industrialized nations from the pains of the relative decline in the importance of manufacturing since the 1960s. A 2009 study by Statistics Canada also found that, while manufacturing declined as a relative percentage of GDP from 24.3% in the 1960s to 15.6% in 2005, manufacturing volumes between 1961 and 2005 kept pace with the overall growth in the volume index of GDP. Manufacturing in Canada declined significantly during the Great Recession. As of 2017, manufacturing accounts for 10% of Canada's GDP, a relative decline of more than 5% of GDP since 2005.\nCentral Canada is home to branch plants to all the major American and Japanese automobile makers and many parts factories owned by Canadian firms such as Magna International and Linamar Corporation.\nSteel.\nCanada was the world's nineteenth-largest steel exporter in 2018. In year-to-date 2019 (through March), further referred to as YTD 2019, Canada exported 1.39 million metric tons of steel, a 22 percent decrease from 1.79 million metric tons in YTD 2018. Based on available data, Canada's exports represented about 1.5 percent of all steel exported globally in 2017. By volume, Canada's 2018 steel exports represented just over one-tenth the volume of the world's largest exporter, China. In value terms, steel represented 1.4 percent of the total goods Canada exported in 2018. The growth in exports in the decade since 2009 has been 29%. The largest producers in 2018 were ArcelorMittal, Essar Steel Algoma, and the first of those alone accounted for roughly half of Canadian steel production through its two subsidiaries. The top two markets for Canada's exports were its NAFTA partners, and by themselves accounted for 92 percent of exports by volume. Canada sent 83 percent of its steel exports to the United States in YTD 2019. The gap between domestic demand and domestic production increased to \u22122.4 million metric tons, up from \u22120.2 million metric tons in YTD 2018. In YTD 2019, exports as a share of production decreased to 41.6 percent from 53 percent in YTD 2018.\nIn 2017, heavy industry accounted for 10.2% of Canada's greenhouse gas emissions.\nMining.\nCanada is one of the largest producers of metals (as of 2019):\nIn 2019, the country was also the 4th largest world producer of sulfur; the 13th largest world producer of gypsum; the 14th worldwide producer of antimony; the world's 10th largest producer of graphite; in addition to being the 6th largest world producer of salt. It was the 2nd largest producer in the world of uranium in 2018.\nEnergy.\nCanada has access to cheap sources of energy because of its geography. This has enabled the creation of several important industries, such as the large aluminum industries in British Columbia and Quebec. Canada is also one of the world's highest per capita consumers of energy.\nElectricity.\nThe electricity sector in Canada has played a significant role in the economic and political life of the country since the late 19th century. The sector is organized along provincial and territorial lines. In a majority of provinces, large government-owned integrated public utilities play a leading role in the generation, transmission and distribution of electricity. Ontario and Alberta have created electricity markets in the last decade in order to increase investment and competition in this sector of the economy. In 2017, the electricity sector accounted for 10% of total national greenhouse gas emissions. Canada has substantial electricity trade with the neighbouring United States amounting to 72 TWh exports and 10 TWh imports in 2017.\nHydroelectricity accounted for 59% of all electric generation in Canada in 2016, making Canada the world's second-largest producer of hydroelectricity after China. Since 1960, large hydroelectric projects, especially in Quebec, British Columbia, Manitoba and Newfoundland and Labrador, have significantly increased the country's generation capacity.\nThe second-largest single source of power (15% of the total) is nuclear power, with several plants in Ontario generating more than half of that province's electricity and one generator in New Brunswick. This makes Canada the world's sixth-largest electricity producer generated by nuclear power, producing 95 TWh in 2017.\nFossil fuels provide 19% of Canadian electric power, about half as coal (9% of the total), and the remainder a mix of natural gas and oil. Only five provinces use coal for electricity generation. Alberta, Saskatchewan, and Nova Scotia rely on coal for nearly half of their generation, while other provinces and territories use little or none. Alberta and Saskatchewan also use a substantial amount of natural gas. Remote communities, including all of Nunavut and much of the Northwest Territories, produce most of their electricity from diesel generators at high economic and environmental costs. The federal government has set up initiatives to reduce dependence on diesel-fired electricity.\nNon-hydro renewables are a fast-growing portion of the total, at 7% in 2016.\nOil and gas.\nCanada possesses extensive oil and gas resources centered in Alberta, and the Northern Territories but is also present in neighboring British Columbia and Saskatchewan. The vast Athabasca oil sands give Canada the world's third-largest reserves of oil after Saudi Arabia and Venezuela, according to USGS. The oil and gas industry represents 27% of Canada's total greenhouse gas emissions, an increase of 84% since 1990, mostly due to the development of the oil sands.\nHistorically, an important issue in Canadian politics is the interplay between the oil and energy industry in Western Canada and the industrial heartland of Southern Ontario. Foreign investment in Western oil projects has fueled Canada's rising dollar. This has raised the price of Ontario's manufacturing exports and made them less competitive, a problem similar to the decline of the manufacturing sector in the Netherlands.\nThe National Energy Policy of the early 1980s attempted to make Canada oil-sufficient and to ensure equal supply and price of oil in all parts of Canada, especially for the eastern manufacturing base. This policy proved deeply divisive as it forced Alberta to sell low-priced oil to eastern Canada. The policy was eliminated 5 years after it was first announced amid a collapse of oil prices in 1985. The new Prime Minister Brian Mulroney had campaigned against the policy in the 1984 Canadian federal election. One of the most controversial sections of the Canada\u2013United States Free Trade Agreement of 1988 was a promise that Canada would never charge the United States more for energy than fellow Canadians.\nAgriculture.\nCanada is one of the world's largest suppliers of agricultural products, particularly wheat and other grains. Canada is a major exporter of agricultural products, to the United States and Asia. As with all other developed nations, the proportion of the population and GDP devoted to agriculture fell dramatically over the 20th century. The agriculture and agri-food manufacturing sector created $49.0 billion to Canada's GDP in 2015, accounting for 2.6% of total GDP. This sector also accounts for 8.4% of Canada's Greenhouse gas emissions.\nThe Canadian agriculture industry receives significant government subsidies and support as with other developed nations. However, Canada has strongly supported reducing market influencing subsidies through the World Trade Organization. In 2000, Canada spent approximately CDN$4.6 billion on support for the industry. $2.32 billion was classified under the WTO designation of \"green box\" license, meaning it did not directly influence the market, such as money for research or disaster relief. All but $848.2 million were subsidies worth less than 5% of the value of the crops they were provided for.\nFree-trade agreements.\nOngoing free-trade agreements negotiations.\nCanada is negotiating bilateral FTAs with the following countries respectively trade blocs:\nCanada has been involved in negotiations to create the following regional trade blocks:\nPolitical issues.\nCanada\u2013United States trade relations.\nCanada and the United States share a common trading relationship. Canada's job market continues to perform well along with the US, reaching a 30-year low in the unemployment rate in December 2006, following 14 consecutive years of employment growth.\nThe United States is by far Canada's largest trading partner, with more than $1.7 billion CAD in trade per day in 2005. In 2009, 73% of Canada's exports went to the United States, and 63% of Canada's imports were from the United States. Trade with Canada makes up 23% of the United States' exports and 17% of its imports. By comparison, in 2005 this was more than U.S. trade with all countries in the European Union combined, and well over twice U.S. trade with all the countries of Latin America combined. Just the two-way trade that crosses the Ambassador Bridge between Michigan and Ontario equals all U.S. exports to Japan. Canada's importance to the United States is not just a border-state phenomenon: Canada is the leading export market for 35 of 50 U.S. states, and is the United States' largest foreign supplier of energy.\nBilateral trade increased by 52% between 1989, when the U.S.\u2013Canada Free Trade Agreement (FTA) went into effect, and 1994, when the North American Free Trade Agreement (NAFTA) superseded it. Trade has since increased by 40%. NAFTA continues the FTA's moves toward reducing trade barriers and establishing agreed-upon trade rules. It also resolves some long-standing bilateral irritants and liberalizes rules in several areas, including agriculture, services, energy, financial services, investment, and government procurement. NAFTA forms the largest trading area in the world, embracing the 405 million people of the three North American countries.\nThe largest component of U.S.\u2013Canada trade is in the commodity sector.\nThe U.S. is Canada's largest agricultural export market, taking well over half of all Canadian food exports. Nearly two-thirds of Canada's forest products, including pulp and paper, are exported to the United States; 72% of Canada's total newsprint production also is exported to the U.S.\nAt $73.6 billion in 2004, U.S.-Canada trade in energy is the largest U.S. energy trading relationship, with the overwhelming majority ($66.7 billion) being exports from Canada. The primary components of U.S. energy trade with Canada are petroleum, natural gas, and electricity. Canada is the United States' largest oil supplier and the fifth-largest energy producing country in the world. Canada provides about 16% of U.S. oil imports and 14% of total U.S. consumption of natural gas. The United States and Canada's national electricity grids are linked, and both countries share hydropower facilities on the western borders.\nWhile most of U.S.-Canada trade flows smoothly, there are occasionally bilateral trade disputes, particularly in the agricultural and cultural fields. Usually these issues are resolved through bilateral consultative forums or referral to World Trade Organization (WTO) or NAFTA dispute resolution. In May 1999, the U.S. and Canadian governments negotiated an agreement on magazines that provides increased access for the U.S. publishing industry to the Canadian market. The United States and Canada also have resolved several major issues involving fisheries. By common agreement, the two countries submitted a Gulf of Maine boundary dispute to the International Court of Justice in 1981; both accepted the court's October 12, 1984 ruling which demarcated the territorial sea boundary. A current issue between the United States and Canada is the ongoing softwood lumber dispute, as the U.S. alleges that Canada unfairly subsidizes its forestry industry.\nIn 1990, the United States and Canada signed a bilateral Fisheries Enforcement Agreement, which has served to deter illegal fishing activity and reduce the risk of injury during fisheries enforcement incidents. The U.S. and Canada signed a Pacific Salmon Agreement in June 1999 that settled differences over implementation of the 1985 Pacific Salmon Treaty for the next decade.\nCanada and the United States signed an aviation agreement during Bill Clinton's visit to Canada in February 1995, and air traffic between the two countries has increased dramatically as a result. The two countries also share in operation of the St. Lawrence Seaway, connecting the Great Lakes to the Atlantic Ocean.\nThe U.S. remains Canada's largest foreign investor and the most popular destination for Canadian foreign investments. In 2018, the stock of U.S. direct investment in Canada totaled $406 billion, while the stock of Canadian investment in the U.S. totaled $595 billion, or 46% of the overall CDIA stock for 2018. This made Canada the second largest investing country in the U.S. for 2018 US investments are primarily directed at Canada's mining and smelting industries, petroleum, chemicals, the manufacture of machinery and transportation equipment, and finance, while Canadian investment in the United States is concentrated in manufacturing, wholesale trade, real estate, petroleum, finance, insurance and other services.\nDebt.\nCanadian government debt.\nCanadian government debt, also called Canada's public debt, is the liabilities of the government sector. For 2019 (the fiscal year ending 31 March 2020), total financial liabilities or gross debt was $2.434 trillion for the consolidated Canadian general government (federal, provincial, territorial, and local governments combined). This corresponds to 105.3% as a ratio of GDP (GDP was $2.311 trillion). Of the $2.434 trillion, $1.146 trillion or 47% was federal (central) government liabilities (49.6% as a ratio of GDP). Provincial government liabilities comprise most of the remaining liabilities.\nHousehold debt.\nHousehold debt, the amount of money that all adults in the household owe financial institutions, includes consumer debt and mortgage loans. In March 2015, the International Monetary Fund reported that Canada's high household debt was one of two vulnerable domestic areas in Canada's economy; the second is its overheated housing market.\nAccording to Statistics Canada, total household credit as of July 2019 was CAD$2.2 trillion. According to Philip Cross of the Fraser Institute, in May 2015, while the Canadian household debt-to-income ratio is similar to that in the US, however lending standards in Canada are tighter than those in the United States to protect against high-risk borrowers taking out unsustainable debt.\nMergers and acquisitions.\nSince 1985, 63,755 deals in- and outbound Canada have been announced, with an overall value of US$3.7 billion. Almost 50% of the targets of Canadian companies (outbound deals) have a parent company in the US. Inbound deals are 82% percent from the US.\nHere is a list of the biggest deals in Canadian history:\nRaw data.\nThe following table shows the main economic indicators in 1980\u20132021 (with IMF staff estimates for 2022\u20132027). Inflation below 5% is in green.\nExport trade.\nExport trade from Canada measured in US dollars. In 2021, Canada exported US$503.4 billion.\nThat dollar amount reflects a 19.5% gain since 2017 and a 29.1% increase from 2020 to 2021.\nImport trade.\nImport trade in 2017 measured in US dollars."}
{"id": "5196", "revid": "6289403", "url": "https://en.wikipedia.org/wiki?curid=5196", "title": "Telecommunications in Canada", "text": "Present-day telecommunications in Canada include telephone, radio, television, and internet usage. In the past, telecommunications included telegraphy available through Canadian Pacific and Canadian National.\nHistory.\nThe history of telegraphy in Canada dates back to the Province of Canada. While the first telegraph company was the Toronto, Hamilton and Niagara Electro-Magnetic Telegraph Company, founded in 1846, it was the Montreal Telegraph Company, controlled by Hugh Allan and founded a year later, that dominated in Canada during the technology's early years.\nFollowing the 1852 Telegraph Act, Canada's first permanent transatlantic telegraph link was a submarine cable built in 1866 between Ireland and Newfoundland. Telegrams were sent through networks built by Canadian Pacific and Canadian National.\nIn 1868 Montreal Telegraph began facing competition from the newly established Dominion Telegraph Company. 1880 saw the Great North Western Telegraph Company established to connect Ontario and Manitoba but within a year it was taken over by Western Union, leading briefly to that company's control of almost all telegraphy in Canada. In 1882, Canadian Pacific transmitted its first commercial telegram over telegraph lines they had erected alongside its tracks, breaking Western Union's monopoly. Great North Western Telegraph, facing bankruptcy, was taken over in 1915 by Canadian Northern.\nBy the end of World War II, Canadians communicated by telephone more than any other country. In 1967 the CP and CN networks were merged to form CNCP Telecommunications.\nAs of 1951, approximately 7000 messages were sent daily from the United States to Canada. An agreement with Western Union required that U.S. company to route messages in a specified ratio of 3:1, with three telegraphic messages transmitted to Canadian National for every message transmitted to Canadian Pacific. The agreement was complicated by the fact that some Canadian destinations were served by only one of the two networks.\nFixed-line telephony.\nTelephones - fixed lines: total subscriptions: 13.926 million (2020)\nTelephones - mobile cellular: 36,093,021 (2020)\nTelephone system: (2019)\nCall signs.\nITU prefixes: Letter combinations available for use in Canada as the first two letters of a television or radio station's call sign are CF, CG, CH, CI, CJ, CK, CY, CZ, VA, VB, VC, VD, VE, VF, VG, VO, VX, VY, XJ, XK, XL, XM, XN and XO. Only CF, CH, CI, CJ and CK are currently in common use, although four radio stations in St. John's, Newfoundland and Labrador retained call letters beginning with VO when Newfoundland joined Canadian Confederation in 1949. Stations owned by the Canadian Broadcasting Corporation use CB through a special agreement with the government of Chile. Some codes beginning with VE and VF are also in use to identify radio repeater transmitters.\nRadio.\nAs of 2016, there were over 1,100 radio stations and audio services broadcasting in Canada. Of these, 711 are private commercial radio stations. These commercial stations account for over three quarters of radio stations in Canada. The remainder of the radio stations are a mix of public broadcasters, such as CBC Radio, as well as campus, community, and Aboriginal stations.\nTelevision.\nAs of 2018, 762 TV services were broadcasting in Canada. This includes both conventional television stations and discretionary services.\nCable and satellite television services are available throughout Canada. The largest cable providers are Bell Canada, Rogers Cable, Vid\u00e9otron, Telus and Cogeco, while the two licensed satellite providers are Bell Satellite TV and Shaw Direct.\nInternet.\nBell, Rogers, Telus, and Shaw are among the bigger ISPs in Canada. Depending on your location, Bell and Rogers would be the big internet service providers in Eastern provinces, while Shaw and Telus are the main players competing in western provinces.\nMobile networks.\nThe three major mobile network operators are Rogers Wireless (13.7 million subscribers), Bell Mobility (10.29 million) and Telus Mobility (9.5 million), which have a combined 86% of market share.\nAdministration and Government.\nFederally, telecommunications are overseen by the Canadian Radio-television and Telecommunications Commission ()\u2013CRTC as outlined under the provisions of both the Telecommunications Act and Radiocommunication Acts. CRTC further works with Innovation, Science and Economic Development Canada (formerly Industry Canada) on various technical aspects including: allocating frequencies and call signs, managing the broadcast spectrum, and regulating other technical issues such as interference with electronics equipment. As Canada comprises a part of the North American Numbering Plan for area codes, the Canadian Numbering Administration Consortium within Canada is responsible for allocating and managing area codes in Canada.\nFurther reading.\nBibliography"}
{"id": "5197", "revid": "326155", "url": "https://en.wikipedia.org/wiki?curid=5197", "title": "Transportation in Canada", "text": " \nCanada, the world's second-largest country in total area, is dedicated to having an efficient, high-capacity multimodal transportation spanning often vast distances between natural resource extraction sites, agricultural and urban areas. Canada's transportation system includes more than of roads, 10 major international airports, 300 smaller airports, of functioning railway track, and more than 300 commercial ports and harbours that provide access to the Pacific, Atlantic and Arctic oceans as well as the Great Lakes and the St. Lawrence Seaway. In 2005, the transportation sector made up 4.2% of Canada's GDP, compared to 3.7% for Canada's mining and oil and gas extraction industries.\nTransport Canada oversees and regulates most aspects of transportation within federal jurisdiction, including interprovincial transport. This primarily includes rail, air and maritime transportation. Transport Canada is under the direction of the federal government's Minister of Transport. The Transportation Safety Board of Canada is responsible for maintaining transportation safety in Canada by investigating accidents and making safety recommendations.\nHistory.\nThe standard history covers the French regime, fur traders, the canals, and early roads, and gives extensive attention to the railways.\nEuropean contact.\nPrior to the arrival of European settlers, Aboriginal peoples in Canada walked. They also used canoes, kayaks, umiaks and Bull Boats, in addition to the snowshoe, toboggan and sled in winter. They had no wheeled vehicles, and no animals larger than dogs.\nEuropeans adopted canoes as they pushed deeper into the continent's interior, and were thus able to travel via the waterways that fed from the St. Lawrence River and Hudson Bay.\nIn the 19th century and early 20th century transportation relied on harnessing oxen to \"Red River ox carts\" or horse to wagon. Maritime transportation was via manual labour such as canoe or wind on sail. Water or land travel speeds was approximately .\nSettlement was along river routes. Agricultural commodities were perishable, and trade centres were within . Rural areas centred around villages, and they were approximately apart. The advent of steam railways and steamships connected resources and markets of vast distances in the late 19th century. Railways also connected city centres, in such a way that the traveller went by sleeper, railway hotel, to the cities. Crossing the country by train took four or five days, as it still does by car. People generally lived within of the downtown core thus the train could be used for inter-city travel and the tram for commuting.\nThe advent of controlled-access highways in Canada established ribbon development, truck stops, and industrial corridors along throughways.\nEvolution.\nThe Federal Department of Transport (established November 2, 1936) supervised railways, canals, harbours, marine and shipping, civil aviation, radio and meteorology. The Transportation Act of 1938 and the amended Railway Act, placed control and regulation of carriers in the hands of the Board of Transport commissioners for Canada. The Royal Commission on Transportation was formed December 29, 1948, to examine transportation services to all areas of Canada to eliminate economic or geographic disadvantages. The commission also reviewed the Railway Act to provide uniform yet competitive freight-rates.\nRoads.\nThere is a total of of roads in Canada, of which are paved, including of expressways (the third-longest collection in the world, behind the Interstate Highway System of the United States and China's National Trunk Highway System). As of 2008, were unpaved. There are no regulations at a federal level that regulate Canada's road infrastructure, highway system, or traffic safety laws; it is left to the individual provinces and territories to regulate these elements. Regulations on a provincial level include Ontario's Highway Traffic Act, Alberta's Traffic Safety Act, and British Columbia's Motor Vehicle Act, for example. The only regulation at a federal level that relates to motor vehicles is the Motor Vehicle Safety Act, which deals with the manufacturing and importing of motor vehicles and motor vehicle equipment within the country.\nIn 2009, there were 20,706,616 road vehicles registered in Canada, of which 96% were vehicles under , 2.4% were vehicles between and 1.6% were or greater. These vehicles travelled a total of 333.29\u00a0billion kilometres, of which 303.6\u00a0billion was for vehicles under 4.5 tonnes, 8.3\u00a0billion was for vehicles between 4.5 and 15 tonnes and 21.4\u00a0billion was for vehicles over 15 tonnes. For the 4.5- to 15-tonne trucks, 88.9% of vehicle-kilometres were intra-province trips, 4.9% were inter-province, 2.8% were between Canada and the US and 3.4% made outside of Canada. For the trucks over 15 tonnes, 59.1% of vehicle-kilometres were intra-province trips, 20% inter-province trips, 13.8% Canada-US trips and 7.1% trips made outside of Canada.\nCanada's vehicles consumed a total of of gasoline and of diesel. Trucking generated 35% of the total GDP from transport, compared to 25% for rail, water and air combined (the remainder being generated by the industry's transit, pipeline, scenic and support activities). Hence roads are the dominant means of passenger and freight transport in Canada.\nRoads and highways were managed by provincial and municipal authorities until construction of the Northwest Highway System (the Alaska Highway) and the Trans-Canada Highway project initiation. The Alaska Highway of 1942 was constructed during World War II for military purposes connecting Fort St. John, British Columbia with Fairbanks, Alaska. The transcontinental highway, a joint national and provincial expenditure, was begun in 1949 under the initiation of the Trans Canada Highway Act on December 10, 1949. The highway was completed in 1962 at a total expenditure of $1.4\u00a0billion.\nInternationally, Canada has road links with both the lower 48 US states and Alaska. The Ministry of Transportation maintains the road network in Ontario and also employs Ministry of Transport Enforcement Officers for the purpose of administering the Canada Transportation Act and related regulations. The Department of Transportation in New Brunswick performs a similar task in that province as well.\nThe safety of Canada's roads is moderately good by international standards, and is improving both in terms of accidents per head of population and per billion vehicle kilometers.\nAir transport.\nAir transportation made up 9% of the transport sector's GDP generation in 2005. Canada's largest air carrier and its flag carrier is Air Canada, which had 34\u00a0million customers in 2006 and, as of April 2010, operates 363 aircraft (including Air Canada Jazz). CHC Helicopter, the largest commercial helicopter operator in the world, is second with 142 aircraft and WestJet, a low-cost carrier formed in 1996, is third with 100 aircraft. Canada's airline industry saw significant change following the signing of the US-Canada open skies agreement in 1995, when the marketplace became less regulated and more competitive.\nAccording to a 2016 report, Canada's air transportation was the most expensive for consumers globally; however, this was prior to the emergence of ultra-low-cost carriers such as Flair Airlines.\nThe Canadian Transportation Agency employs transportation enforcement officers to maintain aircraft safety standards, and conduct periodic aircraft inspections, of all air carriers. The Canadian Air Transport Security Authority is charged with the responsibility for the security of air traffic within Canada. In 1994 the National Airports Policy was enacted\nPrincipal airports.\nOf over 1,800 registered Canadian aerodromes, certified airports, heliports, and floatplane bases, 26 are specially designated under Canada's National Airports System (NAS): these include all airports that handle 200,000 or more passengers each year, as well as the principal airport serving each federal, provincial, and territorial capital. However, since the introduction of the policy only one, Iqaluit Airport, has been added and no airports have been removed despite dropping below 200,000 passengers. The Government of Canada, with the exception of the three territorial capitals, retains ownership of these airports and leases them to local authorities. The next tier consists of 64 regional/local airports formerly owned by the federal government, most of which have now been transferred to other owners (most often to municipalities).\nBelow is a table of Canada's ten biggest airports by passenger traffic in 2019.\nRailways.\nIn 2007, Canada had a total of of freight and passenger railway, of which is electrified. While intercity passenger transportation by rail is now very limited, freight transport by rail remains common. Total revenues of rail services in 2006 was $10.4\u00a0billion, of which only 2.8% was from passenger services. In a year are usually earned about $11\u00a0billion, of which 3.2% is from passengers and the rest from freight. The Canadian National Railway and Canadian Pacific Kansas City are Canada's two major freight railway companies, each having operations throughout North America. In 2007, 357\u00a0billion tonne-kilometres of freight were transported by rail, and 4.33\u00a0million passengers travelled 1.44\u00a0billion passenger-kilometres (an almost negligible amount compared to the 491\u00a0billion passenger-kilometres made in light road vehicles). 34,281 people were employed by the rail industry in the same year.\nNationwide passenger services are provided by the federal crown corporation Via Rail. VIA Rail has faced criticism for frequent delays, and low speeds compared to peer countries and historical train travel times, such as the records set by the TurboTrain during the 1970s.\nThree Canadian cities have commuter rail services: in the Montreal area by Exo, in the Toronto area by GO Transit, and in the Vancouver area by West Coast Express. Smaller railways such as Ontario Northland, Rocky Mountaineer, and Algoma Central also run passenger trains to remote rural areas.\nIn Canada railways are served by standard gauge, , rails. See also track gauge in Canada.\nCanada has railway links with the lower 48 US States, but no connection with Alaska, although a line has been proposed. There are no other international rail connections.\nWaterways.\nIn 2005, of cargo was loaded and unloaded at Canadian ports. The Port of Vancouver is the busiest port in Canada, moving or 15% of Canada's total in domestic and international shipping in 2003.\nTransport Canada oversees most of the regulatory functions related to marine registration, safety of large vessel, and port pilotage duties. Many of Canada's port facilities are in the process of being divested from federal responsibility to other agencies or municipalities.\nInland waterways comprise , including the St. Lawrence Seaway. Transport Canada enforces acts and regulations governing water transportation and safety.\nCanals.\nThe main route canals of Canada are those of the St. Lawrence River and the Great Lakes. The others are subsidiary canals.\nPorts and harbours.\nThe National Harbours Board administered Halifax, Saint John, Chicoutimi, Trois-Rivi\u00e8res, Churchill, and Vancouver until 1983. At one time, over 300 harbours across Canada were supervised by the Department of Transport. A program of divestiture was implemented around the turn of the millennium, and as of 2014, 493 of the 549 sites identified for divestiture in 1995 have been sold or otherwise transferred, as indicated by a DoT list. The government maintains an active divestiture programme, and after divestiture Transport Canada oversees only 17 Canada Port Authorities for the 17 largest shipping ports.\nMerchant marine.\nCanada's merchant marine comprised a \"total\" of 173 ships ( or over) or at the end of 2007.\nPipelines.\nPipelines are part of the energy extraction and transportation network of Canada and are used to transport natural gas, natural gas liquids, crude oil, synthetic crude and other petroleum based products. Canada has of pipeline for transportation of crude and refined oil, and for liquefied petroleum gas.\nPublic transit.\nMost Canadian cities have public transport, if only a bus system. Three Canadian cities have rapid transit systems, four have light rail systems, and three have commuter rail systems (see below). In 2016, 12.4% of Canadians used public transportation to get to work. This compares to 79.5% that got to work using a car (67.4% driving alone, 12.1% as part of a carpool), 5.5% that walked and 1.4% that rode a bike.\nGovernment organizations across Canada owned 17,852 buses of various types in 2016. Organizations in Ontario (38.8%) and Quebec (21.9%) accounted for just over three-fifths of the country's total bus fleet. Urban municipalities owned more than 85% of all buses.\nin 2016, diesel buses were the leading bus type in Canada (65.9%), followed by bio-diesel (18.1%) and hybrid (9.4%) buses. Electric, natural gas and other buses collectively accounted for the remaining 6.6%.\nRapid transit systems.\nThere are three rapid transit systems operating in Canada: the Montreal Metro, the Toronto subway, and the Vancouver SkyTrain.\nThere is also an airport circulator, the Link Train, at Toronto Pearson International Airport. It operates 24 hours a day, 7 days a week and is wheelchair-accessible. It is free of cost.\nLight rail systems.\nThere are light rail systems in four cities \u2013 the Calgary CTrain, the Edmonton LRT, the Ottawa O-Train, and Waterloo Region's Ion \u2013 while Toronto has an extensive streetcar system.\nThe 2016 Canada's Core Public Infrastructure Survey from Statistics Canada found that all of Canada's 247 streetcars were owned by the City of Toronto. The vast majority (87.9%) of these streetcars were purchased from 1970 to 1999, while 12.1% were purchased in 2016. Reflecting the age of the streetcars, 88.0% were reported to be in very poor condition, while 12.0% were reported to be in good condition.\nCommuter train systems.\nCommuter trains serve the cities and surrounding areas of Montreal, Toronto and Vancouver:"}
{"id": "5198", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5198", "title": "Military of Canada", "text": ""}
{"id": "5199", "revid": "8729451", "url": "https://en.wikipedia.org/wiki?curid=5199", "title": "Canada\u2013United States relations", "text": "Canada's long and complex relationship with the United States has had a significant impact on its history, economy, and culture, heritage. The two countries consider themselves among the \"closest [of] allies\". They share the longest border () between any two nations in the world, and also have significant military interoperability. Both Americans and Canadians have generally ranked each other as one of their respective \"favorite nations\". \nThe economies and supply chains of both countries are fully integrated. Every day, around 400,000 people and $2.7 billion in goods and services cross the Canada-U. S. border. The close economic partnership has been facilitated by shared values and strong bilateral trade agreements. The North American Free Trade Agreement (NAFTA) and its successor, the United States-Mexico-Canada Agreement (USMCA), have played a pivotal role in fostering economic cooperation and integration between the two nations. Cross-border projects, such as communications, highways, bridges, and pipelines have led to shared energy networks and transportation systems. The countries have established joint inspection agencies, share data and have harmonized regulations on everything from food to manufactured goods. Despite these facts, disputes have included repeated trade disagreements, environmental concerns, Canadian concern for the future of oil exports, the issue of illegal immigration, the threat of terrorism and illicit drug trade.\nMilitary collaboration was close during World War II and continued throughout the Cold War, bilaterally through NORAD and multilaterally through NATO. However, Canada has long been reluctant to participate in US military operations that are not sanctioned by the United Nations, such as the Vietnam War or the 2003 invasion of Iraq. Canadian peacekeeping is a distinguishing feature that Canadians feel sets their military foreign policy apart from the United States. \nCanadian anti-Americanism has manifested itself in a variety of ways, ranging from political, to cultural. Not being an \"American\" is a popular theme of Canadian identity. Starting with the American Revolution, when Loyalists were resettled in Canada, a vocal element in Canada has warned against American dominance or annexation. The War of 1812 saw invasions across the border in both directions, but the war ended with unchanged borders. The British ceased aiding Native American attacks on the United States, and the United States never again attempted to invade Canada. Apart from minor unsuccessful raids, it has remained peaceful. As Britain decided to disengage, fears of an American takeover played a role in the Canadian Confederation (1867), and Canada's rejection of free trade (1911).\nHistory.\nColonial wars.\nBefore the British conquest of French Canada in 1760, there had been a series of wars between the British and the French that were fought out in the colonies as well as in Europe and the high seas. In general, the British heavily relied on American colonial militia units, while the French heavily relied on their First Nation allies. The Iroquois Nation were important British allies. Much of the fighting involved ambushes and small-scale warfare in the villages along the border between New England and Quebec. The New England colonies had a much larger population than Quebec, so major invasions came from south to north. The First Nation allies, only loosely controlled by the French, repeatedly raided New England villages to kidnap women and children, and torture and kill the men. Those who survived were brought up as Francophone Catholics. The tension along the border was exacerbated by religion, the French Catholics and English Protestants had a deep mutual distrust. There was a naval dimension as well, involving privateers attacking enemy merchant ships.\nEngland seized Quebec from 1629 to 1632, and Acadia in 1613 and again from 1654 to 1670; These territories were returned to France by the peace treaties. The major wars were (to use American names), King William's War (1689\u20131697); Queen Anne's War (1702\u20131713); King George's War (1744\u20131748), and from 1755 to 1763 the French and Indian War (known in Europe as the Seven Years\u2019 War).\nNew England soldiers and sailors were critical to the successful British campaign to capture the French fortress of Louisbourg in 1745, and (after it had been returned by treaty) to capture it again in 1758.\nAmerican Revolutionary War.\nAt the outset of the American Revolutionary War, the American revolutionaries hoped the French Canadians in Quebec and the Colonists in Nova Scotia would join their rebellion. They were pre-approved for joining the United States in the Articles of Confederation. \nWhen northeastern Quebec was invaded, thousands joined the American cause and formed regiments that fought during the war; however, most remained neutral and some joined the British effort. Britain advised the French Canadians that the British Empire already enshrined their rights in the Quebec Act, which the American colonies had viewed as one of the Intolerable Acts. The American invasion was a fiasco and Britain tightened its grip on its northern possessions; in 1777, a major British invasion into New York led to the surrender of the entire British army at Saratoga and led France to enter the war as an ally of the U.S. The French Canadians largely ignored France's appeals for solidarity.\nThe American forces had much better success in southwestern Quebec, owing to the leadership of Virginia militia leader George Rogers Clark. In 1778, 200 men under Clark, supplied and supported mainly by Virginia, came down the Ohio River near Louisville, Kentucky, marched across southern Illinois, and then captured Kaskaskia without loss of life. From there, part of his men took Vincennes, but was soon lost to British Lieutenant Colonel Henry Hamilton, the commander at Fort Detroit. Clark later retook it in the Siege of Fort Vincennes in February 1779. Roughly half of Clark's militia in the theater were Canadian volunteers sympathetic to the American cause.\nIn the end, America won its independence and the Treaty of Paris compelled Britain to cede parts of southwestern Canada to them. Following America's independence, Canada became a refuge for about an estimated 70,000 or 15% of Loyalists who either wanted to leave the U.S. or were compelled by Patriot reprisals to do so. Among the original Loyalists, there were 3,500 free African Americans. Most went to Nova Scotia and in 1792, 1,200 migrated to Sierra Leone. About 2,000 black slaves were brought in by Loyalist owners; they remained slaves in Canada until the Empire abolished slavery in 1833. Around 85% of the loyalists remained in the new United States and became American citizens.\nWar of 1812 (1812-1815).\nThe Treaty of Paris of 1783, which ended the American Revolutionary War, called for British forces to vacate all their forts south of the Great Lakes border. Britain refused to do so, citing the failure of the newly-independent United States to provide financial restitution for Loyalists who had lost property in the war. The Jay Treaty in 1795 with Great Britain resolved that lingering issue and the British departed the forts. Thomas Jefferson saw the nearby British presence as a threat to the United States, and so he opposed the Jay Treaty, and it became one of the major political issues in the United States at the time. Thousands of Americans immigrated to Upper Canada (Ontario) from 1785 to 1812 to obtain cheaper land and better tax rates prevalent in that province; despite expectations that they would be loyal to the U.S. if a war broke out, in the event they were largely non-political.\nTensions mounted again after 1805, erupting into the War of 1812 (1812-1815), when the United States Congress, approved / signed by the fourth President James Madison (1751-1836, served 1809-1817), declared war in June 1812 on Britain. The Americans were angered by British harassment of U.S. ships on the high seas and seizure of 6,000 sailors from American ships, severe restrictions against neutral American trade with France, and British support for hostile Native American tribes in Ohio and territories the U.S. had gained in 1783. American \"honor\" was an implicit issue. While the Americans could not hope to defeat the Royal Navy and control the seas, they could call on an army much larger than the British garrison in Canada, and so a land invasion of Canada was proposed as the most advantageous means of attacking the British Empire. Americans on the western frontier also hoped an invasion would bring an end to British support of Native American resistance to American expansion, typified by Tecumseh's coalition of tribes. Americans may also have wanted to acquire Canada.\nOnce war broke out, the American strategy was to seize Canada. There was some hope that settlers in western Canada\u2014most of them recent immigrants from the U.S.\u2014would welcome the chance to overthrow their British rulers. However, the American invasions were defeated primarily by British regulars with support from Native Americans and Upper Canada militia. Aided by the large Royal Navy, a series of British raids on the American coast were highly successful, culminating with an attack on Washington that resulted in the British burning of the White House, the Capitol, and other public buildings. At the end of the war, Britain's American Indian allies had largely been defeated, and the Americans controlled a strip of Western Ontario centered on Fort Malden. However, Britain held much of Maine, and, with the support of their remaining American Indian allies, huge areas of the Old Northwest, including Wisconsin and much of Michigan and Illinois. With the surrender of Napoleon in 1814, Britain ended naval policies that angered Americans; with the defeat of the Indian tribes, the threat to American expansion was ended. The upshot was both the United States and Canada asserted their sovereignty, Canada remained under British rule, and London and Washington had nothing more to fight over. The war was ended by the Treaty of Ghent, which took effect in February 1815. A series of postwar agreements further stabilized peaceful relations along the Canada\u2013US border. Canada reduced American immigration for fear of undue American influence and built up the Anglican Church of Canada as a counterweight to the largely American Baptist and Methodist churches.\nIn later years, Anglophone Canadians, especially in Ontario, viewed the War of 1812 as a heroic and successful resistance against invasion and as a victory that defined them as a people. The myth that the Canadian militia had defeated the invasion almost single-handed, known logically as the \"militia myth\", became highly prevalent after the war, having been propounded by John Strachan, Anglican Bishop of York.\nPost War of 1812 and mid-19th century.\nIn the aftermath of the War of 1812, pro-British conservatives led by Anglican Bishop John Strachan took control in Ontario (\"Upper Canada\") and promoted the Anglican religion as opposed to the more republican Methodist and Baptist churches. A small interlocking elite, known as the Family Compact took full political control. Democracy, as practiced in the United States, was ridiculed. The policies had the desired effect of deterring immigration from the United States. Revolts in favor of democracy in Ontario and Quebec (\"Lower Canada\") in 1837 were suppressed; many of the leaders fled to the US. The American policy was to largely ignore the rebellions, and indeed ignore Canada generally in favor of the westward expansion of the American Frontier.\nThe Webster\u2013Ashburton Treaty formalized the U.S.\u2013Canada border in Maine, averting the Aroostook War. During the Manifest Destiny era, the \"Fifty-Four Forty or Fight\" agenda called for U.S. annexation of what became Western Canada; the U.S. and Britain instead agreed to a boundary of the 49th parallel. As harsher fugitive slave laws were passed, Canada became a destination for slaves escaping on the Underground Railroad.\nAmerican Civil War.\nThe British Empire was neutral during the American Civil War. About 40,000 Canadians volunteered for the Union Army\u2014many already lived in the U.S., and a few for the Confederate Army. However, hundreds of Americans who were called up in the draft fled to Canada.\nSeveral events caused strained relations between the British Empire and the United States, over the former's unofficial role in supporting the Confederacy. Blockade runners loaded with arms came from Great Britain and made use of Canadian ports in the Maritimes to break through the Union blockade to deliver the weaponry to the Confederacy in exchange for cotton. Attacks were made on American merchant shipping by British-built Confederate warships such as CSS \"Alabama\". On December 7, 1863, pro-Confederate Canadian sympathizers hijacked an American steamer and killed a crew member off the coast of Cape Cod, Massachusetts then used the steamer, originally intended as a blockade runner, to flee back to the Maritimes where they were later able to escape justice for murder and piracy. Confederate Secret Service agents also used Canada as a base to attack American border towns, such as St. Albans, Vermont on October 19, 1864, where they killed an American citizen, robbed three banks of over US$200,000, then escaped to Canada where they were arrested but then released by a Canadian court to widespread American anger. Many Americans falsely suspected that the Canadian government knew of the raid ahead of time. American Secretary of State William H. Seward let the British government know that \"it is impossible to consider those proceedings as either legal, just or friendly towards the United States.\"\nAlabama claims.\nAmericans were angry at Britain's perceived support for the Confederacy during the American Civil War. Some leaders demanded a huge payment, on the premise that British involvement had lengthened the war by two years, a claim confirmed by post-Civil War historians and scholars. Senator Charles Sumner, the chairman of the Senate Foreign Relations Committee, originally wanted to ask for $2 billion in war reparations, or alternatively the ceding of all of Canada to the United States.\nWhen American Secretary of State William H. Seward negotiated the Alaska Purchase with Russia in 1867, he intended it as the first step in a comprehensive plan to gain control of the entire northwest Pacific Coast. Seward was a firm believer in Manifest Destiny, primarily for its commercial advantages to the U.S. Seward expected British Columbia to seek annexation to the U.S. and thought Britain might accept this in exchange for the \"Alabama\" claims. Soon other elements endorsed annexation, they planned to annex British Columbia, Red River Colony (Manitoba), and Nova Scotia, in exchange for dropping the damage claims. The idea peaked in the spring and summer of 1870, with American expansionists, Canadian separatists, and pro-American Englishmen seemingly combining forces. The plan was dropped for multiple reasons. London continued to stall, American commercial and financial groups pressed Washington for a quick settlement of the dispute on a cash basis, growing Canadian nationalist sentiment in British Columbia called for staying inside the British Empire, Congress became preoccupied with Reconstruction, and most Americans showed little interest in territorial expansion.\nThe \"Alabama Claims\" dispute went to international arbitration. In one of the first major cases of arbitration, the tribunal in 1872 rejected the American claims for damages relating to the British blockade running but ordered Britain to pay $15.5 million only for damages caused by British-built Confederate ships. Britain paid and the episode ended in peaceful relations.\nLate 19th century.\nCanada became a self-governing dominion in 1867 in internal affairs while Britain retained control of diplomacy and defence policy. Before Confederation, there was an Oregon boundary dispute in which the Americans claimed the 54th degree latitude. The Oregon Treaty of 1846 largely resolved the issue, splitting the disputed territory \u2013 the northern half became British Columbia, and the southern half eventually formed the states of Washington and Oregon. \nStrained relations with America continued, however, due to a series of small-scale armed incursions called the \"Fenian raids\" conducted by Irish-American Civil War veterans across the border from 1866 to 1871 in an attempt to trade Canada for Irish independence. The American government, angry at Canadian tolerance of Confederate raiders during the American Civil War of 1861 to 1865, moved very slowly to disarm the Fenians. The Fenian raids were small-scale attacks carried out by the Fenian Brotherhood, an Irish Republican organization based among Irish Catholics in the United States. Targets included British Army forts, customs posts, and other locations near the border. The raids were small, unsuccessful episodes in 1866, and again from 1870 to 1871. They aimed to bring pressure on Great Britain to withdraw from Ireland. None of these raids achieved their aims and all were quickly defeated by local Canadian forces.\nThe British government, in charge of diplomatic relations, protested cautiously, as Anglo-American relations were tense. Much of the tension was relieved as the Fenians faded away and in 1872 by the settlement of the Alabama Claims, when Britain paid the U.S. $15.5 million for war losses caused by warships built in Britain and sold to the Confederacy.\nDisputes over ocean boundaries on Georges Bank and fishing, whaling, and sealing rights in the Pacific were settled by international arbitration, setting an important precedent.\nEarly 20th century.\nAlaska boundary.\nA short-lived controversy was the Alaska boundary dispute, settled in favor of the United States in 1903. The issue was unimportant until the Klondike Gold Rush brought tens of thousands of men to Canada's Yukon, and they had to arrive through American ports. Canada needed its port and claimed that it had a legal right to a port near the present American town of Haines, Alaska. It would provide an all-Canadian route to the rich goldfields. The dispute was settled by arbitration, and the British delegate voted with the Americans\u2014to the astonishment and disgust of Canadians who suddenly realized that Britain considered its relations with the United States paramount compared to those with Canada. The arbitration validated the status quo, but made Canada angry at London.\n1907 saw a minor controversy over USS \"Nashville\" sailing into the Great Lakes via Canada without Canadian permission. To head off future embarrassments, in 1909 the two sides signed the International Boundary Waters Treaty, and the International Joint Commission was established to manage the Great Lakes and keep them disarmed. It was amended in World War II to allow the building and training of warships.\nFree trade rejected.\nAnti-Americanism reached a shrill peak in 1911 in Canada. The Liberal government in 1911 negotiated a Reciprocity treaty with the U.S. that would lower trade barriers. Canadian manufacturing interests were alarmed that free trade would allow the bigger and more efficient American factories to take their markets. The Conservatives made it a central campaign issue in the 1911 election, warning that it would be a \"sell-out\" to the United States with economic annexation a special danger. The Conservative slogan was \"No truck or trade with the Yankees\", as they appealed to Canadian nationalism and nostalgia for the British Empire to win a major victory.\nWorld War I.\nBritish Canadians were annoyed during a brief period from 1914 to 1916, when the United States insisted on neutrality and seemed to profit heavily, while Canada was sacrificing its wealth and its youth. However, when the US finally declared war on Germany in April 1917, there was swift cooperation and friendly coordination, as one historian reports: Official co-operation between Canada and the United States\u2014the pooling of grain, fuel, power, and transportation resources, the underwriting of a Canadian loan by bankers of New York\u2014produced a good effect on the public mind. Canadian recruiting detachments were welcomed in the United States, while a reciprocal agreement was ratified to facilitate the return of draft evaders. A Canadian War Mission was established at Washington, and in many other ways, the activities of the two countries were coordinated for efficiency. Immigration regulations were relaxed and thousands of American farmhands crossed the border to assist in harvesting Canadian crops. Officially and publicly, at least, the two nations were on better terms than ever before in their history, and on the American side, this attitude extended through almost all classes of society.\nPost-World War I.\nCanada demanded and received permission from London to send its delegation to the Versailles Peace Talks in 1919, with the proviso that it sign the treaty under the British Empire. Throughout the 1920s, Canada began assuming greater responsibility for its own foreign and military affairs. In 1927, the U.S. and Canada exchanged ambassadors for the first time with Canada appointing Vincent Massey and America William Phillips respectively. The postwar era saw the United States pursue isolationism while Canada became an active member of the British Commonwealth, the League of Nations, and the World Court.\nIn July 1923, as part of his Pacific Northwest tour and a week before his death, U.S. President Warren Harding visited Vancouver, making him the first American head of state to visit confederated Canada. The then Premier of British Columbia, John Oliver, and then mayor of Vancouver, Charles Tisdall, hosted a lunch in his honor at the Hotel Vancouver. Over 50,000 people heard Harding speak in Stanley Park. A monument to Harding designed by Charles Marega was unveiled in Stanley Park in 1925.\nRelations with the United States remained cordial until 1930 when Canada vehemently protested the new Smoot\u2013Hawley Tariff Act by which the U.S. raised tariffs on products imported from Canada. Canada retaliated with higher tariffs of its own against American products and moved toward more trade within the British Commonwealth. U.S.\u2013Canadian trade fell 75% as the Great Depression dragged both countries down.\nDuring the 1920s, the war and naval departments of both nations designed war game scenarios with the other as an enemy as part of routine training exercises. In 1921, Canada developed Defence Scheme No. 1 for an attack on American cities and for forestalling an invasion by the United States until British reinforcements could arrive. Throughout the later 1920s and 1930s, the United States Army War College developed a plan for a war with the British Empire waged largely on North American territory: War Plan Red.\nHerbert Hoover's meeting in 1927 with British Ambassador Sir Esme Howard agreed on the \"absurdity of contemplating the possibility of war between the United States and the British Empire\".\nIn 1938, as the roots of World War II were set in motion, U.S. President Franklin Roosevelt gave a public speech at Queen's University in Kingston, Ontario, declaring that the United States would not sit idly by if another power tried to dominate Canada. Diplomats saw it as a clear warning to Germany not to attack Canada.\nWorld War II.\nThe two nations cooperated closely in World War II, as both nations saw new levels of prosperity and a determination to defeat the Axis powers. Prime Minister William Lyon Mackenzie King and President Franklin D. Roosevelt were determined not to repeat the mistakes of their predecessors. They met in August 1940 at Ogdensburg, issuing a declaration calling for close cooperation, and formed the Permanent Joint Board on Defense (PJBD).\nKing sought to raise Canada's international visibility by hosting the August 1943 Quadrant conference in Quebec on military and political strategy; he was a gracious host but was kept out of the important meetings by Winston Churchill and Roosevelt.\nCanada allowed the construction of the Alaska Highway and participated in the building of the atomic bomb. 49,000 Americans joined the RCAF (Canadian) or RAF (British) air forces through the Clayton Knight Committee, which had Roosevelt's permission to recruit in the U.S. in 1940\u201342.\nAmerican attempts in the mid-1930s to integrate British Columbia into a united West Coast military command had aroused Canadian opposition. Fearing a Japanese invasion of Canada's vulnerable British Columbia Coast, American officials urged the creation of a united military command for an eastern Pacific Ocean theater of war. Canadian leaders feared American imperialism and the loss of autonomy more than a Japanese invasion. In 1941, Canadians successfully argued within the PJBD for cooperation rather than the unified command for the West Coast.\nNewfoundland.\nThe United States built large military bases in Newfoundland during World War II. At the time it was a British crown colony, having lost dominion status. The American spending ended the depression and brought new prosperity; Newfoundland's business community sought closer ties with the United States as expressed by the Economic Union Party. Ottawa took notice and wanted Newfoundland to join Canada, which it did after hotly contested referendums. There was little demand in the United States for the acquisition of Newfoundland, so the United States did not protest the British decision not to allow an American option on the Newfoundland referendum.\nCold War.\nPrime Minister William Lyon Mackenzie King, working closely with his Foreign Minister Louis St. Laurent, handled foreign relations 1945\u201348 cautiously. Canada donated money to the United Kingdom to help it rebuild; was elected to the UN Security Council; and helped design NATO. However, Mackenzie King rejected free trade with the United States, and decided not to play a role in the Berlin airlift. Canada had been actively involved in the League of Nations, primarily because it could act separately from Britain. It played a modest role in the postwar formation of the United Nations, as well as the International Monetary Fund. It played a somewhat larger role in 1947 in designing the General Agreement on Tariffs and Trade. \nAfter the mid-20th century onwards, Canada and the United States became extremely close partners. Canada was a close ally of the United States during the Cold War.\nVietnam War resisters.\nWhile Canada openly accepted draft evaders and later deserters from the United States, there was never a serious international dispute due to Canada's actions, while Sweden's acceptance was heavily criticized by the United States. The issue of accepting American exiles became a local political debate in Canada that focused on Canada's sovereignty in its immigration law. The United States did not become involved because American politicians viewed Canada as a geographically close ally not worth disturbing.\nNixon Shock 1971.\nThe United States had become Canada's largest market, and after the war, the Canadian economy became dependent on smooth trade flows with the United States so much that in 1971 when the United States enacted the \"Nixon Shock\" economic policies (including a 10% tariff on all imports) it put the Canadian government into a panic. Washington refused to exempt Canada from its 1971 New Economic Policy, so Trudeau saw a solution in closer economic ties with Europe. Trudeau proposed a \"Third Option\" policy of diversifying Canada's trade and downgrading the importance of the American market. In a 1972 speech in Ottawa, Nixon declared the \"special relationship\" between Canada and the United States dead.\nRelations deteriorated on many points in the Nixon years (1969\u201374), including trade disputes, defense agreements, energy, fishing, the environment, cultural imperialism, and foreign policy. They changed for the better when Trudeau and Carter found a better rapport. The late 1970s saw a more sympathetic American attitude toward Canadian political and economic needs, the pardoning of draft evaders who had moved to Canada, and the passing of old such as the Watergate scandal and the Vietnam War. Canada more than ever welcomed American investments during \"the stagflation\" that hurt both nations.\n1990s.\nThe main issues in Canada\u2013US relations in the 1990s focused on the North American Free Trade Agreement, which was signed in 1994. It created a common market that by 2014 was worth $19 trillion, encompassed 470 million people, and had created millions of jobs. Wilson says, \"Few dispute that NAFTA has produced large and measurable gains for Canadian consumers, workers, and businesses\". However, he adds, \"NAFTA has fallen well short of expectations.\"\nMigration history.\nFrom the 1750s to the 21st century, there has been an extensive mingling of the Canadian and American populations, with large movements in both directions.\nNew England Yankee settled large parts of Nova Scotia before 1775 and were neutral during the American Revolution. At the end of the American Revolution, about 75,000 United Empire Loyalists moved out of the new United States to the eastern Atlantic provinces and south of Quebec. From 1790 to 1812 many farmers moved from New York and New England into Upper Canada (mostly to Niagara, and the north shore of Lake Ontario). In the mid and late 19th century gold rushes attracted American prospectors, mostly to British Columbia after the Cariboo Gold Rush, Fraser Canyon Gold Rush, and later to the Yukon Territory. In the early 20th century, the opening of land blocks in the Prairie Provinces attracted many farmers from the American Midwest. Many Mennonites immigrated from Pennsylvania and formed their colonies. In the 1890s some Mormons went north to form communities in Alberta after the Church of Jesus Christ of Latter-day Saints rejected plural marriage. The 1960s saw the arrival of about 50,000 draft-dodgers who opposed the Vietnam War.\nCanada was a way station through which immigrants from other lands stopped for a while, ultimately heading to the U.S. Between 1851 and 1951, 7.1 million people arrived in Canada (mostly from Continental Europe), and 6.6 million left Canada, most of them to the U.S. After 1850, the pace of industrialization and urbanization was much faster in the United States, drawing a wide range of immigrants from the North. By 1870, 1/6 of all the people born in Canada had moved to the United States, with the highest concentrations in New England, which was the destination of Francophone emigrants from Quebec and Anglophone emigrants from the Maritimes. It was common for people to move back and forth across the border, such as seasonal lumberjacks, entrepreneurs looking for larger markets, and families looking for jobs in the textile mills that paid much higher wages than in Canada.\nThe southward migration slacked off after 1890, as Canadian industry began a growth spurt. By then, the American frontier was closing, and thousands of farmers looking for fresh land moved from the United States north into the Prairie Provinces. The net result of the flows was that in 1901 there were 128,000 American-born residents in Canada (3.5% of the Canadian population) and 1.18 million Canadian-born residents in the United States (1.6% of the U.S. population).\nIn the late 19th and early 20th centuries, about 900,000 French Canadians moved to the U.S., with 395,000 residents there in 1900. Two-thirds went to mill towns in New England, where they formed distinctive ethnic communities. By the late 20th century, most had abandoned the French language (see New England French), but most kept the Catholic religion. About twice as many English Canadians came to the U.S., but they did not form distinctive ethnic settlements.\nRelations between political executives.\nThe executive of each country is represented differently. The President of the United States serves as both the head of state and head of government, and his \"administration\" is the executive, while the Prime Minister of Canada is head of government only, and his or her \"government\" or \"ministry\" directs the executive.\nW. L. Mackenzie King and Franklin D. Roosevelt (October 1935 \u2013 April 1945).\nIn 1940, W.L. Mackenzie King and Franklin D. Roosevelt signed a defense pact, known as the Ogdensburg Agreement. King hosted conferences for Churchill and Roosevelt, but did not participate in the talks.\nLouis St. Laurent and Harry S. Truman (November 1948 \u2013 January 1953).\nPrime Minister Laurent and President Truman were both anti-communist during the early years of the Cold War.\nJohn G. Diefenbaker and Dwight Eisenhower (June 1957 \u2013 January 1961).\nPresident Dwight Eisenhower (1952\u20131961) took pains to foster good relations with Progressive Conservative John Diefenbaker (1957\u20131963). That led to the approval of plans to join in NORAD, an integrated air defense system, in mid-1957. Relations with President John Kennedy were much less cordial. Diefenbaker opposed apartheid in the South Africa and helped force it out of the Commonwealth of Nations. His indecision on whether to accept Bomarc nuclear missiles from the United States led to his government's downfall.\nJohn G. Diefenbaker and John F. Kennedy (January 1961 \u2013 April 1963).\nDiefenbaker and President John F. Kennedy did not get along well personally. This was evident in Diefenbaker's response to the Cuban Missile Crisis, where he was slow to support the United States. However, Diefenbaker's Minister of Defence went behind Diefenbaker's back and sent Canada's military to high alert given Canada's legal treaty obligations, to try and appease Kennedy.\nLester B. Pearson and Lyndon B. Johnson (November 1963 \u2013 April 1968).\nIn 1965, Prime Minister Lester B. Pearson gave a speech in Philadelphia criticizing American involvement in the Vietnam War. This infuriated Lyndon B. Johnson, who gave him a harsh talk, saying \"You don't come here and piss on my rug\".\nBrian Mulroney and Ronald Reagan (September 1984 \u2013 January 1989).\nRelations between Brian Mulroney and Ronald Reagan were famously close. This relationship resulted in negotiations for the Canada\u2013United States Free Trade Agreement, and the U.S.\u2013Canada Air Quality Agreement to reduce acid-rain-causing emissions, both major policy goals of Mulroney, that would be finalized under the presidency of George H. W. Bush. Mulroney delivered eulogies at the funerals of both Ronald Reagan and George H. W. Bush.\nJean Chr\u00e9tien and Bill Clinton (November 1993 \u2013 January 2001).\nAlthough Jean Chr\u00e9tien was wary of appearing too close to President Bill Clinton, both men had a passion for golf. During a news conference with Prime Minister Chr\u00e9tien in April 1997, President Clinton quipped \"I don't know if any two world leaders have played golf together more than we have, but we meant to break a record\". Their governments had many small trade quarrels over the Canadian content of American magazines, softwood lumber, and so on, but on the whole were quite friendly. Both leaders had run on reforming or abolishing NAFTA, but the agreement went ahead with the addition of environmental and labor side agreements. Crucially, the Clinton administration lent rhetorical support to Canadian unity during the 1995 referendum in Quebec on separation from Canada.\nJean Chr\u00e9tien and George W. Bush (January 2001 \u2013 December 2003).\nRelations between Chr\u00e9tien and George W. Bush were strained throughout their overlapping times in office. Canada offered its full assistance to the U.S. as the September 11 attacks were unfolding. One tangible show of support was Operation Yellow Ribbon, in which more than 200 U.S.-bound flights were diverted to Canada after the U.S. shut down their airspace. Later, however, Chr\u00e9tien publicly mused that U.S. foreign policy might be part of the \"root causes\" of terrorism. Some Americans criticized his \"smug moralism\", and Chr\u00e9tien's public refusal to support the 2003 Iraq war was met with negative responses in the United States, especially among conservatives.\nStephen Harper and George W. Bush (February 2006 \u2013 January 2009).\nStephen Harper and George W. Bush were thought to share warm personal relations and also close ties between their administrations. Because Bush was so unpopular among liberals in Canada (particularly in the media), this was underplayed by the Harper government.\nShortly after being congratulated by Bush for his victory in February 2006, Harper rebuked the U.S. ambassador to Canada David Wilkins for criticizing the Conservatives' plans to assert Canada's sovereignty over the Arctic Ocean waters with military force.\nStephen Harper and Barack Obama (January 2009 \u2013 November 2015).\nPresident Barack Obama's first international trip was to Canada on February 19, 2009, thereby sending a strong message of peace and cooperation. Except Canadian lobbying against \"Buy American\" provisions in the U.S. stimulus package, relations between the two administrations were smooth.\nThey also held friendly bets on hockey games during the Winter Olympic season. In the 2010 Winter Olympics hosted by Canada in Vancouver, Canada defeated the U.S. in both gold medal matches, entitling Stephen Harper to receive a case of Molson Canadian beer from Barack Obama; in reverse, if Canada had lost, Harper would have provided a case of Yuengling beer to Obama. During the 2014 Winter Olympics, alongside U.S. Secretary of State John Kerry &amp; Minister of Foreign Affairs John Baird, Stephen Harper was given a case of Samuel Adams beer by Obama for the Canadian gold medal victory over the U.S. in women's hockey, and the semi-final victory over the U.S. in men's hockey.\nCanada\u2013United States Regulatory Cooperation Council (RCC) (2011).\nOn February 4, 2011, Harper and Obama issued a \"Declaration on a Shared Vision for Perimeter Security and Economic Competitiveness\" and announced the creation of the Canada\u2013United States Regulatory Cooperation Council (RCC) \"to increase regulatory transparency and coordination between the two countries.\"\nHealth Canada and the United States Food and Drug Administration (FDA) under the RCC mandate, undertook the \"first of its kind\" initiative by selecting \"as its first area of alignment common cold indications for certain over-the-counter antihistamine ingredients (GC January 10, 2013)\".\nOn December 7, 2011, Harper flew to Washington, met with Obama, and signed an agreement to implement the joint action plans that had been developed since the initial meeting in February. The plans called on both countries to spend more on border infrastructure, share more information on people who cross the border, and acknowledge more of each other's safety and security inspection on third-country traffic. An editorial in \"The Globe and Mail\" praised the agreement for giving Canada the ability to track whether failed refugee claimants have left Canada via the U.S. and for eliminating \"duplicated baggage screenings on connecting flights\". The agreement is not a legally binding treaty and relies on the political will and ability of the executives of both governments to implement the terms of the agreement. These types of executive agreements are routine\u2014on both sides of the Canada\u2013U.S. border.\nJustin Trudeau and Barack Obama (November 2015 \u2013 January 2017).\nPresident Barack Obama and Prime Minister Justin Trudeau first met formally at the APEC summit meeting in Manila, Philippines in November 2015, nearly a week after the latter was sworn into the office. Both leaders expressed eagerness for increased cooperation and coordination between the two countries during Trudeau's government with Trudeau promising an \"enhanced Canada\u2013U.S. partnership\".\nOn November 6, 2015, Obama announced the U.S. State Department's rejection of the proposed Keystone XL pipeline, the fourth phase of the Keystone oil pipeline system running between Canada and the United States, to which Trudeau expressed disappointment but said that the rejection would not damage Canada\u2013U.S. relations and would instead provide a \"fresh start\" to strengthening ties through cooperation and coordination, saying that \"Canada\u2013U.S. relationship is much bigger than any one project.\" Obama has since praised Trudeau's efforts to prioritize the reduction of climate change, calling it \"extraordinarily helpful\" to establish a worldwide consensus on addressing the issue.\nAlthough Trudeau has told Obama his plans to withdraw Canada's McDonnell Douglas CF-18 Hornet jets assisting in the American-led intervention against ISIL, Trudeau said that Canada will still \"do more than its part\" in combating the terrorist group by increasing the number of Canadian special forces members training and fighting on the ground in Iraq and Syria.\nTrudeau visited the White House for an official visit and state dinner on March 10, 2016. Trudeau and Obama were reported to have shared warm personal relations during the visit, making humorous remarks about which country was better at hockey and which country had better beer. Obama complimented Trudeau's 2015 election campaign for its \"message of hope and change\" and \"positive and optimistic vision\". Obama and Trudeau also held \"productive\" discussions on climate change and relations between the two countries, and Trudeau invited Obama to speak in the Canadian parliament in Ottawa later in the year.\nJustin Trudeau and Donald Trump (January 2017 \u2013 January 2021).\nFollowing the victory of Donald Trump in the 2016 U.S. presidential election, Trudeau congratulated him and invited him to visit Canada at the \"earliest opportunity\". Prime Minister Trudeau and President Trump formally met for the first time at the White House on February 13, 2017, nearly a month after Trump was sworn into the office. Trump has ruffled relations with Canada with tariffs on softwood lumber. Diafiltered Milk was brought up by Trump as an area that needed negotiating.\nIn 2018, Trump and Trudeau negotiated the United States\u2013Mexico\u2013Canada Agreement (USMCA), a free trade agreement concluded between Canada, Mexico, and the United States that succeeded the North American Free Trade Agreement (NAFTA). The agreement has been characterized as \"NAFTA 2.0\", or \"New NAFTA\", since many provisions from NAFTA were incorporated and its changes were seen as largely incremental. On July 1, 2020, the USMCA entered into force in all member states.\nIn June 2018, after Trudeau explained that Canadians would not be \"pushed around\" by the first Trump tariffs on Canada's aluminum and steel, Trump labeled Trudeau as \"dishonest\" and \"meek\", and accused Trudeau of making \"false statements\", although it is unclear which statements Trump was referring to. Trump's adviser on trade, Peter Navarro, said that there was a \"special place in hell\" for Trudeau as he employed \"bad faith diplomacy with President Donald J. Trump and then tried to stab him in the back on the way out the door ... that comes right from Air Force One.\" Days later, Trump said that Trudeau's comments are \"going to cost a lot of money for the people of Canada\".\nIn June 2019, the U.S. State Department spokesperson Morgan Ortagus said the U.S. \"view Canada's claim that the waters of the Northwest Passage are internal waters of Canada as inconsistent with international law\".\nJustin Trudeau and Joe Biden (January 2021 \u2013 January 2025).\nFollowing the victory of Joe Biden in the 2020 U.S. presidential election, Trudeau congratulated him on his victory; indicating a significant improvement in Canada\u2013U.S. relationships, which had been strained in the years prior during the Presidency of Donald Trump.\nOn January 22, 2021, Biden and Trudeau held their first phone call. Trudeau was the first foreign leader to receive a phone call from Biden as President.\nOn February 23, 2021, Biden and Trudeau held their first bilateral meeting. Although virtual, the bilateral meeting was Biden's first as President. The two leaders discussed \"COVID-19, economic recovery, climate change, and refugees and migration\" among other subjects.\nJustin Trudeau and Donald Trump (January 2025 \u2013 March/April 2025).\nDuring his 2024 campaign, Trump repeatedly talked about imposing tariffs and made controversial remarks about making Canada the 51st state. On November 29, 2024, Trudeau met with Trump to address trade issues after Trump threatened a 25% tariff on Canadian imports and planned to rethink the USMCA. Trudeau warned of retaliation if tariffs were enacted. Trump continued his comments throughout December, calling Canada a state and Trudeau a governor. On December 18, he claimed many Canadians supported the idea of becoming the 51st state. Trudeau firmly rejected any possibility of annexation on January 7.\nMilitary and security.\nThe Canadian military, like forces of other NATO countries, fought in cooperation with the United States in most major conflicts since World War II, including the Korean War, the Gulf War, the Kosovo War, and most recently the war in Afghanistan. The main exceptions to this were the Canadian government's opposition to some CIA activities in Canada, the Vietnam War, and the Iraq War, which caused some brief diplomatic tensions. Despite these issues, military relations have remained close.\nAmerican defense arrangements with Canada are more extensive than with any other country. The Permanent Joint Board of Defense, established in 1940, provides policy-level consultation on bilateral defense matters. The United States and Canada share North Atlantic Treaty Organization (NATO) mutual security commitments. In addition, American and Canadian military forces have cooperated since 1958 on continental air defense within the framework of the North American Aerospace Defense Command (NORAD). Canadian forces have provided indirect support for the American invasion of Iraq that began in 2003. Moreover, interoperability with the American armed forces has been a guiding principle of Canadian military force structuring and doctrine since the end of the Cold War. Canadian navy frigates, for instance, integrate seamlessly into American carrier battle groups.\nIn commemoration of the 200th Anniversary of the War of 1812 ambassadors from Canada and the United States, and naval officers from both countries gathered at the Pritzker Military Library on August 17, 2012, for a panel discussion on Canada-U.S. relations with emphasis on national security-related matters. Also as part of the commemoration, the navies of both countries sailed together throughout the Great Lakes region.\nAccording to Canadian and U.S. officials, a U.S. fighter jet shot down an unidentified object over Canada on February 23, 2023, on the orders of Prime Minister Justin Trudeau. The operation was coordinated by the North American Aerospace Defense Command (NORAD), a joint U.S.-Canadian air defense organization. Prime Minister Trudeau said investigators were looking for debris. This decision was made following the conversation between Biden and Trudeau.\nThe foreign policies of the countries have been closely aligned, yet ultimately independent, since the Cold War. There is also debate on whether the Northwest Passage is in international waters or under Canadian sovereignty.\nIran hostage crisis.\nDuring the 1979 revolution, protesters invaded the U.S. embassy and took many hostages. Six Americans evaded capture and were sheltered by the British and Canadian diplomatic missions. After a U.S. military operation to get them out of Iran failed, Canadian diplomat Ken Taylor, Secretary of State for External Affairs Flora MacDonald, and Prime Minister Joe Clark decided to smuggle the six Americans out of Iran on an international flight by using Canadian passports. An Order in Council was made to issue multiple official copies of Canadian passports with fake identities to the American diplomats in the Canadian sanctuary. The passports contained forged Iranian visas prepared by the U.S. Central Intelligence Agency.\nWar in Afghanistan.\nCanada's elite JTF2 unit joined American special forces in Afghanistan shortly after the al-Qaeda attacks on September 11, 2001. Canadian forces joined the multinational coalition in Operation Anaconda in January 2002. On April 18, 2002, an American pilot bombed Canadian forces involved in a training exercise, killing four and wounding eight Canadians. A joint American-Canadian inquiry determined the cause of the incident to be pilot error, in which the pilot interpreted ground fire as an attack; the pilot ignored orders that he felt were \"second-guessing\" his field tactical decision. Canadian forces assumed a six-month command rotation of the International Security Assistance Force in 2003; in 2005, Canadians assumed operational command of the multi-national Brigade in Kandahar, with 2,300 troops, and supervises the Provincial Reconstruction Team in Kandahar, where al-Qaida forces are most active. Canada has also deployed naval forces in the Persian Gulf since 1991 in support of the UN Gulf Multinational Interdiction Force.\nThe Canadian Embassy in Washington, D.C. maintains a public relations website named CanadianAlly.com, which is intended \"to give American citizens a better sense of the scope of Canada's role in North American and Global Security and the War on Terror\".\nThe New Democratic Party and some recent Liberal leadership candidates have expressed opposition to Canada's expanded role in the Afghan conflict because it is inconsistent with Canada's historic role (since the Second World War) of peacekeeping operations.\n2003 Invasion of Iraq.\nAccording to contemporary polls, 71% of Canadians were opposed to the 2003 invasion of Iraq. Many Canadians, and the former Liberal Cabinet headed by Paul Martin (as well as many Americans such as Bill Clinton and Barack Obama), made a policy distinction between conflicts in Afghanistan and Iraq, unlike the Bush Doctrine, which linked these together in a \"Global war on terror\".\nResponding to ISIS/Daesh.\nCanada has been involved in international responses to the threats from Daesh/ISIS/ISIL in Syria and Iraq and is a member of the Global Coalition to Counter Daesh. In October 2016, Foreign Affairs Minister Dion and National Defence Minister Sajjan met the U.S. special envoy for this coalition. The Americans thanked Canada \"for the role of Canadian Armed Forces (CAF) in providing training and assistance to Iraqi security forces, as well as the CAF's role in improving essential capacity-building capabilities with regional forces\".\nIllicit drugs.\nIn 2003, the American government became concerned when members of the Canadian government announced plans to decriminalize the use of cannabis. David Murray, an assistant to the U.S. Drug Czar John P. Walters, said in a CBC interview that, \"We would have to respond. We would be forced to respond.\" However, the election of the Conservative Party in early 2006 halted the liberalization of cannabis laws until the Liberal Party of Canada legalized recreational cannabis use in 2018.\nA 2007 joint report by American and Canadian officials on cross-border drug smuggling indicated that, despite their best efforts, \"drug trafficking still occurs in significant quantities in both directions across the border. The principal illicit substances smuggled across our shared border are MDMA (\"Ecstasy\"), cocaine, and cannabis\" The report indicated that Canada was a major producer of \"Ecstasy\" and marijuana for the U.S. market, while the U.S. was a transit country for cocaine entering Canada.\nTrade.\nCanada and the United States have the world's second-largest trading relationship, with huge quantities of goods and people flowing across the border each year. Since the 1987 Canada\u2013United States Free Trade Agreement, there have been no tariffs on most goods passed between the two countries.\nIn the course of the softwood lumber dispute, the U.S. has placed tariffs on Canadian softwood lumber because of what it argues is an unfair Canadian government subsidy, a claim that Canada disputes. The dispute has cycled through several agreements and arbitration cases. Other notable disputes include the Canadian Wheat Board, and Canadian cultural protectionism in cultural industries such as magazines, radio, and television. Canadians have been criticized about such things as the ban on beef since a case of Mad Cow disease was discovered in 2003 in cows from the United States (and a few subsequent cases) and the high American agricultural subsidies. Concerns in Canada also run high over aspects of the North American Free Trade Agreement (NAFTA) such as Chapter 11, prior to its suspension and replacement with USMCA.\nCultural relations.\nSports.\nSeveral major sports that are popular in the United States have origins in or influences from Canada, such as basketball, which was invented by Canadian-American James Naismith.\nEnvironmental issues.\nA principal instrument of this cooperation is the International Joint Commission (IJC), established as part of the Boundary Waters Treaty of 1909 to resolve differences and promote international cooperation on boundary waters. The Great Lakes Water Quality Agreement of 1972 is another historic example of cooperation in controlling trans-border water pollution. However, there have been some disputes. Most recently, the Devil's Lake Outlet, a project instituted by North Dakota, has angered Manitobans who fear that their water may soon become polluted as a result of this project.\nBeginning in 1986, the Canadian government of Brian Mulroney began pressing the Reagan administration for an \"Acid Rain Treaty\" to do something about U.S. industrial air pollution causing acid rain in Canada. The Reagan administration was hesitant and questioned the science behind Mulroney's claims. However, Mulroney was able to prevail. The product was the signing and ratification of the Air Quality Agreement of 1991 by the first Bush administration. Under that treaty, the two governments consult semi-annually on trans-border air pollution, which has demonstrably reduced acid rain, and they have since signed an annex to the treaty dealing with ground level ozone in 2000. Despite this, trans-border air pollution remains an issue, particularly in the Great Lakes-St. Lawrence watershed during the summer. The main source of this trans-border pollution results from coal-fired power stations, most of them located in the Midwestern United States. As part of the negotiations to create NAFTA, Canada and the U.S. signed, along with Mexico, the North American Agreement on Environmental Cooperation that created the Commission for Environmental Cooperation that monitors environmental issues across the continent, publishing the North American Environmental Atlas as one aspect of its monitoring duties.\nCurrently, neither of the countries' governments support the Kyoto Protocol, which set out time scheduled curbing of greenhouse gas emissions. Unlike the United States, Canada has ratified the agreement. Yet after ratification, due to internal political conflict within Canada, the Canadian government does not enforce the Kyoto Protocol and has received criticism from environmental groups and other governments for its climate change positions. In January 2011, the Canadian minister of the environment, Peter Kent, explicitly stated that the policy of his government about greenhouse gas emissions reductions is to wait for the United States to act first, and then try to harmonize with that action \u2013 a position that has been condemned by environmentalists and Canadian nationalists, and as well as scientists and government think-tanks.\nWith large freshwater supplies in Canada and long-term concern about water scarcity in parts of the United States, water export availability or restriction has been identified as an issue of possible future contention between the countries.\nNewfoundland fisheries dispute.\nThe United States and Britain had a long-standing dispute about the rights of Americans fishing in the waters near Newfoundland. Before 1776, there was no question that American fishermen, mostly from Massachusetts, had rights to use the waters off Newfoundland. In the peace treaty negotiations of 1783, the Americans insisted on a statement of these rights. However, France, an American ally, disputed the American position because France had its own specified rights in the area and wanted them to be exclusive. The Treaty of Paris (1783) gave the Americans not rights, but rather \"liberties\" to fish within the territorial waters of British North America and to dry fish on certain coasts.\nAfter the War of 1812, the Convention of 1818 between the United States and Britain specified exactly what liberties were involved. Canadian and Newfoundland fishermen contested these liberties in the 1830s and 1840s. The Canadian\u2013American Reciprocity Treaty of 1854, and the Treaty of Washington of 1871 spelled out the liberties in more detail. However the Treaty of Washington expired in 1885, and there was a continuous round of disputes over jurisdictions and liberties. Britain and the United States sent the issue to the Permanent Court of Arbitration in The Hague in 1909. It produced a compromise settlement that permanently ended the problems.\nCommon memberships.\nCanada and the United States both hold membership in several multinational organizations, including:\nTerritorial disputes.\nThe two countries have had several territorial disputes throughout their histories. Current maritime territorial disputes between the two countries include the Beaufort Sea, Dixon Entrance, Strait of Juan de Fuca, San Juan Islands, Machias Seal Island, and North Rock. Additionally, the United States is one of several countries that contends the Northwest Passage is international waters; whereas the Canadian government asserts it forms Canadian Internal Waters. The Inside Passage is also disputed as international waters by the United States.\nHistorical boundary disputes include the Aroostook War at the Maine\u2013New Brunswick border; the Oregon boundary dispute at the present day British Columbia\u2013Washington border; and the Alaska Boundary Dispute at the Alaska\u2013British Columbia border. The Maine\u2013New Brunswick boundary dispute was resolved through the Webster\u2013Ashburton Treaty in 1842, the Oregon boundary dispute through the Oregon Treaty of 1846, and the Alaska boundary dispute through arbitration in 1903.\nNorthwest Passage.\nA long-simmering dispute between Canada and the U.S. involves the issue of Canadian sovereignty over the Northwest Passage (the sea passages in the Arctic). Canada's assertion that the Northwest Passage represents internal (territorial) waters has been challenged by other countries, especially the U.S., which argue that these waters constitute an international strait. Canadians were alarmed when Americans drove the reinforced oil tanker through the Northwest Passage in 1969, followed by the icebreaker Polar Sea in 1985, which resulted in a minor diplomatic incident. In 1970, the Canadian parliament enacted the Arctic Waters Pollution Prevention Act, which asserts Canadian regulatory control over pollution within a 100-mile zone. In response, the United States in 1970 stated, \"We cannot accept the assertion of a Canadian claim that the Arctic waters are internal waters of Canada. ... Such acceptance would jeopardize the freedom of navigation essential for United States naval activities worldwide.\" A compromise of sorts was reached in 1988, by an agreement on \"Arctic Cooperation\", which pledges that voyages of American icebreakers \"will be undertaken with the consent of the Government of Canada\". However, the agreement did not alter either country's basic legal position. Paul Cellucci, the American ambassador to Canada, in 2005 suggested to Washington that it should recognize the straits as belonging to Canada. His advice was rejected and Harper took opposite positions. The U.S. opposes Harper's proposed plan to deploy military icebreakers in the Arctic to detect interlopers and assert Canadian sovereignty over those waters.\nViews of presidents and prime ministers.\nPresidents and prime ministers typically make formal or informal statements that indicate the diplomatic policy of their administration. Diplomats and journalists at the time\u2014and historians since\u2014dissect the nuances and tone to detect the warmth or coolness of the relationship.\nCanada's first Prime Minister also said:\nPublic opinion.\nToday there remain cross-border cultural ties and according to Gallup's annual public opinion polls, Canada has consistently been Americans' favorite nation, with 96% of Americans viewing Canada favorably in 2012. As of spring 2013, 64% of Canadians had a favorable view of the U.S. and 81% expressed confidence in then-US President Obama to do the right thing in international matters. According to the same poll, 30% viewed the U.S. negatively. In addition, according to Spring 2017 Global Attitudes Survey, 43% of Canadians view the U.S. positively, while 51% hold a negative view. More recently, however, a poll in January 2018 showed Canadians' approval of U.S. leadership dropped by over 40 percentage points under President Donald Trump, in line with the view of residents of many other U.S. allied and neutral countries. Since then, Canadian opinion of the U.S. has improved significantly, following an international rebound in the U.S. image abroad following the transition as President of the United States from Donald Trump to Joe Biden, with 61% of Canadians having a favorable opinion of the United States in 2021.\nAnti-Americanism.\nAnti-Americanism in Canada has unique historic roots. Since the arrival of the Loyalists as refugees from the American Revolution in the 1780s, historians have identified a constant theme of Canadian fear of the United States and of \"Americanization\" or a cultural takeover. In the War of 1812, for example, the enthusiastic response by French militia to defend Lower Canada reflected, according to Heidler and Heidler (2004), \"the fear of Americanization\". Scholars have traced this attitude over time in Ontario and Quebec.\nCanadian intellectuals who wrote about the U.S. in the first half of the 20th century identified America as the world center of modernity and deplored it. Anti-American Canadians (who admired the British Empire) explained that Canada had narrowly escaped American conquest with its rejection of tradition, its worship of \"progress\" and technology, and its mass culture; they explained that Canada was much better because of its commitment to orderly government and societal harmony. There were a few ardent defenders of the nation to the south, notably liberal and socialist intellectuals such as F. R. Scott and Jean-Charles Harvey (1891\u20131967).\nLooking at television, Collins (1990) finds that it is in Anglophone Canada that fear of cultural Americanization is most powerful, for there the attractions of the U.S. are strongest. Meren (2009) argues that after 1945, the emergence of Quebec nationalism and the desire to preserve French-Canadian cultural heritage led to growing anxiety regarding American cultural imperialism and Americanization. In 2006 surveys showed that 60 percent of Qu\u00e9b\u00e9cois had a fear of Americanization, while other surveys showed they preferred their current situation to that of the Americans in the realms of health care, quality of life as seniors, environmental quality, poverty, educational system, racism and standard of living. While agreeing that job opportunities are greater in America, 89 percent disagreed with the notion that they would rather be in the United States, and they were more likely to feel closer to English Canadians than to Americans. However, there is evidence that the elites and Quebec are much less fearful of Americanization and much more open to economic integration than the general public.\nThe history has been traced in detail by a leading Canadian historian J.L. Granatstein in \"Yankee Go Home: Canadians and Anti-Americanism\" (1997). Current studies report the phenomenon persists. Two scholars report, \"Anti-Americanism is alive and well in Canada today, strengthened by, among other things, disputes related to NAFTA, American involvement in the Middle East, and the ever-increasing Americanization of Canadian culture.\" Jamie Glazov writes, \"More than anything else, Diefenbaker became the tragic victim of Canadian anti-Americanism, a sentiment the prime minister had fully embraced by 1962. [He was] unable to imagine himself (or his foreign policy) without enemies.\" Historian J. M. Bumsted says, \"In its most extreme form, Canadian suspicion of the United States has led to outbreaks of overt anti-Americanism, usually spilling over against American residents in Canada.\" John R. Wennersten writes, \"But at the heart of Canadian anti-Americanism lies a cultural bitterness that takes an American expatriate unaware. Canadians fear the American media's influence on their culture and talk critically about how Americans are exporting a culture of violence in its television programming and movies.\" However Kim Nossal points out that the Canadian variety is much milder than anti-Americanism in some other countries. By contrast, Americans show very little knowledge or interest one way or the other regarding Canadian affairs. Canadian historian Frank Underhill, quoting Canadian playwright Merrill Denison summed it up: \"Americans are benevolently ignorant about Canada, whereas Canadians are malevolently informed about the United States.\"\nCanadian public opinion on U.S. presidents.\nUnited States President George W. Bush was \"deeply disliked\" by a majority of Canadians according to the \"Arizona Daily Sun\". A 2004 poll found that more than two-thirds of Canadians favored Democrat John Kerry over Bush in the 2004 presidential election, with Bush's lowest approval ratings in Canada being in the province of Quebec where just 11% of the population supported him. Canadian public opinion of Barack Obama was significantly more positive. A 2012 poll found that 65% of Canadians would vote for Obama in the 2012 presidential election \"if they could\" while only 9% of Canadians would vote for his Republican opponent Mitt Romney. The same study found that 61% of Canadians felt that the Obama administration had been \"good\" for America, while only 12% felt it had been \"bad\". Similarly, a Pew Research poll conducted in June 2016 found that 83% of Canadians were \"confident in Obama to do the right thing regarding world affairs\". The study also found that a majority of members of all three major Canadian political parties supported Obama, and also found that Obama had slightly higher approval ratings in Canada in 2012 than he did in 2008. John Ibbitson of \"The Globe and Mail\" stated in 2012 that Canadians generally supported Democratic presidents over Republican presidents, citing how President Richard Nixon was \"never liked\" in Canada and that Canadians generally did not approve of Prime Minister Brian Mulroney's friendship with President Ronald Reagan.\nA November 2016 poll found 82% of Canadians preferred Hillary Clinton over Donald Trump. A January 2017 poll found that 66% of Canadians \"disapproved\" of Donald Trump, with 23% approving of him and 11% being \"unsure\". The poll also found that only 18% of Canadians believed Trump's presidency would have a positive impact on Canada, while 63% believed it would have a negative effect. A July 2019 poll found 79% of Canadians preferred Joe Biden or Bernie Sanders over Trump. A Pew Research poll released in June 2021, showed that Canadian opinion of American president Joe Biden is much more favorable than his predecessor Donald Trump, with 77% approving of his leadership and having confidence in him to do the right thing."}
{"id": "5202", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5202", "title": "Canada/cities", "text": ""}
{"id": "5206", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5206", "title": "Computer-generated art", "text": ""}
{"id": "5208", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5208", "title": "CountriesW", "text": ""}
{"id": "5209", "revid": "42316941", "url": "https://en.wikipedia.org/wiki?curid=5209", "title": "CIAWorldFactbook", "text": ""}
{"id": "5210", "revid": "205121", "url": "https://en.wikipedia.org/wiki?curid=5210", "title": "C.S. Lewis", "text": ""}
{"id": "5211", "revid": "14797726", "url": "https://en.wikipedia.org/wiki?curid=5211", "title": "Christianity", "text": "Christianity is an Abrahamic monotheistic religion, professing that Jesus was raised from the dead and is the Son of God, whose coming as the Messiah (Christ) was prophesied in the Hebrew Bible (called the Old Testament in Christianity) and chronicled in the New Testament. It is the world's largest and most widespread religion with over 2.38\u00a0billion followers, comprising around 31.2% of the world population. Its adherents, known as Christians, are estimated to make up a majority of the population in 157 countries and territories. \nChristianity remains culturally diverse in its Western and Eastern branches, and doctrinally diverse concerning justification and the nature of salvation, ecclesiology, ordination, and Christology. The creeds of various Christian denominations generally hold in common Jesus as the Son of God\u2014the Logos incarnated\u2014who ministered, suffered, and died on a cross, but rose from the dead for the salvation of humankind; and referred to as the gospel, meaning the \"good news\". The four canonical gospels of Matthew, Mark, Luke and John describe Jesus's life and teachings as preserved in the early Christian tradition, with the Old Testament as the gospels' respected background.\nChristianity began in the 1st century, after the death of Jesus, as a Judaic sect with Hellenistic influence in the Roman province of Judaea. The disciples of Jesus spread their faith around the Eastern Mediterranean area, despite significant persecution. The inclusion of Gentiles led Christianity to slowly separate from Judaism (2nd century). Emperor Constantine I decriminalized Christianity in the Roman Empire by the Edict of Milan (313), later convening the Council of Nicaea (325) where Early Christianity was consolidated into what would become the state religion of the Roman Empire (380). The Church of the East and Oriental Orthodoxy both split over differences in Christology (5th century), while the Eastern Orthodox Church and the Catholic Church separated in the East\u2013West Schism (1054). Protestantism split into numerous denominations from the Catholic Church in the Reformation era (16th century). Following the Age of Discovery (15th\u201317th century), Christianity expanded throughout the world via missionary work, evangelism, immigration and extensive trade. Christianity played a prominent role in the development of Western civilization, particularly in Europe from late antiquity and the Middle Ages.\nThe three main branches of Christianity are Roman Catholicism (1.3\u00a0billion people), Protestantism (625\u00a0million-900\u00a0million), and Eastern Orthodoxy (230\u00a0million) while other prominent braches include Oriental Orthodoxy (60\u00a0million), Restorationism (35\u00a0million), and the Church of the East (600,000). Smaller church communities number in the thousands despite efforts toward unity (ecumenism). In the West, Christianity remains the dominant religion even with a decline in adherence, with about 70% of that population identifying as Christian. Christianity is growing in Africa and Asia, the world's most populous continents. Christians are persecuted in some regions of the world, particularly where they are in minority in the Middle East, North Africa, East Asia, and South Asia.\nEtymology.\nEarly Jewish Christians referred to themselves as 'The Way' (), probably coming from , \"prepare the way of the Lord\". According to , the term \"Christian\" (, ), meaning \"followers of Christ\" in reference to Jesus's disciples, was first used in the city of Antioch by the non-Jewish inhabitants there. The earliest recorded use of the term \"Christianity/Christianism\" (, ) was by Ignatius of Antioch around 100\u00a0AD. The name Jesus comes from , likely from \"Y\u0113\u0161\u016ba\u02bf.\"\nHistory.\nEarly Christianity.\nApostolic Age.\nChristianity developed during the 1st century AD as a Jewish Christian sect with Hellenistic influence of Second Temple Judaism. An early Jewish Christian community was founded in Jerusalem under the leadership of the Pillars of the Church, namely James the Just, the brother of Jesus, Peter, and John.\nJewish Christianity soon attracted Gentile God-fearers, posing a problem for its Jewish religious outlook, which insisted on close observance of the Jewish commandments. Paul the Apostle solved this by insisting that salvation by faith in Christ, and participation in his death and resurrection by their baptism, sufficed. At first he persecuted the early Christians, but after a conversion experience he preached to the gentiles, and is regarded as having had a formative effect on the emerging Christian identity as separate from Judaism. Eventually, his departure from Jewish customs would result in the establishment of Christianity as an independent religion.\nAnte-Nicene period.\nThis formative period was followed by the early bishops, whom Christians consider the successors of Christ's apostles. From the year 150, Christian teachers began to produce theological and apologetic works aimed at defending the faith. These authors are known as the Church Fathers, and the study of them is called patristics. Notable early Fathers include Ignatius of Antioch, Polycarp, Justin Martyr, Irenaeus, Tertullian, Clement of Alexandria and Origen.\nPersecution of Christians occurred intermittently and on a small scale by both Jewish and Roman authorities, with Roman action starting at the time of the Great Fire of Rome in 64 AD. Examples of early executions under Jewish authority reported in the New Testament include the deaths of Saint Stephen and James, son of Zebedee. The Decian persecution was the first empire-wide conflict, when the edict of Decius in 250 AD required everyone in the Roman Empire (except Jews) to perform a sacrifice to the Roman gods. The Diocletianic Persecution beginning in 303 AD was also particularly severe. Roman persecution ended in 313 AD with the Edict of Milan.\nWhile Proto-orthodox Christianity was becoming dominant, heterodox sects also existed at the same time, which held radically different beliefs. Gnostic Christianity developed a duotheistic doctrine based on illusion and enlightenment rather than forgiveness of sin. With only a few scriptures overlapping with the developing orthodox canon, most Gnostic texts and Gnostic gospels were eventually considered heretical and suppressed by mainstream Christians. A gradual splitting off of Gentile Christianity left Jewish Christians continuing to follow the Law of Moses, including practices such as circumcision. By the fifth century, they and the Jewish\u2013Christian gospels would be largely suppressed by the dominant sects in both Judaism and Christianity.\nSpread and acceptance in Roman Empire.\nChristianity spread to Aramaic-speaking peoples along the Mediterranean coast and also to the inland parts of the Roman Empire and beyond that into the Parthian Empire and the later Sasanian Empire, including Mesopotamia, which was dominated at different times and to varying extents by these empires. The presence of Christianity in Africa began in the middle of the 1st century in Egypt and by the end of the 2nd century in the region around Carthage. Mark the Evangelist is claimed to have started the Church of Alexandria in about 43 AD; various later churches claim this as their own legacy, including the Coptic Orthodox Church. Important Africans who influenced the early development of Christianity include Tertullian, Clement of Alexandria, Origen of Alexandria, Cyprian, Athanasius, and Augustine of Hippo.\nKing Tiridates III made Christianity the state religion in Armenia in the early 4th century AD, making Armenia the first officially Christian state. It was not an entirely new religion in Armenia, having penetrated into the country from at least the third century, but it may have been present even earlier.\nConstantine I was exposed to Christianity in his youth, and throughout his life his support for the religion grew, culminating in baptism on his deathbed. During his reign, state-sanctioned persecution of Christians was ended with the Edict of Toleration in 311 and the Edict of Milan in 313. At that point, Christianity was still a minority belief, comprising perhaps only 5% of the Roman population. Influenced by his adviser Mardonius, Constantine's nephew Julian unsuccessfully tried to suppress Christianity. On 27 February 380, Theodosius I, Gratian, and Valentinian II established Nicene Christianity as the State church of the Roman Empire. As soon as it became connected to the state, Christianity grew wealthy; the Church solicited donations from the rich and could now own land.\nConstantine was also instrumental in the convocation of the First Council of Nicaea in 325, which sought to address Arianism and formulated the Nicene Creed, which is still used by in Catholicism, Eastern Orthodoxy, Lutheranism, Anglicanism, and many other Protestant churches. Nicaea was the first of a series of ecumenical councils, which formally defined critical elements of the theology of the Church, notably concerning Christology. The Church of the East did not accept the third and following ecumenical councils and is still separate today by its successors (Assyrian Church of the East).\nIn terms of prosperity and cultural life, the Byzantine Empire was one of the peaks in Christian history and Christian civilization, and Constantinople remained the leading city of the Christian world in size, wealth, and culture. There was a renewed interest in classical Greek philosophy, as well as an increase in literary output in vernacular Greek. Byzantine art and literature held a preeminent place in Europe, and the cultural impact of Byzantine art on the West during this period was enormous and of long-lasting significance. The later rise of Islam in North Africa reduced the size and numbers of Christian congregations, leaving in large numbers only the Coptic Church in Egypt, the Ethiopian Orthodox Tewahedo Church in the Horn of Africa and the Nubian Church in the Sudan (Nobatia, Makuria and Alodia).\nMiddle Ages.\nEarly Middle Ages.\nWith the decline and fall of the Roman Empire in the West, the papacy became a political player, first visible in Pope Leo's diplomatic dealings with Huns and Vandals. The church also entered into a long period of missionary activity and expansion among the various tribes. While Arianists instituted the death penalty for practicing pagans (see the Massacre of Verden, for example), Catholicism also spread among the Hungarians, the Germanic, the Celtic, the Baltic and some Slavic peoples.\nAround 500, Christianity was thoroughly integrated into Byzantine and Kingdom of Italy culture and Benedict of Nursia set out his Monastic Rule, establishing a system of regulations for the foundation and running of monasteries. Monasticism became a powerful force throughout Europe, and gave rise to many early centers of learning, most famously in Ireland, Scotland, and Gaul, contributing to the Carolingian Renaissance of the 9th century.\nIn the 7th century, Muslims conquered Syria (including Jerusalem), North Africa, and Spain, converting some of the Christian population to Islam, including some of the Christian populations in pre-Islamic Arabia, and placing the rest under a separate legal status. Part of the Muslims' success was due to the exhaustion of the Byzantine Empire in its decades long conflict with Persia. Beginning in the 8th century, with the rise of Carolingian leaders, the Papacy sought greater political support in the Frankish Kingdom.\nThe Middle Ages brought about major changes within the church. Pope Gregory the Great dramatically reformed the ecclesiastical structure and administration. In the early 8th century, iconoclasm became a divisive issue, when it was sponsored by the Byzantine emperors. The Second Ecumenical Council of Nicaea (787) finally pronounced in favor of icons. In the early 10th century, Western Christian monasticism was further rejuvenated through the leadership of the great Benedictine monastery of Cluny.\nHigh and Late Middle Ages.\nIn the West, from the 11th century onward, some older cathedral schools became universities (see, for example, University of Oxford, University of Paris and University of Bologna). Previously, higher education had been the domain of Christian cathedral schools or monastic schools (\"Scholae monasticae\"), led by monks and nuns. Evidence of such schools dates back to the 6th century\u00a0AD. These new universities expanded the curriculum to include academic programs for clerics, lawyers, civil servants, and physicians. The university is generally regarded as an institution that has its origin in the Medieval Christian setting.\nAccompanying the rise of the \"new towns\" throughout Europe, mendicant orders were founded, bringing the consecrated religious life out of the monastery and into the new urban setting. The two principal mendicant movements were the Franciscans and the Dominicans, founded by Francis of Assisi and Dominic, respectively. Both orders made significant contributions to the development of the great universities of Europe. Another new order was the Cistercians, whose large, isolated monasteries spearheaded the settlement of former wilderness areas. In this period, church building and ecclesiastical architecture reached new heights, culminating in the orders of Romanesque and Gothic architecture and the building of the great European cathedrals.\nChristian nationalism emerged during this era in which Christians felt the desire to recover lands in which Christianity had historically flourished. From 1095 under the pontificate of Urban II, the First Crusade was launched. These were a series of military campaigns in the Holy Land and elsewhere, initiated in response to pleas from the Byzantine Emperor Alexios I for aid against Turkish expansion. The Crusades ultimately failed to stifle Islamic aggression and even contributed to Christian enmity with the sacking of Constantinople during the Fourth Crusade.\nThe Christian Church experienced internal conflict between the 7th and 13th centuries that resulted in a schism between the Latin Church of Western Christianity branch, the now-Catholic Church, and an Eastern, largely Greek, branch (the Eastern Orthodox Church). The two sides disagreed on a number of administrative, liturgical and doctrinal issues, most prominently Eastern Orthodox opposition to papal supremacy. The Second Council of Lyon (1274) and the Council of Florence (1439) attempted to reunite the churches, but in both cases, the Eastern Orthodox refused to implement the decisions, and the two principal churches remain in schism to the present day. However, the Catholic Church has achieved union with various smaller eastern churches.\nIn the thirteenth century, a new emphasis on Jesus' suffering, exemplified by the Franciscans' preaching, had the consequence of turning worshippers' attention towards Jews, on whom Christians had placed the blame for Jesus' death. Christianity's limited tolerance of Jews was not new\u2014Augustine of Hippo said that Jews should not be allowed to enjoy the citizenship that Christians took for granted\u2014but the growing antipathy towards Jews was a factor that led to the expulsion of Jews from England in 1290, the first of many such expulsions in Europe.\nBeginning around 1184, following the crusade against Cathar heresy, various institutions, broadly referred to as the Inquisition, were established with the aim of suppressing heresy and securing religious and doctrinal unity within Christianity through conversion and prosecution.\nModern era.\nProtestant Reformation and Counter-Reformation.\nThe 15th-century Renaissance brought about a renewed interest in ancient and classical learning. During the Reformation, Martin Luther posted the \"Ninety-five Theses\" 1517 against the sale of indulgences. Printed copies soon spread throughout Europe. In 1521 the Edict of Worms condemned and excommunicated Luther and his followers, resulting in the schism of the Western Christendom into several branches.\nOther reformers like Zwingli, Oecolampadius, Calvin, Knox, and Arminius further criticized Catholic teaching and worship. These challenges developed into the movement called Protestantism, which repudiated the primacy of the pope, the role of tradition, the seven sacraments, and other doctrines and practices. The Reformation in England began in 1534, when King Henry VIII had himself declared head of the Church of England. Beginning in 1536, the monasteries throughout England, Wales and Ireland were dissolved.\nThomas M\u00fcntzer, Andreas Karlstadt and other theologians perceived both the Catholic Church and the confessions of the Magisterial Reformation as corrupted. Their activity brought about the Radical Reformation, which gave birth to various Anabaptist denominations.\nPartly in response to the Protestant Reformation, the Catholic Church engaged in a substantial process of reform and renewal, known as the Counter-Reformation or Catholic Reform. The Council of Trent clarified and reasserted Catholic doctrine. During the following centuries, competition between Catholicism and Protestantism became deeply entangled with political struggles among European states.\nMeanwhile, the discovery of America by Christopher Columbus in 1492 brought about a new wave of missionary activity. Partly from missionary zeal, but under the impetus of colonial expansion by the European powers, Christianity spread to the Americas, Oceania, East Asia and sub-Saharan Africa.\nThroughout Europe, the division caused by the Reformation led to outbreaks of religious violence and the establishment of separate state churches in Europe. Lutheranism spread into the northern, central, and eastern parts of present-day Germany, Livonia, and Scandinavia. Anglicanism was established in England in 1534. Calvinism and its varieties, such as Presbyterianism, were introduced in Scotland, the Netherlands, Hungary, Switzerland, and France. Arminianism gained followers in the Netherlands and Frisia. Ultimately, these differences led to the outbreak of conflicts in which religion played a key factor. The Thirty Years' War, the English Civil War, and the French Wars of Religion are prominent examples. These events intensified the Christian debate on persecution and toleration.\nIn the revival of neoplatonism Renaissance humanists did not reject Christianity; quite the contrary, many of the greatest works of the Renaissance were devoted to it, and the Catholic Church patronized many works of Renaissance art. Much, if not most, of the new art was commissioned by or in dedication to the Church. Some scholars and historians attribute Christianity to having contributed to the rise of the Scientific Revolution. Many well-known historical figures who influenced Western science considered themselves Christian such as Nicolaus Copernicus, Galileo Galilei, Johannes Kepler, Isaac Newton and Robert Boyle.\nPost-Enlightenment.\nIn the era known as the Great Divergence, when in the West, the Age of Enlightenment and the scientific revolution brought about great societal changes, Christianity was confronted with various forms of skepticism and with certain modern political ideologies, such as versions of socialism and liberalism. Events ranged from mere anti-clericalism to violent outbursts against Christianity, such as the dechristianization of France during the French Revolution, the Spanish Civil War, and certain Marxist movements, especially the Russian Revolution and the persecution of Christians in the Soviet Union under state atheism.\nEspecially pressing in Europe was the formation of nation states after the Napoleonic era. In all European countries, different Christian denominations found themselves in competition to greater or lesser extents with each other and with the state. Variables were the relative sizes of the denominations and the religious, political, and ideological orientation of the states. Urs Altermatt of the University of Fribourg, looking specifically at Catholicism in Europe, identifies four models for the European nations. In traditionally Catholic-majority countries such as Belgium, Spain, and Austria, to some extent, religious and national communities are more or less identical. Cultural symbiosis and separation are found in Poland, the Republic of Ireland, and Switzerland, all countries with competing denominations. Competition is found in Germany, the Netherlands, and again Switzerland, all countries with minority Catholic populations, which to a greater or lesser extent identified with the nation. Finally, separation between religion (again, specifically Catholicism) and the state is found to a great degree in France and Italy, countries where the state actively opposed itself to the authority of the Catholic Church.\nThe combined factors of the formation of nation states and ultramontanism, especially in Germany and the Netherlands, but also in England to a much lesser extent, often forced Catholic churches, organizations, and believers to choose between the national demands of the state and the authority of the Church, specifically the papacy. This conflict came to a head in the First Vatican Council, and in Germany would lead directly to the \"Kulturkampf\".\nChristian commitment in Europe dropped as modernity and secularism came into their own, particularly in the Czech Republic and Estonia, while religious commitments in America have been generally high in comparison to Europe. Changes in worldwide Christianity over the last century have been significant, since 1900, Christianity has spread rapidly in the Global South and Third World countries. The late 20th century has shown the shift of Christian adherence to the Third World and the Southern Hemisphere in general, with the West no longer the chief standard bearer of Christianity. Approximately 7 to 10% of Arabs are Christians, most prevalent in Egypt, Syria and Lebanon.\nBeliefs.\nWhile Christians worldwide share basic convictions, there are differences of interpretations and opinions of the Bible and sacred traditions on which Christianity is based.\nCreeds.\nConcise doctrinal statements or confessions of religious beliefs are known as creeds. They began as baptismal formulae and were later expanded during the Christological controversies of the 4th and 5th centuries to become statements of faith. \"Jesus is Lord\" is the earliest creed of Christianity and continues to be used, as with the World Council of Churches.\nThe Apostles' Creed is the most widely accepted statement of the articles of Christian faith. It is used by a number of Christian denominations for both liturgical and catechetical purposes, most visibly by liturgical churches of Western Christian tradition, including the Latin Church of the Catholic Church, Lutheranism, Anglicanism, and Western Rite Orthodoxy. It is also used by Presbyterians, Methodists, and Congregationalists.\nThis particular creed was developed between the 2nd and 9th centuries. Its central doctrines are those of the Trinity and God the Creator. Each of the doctrines found in this creed can be traced to statements current in the apostolic period. The creed was apparently used as a summary of Christian doctrine for baptismal candidates in the churches of Rome. Its points include:\nThe Nicene Creed was formulated, largely in response to Arianism, at the Councils of Nicaea and Constantinople in 325 and 381 respectively, and ratified as the universal creed of Christendom by the First Council of Ephesus in 431.\nThe Chalcedonian Definition, or Creed of Chalcedon, developed at the Council of Chalcedon in 451, though rejected by the Oriental Orthodox, taught Christ \"to be acknowledged in two natures, inconfusedly, unchangeably, indivisibly, inseparably\": one divine and one human, and that both natures, while perfect in themselves, are nevertheless also perfectly united into one person.\nThe Athanasian Creed, received in the Western Church as having the same status as the Nicene and Chalcedonian, says: \"We worship one God in Trinity, and Trinity in Unity; neither confounding the Persons nor dividing the Substance\".\nMost Christians (Catholic, Eastern Orthodox, Oriental Orthodox, and Protestant alike) accept the use of creeds and subscribe to at least one of the creeds mentioned above.\nCertain Evangelical Protestants, though not all of them, reject creeds as definitive statements of faith, even while agreeing with some or all of the substance of the creeds. Also rejecting creeds are groups with roots in the Restoration Movement, such as the Christian Church (Disciples of Christ), the Evangelical Christian Church in Canada, and the Churches of Christ.\nJesus.\nThe central tenet of Christianity is the belief in Jesus as the Son of God and the Messiah (Christ). Christians believe that Jesus, as the Messiah, was anointed by God as savior of humanity and hold that Jesus's coming was the fulfillment of messianic prophecies of the Old Testament. The Christian concept of messiah differs significantly from the contemporary Jewish concept. The core Christian belief is that through belief in and acceptance of the death and resurrection of Jesus, sinful humans can be reconciled to God, and thereby are offered salvation and the promise of eternal life.\nWhile there have been many theological disputes over the nature of Jesus over the earliest centuries of Christian history, generally, Christians believe that Jesus is God incarnate and \"true God and true man\" (or both fully divine and fully human). Jesus, having become fully human, suffered the pains and temptations of a mortal man, but did not sin. As fully God, he rose to life again. According to the New Testament, he rose from the dead, ascended to heaven, is seated at the right hand of the Father, and will ultimately return to fulfill the rest of the Messianic prophecy, including the resurrection of the dead, the Last Judgment, and the final establishment of the Kingdom of God.\nAccording to the canonical gospels of Matthew and Luke, Jesus was conceived by the Holy Spirit and born from the Virgin Mary. Little of Jesus's childhood is recorded in the canonical gospels, although infancy gospels were popular in antiquity. In comparison, his adulthood, especially the week before his death, is well documented in the gospels contained within the New Testament, because that part of his life is believed to be most important. The biblical accounts of Jesus's ministry include: his baptism, miracles, preaching, teaching, and deeds.\nDeath and resurrection.\nChristians consider the resurrection of Jesus to be the cornerstone of their faith (see 1 Corinthians 15) and the most important event in history. Among Christian beliefs, the death and resurrection of Jesus are two core events on which much of Christian doctrine and theology is based. According to the New Testament, Jesus was crucified, died a physical death, was buried within a tomb, and rose from the dead three days later.\nThe New Testament mentions several post-resurrection appearances of Jesus on different occasions to his twelve apostles and disciples, including \"more than five hundred brethren at once\", before Jesus's ascension to heaven. Jesus's death and resurrection are commemorated by Christians in all worship services, with special emphasis during Holy Week, which includes Good Friday and Easter Sunday.\nThe death and resurrection of Jesus are usually considered the most important events in Christian theology, partly because they demonstrate that Jesus has power over life and death and therefore has the authority and power to give people eternal life.\nChristian churches accept and teach the New Testament account of the resurrection of Jesus with very few exceptions. Some modern scholars use the belief of Jesus's followers in the resurrection as a point of departure for establishing the continuity of the historical Jesus and the proclamation of the early church. Some liberal Christians do not accept a literal bodily resurrection, seeing the story as richly symbolic and spiritually nourishing myth. Arguments over death and resurrection claims occur at many religious debates and interfaith dialogues. Paul the Apostle, an early Christian convert and missionary, wrote, \"If Christ was not raised, then all our preaching is useless, and your trust in God is useless\".\nSalvation.\nPaul the Apostle, like Jews and Roman pagans of his time, believed that sacrifice can bring about new kinship ties, purity, and eternal life. For Paul, the necessary sacrifice was the death of Jesus: Gentiles who are \"Christ's\" are, like Israel, descendants of Abraham and \"heirs according to the promise\" The God who raised Jesus from the dead would also give new life to the \"mortal bodies\" of Gentile Christians, who had become with Israel, the \"children of God\", and were therefore no longer \"in the flesh\".\nModern Christian churches tend to be much more concerned with how humanity can be saved from a universal condition of sin and death than the question of how both Jews and Gentiles can be in God's family. According to Eastern Orthodox theology, based upon their understanding of the atonement as put forward by Irenaeus' recapitulation theory, Jesus' death is a ransom. This restores the relation with God, who is loving and reaches out to humanity, and offers the possibility of \"theosis\" c.q. divinization, becoming the kind of humans God wants humanity to be. According to Catholic doctrine, Jesus' death satisfies the wrath of God, aroused by the offense to God's honor caused by human's sinfulness. The Catholic Church teaches that salvation does not occur without faithfulness on the part of Christians; converts must live in accordance with principles of love and ordinarily must be baptized. In Protestant theology, Jesus' death is regarded as a substitutionary penalty carried by Jesus, for the debt that has to be paid by humankind when it broke God's moral law.\nChristians differ in their views on the extent to which individuals' salvation is pre-ordained by God. Reformed theology places distinctive emphasis on grace by teaching that individuals are completely incapable of self-redemption, but that sanctifying grace is irresistible. In contrast Catholics, Orthodox Christians, and Arminian Protestants believe that the exercise of free will is necessary to have faith in Jesus.\nTrinity.\n\"Trinity\" refers to the teaching that the one God comprises three distinct, eternally co-existing persons: the \"Father\", the \"Son\" (incarnate in Jesus Christ) and the \"Holy Spirit\". Together, these three persons are sometimes called the Godhead, although there is no single term in use in Scripture to denote the unified Godhead. In the words of the Athanasian Creed, an early statement of Christian belief, \"the Father is God, the Son is God, and the Holy Spirit is God, and yet there are not three Gods but one God\". They are distinct from another: the Father has no source, the Son is begotten of the Father, and the Spirit proceeds from the Father. Though distinct, the three persons cannot be divided from one another in being or in operation. While some Christians also believe that God appeared as the Father in the Old Testament, it is agreed that he appeared as the Son in the New Testament and will still continue to manifest as the Holy Spirit in the present. But still, God still existed as three persons in each of these times. However, traditionally there is a belief that it was the Son who appeared in the Old Testament because, for example, when the Trinity is depicted in art, the Son typically has the distinctive appearance, a cruciform halo identifying Christ, and in depictions of the Garden of Eden, this looks forward to an Incarnation yet to occur. In some Early Christian sarcophagi, the Logos is distinguished with a beard, \"which allows him to appear ancient, even pre-existent\".\nThe Trinity is an essential doctrine of mainstream Christianity. From earlier than the times of the Nicene Creed (325) Christianity advocated the triune mystery-nature of God as a normative profession of faith. According to Roger E. Olson and Christopher Hall, through prayer, meditation, study and practice, the Christian community concluded \"that God must exist as both a unity and trinity\", codifying this in ecumenical council at the end of the 4th century.\nAccording to this doctrine, God is not divided in the sense that each person has a third of the whole; rather, each person is considered to be fully God (see Perichoresis). The distinction lies in their relations, the Father being unbegotten; the Son being begotten of the Father; and the Holy Spirit proceeding from the Father and (in Western Christian theology) from the Son. Regardless of this apparent difference, the three \"persons\" are each eternal and omnipotent. Other Christian religions including Unitarian Universalism, Jehovah's Witnesses, and Mormonism, do not share those views on the Trinity.\nThe Greek word \"trias\" is first seen in this sense in the works of Theophilus of Antioch; his text reads: \"of the Trinity, of God, and of His Word, and of His Wisdom\". The term may have been in use before this time; its Latin equivalent, \"trinitas\", appears afterwards with an explicit reference to the Father, the Son, and the Holy Spirit, in Tertullian. In the following century, the word was in general use. It is found in many passages of Origen.\nTrinitarianism.\n\"Trinitarianism\" denotes Christians who believe in the concept of the Trinity. Almost all Christian denominations and churches hold Trinitarian beliefs. Although the words \"Trinity\" and \"Triune\" do not appear in the Bible, beginning in the 3rd century theologians developed the term and concept to facilitate apprehension of the New Testament teachings of God as being Father, Son, and Holy Spirit. Since that time, Christian theologians have been careful to emphasize that Trinity does not imply that there are three gods (the antitrinitarian heresy of Tritheism), nor that each hypostasis of the Trinity is one-third of an infinite God (partialism), nor that the Son and the Holy Spirit are beings created by and subordinate to the Father (Arianism). Rather, the Trinity is defined as one God in three persons.\nNontrinitarianism.\n\"Nontrinitarianism\" (or \"antitrinitarianism\") refers to theology that rejects the doctrine of the Trinity. Various nontrinitarian views, such as adoptionism or modalism, existed in early Christianity, leading to disputes about Christology. Nontrinitarianism reappeared in the Gnosticism of the Cathars between the 11th and 13th centuries, among groups with Unitarian theology in the Protestant Reformation of the 16th century, in the 18th-century Enlightenment, among Restorationist groups arising during the Second Great Awakening of the 19th century, and most recently, in Oneness Pentecostal churches.\nEschatology.\nThe end of things, whether the end of an individual life, the end of the age, or the end of the world, broadly speaking, is Christian eschatology; the study of the destiny of humans as it is revealed in the Bible. The major issues in Christian eschatology are the Tribulation, death and the afterlife, (mainly for Evangelical groups) the Millennium and the following Rapture, the Second Coming of Jesus, Resurrection of the Dead, Heaven, (for liturgical branches) Purgatory, and Hell, the Last Judgment, the end of the world, and the New Heavens and New Earth.\nChristians believe that the second coming of Christ will occur at the end of time, after a period of severe persecution (the Great Tribulation). All who have died will be resurrected bodily from the dead for the Last Judgment. Jesus will fully establish the Kingdom of God in fulfillment of scriptural prophecies.\nDeath and afterlife.\nMost Christians believe that human beings experience divine judgment and are rewarded either with eternal life or eternal damnation. This includes the general judgement at the resurrection of the dead as well as the belief (held by Catholics, Orthodox and most Protestants) in a judgment particular to the individual soul upon physical death.\nIn the Catholic branch of Christianity, those who die in a state of grace, i.e., without any mortal sin separating them from God, but are still imperfectly purified from the effects of sin, undergo purification through the intermediate state of purgatory to achieve the holiness necessary for entrance into God's presence. Those who have attained this goal are called \"saints\" (Latin \"sanctus\", \"holy\").\nSome Christian groups, such as Seventh-day Adventists, hold to mortalism, the belief that the human soul is not naturally immortal, and is unconscious during the intermediate state between bodily death and resurrection. These Christians also hold to Annihilationism, the belief that subsequent to the final judgement, the wicked will cease to exist rather than suffer everlasting torment. Jehovah's Witnesses hold to a similar view.\nPractices.\nDepending on the specific denomination of Christianity, practices may include baptism, the Eucharist (Holy Communion or the Lord's Supper), prayer (including the Lord's Prayer), confession, confirmation, burial rites, marriage rites and the religious education of children. Most denominations have ordained clergy who lead regular communal worship services.\nChristian rites, rituals, and ceremonies are not celebrated in one single sacred language. Many ritualistic Christian churches make a distinction between sacred language, liturgical language and vernacular language. The three important languages in the early Christian era were: Latin, Greek and Syriac.\nCommunal worship.\nServices of worship typically follow a pattern or form known as liturgy. Justin Martyr described 2nd-century Christian liturgy in his \"First Apology\" () to Emperor Antoninus Pius, and his description remains relevant to the basic structure of Christian liturgical worship:\nThus, as Justin described, Christians assemble for communal worship typically on Sunday, the day of the resurrection, though other liturgical practices often occur outside this setting. Scripture readings are drawn from the Old and New Testaments, but especially the gospels. Instruction is given based on these readings, in the form of a sermon or homily. There are a variety of congregational prayers, including thanksgiving, confession, and intercession, which occur throughout the service and take a variety of forms including recited, responsive, silent, or sung. Psalms, hymns, worship songs, and other church music may be sung. Services can be varied for special events like significant feast days.\nNearly all forms of worship incorporate the Eucharist, which consists of a meal. It is reenacted in accordance with Jesus' instruction at the Last Supper that his followers do in remembrance of him as when he gave his disciples bread, saying, \"This is my body\", and gave them wine saying, \"This is my blood\". In the early church, Christians and those yet to complete initiation would separate for the Eucharistic part of the service. Some denominations such as Confessional Lutheran churches continue to practice 'closed communion'. They offer communion to those who are already united in that denomination or sometimes individual church. Catholics further restrict participation to their members who are not in a state of mortal sin. Many other churches, such as Anglican Communion and the Methodist Churches (such as the Free Methodist Church and United Methodist Church), practice 'open communion' since they view communion as a means to unity, rather than an end, and invite all believing Christians to participate.\nSacraments or ordinances.\nIn Christian belief and practice, a \"sacrament\" is a rite, instituted by Christ, that confers grace, constituting a sacred mystery. The term is derived from the Latin word \"sacramentum\", which was used to translate the Greek word for \"mystery\". Views concerning both which rites are sacramental, and what it means for an act to be a sacrament, vary among Christian denominations and traditions.\nThe most conventional functional definition of a sacrament is that it is an outward sign, instituted by Christ, that conveys an inward, spiritual grace through Christ. The two most widely accepted sacraments are Baptism and the Eucharist; however, the majority of Christians also recognize five additional sacraments: Confirmation (Chrismation in the Eastern tradition), Holy Orders (or ordination), Penance (or Confession), Anointing of the Sick, and Matrimony (see Christian views on marriage).\nTaken together, these are the Seven Sacraments as recognized by churches in the High Church tradition\u2014notably Catholic, Eastern Orthodox, Oriental Orthodox, Independent Catholic, Old Catholic, some Lutherans and Anglicans. Most other denominations and traditions typically affirm only Baptism and Eucharist as sacraments, while some Protestant groups, such as the Quakers, reject sacramental theology. Certain denominations of Christianity, such as Anabaptists, use the term \"ordinances\" to refer to rites instituted by Jesus for Christians to observe. Seven ordinances have been taught in many Conservative Mennonite Anabaptist churches, which include \"baptism, communion, footwashing, marriage, anointing with oil, the holy kiss, and the prayer covering\". \nIn addition to this, the Church of the East has two additional sacraments in place of the traditional sacraments of Matrimony and the Anointing of the Sick. These include Holy Leaven (Melka) and the sign of the cross. The Schwarzenau Brethren Anabaptist churches, such as the Dunkard Brethren Church, observe the agape feast (lovefeast), a rite also observed by Moravian Church and Methodist Churches.\nLiturgical calendar.\nCatholics, Eastern Christians, Lutherans, Anglicans and other traditional Protestant communities frame worship around the liturgical year. The liturgical cycle divides the year into a series of seasons, each with their theological emphases, and modes of prayer, which can be signified by different ways of decorating churches, colors of paraments and vestments for clergy, scriptural readings, themes for preaching and even different traditions and practices often observed personally or in the home.\nWestern Christian liturgical calendars are based on the cycle of the Roman Rite of the Catholic Church, and Eastern Christians use analogous calendars based on the cycle of their respective rites. Calendars set aside holy days, such as solemnities which commemorate an event in the life of Jesus, Mary, or the saints, and periods of fasting, such as Lent and other pious events such as memoria, or lesser festivals commemorating saints. Christian groups that do not follow a liturgical tradition often retain certain celebrations, such as Christmas, Easter, and Pentecost: these are the celebrations of Christ's birth, resurrection, and the descent of the Holy Spirit upon the Church, respectively. A few denominations such as Quaker Christians make no use of a liturgical calendar.\nSymbols.\nMost Christian denominations have not generally practiced aniconism, the avoidance or prohibition of devotional images, even if early Jewish Christians, invoking the Decalogue's prohibition of idolatry, avoided figures in their symbols.\nThe cross, today one of the most widely recognized symbols, was used by Christians from the earliest times. Tertullian, in his book \"De Corona\", tells how it was already a tradition for Christians to trace the sign of the cross on their foreheads. Although the cross was known to the early Christians, the crucifix did not appear in use until the 5th century.\nAmong the earliest Christian symbols, that of the fish or Ichthys seems to have ranked first in importance, as seen on monumental sources such as tombs from the first decades of the 2nd century. Its popularity seemingly arose from the Greek word \"ichthys\" (fish) forming an acrostic for the Greek phrase \"Iesous Christos Theou Yios Soter\" (\u1f38\u03b7\u03c3\u03bf\u1fe6\u03c2 \u03a7\u03c1\u03b9\u03c3\u03c4\u03cc\u03c2, \u0398\u03b5\u03bf\u1fe6 \u03a5\u1f31\u03cc\u03c2, \u03a3\u03c9\u03c4\u03ae\u03c1), (Jesus Christ, Son of God, Savior), a concise summary of Christian faith.\nOther major Christian symbols include the chi-rho monogram, the dove and olive branch (symbolic of the Holy Spirit), the sacrificial lamb (representing Christ's sacrifice), the vine (symbolizing the connection of the Christian with Christ) and many others. These all derive from passages of the New Testament.\nBaptism.\nBaptism is the ritual act, with the use of water, by which a person is admitted to membership of the Church. Beliefs on baptism vary among denominations. Differences occur firstly on whether the act has any spiritual significance. Some, such as the Catholic and Eastern Orthodox churches, as well as Lutherans and Anglicans, hold to the doctrine of baptismal regeneration, which affirms that baptism creates or strengthens a person's faith, and is intimately linked to salvation. Baptists and Plymouth Brethren view baptism as a purely symbolic act, an external public declaration of the inward change which has taken place in the person, but not as spiritually efficacious. Secondly, there are differences of opinion on the methodology (or mode) of the act. These modes are: by \"immersion\"; if immersion is total, by \"submersion\"; by affusion (pouring); and by aspersion (sprinkling). Those who hold the first view may also adhere to the tradition of infant baptism; the Orthodox Churches all practice infant baptism and always baptize by total immersion repeated three times in the name of the Father, the Son, and the Holy Spirit. The Lutheran Church and the Catholic Church also practice infant baptism, usually by affusion, and using the Trinitarian formula. Anabaptist Christians practice believer's baptism, in which an adult chooses to receive the ordinance after making a decision to follow Jesus. Anabaptist denominations such as the Mennonites, Amish and Hutterites use pouring as the mode to administer believer's baptism, whereas Anabaptists of the Schwarzenau Brethren and River Brethren traditions baptize by immersion.\nPrayer.\nIn the Gospel of Saint Matthew, Jesus taught the Lord's Prayer, which has been seen as a model for Christian prayer. The injunction for Christians to pray the Lord's prayer thrice daily was given in the \"Didache\" and came to be recited by Christians at 9 am, 12 pm, and 3 pm.\nIn the second century \"Apostolic Tradition\", Hippolytus instructed Christians to pray at seven fixed prayer times: \"on rising, at the lighting of the evening lamp, at bedtime, at midnight\" and \"the third, sixth and ninth hours of the day, being hours associated with Christ's Passion\". Prayer positions, including kneeling, standing, and prostrations have been used for these seven fixed prayer times since the days of the early Church. Breviaries such as the Shehimo and Agpeya are used by Oriental Orthodox Christians to pray these canonical hours while facing in the eastward direction of prayer.\nThe \"Apostolic Tradition\" directed that the sign of the cross be used by Christians during the minor exorcism of baptism, during ablutions before praying at fixed prayer times, and in times of temptation.\n\"Intercessory prayer\" is prayer offered for the benefit of other people. There are many intercessory prayers recorded in the Bible, including prayers of the Apostle Peter on behalf of sick persons and by prophets of the Old Testament in favor of other people. In the Epistle of James, no distinction is made between the intercessory prayer offered by ordinary believers and the prominent Old Testament prophet Elijah. The effectiveness of prayer in Christianity derives from the power of God rather than the status of the one praying.\nThe ancient church, in both Eastern and Western Christianity, developed a tradition of asking for the intercession of (deceased) saints, and this remains the practice of most Eastern Orthodox, Oriental Orthodox, Catholic, and some Lutheran and Anglican churches. Apart from certain sectors within the latter two denominations, other Churches of the Protestant Reformation, however, rejected prayer to the saints, largely on the basis of the sole mediatorship of Christ. The reformer Huldrych Zwingli admitted that he had offered prayers to the saints until his reading of the Bible convinced him that this was idolatrous.\nAccording to the \"Catechism of the Catholic Church\": \"Prayer is the raising of one's mind and heart to God or the requesting of good things from God\". The \"Book of Common Prayer\" in the Anglican tradition is a guide which provides a set order for services, containing set prayers, scripture readings, and hymns or sung Psalms. Frequently in Western Christianity, when praying, the hands are placed palms together and forward as in the feudal commendation ceremony. At other times the older orans posture may be used, with palms up and elbows in.\nScriptures.\nChristianity, like other religions, has adherents whose beliefs and biblical interpretations vary. Christianity regards the biblical canon, the Old Testament and the New Testament, as the inspired word of God. The traditional view of inspiration is that God worked through human authors so that what they produced was what God wished to communicate. The Greek word referring to inspiration in is \"theopneustos\", which literally means \"God-breathed\".\nSome believe that divine inspiration makes present Bibles inerrant, while others claim inerrancy for the Bible in its original manuscripts, although none of those are extant. Still others maintain that only a particular translation is inerrant, such as the King James Version. Another closely related view is biblical infallibility or limited inerrancy, which affirms that the Bible is free of error as a guide to salvation, but may include errors on matters such as history, geography, or science.\nThe canon of the Old Testament accepted by Protestant churches, which is only the Tanakh (the canon of the Hebrew Bible), is shorter than that accepted by the Orthodox and Catholic churches which also include the deuterocanonical books which appear in the Septuagint, the Orthodox canon being slightly larger than the Catholic; Protestants regard the latter as apocryphal, important historical documents which help to inform the understanding of words, grammar, and syntax used in the historical period of their conception. Some versions of the Bible include a separate Apocrypha section between the Old Testament and the New Testament. The New Testament, originally written in Koine Greek, contains 27 books which are agreed upon by all major churches.\nSome denominations have additional canonical holy scriptures beyond the Bible, including the standard works of the Latter Day Saints movement and \"Divine Principle\" in the Unification Church.\nCatholic interpretation.\nIn antiquity, two schools of exegesis developed in Alexandria and Antioch. The Alexandrian interpretation, exemplified by Origen, tended to read Scripture allegorically, while the Antiochene interpretation adhered to the literal sense, holding that other meanings (called \"theoria\") could only be accepted if based on the literal meaning.\nCatholic theology distinguishes two senses of scripture: the literal and the spiritual.\nThe \"literal\" sense of understanding scripture is the meaning conveyed by the words of Scripture. The \"spiritual\" sense is further subdivided into:\nRegarding exegesis, following the rules of sound interpretation, Catholic theology holds:\nProtestant interpretation.\nQualities of Scripture.\nMany Protestant Christians, such as Lutherans and the Reformed, believe in the doctrine of \"sola scriptura\"\u2014that the Bible is a self-sufficient revelation, the final authority on all Christian doctrine, and revealed all truth necessary for salvation; other Protestant Christians, such as Methodists and Anglicans, affirm the doctrine of \"prima scriptura\" which teaches that Scripture is the primary source for Christian doctrine, but that \"tradition, experience, and reason\" can nurture the Christian religion as long as they are in harmony with the Bible. Protestants characteristically believe that ordinary believers may reach an adequate understanding of Scripture because Scripture itself is clear in its meaning (or \"perspicuous\"). Martin Luther believed that without God's help, Scripture would be \"enveloped in darkness\". He advocated for \"one definite and simple understanding of Scripture\". John Calvin wrote, \"all who refuse not to follow the Holy Spirit as their guide, find in the Scripture a clear light\". Related to this is \"efficacy\", that Scripture is able to lead people to faith; and \"sufficiency\", that the Scriptures contain everything that one needs to know to obtain salvation and to live a Christian life.\nOriginal intended meaning of Scripture.\nProtestants stress the meaning conveyed by the words of Scripture, the historical-grammatical method. The historical-grammatical method or grammatico-historical method is an effort in Biblical hermeneutics to find the intended original meaning in the text. This original intended meaning of the text is drawn out through examination of the passage in light of the grammatical and syntactical aspects, the historical background, the literary genre, as well as theological (canonical) considerations. The historical-grammatical method distinguishes between the one original meaning and the significance of the text. The significance of the text includes the ensuing use of the text or application. The original passage is seen as having only a single meaning or sense. As Milton S. Terry said: \"A fundamental principle in grammatico-historical exposition is that the words and sentences can have but one significance in one and the same connection. The moment we neglect this principle we drift out upon a sea of uncertainty and conjecture\". Technically speaking, the grammatical-historical method of interpretation is distinct from the determination of the passage's significance in light of that interpretation. Taken together, both define the term (Biblical) hermeneutics.\nSome Protestant interpreters make use of typology.\nDemographics.\nWith around 2.8\u00a0billion adherents according to a 2022 estimation by World History Encyclopedia, split into three main branches of Catholic, Protestant, and Eastern Orthodox, Christianity is the world's largest religion. High birth rates and conversions in the global South were cited as the reasons for the Christian population growth. For the last hundred years, the Christian share has stood at around 33% of the world population. This masks a major shift in the demographics of Christianity; large increases in the developing world have been accompanied by substantial declines in the developed world, mainly in Western Europe and North America. According to a 2015 Pew Research Center study, within the next four decades, Christianity will remain the largest religion; and by 2050, the Christian population is expected to exceed 3\u00a0billion.\nAccording to some scholars, Christianity ranks at first place in net gains through religious conversion. As a percentage of Christians, the Catholic Church and Orthodoxy (both Eastern and Oriental) are declining in some parts of the world (though Catholicism is growing in Asia, in Africa, vibrant in Eastern Europe, etc.), while Protestants and other Christians are on the rise in the developing world. The so-called \"popular Protestantism\" is one of the fastest growing religious categories in the world. Nevertheless, Catholicism will also continue to grow to 1.63\u00a0billion by 2050, according to Todd Johnson of the Center for the Study of Global Christianity. Africa alone, by 2015, will be home to 230 million African Catholics. And if in 2018, the U.N. projects that Africa's population will reach 4.5\u00a0billion by 2100 (not 2\u00a0billion as predicted in 2004), Catholicism will indeed grow, as will other religious groups. According to Pew Research Center, Africa is expected to be home to 1.1\u00a0billion African Christians by 2050.\nIn 2010, 87% of the world's Christian population lived in countries where Christians are in the majority, while 13% of the world's Christian population lived in countries where Christians are in the minority. Christianity is the predominant religion in Europe, the Americas, Oceania, and Sub-Saharan Africa. There are also large Christian communities in other parts of the world, such as Central Asia, the Middle East and North Africa, East Asia, Southeast Asia, and the Indian subcontinent. In Asia, it is the dominant religion in Armenia, Cyprus, Georgia, East Timor, and the Philippines. However, it is declining in some areas including the northern and western United States, some areas in Oceania (Australia and New Zealand), northern Europe (including Great Britain, Scandinavia and other places), France, Germany, Canada, and some parts of Asia (especially the Middle East, due to the Christian emigration, and Macau).\nThe total Christian population is not decreasing in Brazil and the southern United States, however, the percentage of the population identifying as Christian is in decline. Since the fall of communism, the proportion of Christians has been largely stable in Central Europe, except in the Czech Republic. On the other hand, Christianity is growing rapidly in both numbers and percentages in Eastern Europe, China, other Asian countries, Sub-Saharan Africa, Latin America, North Africa (Maghreb), Gulf Cooperation Council countries, and Oceania.\nDespite a decline in adherence in the West, Christianity remains the dominant religion in the region, with about 70% of that population identifying as Christian. Christianity remains the largest religion in Western Europe, where 71% of Western Europeans identified themselves as Christian in 2018. A 2011 Pew Research Center survey found that 76% of Europeans, 73% in Oceania and about 86% in the Americas (90% in Latin America and 77% in North America) identified themselves as Christians. By 2010 about 157 countries and territories in the world had Christian majorities.\nThere are many charismatic movements that have become well established over large parts of the world, especially Africa, Latin America, and Asia. Since 1900, primarily due to conversion, Protestantism has spread rapidly in Africa, Asia, Oceania, and Latin America. From 1960 to 2000, the global growth of the number of reported Evangelical Protestants grew three times the world's population rate, and twice that of Islam. According to the historian Geoffrey Blainey from the University of Melbourne, since the 1960s there has been a substantial increase in the number of conversions from Islam to Christianity, mostly to the Evangelical and Pentecostal forms.\nA study conducted by St. Mary's University estimated about 10.2 million Muslim converts to Christianity in 2015; according to the study significant numbers of Muslim converts to Christianity can be found in Afghanistan, Azerbaijan, Central Asia (including Kazakhstan, Kyrgyzstan, and other countries), Indonesia, Malaysia, the Middle East (including Iran, Saudi Arabia, Turkey, and other countries), North Africa (including Algeria, Morocco, and Tunisia), Sub-Saharan Africa, and the Western World (including Albania, Belgium, France, Germany, Kosovo, the Netherlands, Russia, Scandinavia, United Kingdom, the United States, and other western countries). It is also reported that Christianity is popular among people of different backgrounds in Africa and Asia; according to a report by the Singapore Management University, more people in Southeast Asia are converting to Christianity, many of them young and having a university degree. According to scholar Juliette Koning and Heidi Dahles of there is a \"rapid expansion\" of Christianity in Singapore, China, Hong Kong, Taiwan, Indonesia, Malaysia, and South Korea. According to scholar Terence Chong from the Institute of Southeast Asian Studies, since the 1980s Christianity is expanding in China, Singapore, Indonesia, Japan, Malaysia, Taiwan, South Korea, and Vietnam.\nIn most countries in the developed world, church attendance among people who continue to identify themselves as Christians has been falling over the last few decades. Some sources view this as part of a drift away from traditional membership institutions, while others link it to signs of a decline in belief in the importance of religion in general. Europe's Christian population, though in decline, still constitutes the largest geographical component of the religion. According to data from the 2012 European Social Survey, around a third of European Christians say they attend services once a month or more. Conversely, according to the World Values Survey, about more than two-thirds of Latin American Christians, and about 90% of African Christians (in Ghana, Nigeria, Rwanda, South Africa and Zimbabwe) said they attended church regularly. According to a 2018 study by the Pew Research Center, Christians in Africa and Latin America and the United States have high levels of commitment to their faith.\nChristianity, in one form or another, is the sole state religion of the following nations: Argentina (Catholic), Costa Rica (Catholic), the Kingdom of Denmark (Lutheran), England (Anglican), Greece (Greek Orthodox), Iceland (Lutheran), Liechtenstein (Catholic), Malta (Catholic), Monaco (Catholic), Norway (Lutheran), Samoa, Tonga (Methodist), Tuvalu (Reformed), and Vatican City (Catholic).\nThere are numerous other countries, such as Cyprus, which although do not have an established church, still give official recognition and support to a specific Christian denomination.\nChurches and denominations.\nChristianity can be taxonomically divided into six main groups: Roman Catholicism, Protestantism, Oriental Orthodoxy, Eastern Orthodoxy, the Church of the East, and Restorationism. A broader distinction that is sometimes drawn is between Eastern Christianity and Western Christianity, which has its origins in the East\u2013West Schism (Great Schism) of the 11th century. Recently, neither Western nor Eastern World Christianity has also stood out, for example, in African-initiated churches. However, there are other present and historical Christian groups that do not fit neatly into one of these primary categories.\nThere is a diversity of doctrines and liturgical practices among groups calling themselves Christian. These groups may vary ecclesiologically in their views on a classification of Christian denominations. The Nicene Creed (325), however, is typically accepted as authoritative by most Christians, including the Catholic, Eastern Orthodox, Oriental Orthodox, and major Protestant, such as Lutheran and Anglican denominations.\nCatholic Church.\nThe Catholic Church consists of those particular Churches, headed by bishops, in communion with the pope, the bishop of Rome, as its highest authority in matters of faith, morality, and church governance. Like Eastern Orthodoxy, the Catholic Church, through apostolic succession, traces its origins to the Christian community founded by Jesus Christ. Catholics maintain that the \"one, holy, catholic, and apostolic church\" founded by Jesus subsists fully in the Catholic Church, but also acknowledges other Christian churches and communities and works towards reconciliation among all Christians. The Catholic faith is detailed in the \"Catechism of the Catholic Church\".\nOf its seven sacraments, the Eucharist is the principal one, celebrated liturgically in the Mass. The church teaches that through consecration by a priest, the sacrificial bread and wine become the body and blood of Christ. The Virgin Mary is venerated in the Catholic Church as Mother of God and Queen of Heaven, honoured in dogmas and devotions. Its teaching includes Divine Mercy, sanctification through faith and evangelization of the Gospel as well as Catholic social teaching, which emphasizes voluntary support for the sick, the poor, and the afflicted through the corporal and spiritual works of mercy. The Catholic Church operates thousands of Catholic schools, universities, hospitals, and orphanages around the world, and is the largest non-government provider of education and health care in the world. Among its other social services are numerous charitable and humanitarian organizations.\nCanon law () is the system of laws and legal principles made and enforced by the hierarchical authorities of the Catholic Church to regulate its external organisation and government and to order and direct the activities of Catholics toward the mission of the church. The canon law of the Latin Church was the first modern Western legal system, and is the oldest continuously functioning legal system in the West. while the distinctive traditions of Eastern Catholic canon law govern the 23 Eastern Catholic particular churches \"sui iuris.\"\nAs the world's oldest and largest continuously functioning international institution, it has played a prominent role in the history and development of Western civilization. The 2,834 sees are grouped into 24 particular autonomous Churches (the largest of which being the Latin Church), each with its own distinct traditions regarding the liturgy and the administering of sacraments. With more than 1.1\u00a0billion baptized members, the Catholic Church is the largest Christian church and represents 50.1% of all Christians as well as 16.7% of the world's population. Catholics live all over the world through missions, diaspora, and conversions.\nEastern Orthodox Church.\nThe Eastern Orthodox Church consists of those churches in communion with the patriarchal sees of the East, such as the Ecumenical Patriarch of Constantinople. Like the Catholic Church, the Eastern Orthodox Church also traces its heritage to the foundation of Christianity through apostolic succession and has an episcopal structure, though the autonomy of its component parts is emphasized, and most of them are national churches.\nEastern Orthodox theology is based on holy tradition which incorporates the dogmatic decrees of the seven Ecumenical Councils, the Scriptures, and the teaching of the Church Fathers. The church teaches that it is the one, holy, catholic and apostolic church established by Jesus Christ in his Great Commission, and that its bishops are the successors of Christ's apostles. It maintains that it practises the original Christian faith, as passed down by holy tradition. Its patriarchates, reminiscent of the pentarchy, and other autocephalous and autonomous churches reflect a variety of hierarchical organisation. It recognizes seven major sacraments, of which the Eucharist is the principal one, celebrated liturgically in synaxis. The church teaches that through consecration invoked by a priest, the sacrificial bread and wine become the body and blood of Christ. The Virgin Mary is venerated in the Eastern Orthodox Church as the \"Theotokos\", meaning God-bearer, and is honoured in devotions.\nEastern Orthodoxy is the second largest single denomination in Christianity, with an estimated 230 million adherents, although Protestants collectively outnumber them, substantially. As one of the oldest surviving religious institutions in the world, the Eastern Orthodox Church has played a prominent role in the history and culture of Eastern and Southeastern Europe, the Caucasus, and the Near East. The majority of Eastern Orthodox Christians live mainly in Southeast and Eastern Europe, Cyprus, Georgia, and parts of the Caucasus region, Siberia, and the Russian Far East. Over half of Eastern Orthodox Christians follow the Russian Orthodox Church, while the vast majority live within Russia. There are also communities in the former Byzantine regions of Africa, the Eastern Mediterranean, and in the Middle East. Eastern Orthodox communities are also present in many other parts of the world, particularly North America, Western Europe, and Australia, formed through diaspora, conversions, and missionary activity.\nOriental Orthodoxy.\nThe Oriental Orthodox Churches (also called \"Old Oriental\" churches) are those eastern churches that recognize the first three ecumenical councils\u2014Nicaea, Constantinople, and Ephesus\u2014but reject the dogmatic definitions of the Council of Chalcedon and instead espouse a Miaphysite christology.\nThe Oriental Orthodox communion consists of six groups: Syriac Orthodox, Coptic Orthodox, Ethiopian Orthodox, Eritrean Orthodox, Malankara Orthodox Syrian Church (India), and Armenian Apostolic churches. These six churches, while being in communion with each other, are completely independent hierarchically. These churches are generally not in communion with the Eastern Orthodox Church, with whom they are in dialogue for erecting a communion. Together, they have about 62 million members worldwide.\nAs some of the oldest religious institutions in the world, the Oriental Orthodox Churches have played a prominent role in the history and culture of Armenia, Egypt, Turkey, Eritrea, Ethiopia, Sudan, Iran, Azerbaijan and parts of the Middle East and India. An Eastern Christian body of autocephalous churches, its bishops are equal by virtue of episcopal ordination, and its doctrines can be summarized in that the churches recognize the validity of only the first three ecumenical councils.\nSome Oriental Orthodox Churches such as the Coptic Orthodox, Ethiopian Orthodox, Eritrean Orthodox, places a heavier emphasis on Old Testament teachings than one might find in other Christian denominations, and its followers adhere to certain practices: following dietary rules that are similar to Jewish Kashrut, require that their male members undergo circumcision, and observes ritual purification.\nChurch of the East.\nThe Church of the East, which was part of the Great Church, shared communion with those in the Roman Empire until the Council of Ephesus condemned Nestorius in 431. Continuing as a \"dhimmi\" community under the Rashidun Caliphate after the Muslim conquest of Persia (633\u2013654), the Church of the East played a major role in the history of Christianity in Asia. Between the 9th and 14th centuries, it represented the world's largest Christian denomination in terms of geographical extent. It established dioceses and communities stretching from the Mediterranean Sea and today's Iraq and Iran, to India (the Saint Thomas Syrian Christians of Kerala), the Mongol kingdoms in Central Asia, and China during the Tang dynasty (7th\u20139th centuries). In the 13th and 14th centuries, the church experienced a final period of expansion under the Mongol Empire, where influential Church of the East clergy sat in the Mongol court.\nThe Assyrian Church of the East, with an unbroken patriarchate established in the 17th century, is an independent Eastern Christian denomination which claims continuity from the Church of the East\u2014in parallel to the Catholic patriarchate established in the 16th century that evolved into the Chaldean Catholic Church, an Eastern Catholic church in full communion with the Pope. It is an Eastern Christian church that follows the traditional christology and ecclesiology of the historical Church of the East. Largely aniconic and not in communion with any other church, it belongs to the eastern branch of Syriac Christianity, and uses the East Syriac Rite in its liturgy.\nIts main spoken language is Syriac, a dialect of Eastern Aramaic, and the majority of its adherents are ethnic Assyrians, mostly living in Iran, Iraq, Syria, Turkey, India (Chaldean Syrian Church), and in the Assyrian diaspora. It is officially headquartered in the city of Erbil in northern Iraqi Kurdistan, and its original area also spreads into south-eastern Turkey and north-western Iran, corresponding to ancient Assyria. Its hierarchy is composed of metropolitan bishops and diocesan bishops, while lower clergy consists of priests and deacons, who serve in dioceses (eparchies) and parishes throughout the Middle East, India, North America, Oceania, and Europe (including the Caucasus and Russia).\nThe Ancient Church of the East distinguished itself from the Assyrian Church of the East in 1964. It is one of the Assyrian churches that claim continuity with the historical Church of the East, one of the oldest Christian churches in Mesopotamia. It is officially headquartered in the city of Baghdad, Iraq. The majority of its adherents are ethnic Assyrians.\nProtestantism.\nIn 1521, the Edict of Worms condemned Martin Luther and officially banned citizens of the Holy Roman Empire from defending or propagating his ideas. This split within the Roman Catholic church is now called the Reformation. Prominent Reformers included Martin Luther, Huldrych Zwingli, and John Calvin. The 1529 Protestation at Speyer against being excommunicated gave this party the name Protestantism. Luther's primary theological heirs are known as Lutherans. Zwingli and Calvin's heirs are far broader denominationally and are referred to as the Reformed tradition. The Anglican churches descended from the Church of England and organized in the Anglican Communion. Some Lutherans identify as Evangelical Catholics and some but not all Anglicans consider themselves both Protestant and Catholic. Protestants have developed their own culture, with major contributions in education, the humanities and sciences, the political and social order, the economy and the arts, and many other fields.\nSince the Anglican, Lutheran, and the Reformed branches of Protestantism originated for the most part in cooperation with the government, these movements are termed the \"Magisterial Reformation\". On the other hand, groups such as the Anabaptists, who often do not consider themselves to be Protestant, originated in the Radical Reformation, which though sometimes protected under \"Acts of Toleration\", do not trace their history back to any state church. They are further distinguished by their rejection of infant baptism; they believe in baptism only of adult believers\u2014credobaptism (Anabaptists include the Amish, Apostolic, Bruderhof, Mennonites, Hutterites, River Brethren and Schwarzenau Brethren groups.)\nThe term \"Protestant\" also refers to any churches which formed later, with either the Magisterial or Radical traditions. In the 18th century, for example, Methodism grew out of Anglican minister John Wesley's evangelical revival movement. Several Pentecostal and non-denominational churches, which emphasize the cleansing power of the Holy Spirit, in turn grew out of Methodism. Because Methodists, Pentecostals and other evangelicals stress \"accepting Jesus as your personal Lord and Savior\", which comes from Wesley's emphasis of the New Birth, they often refer to themselves as being born-again.\nProtestantism is the second largest major group of Christians after Catholicism by number of followers, although the Eastern Orthodox Church is larger than any single Protestant denomination. Estimates vary, mainly over the question of which denominations to classify as Protestant. The total Protestant population has reached 1.17 billion in 2024, corresponding to nearly 44% of the world's Christians. The majority of Protestants are members of just a handful of denominational families, i.e. Adventism, Anabaptism (Amish, Apostolic, Bruderhof, Hutterites, Mennonites, River Brethren, and Schwarzenau Brethren), Anglicanism, Baptists, Lutheranism, Methodism, Moravianism/Hussites, Pentecostalism, Plymouth Brethren, Quakerism, Reformed Christianity (Congregationalists, Continental Reformed, Reformed Anglicans, and Presbyterians), and Waldensianism are the main families of Protestantism. Nondenominational, evangelical, charismatic, neo-charismatic, independent, and other churches are on the rise, and constitute a significant part of Protestant Christianity.\nSome groups of individuals who hold basic Protestant tenets identify themselves as \"Christians\" or \"born-again Christians\". They typically distance themselves from the confessionalism and creedalism of other Christian communities by calling themselves \"non-denominational\" or \"evangelical\". Often founded by individual pastors, they have little affiliation with historic denominations.\nRestorationism.\nThe Second Great Awakening, a period of religious revival that occurred in the United States during the early 1800s, saw the development of a number of unrelated churches. They generally saw themselves as restoring the original church of Jesus Christ rather than reforming one of the existing churches. A common belief held by Restorationists was that the other divisions of Christianity had introduced doctrinal defects into Christianity, which was known as the Great Apostasy. In Asia, is a known Restorationist denomination that was established during the early 1900s. Other examples of Restorationist denominations include Irvingianism and Swedenborgianism.\nSome of the churches originating during this period are historically connected to early 19th-century camp meetings in the Midwest and upstate New York. One of the largest churches produced from the movement is the Church of Jesus Christ of Latter-day Saints. American Millennialism and Adventism, which arose from Evangelical Protestantism, influenced the Jehovah's Witnesses movement and, as a reaction specifically to William Miller, the Seventh-day Adventists. Others, including the Christian Church (Disciples of Christ), Evangelical Christian Church in Canada, Churches of Christ, and the Christian churches and churches of Christ, have their roots in the contemporaneous Stone-Campbell Restoration Movement, which was centered in Kentucky and Tennessee. Other groups originating in this time period include the Christadelphians and the previously mentioned Latter Day Saints movement. While the churches originating in the Second Great Awakening have some superficial similarities, their doctrine and practices vary significantly.\nOther.\nWithin Italy, Poland, Lithuania, Transylvania, Hungary, Romania, and the United Kingdom, Unitarian Churches emerged from the Reformed tradition in the 16th century; the Unitarian Church of Transylvania is an example of such a denomination that arose in this era. They adopted the Anabaptist doctrine of credobaptism.\nVarious smaller Independent Catholic communities, such as the Old Catholic Church, include the word \"Catholic\" in their title, and arguably have more or less liturgical practices in common with the Catholic Church but are no longer in full communion with the Holy See.\nSpiritual Christians, such as the Doukhobors and Molokans, broke from the Russian Orthodox Church and maintain close association with Mennonites and Quakers due to similar religious practices; all of these groups are furthermore collectively considered to be peace churches due to their belief in pacifism.\nMessianic Judaism (or the Messianic Movement) is the name of a Christian movement comprising a number of streams, whose members may consider themselves Jewish. The movement originated in the 1960s and 1970s, and it blends elements of religious Jewish practice with evangelical Christianity. Messianic Judaism affirms Christian creeds such as the messiahship and divinity of \"Yeshua\" (the Hebrew name of Jesus) and the Triune Nature of God, while also adhering to some Jewish dietary laws and customs.\nEsoteric Christians, such as The Christian Community, regard Christianity as a mystery religion and profess the existence and possession of certain esoteric doctrines or practices, hidden from the public and accessible only to a narrow circle of \"enlightened\", \"initiated\", or highly educated people.\nNondenominational Christianity or non-denominational Christianity consists of churches which typically distance themselves from the confessionalism or creedalism of other Christian communities by not formally aligning with a specific Christian denomination. Nondenominational Christianity first arose in the 18th century through the Stone-Campbell Restoration Movement, with followers organizing themselves as \"Christians\" and \"Disciples of Christ\", but many typically adhere to evangelical Christianity.\nCultural influence.\nThe history of the Christendom spans about 1,700 years and includes a variety of socio-political developments, as well as advances in the arts, architecture, literature, science, philosophy, and technology. Since the spread of Christianity from the Levant to Europe and North Africa during the early Roman Empire, Christendom has been divided in the pre-existing Greek East and Latin West. Consequently, different versions of the Christian cultures arose with their own rites and practices, centered around the cities of Rome (Western Christianity) and Carthage, whose communities were called Western or Latin Christendom, and Constantinople (Eastern Christianity), Antioch (Syriac Christianity), Kerala (Indian Christianity) and Alexandria (Coptic Christianity), whose communities were called Eastern or Oriental Christendom. The Byzantine Empire was one of the peaks in Christian history and Eastern Christian civilization. From the 11th to 13th centuries, Latin Christendom rose to the central role of the Western world.\nThe Bible has had a profound influence on Western civilization and on cultures around the globe; it has contributed to the formation of Western law, art, texts, and education. With a literary tradition spanning two millennia, the Bible is one of the most influential works ever written. From practices of personal hygiene to philosophy and ethics, the Bible has directly and indirectly influenced politics and law, war and peace, sexual morals, marriage and family life, toilet etiquette, letters and learning, the arts, economics, social justice, medical care and more.\nChristians have made a myriad of contributions to human progress in a broad and diverse range of fields, including philosophy, science and technology, medicine, fine arts and architecture, politics, literatures, music, and business. According to \"100 Years of Nobel Prizes\" a review of the Nobel Prizes award between 1901 and 2000 reveals that (65.4%) of Nobel Prizes Laureates, have identified Christianity in its various forms as their religious preference.\nOutside the Western world, Christianity has had an influence on various cultures, such as in Africa, the Near East, Middle East, East Asia, Southeast Asia, and the Indian subcontinent. Eastern Christian scientists and scholars of the medieval Islamic world (particularly Jacobite and Nestorian Christians) contributed to the Arab Islamic civilization during the reign of the Ummayyads and the Abbasids, by translating works of Greek philosophers to Syriac and afterwards, to Arabic. They also excelled in philosophy, science, theology, and medicine. Scholars and intellectuals agree Christians in the Middle East have made significant contributions to Arab and Islamic civilization since the introduction of Islam, and they have had a significant impact contributing the culture of the Mashriq, Turkey, and Iran.\nInfluence on Western culture.\nWestern culture, throughout most of its history, has been nearly equivalent to Christian culture, and a large portion of the population of the Western Hemisphere can be described as practicing or nominal Christians. The notion of \"Europe\" and the \"Western World\" has been intimately connected with the concept of \"Christianity and Christendom\". Many historians even attribute Christianity for being the link that created a unified European identity.\nThough Western culture contained several polytheistic religions during its early years under the Greek and Roman Empires, as the centralized Roman power waned, the dominance of the Catholic Church was the only consistent force in Western Europe. Until the Age of Enlightenment, Christian culture guided the course of philosophy, literature, art, music and science. Christian disciplines of the respective arts have subsequently developed into Christian philosophy, Christian art, Christian music, Christian literature, and so on.\nChristianity has had a significant impact on education, as the church created the bases of the Western system of education, and was the sponsor of founding universities in the Western world, as the university is generally regarded as an institution that has its origin in the Medieval Christian setting. Historically, Christianity has often been a patron of science and medicine; many Catholic clergy, Jesuits in particular, have been active in the sciences throughout history and have made significant contributions to the development of science. Some scholars state that Christianity contributed to the rise of the Scientific Revolution. Protestantism also has had an important influence on science. According to the Merton Thesis, there was a positive correlation between the rise of English Puritanism and German Pietism on the one hand, and early experimental science on the other.\nThe civilizing influence of Christianity includes social welfare, contribution to the medical and health care, founding hospitals, economics (as the Protestant work ethic), architecture, literature, personal hygiene (ablution), and family life. Historically, \"extended families\" were the basic family unit in the Christian culture and countries.\nCultural Christians are secular people with a Christian heritage who may not believe in the religious claims of Christianity, but who retain an affinity for the popular culture, art, music, and so on related to the religion.\n\"Postchristianity\" is the term for the decline of Christianity, particularly in Europe, Canada, Australia, and to a minor degree the Southern Cone, in the 20th and 21st centuries, considered in terms of postmodernism. It refers to the loss of Christianity's monopoly on values and world view in historically Christian societies.\nEcumenism.\nChristian groups and denominations have long expressed ideals of being reconciled, and in the 20th century, Christian ecumenism advanced in two ways. One way was greater cooperation between groups, such as the World Evangelical Alliance founded in 1846 in London or the Edinburgh Missionary Conference of Protestants in 1910, the Justice, Peace and Creation Commission of the World Council of Churches founded in 1948 by Protestant and Orthodox churches, and similar national councils like the National Council of Churches in Australia, which includes Catholics.\nThe other way was an institutional union with united churches, a practice that can be traced back to unions between Lutherans and Calvinists in early 19th-century Germany. Congregationalist, Methodist, and Presbyterian churches united in 1925 to form the United Church of Canada, and in 1977 to form the Uniting Church in Australia. The Church of South India was formed in 1947 by the union of Anglican, Baptist, Methodist, Congregationalist, and Presbyterian churches.\nThe Christian Flag is an ecumenical flag designed in the early 20th century to represent all of Christianity and Christendom.\nThe ecumenical, monastic Taiz\u00e9 Community is notable for being composed of more than one hundred brothers from Protestant and Catholic traditions. The community emphasizes the reconciliation of all denominations and its main church, located in Taiz\u00e9, Sa\u00f4ne-et-Loire, France, is named the \"Church of Reconciliation\". The community is internationally known, attracting over 100,000 young pilgrims annually.\nSteps towards reconciliation on a global level were taken in 1965 by the Catholic and Orthodox churches, mutually revoking the excommunications that marked their Great Schism in 1054; the Anglican Catholic International Commission (ARCIC) working towards full communion between those churches since 1970; and some Lutheran and Catholic churches signing the Joint Declaration on the Doctrine of Justification in 1999 to address conflicts at the root of the Protestant Reformation. In 2006, the World Methodist Council, representing all Methodist denominations, adopted the declaration.\nCriticism, persecution, and apologetics.\nCriticism.\nCriticism of Christianity and Christians goes back to the Apostolic Age, with the New Testament recording friction between the followers of Jesus and the Pharisees and scribes (e.g., and ). In the 2nd century, Christianity was criticized by the Jews on various grounds, e.g., that the prophecies of the Hebrew Bible could not have been fulfilled by Jesus, given that he did not have a successful life. Additionally, a sacrifice to remove sins in advance, for everyone or as a human being, did not fit the Jewish sacrifice ritual; furthermore, God in Judaism is said to judge people on their deeds instead of their beliefs. One of the first comprehensive attacks on Christianity came from the Greek philosopher Celsus, who wrote \"The True Word\", a polemic criticizing Christians as being unprofitable members of society. In response, the church father Origen published his treatise \"Contra Celsum\", or \"Against Celsus\", a seminal work of Christian apologetics, which systematically addressed Celsus's criticisms and helped bring Christianity a level of academic respectability.\nBy the 3rd century, criticism of Christianity had mounted. Wild rumors about Christians were widely circulated, claiming that they were atheists and that, as part of their rituals, they devoured human infants and engaged in incestuous orgies. The Neoplatonist philosopher Porphyry wrote the fifteen-volume \"Adversus Christianos\" as a comprehensive attack on Christianity, in part building on the teachings of Plotinus.\nBy the 12th century, the Mishneh Torah (i.e., Rabbi Moses Maimonides) was criticizing Christianity on the grounds of idol worship, in that Christians attributed divinity to Jesus, who had a physical body. In the 19th century, Nietzsche began to write a series of polemics on the \"unnatural\" teachings of Christianity (e.g. sexual abstinence), and continued his criticism of Christianity to the end of his life. In the 20th century, the philosopher Bertrand Russell expressed his criticism of Christianity in \"Why I Am Not a Christian\", formulating his rejection of Christianity.\nCriticism of Christianity continues to date, e.g. Jewish and Muslim theologians criticize the doctrine of the Trinity held by most Christians, stating that this doctrine in effect assumes that there are three gods, running against the basic tenet of monotheism. New Testament scholar Robert M. Price has outlined the possibility that some Bible stories are based partly on myth in \"The Christ Myth Theory and its problems\".\nPersecution.\nChristians are one of the most persecuted religious groups in the world, especially in the Middle-East, North Africa and South and East Asia. In 2017, Open Doors estimated approximately 260 million Christians are subjected annually to \"high, very high, or extreme persecution\" with North Korea considered the most hazardous nation for Christians. In 2019, a report commissioned by the United Kingdom's Secretary of State of the Foreign and Commonwealth Office (FCO) to investigate global persecution of Christians found persecution has increased, and is highest in the Middle East, North Africa, India, China, North Korea, and Latin America, among others, and that it is global and not limited to Islamic states. This investigation found that approximately 80% of persecuted believers worldwide are Christians.\nApologetics.\nChristian apologetics aims to present a rational basis for Christianity. The word \"apologetic\" (Greek: \u1f00\u03c0\u03bf\u03bb\u03bf\u03b3\u03b7\u03c4\u03b9\u03ba\u03cc\u03c2 \"apolog\u0113tikos\") comes from the Greek verb \u1f00\u03c0\u03bf\u03bb\u03bf\u03b3\u03ad\u03bf\u03bc\u03b1\u03b9 \"apologeomai\", meaning \"(I) speak in defense of\". Christian apologetics has taken many forms over the centuries, starting with Paul the Apostle. The philosopher Thomas Aquinas presented five arguments for God's existence in the \"Summa Theologica\", while his \"Summa contra Gentiles\" was a major apologetic work. Another famous apologist, G. K. Chesterton, wrote in the early twentieth century about the benefits of religion and, specifically, Christianity. Famous for his use of paradox, Chesterton explained that while Christianity had the most mysteries, it was the most practical religion. He pointed to the advance of Christian civilizations as proof of its practicality. The physicist and priest John Polkinghorne, in his \"Questions of Truth\", discusses the subject of religion and science, a topic that other Christian apologists such as Ravi Zacharias, John Lennox, and William Lane Craig have engaged, with the latter two men opining that the inflationary Big Bang model is evidence for the existence of God. Creationist apologetics is apologetics that aims to defend creationism."}
{"id": "5213", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5213", "title": "Computing", "text": "Computing is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes the study and experimentation of algorithmic processes, and the development of both hardware and software. Computing has scientific, engineering, mathematical, technological, and social aspects. Major computing disciplines include computer engineering, computer science, cybersecurity, data science, information systems, information technology, and software engineering.\nThe term \"computing\" is also synonymous with counting and calculating. In earlier times, it was used in reference to the action performed by mechanical computing machines, and before that, to human computers.\nHistory.\nThe history of computing is longer than the history of computing hardware and includes the history of methods intended for pen and paper (or for chalk and slate) with or without the aid of tables. Computing is intimately tied to the representation of numbers, though mathematical concepts necessary for computing existed before numeral systems. The earliest known tool for use in computation is the abacus, and it is thought to have been invented in Babylon circa between 2700 and 2300 BC. Abaci, of a more modern design, are still used as calculation tools today.\nThe first recorded proposal for using digital electronics in computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by C. E. Wynn-Williams. Claude Shannon's 1938 paper \"A Symbolic Analysis of Relay and Switching Circuits\" then introduced the idea of using electronics for Boolean algebraic operations.\nThe concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947. In 1953, the University of Manchester built the first transistorized computer, the Manchester Baby. However, early junction transistors were relatively bulky devices that were difficult to mass-produce, which limited them to a number of specialised applications. \nIn 1957, Frosch and Derick were able to manufacture the first silicon dioxide field effect transistors at Bell Labs, the first transistors in which drain and source were adjacent at the surface. Subsequently, a team demonstrated a working MOSFET at Bell Labs 1960. The MOSFET made it possible to build high-density integrated circuits, leading to what is known as the computer revolution or microcomputer revolution.\nComputer.\nA computer is a machine that manipulates data according to a set of instructions called a computer program. The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop a sequence of steps known as an algorithm. Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the CPU type.\nThe execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions.\nComputer hardware.\nComputer hardware includes the physical parts of a computer, including the central processing unit, memory, and input/output. Computational logic and computer architecture are key topics in the field of computer hardware.\nComputer software.\nComputer software, or just \"software\", is a collection of computer programs and related data, which provides instructions to a computer. Software refers to one or more computer programs and data held in the storage of the computer. It is a set of \"programs, procedures, algorithms,\" as well as its \"documentation\" concerned with the operation of a data processing system. Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term \"hardware\" (meaning physical devices). In contrast to hardware, software is intangible.\nSoftware is also sometimes used in a more narrow sense, meaning application software only.\nSystem software.\nSystem software, or systems software, is computer software designed to operate and control computer hardware, and to provide a platform for running application software. System software includes operating systems, utility software, device drivers, window systems, and firmware. Frequently used development tools such as compilers, linkers, and debuggers are classified as system software. System software and middleware manage and integrate a computer's capabilities, but typically do not directly apply them in the performance of tasks that benefit the user, unlike application software.\nApplication software.\nApplication software, also known as an \"application\" or an \"app\", is computer software designed to help the user perform specific tasks. Examples include enterprise software, accounting software, office suites, graphics software, and media players. Many application programs deal principally with documents. Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install additional applications. The system software manages the hardware and serves the application, which in turn serves the user.\nApplication software applies the power of a particular computing platform or system software to a particular purpose. Some apps, such as Microsoft Office, are developed in multiple versions for several different platforms; others have narrower requirements and are generally referred to by the platform they run on. For example, a \"geography application for Windows\" or an \"Android application for education\" or \"Linux gaming\". Applications that run only on one platform and increase the desirability of that platform due to the popularity of the application, known as killer applications.\nComputer network.\nA computer network, often simply referred to as a network, is a collection of hardware components and computers interconnected by communication channels that allow the sharing of resources and information. When at least one process in one device is able to send or receive data to or from at least one process residing in a remote device, the two devices are said to be in a network. Networks may be classified according to a wide variety of characteristics such as the medium used to transport the data, communications protocol used, scale, topology, and organizational scope.\nCommunications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for network programming. One well-known communications protocol is Ethernet, a hardware and link layer standard that is ubiquitous in local area networks. Another common protocol is the Internet Protocol Suite, which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, host-to-host data transfer, and application-specific data transmission formats.\nComputer networking is sometimes considered a sub-discipline of electrical engineering, telecommunications, computer science, information technology, or computer engineering, since it relies upon the theoretical and practical application of these disciplines.\nInternet.\nThe Internet is a global system of interconnected computer networks that use the standard Internet Protocol Suite (TCP/IP) to serve billions of users. This includes millions of private, public, academic, business, and government networks, ranging in scope from local to global. These networks are linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web and the infrastructure to support email.\nComputer programming.\nComputer programming is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a programming language, which is an artificial language that is often more restrictive than natural languages, but easily translated by the computer. Programming is used to invoke some desired behavior (customization) from the machine.\nWriting high-quality source code requires knowledge of both the computer science domain and the domain in which the application will be used. The highest-quality software is thus often developed by a team of domain experts, each a specialist in some area of development. However, the term \"programmer\" may apply to a range of program quality, from hacker to open source contributor to professional. It is also possible for a single programmer to do most or all of the computer programming needed to generate the proof of concept to launch a new killer application.\nComputer programmer.\nA programmer, computer programmer, or coder is a person who writes computer software. The term \"computer programmer\" can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (C, C++, Java, Lisp, Python, etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with \"Web\". The term \"programmer\" can be used to refer to a software developer, software engineer, computer scientist, or software analyst. However, members of these professions typically possess other software engineering skills, beyond programming.\nComputer industry.\nThe computer industry is made up of businesses involved in developing computer software, designing computer hardware and computer networking infrastructures, manufacturing computer components, and providing information technology services, including system administration and maintenance.\nThe software industry includes businesses engaged in development, maintenance, and publication of software. The industry also includes software services, such as training, documentation, and consulting.\nSub-disciplines of computing.\nComputer engineering.\nComputer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration, rather than just software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering includes not only the design of hardware within its own domain, but also the interactions between hardware and the context in which it operates.\nSoftware engineering.\nSoftware engineering is the application of a systematic, disciplined, and quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches. That is, the application of engineering to software. It is the act of using insights to conceive, model and scale a solution to a problem. The first reference to the term is the 1968 NATO Software Engineering Conference, and was intended to provoke thought regarding the perceived \"software crisis\" at the time. Software development, a widely used and more generic term, does not necessarily subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the Software Engineering Body of Knowledge (SWEBOK). The SWEBOK has become an internationally accepted standard in ISO/IEC TR 19759:2015.\nComputer science.\nComputer science or computing science (abbreviated CS or Comp Sci) is the scientific and practical approach to computation and its applications. A computer scientist specializes in the theory of computation and the design of computational systems.\nIts subfields can be divided into practical techniques for its implementation and application in computer systems, and purely theoretical areas. Some, such as computational complexity theory, which studies fundamental properties of computational problems, are highly abstract, while others, such as computer graphics, emphasize real-world applications. Others focus on the challenges in implementing computations. For example, programming language theory studies approaches to the description of computations, while the study of computer programming investigates the use of programming languages and complex systems. The field of human\u2013computer interaction focuses on the challenges in making computers and computations useful, usable, and universally accessible to humans. \nCybersecurity.\nThe field of cybersecurity pertains to the protection of computer systems and networks. This includes information and data privacy, preventing disruption of IT services and prevention of theft of and damage to hardware, software, and data.\nData science.\nData science is a field that uses scientific and computing tools to extract information and insights from data, driven by the increasing volume and availability of data. Data mining, big data, statistics, machine learning and deep learning are all interwoven with data science.\nInformation systems.\nInformation systems (IS) is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute data. The ACM's \"Computing Careers\" describes IS as:\nThe study of IS bridges business and computer science, using the theoretical foundations of information and computation to study various business models and related algorithmic processes within a computer science discipline. The field of Computer Information Systems (CIS) studies computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society while IS emphasizes functionality over design.\nInformation technology.\nInformation technology (IT) is the application of computers and telecommunications equipment to store, retrieve, transmit, and manipulate data, often in the context of a business or other enterprise. The term is commonly used as a synonym for computers and computer networks, but also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, e-commerce, and computer services.\nResearch and emerging technologies.\nDNA-based computing and quantum computing are areas of active research for both computing hardware and software, such as the development of quantum algorithms. Potential infrastructure for future technologies includes DNA origami on photolithography and quantum antennae for transferring information between ion traps. By 2011, researchers had entangled 14 qubits. Fast digital circuits, including those based on Josephson junctions and rapid single flux quantum technology, are becoming more nearly realizable with the discovery of nanoscale superconductors.\nFiber-optic and photonic (optical) devices, which already have been used to transport data over long distances, are starting to be used by data centers, along with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. IBM has created an integrated circuit with both electronic and optical information processing in one chip. This is denoted CMOS-integrated nanophotonics (CINP). One benefit of optical interconnects is that motherboards, which formerly required a certain kind of system on a chip (SoC), can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs.\nAnother field of research is spintronics. Spintronics can provide computing power and storage, without heat buildup. Some research is being done on hybrid chips, which combine photonics and spintronics. There is also research ongoing on combining plasmonics, photonics, and electronics.\nCloud computing.\nCloud computing is a model that allows for the use of computing resources, such as servers or applications, without the need for interaction between the owner of these resources and the end user. It is typically offered as a service, making it an example of Software as a Service, Platforms as a Service, and Infrastructure as a Service, depending on the functionality offered. Key characteristics include on-demand access, broad network access, and the capability of rapid scaling. It allows individual users or small business to benefit from economies of scale.\nOne area of interest in this field is its potential to support energy efficiency. Allowing thousands of instances of computation to occur on one single machine instead of thousands of individual machines could help save energy. It could also ease the transition to renewable energy source, since it would suffice to power one server farm with renewable energy, rather than millions of homes and offices.\nHowever, this centralized computing model poses several challenges, especially in security and privacy. Current legislation does not sufficiently protect users from companies mishandling their data on company servers. This suggests potential for further legislative regulations on cloud computing and tech companies.\nQuantum computing.\nQuantum computing is an area of research that brings together the disciplines of computer science, information theory, and quantum physics. While the idea of information as part of physics is relatively new, there appears to be a strong tie between information theory and quantum mechanics. Whereas traditional computing operates on a binary system of ones and zeros, quantum computing uses qubits. Qubits are capable of being in a superposition, i.e. in both states of one and zero, simultaneously. Thus, the value of the qubit is not between 1 and 0, but changes depending on when it is measured. This trait of qubits is known as quantum entanglement, and is the core idea of quantum computing that allows quantum computers to do large scale computations. Quantum computing is often used for scientific research in cases where traditional computers do not have the computing power to do the necessary calculations, such in molecular modeling. Large molecules and their reactions are far too complex for traditional computers to calculate, but the computational power of quantum computers could provide a tool to perform such calculations."}
{"id": "5215", "revid": "196446", "url": "https://en.wikipedia.org/wiki?curid=5215", "title": "Casino", "text": "A casino is a facility for certain types of gambling. Casinos are often built near or combined with hotels, resorts, restaurants, retail shops, cruise ships, and other tourist attractions. Some casinos are also known for hosting live entertainment, such as stand-up comedy, concerts, and sports.\nand usage.\n\"Casino\" is of Italian origin; the root means a house. The term \"casino\" may mean a small country villa, summerhouse, or social club. During the 19th century, \"casino\" came to include other public buildings where pleasurable activities took place; such edifices were usually built on the grounds of a larger Italian villa or palazzo, and were used to host civic town functions, including dancing, gambling, music listening, and sports. Examples in Italy include Villa Farnese and Villa Giulia, and in the US the Newport Casino in Newport, Rhode Island. In modern-day Italian, a is a brothel (also called , literally \"closed house\"), a mess (confusing situation), or a noisy environment; a gaming house is spelt , with an accent.\nNot all casinos are used for gaming. The Catalina Casino, on Santa Catalina Island, California, has never been used for traditional games of chance, which were already outlawed in California by the time it was built. The Copenhagen Casino was a Danish theatre which also held public meetings during the 1848 Revolution, which made Denmark a constitutional monarchy.\nIn military and non-military usage, a (Spanish) or (German) is an officers' mess.\nHistory of gambling houses.\nThe precise origin of gambling is unknown. It is generally believed that gambling in some form or another has been seen in almost every society in history. From Ancient Mesopotamia, Greeks and Romans to Napoleon's France and Elizabethan England, much of history is filled with stories of entertainment based on games of chance.\nThe first known European gambling house, not called a casino although meeting the modern definition, was the Ridotto, established in Venice, Italy, in 1638 by the Great Council of Venice to provide controlled gambling during the carnival season. It was closed in 1774 as the city government felt it was impoverishing the local gentry.\nIn American history, early gambling establishments were known as saloons. The creation and importance of saloons was greatly influenced by four major cities: New Orleans, St. Louis, Chicago and San Francisco. It was in the saloons that travelers could find people to talk to, drink with, and often gamble with. During the early 20th century in the US, gambling was outlawed by state legislation. However, in 1931, gambling was legalized throughout the state of Nevada, where the US's first legalized casinos were set up. In 1976 New Jersey allowed gambling in Atlantic City, now the US's second largest gambling city.\nGambling in casinos.\nMost jurisdictions worldwide have a minimum gambling age of 18 to 21.\nCustomers gamble by playing games of chance, in some cases with an element of skill, such as craps, roulette, baccarat, blackjack, and video poker. Most games have mathematically determined odds that ensure the house has at all times an advantage over the players. This can be expressed more precisely by the notion of expected value, which is uniformly negative (from the player's perspective). This advantage is called the \"house edge\". In games such as poker where players play against each other, the house takes a commission called the rake. Casinos sometimes give out complimentary items or comps to gamblers.\n\"Payout\" is the percentage of funds (\"winnings\") returned to players.\nCasinos in the United States say that a player staking money won from the casino is \"playing with the house's money\".\nVideo lottery machines (slot machines) have become one of the most popular forms of gambling in casinos. investigative reports have started calling into question whether the modern-day slot-machine is addictive.\nDesign.\nFactors influencing gambling tendencies include sound, odour and lighting. Natasha Dow Sch\u00fcll, an anthropologist at the Massachusetts Institute of Technology, highlights the decision of the audio directors at Silicon Gaming to make its slot machines resonate in \"the universally pleasant tone of C, sampling existing casino soundscapes to create a sound that would please but not clash\".\nAlan Hirsch, founder of the Smell &amp; Taste Treatment and Research Foundation in Chicago, studied the impact of certain scents on gamblers, discerning that a pleasant albeit unidentifiable odor released by Las Vegas slot machines generated about 50% more in daily revenue. He suggested that the scent acted as an aphrodisiac, causing a more aggressive form of gambling.\nMarkets.\nThe following lists major casino markets in the world with casino revenue of over US$1 billion as published in PricewaterhouseCoopers's report on\nthe outlook for the global casino market:\nBy company.\nAccording to Bloomberg, accumulated revenue of the biggest casino operator companies worldwide amounted to almost US$55 billion in 2011. SJM Holdings Ltd. was the leading company in this field, earning $9.7 bn in 2011, followed by Las Vegas Sands Corp. at $7.4 bn. The third-biggest casino operator company (based on revenue) was Caesars Entertainment, with revenue of US$6.2 bn.\nSignificant sites.\nWhile there are casinos in many places, a few places have become well known specifically for gambling. Perhaps the place almost defined by its casino is Monte Carlo, but other places are known as gambling centers.\nMonte Carlo, Monaco.\nMonte Carlo Casino, located in Monte Carlo city, in Monaco, is a casino and a tourist attraction.\nMonte Carlo Casino has been depicted in many books, including Ben Mezrich's \"Busting Vegas\", where a group of Massachusetts Institute of Technology students beat the casino out of nearly $1 million. This book is based on real people and events; however, many of those events are contested by main character Semyon Dukach. Monte Carlo Casino has also been featured in multiple James Bond novels and films.\nThe casino is mentioned in the song \"The Man Who Broke the Bank at Monte Carlo\" as well as the film of the same name.\nCampione d'Italia.\nCasin\u00f2 di Campione is located in the tiny Italian enclave of Campione d'Italia, within Ticino, Switzerland. The casino was founded in 1917 as a site to gather information from foreign diplomats during the First World War. Today it is owned by the Italian government, and operated by the municipality. With gambling laws being less strict than in Italy and Switzerland, it is among the most popular gambling destination besides Monte Carlo. The income from the casino is sufficient for the operation of Campione without the imposition of taxes, or obtaining of other revenue. In 2007, the casino moved into new premises of more than , making it the largest casino in Europe. The new casino was built alongside the old one, which dated from 1933 and has since been demolished.\nMalta.\nThe archipelago of Malta is a particularly famous place for casinos, standing out mainly with the historic casino located at the princely residence of Dragonara. Dragonara Palace was built in 1870. Its name comes from the Dragonara Point, the peninsula where it is built. On 15 July 1964, the palace opened as a casino.\nMacau.\nThe former Portuguese colony of Macau, a special administrative region of the People's Republic of China since 1999, is a popular destination for visitors who wish to gamble. This started in Portuguese times, when Macau was popular with visitors from nearby Hong Kong, where gambling was more closely regulated. The Venetian Macao is currently the largest casino in the world. Macau also surpassed Las Vegas as the largest gambling market in the world.\nGermany.\nMachine-based gaming is only permitted in land-based casinos, restaurants, bars and gaming halls, and only subject to a licence. Online slots are, at the moment, only permitted if they are operated under a Schleswig-Holstein licence. AWPs are governed by federal law \u2013 the Trade Regulation Act and the Gaming Ordinance.\nPortugal.\nThe Casino Estoril, located in the municipality of Cascais, on the Portuguese Riviera, near Lisbon, is the largest casino in Europe by capacity.\nDuring the World War II, it was reputed to be a gathering point for spies, dispossessed royals, and wartime adventurers; it became an inspiration for Ian Fleming's James Bond 007 novel \"Casino Royale\".\nRussia.\nThere are four legal gaming zones in Russia: \"Siberian Coin\" (Altay), \"Yantarnaya\" (Kaliningrad region), \"Azov-city\" (Rostov region) and \"Primorie\" (Primorie region).\nSingapore.\nSingapore is an up-and-coming destination for visitors wanting to gamble, although there are currently only two casinos (both foreign owned), in Singapore. The Marina Bay Sands is the second most expensive standalone casino in the world, at a price of US$6.8 billion, and is among the world's ten most expensive buildings.\nUnited States.\nWith currently over 1,000 casinos, the United States has the largest number of casinos in the world. The number continues to grow steadily as more states seek to legalize casinos. 40 states now have some form of casino gambling. Interstate competition, such as gaining tourism, has been a driving factor to continuous legalization. Relatively small places such as Las Vegas are best known for gambling; larger cities such as Chicago are not defined by their casinos in spite of the large turnover.\nThe Las Vegas Valley has the largest concentration of casinos in the United States. Based on revenue, Atlantic City, New Jersey, ranks second, and the Chicago region third.\nTop American casino markets by revenue (2022 annual revenues):\nThe Nevada Gaming Control Board divides Clark County, which is coextensive with the Las Vegas metropolitan area, into seven market regions for reporting purposes.\nNative American gaming has been responsible for a rise in the number of casinos outside of Las Vegas and Atlantic City.\nVietnam.\nIn Vietnam, the term \"casinos\" encompasses gambling activities within the country. Unofficially defined, a \"casino\" typically denotes a well-established and professional gambling establishment that is generally lawful but exclusively caters to foreign players. On the other hand, \"gambling houses\" or \"gambling dens\" are smaller, illicit gambling venues.\nAs of 2022, Vietnam has 9 operating casinos, including: \u0110\u1ed3 S\u01a1n casino (H\u1ea3i Ph\u00f2ng), L\u1ee3i Lai casino, Ho\u00e0ng Gia casino (Qu\u1ea3ng Ninh), H\u1ed3ng V\u1eadn casino (Qu\u1ea3ng Ninh), L\u00e0o Cai casino (L\u00e0o Cai), Silver Shores casino (\u0110\u00e0 N\u1eb5ng), H\u1ed3 Tr\u00e0m casino (B\u00e0 R\u1ecba \u2013 V\u0169ng T\u00e0u), Nam H\u1ed9i An casino (Qu\u1ea3ng Nam), Ph\u00fa Qu\u1ed1c casino in Kien Giang.\nIn a survey conducted by the Vietnamese Institute for Sustainable Regional Development and released on September 30, 2015, it was found that 71% of the respondents held the belief that allowing Vietnamese individuals to access casinos would result in an increase in the number of players. Furthermore, 47.4% of the participants expressed the view that engaging in rewarding recreational activities has a positive impact on job opportunities for residents. Additionally, 46.2% of the respondents believed that such activities contribute positively to Vietnam's ability to attract investments.\nSecurity.\nGiven the large amounts of currency handled within a casino, both patrons and staff may be tempted to cheat and steal, in collusion or independently; most casinos have security measures to prevent this. Security cameras located throughout the casino are the most basic measure.\nModern casino security is usually divided between a physical security force and a specialized surveillance department. The physical security force usually patrols the casino and responds to calls for assistance and reports of suspicious or definite criminal activity. A specialized surveillance department operates the casino's closed-circuit-television system, known in the industry as the eye in the sky. Both of these specialized casino security departments work very closely with each other to ensure the safety of both guests and the casino's assets, and have been quite successful in preventing crime. Some casinos also have catwalks in the ceiling above the casino floor, which allow surveillance personnel to look directly down, through one way glass, on the activities at the tables and slot machines.\nWhen it opened in 1989, The Mirage was the first casino to use cameras full-time on all table games.\nIn addition to cameras and other technological measures, casinos also enforce security through rules of conduct and behavior; for example, players at card games are required to keep the cards they are holding in their hands visible at all times.\nBusiness practices.\nOver the past few decades, casinos have developed many different marketing techniques for attracting and maintaining loyal patrons. Many casinos use a loyalty rewards program used to track players' spending habits and target their patrons more effectively, by sending mailings with free slot play and other promotions. Casino Helsinki in Helsinki, Finland, for example, donates all of its profits to charity.\nCrime.\nCasinos have been linked to organised crime, with early casinos in Las Vegas originally dominated by the American Mafia and in Macau by Triad syndicates.\nAccording to some police reports, local incidence of reported crime often doubles or triples within three years of a casino's opening. In a 2004 report by the US Department of Justice, researchers interviewed people who had been arrested in Las Vegas and Des Moines and found that the percentage of problem or pathological gamblers among the arrestees was three to five times higher than in the general population.\nIt has been said that economic studies showing a positive relationship between casinos and crime usually fail to consider the visiting population: they count crimes committed by visitors but do not count visitors in the population measure, which overstates the crime rate. Part of the reason this methodology is used, despite the overstatement, is that reliable data on tourist count are often not available.\nOccupational health and safety.\nCasinos are subject to specific regulations for worker safety, as casino employees are both at greater risk for cancers resulting from exposure to second-hand tobacco smoke and musculoskeletal injuries from repetitive motions while running table games over many hours. These are not the extent of the hazards casino workers may be exposed to, but every location is different. Most casinos do not meet the requirements for certain protective measures, such as protection against airborne metal dusts from coins or hearing protection against high noise levels, though these measures are still implemented when evaluated and determined necessary."}
{"id": "5216", "revid": "29463730", "url": "https://en.wikipedia.org/wiki?curid=5216", "title": "Khmer language", "text": "Khmer ( ; , UNGEGN: ) is an Austroasiatic language spoken natively by the Khmer people. This language is an official language and national language of Cambodia. The language is also widely spoken by Khmer people in Eastern Thailand and Isan, Thailand, also in Southeast and Mekong Delta of Vietnam. \nKhmer has been influenced considerably by Sanskrit and Pali especially in the royal and religious registers, through Hinduism and Buddhism, due to Old Khmer being the language of the historical empires of Chenla and Angkor. \nThe vast majority of Khmer speakers speak \"Central Khmer\", the dialect of the central plain where the Khmer are most heavily concentrated. Within Cambodia, regional accents exist in remote areas but these are regarded as varieties of Central Khmer. Two exceptions are the speech of the capital, Phnom Penh, and that of the Khmer Khe in Stung Treng province, both of which differ sufficiently enough from Central Khmer to be considered separate dialects of Khmer.\nOutside of Cambodia, three distinct dialects are spoken by ethnic Khmers native to areas that were historically part of the Khmer Empire. The Northern Khmer dialect is spoken by over a million Khmers in the southern regions of Northeast Thailand and is treated by some linguists as a separate language. Khmer Krom, or Southern Khmer, is the first language of the Khmer of Vietnam, while the Khmer living in the remote Cardamom Mountains speak a very conservative dialect that still displays features of the Middle Khmer language.\nKhmer is primarily an analytic, isolating language. There are no inflections, conjugations or case endings. Instead, particles and auxiliary words are used to indicate grammatical relationships. General word order is subject\u2013verb\u2013object, and modifiers follow the word they modify. Classifiers appear after numbers when used to count nouns, though not always so consistently as in languages like Chinese. In spoken Khmer, topic-comment structure is common, and the perceived social relation between participants determines which sets of vocabulary, such as pronouns and honorifics, are proper.\nKhmer differs from neighboring languages such as Burmese, Thai, Lao, and Vietnamese in that it is not a tonal language. Words are stressed on the final syllable, hence many words conform to the typical Mon\u2013Khmer pattern of a stressed syllable preceded by a minor syllable. The language has been written in the Khmer script, an abugida descended from the Brahmi script via the southern Indian Pallava script, since at least the 7th century. The script's form and use has evolved over the centuries; its modern features include subscripted versions of consonants used to write clusters and a division of consonants into two series with different inherent vowels.\nClassification.\nKhmer is a member of the Austroasiatic language family, the autochthonous family in an area that stretches from the Malay Peninsula through Southeast Asia to East India. Austroasiatic, which also includes Mon, Vietnamese and Munda, has been studied since 1856 and was first proposed as a language family in 1907. Despite the amount of research, there is still doubt about the internal relationship of the languages of Austroasiatic.\nDiffloth places Khmer in an eastern branch of the Mon-Khmer languages. In these classification schemes Khmer's closest genetic relatives are the Bahnaric and Pearic languages. More recent classifications doubt the validity of the Mon-Khmer sub-grouping and place the Khmer language as its own branch of Austroasiatic equidistant from the other 12 branches of the family.\nGeographic distribution and dialects.\nKhmer is spoken by some 13 million people in Cambodia, where it is the official language. It is also a second language for most of the minority groups and indigenous hill tribes there. Additionally there are a million speakers of Khmer native to southern Vietnam (1999 census) and 1.4 million in northeast Thailand (2006).\nKhmer dialects, although mutually intelligible, are sometimes quite marked. Notable variations are found in speakers from Phnom Penh (Cambodia's capital city), the rural Battambang area, the areas of Northeast Thailand adjacent to Cambodia such as Surin province, the Cardamom Mountains, and southern Vietnam. The dialects form a continuum running roughly north to south. Standard Cambodian Khmer is mutually intelligible with the others but a Khmer Krom speaker from Vietnam, for instance, may have great difficulty communicating with a Khmer native of Sisaket Province in Thailand.\nThe following is a classification scheme showing the development of the modern Khmer dialects.\nStandard Khmer, or Central Khmer, the language as taught in Cambodian schools and used by the media, is based on the dialect spoken throughout the Central Plain, a region encompassed by the northwest and central provinces.\nNorthern Khmer (called in Khmer) refers to the dialects spoken by many in several border provinces of present-day northeast Thailand. After the fall of the Khmer Empire in the early 15th century, the Dongrek Mountains served as a natural border leaving the Khmer north of the mountains under the sphere of influence of the Kingdom of Lan Xang. The conquests of Cambodia by Naresuan the Great for Ayutthaya furthered their political and economic isolation from Cambodia proper, leading to a dialect that developed relatively independently from the midpoint of the Middle Khmer period.\nThis has resulted in a distinct accent influenced by the surrounding tonal languages Lao and Thai, lexical differences, and phonemic differences in both vowels and distribution of consonants. Syllable-final , which has become silent in other dialects of Khmer, is still pronounced in Northern Khmer. Some linguists classify Northern Khmer as a separate but closely related language rather than a dialect.\nWestern Khmer, also called Cardamom Khmer or Chanthaburi Khmer, is spoken by a very small, isolated population in the Cardamom mountain range extending from western Cambodia into eastern Central Thailand. Although little studied, this variety is unique in that it maintains a definite system of vocal register that has all but disappeared in other dialects of modern Khmer.\nPhnom Penh Khmer is spoken in the capital and surrounding areas. This dialect is characterized by merging or complete elision of syllables, which speakers from other regions consider a \"relaxed\" pronunciation. For instance, \"Phnom Penh\" is sometimes shortened to \"m'Penh\". Another characteristic of Phnom Penh speech is observed in words with an \"r\" either as an initial consonant or as the second member of a consonant cluster (as in the English word \"bread\"). The \"r\", trilled or flapped in other dialects, is either pronounced as a uvular trill or not pronounced at all.\nThis alters the quality of any preceding consonant, causing a harder, more emphasized pronunciation. Another unique result is that the syllable is spoken with a low-rising or \"dipping\" tone much like the \"h\u1ecfi\" tone in Vietnamese. For example, some people pronounce ('fish') as : the is dropped and the vowel begins by dipping much lower in tone than standard speech and then rises, effectively doubling its length. Another example is the word ('study'), which is pronounced , with the uvular \"r\" and the same intonation described above.\nKhmer Krom or Southern Khmer is spoken by the indigenous Khmer population of the Mekong Delta, formerly controlled by the Khmer Empire but part of Vietnam since 1698. Khmers are persecuted by the Vietnamese government for using their native language and, since the 1950s, have been forced to take Vietnamese names. Consequently, very little research has been published regarding this dialect. It has been generally influenced by Vietnamese for three centuries and accordingly displays a pronounced accent, tendency toward monosyllabic words and lexical differences from Standard Khmer.\nKhmer Khe is spoken in the Se San, Srepok and Sekong river valleys of Sesan and Siem Pang districts in Stung Treng Province. Following the decline of Angkor, the Khmer abandoned their northern territories, which the Lao then settled. In the 17th century, Chey Chetha XI led a Khmer force into Stung Treng to retake the area. The Khmer Khe living in this area of Stung Treng in modern times are presumed to be the descendants of this group. Their dialect is thought to resemble that of pre-modern Siem Reap.\nHistorical periods.\nLinguistic study of the Khmer language divides its history into four periods one of which, the Old Khmer period, is subdivided into pre-Angkorian and Angkorian. Pre-Angkorian Khmer is the Old Khmer language from 600 CE through 800. Angkorian Khmer is the language as it was spoken in the Khmer Empire from the 9th century until the 13th century.\nThe following centuries saw changes in morphology, phonology and lexicon. The language of this transition period, from about the 14th to 18th centuries, is referred to as Middle Khmer and saw borrowings from Thai in the literary register. Modern Khmer is dated from the 19th century to today.\nThe following table shows the conventionally accepted historical stages of Khmer.\nJust as modern Khmer was emerging from the transitional period represented by Middle Khmer, Cambodia fell under the influence of French colonialism. Thailand, which had for centuries claimed suzerainty over Cambodia and controlled succession to the Cambodian throne, began losing its influence on the language. In 1887 Cambodia was fully integrated into French Indochina, which brought in a French-speaking aristocracy. This led to French becoming the language of higher education and the intellectual class. By 1907, the French had wrested over half of modern-day Cambodia, including the north and northwest where Thai had been the prestige language, back from Thai control and reintegrated it into the country.\nMany native scholars in the early 20th century, led by a monk named Chuon Nath, resisted the French and Thai influences on their language. Forming the government sponsored Cultural Committee to define and standardize the modern language, they championed Khmerization, purging of foreign elements, reviving affixation, and the use of Old Khmer roots and historical Pali and Sanskrit to coin new words for modern ideas. Opponents, led by Keng Vannsak, who embraced \"total Khmerization\" by denouncing the reversion to classical languages and favoring the use of contemporary colloquial Khmer for neologisms, and Ieu Koeus, who favored borrowing from Thai, were also influential.\nKoeus later joined the Cultural Committee and supported Nath. Nath's views and prolific work won out and he is credited with cultivating modern Khmer-language identity and culture, overseeing the translation of the entire Pali Buddhist canon into Khmer. He also created the modern Khmer language dictionary that is still in use today, helping preserve Khmer during the French colonial period.\nPhonology.\nThe phonological system described here is the inventory of sounds of the standard spoken language, represented using the International Phonetic Alphabet (IPA).\nConsonants.\nThe voiceless plosives may occur with or without aspiration (as vs. , etc.); this difference is contrastive before a vowel. However, the aspirated sounds in that position may be analyzed as sequences of two phonemes: . This analysis is supported by the fact that infixes can be inserted between the stop and the aspiration; for example ('big') becomes ('size') with a nominalizing infix. When one of these plosives occurs initially before another consonant, aspiration is no longer contrastive and can be regarded as mere phonetic detail: slight aspiration is expected when the following consonant is not one of (or if the initial plosive is ).\nThe voiced plosives are pronounced as implosives by most speakers, but this feature is weak in educated speech, where they become .\nIn syllable-final position, and approach and respectively. The stops are unaspirated and have no audible release when occurring as syllable finals.\nIn addition, the consonants , , and occur occasionally in recent loan words in the speech of Cambodians familiar with French and other languages.\nVowels.\nVarious authors have proposed slightly different analyses of the Khmer vowel system. This may be in part because of the wide degree of variation in pronunciation between individual speakers, even within a dialectal region. The description below follows Huffman (1970). The number of vowel nuclei and their values vary between dialects; differences exist even between the Standard Khmer system and that of the Battambang dialect on which the standard is based.\nIn addition, some diphthongs and triphthongs are analyzed as a vowel nucleus plus a semivowel ( or ) coda because they cannot be followed by a final consonant. These include: (with short monophthongs) , , , , ; (with long monophthongs) , ; (with long diphthongs) , , , , and .\nThe independent vowels are the vowels that can exist without a preceding or trailing consonant. The independent vowels may be used as monosyllabic words, or as the initial syllables in longer words. Khmer words never begin with regular vowels; they can, however, begin with independent vowels. Example: \u17b0\u178a\u17cf, \u17a7\u1791\u17b6\u17a0\u179a\u178e\u17cd, \u17a7\u178f\u17d2\u178f\u1798, \u17b1\u1780\u17b6\u179f...\u17d4\nSyllable structure.\nA Khmer syllable begins with a single consonant, or else with a cluster of two, or rarely three, consonants. The only possible clusters of three consonants at the start of a syllable are , and (with aspirated consonants analyzed as two-consonant sequences) . There are 85 possible two-consonant clusters (including [p\u02b0] etc. analyzed as etc.). All the clusters are shown in the following table, phonetically, i.e. superscript can mark either contrastive or non-contrastive aspiration (see above).\nSlight vowel epenthesis occurs in the clusters consisting of a plosive followed by , in those beginning , and in the cluster .\nAfter the initial consonant or consonant cluster comes the syllabic nucleus, which is one of the vowels listed above. This vowel may end the syllable or may be followed by a coda, which is a single consonant. If the syllable is stressed and the vowel is short, there must be a final consonant. All consonant sounds except and the aspirates can appear as the coda (although final is heard in some dialects, most notably in Northern Khmer).\nA minor syllable (unstressed syllable preceding the main syllable of a word) has a structure of CV-, CrV-, CVN- or CrVN- (where C is a consonant, V a vowel, and N a nasal consonant). The vowels in such syllables are usually short; in conversation they may be reduced to , although in careful or formal speech, including on television and radio, they are clearly articulated. An example of such a word is \"m\u0254nuh, m\u0254n\u0268h, m\u0115\u0259\u02benuh\" ('person'), pronounced , or more casually .\nStress.\nStress in Khmer falls on the final syllable of a word. Because of this predictable pattern, stress is non-phonemic in Khmer (it does not distinguish different meanings).\nMost Khmer words consist of either one or two syllables. In most native disyllabic words, the first syllable is a minor (fully unstressed) syllable. Such words have been described as \"sesquisyllabic\" (i.e. as having one-and-a-half syllables). There are also some disyllabic words in which the first syllable does not behave as a minor syllable, but takes secondary stress. Most such words are compounds, but some are single morphemes (generally loanwords). An example is ('language'), pronounced .\nWords with three or more syllables, if they are not compounds, are mostly loanwords, usually derived from Pali, Sanskrit, or more recently, French. They are nonetheless adapted to Khmer stress patterns. Primary stress falls on the final syllable, with secondary stress on every second syllable from the end. Thus in a three-syllable word, the first syllable has secondary stress; in a four-syllable word, the second syllable has secondary stress; in a five-syllable word, the first and third syllables have secondary stress, and so on. Long polysyllables are not often used in conversation.\nCompounds, however, preserve the stress patterns of the constituent words. Thus , the name of a kind of cookie (literally 'bird's nest'), is pronounced , with secondary stress on the second rather than the first syllable, because it is composed of the words ('nest') and ('bird').\nPhonation and tone.\nKhmer once had a phonation distinction in its vowels, but this now survives only in the most archaic dialect (Western Khmer). The distinction arose historically when vowels after Old Khmer voiced consonants became breathy voiced and diphthongized; for example became . When consonant voicing was lost, the distinction was maintained by the vowel (); later the phonation disappeared as well (). These processes explain the origin of what are now called a-series and o-series consonants in the Khmer script.\nAlthough most Cambodian dialects are not tonal, the colloquial Phnom Penh dialect has developed a tonal contrast (level versus peaking tone) as a by-product of the elision of .\nIntonation.\nIntonation often conveys semantic context in Khmer, as in distinguishing declarative statements, questions and exclamations. The available grammatical means of making such distinctions are not always used, or may be ambiguous; for example, the final interrogative particle can also serve as an emphasizing (or in some cases negating) particle.\nThe intonation pattern of a typical Khmer declarative phrase is a steady rise throughout followed by an abrupt drop on the last syllable.\nOther intonation contours signify a different type of phrase such as the \"full doubt\" interrogative, similar to yes\u2013no questions in English. Full doubt interrogatives remain fairly even in tone throughout, but rise sharply towards the end.\nExclamatory phrases follow the typical steadily rising pattern, but rise sharply on the last syllable instead of falling.\nGrammar.\nKhmer is primarily an analytic language with no inflection. Syntactic relations are mainly determined by word order. Old and Middle Khmer used particles to mark grammatical categories and many of these have survived in Modern Khmer but are used sparingly, mostly in literary or formal language. Khmer makes extensive use of auxiliary verbs, \"directionals\" and serial verb construction. Colloquial Khmer is a zero copula language, instead preferring predicative adjectives (and even predicative nouns) unless using a copula for emphasis or to avoid ambiguity in more complex sentences. Basic word order is subject\u2013verb\u2013object (SVO), although subjects are often dropped; prepositions are used rather than postpositions.\nTopic-Comment constructions are common and the language is generally head-initial (modifiers follow the words they modify). Some grammatical processes are still not fully understood by western scholars. For example, it is not clear if certain features of Khmer grammar, such as actor nominalization, should be treated as a morphological process or a purely syntactic device, and some derivational morphology seems \"purely decorative\" and performs no known syntactic work.\nLexical categories have been hard to define in Khmer. Henri Maspero, an early scholar of Khmer, claimed the language had no parts of speech, while a later scholar, Judith Jacob, posited four parts of speech and innumerable particles. John Haiman, on the other hand, identifies \"a couple dozen\" parts of speech in Khmer with the caveat that Khmer words have the freedom to perform a variety of syntactic functions depending on such factors as word order, relevant particles, location within a clause, intonation and context. Some of the more important lexical categories and their function are demonstrated in the following example sentence taken from a hospital brochure:\nMorphology.\nModern Khmer is an isolating language, which means that it uses little productive morphology. There is some derivation by means of prefixes and infixes, but this is a remnant of Old Khmer and not always productive in the modern language. Khmer morphology is evidence of a historical process through which the language was, at some point in the past, changed from being an agglutinative language to adopting an isolating typology. Affixed forms are lexicalized and cannot be used productively to form new words. Below are some of the most common affixes with examples as given by Huffman.\nCompounding in Khmer is a common derivational process that takes two forms, coordinate compounds and repetitive compounds. Coordinate compounds join two unbound morphemes (independent words) of similar meaning to form a compound signifying a concept more general than either word alone. Coordinate compounds join either two nouns or two verbs. Repetitive compounds, one of the most productive derivational features of Khmer, use reduplication of an entire word to derive words whose meaning depends on the class of the reduplicated word. A repetitive compound of a noun indicates plurality or generality while that of an adjectival verb could mean either an intensification or plurality.\nCoordinate compounds:\nRepetitive compounds:\nNouns and pronouns.\nKhmer nouns do not inflect for grammatical gender or singular/plural. There are no articles, but indefiniteness is often expressed by the word for \"one\" ( ) following the noun as in ( \"a dog\"). Plurality can be marked by postnominal particles, numerals, or reduplication of a following adjective, which, although similar to intensification, is usually not ambiguous due to context.\nClassifying particles are used after numerals, but are not always obligatory as they are in Thai or Chinese, for example, and are often dropped in colloquial speech. Khmer nouns are divided into two groups: mass nouns, which take classifiers; and specific, nouns, which do not. The overwhelming majority are mass nouns.\nPossession is colloquially expressed by word order. The possessor is placed after the thing that is possessed. Alternatively, in more complex sentences or when emphasis is required, a possessive construction using the word (, \"property, object\") may be employed. In formal and literary contexts, the possessive particle () is used:\nPronouns are subject to a complicated system of social register, the choice of pronoun depending on the perceived relationships between speaker, audience and referent (see Social registers below). Khmer exhibits pronoun avoidance, so kinship terms, nicknames and proper names are often used instead of pronouns (including for the first person) among intimates. Subject pronouns are frequently dropped in colloquial conversation.\nAdjectives, verbs and verb phrases may be made into nouns by the use of nominalization particles. Three of the more common particles used to create nouns are , , and . These particles are prefixed most often to verbs to form abstract nouns. The latter, derived from Sanskrit, also occurs as a suffix in fixed forms borrowed from Sanskrit and Pali such as (\"health\") from (\"to be healthy\").\nAdjectives and adverbs.\nAdjectives, demonstratives and numerals follow the noun they modify. Adverbs likewise follow the verb. Morphologically, adjectives and adverbs are not distinguished, with many words often serving either function. Adjectives are also employed as verbs as Khmer sentences rarely use a copula.\nDegrees of comparison are constructed syntactically. Comparatives are expressed using the word : \"A X [B]\" (A is more X [than B]). The most common way to express superlatives is with : \"A X \" (A is the most X). Intensity is also expressed syntactically, similar to other languages of the region, by reduplication or with the use of intensifiers.\nVerbs.\nAs is typical of most East Asian languages, Khmer verbs do not inflect at all; tense, aspect and mood can be expressed using auxiliary verbs, particles (such as , placed before a verb to express continuous aspect) and adverbs (such as \"yesterday\", \"earlier\", \"tomorrow\"), or may be understood from context. Serial verb construction is quite common.\nKhmer verbs are a relatively open class and can be divided into two types, main verbs and auxiliary verbs. Huffman defined a Khmer verb as \"any word that can be (negated)\", and further divided main verbs into three classes.\nTransitive verbs are verbs that may be followed by a direct object:\nIntransitive verbs are verbs that can not be followed by an object:\nAdjectival verbs are a word class that has no equivalent in English. When modifying a noun or verb, they function as adjectives or adverbs, respectively, but they may also be used as main verbs equivalent to English \"be + \"adjective\"\".\nSyntax.\nSyntax is the rules and processes that describe how sentences are formed in a particular language, how words relate to each other within clauses or phrases and how those phrases relate to each other within a sentence to convey meaning. Khmer syntax is very analytic. Relationships between words and phrases are signified primarily by word order supplemented with auxiliary verbs and, particularly in formal and literary registers, grammatical marking particles. Grammatical phenomena such as negation and aspect are marked by particles while interrogative sentences are marked either by particles or interrogative words equivalent to English \"wh-words\".\nA complete Khmer sentence consists of four basic elements\u2014an optional topic, an optional subject, an obligatory predicate, and various adverbials and particles. The topic and subject are noun phrases, predicates are verb phrases and another noun phrase acting as an object or verbal attribute often follows the predicate.\nBasic constituent order.\nWhen combining these noun and verb phrases into a sentence the order is typically SVO:\nWhen both a direct object and indirect object are present without any grammatical markers, the preferred order is SV(DO)(IO). In such a case, if the direct object phrase contains multiple components, the indirect object immediately follows the noun of the direct object phrase and the direct object's modifiers follow the indirect object:\nThis ordering of objects can be changed and the meaning clarified with the inclusion of particles. The word , which normally means \"to arrive\" or \"towards\", can be used as a preposition meaning \"to\":\nAlternatively, the indirect object could precede the direct object if the object-marking preposition were used:\nHowever, in spoken discourse OSV is possible when emphasizing the object in a topic\u2013comment-like structure.\nNoun phrase.\nThe noun phrase in Khmer typically has the following structure:\nThe elements in parentheses are optional. Honorifics are a class of words that serve to index the social status of the referent. Honorifics can be kinship terms or personal names, both of which are often used as first and second person pronouns, or specialized words such as ('god') before royal and religious objects. The most common demonstratives are ('this, these') and ('that, those'). The word ('those over there') has a more distal or vague connotation.\nIf the noun phrase contains a possessive adjective, it follows the noun and precedes the numeral. If a descriptive attribute co-occurs with a possessive, the possessive construction () is expected.\nSome examples of typical Khmer noun phrases are:\nThe Khmer particle marked attributes in Old Khmer noun phrases and is used in formal and literary language to signify that what precedes is the noun and what follows is the attribute. Modern usage may carry the connotation of mild intensity.\nVerb phrase.\nKhmer verbs are completely uninflected, and once a subject or topic has been introduced or is clear from context the noun phrase may be dropped. Thus, the simplest possible sentence in Khmer consists of a single verb. For example, 'to go' on its own can mean \"I'm going.\", \"He went.\", \"They've gone.\", \"Let's go.\", etc. This also results in long strings of verbs such as:\nKhmer uses three verbs for what translates into English as the copula. The general copula is ; it is used to convey identity with nominal predicates. For locative predicates, the copula is . The verb is the \"existential\" copula meaning \"there is\" or \"there exists\".\nNegation is achieved by putting before the verb and the particle at the end of the sentence or clause. In colloquial speech, verbs can also be negated without the need for a final particle, by placing before them.\nPast tense can be conveyed by adverbs, such as \"yesterday\" or by the use of perfective particles such as \nDifferent senses of future action can also be expressed by the use of adverbs like \"tomorrow\" or by the future tense marker , which is placed immediately before the verb, or both:\nImperatives are often unmarked. For example, in addition to the meanings given above, the \"sentence\" can also mean \"Go!\". Various words and particles may be added to the verb to soften the command to varying degrees, including to the point of politeness (jussives):\nProhibitives take the form \" + \" and also are often softened by the addition of the particle to the end of the phrase.\nQuestions.\nThere are three basic types of questions in Khmer. Questions requesting specific information use question words. Polar questions are indicated with interrogative particles, most commonly , a homonym of the negation particle. Tag questions are indicated with various particles and rising inflection. The SVO word order is generally not inverted for questions.\nIn more formal contexts and in polite speech, questions are also marked at their beginning by the particle .\nPassive voice.\nKhmer does not have a passive voice, but there is a construction utilizing the main verb (\"to hit\", \"to be correct\", \"to affect\") as an auxiliary verb meaning \"to be subject to\" or \"to undergo\"\u2014which results in sentences that are translated to English using the passive voice.\nClause syntax.\nComplex sentences are formed in Khmer by the addition of one or more clauses to the main clause. The various types of clauses in Khmer include the coordinate clause, the relative clause and the subordinate clause. Word order in clauses is the same for that of the basic sentences described above. Coordinate clauses do not necessarily have to be marked; they can simply follow one another. When explicitly marked, they are joined by words similar to English conjunctions such as (\"and\") and (\"and then\") or by clause-final conjunction-like adverbs and , both of which can mean \"also\" or \"and also\"; disjunction is indicated by (\"or\").\nRelative clauses can be introduced by (\"that\") but, similar to coordinate clauses, often simply follow the main clause. For example, both phrases below can mean \"the hospital bed that has wheels\".\nRelative clauses are more likely to be introduced with if they do not immediately follow the head noun. Khmer subordinate conjunctions always precede a subordinate clause. Subordinate conjunctions include words such as (\"because\"), (\"seems as if\") and (\"in order to\").\nNumerals.\nCounting in Khmer is based on a biquinary system: the numbers from 6 to 9 have the form \"five one\", \"five two\", etc. The words for multiples of ten from 30 to 90 are not related to the basic Khmer numbers, but are Chinese in origin, and probably came to Khmer via Thai. Khmer numerals, which were inherited directly from Indian numerals, are used more widely than Western numerals, which like Khmer numerals were inherited from Indian, but first passed through the Arabic numerals before reaching the west.\nThe principal number words are listed in the following table, which gives Western and Khmer digits, Khmer spelling and IPA transcription.\nIntermediate numbers are formed by compounding the above elements. Powers of ten are denoted by loan words: (100), (1,000), (10,000), (100,000) and (1,000,000) from Thai and (10,000,000) from Sanskrit.\nOrdinal numbers are formed by placing the particle before the corresponding cardinal number.\nSocial registers.\nKhmer employs a system of registers in which the speaker must always be conscious of the social status of the person spoken to. The different registers, which include those used for common speech, polite speech, speaking to or about royals and speaking to or about monks, employ alternate verbs, names of body parts and pronouns. As an example, the word for \"to eat\" used between intimates or in reference to animals is . Used in polite reference to commoners, it is . When used of those of higher social status, it is or . For monks the word is and for royals, . Another result is that the pronominal system is complex and full of honorific variations, just a few of which are shown in the table below.\nWriting system.\nKhmer is written with the Khmer script, an abugida developed from the Pallava script of India before the 7th century when the first known inscription appeared. Written left-to-right with vowel signs that can be placed after, before, above or below the consonant they follow, the Khmer script is similar in appearance and usage to Thai and Lao, both of which were based on the Khmer system. The Khmer script is also distantly related to the Mon\u2013Burmese script. Within Cambodia, literacy in the Khmer alphabet is estimated at 77.6%.\nConsonant symbols in Khmer are divided into two groups, or series. The first series carries the inherent vowel while the second series carries the inherent vowel . The Khmer names of the series, ('voiceless') and ('voiced'), respectively, indicate that the second series consonants were used to represent the voiced phonemes of Old Khmer. As the voicing of stops was lost, however, the contrast shifted to the phonation of the attached vowels, which, in turn, evolved into a simple difference of vowel quality, often by diphthongization. This process has resulted in the Khmer alphabet having two symbols for most consonant phonemes and each vowel symbol having two possible readings, depending on the series of the initial consonant:\nExamples.\nThe following text is from Article 1 of the Universal Declaration of Human Rights.\nExternal links.\n&lt;section begin=\"list-of-glossing-abbreviations\"/&gt;\nNOUN:noun\nVERB:verb\nOBJ:object\nOM:object marker\nMARKER:marker\nREL:relative\nCOHORT:cohortative\nDIR:directional\nCOMPL:complement\nRESP:respectful\n&lt;section end=\"list-of-glossing-abbreviations\"/&gt;"}
{"id": "5218", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=5218", "title": "Central processing unit", "text": "A central processing unit (CPU), also called a central processor, main processor, or just processor, is the primary processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry, and specialized coprocessors such as graphics processing units (GPUs).\nThe form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic\u2013logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components. Modern CPUs devote a lot of semiconductor area to caches and instruction-level parallelism to increase performance and to CPU modes to support operating systems and virtualization.\nMost modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are called \"multi-core processors\". The individual physical CPUs, called processor cores, can also be multithreaded to support CPU-level multithreading.\nAn IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC).\nHistory.\nEarly computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called \"fixed-program computers\". The \"central processing unit\" term has been in use since as early as 1955. Since the term \"CPU\" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.\nThe idea of a stored-program computer had been already present in the design of John Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner. On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed a paper entitled \"First Draft of a Report on the EDVAC\". It was the outline of a stored-program computer that would eventually be completed in August 1949. EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC was not the first stored-program computer; the Manchester Baby, which was a small-scale experimental stored-program computer, ran its first program on 21 June 1948 and the Manchester Mark 1 ran its first program during the night of 16\u201317 June 1949.\nEarly CPUs were custom designs used as part of a larger and sometimes distinctive computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discrete transistor mainframes and minicomputers, and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles to cellphones, and sometimes even in toys.\nWhile von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as the von Neumann architecture, others before him, such as Konrad Zuse, had suggested and implemented similar ideas. The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also used a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard-architecture processors.\nRelays and vacuum tubes (thermionic tubes) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Vacuum-tube computers such as EDVAC tended to average eight hours between failures, whereas relay computers\u2014such as the slower but earlier Harvard Mark I\u2014failed very rarely. In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4\u00a0MHz were very common at this time, limited largely by the speed of the switching devices they were built with.\nTransistor CPUs.\nThe design complexity of CPUs increased as various technologies facilitated the building of smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements, like vacuum tubes and relays. With this improvement, more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.\nIn 1964, IBM introduced its IBM System/360 computer architecture that was used in a series of computers capable of running the same programs with different speeds and performances. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM used the concept of a microprogram (often called \"microcode\"), which still sees widespread use in modern CPUs. The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is continued by similar modern computers like the IBM zSeries. In 1965, Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets\u2014the PDP-8.\nTransistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. The increased reliability and dramatically increased speed of the switching elements, which were almost exclusively transistors by this time; CPU clock rates in the tens of megahertz were easily obtained during this period. Additionally, while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like single instruction, multiple data (SIMD) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc and Fujitsu Ltd.\nSmall-scale integration CPUs.\nDuring this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or \"chip\". At first, only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based on these \"building block\" ICs are generally referred to as \"small-scale integration\" (SSI) devices. SSI ICs, such as the ones used in the Apollo Guidance Computer, usually contained up to a few dozen transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.\nIBM's System/370, follow-on to the System/360, used SSI ICs rather than Solid Logic Technology discrete-transistor modules. DEC's PDP-8/I and KI10 PDP-10 also switched from the individual transistors used by the PDP-8 and PDP-10 to SSI ICs, and their extremely popular PDP-11 line was originally built with SSI ICs, but was eventually implemented with LSI components once these became practical.\nLarge-scale integration CPUs.\nLee Boysel published influential articles, including a 1967 \"manifesto\", which described how to build the equivalent of a 32-bit mainframe computer from a relatively small number of large-scale integration circuits (LSI). The only way to build LSI chips, which are chips with a hundred or more gates, was to build them using a metal\u2013oxide\u2013semiconductor (MOS) semiconductor manufacturing process (either PMOS logic, NMOS logic, or CMOS logic). However, some companies continued to build processors out of bipolar transistor\u2013transistor logic (TTL) chips because bipolar junction transistors were faster than MOS chips up until the 1970s (a few companies such as Datapoint continued to build processors out of TTL chips until the early 1980s). In the 1960s, MOS ICs were slower and initially considered useful only in applications that required low power. Following the development of silicon-gate MOS technology by Federico Faggin at Fairchild Semiconductor in 1968, MOS ICs largely replaced bipolar TTL as the standard chip technology in the early 1970s.\nAs the microelectronic technology advanced, an increasing number of transistors were placed on ICs, decreasing the number of individual ICs needed for a complete CPU. MSI and LSI ICs increased transistor counts to hundreds, and then thousands. By 1968, the number of ICs required to build a complete CPU had been reduced to 24 ICs of eight different types, with each IC containing roughly 1000 MOSFETs. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits.\nMicroprocessors.\nSince microprocessors were first introduced they have almost completely overtaken all other central processing unit implementation methods. The first commercially available microprocessor, made in 1971, was the Intel 4004, and the first widely used microprocessor, made in 1974, was the Intel 8080. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term \"CPU\" is now applied almost exclusively to microprocessors. Several CPUs (denoted \"cores\") can be combined in a single processing chip.\nPrevious generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size, as a result of being implemented on a single die, means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, the ability to construct exceedingly small transistors on an IC has increased the complexity and number of transistors in a single CPU many fold. This widely observed trend is described by Moore's law, which had proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity until 2016.\nWhile the complexity, size, construction and general form of CPUs have changed enormously since 1950, the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines. As Moore's law no longer holds, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the use of parallelism and other methods that extend the usefulness of the classical von Neumann model.\nOperation.\nThe fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions that is called a program. The instructions to be executed are kept in some kind of computer memory. Nearly all CPUs follow the fetch, decode and execute steps in their operation, which are collectively known as the instruction cycle.\nAfter the execution of an instruction, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If a jump instruction was executed, the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally. In more complex CPUs, multiple instructions can be fetched, decoded and executed simultaneously. This section describes what is generally referred to as the \"classic RISC pipeline\", which is quite common among the simple CPUs used in many electronic devices (often called microcontrollers). It largely ignores the important role of CPU cache, and therefore the access stage of the pipeline.\nSome instructions manipulate the program counter rather than producing result data directly; such instructions are generally called \"jumps\" and facilitate program behavior like loops, conditional program execution (through the use of a conditional jump), and existence of functions. In some processors, some other instructions change the state of bits in a \"flags\" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, in such processors a \"compare\" instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal; one of these flags could then be used by a later jump instruction to determine program flow.\nFetch.\nFetch involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. The instruction's location (address) in program memory is determined by the program counter (PC; called the \"instruction pointer\" in Intel x86 microprocessors), which stores a number that identifies the address of the next instruction to be fetched. After an instruction is fetched, the PC is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence. Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below).\nDecode.\nThe instruction that the CPU fetches from memory determines what the CPU will do. In the decode step, performed by binary decoder circuitry known as the \"instruction decoder\", the instruction is converted into signals that control other parts of the CPU.\nThe way in which the instruction is interpreted is defined by the CPU's instruction set architecture (ISA). Often, one group of bits (that is, a \"field\") within the instruction, called the opcode, indicates which operation is to be performed, while the remaining fields usually provide supplemental information required for the operation, such as the operands. Those operands may be specified as a constant value (called an immediate value), or as the location of a value that may be a processor register or a memory address, as determined by some addressing mode.\nIn some CPU designs, the instruction decoder is implemented as a hardwired, unchangeable binary decoder circuit. In others, a microprogram is used to translate instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses. In some cases the memory that stores the microprogram is rewritable, making it possible to change the way in which the CPU decodes instructions.\nExecute.\nAfter the fetch and decode steps, the execute step is performed. Depending on the CPU architecture, this may consist of a single action or a sequence of actions. During each action, control signals electrically enable or disable various parts of the CPU so they can perform all or part of the desired operation. The action is then completed, typically in response to a clock pulse. Very often the results are written to an internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but less expensive and higher capacity main memory.\nFor example, if an instruction that performs addition is to be executed, registers containing operands (numbers to be summed) are activated, as are the parts of the arithmetic logic unit (ALU) that perform addition. When the clock pulse occurs, the operands flow from the source registers into the ALU, and the sum appears at its output. On subsequent clock pulses, other components are enabled (and disabled) to move the output (the sum of the operation) to storage (e.g., a register or memory). If the resulting sum is too large (i.e., it is larger than the ALU's output word size), an arithmetic overflow flag will be set, influencing the next operation.\nStructure and implementation.\nHardwired into a CPU's circuitry is a set of basic operations it can perform, called an instruction set. Such operations may involve, for example, adding or subtracting two numbers, comparing two numbers, or jumping to a different part of a program. Each instruction is represented by a unique combination of bits, known as the machine language opcode. While processing an instruction, the CPU decodes the opcode (via a binary decoder) into control signals, which orchestrate the behavior of the CPU. A complete machine language instruction consists of an opcode and, in many cases, additional bits that specify arguments for the operation (for example, the numbers to be summed in the case of an addition operation). Going up the complexity scale, a machine language program is a collection of machine language instructions that the CPU executes.\nThe actual mathematical operation for each instruction is performed by a combinational logic circuit within the CPU's processor known as the arithmetic\u2013logic unit or ALU. In general, a CPU executes an instruction by fetching it from memory, using its ALU to perform an operation, and then storing the result to memory. Besides the instructions for integer mathematics and logic operations, various other machine instructions exist, such as those for loading data from memory and storing it back, branching operations, and mathematical operations on floating-point numbers performed by the CPU's floating-point unit (FPU).\nControl unit.\nThe control unit (CU) is a component of the CPU that directs the operation of the processor. It tells the computer's memory, arithmetic and logic unit and input and output devices how to respond to the instructions that have been sent to the processor.\nIt directs the operation of the other units by providing timing and control signals. Most computer resources are managed by the CU. It directs the flow of data between the CPU and the other devices. John von Neumann included the control unit as part of the von Neumann architecture. In modern computer designs, the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction.\nArithmetic logic unit.\nThe arithmetic logic unit (ALU) is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations. The inputs to the ALU are the data words to be operated on (called operands), status information from previous operations, and a code from the control unit indicating which operation to perform. Depending on the instruction being executed, the operands may come from internal CPU registers, external memory, or constants generated by the ALU itself.\nWhen all input signals have settled and propagated through the ALU circuitry, the result of the performed operation appears at the ALU's outputs. The result consists of both a data word, which may be stored in a register or memory, and status information that is typically stored in a special, internal CPU register reserved for this purpose.\nModern CPUs typically contain more than one ALU to improve performance.\nAddress generation unit.\nThe address generation unit (AGU), sometimes also called the address computation unit (ACU), is an execution unit inside the CPU that calculates addresses used by the CPU to access main memory. By having address calculations handled by separate circuitry that operates in parallel with the rest of the CPU, the number of CPU cycles required for executing various machine instructions can be reduced, bringing performance improvements.\nWhile performing various operations, CPUs need to calculate memory addresses required for fetching data from the memory; for example, in-memory positions of array elements must be calculated before the CPU can fetch the data from actual memory locations. Those address-generation calculations involve different integer arithmetic operations, such as addition, subtraction, modulo operations, or bit shifts. Often, calculating a memory address involves more than one general-purpose machine instruction, which do not necessarily decode and execute quickly. By incorporating an AGU into a CPU design, together with introducing specialized instructions that use the AGU, various address-generation calculations can be offloaded from the rest of the CPU, and can often be executed quickly in a single CPU cycle.\nCapabilities of an AGU depend on a particular CPU and its architecture. Thus, some AGUs implement and expose more address-calculation operations, while some also include more advanced specialized instructions that can operate on multiple operands at a time. Some CPU architectures include multiple AGUs so more than one address-calculation operation can be executed simultaneously, which brings further performance improvements due to the superscalar nature of advanced CPU designs. For example, Intel incorporates multiple AGUs into its Sandy Bridge and Haswell microarchitectures, which increase bandwidth of the CPU memory subsystem by allowing multiple memory-access instructions to be executed in parallel.\nMemory management unit (MMU).\nMany microprocessors (in smartphones and desktop, laptop, server computers) have a memory management unit, translating logical addresses into physical RAM addresses, providing memory protection and paging abilities, useful for virtual memory. Simpler processors, especially microcontrollers, usually don't include an MMU.\nCache.\nA CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. A cache is a smaller, faster memory, closer to a processor core, which stores copies of the data from frequently used main memory locations. Most CPUs have different independent caches, including instruction and data caches, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2, L3, L4, etc.).\nAll modern (fast) CPUs (with few specialized exceptions) have multiple levels of CPU caches. The first CPUs that used a cache had only one level of cache; unlike later level 1 caches, it was not split into L1d (for data) and L1i (for instructions). Almost all current CPUs with caches have a split L1 cache. They also have L2 caches and, for larger processors, L3 caches as well. The L2 cache is usually not split and acts as a common repository for the already split L1 cache. Every core of a multi-core processor has a dedicated L2 cache and is usually not shared between the cores. The L3 cache, and higher-level caches, are shared between the cores and are not split. An L4 cache is currently uncommon, and is generally on dynamic random-access memory (DRAM), rather than on static random-access memory (SRAM), on a separate die or chip. That was also the case historically with L1, while bigger chips have allowed integration of it and generally all cache levels, with the possible exception of the last level. Each extra level of cache tends to be bigger and is optimized differently.\nOther types of caches exist (that are not counted towards the \"cache size\" of the most important caches mentioned above), such as the translation lookaside buffer (TLB) that is part of the memory management unit (MMU) that most CPUs have.\nCaches are generally sized in powers of two: 2, 8, 16 etc. KiB or MiB (for larger non-L1) sizes, although the IBM z13 has a 96 KiB L1 instruction cache.\nClock rate.\nMost CPUs are synchronous circuits, which means they employ a clock signal to pace their sequential operations. The clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions and, consequently, the faster the clock, the more instructions the CPU will execute each second.\nTo ensure proper operation of the CPU, the clock period is longer than the maximum time needed for all signals to propagate (move) through the CPU. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the \"edges\" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).\nHowever, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue, as clock rates increase dramatically, is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does energy consumption, causing the CPU to require more heat dissipation in the form of CPU cooling solutions.\nOne method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable recent CPU design that uses extensive clock gating is the IBM PowerPC-based Xenon used in the Xbox 360; this reduces the power requirements of the Xbox 360.\nClockless CPUs.\nAnother method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire asynchronous CPUs have been built without using a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS.\nRather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers.\nVoltage regulator module.\nMany modern CPUs have a die-integrated power managing module which regulates on-demand voltage supply to the CPU circuitry allowing it to keep balance between performance and power consumption.\nInteger range.\nEvery CPU represents numerical values in a specific way. For example, some early digital computers represented numbers as familiar decimal (base 10) numeral system values, and others have employed more unusual representations such as ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a \"high\" or \"low\" voltage.\nRelated to numeric representation is the size and precision of integer numbers that a CPU can represent. In the case of a binary CPU, this is measured by the number of bits (significant digits of a binary encoded integer) that the CPU can process in one operation, which is commonly called \"word size\", \"bit width\", \"data path width\", \"integer precision\", or \"integer size\". A CPU's integer size determines the range of integer values on which it can directly operate. For example, an 8-bit CPU can directly manipulate integers represented by eight bits, which have a range of 256 (28) discrete integer values.\nInteger range can also affect the number of memory locations the CPU can directly address (an address is an integer value representing a specific memory location). For example, if a binary CPU uses 32 bits to represent a memory address then it can directly address 232 memory locations. To circumvent this limitation and for various other reasons, some CPUs use mechanisms (such as bank switching) that allow additional memory to be addressed.\nCPUs with larger word sizes require more circuitry and consequently are physically larger, cost more and consume more power (and therefore generate more heat). As a result, smaller 4- or 8-bit microcontrollers are commonly used in modern applications even though CPUs with much larger word sizes (such as 16, 32, 64, even 128-bit) are available. When higher performance is required, however, the benefits of a larger word size (larger data ranges and address spaces) may outweigh the disadvantages. A CPU can have internal data paths shorter than the word size to reduce size and cost. For example, even though the IBM System/360 instruction set architecture was a 32-bit instruction set, the System/360 Model 30 and Model 40 had 8-bit data paths in the arithmetic logical unit, so that a 32-bit add required four cycles, one for each 8 bits of the operands, and, even though the Motorola 68000 series instruction set was a 32-bit instruction set, the Motorola 68000 and Motorola 68010 had 16-bit data paths in the arithmetic logical unit, so that a 32-bit add required two cycles.\nTo gain some of the advantages afforded by both lower and higher bit lengths, many instruction sets have different bit widths for integer and floating-point data, allowing CPUs implementing that instruction set to have different bit widths for different portions of the device. For example, the IBM System/360 instruction set was primarily 32 bit, but supported 64-bit floating-point values to facilitate greater accuracy and range in floating-point numbers. The System/360 Model 65 had an 8-bit adder for decimal and fixed-point binary arithmetic and a 60-bit adder for floating-point arithmetic. Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose use where a reasonable balance of integer and floating-point capability is required.\nParallelism.\nThe description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as \"subscalar\", operates on and executes one instruction on one or two pieces of data at a time, that is less than one instruction per clock cycle ().\nThis process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets \"hung up\" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach \"scalar\" performance (one instruction per clock cycle, ). However, the performance is nearly always subscalar (less than one instruction per clock cycle, ).\nAttempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques:\nEach methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.\nInstruction-level parallelism.\nOne of the simplest methods for increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is a technique known as instruction pipelining, and is used in almost all modern general-purpose CPUs. Pipelining allows multiple instruction to be executed at a time by breaking the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.\nPipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. Therefore, pipelined processors must check for these sorts of conditions and delay a portion of the pipeline if necessary. A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).\nImprovements in instruction pipelining led to further decreases in the idle time of CPU components. Designs that are said to be superscalar include a long instruction pipeline and multiple identical execution units, such as load\u2013store units, arithmetic\u2013logic units, floating-point units and address generation units. In a superscalar pipeline, instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so, they are dispatched to execution units, resulting in their simultaneous execution. In general, the number of instructions that a superscalar CPU will complete in a cycle is dependent on the number of instructions it is able to dispatch simultaneously to execution units.\nMost of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and requires significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, register renaming, out-of-order execution and transactional memory crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of single instruction stream, multiple data stream, a case when a lot of data from the same type has to be processed, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.\nWhen a fraction of the CPU is superscalar, the part that is not suffers a performance penalty due to scheduling stalls. The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock cycle each, but its FPU could not. Thus the P5 was integer superscalar but not floating point superscalar. Intel's successor to the P5 architecture, P6, added superscalar abilities to its floating-point features.\nSimple pipelining and superscalar design increase a CPU's ILP by allowing it to execute instructions at rates surpassing one instruction per clock cycle. Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or instruction set architecture (ISA). The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the CPU's work in boosting ILP and thereby reducing design complexity.\nTask-level parallelism.\nAnother strategy of achieving performance is to execute multiple threads or processes in parallel. This area of research is known as parallel computing. In Flynn's taxonomy, this strategy is known as multiple instruction stream, multiple data stream (MIMD).\nOne technology used for this purpose is multiprocessing (MP). The initial type of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single chip, the technology is known as chip-level multiprocessing (CMP) and the single chip as a multi-core processor.\nIt was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as multi-threading (MT). The approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU are replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as temporal multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly context switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the UltraSPARC T1. Another type of MT is simultaneous multithreading, where instructions from multiple threads are executed in parallel within one CPU clock cycle.\nFor several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.\nCPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or process.\nThis reversal of emphasis is evidenced by the proliferation of dual and more core processor designs and notably, Intel's newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360's triple-core PowerPC design, and the PlayStation 3's 7-core Cell microprocessor.\nData parallelism.\nA less common but increasingly important paradigm of processors (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn's taxonomy, these two schemes of dealing with data are generally referred to as \"single instruction\" stream, \"multiple data\" stream (SIMD) and \"single instruction\" stream, \"single data\" stream (SISD), respectively. The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks include multimedia applications (images, video and sound), as well as many types of scientific and engineering tasks. Whereas a scalar processor must complete the entire process of fetching, decoding and executing each instruction and value in a set of data, a vector processor can perform a single operation on a comparatively large set of data with one instruction. This is only possible when the application tends to require many steps which apply one operation to a large set of data.\nMost early vector processors, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose processors has become significant. Shortly after inclusion of floating-point units started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose processors. Some of these early SIMD specifications \u2013 like HP's Multimedia Acceleration eXtensions (MAX) and Intel's MMX \u2013 were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating-point numbers. Progressively, developers refined and remade these early designs into some of the common modern SIMD specifications, which are usually associated with one instruction set architecture (ISA). Some notable modern examples include Intel's Streaming SIMD Extensions (SSE) and the PowerPC-related AltiVec (also known as VMX).\nHardware performance counter.\nMany modern architectures (including embedded ones) often include hardware performance counters (HPC), which enables low-level (instruction-level) collection, benchmarking, debugging or analysis of running software metrics. HPC may also be used to discover and analyze unusual or suspicious activity of the software, such as return-oriented programming (ROP) or sigreturn-oriented programming (SROP) exploits etc. This is usually done by software-security teams to assess and find malicious binary programs.\nMany major vendors (such as IBM, Intel, AMD, and Arm) provide software interfaces (usually written in C/C++) that can be used to collect data from the CPU's registers in order to get metrics. Operating system vendors also provide software like codice_1 (Linux) to record, benchmark, or trace CPU events running kernels and applications.\nHardware counters provide a low-overhead method for collecting comprehensive performance metrics related to a CPU's core elements (functional units, caches, main memory, etc.) \u2013 a significant advantage over software profilers. Additionally, they generally eliminate the need to modify the underlying source code of a program. Because hardware designs differ between architectures, the specific types and interpretations of hardware counters will also change.\nPrivileged modes.\nMost modern CPUs have privileged modes to support operating systems and virtualization.\nCloud computing can use virtualization to provide virtual central processing units (vCPUs) for separate users.\nA host is the virtual equivalent of a physical machine, on which a virtual system is operating. When there are several physical machines operating in tandem and managed as a whole, the grouped computing and memory resources form a cluster. In some systems, it is possible to dynamically add and remove from a cluster. Resources available at a host and cluster level can be partitioned into resources pools with fine granularity.\nPerformance.\nThe \"performance\" or \"speed\" of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform.\nMany reported IPS values have represented \"peak\" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in IPS calculations. Because of these problems, various standardized tests, often called \"benchmarks\" for this purpose such as SPECinthave been developed to attempt to measure the real effective performance in commonly used applications.\nProcessing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called \"cores\" in this sense) into one integrated circuit. Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, the performance gain is far smaller, only about 50%, due to imperfect software algorithms and implementation. Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information. Multi-core CPUs enhance a computer's ability to run several tasks simultaneously by providing additional processing power. However, the increase in speed is not directly proportional to the number of cores added. This is because the cores need to interact through specific channels, and this inter-core communication consumes a portion of the available processing speed.\nDue to specific capabilities of modern CPUs, such as simultaneous multithreading and uncore, which involve sharing of actual CPU resources while aiming at increased utilization, monitoring performance levels and hardware use gradually became a more complex task. As a response, some CPUs implement additional hardware logic that monitors actual use of various parts of a CPU and provides various counters accessible to software; an example is Intel's \"Performance Counter Monitor\" technology.\nOverclocking.\nOverclocking is a process of increasing the clock speed of a CPU (and other components) to increase the performance of the CPU. Overclocking might increase CPU temperature and cause it to overheat, so most users do not overclock and leave the clock speed unchanged. Some versions of components (such as Intel's U version of its CPUss or Nvidia's OG GPUs) do not allow overclocking."}
{"id": "5220", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5220", "title": "Complex numbers", "text": ""}
{"id": "5221", "revid": "2273576", "url": "https://en.wikipedia.org/wiki?curid=5221", "title": "Carnivora", "text": "Carnivora ( ) is an order of placental mammals specialized primarily in eating flesh, whose members are formally referred to as carnivorans. The order Carnivora is the sixth largest order of mammals, comprising at least 279 species on every major landmass and in a variety of habitats, ranging from the cold polar regions of Earth to the hyper-arid region of the Sahara Desert and the open seas. Carnivorans exhibit a wide array of body plans, varying greatly in size and shape.\nCarnivora are divided into two suborders, the Feliformia, containing the true felids and several animals; and the Caniformia, containing the true canids and many animals. The feliforms include the Felidae, Viverridae, hyena, and mongoose families, the majority of which live only in the Old World; cats are the only exception, occurring in the Old World and the New World, entering the Americas via the Bering Land Bridge. The caniforms include the Caninae, Procyonidae, bears, mustelids, skunks and pinnipeds that occur worldwide with immense diversity in their morphology, diet, and behavior.\nEtymology.\nThe word \"Carnivora\" is derived from Latin \"car\u014d\" (stem \"carn-\") 'flesh' and \"vor\u0101re\" 'to devour'.\nPhylogeny.\nThe oldest known carnivoran line mammals (Carnivoramorpha) appeared in North America 6\u00a0million years after the Cretaceous\u2013Paleogene extinction event. These early ancestors of carnivorans would have resembled small weasel or genet-like mammals, occupying a nocturnal shift on the forest floor or in the trees, as other groups of mammals like the mesonychians and later the creodonts were occupying the megafaunal faunivorous niche. However, following the extinction of mesonychians and the oxyaenid creodonts at the end of the Eocene, carnivorans quickly moved into this niche, with forms like the nimravids being the dominant large-bodied ambush predators during the Oligocene alongside the hyaenodont creodonts (which similarly produced larger, more open-country forms at the start of the Oligocene). By the time Miocene epoch appeared, most if not all of the major lineages and families of carnivorans had diversified and become the most dominant group of large terrestrial predators in Eurasia and North America, with various lineages being successful in megafaunal faunivorous niches at different intervals during the Miocene and later epochs.\nSystematics.\nEvolution.\nThe order Carnivora belongs to a group of mammals known as Laurasiatheria, which also includes other groups such as bats and ungulates. Within this group the carnivorans are placed in the clade Ferae. Ferae includes the closest extant relative of carnivorans, the pangolins, as well as several extinct groups of mostly Paleogene carnivorous placentals such as the creodonts, the arctocyonians, and mesonychians. The creodonts were originally thought of as the sister taxon to the carnivorans, perhaps even ancestral to, based on the presence of the carnassial teeth, but the nature of the carnassial teeth is different between the two groups. In carnivorans, the carnassials are positioned near the front of the molar row, while in the creodonts, they are positioned near the back of the molar row, and this suggests a separate evolutionary history and an order-level distinction. In addition, recent phylogenetic analysis suggests that creodonts are more closely related to pangolins while mesonychians might be the sister group to carnivorans and their stem-relatives.\nThe closest stem-carnivorans are the miacoids. The miacoids include the families Viverravidae and Miacidae, and together the Carnivora and Miacoidea form the stem-clade Carnivoramorpha. The miacoids were small, genet-like carnivoramorphs that occupy a variety of niches such as terrestrial and arboreal habitats. Recent studies have shown a supporting amount of evidence that Miacoidea is an evolutionary grade of carnivoramorphs that, while viverravids are monophyletic basal group, the miacids are paraphyletic in respect to Carnivora (as shown in the phylogeny below).\nCarnivoramorpha as a whole first appeared in the Paleocene of North America about 60\u00a0million years ago. Crown carnivorans first appeared around 42\u00a0million years ago in the Middle Eocene. Their molecular phylogeny shows the extant Carnivora are a monophyletic group, the crown group of the Carnivoramorpha. From there carnivorans have split into two clades based on the composition of the bony structures that surround the middle ear of the skull, the cat-like feliforms and the dog-like caniforms. In feliforms, the auditory bullae are double-chambered, composed of two bones joined by a septum. Caniforms have single-chambered or partially divided auditory bullae, composed of a single bone. Initially, the early representatives of carnivorans were small as the creodonts (specifically, the oxyaenids) and mesonychians dominated the apex predator niches during the Eocene, but in the Oligocene, carnivorans became a dominant group of apex predators with the nimravids, and by the Miocene most of the extant carnivoran families have diversified and become the primary terrestrial predators in the Northern Hemisphere.\nClassification of the extant carnivorans.\nIn 1758, the Swedish botanist Carl Linnaeus placed all carnivorans known at the time into the group Ferae (not to be confused with the modern concept of Ferae which also includes pangolins) in the tenth edition of his book \"Systema Naturae\". He recognized six genera: \"Canis\" (canids and hyaenids), \"Phoca\" (pinnipeds), \"Felis\" (felids), \"Viverra\" (viverrids, herpestids, and mephitids), \"Mustela\" (non-badger mustelids), \"Ursus\" (ursids, large species of mustelids, and procyonids). It was not until 1821 that the English writer and traveler Thomas Edward Bowdich gave the group its modern and accepted name.\nInitially, the modern concept of Carnivora was divided into two suborders: the terrestrial Fissipedia and the marine Pinnipedia. Below is the classification of how the extant families were related to each other after American paleontologist George Gaylord Simpson in 1945:\nSince then, however, the methods in which mammalogists use to assess the phylogenetic relationships among the carnivoran families has been improved with using more complicated and intensive incorporation of genetics, morphology and the fossil record. Research into Carnivora phylogeny since 1945 has found Fisspedia to be paraphyletic in respect to Pinnipedia, with pinnipeds being either more closely related to bears or to weasels. The small carnivoran families Viverridae, Procyonidae, and Mustelidae have been found to be polyphyletic: \nBelow is a table chart of the extant carnivoran families and number of extant species recognized by various authors of the first (2009) and fourth (2014) volumes of the \"Handbook of the Mammals of the World\":\nAnatomy.\nSkull.\nThe canine teeth are usually large, conical, thick and stress resistant. All of the terrestrial species of carnivorans have three incisors on each side of each jaw (the exception is the sea otter (\"Enhydra lutris\") which only has two lower incisor teeth). The third molar has been lost. The carnassial pair is made up of the fourth upper premolar and the first lower molar teeth. Like most mammals, the dentition is heterodont, though in some species, such as the aardwolf (\"Proteles cristata\"), the teeth have been greatly reduced and the cheek teeth are specialised for eating insects. In pinnipeds, the teeth are homodont as they have evolved to grasp or catch fish, and the cheek teeth are often lost. In bears and raccoons, the carnassial pair is secondarily reduced. The skulls are heavily built with a strong zygomatic arch. Often a sagittal crest is present, sometimes more evident in sexually dimorphic species such as sea lions and fur seals, though it has also been greatly reduced in some small carnivorans. The braincase is enlarged with the frontoparietal bone at the front. In most species, the eyes are at the front of the face. In caniforms, the rostrum is usually long with many teeth, while in feliforms it is shorter with fewer teeth. The carnassial teeth of feliforms are generally more sectional than those of caniforms. \nThe turbinates are large and complex in comparison to other mammals, providing a large surface area for olfactory receptors.\nPostcranial region.\nAside from an accumulation of characteristics in the dental and cranial features, not much of their overall anatomy unites carnivorans as a group. All species of carnivorans are quadrupedal and most have five digits on the front feet and four digits on the back feet. In terrestrial carnivorans, the feet have soft pads. The feet can either be digitigrade as seen in cats, hyenas and dogs or plantigrade as seen in bears, skunks, raccoons, weasels, civets and mongooses. In pinnipeds, the limbs have been modified into flippers.\nUnlike cetaceans and sirenians, which have fully functional tails to help them swim, pinnipeds use their limbs underwater to swim. Earless seals use their back flippers; sea lions and fur seals use their front flippers, and the walrus uses all of its limbs. As a result, pinnipeds have significantly shorter tails than other carnivorans.\nAside from the pinnipeds, dogs, bears, hyenas, and cats all have distinct and recognizable appearances. Dogs are usually cursorial mammals and are gracile in appearance, often relying on their teeth to hold prey; bears are much larger and rely on their physical strength to forage for food. Compared to dogs and bears, cats have longer and stronger forelimbs armed with retractable claws to hold on to prey. Hyenas are dog-like feliforms that have sloping backs due to their front legs being longer than their hind legs. The raccoon family and red panda are small, bear-like carnivorans with long tails. The other small carnivoran families Nandiniidae, Prionodontidae, Viverridae, Herpestidae, Eupleridae, Mephitidae and Mustelidae have through convergent evolution maintained the small, ancestral appearance of the miacoids, though there is some variation seen such as the robust and stout physicality of badgers and the wolverine (\"Gulo gulo\"). \nMost carnivoran species have a well-defined breeding season. Male carnivorans usually have bacula, which are absent in hyenas and binturongs.\nThe length and density of the fur vary depending on the environment that the species inhabits. In warm climate species, the fur is often short in length and lighter. In cold climate species, the fur is either dense or long, often with an oily substance that helps to retain heat. The pelage coloration differs between species, often including black, white, orange, yellow, red, and many shades of grey and brown. Some are striped, spotted, blotched, banded, or otherwise boldly patterned. There seems to be a correlation between habitat and color pattern; for example spotted or banded species tend to be found in heavily forested environments. Some species like the grey wolf are polymorphic with different individual having different coat colors. The arctic fox (\"Vulpes lagopus\") and the stoat (\"Mustela erminea\") have fur that changes from white and dense in the winter to brown and sparse in the summer. In pinnipeds and polar bears, a thick insulating layer of blubber helps maintain their body temperature.\nRelationship with humans.\nCarnivorans are arguably the group of mammals of most interest to humans. The dog is noteworthy for not only being the first species of carnivoran to be domesticated, but also the first species of any taxon. In the last 10,000 to 12,000 years, humans have selectively bred dogs for a variety of different tasks and today there are well over 400 breeds. The cat is another domesticated carnivoran and it is today considered one of the most successful species on the planet, due to their close proximity to humans and the popularity of cats as pets. Many other species are popular, and they are often charismatic megafauna. Many civilizations have incorporated a species of carnivoran into their culture: a prominent example is the lion, viewed as a symbol of power and royalty in many societies. Yet many species such as wolves and the big cats have been broadly hunted, resulting in extirpation in some areas. Habitat loss and human encroachment as well as climate change have been the primary cause of many species going into decline. Four species of carnivorans have gone extinct since the 1600s: Falkland Island wolf (\"Dusicyon australis\") in 1876; the sea mink (\"Neogale macrodon\") in 1894; the Japanese sea lion (\"Zalophus japonicus\") in 1951 and the Caribbean monk seal (\"Neomonachus tropicalis\") in 1952. Some species such as the red fox (\"Vulpes vulpes\") and stoat (\"Mustela erminea\") have been introduced to Australasia and have caused many native species to become endangered or even extinct."}
{"id": "5222", "revid": "19014806", "url": "https://en.wikipedia.org/wiki?curid=5222", "title": "Colombia", "text": "Colombia, officially the Republic of Colombia, is a country primarily located in South America with insular regions in North America. The Colombian mainland is bordered by the Caribbean Sea to the north, Venezuela to the east and northeast, Brazil to the southeast, Peru and Ecuador to the south and southwest, the Pacific Ocean to the west, and Panama to the northwest. Colombia is divided into 32 departments. The Capital District of Bogot\u00e1 is also the country's largest city hosting the main financial and cultural hub. Other major urban areas include Medell\u00edn, Cali, Barranquilla, Cartagena, Santa Marta, C\u00facuta, Ibagu\u00e9, Villavicencio and Bucaramanga. It covers an area of 1,141,748 square kilometers (440,831 sq mi) and has a population of around 52 million. Its rich cultural heritage\u2014including language, religion, cuisine, and art\u2014reflects its history as a colony, fusing cultural elements brought by immigration from Europe and the Middle East, with those brought by the African diaspora, as well as with those of the various Indigenous civilizations that predate colonization. Spanish is the official language, although Creole, English and 64 other languages are recognized regionally.\nColombia has been home to many indigenous peoples and cultures since at least 12,000 BCE. The Spanish first landed in La Guajira in 1499, and by the mid-16th century, they had colonized much of present-day Colombia, and established the New Kingdom of Granada, with Santa F\u00e9 de Bogot\u00e1 as its capital. Independence from the Spanish Empire was achieved in 1819, with what is now Colombia emerging as the United Provinces of New Granada. The new polity experimented with federalism as the Granadine Confederation (1858) and then the United States of Colombia (1863), before becoming a republic\u2014the current Republic of Colombia\u2014in 1886. With the backing of the United States and France, Panama seceded from Colombia in 1903, resulting in Colombia's present borders. Beginning in the 1960s, the country has suffered from an asymmetric low-intensity armed conflict and political violence, both of which escalated in the 1990s. Since 2005, there has been significant improvement in security, stability, and rule of law, as well as unprecedented economic growth and development. Colombia is recognized for its healthcare system, being the best healthcare in Latin America according to the World Health Organization and 22nd in the world. Its diversified economy is the third-largest in South America, with macroeconomic stability and favorable long-term growth prospects.\nColombia is one of the world's seventeen megadiverse countries; it has the highest level of biodiversity per square mile in the world and the second-highest level overall. Its territory encompasses Amazon rainforest, highlands, grasslands and deserts. It is the only country in South America with coastlines (and islands) along both the Atlantic and Pacific oceans. Colombia is a key member of major global and regional organizations including the UN, the WTO, the OECD, the OAS, the Pacific Alliance and the Andean Community; it is also a NATO Global Partner and a major non-NATO ally of the United States.\nEtymology.\nThe name \"Colombia\" is derived from the last name of the Italian navigator Christopher Columbus (, , ). It was conceived as a reference to all of the New World. The name was later adopted by the Republic of Colombia of 1819, formed from the territories of the old Viceroyalty of New Granada (modern-day Colombia, Panama, Venezuela, Ecuador, and northwest Brazil).\nWhen Venezuela, Ecuador, and Cundinamarca came to exist as independent states, the former Department of Cundinamarca adopted the name \"Republic of New Granada\". New Granada officially changed its name in 1858 to the Granadine Confederation. In 1863 the name was again changed, this time to United States of Colombia, before finally adopting its present name \u2013 the Republic of Colombia \u2013 in 1886.\nTo refer to this country, the Colombian government uses the terms and .\nHistory.\nPre-Columbian era.\nOwing to its location, the present territory of Colombia was a corridor of early human civilization from Mesoamerica and the Caribbean to the Andes and Amazon basin. The oldest archaeological finds are from the Pubenza and El Totumo sites in the Magdalena Valley southwest of Bogot\u00e1. These sites date from the Paleoindian period (18,000\u20138000\u00a0BCE). At Puerto Hormiga and other sites, traces from the Archaic Period (~8000\u20132000\u00a0BCE) have been found. Vestiges indicate that there was also early occupation in the regions of El Abra and Tequendama in Cundinamarca. The oldest pottery discovered in the Americas, found in San Jacinto, dates to 5000\u20134000\u00a0BCE.\nIndigenous people inhabited the territory that is now Colombia by 12,500\u00a0BCE. Nomadic hunter-gatherer tribes at the El Abra, Tibit\u00f3 and Tequendama sites near present-day Bogot\u00e1 traded with one another and with other cultures from the Magdalena River Valley. A site including of pictographs that is under study at Serran\u00eda de la Lindosa was revealed in November 2020. Their age is suggested as being 12,500 years old (c. 10,480 B.C.) by the anthropologists working on the site, because of extinct fauna depicted. It was during the earliest known human occupation of the area.\nBetween 5000 and 1000\u00a0BCE, hunter-gatherer tribes transitioned to agrarian societies; fixed settlements were established, and pottery appeared. Beginning in the 1st millennium BCE, groups of Amerindians including the Muisca, Zen\u00fa, Quimbaya, and Tairona developed the political system of \"cacicazgos\" with a pyramidal structure of power headed by \"caciques\". The Muisca inhabited mainly the area of what is now the Departments of Boyac\u00e1 and Cundinamarca high plateau (\"Altiplano Cundiboyacense\") where they formed the Muisca Confederation. They farmed maize, potato, quinoa, and cotton, and traded gold, emeralds, blankets, ceramic handicrafts, coca and especially rock salt with neighboring nations. The Tairona inhabited northern Colombia in the isolated mountain range of Sierra Nevada de Santa Marta. The Quimbaya inhabited regions of the Cauca River Valley between the Western and Central Ranges of the Colombian Andes. Most of the Amerindians practiced agriculture and the social structure of each indigenous community was different. Some groups of indigenous people such as the Caribs lived in a state of permanent war, but others had less bellicose attitudes. During the 1200s, Malayo-Polynesians and Native Americans in Colombia made contact, thereby spreading Native American genetics from Precolonial Colombia to some Pacific Ocean islands.\nColonial period.\nAlonso de Ojeda (who had sailed with Columbus) reached the Guajira Peninsula in 1499. Spanish explorers, led by Rodrigo de Bastidas, made the first exploration of the Caribbean coast in 1500. Christopher Columbus navigated near the Caribbean in 1502. In 1508, Vasco N\u00fa\u00f1ez de Balboa accompanied an expedition to the territory through the region of Gulf of Urab\u00e1 and they founded the town of Santa Mar\u00eda la Antigua del Dari\u00e9n in 1510, the first stable settlement on the continent. Santa Marta was founded in 1525, and Cartagena in 1533. Spanish conquistador Gonzalo Jim\u00e9nez de Quesada led an expedition to the interior in April 1536, and christened the districts through which he passed \"New Kingdom of Granada\". In August 1538, he provisionally founded its capital near the Muisca cacicazgo of Muyquyt\u00e1, and named it \"Santa Fe\". The name soon acquired a suffix and was called Santa Fe de Bogot\u00e1. Two other notable journeys by early conquistadors to the interior took place in the same period. Sebasti\u00e1n de Belalc\u00e1zar, conqueror of Quito, traveled north and founded Cali, in 1536, and Popay\u00e1n, in 1537; from 1536 to 1539, German conquistador Nikolaus Federmann crossed the Llanos Orientales and went over the Cordillera Oriental in a search for El Dorado, the \"city of gold\". The legend and the gold would play a pivotal role in luring the Spanish and other Europeans to New Granada during the 16th and 17th centuries.\nThe conquistadors made frequent alliances with the enemies of different indigenous communities. Indigenous allies were crucial to conquest, as well as to creating and maintaining empire. Indigenous peoples in Colombia experienced a decline in population due to conquest as well as Eurasian diseases, such as smallpox, to which they had no immunity. Regarding the land as deserted, the Spanish Crown sold properties to all persons interested in colonized territories, creating large farms and possession of mines. In the 16th century, the nautical science in Spain reached a great development thanks to numerous scientific figures of the Casa de Contrataci\u00f3n and nautical science was an essential pillar of the Iberian expansion. In 1542, the region of New Granada, along with all other Spanish possessions in South America, became part of the Viceroyalty of Peru, with its capital in Lima. In 1547, New Granada became a separate captaincy-general within the viceroyalty, with its capital at Santa Fe de Bogota. In 1549, the Royal Audiencia was created by a royal decree, and New Granada was ruled by the Royal Audience of Santa Fe de Bogot\u00e1, which at that time comprised the provinces of Santa Marta, Rio de San Juan, Popay\u00e1n, Guayana and Cartagena. But important decisions were taken from the colony to Spain by the Council of the Indies.\nIn the 16th century, European slave traders had begun to bring enslaved Africans to the Americas. Spain was the only European power that did not establish factories in Africa to purchase slaves; the Spanish Empire instead relied on the asiento system, awarding merchants from other European nations the license to trade enslaved peoples to their overseas territories. This system brought Africans to Colombia, although many spoke out against the institution. The indigenous peoples could not be enslaved because they were legally subjects of the Spanish Crown. To protect the indigenous peoples, several forms of land ownership and regulation were established by the Spanish colonial authorities: \"resguardos\", \"encomiendas\" and \"haciendas\".\nHowever, secret anti-Spanish discontentment was already brewing for Colombians since Spain prohibited direct trade between the Viceroyalty of Peru, which included Colombia, and the Viceroyalty of New Spain, which included the Philippines, the source of Asian products like silk and porcelain which was in demand in the Americas. Illegal trade between Peruvians, Filipinos, and Mexicans continued in secret, as smuggled Asian goods ended up in C\u00f3rdoba, Colombia, the distribution center for illegal Asian imports, due to the collusion between these peoples against the authorities in Spain. They settled and traded with each other while disobeying the forced Spanish monopoly.\nThe Viceroyalty of New Granada was established in 1717, then temporarily removed, and then re-established in 1739. Its capital was Santa F\u00e9 de Bogot\u00e1. This Viceroyalty included some other provinces of northwestern South America that had previously been under the jurisdiction of the Viceroyalties of New Spain or Peru and correspond mainly to today's Venezuela, Ecuador, and Panama. Bogot\u00e1 became one of the principal administrative centers of the Spanish possessions in the New World, along with Lima and Mexico City, though it remained less developed compared to those two cities in several economic and logistical ways.\nGreat Britain declared war on Spain in 1739, and the city of Cartagena quickly became a top target for the British. A massive British expeditionary force was dispatched to capture the city, but, after achieving initial inroads, devastating outbreaks of disease crippled their numbers, and the British were forced to withdraw. The battle became one of Spain's most decisive victories in the conflict, and secured Spanish dominance in the Caribbean until the Seven Years' War. The 18th-century priest, botanist, and mathematician Jos\u00e9 Celestino Mutis was delegated by Viceroy Antonio Caballero y G\u00f3ngora to conduct an inventory of the nature of New Granada. Started in 1783, this became known as the Royal Botanical Expedition to New Granada. It classified plants and wildlife, and founded the first astronomical observatory in the city of Santa Fe de Bogot\u00e1. In July 1801 the Prussian scientist Alexander von Humboldt reached Santa Fe de Bogot\u00e1 where he met with Mutis. In addition, historical figures in the process of independence in New Granada emerged from the expedition as the astronomer Francisco Jos\u00e9 de Caldas, the scientist Francisco Antonio Zea, the zoologist Jorge Tadeo Lozano and the painter Salvador Rizo.\nIndependence.\nRebellions against Spanish rule had occurred in the empire since the advent of conquest and colonization, but most were either crushed or remained too weak to change the overall situation. The last one that sought outright independence from Spain sprang up around 1810 and culminated in the Colombian Declaration of Independence, issued on 20 July 1810, the day that is now celebrated as the nation's Independence Day. This movement followed the independence of Saint-Domingue (present-day Haiti) in 1804, which provided some support to an eventual leader of this rebellion: Sim\u00f3n Bol\u00edvar. Francisco de Paula Santander also would play a decisive role.\nA movement was initiated by Antonio Nari\u00f1o, who opposed Spanish centralism and led the opposition against the Viceroyalty. Cartagena became independent in November 1811. In 1811, the United Provinces of New Granada were proclaimed, headed by Camilo Torres Tenorio. The emergence of two distinct ideological currents among the patriots (federalism and centralism) gave rise to a period of instability called the Patria Boba. Shortly after the Napoleonic Wars ended, Ferdinand VII, recently restored to the throne in Spain, unexpectedly decided to send military forces to retake most of northern South America. The viceroyalty was restored under the command of Juan de S\u00e1mano, whose regime punished those who participated in the patriotic movements, ignoring the political nuances of the juntas. The retribution stoked renewed rebellion, which, combined with a weakened Spain, made possible a successful rebellion led by the Venezuelan-born Sim\u00f3n Bol\u00edvar, who finally proclaimed independence in 1819. The pro-Spanish resistance was defeated in 1822 in the present territory of Colombia and in 1823 in Venezuela. During the Independence War, between 250 and 400 thousand people (12\u201320% of the pre-war population) died.\nThe territory of the Viceroyalty of New Granada became the Republic of Colombia, organized as a union of the current territories of Colombia, Panama, Ecuador, Venezuela, parts of Guyana and Brazil and north of Mara\u00f1\u00f3n River. The Congress of C\u00facuta in 1821 adopted a constitution for the new Republic. Sim\u00f3n Bol\u00edvar became the first President of Colombia, and Francisco de Paula Santander was made Vice President. However, the new republic was unstable and the Gran Colombia ultimately collapsed.\nModern Colombia comes from one of the countries that emerged after the dissolution of Gran Colombia, the other two being Ecuador and Venezuela. Colombia was the first constitutional government in South America, and the Liberal and Conservative parties, founded in 1848 and 1849, respectively, are two of the oldest surviving political parties in the Americas. Slavery was abolished in the country in 1851.\nInternal political and territorial divisions led to the dissolution of Gran Colombia in 1830. The so-called \"Department of Cundinamarca\" adopted the name \"New Granada\", which it kept until 1858 when it became the \"Confederaci\u00f3n Granadina\" (Granadine Confederation). After a two-year civil war in 1863, the United States of Colombia was created, which became known as the Republic of Colombia in 1886. Internal divisions remained between the bipartisan political forces, occasionally igniting very bloody civil wars, the most significant being the Thousand Days' War (1899\u20131902), in which between 100 and 180 thousand Colombians lost their lives when the Liberal Party, supported by Venezuela, Ecuador, Nicaragua, and Guatemala rebelled against the Nationalist government and took control of Santander, ultimately being defeated in 1902 by nationalist forces.\n20th century.\nThe United States of America's intentions to influence the area (especially the Panama Canal construction and control) led to the secession of the Department of Panama in 1903 and its political independence. The United States paid Colombia $25,000,000 in 1921, seven years after completion of the canal, for redress of President Roosevelt's role in the creation of Panama, and Colombia recognized Panama under the terms of the Thomson\u2013Urrutia Treaty. Colombia and Peru went to war because of territory disputes far in the Amazon basin. The war ended with a peace deal brokered by the League of Nations. The League finally awarded the disputed area to Colombia in June 1934.\nSoon after, Colombia achieved some degree of political stability, which was interrupted by a bloody conflict that took place between the late 1940s and the early 1950s, a period known as \"La Violencia\" (\"The Violence\"). Its cause was mainly mounting tensions between the two leading political parties, which subsequently ignited after the assassination of the Liberal presidential candidate Jorge Eli\u00e9cer Gait\u00e1n on 9 April 1948. The ensuing riots in Bogot\u00e1, known as El Bogotazo, spread throughout the country and claimed the lives of at least 180,000 Colombians.\nColombia entered the Korean War when Laureano G\u00f3mez was elected president. It was the only Latin American country to join the war in a direct military role as an ally of the United States. Particularly important was the resistance of the Colombian troops at Old Baldy.\nThe violence between the two political parties decreased first when Gustavo Rojas deposed the President of Colombia in a coup d'\u00e9tat and negotiated with the guerrillas, and then under the military junta of General Gabriel Par\u00eds.\nAfter Rojas' deposition, the Colombian Conservative Party and the Colombian Liberal Party agreed to create the National Front, a coalition that would jointly govern the country. Under the deal, the presidency would alternate between conservatives and liberals every 4 years for 16 years; the two parties would have parity in all other elective offices. The National Front ended \"La Violencia\", and National Front administrations attempted to institute far-reaching social and economic reforms in cooperation with the Alliance for Progress. Despite the progress in certain sectors, many social and political problems continued, and guerrilla groups were formally created such as the FARC, the ELN and the M-19 to fight the government and political apparatus.\nSince the 1960s, the country has suffered from an asymmetric low-intensity armed conflict between government forces, leftist guerrilla groups and right wing paramilitaries. The conflict escalated in the 1990s, mainly in remote rural areas. Since the beginning of the armed conflict, human rights defenders have fought for the respect for human rights, despite staggering opposition. Several guerrillas' organizations decided to demobilize after peace negotiations in 1989\u20131994.\nThe United States has been heavily involved in the conflict since its beginnings, when in the early 1960s the U.S. government encouraged the Colombian military to attack leftist militias in rural Colombia. This was part of the U.S. fight against communism. Mercenaries and multinational corporations such as Chiquita Brands International are some of the international actors that have contributed to the violence of the conflict.\nBeginning in the mid-1970s Colombian drug cartels became major producers, processors and exporters of illegal drugs, primarily marijuana and cocaine.\nOn 4 July 1991, a new Constitution was promulgated. The changes generated by the new constitution are viewed as positive by Colombian society.\n21st century.\nThe administration of President \u00c1lvaro Uribe (2002\u20132010) adopted the democratic security policy which included an integrated counter-terrorism and counter-insurgency campaign. The government economic plan also promoted confidence in investors. As part of a controversial peace process, the AUC (right-wing paramilitaries) had ceased to function formally as an organization . In February 2008, millions of Colombians demonstrated against FARC and other outlawed groups.\nAfter peace negotiations in Cuba, the Colombian government of President Juan Manuel Santos and the guerrillas of the FARC-EP announced a final agreement to end the conflict. However, a referendum to ratify the deal was unsuccessful. Afterward, the Colombian government and the FARC signed a revised peace deal in November 2016, which the Colombian congress approved. In 2016, President Santos was awarded the Nobel Peace Prize. The Government began a process of attention and comprehensive reparation for victims of conflict. Colombia shows modest progress in the struggle to defend human rights, as expressed by HRW. A Special Jurisdiction of Peace has been created to investigate, clarify, prosecute and punish serious human rights violations and grave breaches of international humanitarian law which occurred during the armed conflict and to satisfy victims' right to justice. During his visit to Colombia, Pope Francis paid tribute to the victims of the conflict.\nIn June 2018, Iv\u00e1n Duque, the candidate of the right-wing Democratic Center party, won the presidential election. On 7 August 2018, he was sworn in as the new President of Colombia to succeed Juan Manuel Santos. Colombia's relations with Venezuela have fluctuated due to ideological differences between the two governments. Colombia has offered humanitarian support with food and medicines to mitigate the shortage of supplies in Venezuela. Colombia's Foreign Ministry said that all efforts to resolve Venezuela's crisis should be peaceful. Colombia proposed the idea of the Sustainable Development Goals and a final document was adopted by the United Nations. In February 2019, Venezuelan president Nicol\u00e1s Maduro cut off diplomatic relations with Colombia after Colombian President Ivan Duque had helped Venezuelan opposition politicians deliver humanitarian aid to their country. Colombia recognized Venezuelan opposition leader Juan Guaid\u00f3 as the country's legitimate president. In January 2020, Colombia rejected Maduro's proposal that the two countries restore diplomatic relations.\nProtests started on 28 April 2021 when the government proposed a tax bill that would greatly expand the range of the 19 percent value-added tax. The 19 June 2022 election run-off vote ended in a win for former guerrilla, Gustavo Petro, taking 50.47% of the vote compared to 47.27% for independent candidate Rodolfo Hern\u00e1ndez. The single-term limit for the country's presidency prevented President Iv\u00e1n Duque from seeking re-election. On 7 August 2022, Petro was sworn in, becoming the country's first leftist president.\nGeography.\nThe geography of Colombia is characterized by its six main natural regions that present their unique characteristics, from the Andes mountain range region; the Pacific Coastal region; the Caribbean coastal region; the \"Llanos\" (plains); the Amazon rainforest region; to the insular area, comprising islands in both the Atlantic and Pacific oceans. It shares its maritime limits with Costa Rica, Nicaragua, Honduras, Jamaica, Haiti, and the Dominican Republic.\nColombia is bordered to the northwest by Panama, to the east by Venezuela and Brazil, and to the south by Ecuador and Peru; it established its maritime boundaries with neighboring countries through seven agreements on the Caribbean Sea and three on the Pacific Ocean. It lies between latitudes 12\u00b0N and 4\u00b0S and between longitudes 67\u00b0 and 79\u00b0W.\nEast of the Andes lies the savanna of the \"Llanos\", part of the Orinoco River basin, and in the far southeast, the jungle of the Amazon rainforest. Together these lowlands make up over half Colombia's territory, but they contain less than 6% of the population. To the north the Caribbean coast, home to 21.9% of the population and the location of the major port cities of Barranquilla and Cartagena, generally consists of low-lying plains, but it also contains the Sierra Nevada de Santa Marta mountain range, which includes the country's tallest peaks (Pico Crist\u00f3bal Col\u00f3n and Pico Sim\u00f3n Bol\u00edvar), and the La Guajira Desert. By contrast the narrow and discontinuous Pacific coastal lowlands, backed by the Serran\u00eda de Baud\u00f3 mountains, are sparsely populated and covered in dense vegetation. The principal Pacific port is Buenaventura.\nPart of the Ring of Fire, a region of the world subject to earthquakes and volcanic eruptions, in the interior of Colombia the Andes are the prevailing geographical feature. Most of Colombia's population centers are located in these interior highlands. Beyond the Colombian Massif (in the southwestern departments of Cauca and Nari\u00f1o), these are divided into three branches known as \"cordilleras\" (mountain ranges): the Cordillera Occidental, running adjacent to the Pacific coast and including the city of Cali; the Cordillera Central, running between the Cauca and Magdalena River valleys (to the west and east, respectively) and including the cities of Medell\u00edn, Manizales, Pereira, and Armenia; and the Cordillera Oriental, extending northeast to the Guajira Peninsula and including Bogot\u00e1, Bucaramanga, and C\u00facuta. Peaks in the Cordillera Occidental exceed , and in the Cordillera Central and Cordillera Oriental they reach . At , Bogot\u00e1 is the highest city of its size in the world.\nThe main rivers of Colombia are Magdalena, Cauca, Guaviare, Atrato, Meta, Putumayo and Caquet\u00e1. Colombia has four main drainage systems: the Pacific drain, the Caribbean drain, the Orinoco Basin and the Amazon Basin. The Orinoco and Amazon Rivers mark limits with Colombia to Venezuela and Peru respectively.\nClimate.\nThe climate of Colombia is characterized for being tropical presenting variations within six natural regions and depending on the altitude, temperature, humidity, winds and rainfall. Colombia has a diverse range of climate zones, including tropical rainforests, savannas, steppes, deserts and mountain climates.\nMountain climate is one of the unique features of the Andes and other high altitude reliefs where climate is determined by elevation. Below in elevation is the warm altitudinal zone, where temperatures are above . About 82.5% of the country's total area lies in the warm altitudinal zone. The temperate climate altitudinal zone located between is characterized for presenting an average temperature ranging between . The cold climate is present between and the temperatures vary between . Beyond lies the alpine conditions of the forested zone and then the treeless grasslands of the p\u00e1ramos. Above , where temperatures are below freezing, the climate is glacial, a zone of permanent snow and ice.\nBiodiversity and conservation.\nColombia is one of the megadiverse countries in biodiversity, ranking first in bird species. Colombia is the country with the planet's highest biodiversity, having the highest rate of species by area as well as the largest number of endemisms (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth live in Colombia, including over 1,900 species of bird, more than in Europe and North America combined. Colombia has 10% of the world's mammals species, 14% of the amphibian species and 18% of the bird species of the world.\nAs for plants, the country has between 40,000 and 45,000 plant species, equivalent to 10 or 20% of total global species, which is even more remarkable given that Colombia is considered a country of intermediate size. Colombia is the second most biodiverse country in the world, lagging only after Brazil which is approximately 7 times bigger.\nColombia has about 2,000 species of marine fish and is the second most diverse country in freshwater fish. It is also the country with the most endemic species of butterflies, is first in orchid species, and has approximately 7,000 species of beetles. Colombia is second in the number of amphibian species and is the third most diverse country in reptiles and palms. There are about 1,900 species of mollusks and according to estimates there are about 300,000 species of invertebrates in the country. In Colombia there are 32 terrestrial biomes and 314 types of ecosystems.\nProtected areas and the \"National Park System\" cover an area of about and account for 12.77% of the Colombian territory. Compared to neighboring countries, rates of deforestation in Colombia are still relatively low. Colombia had a 2018 Forest Landscape Integrity Index mean score of 8.26/10, ranking it 25th globally out of 172 countries. Colombia is the sixth country in the world by magnitude of total renewable freshwater supply, and still has large reserves of freshwater.\nGovernment and politics.\nThe government of Colombia takes place within the framework of a presidential participatory democratic republic as established in the Constitution of 1991. In accordance with the principle of separation of powers, government is divided into three branches: the executive branch, the legislative branch and the judicial branch.\nAs the head of the executive branch, the President of Colombia serves as both head of state and head of government, followed by the Vice President and the Council of Ministers. The president is elected by popular vote to serve a single four-year term (In 2015, Colombia's Congress approved the repeal of a 2004 constitutional amendment that changed the one-term limit for presidents to a two-term limit). At the provincial level executive power is vested in department governors, municipal mayors and local administrators for smaller administrative subdivisions, such as \"corregimientos\" or \"comunas\". All regional elections are held one year and five months after the presidential election.\nThe legislative branch of government is represented nationally by the Congress, a bicameral institution comprising a 166-seat Chamber of Representatives and a 102-seat Senate. The Senate is elected nationally and the Chamber of Representatives is elected in electoral districts. Members of both houses are elected to serve four-year terms two months before the president, also by popular vote.\nThe judicial branch is headed by four high courts, consisting of the Supreme Court which deals with penal and civil matters, the Council of State, which has special responsibility for administrative law and also provides legal advice to the executive, the Constitutional Court, responsible for assuring the integrity of the Colombian constitution, and the Superior Council of Judicature, responsible for auditing the judicial branch. Colombia operates a system of civil law, which since 1991 has been applied through an adversarial system.\nDespite a number of controversies, the democratic security policy has ensured that former President \u00c1lvaro Uribe remained popular among Colombian people, with his approval rating peaking at 76%, according to a poll in 2009. However, having served two terms, he was constitutionally barred from seeking re-election in 2010. In the run-off elections on 20 June 2010 the former Minister of Defense Juan Manuel Santos won with 69% of the vote against the second most popular candidate, Antanas Mockus. A second round was required since no candidate received over the 50% winning threshold of votes. Santos won re-election with nearly 51% of the vote in second-round elections on 15 June 2014, beating right-wing rival \u00d3scar Iv\u00e1n Zuluaga, who won 45%. In 2018, Iv\u00e1n Duque won in the second round of the election with 54% of the vote, against 42% for his left-wing rival, Gustavo Petro. His term as Colombia's president ran for four years, beginning on 7 August 2018. In 2022, Colombia elected Gustavo Petro, who became its first leftist leader, and Francia Marquez, who was the first black person elected as vice president.\nForeign affairs.\nThe foreign affairs of Colombia are headed by the President, as head of state, and managed by the Minister of Foreign Affairs. Colombia has diplomatic missions in all continents.\nColombia was one of the four founding members of the Pacific Alliance, which is a political, economic and co-operative integration mechanism that promotes the free circulation of goods, services, capital and persons between the members, as well as a common stock exchange and joint embassies in several countries. Colombia is also a member of the United Nations, the World Trade Organization, the Organisation for Economic Co-operation and Development, the Organization of American States, the Organization of Ibero-American States, and the Andean Community of Nations.\nColombia is a global partner of NATO and a major non-NATO ally of the United States.\nMilitary.\nThe executive branch of government is responsible for managing the defense of Colombia, with the President commander-in-chief of the armed forces. The Ministry of Defence exercises day-to-day control of the military and the Colombian National Police. Colombia has 455,461 active military personnel. In 2016, 3.4% of the country's GDP went towards military expenditure, placing it 24th in the world. Colombia's armed forces are the largest in Latin America, and it is the second largest spender on its military after Brazil. In 2018, Colombia signed the UN treaty on the Prohibition of Nuclear Weapons.\nThe Colombian military is divided into three branches: the National Army of Colombia; the Colombian Aerospace Force; and the Colombian Navy. The National Police functions as a gendarmerie, operating independently from the military as the law enforcement agency for the entire country. Each of these operates with their own intelligence apparatus separate from the National Intelligence Directorate (DNI, in Spanish).\nThe National Army is formed by divisions, brigades, special brigades, and special units, the Colombian Navy by the Naval Infantry, the Naval Force of the Caribbean, the Naval Force of the Pacific, the Naval Force of the South, the Naval Force of the East, Colombia Coast Guards, Naval Aviation, and the Specific Command of San Andres y Providencia and the Aerospace Force by 15 air units.\nAdministrative divisions.\nColombia is divided into 32 departments and one capital district, which is treated as a department (Bogot\u00e1 also serves as the capital of the department of Cundinamarca). Departments are subdivided into municipalities, each of which is assigned a municipal seat, and municipalities are in turn subdivided into \"corregimientos\" in rural areas and into \"comunas\" in urban areas. Each department has a local government with a governor and assembly directly elected to four-year terms, and each municipality is headed by a mayor and council. There is a popularly elected local administrative board in each of the \"corregimientos\" or \"comunas\".\nIn addition to the capital, four other cities have been designated districts (in effect special municipalities), on the basis of special distinguishing features. These are Barranquilla, Cartagena, Santa Marta and Buenaventura. Some departments have local administrative subdivisions, where towns have a large concentration of population and municipalities are near each other (for example, in Antioquia and Cundinamarca). Where departments have a low population (for example Amazonas, Vaup\u00e9s and Vichada), special administrative divisions are employed, such as \"department \"corregimientos\"\", which are a hybrid of a municipality and a \"corregimiento\".\nEconomy.\nHistorically an agrarian economy, Colombia urbanized rapidly in the 20th century, by the end of which just 15.8% of the workforce were employed in agriculture, generating just 6.6% of GDP; 20% of the workforce were employed in industry and 65% in services, responsible for 33% and 60% of GDP respectively. The country's economic production is dominated by its strong domestic demand. Consumption expenditure by households is the largest component of GDP.\nColombia's market economy grew steadily in the latter part of the 20th century, with gross domestic product (GDP) increasing at an average rate of over 4% per year between 1970 and 1998. The country suffered a recession in 1999 (the first full year of negative growth since the Great Depression), and the recovery was long and painful. However, growth reaching 7% in 2007, one of the highest in Latin America. According to International Monetary Fund estimates, in 2023, Colombia's GDP (PPP) was US$1\u00a0trillion, 32nd in the world and third in South America, after Brazil and Argentina.\nTotal government expenditures account for 28% of the domestic economy. External debt equals 40% of gross domestic product. A strong fiscal climate was reaffirmed by a boost in bond ratings. Annual inflation closed 2017 at 4.09% YoY (vs. 5.75% YoY in 2016). The average national unemployment rate in 2017 was 9.4%, although the informality is the biggest problem facing the labour market (the income of formal workers climbed 24.8% in 5 years while labor incomes of informal workers rose only 9%). Colombia has free-trade zones (FTZ), such as Zona Franca del Pacifico, located in the Valle del Cauca, one of the most striking areas for foreign investment.\nThe financial sector has grown favorably due to good liquidity in the economy, the growth of credit and the positive performance of the Colombian economy. The Colombian Stock Exchange through the Latin American Integrated Market (MILA) offers a regional market to trade equities. Colombia is now one of only three economies with a perfect score on the strength of legal rights index, according to the World Bank.\nColombia is rich in natural resources, and it is heavily dependent on energy and mining exports. Colombia's main exports include mineral fuels, oils, distillation products, fruit and other agricultural products, sugars and sugar confectionery, food products, plastics, precious stones, metals, forest products, chemical goods, pharmaceuticals, vehicles, electronic products, electrical equipment, perfumery and cosmetics, machinery, manufactured articles, textile and fabrics, clothing and footwear, glass and glassware, furniture, prefabricated buildings, military products, home and office material, construction equipment, software, among others. Principal trading partners are the United States, China, the European Union and some Latin American countries.\nNon-traditional exports have boosted the growth of Colombian foreign sales as well as the diversification of destinations of export thanks to new free trade agreements. Recent economic growth has led to a considerable increase of new millionaires, including the new entrepreneurs, Colombians with a net worth exceeding US$1\u00a0billion.\nIn 2017, however, the National Administrative Department of Statistics (DANE) reported that 26.9% of the population were living below the poverty line, of which 7.4% were in \"extreme poverty\". The multidimensional poverty rate stands at 17.0 percent of the population. The Government has also been developing a process of financial inclusion within the country's most vulnerable population.\nThe contribution of tourism to GDP was US$5,880.3bn (2.0% of total GDP) in 2016. Tourism generated 556,135 jobs (2.5% of total employment) in 2016. Foreign tourist visits were predicted to have risen from 0.6\u00a0million in 2007 to 4\u00a0million in 2017.\nAgriculture and natural resources.\nIn agriculture, Colombia is one of the five largest producers in the world of coffee, avocado and palm oil, and one of the 10 largest producers in the world of sugarcane, banana, pineapple and cocoa. The country also has considerable production of rice, potato and cassava. Although it is not the largest coffee producer in the world (Brazil claims that title), the country has been able to carry out, for decades, a global marketing campaign to add value to the country's product. Colombian palm oil production is one of the most sustainable on the planet, compared to the largest existing producers. Colombia is also among the 20 largest producers in the world of beef and chicken meat. Colombia is also the 2nd largest flower exporter in the world, after the Netherlands. Colombian agriculture emits 55% of Colombia's greenhouse gas emissions, mostly from deforestation, over-extensive cattle ranching, land grabbing, and illegal agriculture.\nColombia is an important exporter of coal and petroleum \u2013 in 2020, more than 40% of the country's exports were based on these two products. In 2018 it was the 5th largest coal exporter in the world. In 2019, Colombia was the 20th largest petroleum producer in the world, with 791 thousand barrels/day, exporting a good part of its production \u2013 the country was the 19th largest oil exporter in the world in 2020. In mining, Colombia is the world's largest producer of emerald, and in the production of gold, between 2006 and 2017, the country produced 15 tons per year until 2007, when its production increased significantly, beating the record of 66.1 tons extracted in 2012. In 2017, it extracted 52.2 tons. Currently, the country is among the 25 largest gold producers in the world.\nEnergy and transportation.\nThe electricity production in Colombia comes mainly from Renewable energy sources. 69.93% is obtained from the hydroelectric generation. Colombia's commitment to renewable energy was recognized in the 2014 \"Global Green Economy Index (GGEI)\", ranking among the top 10 nations in the world in terms of greening efficiency sectors.\nTransportation in Colombia is regulated within the functions of the Ministry of Transport and entities such as the National Roads Institute (INV\u00cdAS) responsible for the Highways in Colombia, the Aerocivil, responsible for civil aviation and airports, the National Infrastructure Agency, in charge of concessions through public\u2013private partnerships, for the design, construction, maintenance, operation, and administration of the transport infrastructure, the General Maritime Directorate (Dimar) has the responsibility of coordinating maritime traffic control along with the Colombian Navy, among others, and under the supervision of the Superintendency of Ports and Transport.\nIn 2021, Colombia had of roads, of which were paved. At the end of 2017, the country had around of duplicated highways. Rail transportation in Colombia is dedicated almost entirely to freight shipments and the railway network has a length of 1,700\u00a0km of potentially active rails. Colombia has 3,960 kilometers of gas pipelines, 4,900 kilometers of oil pipelines, and 2,990 kilometers of refined-products pipelines.\nThe Colombian government aimed to build 7,000\u00a0km of roads between 2016 and 2020, which would reduce travel times by an estimated 30 per cent, and transport costs by an estimated 20 per cent. A toll road concession programme will comprise 40 projects, and is part of a larger strategic goal to invest nearly $50 bn in transport infrastructure, including railway systems, making the Magdalena River navigable again, improving port facilities, and an expansion of El Dorado International Airport. Colombia is a middle-income country.\nScience and technology.\nColombia has more than 3,950 research groups in science and technology. iNNpulsa, a government body that promotes entrepreneurship and innovation in the country, provides grants to startups, in addition to other services it and institutions provide. Colombia was ranked 61st in the Global Innovation Index in 2024. Co-working spaces have arisen to serve as communities for startups large and small. Organizations such as the Corporation for Biological Research (CIB) for the support of young people interested in scientific work has been successfully developed in Colombia. The International Center for Tropical Agriculture based in Colombia investigates the increasing challenge of global warming and food security.\nImportant inventions related to medicine have been made in Colombia, such as the first external artificial pacemaker with internal electrodes, invented by the electrical engineer Jorge Reynolds Pombo, an invention of great importance for those who suffer from heart failure. Also invented in Colombia were the microkeratome and keratomileusis technique, which form the fundamental basis of what now is known as LASIK (one of the most important techniques for the correction of refractive errors of vision) and the Hakim valve for the treatment of hydrocephalus. Colombia has begun to innovate in military technology for its army and other armies of the world; especially in the design and creation of personal ballistic protection products, military hardware, military robots, bombs, simulators and radar.\nSome leading Colombian scientists are Joseph M. Tohme, researcher recognized for his work on the genetic diversity of food, Manuel Elkin Patarroyo who is known for his groundbreaking work on synthetic vaccines for malaria, Francisco Lopera who discovered the \"Paisa Mutation\" or a type of early-onset Alzheimer's, Rodolfo Llin\u00e1s known for his study of the intrinsic neurons properties and the theory of a syndrome that had changed the way of understanding the functioning of the brain, Jairo Quiroga Puello recognized for his studies on the characterization of synthetic substances which can be used to fight fungus, tumors, tuberculosis and even some viruses and \u00c1ngela Restrepo who established accurate diagnoses and treatments to combat the effects of a disease caused by \"Paracoccidioides brasiliensis\".\nDemographics.\nWith an estimated 50\u00a0million people in 2020, Colombia is the third-most populous country in Latin America, after Brazil and Mexico. At the beginning of the 20th century, Colombia's population was approximately 4\u00a0million. Since the early 1970s Colombia has experienced steady declines in its fertility, mortality, and population growth rates. The population growth rate for 2016 is estimated to be 0.9%. About 26.8% of the population were 15 years old or younger, 65.7% were between 15 and 64 years old, and 7.4% were over 65 years old. The proportion of older persons in the total population has begun to increase substantially. Colombia is projected to have a population of 55.3\u00a0million by 2050.\nEstimates for the population of the area that is now Colombia range between 2.5 and 12 million people in 1500; estimates between the extremes include figures of 6 and 7 million. With the Spanish conquest, the region's population had collapsed to around 1.2 million people in 1600, for an estimated decrease of 52\u201390%. By the end of the colonial period, it had declined further to around 800,000; it began rising in the early 19th century to around 1.4 million, where it would drop again in the Colombian War of Independence to between 1 and 1.2 million. The country's population did not recover to pre-conquest levels until the 1940s, nearly 450 years after its 16th-century peak.\nThe population is concentrated in the Andean highlands and along the Caribbean coast, also the population densities are generally higher in the Andean region. The nine eastern lowland departments, comprising about 54% of Colombia's area, have less than 6% of the population. Traditionally a rural society, movement to urban areas was very heavy in the mid-20th century, and Colombia is now one of the most urbanized countries in Latin America. The urban population increased from 31% of the total in 1938 to nearly 60% in 1973, and by 2014 the figure stood at 76%. The population of Bogot\u00e1 alone has increased from just over 300,000 in 1938 to approximately 8\u00a0million today. In total seventy-two cities now have populations of 100,000 or more (2015). Colombia has the world's largest populations of internally displaced persons (IDPs), estimated to be up to 4.9\u00a0million people.\nThe life expectancy was 74.8 years in 2015, and infant mortality was 13.1 per thousand in 2016. In 2015, 94.58% of adults and 98.66% of youth are literate and the government spends about 4.49% of its GDP on education.\nLanguages.\nAround 99.2% of Colombians speak Spanish, also called Castilian; 65 Amerindian languages, two Creole languages, the Romani language and Colombian Sign Language are also used in the country. English has official status in the archipelago of San Andr\u00e9s, Providencia and Santa Catalina.\nIncluding Spanish, a total of 101 languages are listed for Colombia in the Ethnologue database. The specific number of spoken languages varies slightly since some authors consider as different languages what others consider to be varieties or dialects of the same language. Best estimates recorded 71 languages that are spoken in-country today\u00a0\u2013 most of which belong to the Chibchan, Tucanoan, Bora\u2013Witoto, Guajiboan, Arawakan, Cariban, Barbacoan, and Saliban language families. There are currently more than 850,000 speakers of native languages.\nEthnic groups.\nColombia is ethnically diverse, its people descending from the original Native inhabitants, Spanish conquistadors, Africans originally brought to the country as slaves, and 20th-century immigrants from Europe and the Middle East, all contributing to a diverse cultural heritage. The demographic distribution reflects a pattern that is influenced by colonial history. Whites live all throughout the country, mainly in urban centers and the burgeoning highland and coastal cities. The populations of the major cities also include mestizos. Mestizo \"campesinos\" (people living in rural areas) also live in the Andean highlands where some Spanish conquerors mixed with the women of Amerindian chiefdoms. Mestizos include artisans and small tradesmen that have played a major part in the urban expansion of recent decades. In a study by the American Journal of Physical Anthropology, Colombians have an average ancestry of 47% Amerindian DNA, 42% European DNA, and 11% African DNA.\nThe 2018 census reported that the \"non-ethnic population\", consisting of whites and mestizos (those of mixed European and Amerindian ancestry), constituted 87.6% of the national population. 6.7% is of African ancestry. Indigenous Amerindians constitute 4.3% of the population. Raizal people constitute 0.06% of the population. Palenquero people constitute 0.02% of the population. 0.01% of the population are Roma. A study by Latinobar\u00f3metro in 2023 estimates that 50.3% of the population are Mestizo, 26.4% are White, 9.5% are Indigenous, 9.0% are Black, 4.4% are Mulatto, and 0.4% are Asian, these estimates would equate to around 26 million people being Mestizo, 14 million being White, 5 million being Indigenous, 5 million being Black, 2 million being Mulatto, and 200k being Asian.\nThe Federal Research Division estimated that the 86% of the population that did not consider themselves part of one of the ethnic groups indicated by the 2006 census, white Colombians are mainly of Spanish lineage, but there is also a large population of Middle East descent; in some areas there is a considerable input of German and Italian ancestry.\nMany of the Indigenous peoples experienced a reduction in population during the Spanish rule and many others were absorbed into the mestizo population, but the remainder currently represents over eighty distinct cultures. Reserves (\"resguardos\") established for indigenous peoples occupy (27% of the country's total) and are inhabited by more than 800,000 people. Some of the largest indigenous groups are the Wayuu, the Paez, the Pastos, the Ember\u00e1 and the Zen\u00fa. The departments of La Guajira, Cauca, Nari\u00f1o, C\u00f3rdoba and Sucre have the largest indigenous populations.\nThe Organizaci\u00f3n Nacional Ind\u00edgena de Colombia (ONIC), founded at the first National Indigenous Congress in 1982, is an organization representing the indigenous peoples of Colombia. In 1991, Colombia signed and ratified the current international law concerning indigenous peoples, Indigenous and Tribal Peoples Convention, 1989.\nSub-Saharan Africans were brought as slaves, mostly to the coastal lowlands, beginning early in the 16th century and continuing into the 19th century. Large Afro-Colombian communities are found today on the Pacific Coast. Numerous Jamaicans migrated mainly to the islands of San Andres and Providencia. A number of other Europeans and North Americans migrated to the country in the late 19th and early 20th centuries, including people from the former USSR during and after the Second World War.\nMany immigrant communities have settled on the Caribbean coast, in particular recent immigrants from the Middle East and Europe. Barranquilla (the largest city of the Colombian Caribbean) and other Caribbean cities have the largest populations of Lebanese, Palestinian, and other Levantines. There are also important communities of Romanis and Jews. There is a major migration trend of Venezuelans, due to the political and economic situation in Venezuela. In August 2019, Colombia offered citizenship to more than 24,000 children of Venezuelan refugees who were born in Colombia.\nReligion.\nThe National Administrative Department of Statistics (DANE) does not collect religious statistics, and accurate reports are difficult to obtain. However, based on various studies and a survey, about 90% of the population adheres to Christianity, the majority of which (70.9%\u201379%) are Roman Catholic, while a significant minority (16.7%) adhere to Protestantism (primarily Evangelicalism). Some 4.7% of the population is atheist or agnostic, while 3.5% claim to believe in God but do not follow a specific religion. 1.8% of Colombians adhere to Jehovah's Witnesses and Adventism and less than 1% adhere to other religions, such as the Bah\u00e1\u02bc\u00ed Faith, Islam, Judaism, Buddhism, Mormonism, Hinduism, Indigenous religions, Hare Krishna movement, Rastafari movement, Eastern Orthodox Church, and spiritual studies. The remaining people either did not respond or replied that they did not know. In addition to the above statistics, 35.9% of Colombians reported that they did not practice their faith actively. 1,519,562 people in Colombia, or around 3% of the population reported following an Indigenous religion.\nWhile Colombia remains a mostly Roman Catholic country by baptism numbers, the 1991 Colombian constitution guarantees freedom of religion and all religious faiths and churches are equally free before the law.\nHealth.\nThe overall life expectancy in Colombia at birth is 79.3 years (76.7 years for males and 81.9 years for females). Healthcare reforms have led to massive improvements in the healthcare systems of the country, with health standards in Colombia improving very much since the 1980s. The new system has widened population coverage by the social and health security system from 21% (pre-1993) to 96% in 2012. In 2017, the government declared a cancer research and treatment center as a Project of National Strategic Interest.\nA 2016 study conducted by \"Am\u00e9rica Econom\u00eda\" magazine ranked 21 Colombian health care institutions among the top 44 in Latin America, amounting to 48 percent of the total. In 2022, 26 Colombian hospitals were among the 61 best in Latin America (42% total). Also in 2023, two Colombian hospitals were among the top 75 of the world.\nEducation.\nThe educational experience of many Colombian children begins with attendance at a preschool academy until age five (\"Educaci\u00f3n preescolar\"). Basic education (\"Educaci\u00f3n b\u00e1sica\") is compulsory by law. It has two stages: Primary basic education (\"Educaci\u00f3n b\u00e1sica primaria\") which goes from first to fifth grade \u2013 children from six to ten years old, and Secondary basic education (\"Educaci\u00f3n b\u00e1sica secundaria\"), which goes from sixth to ninth grade. Basic education is followed by Middle vocational education (\"Educaci\u00f3n media vocacional\") that comprises the tenth and eleventh grades. It may have different vocational training modalities or specialties (academic, technical, business, and so on.) according to the curriculum adopted by each school.\nAfter the successful completion of all the basic and middle education years, a high-school diploma is awarded. The high-school graduate is known as a \"bachiller\", because secondary basic school and middle education are traditionally considered together as a unit called \"bachillerato\" (sixth to eleventh grade). Students in their final year of middle education take the ICFES test (now renamed Saber 11) to gain access to higher education (\"Educaci\u00f3n superior\"). This higher education includes undergraduate professional studies, technical, technological and intermediate professional education, and post-graduate studies. Technical professional institutions of Higher Education are also opened to students holder of a qualification in Arts and Business. This qualification is usually awarded by the SENA after a two years curriculum.\n\"Bachilleres\" (high-school graduates) may enter into a professional undergraduate career program offered by a university; these programs last up to five years (or less for technical, technological and intermediate professional education, and post-graduate studies), even as much to six to seven years for some careers, such as medicine. In Colombia, there is not an institution such as college; students go directly into a career program at a university or any other educational institution to obtain a professional, technical or technological title. Once graduated from the university, people are granted a (professional, technical or technological) diploma and licensed (if required) to practice the career they have chosen. For some professional career programs, students are required to take the Saber-Pro test, in their final year of undergraduate academic education.\nPublic spending on education as a proportion of gross domestic product in 2015 was 4.49%. This represented 15.05% of total government expenditure. The primary and secondary gross enrolment ratios stood at 113.56% and 98.09% respectively. School-life expectancy was 14.42 years. A total of 94.58% of the population aged 15 and older were recorded as literate, including 98.66% of those aged 15\u201324.\nUrbanization.\nColombia is a highly urbanized country with 77.1% of the population living in urban areas. The largest cities in the country are Bogot\u00e1, with 7,387,400 inhabitants, Medell\u00edn, with 2,382,399 inhabitants, Cali, with 2,172,527 inhabitants, and Barranquilla, with 1,205,284 inhabitants.\nCulture.\nColombia lies at the crossroads of Latin America and the broader American continent, and as such has been hit by a wide range of cultural influences. Native American, Spanish and other European, African, American, Caribbean, and Middle Eastern influences, as well as other Latin American cultural influences, are all present in Colombia's modern culture. Urban migration, industrialization, globalization, and other political, social and economic changes have also left an impression.\nMany national symbols, both objects and themes, have arisen from Colombia's diverse cultural traditions and aim to represent what Colombia, and the Colombian people, have in common. Cultural expressions in Colombia are promoted by the government through the Ministry of Culture.\nLiterature.\nColombian literature dates back to pre-Columbian era; a notable example of the period is the epic poem known as the \"Legend of Yurupary\". In Spanish colonial times, notable writers include Juan de Castellanos (\"Eleg\u00edas de varones ilustres de Indias\"), Hernando Dom\u00ednguez Camargo and his epic poem to San Ignacio de Loyola, Pedro Sim\u00f3n and Juan Rodr\u00edguez Freyle.\nPost-independence literature linked to Romanticism highlighted Antonio Nari\u00f1o, Jos\u00e9 Fern\u00e1ndez Madrid, Camilo Torres Tenorio and Francisco Antonio Zea. In the second half of the nineteenth century and early twentieth century the literary genre known as \"costumbrismo\" became popular; great writers of this period were Tom\u00e1s Carrasquilla, Jorge Isaacs and Rafael Pombo (the latter of whom wrote notable works of children's literature). Within that period, authors such as Jos\u00e9 Asunci\u00f3n Silva, Jos\u00e9 Eustasio Rivera, Le\u00f3n de Greiff, Porfirio Barba-Jacob and Jos\u00e9 Mar\u00eda Vargas Vila developed the modernist movement. In 1872, Colombia established the Colombian Academy of Language, the first Spanish language academy in the Americas. Candelario Obeso wrote the groundbreaking \"Cantos Populares de mi Tierra\" (1877), the first book of poetry by an Afro-Colombian author.\nBetween 1939 and 1940 seven books of poetry were published under the name \"Stone and Sky\" in the city of Bogot\u00e1 that significantly influenced the country; they were edited by the poet Jorge Rojas. In the following decade, Gonzalo Arango founded the movement of \"nothingness\" in response to the violence of the time; he was influenced by nihilism, existentialism, and the thought of another great Colombian writer: Fernando Gonz\u00e1lez Ochoa. During the boom in Latin American literature, successful writers emerged, led by Nobel laureate Gabriel Garc\u00eda M\u00e1rquez and his magnum opus, \"One Hundred Years of Solitude\", Eduardo Caballero Calder\u00f3n, Manuel Mej\u00eda Vallejo, and \u00c1lvaro Mutis, a writer who was awarded the Cervantes Prize and the Prince of Asturias Award for Letters.\nVisual arts.\nColombian art has over 3,000 years of history. Colombian artists have captured the country's changing political and cultural backdrop using a range of styles and mediums. There is archeological evidence of ceramics being produced earlier in Colombia than anywhere else in the Americas, dating as early as 3,000\u00a0BCE.\nThe earliest examples of gold craftsmanship have been attributed to the Tumaco people of the Pacific coast and date to around 325\u00a0BCE. Roughly between 200\u00a0BCE and 800 CE, the San Agust\u00edn culture, masters of stonecutting, entered its \"classical period\". They erected raised ceremonial centers, sarcophagi, and large stone monoliths depicting anthropomorphic and zoomorphic forms out of stone.\nColombian art has followed the trends of the time, so during the 16th to 18th centuries, Spanish Catholicism had a huge influence on Colombian art, and the popular baroque style was replaced with rococo when the Bourbons ascended to the Spanish crown. During this era, in the Spanish colony, the most important Neogranadine (Colombian) painters were Gregorio V\u00e1squez de Arce y Ceballos, Gaspar de Figueroa, Baltasar Vargas de Figueroa, Baltasar de Figueroa (the Elder), Antonio Acero de la Cruz and Joaqu\u00edn Guti\u00e9rrez, of which their works are preserved. Also important was Alonso de Narv\u00e1ez who, although born in the Province of Seville, spent most of his life in colonial Colombia, also the Italian Angelino Medoro, lived in Colombia and Peru, and left works of art preserved in several churches in Tunja city.\nDuring the mid-19th century, one of the most remarkable painters was Ram\u00f3n Torres M\u00e9ndez, who produced a series of good quality paintings depicting the people and their customs of different Colombian regions. Also noteworthy in the 19th century were Andr\u00e9s de Santa Mar\u00eda, Pedro Jos\u00e9 Figueroa, Epifanio Garay, Mercedes Delgado Mallarino, Jos\u00e9 Mar\u00eda Espinosa, Ricardo Acevedo Bernal, between many others.\nMore recently, Colombian artists Pedro Nel G\u00f3mez and Santiago Mart\u00ednez Delgado started the Colombian Murial Movement in the 1940s, featuring the neoclassical features of Art Deco. Since the 1950s, the Colombian art started to have a distinctive point of view, reinventing traditional elements under the concepts of the 20th century. Examples of this are the Greiff portraits by Ignacio G\u00f3mez Jaramillo, showing what the Colombian art could do with the new techniques applied to typical Colombian themes. Carlos Correa, with his paradigmatic \"Naturaleza muerta en silencio\" (silent dead nature), combines geometrical abstraction and cubism. Alejandro Obreg\u00f3n is often considered as the father of modern Colombian painting, and one of the most influential artist in this period, due to his originality, the painting of Colombian landscapes with symbolic and expressionist use of animals, (specially the Andean condor). Fernando Botero, Omar Rayo, Enrique Grau, \u00c9dgar Negret, David Manzur, Rodrigo Arenas Betancourt, Oscar Murillo, Doris Salcedo and Oscar Mu\u00f1oz are some of the Colombian artists featured at the international level.\nThe Colombian sculpture from the sixteenth to 18th centuries was mostly devoted to religious depictions of ecclesiastic art, strongly influenced by the Spanish schools of sacred sculpture. During the early period of the Colombian republic, the national artists were focused in the production of sculptural portraits of politicians and public figures, in a plain neoclassicist trend. During the 20th century, the Colombian sculpture began to develop a bold and innovative work with the aim of reaching a better understanding of national sensitivity.\nColombian photography was marked by the arrival of the daguerreotype. Jean-Baptiste Louis Gros was who brought the daguerreotype process to Colombia in 1841. The Piloto public library has Latin America's largest archive of negatives, containing 1.7\u00a0million antique photographs covering Colombia 1848 until 2005.\nThe Colombian press has promoted the work of the cartoonists. In recent decades, fanzines, internet and independent publishers have been fundamental to the growth of the comic in Colombia.\nArchitecture.\nThroughout the times, there have been a variety of architectural styles, from those of indigenous peoples to contemporary ones, passing through colonial (military and religious), Republican, transition and modern styles.\nAncient habitation areas, longhouses, crop terraces, roads as the Inca road system, cemeteries, hypogeums and necropolises are all part of the architectural heritage of indigenous peoples. Some prominent indigenous structures are the preceramic and ceramic archaeological site of Tequendama, Tierradentro (a park that contains the largest concentration of pre-Columbian monumental shaft tombs with side chambers), the largest collection of religious monuments and megalithic sculptures in South America, located in San Agust\u00edn, Huila, Lost city (an archaeological site with a series of terraces carved into the mountainside, a net of tiled roads, and several circular plazas), and the large villages mainly built with stone, wood, cane, and mud.\nArchitecture during the period of conquest and colonization is mainly derived of adapting European styles to local conditions, and Spanish influence, especially Andalusian and Extremaduran, can be easily seen. When Europeans founded cities two things were making simultaneously: the dimensioning of geometrical space (town square, street), and the location of a tangible point of orientation. The construction of forts was common throughout the Caribbean and in some cities of the interior, because of the dangers posed to Spanish colonial settlements from English, French and Dutch pirates and hostile indigenous groups. Churches, chapels, schools, and hospitals belonging to religious orders have a great urban influence. Baroque architecture is used in military buildings and public spaces. Marcelino Arroyo, Francisco Jos\u00e9 de Caldas and Domingo de Petr\u00e9s were great representatives of neo-classical architecture.\nThe National Capitol is a great representative of romanticism. Wood was extensively used in doors, windows, railings, and ceilings during the colonization of Antioquia. The Caribbean architecture acquires a strong Arabic influence. The Teatro Col\u00f3n in Bogot\u00e1 is a lavish example of architecture from the 19th century. The quintas houses with innovations in the volumetric conception are some of the best examples of the Republican architecture; the Republican action in the city focused on the design of three types of spaces: parks with forests, small urban parks and avenues and the Gothic style was most commonly used for the design of churches.\nDeco style, modern neoclassicism, eclecticism folklorist and art deco ornamental resources significantly influenced the architecture of Colombia, especially during the transition period. Modernism contributed with new construction technologies and new materials (steel, reinforced concrete, glass and synthetic materials) and the topology architecture and lightened slabs system also have a great influence. The most influential architects of the modern movement were Rogelio Salmona and Fernando Mart\u00ednez Sanabria.\nThe contemporary architecture of Colombia is designed to give greater importance to the materials, this architecture takes into account the specific natural and artificial geographies and is also an architecture that appeals to the senses. The conservation of the architectural and urban heritage of Colombia has been promoted in recent years.\nMusic.\nColombia has a vibrant collage of talent that touches a full spectrum of rhythms. It is known as the land of a thousand rhythms, at around 1,024 folk rhythms. Musicians, composers, music producers and singers from Colombia are recognized internationally such as Shakira, Juanes, Carlos Vives and others. Colombian music blends European-influenced guitar and song structure with large gaita flutes and percussion instruments from the indigenous population, while its percussion structure and dance forms come from Africa. Colombia has a diverse and dynamic musical environment.\nGuillermo Uribe Holgu\u00edn, an important cultural figure in the National Symphony Orchestra of Colombia, Luis Antonio Calvo and Blas Emilio Atehort\u00faa are some of the greatest exponents of the art music. The Bogot\u00e1 Philharmonic Orchestra is one of the most active orchestras in Colombia.\nCaribbean music has many vibrant rhythms, such as cumbia (it is played by the maracas, the drums, the gaitas and guacharaca), porro (it is a monotonous but joyful rhythm), mapal\u00e9 (with its fast rhythm and constant clapping) and the \"vallenato\", which originated in the northern part of the Caribbean coast (the rhythm is mainly played by the caja, the guacharaca, and accordion).\nThe music from the Pacific coast, such as the currulao, is characterized by its strong use of drums (instruments such as the native marimba, the conunos, the bass drum, the side drum, and the cuatro guasas or tubular rattle). An important rhythm of the south region of the Pacific coast is the contradanza (it is used in dance shows due to the striking colours of the costumes). Marimba music, traditional chants and dances from the Colombia South Pacific region are on UNESCO's Representative List of the Intangible Cultural Heritage of Humanity.\nImportant musical rhythms of the Andean Region are the danza (dance of Andean folklore arising from the transformation of the European contredance), the bambuco (it is played with guitar, tiple and mandolin, the rhythm is danced by couples), the pasillo (a rhythm inspired by the Austrian waltz and the Colombian \"danza\", the lyrics have been composed by well-known poets), the guabina (the tiple, the bandola and the requinto are the basic instruments), the sanjuanero (it originated in Tolima and Huila Departments, the rhythm is joyful and fast). Apart from these traditional rhythms, salsa music has spread throughout the country, and the city of Cali is considered by many salsa singers to be 'The New Salsa Capital of the World'.\nThe instruments that distinguish the music of the Eastern Plains are the harp, the cuatro (a type of four-stringed guitar) and maracas. Important rhythms of this region are the joropo (a fast rhythm and there is also tapping as a result of its flamenco ancestry) and the galeron (it is heard a lot while cowboys are working).\nThe music of the Amazon region is strongly influenced by the indigenous religious practices. Some of the musical instruments used are the manguar\u00e9 (a musical instrument of ceremonial type, consisting of a pair of large cylindrical drums), the quena (melodic instrument), the rondador, the congas, bells, and different types of flutes.\nThe music of the Archipelago of San Andr\u00e9s, Providencia and Santa Catalina is usually accompanied by a mandolin, a tub-bass, a jawbone, a guitar and maracas. Some popular archipelago rhythms are the Schottische, the Calypso, the Polka and the Mento.\nPopular culture.\nTheater was introduced in Colombia during the Spanish colonization in 1550 through zarzuela companies. Colombian theater is supported by the Ministry of Culture and a number of private and state owned organizations. The Ibero-American Theater Festival of Bogot\u00e1 is the cultural event of the highest importance in Colombia and one of the biggest theater festivals in the world. Other important theater events are: The Festival of Puppet The Fanfare (Medell\u00edn), The Manizales Theater Festival, The Caribbean Theatre Festival (Santa Marta) and The Art Festival of Popular Culture \"Cultural Invasion\" (Bogot\u00e1).\nAlthough the Colombian cinema is young as an industry, more recently the film industry was growing with support from the Film Act passed in 2003. Many film festivals take place in Colombia, but the two most important are the Cartagena Film Festival, which is the oldest film festival in Latin America, and the Bogot\u00e1 Film Festival.\nSome important national circulation newspapers are \"El Tiempo\" and \"El Espectador\". Television in Colombia has two privately owned TV networks and three state-owned TV networks with national coverage, as well as six regional TV networks and dozens of local TV stations. Private channels, RCN and Caracol are the highest-rated. The regional channels and regional newspapers cover a department or more and its content is made in these particular areas.\nColombia has three major national radio networks: Radiodifusora Nacional de Colombia, a state-run national radio; Caracol Radio and RCN Radio, privately owned networks with hundreds of affiliates. There are other national networks, including Cadena Super, Todelar, and Colmundo. Many hundreds of radio stations are registered with the Ministry of Information Technologies and Communications.\nCuisine.\nColombia's varied cuisine is influenced by its diverse fauna and flora as well as the cultural traditions of the ethnic groups. Colombian dishes and ingredients vary widely by region. Some of the most common ingredients are: cereals such as rice and maize; tubers such as potato and cassava; assorted legumes; meats, including beef, chicken, pork and goat; fish; and seafood. Colombia cuisine also features a variety of tropical fruits such as cape gooseberry, feijoa, araz\u00e1, dragon fruit, mangostino, granadilla, papaya, guava, mora (blackberry), lulo, soursop and passionfruit. Colombia is one of the world's largest consumers of fruit juices.\nAmong the most representative appetizers and soups are patacones (fried green plantains), sancocho de gallina (chicken soup with root vegetables) and ajiaco (potato and corn soup). Representative snacks and breads are pandebono, arepas (corn cakes), aborrajados (fried sweet plantains with cheese), torta de choclo, empanadas and almoj\u00e1banas. Representative main courses are bandeja paisa, lechona tolimense, mamona, tamales and fish dishes (such as arroz de lisa), especially in coastal regions where kibbeh, suero, coste\u00f1o cheese and carima\u00f1olas are also eaten. Representative side dishes are papas chorreadas (potatoes with cheese), remolachas rellenas con huevo duro (beets stuffed with hard-boiled egg) and arroz con coco (coconut rice). Organic food is a current trend in big cities, although in general across the country the fruits and veggies are very natural and fresh.\nRepresentative desserts are bu\u00f1uelos, natillas, Maria Luisa cake, bocadillo made of guayaba (guava jelly), cocadas (coconut balls), casquitos de guayaba (candied guava peels), torta de natas, obleas, flan de mango, rosc\u00f3n, milhoja, manjar blanco, dulce de feijoa, dulce de papayuela, torta de mojic\u00f3n, and esponjado de curuba. Typical sauces (salsas) are hogao (tomato and onion sauce) and Colombian-style aj\u00ed.\nSome representative beverages are coffee (Tinto), champ\u00fas, cholado, lulada, avena colombiana, sugarcane juice, aguapanela, aguardiente, hot chocolate and fresh fruit juices (often made with water or milk).\nSports.\nTejo is Colombia's national sport and is a team sport that involves launching projectiles to hit a target. But of all sports in Colombia, football is the most popular. Colombia was the champion of the 2001 Copa Am\u00e9rica, in which they set a new record of being undefeated, conceding no goals and winning each match. Colombia has been awarded \"mover of the year\" twice.\nColombia is a hub for roller skaters. The national team is a perennial powerhouse at the World Roller Speed Skating Championships. Colombia has traditionally been very good in cycling and a large number of Colombian cyclists have triumphed in major competitions of cycling.\nBaseball is popular in cities like Cartagena and Barranquilla. Of those cities have come good players like: Orlando Cabrera, \u00c9dgar Renter\u00eda, who was champion of the World Series in 1997 and 2010 and others who have played in Major League Baseball. Colombia was world amateur champion in 1947 and 1965.\nBoxing is one of the sports that has produced more world champions for Colombia.\nMotorsports also occupies an important place in the sporting preferences of Colombians; Juan Pablo Montoya is a race car driver known for winning 7 Formula One events. Colombia also has excelled in sports such as BMX, judo, shooting sport, taekwondo, wrestling, high diving and athletics, also has a long tradition in weightlifting and bowling."}
{"id": "5224", "revid": "48887646", "url": "https://en.wikipedia.org/wiki?curid=5224", "title": "Citizen Kane", "text": "Citizen Kane is a 1941 American drama film directed by, produced by and starring Orson Welles and co-written by Welles and Herman J. Mankiewicz. It was Welles's first feature film. \n\"Citizen Kane\" is frequently cited as the greatest film ever made. For 40 years (five decennial polls: 1962, 1972, 1982, 1992 and 2002), it stood at number one in the British Film Institute's \"Sight &amp; Sound\" decennial poll of critics, and it topped the American Film Institute's 100 Years\u00a0... 100 Movies list in 1998, as well as its 2007 update. The film was nominated for Academy Awards in nine categories and it won for Best Writing (Original Screenplay) by Mankiewicz and Welles. \"Citizen Kane\" is praised for Gregg Toland's cinematography, Robert Wise's editing, Bernard Herrmann's score and its narrative structure, all of which have been considered innovative and precedent-setting.\nThe quasi-biographical film examines the life and legacy of Charles Foster Kane, played by Welles, a composite character based on American media barons William Randolph Hearst and Joseph Pulitzer, Chicago tycoons Samuel Insull and Harold McCormick, as well as aspects of the screenwriters' own lives. Upon its release, Hearst prohibited any mention of the film in his newspapers.\nAfter the Broadway success of Welles's Mercury Theatre and the controversial 1938 radio broadcast \"The War of the Worlds\" on \"The Mercury Theatre on the Air\", Welles was courted by Hollywood. He signed a contract with RKO Pictures in 1939. Although it was unusual for an untried director, he was given freedom to develop his own story, to use his own cast and crew, and to have final cut privilege. Following two abortive attempts to get a project off the ground, he wrote the screenplay for \"Citizen Kane\" with Herman J. Mankiewicz. Principal photography took place in 1940, the same year its innovative trailer was shown, and the film was released in 1941. Cecilia Ager, reviewing it in \"PM Magazine\", wrote: \u201cSeeing it, it\u2019s as if you never really saw a movie before.\u201d\nAlthough it was a critical success, \"Citizen Kane\" failed to recoup its costs at the box office. The film faded from view after its release, but it returned to public attention when it was praised by French critics such as Andr\u00e9 Bazin and re-released in 1956. In 1958, the film was voted number nine on the prestigious Brussels 12 list at the 1958 World Expo. The Library of Congress selected \"Citizen Kane\" as an inductee of the 1989 inaugural group of 25 films for preservation in the United States National Film Registry for being \"culturally, historically, or aesthetically significant\".\nRoger Ebert wrote of it: Its surface is as much fun as any movie ever made. Its depths surpass understanding. I have analyzed it a shot at a time with more than 30 groups, and together we have seen, I believe, pretty much everything that is there on the screen. The more clearly I can see its physical manifestation, the more I am stirred by its mystery.\nPlot.\nIn a mansion called Xanadu, part of a vast palatial estate in Florida, the elderly Charles Foster Kane is on his deathbed. Holding a snow globe, he utters his last word, \"Rosebud\", and dies. A newsreel obituary tells the life story of Kane, an enormously wealthy newspaper publisher and industry magnate. Kane's death becomes sensational news around the world, and the newsreel's producer tasks reporter Jerry Thompson with discovering the meaning of \"Rosebud\".\nThompson sets out to interview Kane's friends and associates. He tries to approach Kane's second wife, Susan Alexander Kane, now an alcoholic nightclub owner, but she refuses to talk to him. Thompson goes to the private archive of the late banker Walter Parks Thatcher. Through Thatcher's written memoirs, Thompson learns about Kane's rise from a Colorado boarding house and the decline of his fortune.\nIn 1871, gold was discovered through a mining deed belonging to Kane's mother, Mary Kane. She hired Thatcher to establish a trust that would provide for Kane's education and assume guardianship of him. While the parents and Thatcher discussed arrangements inside the boarding house, the young Kane played happily with a sled in the snow outside. When Kane's parents introduced him to Thatcher and told him he would live with Thatcher, the boy struck Thatcher with his sled and attempted to run away.\nBy the time Kane gained control of his trust at the age of 25, the mine's productivity and Thatcher's prudent investing had made Kane one of the richest men in the world. Kane took control of the \"New York Inquirer\" newspaper and embarked on a career of yellow journalism, publishing scandalous articles that attacked Thatcher's (and his own) business interests. Kane sold his newspaper empire to Thatcher after the 1929 stock market crash left him short of cash.\nThompson interviews Kane's personal business manager, Mr. Bernstein. Bernstein recalls that Kane hired the best journalists available to build the \"Inquirer\"s circulation. Kane rose to power by successfully manipulating public opinion regarding the Spanish\u2013American War and marrying Emily Norton, the niece of the President of the United States.\nThompson interviews Kane's estranged best friend, Jedediah Leland, in a retirement home. Leland says that Kane's marriage to Emily disintegrated over the years, and he began an affair with amateur singer Susan Alexander while running for Governor of New York. Both his wife and his political opponent discovered the affair, and the public scandal ended his political career. Kane married Susan and forced her into a humiliating career as an opera singer (for which she had neither the talent nor the ambition). Kane arranged for a large opera house to be built in Chicago for Susan to perform in. After Leland began to write a negative review of Susan's disastrous opera debut, Kane fired him but finished the negative review and printed it. Susan protested that she never wanted the opera career anyway, but Kane forced her to continue the season.\nSusan consents to an interview with Thompson and describes the aftermath of her opera career. She attempted suicide, and Kane finally allowed her to abandon singing. After many unhappy years living at Xanadu with Kane, the two had an argument that culminated in Kane slapping Susan. Susan decided to leave Kane. Kane's butler Raymond recounts that, after Susan moved out of Xanadu, Kane began violently destroying the contents of her former bedroom. When Kane discovered a snow globe, he calmed down and tearfully said \"Rosebud\". Thompson concludes that he cannot solve the mystery and that the meaning of Kane's last word will remain unknown.\nAt Xanadu, Kane's belongings are cataloged or discarded by the mansion's staff. They find a sled, the one on which eight-year-old Kane was playing on the day that he was taken from his home in Colorado, and throw it into a furnace with other items. Unknown to the staff, the sled's trade name, printed on top, becomes visible through the flames: \"Rosebud\".\nCast.\nThe beginning of the film's ending credits states that \"Most of the principal actors in \"Citizen Kane\" are new to motion pictures. The Mercury Theatre is proud to introduce them.\" The cast is then listed in the following order, with Orson Welles' credit for playing Charles Foster Kane appearing last:\nAdditionally, Charles Bennett appears as the entertainer at the head of the chorus line in the \"Inquirer\" party sequence, and cinematographer Gregg Toland makes a cameo appearance as an interviewer depicted in part of the \"News on the March\" newsreel. Actor Alan Ladd, still unknown at that time, makes a small appearance as a reporter smoking a pipe at the end of the film.\nProduction.\nDevelopment.\nHollywood had shown interest in Welles as early as 1936. He turned down three scripts sent to him by Warner Bros. In 1937, he declined offers from David O. Selznick, who asked him to head his film company's story department, and William Wyler, who wanted him for a supporting role in \"Wuthering Heights\". \"Although the possibility of making huge amounts of money in Hollywood greatly attracted him,\" wrote biographer Frank Brady, \"he was still totally, hopelessly, insanely in love with the theater, and it is there that he had every intention of remaining to make his mark.\"\nFollowing the 1938 \"The War of the Worlds\" broadcast of his CBS radio series \"The Mercury Theatre on the Air\", Welles was lured to Hollywood with a remarkable contract. RKO Pictures studio head George J. Schaefer wanted to work with Welles after the notorious broadcast, believing that Welles had a gift for attracting mass attention. RKO was also uncharacteristically profitable and was entering into a series of independent production contracts that would add more artistically prestigious films to its roster. Throughout the spring and early summer of 1939, Schaefer constantly tried to lure the reluctant Welles to Hollywood. Welles was in financial trouble after failure of his plays \"Five Kings\" and \"The Green Goddess\". At first he simply wanted to spend three months in Hollywood and earn enough money to pay his debts and fund his next theatrical season. Welles first arrived on July 20, 1939, and on his first tour, he called the movie studio \"the greatest electric train set a boy ever had\".\nWelles signed his contract with RKO on August 21, which stipulated that Welles would act in, direct, produce and write two films. Mercury would get $100,000 for the first film by January 1, 1940, plus 20% of profits after RKO recouped $500,000, and $125,000 for a second film by January 1, 1941, plus 20% of profits after RKO recouped $500,000. The most controversial aspect of the contract was granting Welles complete artistic control of the two films so long as RKO approved both projects' stories and the budget did not exceed $500,000. RKO executives would not be allowed to see any footage until Welles chose to show it to them, and no cuts could be made to either film without Welles's approval. Welles was allowed to develop the story without interference, select his own cast and crew, and have the right of final cut. Granting the final cut privilege was unprecedented for a studio because it placed artistic considerations over financial investment. The contract was deeply resented in the film industry, and the Hollywood press took every opportunity to mock RKO and Welles. Schaefer remained a great supporter and saw the unprecedented contract as good publicity. Film scholar Robert L. Carringer wrote: \"The simple fact seems to be that Schaefer believed Welles was going to pull off something really big almost as much as Welles did himself.\"\nWelles spent the first five months of his RKO contract trying to get his first project going, without success. \"They are laying bets over on the RKO lot that the Orson Welles deal will end up without Orson ever doing a picture there,\" wrote \"The Hollywood Reporter\". It was agreed that Welles would film \"Heart of Darkness\", previously adapted for \"The Mercury Theatre on the Air\", which would be presented entirely through a first-person camera. After elaborate pre-production and a day of test shooting with a hand-held camera\u2014unheard of at the time\u2014the project never reached production because Welles was unable to trim $50,000 from its budget. Schaefer told Welles that the $500,000 budget could not be exceeded; as war loomed, revenue was declining sharply in Europe by the fall of 1939.\nHe then started work on the idea that became \"Citizen Kane\". Knowing the script would take time to prepare, Welles suggested to RKO that while that was being done\u2014\"so the year wouldn't be lost\"\u2014he make a humorous political thriller. Welles proposed \"The Smiler with a Knife\", from a novel by Cecil Day-Lewis. When that project stalled in December 1939, Welles began brainstorming other story ideas with screenwriter Herman J. Mankiewicz, who had been writing Mercury radio scripts. \"Arguing, inventing, discarding, these two powerful, headstrong, dazzlingly articulate personalities thrashed toward \"Kane\"\", wrote biographer Richard Meryman.\nScreenplay.\nWelles explained that I wished to make a motion picture which was not a narrative of action so much as an examination of character. For this, I desired a man of many sides and many aspects. It was my idea to show that six or more people could have as many widely divergent opinions concerning the nature of a single personality...There have been many motion pictures and novels rigorously obeying the formula of the 'success story.' I wished to do something quite different. I wished to make a picture which might be called a 'failure story'.\nOne of the long-standing controversies about \"Citizen Kane\" has been the authorship of the screenplay. Welles conceived the project with screenwriter Herman J. Mankiewicz, who was writing radio plays for Welles's CBS Radio series, \"The Campbell Playhouse\". Mankiewicz based the original outline on the life of William Randolph Hearst, whom he knew socially and came to hate after being exiled from Hearst's circle.\nIn February 1940 Welles supplied Mankiewicz with 300 pages of notes and put him under contract to write the first draft screenplay under the supervision of John Houseman, Welles's former partner in the Mercury Theatre. Welles later explained, \"I left him on his own finally, because we'd started to waste too much time haggling. So, after mutual agreements on storyline and character, Mank went off with Houseman and did his version, while I stayed in Hollywood and wrote mine.\" Taking these drafts, Welles drastically condensed and rearranged them, then added scenes of his own. The industry accused Welles of underplaying Mankiewicz's contribution to the script, but Welles countered the attacks by saying, \"At the end, naturally, I was the one making the picture, after all\u2014who had to make the decisions. I used what I wanted of Mank's and, rightly or wrongly, kept what I liked of my own.\"\nThe contract stated that Mankiewicz was to receive no credit for his work, as he was hired as a script doctor. Before he signed the contract Mankiewicz was advised by his agents that all credit for his work belonged to Welles and the Mercury Theatre, the \"author and creator\". As the film neared release, however, Mankiewicz began wanting a writing credit and even threatened to take out full-page advertisements in trade papers and to get his friend Ben Hecht to write an expos\u00e9 for \"The Saturday Evening Post\". Mankiewicz also threatened to go to the Screen Writers Guild and claim full credit for writing the entire script by himself.\nAfter lodging a protest with the Screen Writers Guild, Mankiewicz withdrew it, then vacillated. The question was resolved in January 1941 when the studio, RKO Pictures, awarded Mankiewicz credit. The guild credit form listed Welles first, Mankiewicz second. Welles's assistant Richard Wilson said that the person who circled Mankiewicz's name in pencil, then drew an arrow that put it in first place, was Welles. The official credit reads, \"Screenplay by Herman J. Mankiewicz and Orson Welles\". Mankiewicz's rancor toward Welles grew over the remaining twelve years of his life.\nQuestions over the authorship of the \"Citizen Kane\" screenplay were revived in 1971 by influential film critic Pauline Kael, whose controversial 50,000-word essay \"Raising Kane\" was commissioned as an introduction to the shooting script in \"The Citizen Kane Book\", published in October 1971. The book-length essay first appeared in February 1971, in two consecutive issues of \"The New Yorker\" magazine.\nIn the ensuing controversy, Welles was defended by colleagues, critics, biographers and scholars, but his reputation was damaged by its charges. The essay's thesis was later questioned and some of Kael's findings were also contested in later years.\nQuestions of authorship continued to come into sharper focus with Carringer's 1978 thoroughly researched essay, \"The Scripts of \"Citizen Kane\"\". Carringer studied the collection of script records\u2014\"almost a day-to-day record of the history of the scripting\"\u2014that was then still intact at RKO. He reviewed all seven drafts and concluded that \"the full evidence reveals that Welles's contribution to the \"Citizen Kane\" script was not only substantial but definitive.\"\nCasting.\n\"Citizen Kane\" was a rare film in that its principal roles were played by actors new to motion pictures. Ten were billed as Mercury Actors, members of the skilled repertory company assembled by Welles for the stage and radio performances of the Mercury Theatre, an independent theater company he founded with Houseman in 1937. \"He loved to use the Mercury players,\" wrote biographer Charles Higham, \"and consequently he launched several of them on movie careers.\"\nThe film represents the feature film debuts of William Alland, Ray Collins, Joseph Cotten, Agnes Moorehead, Erskine Sanford, Everett Sloane, Paul Stewart and Welles himself. Despite never having appeared in feature films, some of the cast members were already well known to the public. Cotten had recently become a Broadway star in the hit play \"The Philadelphia Story\" with Katharine Hepburn and Sloane was well known for his role on the radio show \"The Goldbergs\". Mercury actor George Coulouris was a star of the stage in New York and London.\nNot all of the cast came from the Mercury Players. Welles cast Dorothy Comingore, an actress who played supporting parts in films since 1934 using the name \"Linda Winters\", as Susan Alexander Kane. A discovery of Charlie Chaplin, Comingore was recommended to Welles by Chaplin, who then met Comingore at a party in Los Angeles and immediately cast her.\nWelles had met stage actress Ruth Warrick while visiting New York on a break from Hollywood and remembered her as a good fit for Emily Norton Kane, later saying that she looked the part. Warrick told Carringer that she was struck by the extraordinary resemblance between herself and Welles's mother when she saw a photograph of Beatrice Ives Welles. She characterized her own personal relationship with Welles as motherly.\n\"He trained us for films at the same time that he was training himself,\" recalled Agnes Moorehead. \"Orson believed in good acting, and he realized that rehearsals were needed to get the most from his actors. That was something new in Hollywood: nobody seemed interested in bringing in a group to rehearse before scenes were shot. But Orson knew it was necessary, and we rehearsed every sequence before it was shot.\"\nWhen \"The March of Time\" narrator Westbrook Van Voorhis asked for $25,000 to narrate the \"News on the March\" sequence, Alland demonstrated his ability to imitate Van Voorhis, and Welles cast him.\nWelles later said that casting character actor Gino Corrado in the small part of the waiter at the El Rancho broke his heart. Corrado had appeared in many Hollywood films, often as a waiter, and Welles wanted all of the actors to be new to films. Other uncredited roles went to Thomas A. Curran as Teddy Roosevelt in the faux newsreel; Richard Baer as Hillman, a man at Madison Square Garden, and a man in the \"News on the March\" screening room; and Alan Ladd, Arthur O'Connell and Louise Currie as reporters at Xanadu.\nFilming.\nProduction advisor Miriam Geiger quickly compiled a handmade film textbook for Welles, a practical reference book of film techniques that he studied carefully. He then taught himself filmmaking by matching its visual vocabulary to \"The Cabinet of Dr. Caligari\", which he ordered from the Museum of Modern Art, and films by Frank Capra, Ren\u00e9 Clair, Fritz Lang, King Vidor and Jean Renoir. The one film he genuinely studied was John Ford's \"Stagecoach\", which he watched 40 times. \"As it turned out, the first day I ever walked onto a set was my first day as a director,\" Welles said. \"I'd learned whatever I knew in the projection room\u2014from Ford. After dinner every night for about a month, I'd run \"Stagecoach\", often with some different technician or department head from the studio, and ask questions. 'How was this done?' 'Why was this done?' It was like going to school.\"\nWelles's cinematographer for the film was Gregg Toland, described by Welles as \"just then, the number-one cameraman in the world.\" To Welles's astonishment, Toland visited him at his office and said, \"I want you to use me on your picture.\" He had seen some of the Mercury stage productions (including \"Caesar\") and said he wanted to work with someone who had never made a movie. RKO hired Toland on loan from Samuel Goldwyn Productions in the first week of June 1940. \n\"And he never tried to impress us that he was doing any miracles,\" Welles recalled. \"I was calling for things only a beginner would have been ignorant enough to think anybody could ever do, and there he was, \"doing\" them.\" Toland later explained that he wanted to work with Welles because he anticipated the first-time director's inexperience and reputation for audacious experimentation in the theater would allow the cinematographer to try new and innovative camera techniques that typical Hollywood films would never have allowed him to do. Unaware of filmmaking protocol, Welles adjusted the lights on set as he was accustomed to doing in the theater; Toland quietly re-balanced them, and was angry when one of the crew informed Welles that he was infringing on Toland's responsibilities. During the first few weeks of June, Welles had lengthy discussions about the film with Toland and art director Perry Ferguson in the morning, and in the afternoon and evening he worked with actors and revised the script.\nOn June 29, 1940\u2014a Saturday morning when few inquisitive studio executives would be around\u2014Welles began filming \"Citizen Kane\". After the disappointment of having \"Heart of Darkness\" canceled, Welles followed Ferguson's suggestion and deceived RKO into believing that he was simply shooting camera tests. \"But we were shooting the \"picture\",\" Welles said, \"because we wanted to get started and be already into it before anybody knew about it.\"\nAt the time RKO executives were pressuring him to agree to direct a film called \"The Men from Mars\", to capitalize on \"The War of the Worlds\" radio broadcast. Welles said that he would consider making the project but wanted to make a different film first. At this time he did not inform them that he had already begun filming \"Citizen Kane\".\nThe early footage was called \"Orson Welles Tests\" on all paperwork. The first \"test\" shot was the \"News on the March\" projection room scene, economically filmed in a real studio projection room in darkness that masked many actors who appeared in other roles later in the film. \"At $809 Orson did run substantially beyond the test budget of $528\u2014to create one of the most famous scenes in movie history,\" wrote Barton Whaley.\nThe next scenes were the El Rancho nightclub scenes and the scene in which Susan attempts suicide. Welles later said that the nightclub set was available after another film had wrapped and that filming took 10 to 12 days to complete. For these scenes Welles had Comingore's throat sprayed with chemicals to give her voice a harsh, raspy tone. Other scenes shot in secret included those in which Thompson interviews Leland and Bernstein, which were also shot on sets built for other films.\nDuring production, the film was referred to as \"RKO 281\". Most of the filming took place in what is now Stage 19 on the Paramount Pictures lot in Hollywood. There was some location filming at Balboa Park in San Diego and the San Diego Zoo. Photographs of German-Jewish investment banker Otto Hermann Kahn's real-life estate Oheka Castle were used to portray the fictional Xanadu.\nIn the end of July, RKO approved the film and Welles was allowed to officially begin shooting, despite having already been filming \"tests\" for several weeks. Welles leaked stories to newspaper reporters that the \"tests\" had been so good that there was no need to re-shoot them. The first \"official\" scene to be shot was the breakfast montage sequence between Kane and his first wife Emily. To strategically save money and appease the RKO executives who opposed him, Welles rehearsed scenes extensively before actually shooting and filmed very few takes of each shot set-up. Welles never shot master shots for any scene after Toland told him that Ford never shot them. To appease the increasingly curious press, Welles threw a cocktail party for selected reporters, promising that they could watch a scene being filmed. When the journalists arrived Welles told them they had \"just finished\" shooting for the day but still had the party. Welles told the press that he was ahead of schedule (without factoring in the month of \"test shooting\"), thus discrediting claims that after a year in Hollywood without making a film he was a failure in the film industry.\nWelles usually worked 16 to 18 hours a day on the film. He often began work at 4\u00a0a.m. since the special effects make-up used to age him for certain scenes took up to four hours to apply. Welles used this time to discuss the day's shooting with Toland and other crew members. The special contact lenses used to make Welles look elderly proved very painful, and a doctor was employed to place them into Welles's eyes. Welles had difficulty seeing clearly while wearing them, which caused him to badly cut his wrist when shooting the scene in which Kane breaks up the furniture in Susan's bedroom. While shooting the scene in which Kane shouts at Gettys on the stairs of Susan Alexander's apartment building, Welles fell ten feet; an X-ray revealed two bone chips in his ankle.\nThe injury required him to direct the film from a wheelchair for two weeks.&lt;ref name=\"cinephiliabeyond/citizen-kane\"&gt;&lt;/ref&gt; He eventually wore a steel brace to resume performing on camera; it is visible in the low-angle scene between Kane and Leland after Kane loses the election. For the final scene, a stage at the Selznick studio was equipped with a working furnace, and multiple takes were required to show the sled being put into the fire and the word \"Rosebud\" consumed. Paul Stewart recalled that on the ninth take the Culver City Fire Department arrived in full gear because the furnace had grown so hot the flue caught fire. \"Orson was delighted with the commotion\", he said.\nWhen \"Rosebud\" was burned, Welles choreographed the scene while he had composer Bernard Herrmann's cue playing on the set.\nUnlike Schaefer, many members of RKO's board of governors did not like Welles or the control that his contract gave him. However such board members as Nelson Rockefeller and NBC chief David Sarnoff were sympathetic to Welles. Throughout production Welles had problems with these executives not respecting his contract's stipulation of non-interference and several spies arrived on set to report what they saw to the executives. When the executives would sometimes arrive on set unannounced the entire cast and crew would suddenly start playing softball until they left. Before official shooting began the executives intercepted all copies of the script and delayed their delivery to Welles. They had one copy sent to their office in New York, resulting in it being leaked to press.\nPrincipal shooting wrapped October 24. Welles then took several weeks away from the film for a lecture tour, during which he also scouted additional locations with Toland and Ferguson. Filming resumed November 15 with some re-shoots. Toland had to leave due to a commitment to shoot Howard Hughes' \"The Outlaw\", but Toland's camera crew continued working on the film and Toland was replaced by RKO cinematographer Harry J. Wild. The final day of shooting on November 30 was Kane's death scene. Welles boasted that he only went 21 days over his official shooting schedule, without factoring in the month of \"camera tests\". According to RKO records, the film cost $839,727. Its estimated budget had been $723,800.\nPost-production.\n\"Citizen Kane\" was edited by Robert Wise and assistant editor Mark Robson. Both would become successful film directors. Wise was hired after Welles finished shooting the \"camera tests\" and began officially making the film. Wise said that Welles \"had an older editor assigned to him for those tests and evidently he was not too happy and asked to have somebody else. I was roughly Orson's age and had several good credits.\" Wise and Robson began editing the film while it was still shooting and said that they \"could tell certainly that we were getting something very special. It was outstanding film day in and day out.\"\nWelles gave Wise detailed instructions and was usually not present during the film's editing. The film was very well planned out and intentionally shot for such post-production techniques as slow dissolves. The lack of coverage made editing easy since Welles and Toland edited the film \"in camera\" by leaving few options of how it could be put together. Wise said the breakfast table sequence took weeks to edit and get the correct \"timing\" and \"rhythm\" for the whip pans and overlapping dialogue. The \"News on the March\" sequence was edited by RKO's newsreel division to give it authenticity. They used stock footage from Path\u00e9 News and the General Film Library.\nDuring post-production Welles and special effects artist Linwood G. Dunn experimented with an optical printer to improve certain scenes that Welles found unsatisfactory from the footage. Whereas Welles was often immediately pleased with Wise's work, he would require Dunn and post-production audio engineer James G. Stewart to re-do their work several times until he was satisfied.\nWelles hired Bernard Herrmann to compose the film's score. Where most Hollywood film scores were written quickly, in as few as two or three weeks after filming was completed, Herrmann was given 12 weeks to write the music. He had sufficient time to do his own orchestrations and conducting, and worked on the film reel by reel as it was shot and cut. He wrote complete musical pieces for some of the montages, and Welles edited many of the scenes to match their length.\nStyle.\nFilm scholars and historians view \"Citizen Kane\" as Welles's attempt to create a new style of filmmaking by studying various forms of it and combining them into one. However, Welles stated that his love for cinema began only when he started working on the film. When asked where he got the confidence as a first-time director to direct a film so radically different from contemporary cinema, he responded, \"Ignorance, ignorance, sheer ignorance\u2014you know there's no confidence to equal it. It's only when you know something about a profession, I think, that you're timid or careful.\"\nDavid Bordwell wrote that \"The best way to understand \"Citizen Kane\" is to stop worshipping it as a triumph of technique.\" Bordwell argues that the film did not invent any of its famous techniques such as deep focus cinematography, shots of the ceilings, chiaroscuro lighting and temporal jump-cuts, and that many of these stylistics had been used in German Expressionist films of the 1920s, such as \"The Cabinet of Dr. Caligari\". But Bordwell asserts that the film did put them all together for the first time and perfected the medium in one single film. In a 1948 interview, D. W. Griffith said, \"I loved \"Citizen Kane\" and particularly loved the ideas he took from me.\"\nArguments against the film's cinematic innovations were made as early as 1946 when French historian Georges Sadoul wrote, \"The film is an encyclopedia of old techniques.\" He pointed out such examples as compositions that used both the foreground and the background in the films of Auguste and Louis Lumi\u00e8re, special effects used in the films of Georges M\u00e9li\u00e8s, shots of the ceiling in Erich von Stroheim's \"Greed\" and newsreel montages in the films of Dziga Vertov.\nFrench film critic Andr\u00e9 Bazin defended the film, writing: \"In this respect, the accusation of plagiarism could very well be extended to the film's use of panchromatic film or its exploitation of the properties of gelatinous silver halide.\" Bazin disagreed with Sadoul's comparison to Lumi\u00e8re's cinematography since \"Citizen Kane\" used more sophisticated lenses, but acknowledged that it had similarities to such previous works as \"The Power and the Glory\". Bazin stated that \"even if Welles did not invent the cinematic devices employed in \"Citizen Kane\", one should nevertheless credit him with the invention of their \"meaning\".\" Bazin championed the techniques in the film for its depiction of heightened reality, but Bordwell believed that the film's use of special effects contradicted some of Bazin's theories.\nStorytelling techniques.\n\"Citizen Kane\" rejects the traditional linear, chronological narrative and tells Kane's story entirely in flashbacks using different points of view, many of them from Kane's aged and forgetful associates, the cinematic equivalent of the unreliable narrator in literature. Welles also dispenses with the idea of a single storyteller and uses multiple narrators to recount Kane's life, a technique not used previously in Hollywood films. Each narrator recounts a different part of Kane's life, with each story overlapping another. The film depicts Kane as an enigma, a complicated man who leaves viewers with more questions than answers as to his character, such as the newsreel footage where he is attacked for being both a communist and a fascist.\nThe technique of flashbacks had been used in earlier films, notably \"The Power and the Glory\" (1933), but no film was as immersed in it as \"Citizen Kane\". Thompson the reporter acts as a surrogate for the audience, questioning Kane's associates and piecing together his life.\nFilms typically had an \"omniscient perspective\" at the time, which Marilyn Fabe says give the audience the \"illusion that we are looking with impunity into a world which is unaware of our gaze\". \"Citizen Kane\" also begins in that fashion until the \"News on the March\" sequence, after which we the audience see the film through the perspectives of others. The \"News on the March\" sequence gives an overview of Kane's entire life (and the film's entire story) at the beginning of the film, leaving the audience without the typical suspense of wondering how it will end. Instead, the film's repetitions of events compels the audience to analyze and wonder why Kane's life happened the way that it did, under the pretext of finding out what \"Rosebud\" means. The film then returns to the omniscient perspective in the final scene, when only the audience discovers what \"Rosebud\" is.\nCinematography.\nThe most innovative technical aspect of \"Citizen Kane\" is the extended use of deep focus, where the foreground, background and everything in between are all in sharp focus. Cinematographer Toland did this through his experimentation with lenses and lighting. Toland described the achievement in an article for \"Theatre Arts\" magazine, made possible by the sensitivity of modern speed film:\nNew developments in the science of motion picture photography are not abundant at this advanced stage of the game but periodically one is perfected to make this a greater art. Of these I am in an excellent position to discuss what is termed \"Pan-focus\", as I have been active for two years in its development and used it for the first time in \"Citizen Kane\". Through its use, it is possible to photograph action from a range of eighteen inches from the camera lens to over two hundred feet away, with extreme foreground and background figures and action both recorded in sharp relief. Hitherto, the camera had to be focused either for a close or a distant shot, all efforts to encompass both at the same time resulting in one or the other being out of focus. This handicap necessitated the breaking up of a scene into long and short angles, with much consequent loss of realism. With pan-focus, the camera, like the human eye, sees an entire panorama at once, with everything clear and lifelike.\nAnother unorthodox method used in the film was the low-angle shots facing upwards, thus allowing ceilings to be shown in the background of several scenes. Every set was built with a ceiling. which broke with studio convention, and many were constructed of fabric that concealed microphones. Welles felt that the camera should show what the eye sees, and that it was a bad theatrical convention to pretend that there was no ceiling\u2014\"a big lie in order to get all those terrible lights up there,\" he said. He became fascinated with the look of low angles, which made even dull interiors look interesting. One extremely low angle is used to photograph the encounter between Kane and Leland after Kane loses the election. A hole was dug for the camera, which required drilling into the concrete floor.\nWelles credited Toland on the same title card as himself. \"It's impossible to say how much I owe to Gregg,\" he said. \"He was superb.\" He called Toland \"the best director of photography that ever existed.\"\nSound.\n\"Citizen Kane\"s sound was recorded by Bailey Fesler and re-recorded in post-production by audio engineer James G. Stewart, both of whom had worked in radio. Stewart said that Hollywood films never deviated from a basic pattern of how sound could be recorded or used, but with Welles \"deviation from the pattern was possible because he demanded it.\" Although the film is known for its complex soundtrack, much of the audio is heard as it was recorded by Fesler and without manipulation.\nWelles used techniques from radio like overlapping dialogue. The scene in which characters sing \"Oh, Mr. Kane\" was especially complicated and required mixing several soundtracks together. He also used different \"sound perspectives\" to create the illusion of distances, such as in scenes at Xanadu where characters speak to each other at far distances. Welles experimented with sound in post-production, creating audio montages, and chose to create all of the sound effects for the film instead of using RKO's library of sound effects.\nWelles used an aural technique from radio called the \"lightning-mix\". Welles used this technique to link complex montage sequences via a series of related sounds or phrases. For example, Kane grows from a child into a young man in just two shots. As Thatcher hands eight-year-old Kane a sled and wishes him a Merry Christmas, the sequence suddenly jumps to a shot of Thatcher fifteen years later, completing the sentence he began in both the previous shot and the chronological past. Other radio techniques include using a number of voices, each saying a sentence or sometimes merely a fragment of a sentence, and splicing the dialogue together in quick succession, such as the projection room scene. The film's sound cost $16,996, but was originally budgeted at $7,288.\nFilm critic and director Fran\u00e7ois Truffaut wrote that \"Before \"Kane\", nobody in Hollywood knew how to set music properly in movies. \"Kane\" was the first, in fact the only, great film that uses radio techniques.\u00a0... A lot of filmmakers know enough to follow Auguste Renoir's advice to fill the eyes with images at all costs, but only Orson Welles understood that the sound track had to be filled in the same way.\" Cedric Belfrage of \"The Clipper\" wrote \"of all of the delectable flavours that linger on the palate after seeing \"Kane\", the use of sound is the strongest.\"\nMake-up.\nThe make-up for \"Citizen Kane\" was created and applied by Maurice Seiderman, a junior member of the RKO make-up department. He had not been accepted into the union, which recognized him as only an apprentice, but RKO nevertheless used him to make up principal actors. \"Apprentices were not supposed to make up any principals, only extras, and an apprentice could not be on a set without a journeyman present,\" wrote make-up artist Dick Smith, who became friends with Seiderman in 1979. \"During his years at RKO I suspect these rules were probably overlooked often.\" \"Seiderman had gained a reputation as one of the most inventive and creatively precise up-and-coming makeup men in Hollywood,\" wrote biographer Frank Brady.\nOn an early tour of RKO, Welles met Seiderman in the small make-up lab that he created for himself in an unused dressing room. \"Welles fastened on to him at once,\" wrote biographer Charles Higham, as Seiderman had developed his own makeup methods \"that ensured complete naturalness of expression\u2014a naturalness unrivaled in Hollywood.\" Seiderman developed a thorough plan for aging the principal characters, first making a plaster cast of the face of each of the actors who aged. He made a plaster mold of Welles's body down to the hips.\n\"My sculptural techniques for the characters' aging were handled by adding pieces of white modeling clay, which matched the plaster, onto the surface of each bust,\" Seiderman told Norman Gambill. When Seiderman achieved the desired effect, he cast the clay pieces in a soft plastic material that he formulated himself. These appliances were then placed onto the plaster bust and a four-piece mold was made for each phase of aging. The castings were then fully painted and paired with the appropriate wig for evaluation.\nBefore the actors went before the cameras each day, the pliable pieces were applied directly to their faces to recreate Seiderman's sculptural image. The facial surface was underpainted in a flexible red plastic compound; The red ground resulted in a warmth of tone that was picked up by the panchromatic film. Over that was applied liquid grease paint, and finally a colorless translucent talcum. Seiderman created the effect of skin pores on Kane's face by stippling the surface with a negative cast made from an orange peel.\nWelles often arrived on the set at 2:30\u00a0am, as application of the sculptural make-up took 3\u00bd hours for the oldest incarnation of Kane. The make-up included appliances to age Welles's shoulders, breast, and stomach. \"In the film and production photographs, you can see that Kane had a belly that overhung,\" Seiderman said. \"That was not a costume, it was the rubber sculpture that created the image. You could see how Kane's silk shirt clung wetly to the character's body. It could not have been done any other way.\"\nSeiderman worked with Charles Wright on the wigs. These went over a flexible skull cover that Seiderman created and sewed into place with elastic thread. When he found the wigs too full, he untied one hair at a time to alter their shape. Kane's mustache was inserted into the makeup surface a few hairs at a time, to realistically vary the color and texture. He also made scleral lenses for Welles, Dorothy Comingore, George Coulouris, and Everett Sloane to dull the brightness of their young eyes. The lenses took a long time to fit properly, and Seiderman began work on them before devising any of the other makeup. \"I painted them to age in phases, ending with the blood vessels and the \"arcus senilis\" of old age.\" Seiderman's tour de force was the breakfast montage, shot all in one day. \"Twelve years, two years shot at each scene,\" he said.\nThe major studios gave screen credit for make-up only to the department head. When RKO make-up department head Mel Berns refused to share credit with Seiderman, who was only an apprentice, Welles told Berns that there would be no make-up credit. Welles signed a large advertisement in the Los Angeles newspaper:\nTHANKS TO EVERYBODY WHO GETS SCREEN CREDIT FOR \"CITIZEN KANE\"AND THANKS TO THOSE WHO DON'TTO ALL THE ACTORS, THE CREW, THE OFFICE, THE MUSICIANS, EVERYBODYAND PARTICULARLY TO MAURICE SEIDERMAN, THE BEST MAKE-UP MAN IN THE WORLD\nSets.\nAlthough credited as an assistant, the film's art direction was done by Perry Ferguson. Welles and Ferguson got along during their collaboration. In the weeks before production began Welles, Toland and Ferguson met regularly to discuss the film and plan every shot, set design and prop. Ferguson would take notes during these discussions and create rough designs of the sets and story boards for individual shots. After Welles approved the rough sketches, Ferguson made miniature models for Welles and Toland to experiment on with a periscope in order to rehearse and perfect each shot. Ferguson then had detailed drawings made for the set design, including the film's lighting design. The set design was an integral part of the film's overall look and Toland's cinematography.\nIn the original script the Great Hall at Xanadu was modeled after the Great Hall in Hearst Castle and its design included a mixture of Renaissance and Gothic styles. \"The Hearstian element is brought out in the almost perverse juxtaposition of incongruous architectural styles and motifs,\" wrote Carringer. Before RKO cut the film's budget, Ferguson's designs were more elaborate and resembled the production designs of early Cecil B. DeMille films and \"Intolerance\". The budget cuts reduced Ferguson's budget by 33 percent and his work cost $58,775 total, which was below average at that time.\nTo save costs Ferguson and Welles re-wrote scenes in Xanadu's living room and transported them to the Great Hall. A large staircase from another film was found and used at no additional cost. When asked about the limited budget, Ferguson said \"Very often\u2014as in that much-discussed 'Xanadu' set in \"Citizen Kane\"\u2014we can make a foreground piece, a background piece, and imaginative lighting suggests a great deal more on the screen than actually exists on the stage.\" According to the film's official budget there were 81 sets built, but Ferguson said there were between 106 and 116.\nStill photographs of Oheka Castle in Huntington, New York, were used in the opening montage, representing Kane's Xanadu estate. Ferguson also designed statues from Kane's collection with styles ranging from Greek to German Gothic. The sets were also built to accommodate Toland's camera movements. Walls were built to fold and furniture could quickly be moved. The film's famous ceilings were made out of muslin fabric and camera boxes were built into the floors for low angle shots. Welles later said that he was proud that the film production value looked much more expensive than the film's budget. Although neither worked with Welles again, Toland and Ferguson collaborated in several films in the 1940s.\nSpecial effects.\nThe film's special effects were supervised by RKO department head Vernon L. Walker. Welles pioneered several visual effects to cheaply shoot things like crowd scenes and large interior spaces. For example, the scene in which the camera in the opera house rises dramatically to the rafters, to show the workmen showing a lack of appreciation for Susan Alexander Kane's performance, was shot by a camera craning upwards over the performance scene, then a curtain wipe to a miniature of the upper regions of the house, and then another curtain wipe matching it again with the scene of the workmen. Other scenes effectively employed miniatures to make the film look much more expensive than it truly was, such as various shots of Xanadu.\nSome shots included rear screen projection in the background, such as Thompson's interview of Leland and some of the ocean backgrounds at Xanadu. Bordwell claims that the scene where Thatcher agrees to be Kane's guardian used rear screen projection to depict young Kane in the background, despite this scene being cited as a prime example of Toland's deep focus cinematography. A special effects camera crew from Walker's department was required for the extreme close-up shots such as Kane's lips when he says \"Rosebud\" and the shot of the typewriter typing Susan's bad review.\nOptical effects artist Dunn claimed that \"up to 80 percent of some reels was optically printed.\" These shots were traditionally attributed to Toland for years. The optical printer improved some of the deep focus shots. One problem with the optical printer was that it sometimes created excessive graininess, such as the optical zoom out of the snow globe. Welles decided to superimpose snow falling to mask the graininess in these shots. Toland said that he disliked the results of the optical printer, but acknowledged that \"RKO special effects expert Vernon Walker, ASC, and his staff handled their part of the production\u2014a by no means inconsiderable assignment\u2014with ability and fine understanding.\"\nAny time deep focus was impossible\u2014as in the scene in which Kane finishes a negative review of Susan's opera while at the same time firing the person who began writing the review\u2014an optical printer was used to make the whole screen appear in focus, visually layering one piece of film onto another. However, some apparently deep-focus shots were the result of in-camera effects, as in the famous scene in which Kane breaks into Susan's room after her suicide attempt. In the background, Kane and another man break into the room, while simultaneously the medicine bottle and a glass with a spoon in it are in closeup in the foreground. The shot was an in-camera matte shot. The foreground was shot first, with the background dark. Then the background was lit, the foreground darkened, the film rewound, and the scene re-shot with the background action.\nMusic.\n\"Kane\" was the first film scored by Bernard Herrmann, who had composed for Welles for his \"Mercury Theatre on the Air\". Because it was Herrmann's first motion picture score, RKO wanted to pay him only a small fee, but Welles insisted he be paid at the same rate as Max Steiner.\nThe score established Herrmann as an important new film composer and eschewed the typical Hollywood practice of scoring a film with virtually non-stop music. Instead Herrmann used what he later described as \"radio scoring\", musical cues typically 5\u201315 seconds in length that bridge the action or suggest a different emotional response. The breakfast montage sequence begins with a graceful waltz theme and gets darker with each variation on that theme as the passage of time leads to the hardening of Kane's personality and the breakdown of his first marriage.\nHerrmann realized that musicians slated to play his music were hired for individual unique sessions; there was no need to write for existing ensembles. This meant that he was free to score for unusual combinations of instruments, even instruments that are not commonly heard. In the opening sequence, for example, the tour of Kane's estate Xanadu, Herrmann introduces a recurring leitmotif played by low woodwinds, including a quartet of alto flutes.\nFor Susan Alexander Kane's operatic sequence, Welles suggested that Herrmann compose a witty parody of a Mary Garden vehicle, an aria from \"Salammb\u00f4\". \"Our problem was to create something that would give the audience the feeling of the quicksand into which this simple little girl, having a charming but small voice, is suddenly thrown,\" Herrmann said. Writing in the style of a 19th-century French Oriental opera, Herrmann put the aria in a key that would force the singer to strain to reach the high notes, culminating in a high D, well outside the range of Susan Alexander. Soprano Jean Forward dubbed the vocal part for Comingore. Houseman claimed to have written the libretto, based on Jean Racine's \"Athalie\" and \"Phedre\", although some confusion remains since Lucille Fletcher remembered preparing the lyrics. Fletcher, then Herrmann's wife, wrote the libretto for his opera \"Wuthering Heights\".\nMusic enthusiasts consider the scene in which Susan Alexander Kane attempts to sing the famous cavatina \"Una voce poco fa\" from \"Il barbiere di Siviglia\" by Gioachino Rossini with vocal coach Signor Matiste as especially memorable for depicting the horrors of learning music through mistakes.\nSome incidental music came from other sources. Welles heard the tune used for the publisher's theme, \"Oh, Mr. Kane\", in Mexico. Called \"A Poco No\", the song was written by Pepe Gu\u00edzar and special lyrics were written by Herman Ruby. \"In a Mizz\", a 1939 jazz song by Charlie Barnet and Haven Johnson, bookends Thompson's second interview of Susan Alexander Kane. \"I kind of based the whole scene around that song,\" Welles said. \"The music is by Nat Cole\u2014it's his trio.\" Later\u2014beginning with the lyrics, \"It can't be love\"\u2014\"In a Mizz\" is performed at the Everglades picnic, framing the fight in the tent between Susan and Kane. Musicians including bandleader Cee Pee Johnson (drums), Alton Redd (vocals), Raymond Tate (trumpet), Buddy Collette (alto sax) and Buddy Banks (tenor sax) are featured. All of the music used in the newsreel came from the RKO music library, edited at Welles's request by the newsreel department to achieve what Herrmann called \"their own crazy way of cutting\". The \"News on the March\" theme that accompanies the newsreel titles is \"Belgian March\" by Anthony Collins, from the film \"Nurse Edith Cavell\". Other examples are an excerpt from Alfred Newman's score for \"Gunga Din\" (the exploration of Xanadu), Roy Webb's theme for the film \"Reno\" (the growth of Kane's empire), and bits of Webb's score for \"Five Came Back\" (introducing Walter Parks Thatcher).\nHerrmann explained his use of motifs: \nI am not a great believer in the leitmotiv as a device for motion picture music\u2014but in this film its use was practically imperative, because of the story itself and the manner in which it is unfolded. \nThere are two main motifs. One\u2014a simple four-note figure in the brass\u2014is that of Kane's power. It is given out in the first two bars of the film. The second motif is that of Rosebud. Heard as a solo in the vibraphone, it first appears in the death scene at the very beginning of the picture. It is heard again and again throughout the film in various guises, and if followed closely, is a clue to the ultimate identity of Rosebud itself.\nThe motif of power is also transformed, becoming a vigorous piece of ragtime, a hornpipe polka and, at the end of the picture, a commentary on Kane's life.\nIn 1972, Herrmann said, \"I was fortunate to start my career with a film like \"Citizen Kane\", it's been a downhill run ever since!\" Welles loved Herrmann's score and told director Henry Jaglom that it was 50 percent responsible for the film's artistic success.\nEditing.\nOne of the editing techniques used in \"Citizen Kane\" was the use of montage to collapse time and space, using an episodic sequence on the same set while the characters changed costume and make-up between cuts so that the scene following each cut would look as if it took place in the same location, but at a time long after the previous cut. In the breakfast montage, Welles chronicles the breakdown of Kane's first marriage in five vignettes that condense 16 years of story time into two minutes of screen time. Welles said that the idea for the breakfast scene \"was stolen from \"The Long Christmas Dinner\" by Thornton Wilder\u00a0... a one-act play, which is a long Christmas dinner that takes you through something like 60 years of a family's life.\" The film often uses long dissolves to signify the passage of time and its psychological effect of the characters, such as the scene in which the abandoned sled is covered with snow after the young Kane is sent away with Thatcher.\nWelles was influenced by the editing theories of Sergei Eisenstein by using jarring cuts that caused \"sudden graphic or associative contrasts\", such as the cut from Kane's deathbed to the beginning of the \"News on the March\" sequence and a sudden shot of a shrieking cockatoo at the beginning of Raymond's flashback. Although the film typically favors mise-en-sc\u00e8ne over montage, the scene in which Kane goes to Susan Alexander's apartment after first meeting her is the only one that is primarily cut as close-ups with shots and counter shots between Kane and Susan. Fabe says that \"by using a standard Hollywood technique sparingly, [Welles] revitalizes its psychological expressiveness.\"\nSources.\nWelles never confirmed a principal source for the character of Charles Foster Kane. Houseman wrote that Kane is a synthesis of different personalities, with Hearst's life used as the main source. Some events and details were invented, and Houseman wrote that he and Mankiewicz also \"grafted anecdotes from other giants of journalism, including Pulitzer, Northcliffe and Mank's first boss, Herbert Bayard Swope\". Welles said, \"Mr. Hearst was quite a bit like Kane, although Kane isn't really founded on Hearst in particular. Many people sat for it, so to speak\". He specifically acknowledged that aspects of Kane were drawn from the lives of two business tycoons familiar from his youth in Chicago\u2014Samuel Insull and Harold Fowler McCormick.\nThe character of Jedediah Leland was based on drama critic Ashton Stevens, George Stevens's uncle and Welles's close boyhood friend. Some detail came from Mankiewicz's own experience as a drama critic in New York.\nMany assumed that the character of Susan Alexander Kane was based on Marion Davies, Hearst's mistress whose career he managed. This assumption was a major reason Hearst tried to destroy \"Citizen Kane\". Welles denied that the character was based on Davies, whom he called \"an extraordinary woman\u2014nothing like the character Dorothy Comingore played in the movie.\" He cited Insull's building of the Chicago Opera House, and McCormick's lavish promotion of the opera career of his second wife, Ganna Walska, as direct influences on the screenplay.\nThe character of political boss Jim W. Gettys is based on Charles F. Murphy, a leader in New York City's infamous Tammany Hall political machine.\nWelles credited \"Rosebud\" to Mankiewicz. Biographer Richard Meryman wrote that the symbol of Mankiewicz's own damaged childhood was a treasured bicycle, stolen while he visited the public library and not replaced by his family as punishment. He regarded it as the prototype of Charles Foster Kane's sled. In his 2015 Welles biography, Patrick McGilligan reported that Mankiewicz himself stated that the word \"Rosebud\" was taken from the name of a famous racehorse, Old Rosebud. Mankiewicz had a bet on the horse in the 1914 Kentucky Derby, which he won, and McGilligan wrote that \"Old Rosebud symbolized his lost youth, and the break with his family\". In testimony for a copyright infringement suit brought by Hearst biographer Ferdinand Lundberg, Mankiewicz said, \"I had undergone psycho-analysis, and Rosebud, under circumstances slightly resembling the circumstances in [\"Citizen Kane\"], played a prominent part.\" Gore Vidal has argued in the \"New York Review of Books\" that \"Rosebud was what Hearst called his friend Marion Davies's clitoris\".\nThe \"News on the March\" sequence that begins the film satirizes the journalistic style of \"The March of Time\", the news documentary and dramatization series presented in movie theaters by Time Inc. From 1935 to 1938 Welles was a member of the uncredited company of actors that presented the original radio version.\nHouseman claimed that banker Walter P. Thatcher was loosely based on J. P. Morgan. Bernstein was named for Dr. Maurice Bernstein, appointed Welles's guardian; Sloane's portrayal was said to be based on Bernard Herrmann. Herbert Carter, editor of \"The Inquirer\", was named for actor Jack Carter.\nPolitical themes.\nLaura Mulvey explored the anti-fascist themes of \"Citizen Kane\" in her 1992 monograph for the British Film Institute. The \"News on the March\" newsreel presents Kane keeping company with Hitler and other dictators while he smugly assures the public that there will be no war. She wrote that the film reflects \"the battle between intervention and isolationism\" then being waged in the United States; the film was released six months before the attack on Pearl Harbor, while President Franklin D. Roosevelt was laboring to win public opinion for entering World War II. \"In the rhetoric of \"Citizen Kane\",\" Mulvey writes, \"the destiny of isolationism is realised in metaphor: in Kane's own fate, dying wealthy and lonely, surrounded by the detritus of European culture and history.\"\nJournalist Ignacio Ramonet has cited the film as an early example of mass media manipulation of public opinion and the power that media conglomerates have on influencing the democratic process. He believes that this early example of a media mogul influencing politics is outdated and that today \"there are media groups with the power of a thousand Citizen Kanes.\" Media mogul Rupert Murdoch is sometimes labeled as a latter-day \"Citizen Kane\".\nComparisons have also been made between the career and character of Donald Trump and Charles Foster Kane. \"Citizen Kane\" is reported to be one of Trump's favorite films, and his biographer Tim O'Brien has said that Trump is fascinated by and identifies with Kane.\nPre-release controversy.\nTo ensure that Hearst's life's influence on \"Citizen Kane\" was a secret, Welles limited access to dailies and managed the film's publicity. A December 1940 feature story in \"Stage\" magazine compared the film's narrative to \"Faust\" and made no mention of Hearst.\nThe film was scheduled to premiere at RKO's flagship theater Radio City Music Hall on February 14, but in early January 1941 Welles was not finished with post-production work and told RKO that it still needed its musical score. Writers for national magazines had early deadlines and so a rough cut was previewed for a select few on January 3, 1941 for such magazines as \"Life\", \"Look\" and \"Redbook\". Gossip columnist Hedda Hopper (an arch-rival of Louella Parsons, the Hollywood correspondent for Hearst papers) showed up to the screening uninvited. Most of the critics at the preview said that they liked the film and gave it good advanced reviews. Hopper wrote negatively about it, calling the film a \"vicious and irresponsible attack on a great man\" and criticizing its corny writing and old fashioned photography.\n\"Friday\" magazine ran an article drawing point-by-point comparisons between Kane and Hearst and documented how Welles had led on Parsons. Up until this Welles had been friendly with Parsons. The magazine quoted Welles as saying that he could not understand why she was so nice to him and that she should \"wait until the woman finds out that the picture's about her boss.\" Welles immediately denied making the statement and the editor of \"Friday\" admitted that it might be false. Welles apologized to Parsons and assured her that he had never made that remark.\nShortly after \"Friday\"s article, Hearst sent Parsons an angry letter complaining that he had learned about \"Citizen Kane\" from Hopper and not her. The incident made a fool of Parsons and compelled her to start attacking Welles and the film. Parsons demanded a private screening of the film and personally threatened Schaefer on Hearst's behalf, first with a lawsuit and then with a vague threat of consequences for everyone in Hollywood. On January 10 Parsons and two lawyers working for Hearst were given a private screening of the film. James G. Stewart was present at the screening and said that she walked out of the film.\nSoon after, Parsons called Schaefer and threatened RKO with a lawsuit if they released \"Kane\". She also contacted the management of Radio City Music Hall and demanded that they should not screen it. The next day, the front page headline in \"Daily Variety\" read, \"HEARST BANS RKO FROM PAPERS.\" Hearst began this ban by suppressing promotion of RKO's \"Kitty Foyle\", but in two weeks the ban was lifted for everything except \"Kane\".\nWhen Schaefer did not submit to Parsons she called other studio heads and made more threats on behalf of Hearst to expose the private lives of people throughout the entire film industry. Welles was then threatened with an expos\u00e9 about his romance with the married actress Dolores del R\u00edo, who wanted the affair kept secret until her divorce was finalized. In a statement to journalists Welles denied that the film was about Hearst. Hearst began preparing an injunction against the film for libel and invasion of privacy, but Welles's lawyer told him that he doubted Hearst would proceed due to the negative publicity and required testimony that an injunction would bring.\n\"The Hollywood Reporter\" ran a front-page story on January 13 that Hearst papers were about to run a series of editorials attacking Hollywood's practice of hiring refugees and immigrants for jobs that could be done by Americans. The goal was to put pressure on the other studios to force RKO to shelve \"Kane\". Many of those immigrants had fled Europe after the rise of fascism and feared losing the haven of the United States. Soon afterwards, Schaefer was approached by Nicholas Schenck, head of Metro-Goldwyn-Mayer's parent company, with an offer on the behalf of Louis B. Mayer and other Hollywood executives to RKO Pictures of $805,000 to destroy all prints of the film and burn the negative.\nOnce RKO's legal team reassured Schaefer, the studio announced on January 21 that \"Kane\" would be released as scheduled, and with one of the largest promotional campaigns in the studio's history. Schaefer brought Welles to New York City for a private screening of the film with the New York corporate heads of the studios and their lawyers. There was no objection to its release provided that certain changes, including the removal or softening of specific references that might offend Hearst, were made. Welles agreed and cut the running time from 122 minutes to 119 minutes. The cuts satisfied the corporate lawyers.\nTrailer.\nNow that the film was completed, RKO had to sell it to moviegoers. The usual method was for a studio film editor to compile a montage of highlights for a coming-attractions trailer, which would be shown to audiences shortly before the film came to their local theater. The trailer for \"Citizen Kane\" was something special, and like the feature itself was radically different from the general run. It was really a pioneer of what is now known as a teaser trailer, which piqued viewers' curiosity about the film without actually revealing any of the content.\nWritten and directed by Welles at Toland's suggestion, the \"Citizen Kane\" trailer does not feature a single second of footage of the actual film itself, but acts as a wholly original, tongue-in-cheek, pseudo-documentary piece on the film's production. Filmed at the same time as \"Citizen Kane\" itself, it offers the only existing behind-the-scenes footage of the film. The trailer, shot by staff cameraman Harry Wild instead of Toland, follows an unseen Welles as he provides narration for a tour around the film set, introductions to the film's core cast members, and a brief overview of Kane's character. The trailer also contains a number of trick shots, including one of Everett Sloane appearing at first to be running into the camera, which turns out to be the reflection of the camera in a mirror.\nAt the time, it was almost unprecedented for a film trailer to not actually feature anything of the film itself; and while \"Citizen Kane\" is frequently cited as a groundbreaking, influential film, Simon Callow argues its trailer was no less original in its approach. Callow writes that it has \"great playful charm\u00a0... it is a miniature documentary, almost an introduction to the cinema\u00a0... Teasing, charming, completely original, it is a sort of conjuring trick: Without his face appearing once on the screen, Welles entirely dominates its five [sic] minutes' duration.\"\nRelease.\nRadio City Music Hall's management refused to screen \"Citizen Kane\" for its premiere. A possible factor was Parsons's threat that \"The American Weekly\" would run a defamatory story on the grandfather of major RKO stockholder Nelson Rockefeller. Other exhibitors feared being sued for libel by Hearst and refused to show the film. In March Welles threatened the RKO board of governors with a lawsuit if they did not release the film. Schaefer stood by Welles and opposed the board of governors. When RKO still delayed the film's release Welles offered to buy the film for $1\u00a0million and the studio finally agreed to release the film on May 1.\nSchaefer managed to book a few theaters willing to show the film. Hearst papers refused to accept advertising. RKO's publicity advertisements for the film erroneously promoted it as a love story.\n\"Kane\" opened at the RKO Palace Theatre on Broadway in New York on May 1, 1941, in Chicago on May 6, and in Los Angeles on May 8. Welles said that at the Chicago premiere that he attended the theater was almost empty.\nContemporary response.\nThe day following the premiere of \"Citizen Kane\", \"The New York Times\" critic Bosley Crowther wrote that \"it comes close to being the most sensational film ever made in Hollywood... Count on Mr. Welles: he doesn't do things by halves.\u00a0... Upon the screen he discovered an area large enough for his expansive whims to have free play. And the consequence is that he has made a picture of tremendous and overpowering scope, not in physical extent so much as in its rapid and graphic rotation of thoughts. Mr. Welles has put upon the screen a motion picture that really moves\".\n\"The Washington Post\" called it \"one of the most important films in the history\" of filmmaking. \"The Washington Evening Star\" said Welles was a genius who created \"a superbly dramatic biography of another genius\" and \"a picture that is revolutionary\". \"New York Daily News\" critic Kate Cameron called it \"one of the most interesting and technically superior films that has ever come out of a Hollywood studio\". \"New York World-Telegram\" critic William Boehnel said that the film was \"staggering and belongs at once among the greatest screen achievements\". \"Time\" magazine wrote that \"it has found important new techniques in picture-making and story-telling.\" \"Life\" magazine's review said that \"few movies have ever come from Hollywood with such powerful narrative, such original technique, such exciting photography.\" John C. Mosher of \"The New Yorker\" called the film's style \"like fresh air\" and raved \"Something new has come to the movie world at last.\" Anthony Bower of \"The Nation\" called it \"brilliant\" and praised the cinematography and performances by Welles, Comingore and Cotten. John O'Hara's \"Newsweek\" review called it the best picture he'd ever seen and said Welles was \"the best actor in the history of acting.\" Welles called O'Hara's review \"the greatest review that anybody ever had.\"\nIn the UK C. A. Lejeune of \"The Observer\" called it \"The most exciting film that has come out of Hollywood in twenty-five years\" and Dilys Powell of \"The Sunday Times\" said the film's style was made \"with the ease and boldness and resource of one who controls and is not controlled by his medium.\" Edward Tangye Lean of \"Horizon\" praised the film's technical style, calling it \"perhaps a decade ahead of its contemporaries.\"\nOther reviews were mixed. Edwin Schallert of the \"Los Angeles Times\" said it was brilliant and skillful at times, but had an ending that \"rather fizzled\". The \"Chicago Tribune\" called the film interesting and different but \"its sacrifice of simplicity to eccentricity robs it of distinction and general entertainment value\". Otis Ferguson of \"The New Republic\" said it was \"the boldest free-hand stroke in major screen production since Griffith and Bitzer were running wild to unshackle the camera\", but also criticized its style, calling it a \"retrogression in film technique\" and stating that \"it holds no great place\" in film history. Ferguson reacted to some of the film's celebrated visual techniques by calling them \"just willful dabbling\" and \"the old shell game.\" In a rare film review, filmmaker Erich von Stroheim criticized the film's story and non-linear structure, but praised the technical style and performances, and wrote \"Whatever the truth may be about it, \"Citizen Kane\" is a great picture and will go down in screen history. More power to Welles!\"\nSome prominent critics wrote negative reviews. None of them dismissed the film as being altogether bad, noting the film's undeniable technical effects, but they did find fault with the narrative. Eileen Creelman of \"The New York Sun\" called it \"a cold picture, unemotional, a puzzle rather than a drama\". In his 1941 review for \"Sur\", Jorge Luis Borges famously called the film \"a labyrinth with no center\" and predicted that its legacy would be a film \"whose historical value is undeniable but which no one cares to see again.\" \"The Argus Weekend Magazine\" critic Erle Cox called the film \"amazing\" but thought that Welles's break with Hollywood traditions was \"overdone\". \"Tatler\"s James Agate called it \"the well-intentioned, muddled, amateurish thing one expects from high-brows\"; he admitted that it was \"a quite good film\" but insisted that it \"tries to run the psychological essay in harness with your detective thriller, and doesn't quite succeed.\" Other people who disliked the film were W. H. Auden and James Agee. After watching the film on January 29, 1942, future British star Kenneth Williams, then aged 15, curtly described the film in his first diary as \"boshey rot\".\nReception from the public.\nThe film did well in cities and larger towns, but it fared poorly in more remote areas. RKO still had problems getting exhibitors to show the film. For example, one chain controlling more than 500 theaters got Welles's film as part of a package but refused to play it, reportedly out of fear of Hearst. Hearst's disruption of the film's release damaged its box office performance and, as a result, it lost $160,000 during its initial run. The film earned $23,878 during its first week in New York. By the ninth week it only made $7,279. Overall it lost money in New York, Boston, Chicago, Los Angeles, San Francisco and Washington, D.C., but made a profit in Seattle.\nMoviegoers who saw the picture generally spread negative word of mouth among their neighbors, and exhibitors in the United States and Canada weren't shy about voicing their reactions, as published in \"Motion Picture Herald\". A few theater owners were discerning, recognizing the startling new techniques but conceding bad box office: \"Is likely to make your auditorium resound from vacuousness like the giant stone walls in Kane's incredible castle. Box office or no box office, this unusual film is without doubt a step toward elevating the artistic plane of the motion picture in general.\" A college-town exhibitor reported, \"I thought it was fine, as did the majority of people who attended the performances. However, there were some who either did not like it or did not get it. Business was just average.\" \"Don't try to tell me Orson Welles isn't a genius; herein he has produced a mighty fine picture, and herewith he has established for me the lowest gross that I have ever, ever experienced. I would have sworn that such ridiculous receipts were utterly impossible. If you cater to film connoisseurs, this picture is made for you. But me, I hurt all over.\" Others were more blunt: \"Nobody liked this and said so. We took in just enough to pay for it so considered ourselves very lucky.\" \"One day after showing this we still feel hesitant about walking abroad without an escort. Half of the few dozen that paid to see this masterpiece walked out, and the other half remained only to think up new dirty cracks to cast in our direction on the way out.\" \"High priced picture. But I made a little money on my help. They took off three days because they were afraid of being all alone in the theatre.\" \"You can stand in front of a mirror and call yourself 'sucker' when you play this one. It does not have one redeeming feature. It will not draw; those that do come will not know what it is all about.\" A Minnesota exhibitor summed up the situation for rural areas: \"My patrons still don't know what it was all about. Too long and too deep. No box office value to small towns.\"\nHearst's response.\nHearing about \"Citizen Kane\" enraged Hearst so much that he banned any advertising, reviewing, or mentioning of it in his papers, and had his journalists libel Welles. Welles used Hearst's opposition as a pretext for previewing the film in several opinion-making screenings in Los Angeles, lobbying for its artistic worth against the hostile campaign that Hearst was waging. A special press screening took place in early March. Henry Luce was in attendance and reportedly wanted to buy the film from RKO for $1\u00a0million to distribute it himself. The reviews for this screening were positive. A \"Hollywood Review\" headline read: \"Mr. Genius Comes Through; 'Kane' Astonishing Picture\". The \"Motion Picture Herald\" reported about the screening and Hearst's intention to sue RKO. \"Time\" wrote that \"The objection of Mr. Hearst, who founded a publishing empire on sensationalism, is ironic. For to most of the several hundred people who have seen the film at private screenings, \"Citizen Kane\" is the most sensational product of the U.S. movie industry.\" A second press screening occurred in April.\nWhen Schaefer rejected Hearst's offer to suppress the film, Hearst banned every newspaper and station in his media conglomerate from reviewing\u2014or even mentioning\u2014the film. He also had many movie theaters ban it, and many did not show it through fear of being socially exposed by his massive newspaper empire. The Oscar-nominated documentary \"The Battle Over Citizen Kane\" lays the blame for the film's relative failure squarely at the feet of Hearst. The film did decent business at the box office; it went on to be the sixth highest grossing film in its year of release, a modest success its backers found acceptable. Nevertheless, the film's commercial performance fell short of its creators' expectations. Hearst's biographer David Nasaw points out that Hearst's actions were not the only reason \"Kane\" failed, however: the innovations Welles made with narrative, as well as the dark message at the heart of the film (that the pursuit of success is ultimately futile) meant that a popular audience could not appreciate its merits.\nHearst's attacks against Welles went beyond attempting to suppress the film. Welles said that while he was on his post-filming lecture tour a police detective approached him at a restaurant and advised him not to go back to his hotel. A 14-year-old girl had reportedly been hidden in the closet of his room, and two photographers were waiting for him to walk in. Knowing he would be jailed after the resulting publicity, Welles did not return to the hotel but waited until the train left town the following morning. \"But that wasn't Hearst,\" Welles said, \"that was a hatchet man from the local Hearst paper who thought he would advance himself by doing it.\"\nIn March 1941, Welles directed a Broadway version of Richard Wright's \"Native Son\" (and, for luck, used a \"Rosebud\" sled as a prop). \"Native Son\" received positive reviews, but Hearst-owned papers used the opportunity to attack Welles as a communist. The Hearst papers vociferously attacked Welles after his April 1941 radio play, \"His Honor, the Mayor\", produced for The Free Company radio series on CBS.\nWelles described his chance encounter with Hearst in an elevator at the Fairmont Hotel on the night \"Citizen Kane\" opened in San Francisco. Hearst and Welles's father were acquaintances, so Welles introduced himself and asked Hearst if he would like to come to the opening. Hearst did not respond. \"As he was getting off at his floor, I said, 'Charles Foster Kane would have accepted.' No reply\", recalled Welles. \"And Kane would have, you know. That was his style\u2014just as he finished Jed Leland's bad review of Susan as an opera singer.\"\nIn 1945, Hearst journalist Robert Shaw wrote that the film got \"a full tide of insensate fury\" from Hearst papers, \"then it ebbed suddenly. With one brain cell working, the chief realized that such hysterical barking by the trained seals would attract too much attention to the picture. But to this day the name of Orson Welles is on the official son-of-a-bitch list of every Hearst newspaper\".\nDespite Hearst's attempts to destroy the film, since 1941 references to his life and career have usually included a reference to \"Citizen Kane\", such as the headline 'Son of Citizen Kane Dies' for the obituary of Hearst's son. In 2012, the Hearst estate agreed to screen the film at Hearst Castle in San Simeon, breaking Hearst's ban on the film.\nRe-evaluation.\nModern critics have given \"Citizen Kane\" an even more positive response. Review aggregation website Rotten Tomatoes reports that 99% of 125 critics gave the film a positive review, with an average rating of 9.70/10. In April 2021, it was noted that the addition of an 80-year-old negative review from the \"Chicago Tribune\" reduced the film's rating from 100% to 99% on the site; \"Citizen Kane\" held its 100% rating until early 2021. On Metacritic, however, the film still has a rare weighted average score of 100 out of 100 based on 19 critics, indicating \"universal acclaim\".\nThe film was included by the Vatican in a list of important films compiled in 1995, under the category of \"Art\".\nAccolades.\nIt was widely believed the film would win most of its Academy Award nominations, but it received only the award for Best Original Screenplay. \"Variety\" reported that block voting by screen extras deprived \"Citizen Kane\" of Best Picture and Best Actor, and similar prejudices were likely to have been responsible for the film receiving no technical awards.\nLegacy.\n\"Citizen Kane\" was the only film made under Welles's original contract with RKO Pictures, which gave him complete creative control. Welles's new business manager and attorney permitted the contract to lapse. In July 1941, Welles reluctantly signed a new and less favorable deal with RKO under which he produced and directed \"The Magnificent Ambersons\" (1942), produced \"Journey into Fear\" (1943), and began \"It's All True\", a film he agreed to do without payment. In the new contract Welles was an employee of the studio and lost the right to final cut, which later allowed RKO to modify and re-cut \"The Magnificent Ambersons\" over his objections. In June 1942, Schaefer resigned the presidency of RKO Pictures and Welles's contract was terminated by his successor.\nThe European release of \"Kane\" was delayed until after World War II, premiering in Paris in 1946. Initial reception by French critics was influenced by negative views from Jean-Paul Sartre and Georges Sadoul, who criticized Hollywood's cultural sophistication and the film's nostalgic use of flashbacks. However, critic Andr\u00e9 Bazin delivered a transformative speech in 1946 that shifted public opinion. Bazin praised the film for its innovative use of mise-en-sc\u00e8ne and deep focus cinematography, advocating for a filmic realism that allows audiences to engage more actively with the narrative. Bazin's essays, especially \"The Technique of Citizen Kane,\" played a crucial role in enhancing the film's reputation, arguing it revolutionized film language and aesthetics. His defense of \"Citizen Kane\" as a work of art influenced other critics and contributed to a broader re-evaluation of the film in Europe and the United States.\nIn the U.S., the film was initially neglected until it began appearing on television in the 1950s and was re-released in theaters. American film critic Andrew Sarris was significant in reviving its reputation, describing it as a profoundly influential American film. Over the decades, \"Citizen Kane\" has been consistently ranked highly in critical surveys and polls, often cited as the greatest film ever made.'\nThe film's narrative structure, cinematography and themes have influenced countless filmmakers and films worldwide, asserting its place as a cornerstone in the history of cinema. Martin Scorsese recalls: \"I saw \"Citizen Kane\" on TV...And I began to become aware of editing and camera positions...Welles was not afraid of being self-conscious with the camera and making self-referential remarks with the camera.\" Steven Spielberg says It means everything to me... It is an icon of courage...It's about courage and audacity, and I'm making this my way, and I'm going to make this my way, and I'm going to deepen the focus...we're going to see from one inch to infinity in every shot, and we're going to see ceilings, and we're going to tell a very convoluted mystery story about a man's life. It is just one of the greatest movies ever made. In 1982, Spielberg bought one of the prop sleds from \"Citizen Kane\". Spielberg called \"Kane\" 'the most classic movie ever made,\" and the sled \"a symbolic emblem of quality in the film business.\"\nRoger Ebert wrote: \nRights and home media.\nThe composited camera negative of \"Citizen Kane\" is believed to be lost forever. The most commonly reported explanation is that it was destroyed in a New Jersey film laboratory fire in the 1970s. However, in 2021, Nicolas Falacci revealed that he had been told \"the real story\" by a colleague, when he was one of two employees in the film restoration lab which assembled the 1991 \"restoration\" from the best available elements. Falacci noted that throughout the process he had daily visits in 1990-91 from an unnamed \"older RKO executive showing up every day \u2013 nervous and sweating\". According to Falacci's colleague, this elderly man was keen to cover up a clerical error he had made decades earlier when in charge of the studio's inventory, which had resulted in the original camera negatives being sent to a silver reclamation plant, destroying the nitrate film to extract its valuable silver content. Falacci's account is impossible to verify, but it would have been fully in keeping with industry standard practice for many decades, which was to destroy prints and negatives of countless older films deemed non-commercially viable, to extract the silver.\nSubsequent prints were derived from a master positive (a fine-grain preservation element) made in the 1940s and originally intended for use in overseas distribution. Modern techniques were used to produce a pristine print for a 50th Anniversary theatrical reissue in 1991 which Paramount Pictures released for then-owner Turner Broadcasting System, which earned $1.6\u00a0million in North America and worldwide.\nIn 1955, RKO sold the American television rights to its film library, including \"Citizen Kane\", to C&amp;C Television Corp. In 1960, television rights to the pre-1959 RKO's live-action library were acquired by United Artists. RKO kept the non-broadcast television rights to its library.\nIn 1976, when home video was in its infancy, entrepreneur Snuff Garrett bought cassette rights to the RKO library for what United Press International termed \"a pittance\". In 1978 The Nostalgia Merchant released the film through Media Home Entertainment. By 1980 the 800-title library of The Nostalgia Merchant was earning $2.3\u00a0million a year. \"Nobody wanted cassettes four years ago,\" Garrett told UPI. \"It wasn't the first time people called me crazy. It was a hobby with me which became big business.\" RKO Home Video released the film on VHS and Betamax in 1985.\nOn December 3, 1984, The Criterion Collection released the film as its first LaserDisc. It was made from a fine grain master positive provided by the UCLA Film and Television Archive. When told about the then-new concept of having an audio commentary on the disc, Welles was skeptical but said \"theoretically, that's good for teaching movies, so long as they don't talk nonsense.\" In 1992 Criterion released a new 50th Anniversary Edition LaserDisc. This version had an improved transfer and additional special features, including the documentary \"The Legacy of Citizen Kane\" and Welles's early short \"The Hearts of Age\".\nTurner Broadcasting System acquired broadcast television rights to the RKO library in 1986 and the full worldwide rights to the library in 1987. The RKO Home Video unit was reorganized into Turner Home Entertainment that year. In 1991 Turner released a 50th Anniversary Edition on VHS and as a collector's edition that includes the film, the documentary \"Reflections On Citizen Kane\", Harlan Lebo's 50th anniversary album, a poster and a copy of the original script. In 1996, Time Warner acquired Turner and Warner Home Video absorbed Turner Home Entertainment. In 2011, Time Warner's Warner Bros. unit had distribution rights for the film.\nIn 2001, Warner Home Video released a 60th Anniversary Collectors Edition DVD. The two-disc DVD included feature-length commentaries by Roger Ebert and Peter Bogdanovich, as well as a second DVD with the feature-length documentary \"The Battle Over Citizen Kane\" (1999). It was simultaneously released on VHS. The DVD was criticized for being \" bright, ; the dirt and grime had been cleared away, but so had a good deal of the texture, the depth, and the sense of film grain.\"\nIn 2003, Welles's daughter Beatrice Welles sued Turner Entertainment, claiming the Welles estate is the legal copyright holder of the film. She claimed that Welles's deal to terminate his contracts with RKO meant that Turner's copyright of the film was null and void. She also claimed that the estate of Orson Welles was owed 20% of the film's profits if her copyright claim was not upheld. In 2007 she was allowed to proceed with the lawsuit, overturning the 2004 decision in favor of Turner Entertainment on the issue of video rights.\nIn 2011, it was released on Blu-ray and DVD in a 70th Anniversary Edition. The \"San Francisco Chronicle\" called it \"the Blu-ray release of the year.\" Supplements included everything available on the 2001 Warner Home Video release, including \"The Battle Over Citizen Kane\" DVD. A 70th Anniversary Ultimate Collector's Edition added a third DVD with \"RKO 281\" (1999), an award-winning TV movie about the making of the film. Its packaging extras included a hardcover book and a folio containing mini reproductions of the original souvenir program, lobby cards, and production memos and correspondence. The transfer for the US releases were scanned as 4K resolution from three different 35mm prints and rectified the quality issues of the 2001 DVD. The rest of the world continued to receive home video releases based on the older transfer. This was partially rectified in 2016 with the release of the 75th Anniversary Edition in both the UK and US, which was a straight repackaging of the main disc from the 70th Anniversary Edition.\nOn August 11, 2021 Criterion announced their first 4K Ultra HD releases, a six-film slate, would include \"Citizen Kane\". Criterion indicated each title was to be available in a combo pack including a 4K UHD disc of the feature film as well as the film and special features on the companion Blu-rays. \"Citizen Kane\" was released on November 23, 2021 by the collection as a 4K and 3-Blu-ray-disc package. However, the release was recalled because at the half-hour mark on the regular Blu-ray, the contrast fell sharply, which resulted in a much darker image than what was supposed to occur. However, this issue does not apply to the 4K version itself.\nColorization controversy.\nIn the 1980s, \"Citizen Kane\" became a catalyst in the controversy over the colorization of black-and-white films. One proponent of film colorization was Ted Turner, whose Turner Entertainment Company owned the RKO library. A Turner Entertainment spokesperson initially stated that \"Citizen Kane\" would not be colorized, but in July 1988 Turner said, \"\"Citizen Kane?\" I'm thinking of colorizing it.\" In early 1989 it was reported that two companies were producing color tests for Turner Entertainment. Criticism increased when filmmaker Henry Jaglom stated that shortly before his death Welles had implored him \"don't let Ted Turner deface my movie with his crayons.\"\nIn February 1989, Turner Entertainment President Roger Mayer announced that work to colorize the film had been stopped due to provisions in Welles's 1939 contract with RKO that \"could be read to prohibit colorization without permission of the Welles estate.\" Mayer added that Welles's contract was \"quite unusual\" and \"other contracts we have checked out are not like this at all.\" Turner had only colorized the final reel of the film before abandoning the project. In 1991 one minute of the colorized test footage was included in the BBC \"Arena\" documentary \"The Complete Citizen Kane\".\nThe colorization controversy was a factor in the passage of the National Film Preservation Act in 1988 which created the National Film Registry the following year. ABC News anchor Peter Jennings reported that \"one major reason for doing this is to require people like the broadcaster Ted Turner, who's been adding color to some movies and re-editing others for television, to put notices on those versions saying that the movies have been altered\"."}
{"id": "5225", "revid": "8429272", "url": "https://en.wikipedia.org/wiki?curid=5225", "title": "Code", "text": "In communications and information processing, code is a system of rules to convert information\u2014such as a letter, word, sound, image, or gesture\u2014into another form, sometimes shortened or secret, for communication through a communication channel or storage in a storage medium. An early example is an invention of language, which enabled a person, through speech, to communicate what they thought, saw, heard, or felt to others. But speech limits the range of communication to the distance a voice can carry and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time.\nThe process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands, such as English or/and Spanish.\nOne reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaler or the arms of a semaphore tower encodes parts of the message, typically individual letters, and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.\nTheory.\nIn information theory and computer science, a code is usually considered as an algorithm that uniquely represents symbols from some source alphabet, by \"encoded\" strings, which may be in some other target alphabet. An extension of the code for representing sequences of symbols over the source alphabet is obtained by concatenating the encoded strings.\nBefore giving a mathematically precise definition, this is a brief example. The mapping\nis a code, whose source alphabet is the set formula_2 and whose target alphabet is the set formula_3. Using the extension of the code, the encoded string 0011001 can be grouped into codewords as 0\u2002011\u20020\u200201, and these in turn can be decoded to the sequence of source symbols \"acab\".\nUsing terms from formal language theory, the precise mathematical definition of this concept is as follows: let S and T be two finite sets, called the source and target alphabets, respectively. A code formula_4 is a total function mapping each symbol from S to a sequence of symbols over T. The extension formula_5 of formula_6, is a homomorphism of formula_7 into formula_8, which naturally maps each sequence of source symbols to a sequence of target symbols.\nVariable-length codes.\nIn this section, we consider codes that encode each source (clear text) character by a code word from some dictionary, and concatenation of such code words give us an encoded string. Variable-length codes are especially useful when clear text characters have different probabilities; see also entropy encoding.\nA \"prefix code\" is a code with the \"prefix property\": there is no valid code word in the system that is a prefix (start) of any other valid code word in the set. Huffman coding is the most known algorithm for deriving prefix codes. Prefix codes are widely referred to as \"Huffman codes\" even when the code was not produced by a Huffman algorithm. Other examples of prefix codes are country calling codes, the country and publisher parts of ISBNs, and the Secondary Synchronization Codes used in the UMTS WCDMA 3G Wireless Standard.\nKraft's inequality characterizes the sets of codeword lengths that are possible in a prefix code. Virtually any uniquely decodable one-to-many code, not necessarily a prefix one, must satisfy Kraft's inequality.\nError-correcting codes.\nCodes may also be used to represent data in a way more resistant to errors in transmission or storage. This so-called error-correcting code works by including carefully crafted redundancy with the stored (or transmitted) data. Examples include Hamming codes, Reed\u2013Solomon, Reed\u2013Muller, Walsh\u2013Hadamard, Bose\u2013Chaudhuri\u2013Hochquenghem, Turbo, Golay, algebraic geometry codes, low-density parity-check codes, and space\u2013time codes.\nError detecting codes can be optimised to detect \"burst errors\", or \"random errors\".\nExamples.\nCodes in communication used for brevity.\nA cable code replaces words (e.g. \"ship\" or \"invoice\") with shorter words, allowing the same information to be sent with fewer characters, more quickly, and less expensively.\nCodes can be used for brevity. When telegraph messages were the state of the art in rapid long-distance communication, elaborate systems of commercial codes that encoded complete phrases into single mouths (commonly five-minute groups) were developed, so that telegraphers became conversant with such \"words\" as \"BYOXO\" (\"Are you trying to weasel out of our deal?\"), \"LIOUY\" (\"Why do you not answer my question?\"), \"BMULD\" (\"You're a skunk!\"), or \"AYYLU\" (\"Not clearly coded, repeat more clearly.\"). Code words were chosen for various reasons: length, pronounceability, etc. Meanings were chosen to fit perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers proliferated, including one run as a front for the American Black Chamber run by Herbert Yardley between the First and Second World Wars. The purpose of most of these codes was to save on cable costs. The use of data coding for data compression predates the computer era; an early example is the telegraph Morse code where more-frequently used characters have shorter representations. Techniques such as Huffman coding are now used by computer-based algorithms to compress large data files into a more compact form for storage or transmission.\nCharacter encodings.\nCharacter encodings are representations of textual data. A given character encoding may be associated with a specific character set (the collection of characters which it can represent), though some character sets have multiple character encodings and vice versa. Character encodings may be broadly grouped according to the number of bytes required to represent a single character: there are single-byte encodings, multibyte (also called wide) encodings, and variable-width (also called variable-length) encodings. The earliest character encodings were single-byte, the best-known example of which is ASCII. ASCII remains in use today, for example in HTTP headers. However, single-byte encodings cannot model character sets with more than 256 characters. Scripts that require large character sets such as Chinese, Japanese and Korean must be represented with multibyte encodings. Early multibyte encodings were fixed-length, meaning that although each character was represented by more than one byte, all characters used the same number of bytes (\"word length\"), making them suitable for decoding with a lookup table. The final group, variable-width encodings, is a subset of multibyte encodings. These use more complex encoding and decoding logic to efficiently represent large character sets while keeping the representations of more commonly used characters shorter or maintaining backward compatibility properties. This group includes UTF-8, an encoding of the Unicode character set; UTF-8 is the most common encoding of text media on the Internet.\nGenetic code.\nBiological organisms contain genetic material that is used to control their function and development. This is DNA, which contains units named genes from which messenger RNA is derived. This in turn produces proteins through a genetic code in which a series of triplets (codons) of four possible nucleotides can be translated into one of twenty possible amino acids. A sequence of codons results in a corresponding sequence of amino acids that form a protein molecule; a type of codon called a stop codon signals the end of the sequence.\nG\u00f6del code.\nIn mathematics, a G\u00f6del code is the basis for the proof of G\u00f6del's incompleteness theorem. Here, the idea is to map mathematical notation to a natural number (using a G\u00f6del numbering).\nOther.\nThere are codes using colors, like traffic lights, the color code employed to mark the nominal value of the electrical resistors or that of the trashcans devoted to specific types of garbage (paper, glass, organic, etc.).\nIn marketing, coupon codes can be used for a financial discount or rebate when purchasing a product from a (usual internet) retailer.\nIn military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry on the battlefield, etc.\nCommunication systems for sensory impairments, such as sign language for deaf people and braille for blind people, are based on movement or tactile codes.\nMusical scores are the most common way to encode music.\nSpecific games have their own code systems to record the matches, e.g. chess notation.\nCryptography.\nIn the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead.\nSecret codes intended to obscure the real messages, ranging from serious (mainly espionage in military, diplomacy, business, etc.) to trivial (romance, games) can be any kind of imaginative encoding: flowers, game cards, clothes, fans, hats, melodies, birds, etc., in which the sole requirement is the pre-agreement on the meaning by both the sender and the receiver.\nOther examples.\nOther examples of encoding include:\nOther examples of decoding include:\nCodes and acronyms.\nAcronyms and abbreviations can be considered codes, and in a sense, all languages and writing systems are codes for human thought.\nInternational Air Transport Association airport codes are three-letter codes used to designate airports and used for bag tags. Station codes are similarly used on railways but are usually national, so the same code can be used for different stations if they are in different countries.\nOccasionally, a code word achieves an independent existence (and meaning) while the original equivalent phrase is forgotten or at least no longer has the precise meaning attributed to the code word. For example, '30' was widely used in journalism to mean \"end of story\", and has been used in other contexts to signify \"the end\"."}
{"id": "5227", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=5227", "title": "Chess Board", "text": ""}
{"id": "5228", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5228", "title": "Cheirogaleidae", "text": "The Cheirogaleidae are the family of strepsirrhine primates containing the various dwarf and mouse lemurs. Like all other lemurs, cheirogaleids live exclusively on the island of Madagascar.\nCharacteristics.\nCheirogaleids are smaller than the other lemurs and, in fact, they are the smallest primates. They have soft, long fur, colored grey-brown to reddish on top, with a generally brighter underbelly. Typically, they have small ears, large, close-set eyes, and long hind legs. Like all strepsirrhines, they have fine claws at the second toe of the hind legs. They grow to a size of only 13 to 28\u00a0cm, with a tail that is very long, sometimes up to one and a half times as long as the body. They weigh no more than 500\u00a0grams, with some species weighing as little as 60\u00a0grams.\nDwarf and mouse lemurs are nocturnal and arboreal. They are excellent climbers and can also jump far, using their long tails for balance. When on the ground (a rare occurrence), they move by hopping on their hind legs. They spend the day in tree hollows or leaf nests. Cheirogaleids are typically solitary, but sometimes live together in pairs.\nTheir eyes possess a tapetum lucidum, a light-reflecting layer that improves their night vision. Some species, such as the lesser dwarf lemur, store fat at the hind legs and the base of the tail, and hibernate. Unlike lemurids, they have long upper incisors, although they do have the comb-like teeth typical of all strepsirhines. They have the dental formula: \nCheirogaleids are omnivores, eating fruits, flowers and leaves (and sometimes nectar), as well as insects, spiders, and small vertebrates.\nThe females usually have three pairs of nipples. After a meager 60-day gestation, they will bear two to four (usually two or three) young. After five to six weeks, the young are weaned and become fully mature near the end of their first year or sometime in their second year, depending on the species. In human care, they can live for up to 15 years, although their life expectancy in the wild is probably significantly shorter.\nClassification.\nThe five genera of cheirogaleids contain 42 species."}
{"id": "5229", "revid": "33145", "url": "https://en.wikipedia.org/wiki?curid=5229", "title": "Callitrichidae", "text": "The Callitrichidae (also called Arctopitheci or Hapalidae) are a family of New World monkeys, including marmosets, tamarins, and lion tamarins. At times, this group of animals has been regarded as a subfamily, called the Callitrichinae, of the family Cebidae.\nThis taxon was traditionally thought to be a primitive lineage, from which all the larger-bodied platyrrhines evolved. However, some works argue that callitrichids are actually a dwarfed lineage.\nAncestral stem-callitrichids likely were \"normal-sized\" ceboids that were dwarfed through evolutionary time. This may exemplify a rare example of insular dwarfing in a mainland context, with the \"islands\" being formed by biogeographic barriers during arid climatic periods when forest distribution became patchy, and/or by the extensive river networks in the Amazon Basin.\nAll callitrichids are arboreal. They are the smallest of the simian primates. They eat insects, fruit, and the sap or gum from trees; occasionally, they take small vertebrates. The marmosets rely quite heavily on tree exudates, with some species (e.g. \"Callithrix jacchus\" and \"Cebuella pygmaea\") considered obligate exudativores.\nCallitrichids typically live in small, territorial groups of about five or six animals. Their social organization is unique among primates, and is called a \"cooperative polyandrous group\". This communal breeding system involves groups of multiple males and females, but only one female is reproductively active. Females mate with more than one male and each shares the responsibility of carrying the offspring.\nThey are the only primate group that regularly produces twins, which constitute over 80% of births in species that have been studied. Unlike other male primates, male callitrichids generally provide as much parental care as females. Parental duties may include carrying, protecting, feeding, comforting, and even engaging in play behavior with offspring. In some cases, such as in the cotton-top tamarin (\"Saguinus oedipus\"), males, particularly those that are paternal, even show a greater involvement in caregiving than females. The typical social structure seems to constitute a breeding group, with several of their previous offspring living in the group and providing significant help in rearing the young.\nSpecies and subspecies list.\nTaxa included in the Callitrichidae are:"}
{"id": "5230", "revid": "33145", "url": "https://en.wikipedia.org/wiki?curid=5230", "title": "Cebidae", "text": "The Cebidae are one of the five families of New World monkeys now recognised. Extant members are the capuchin and squirrel monkeys. These species are found throughout tropical and subtropical South and Central America.\nCharacteristics.\nCebid monkeys are arboreal animals that only rarely travel on the ground. They are generally small monkeys, ranging in size up to that of the brown capuchin, with a body length of 33 to 56\u00a0cm, and a weight of 2.5 to 3.9 kilograms. They are somewhat variable in form and coloration, but all have the wide, flat, noses typical of New World monkeys.\nThey are omnivorous, mostly eating fruit and insects, although the proportions of these foods vary greatly between species. They have the dental formula:\nFemales give birth to one or two young after a gestation period of between 130 and 170 days, depending on species. They are social animals, living in groups of between five and forty individuals, with the smaller species typically forming larger groups. They are generally diurnal in habit.\nClassification.\nPreviously, New World monkeys were divided between Callitrichidae and this family. For a few recent years, marmosets, tamarins, and lion tamarins were placed as a subfamily (Callitrichinae) in Cebidae, while moving other genera from Cebidae into the families Aotidae, Pitheciidae and Atelidae. The most recent classification of New World monkeys again splits the callitrichids off, leaving only the capuchins and squirrel monkeys in this family."}
{"id": "5231", "revid": "33145", "url": "https://en.wikipedia.org/wiki?curid=5231", "title": "Cercopithecidae", "text": ""}
{"id": "5232", "revid": "1271128668", "url": "https://en.wikipedia.org/wiki?curid=5232", "title": "Chondrichthyes", "text": "Chondrichthyes (; ) is a class of jawed fish that contains the cartilaginous fish or chondrichthyans, which all have skeletons primarily composed of cartilage. They can be contrasted with the Osteichthyes or \"bony fish\", which have skeletons primarily composed of bone tissue. Chondrichthyes are aquatic vertebrates with paired fins, paired nares, placoid scales, conus arteriosus in the heart, and a lack of opercula and swim bladders. Within the infraphylum Gnathostomata, cartilaginous fishes are distinct from all other jawed vertebrates.\nThe class is divided into two subclasses: Elasmobranchii (sharks, rays, skates and sawfish) and Holocephali (chimaeras, sometimes called ghost sharks, which are sometimes separated into their own class). Extant chondrichthyans range in size from the finless sleeper ray to the over whale shark.\nAnatomy.\nSkeleton.\nThe skeleton is cartilaginous. The notochord is gradually replaced by a vertebral column during development, except in Holocephali, where the notochord stays intact. In some deepwater sharks, the column is reduced.\nAs they do not have bone marrow, red blood cells are produced in the spleen and the epigonal organ (special tissue around the gonads, which is also thought to play a role in the immune system). They are also produced in the Leydig's organ, which is only found in certain cartilaginous fishes. The subclass Holocephali, which is a very specialized group, lacks both the Leydig's and epigonal organs.\nAppendages.\nApart from electric rays, which have a thick and flabby body, with soft, loose skin, chondrichthyans have tough skin covered with dermal teeth (again, Holocephali is an exception, as the teeth are lost in adults, only kept on the clasping organ seen on the caudal ventral surface of the male), also called placoid scales (or \"dermal denticles\"), making it feel like sandpaper. In most species, all dermal denticles are oriented in one direction, making the skin feel very smooth if rubbed in one direction and very rough if rubbed in the other.\nOriginally, the pectoral and pelvic girdles, which do not contain any dermal elements, did not connect. In later forms, each pair of fins became ventrally connected in the middle when scapulocoracoid and puboischiadic bars evolved. In rays, the pectoral fins are connected to the head and are very flexible.\nOne of the primary characteristics present in most sharks is the heterocercal tail, which aids in locomotion.\nBody covering.\nChondrichthyans have tooth-like scales called dermal denticles or placoid scales. Denticles usually provide protection, and in most cases, streamlining. Mucous glands exist in some species, as well.\nIt is assumed that their oral teeth evolved from dermal denticles that migrated into the mouth, but it could be the other way around, as the teleost bony fish \"Denticeps clupeoides\" has most of its head covered by dermal teeth (as does, probably, \"Atherion elymus\", another bony fish). This is most likely a secondary evolved characteristic, which means there is not necessarily a connection between the teeth and the original dermal scales.\nThe old placoderms did not have teeth at all, but had sharp bony plates in their mouth. Thus, it is unknown whether the dermal or oral teeth evolved first. It has even been suggested that the original bony plates of \"all\" vertebrates are now gone and that the present scales are just modified teeth, even if both the teeth and body armor had a common origin a long time ago. However, there is currently no evidence of this.\nRespiratory system.\nAll chondrichthyans breathe through five to seven pairs of gills, depending on the species. In general, pelagic species must keep swimming to keep oxygenated water moving through their gills, whilst demersal species can actively pump water in through their spiracles and out through their gills. However, this is only a general rule and many species differ.\nA spiracle is a small hole found behind each eye. These can be tiny and circular, such as found on the nurse shark (\"Ginglymostoma cirratum\"), to extended and slit-like, such as found on the wobbegongs (Orectolobidae). Many larger, pelagic species, such as the mackerel sharks (Lamnidae) and the thresher sharks (Alopiidae), no longer possess them.\nNervous system.\nIn chondrichthyans, the nervous system is composed of a small brain, 8\u201310 pairs of cranial nerves, and a spinal cord with spinal nerves. They have several sensory organs which provide information to be processed. Ampullae of Lorenzini are a network of small jelly filled pores called electroreceptors which help the fish sense electric fields in water. This aids in finding prey, navigation, and sensing temperature. The lateral line system has modified epithelial cells located externally which sense motion, vibration, and pressure in the water around them. Most species have large well-developed eyes. Also, they have very powerful nostrils and olfactory organs. Their inner ears consist of 3 large semicircular canals which aid in balance and orientation. Their sound detecting apparatus has limited range and is typically more powerful at lower frequencies. Some species have electric organs which can be used for defense and predation. They have relatively simple brains with the forebrain not greatly enlarged. The structure and formation of myelin in their nervous systems are nearly identical to that of tetrapods, which has led evolutionary biologists to believe that Chondrichthyes were a cornerstone group in the evolutionary timeline of myelin development.\nImmune system.\nLike all other jawed vertebrates, members of Chondrichthyes have an adaptive immune system.\nReproduction.\nFertilization is internal. Development is usually live birth (ovoviviparous species) but can be through eggs (oviparous). Some rare species are viviparous. There is no parental care after birth; however, some chondrichthyans do guard their eggs.\nCapture-induced premature birth and abortion (collectively called capture-induced parturition) occurs frequently in sharks/rays when fished. Capture-induced parturition is often mistaken for natural birth by recreational fishers and is rarely considered in commercial fisheries management despite being shown to occur in at least 12% of live bearing sharks and rays (88 species to date).\nClassification.\nThe class Chondrichthyes has two subclasses: the subclass Elasmobranchii (sharks, rays, skates, and sawfish) and the subclass Holocephali (chimaeras). To see the full list of the species, click here.\nEvolution.\nCartilaginous fish are considered to have evolved from acanthodians. The discovery of \"Entelognathus\" and several examinations of acanthodian characteristics indicate that bony fish evolved directly from placoderm like ancestors, while acanthodians represent a paraphyletic assemblage leading to Chondrichthyes. Some characteristics previously thought to be exclusive to acanthodians are also present in basal cartilaginous fish. In particular, new phylogenetic studies find cartilaginous fish to be well nested among acanthodians, with \"Doliodus\" and \"Tamiobatis\" being the closest relatives to Chondrichthyes. Recent studies vindicate this, as \"Doliodus\" had a mosaic of chondrichthyan and acanthodian traits. Dating back to the Middle and Late Ordovician Period, many isolated scales, made of dentine and bone, have a structure and growth form that is chondrichthyan-like. They may be the remains of stem-chondrichthyans, but their classification remains uncertain.\nThe earliest unequivocal fossils of acanthodian-grade cartilaginous fishes are \"Qianodus\" and \"Fanjingshania\" from the early Silurian (Aeronian) of Guizhou, China around 439 million years ago, which are also the oldest unambiguous remains of any jawed vertebrates. \"Shenacanthus vermiformis\", which lived 436 million years ago, had thoracic armour plates resembling those of placoderms.\nBy the start of the Early Devonian, 419 million years ago, jawed fishes had divided into three distinct groups: the now extinct placoderms (a paraphyletic assemblage of ancient armoured fishes), the bony fishes, and the clade that includes spiny sharks and early cartilaginous fish. The modern bony fishes, class Osteichthyes, appeared in the late Silurian or early Devonian, about 416 million years ago. The first abundant genus of shark, \"Cladoselache\", appeared in the oceans during the Devonian Period. The first cartilaginous fishes evolved from \"Doliodus\"-like spiny shark ancestors.\nTaxonomy.\n Subphylum Vertebrata\n \u2514\u2500Infraphylum Gnathostomata\n \u251c\u2500Placodermi \u2014 \"extinct\" (armored gnathostomes)\n \u2514Eugnathostomata (true jawed vertebrates)\n \u251c\u2500Acanthodii (stem cartilaginous fish)\n \u2514\u2500Chondrichthyes (true cartilaginous fish)\n \u251c\u2500Holocephali (chimaeras + several extinct clades)\n \u2514Elasmobranchii (shark and rays)\n \u251c\u2500Selachii (true sharks)\n \u2514\u2500Batoidea (rays and relatives)"}
{"id": "5233", "revid": "1827553", "url": "https://en.wikipedia.org/wiki?curid=5233", "title": "Carl Linnaeus", "text": "Carl Linnaeus (23 May 1707 \u2013 10 January 1778), also known after ennoblement in 1761 as Carl von Linn\u00e9, was a Swedish biologist and physician who formalised binomial nomenclature, the modern system of naming organisms. He is known as the \"father of modern taxonomy\". Many of his writings were in Latin; his name is rendered in Latin as ' and, after his 1761 ennoblement, as '.\nLinnaeus was the son of a curate and was born in R\u00e5shult, in the countryside of Sm\u00e5land, southern Sweden. He received most of his higher education at Uppsala University and began giving lectures in botany there in 1730. He lived abroad between 1735 and 1738, where he studied and also published the first edition of his \"\" in the Netherlands. He then returned to Sweden where he became professor of medicine and botany at Uppsala. In the 1740s, he was sent on several journeys through Sweden to find and classify plants and animals. In the 1750s and 1760s, he continued to collect and classify animals, plants, and minerals, while publishing several volumes. By the time of his death in 1778, he was one of the most acclaimed scientists in Europe.\nPhilosopher Jean-Jacques Rousseau sent him the message: \"Tell him I know no greater man on Earth.\" Johann Wolfgang von Goethe wrote: \"With the exception of William Shakespeare and Baruch Spinoza, I know no one among the no longer living who has influenced me more strongly.\" Swedish author August Strindberg wrote: \"Linnaeus was in reality a poet who happened to become a naturalist.\" Linnaeus has been called \"\" (Prince of Botanists) and \"The Pliny of the North\". He is also considered one of the founders of modern ecology.\nIn botany, the abbreviation L. is used to indicate Linnaeus as the authority for a species' name. In zoology, the abbreviation Linnaeus is generally used; the abbreviations L., Linn\u00e6us and Linn\u00e9 are also used. In older publications, the abbreviation \"Linn.\" is found. Linnaeus's remains constitute the type specimen for the species \"Homo sapiens\" following the International Code of Zoological Nomenclature, since the sole specimen that he is known to have examined was himself.\nEarly life.\nChildhood.\nLinnaeus was born in the village of R\u00e5shult in Sm\u00e5land, Sweden, on 23 May 1707. He was the first child of Nicolaus (Nils) Ingemarsson (who later adopted the family name Linnaeus) and Christina Brodersonia. His siblings were Anna Maria Linn\u00e6a, Sofia Juliana Linn\u00e6a, Samuel Linn\u00e6us (who would eventually succeed their father as rector of Stenbrohult and write a manual on beekeeping), and Emerentia Linn\u00e6a. His father taught him Latin as a small child.\nOne of a long line of peasants and priests, Nils was an amateur botanist, a Lutheran minister, and the curate of the small village of Stenbrohult in Sm\u00e5land. Christina was the daughter of the rector of Stenbrohult, Samuel Brodersonius.\nA year after Linnaeus's birth, his grandfather Samuel Brodersonius died, and his father Nils became the rector of Stenbrohult. The family moved into the rectory from the curate's house.\nEven in his early years, Linnaeus seemed to have a liking for plants, flowers in particular. Whenever he was upset, he was given a flower, which immediately calmed him. Nils spent much time in his garden and often showed flowers to Linnaeus and told him their names. Soon Linnaeus was given his own patch of earth where he could grow plants.\nCarl's father was the first in his ancestry to adopt a permanent surname. Before that, ancestors had used the patronymic naming system of Scandinavian countries: his father was named Ingemarsson after his father Ingemar Bengtsson. When Nils was admitted to the Lund University, he had to take on a family name. He adopted the Latinate name Linn\u00e6us after a giant linden tree (or lime tree), \"\" in Swedish, that grew on the family homestead. This name was spelled with the \u00e6 ligature. When Carl was born, he was named Carl Linn\u00e6us, with his father's family name. The son also always spelled it with the \u00e6 ligature, both in handwritten documents and in publications. Carl's patronymic would have been Nilsson, as in Carl Nilsson Linn\u00e6us.\nEarly education.\nLinnaeus's father began teaching him basic Latin, religion, and geography at an early age. When Linnaeus was seven, Nils decided to hire a tutor for him. The parents picked Johan Telander, a son of a local yeoman. Linnaeus did not like him, writing in his autobiography that Telander \"was better calculated to extinguish a child's talents than develop them\".\nTwo years after his tutoring had begun, he was sent to the Lower Grammar School at V\u00e4xj\u00f6 in 1717. Linnaeus rarely studied, often going to the countryside to look for plants. At some point, his father went to visit him and, after hearing critical assessments by his preceptors, he decided to put the youth as an apprentice to some honest cobbler. He reached the last year of the Lower School when he was fifteen, which was taught by the headmaster, Daniel Lannerus, who was interested in botany. Lannerus noticed Linnaeus's interest in botany and gave him the run of his garden.\nHe also introduced him to Johan Rothman, the state doctor of Sm\u00e5land and a teacher at Katedralskolan (a gymnasium) in V\u00e4xj\u00f6. Also a botanist, Rothman broadened Linnaeus's interest in botany and helped him develop an interest in medicine. By the age of 17, Linnaeus had become well acquainted with the existing botanical literature. He remarks in his journal that he \"read day and night, knowing like the back of my hand, Arvidh M\u00e5nsson's \"Rydaholm Book of Herbs\", Tillandz's \"Flora \u00c5boensis\", Palmberg's \"Serta Florea Suecana\", Bromelii's \"Chloros Gothica\" and Rudbeckii's \"Hortus Upsaliensis\"\".\nLinnaeus entered the V\u00e4xj\u00f6 Katedralskola in 1724, where he studied mainly Greek, Hebrew, theology and mathematics, a curriculum designed for boys preparing for the priesthood. In the last year at the gymnasium, Linnaeus's father visited to ask the professors how his son's studies were progressing; to his dismay, most said that the boy would never become a scholar. Rothman believed otherwise, suggesting Linnaeus could have a future in medicine. The doctor offered to have Linnaeus live with his family in V\u00e4xj\u00f6 and to teach him physiology and botany. Nils accepted this offer.\nUniversity studies.\nLund.\nRothman showed Linnaeus that botany was a serious subject. He taught Linnaeus to classify plants according to Tournefort's system. Linnaeus was also taught about the sexual reproduction of plants, according to S\u00e9bastien Vaillant. In 1727, Linnaeus, age 21, enrolled in Lund University in Sk\u00e5ne. He was registered as \"\", the Latin form of his full name, which he also used later for his Latin publications.\nProfessor Kilian Stob\u00e6us, natural scientist, physician and historian, offered Linnaeus tutoring and lodging, as well as the use of his library, which included many books about botany. He also gave the student free admission to his lectures. In his spare time, Linnaeus explored the flora of Sk\u00e5ne, together with students sharing the same interests.\nUppsala.\nIn August 1728, Linnaeus decided to attend Uppsala University on the advice of Rothman, who believed it would be a better choice if Linnaeus wanted to study both medicine and botany. Rothman based this recommendation on the two professors who taught at the medical faculty at Uppsala: Olof Rudbeck the Younger and Lars Roberg. Although Rudbeck and Roberg had undoubtedly been good professors, by then they were older and not so interested in teaching. Rudbeck no longer gave public lectures, and had others stand in for him. The botany, zoology, pharmacology and anatomy lectures were not in their best state. In Uppsala, Linnaeus met a new benefactor, Olof Celsius, who was a professor of theology and an amateur botanist. He received Linnaeus into his home and allowed him use of his library, which was one of the richest botanical libraries in Sweden.\nIn 1729, Linnaeus wrote a thesis, ' on plant sexual reproduction. This attracted the attention of Rudbeck; in May 1730, he selected Linnaeus to give lectures at the University although the young man was only a second-year student. His lectures were popular, and Linnaeus often addressed an audience of 300 people. In June, Linnaeus moved from Celsius's house to Rudbeck's to become the tutor of the three youngest of his 24 children. His friendship with Celsius did not wane and they continued their botanical expeditions. Over that winter, Linnaeus began to doubt Tournefort's system of classification and decided to create one of his own. His plan was to divide the plants by the number of stamens and pistils. He began writing several books, which would later result in, for example, ' and '. He also produced a book on the plants grown in the Uppsala Botanical Garden, '.\nRudbeck's former assistant, Nils Ros\u00e9n, returned to the University in March 1731 with a degree in medicine. Ros\u00e9n started giving anatomy lectures and tried to take over Linnaeus's botany lectures, but Rudbeck prevented that. Until December, Ros\u00e9n tutored Linnaeus privately in medicine. In December, Linnaeus had a \"disagreement\" with Rudbeck's wife and had to move out of his mentor's house; his relationship with Rudbeck did not appear to suffer. That Christmas, Linnaeus returned home to Stenbrohult to visit his parents for the first time in about three years. His mother had disapproved of his failing to become a priest, but she was pleased to learn he was teaching at the University.\nExpedition to Lapland.\nDuring a visit with his parents, Linnaeus told them about his plan to travel to Lapland; Rudbeck had made the journey in 1695, but the detailed results of his exploration were lost in a fire seven years afterwards. Linnaeus's hope was to find new plants, animals and possibly valuable minerals. He was also curious about the customs of the native Sami people, reindeer-herding nomads who wandered Scandinavia's vast tundras. In April 1732, Linnaeus was awarded a grant from the Royal Society of Sciences in Uppsala for his journey.\nLinnaeus began his expedition from Uppsala on 12 May 1732, just before he turned 25. He travelled on foot and horse, bringing with him his journal, botanical and ornithological manuscripts and sheets of paper for pressing plants. Near G\u00e4vle he found great quantities of \"Campanula serpyllifolia\", later known as \"Linnaea borealis\", the twinflower that would become his favourite. He sometimes dismounted on the way to examine a flower or rock and was particularly interested in mosses and lichens, the latter a main part of the diet of the reindeer, a common and economically important animal in Lapland.\nLinnaeus travelled clockwise around the coast of the Gulf of Bothnia, making major inland incursions from Ume\u00e5, Lule\u00e5 and Tornio. He returned from his six-month-long, over expedition in October, having gathered and observed many plants, birds and rocks. Although Lapland was a region with limited biodiversity, Linnaeus described about 100 previously unidentified plants. These became the basis of his book \"\". However, on the expedition to Lapland, Linnaeus used Latin names to describe organisms because he had not yet developed the binomial system.\nIn ' Linnaeus's ideas about nomenclature and classification were first used in a practical way, making this the first proto-modern Flora. The account covered 534 species, used the Linnaean classification system and included, for the described species, geographical distribution and taxonomic notes. It was Augustin Pyramus de Candolle who attributed Linnaeus with ' as the first example in the botanical genre of Flora writing. Botanical historian E. L. Greene described \"\" as \"the most classic and delightful\" of Linnaeus's works.\nIt was during this expedition that Linnaeus had a flash of insight regarding the classification of mammals. Upon observing the lower jawbone of a horse at the side of a road he was travelling, Linnaeus remarked: \"If I only knew how many teeth and of what kind every animal had, how many teats and where they were placed, I should perhaps be able to work out a perfectly natural system for the arrangement of all quadrupeds.\"\nIn 1734, Linnaeus led a small group of students to Dalarna. Funded by the Governor of Dalarna, the expedition was to catalogue known natural resources and discover new ones, but also to gather intelligence on Norwegian mining activities at R\u00f8ros.\nYears in the Dutch Republic (1735\u201338).\nDoctorate.\nHis relations with Nils Ros\u00e9n having worsened, Linnaeus accepted an invitation from Claes Sohlberg, son of a mining inspector, to spend the Christmas holiday in Falun, where Linnaeus was permitted to visit the mines.\nIn April 1735, at the suggestion of Sohlberg's father, Linnaeus and Sohlberg set out for the Dutch Republic, where Linnaeus intended to study medicine at the University of Harderwijk while tutoring Sohlberg in exchange for an annual salary. At the time, it was common for Swedes to pursue doctoral degrees in the Netherlands, then a highly revered place to study natural history.\nOn the way, the pair stopped in Hamburg, where they met the mayor, who proudly showed them a supposed wonder of nature in his possession: the taxidermied remains of a seven-headed hydra. Linnaeus quickly discovered the specimen was a fake, cobbled together from the jaws and paws of weasels and the skins of snakes. The provenance of the hydra suggested to Linnaeus that it had been manufactured by monks to represent the Beast of Revelation. Even at the risk of incurring the mayor's wrath, Linnaeus made his observations public, dashing the mayor's dreams of selling the hydra for an enormous sum. Linnaeus and Sohlberg were forced to flee from Hamburg.\nLinnaeus began working towards his degree as soon as he reached Harderwijk, a university known for awarding degrees in as little as a week. He submitted a dissertation, written back in Sweden, entitled \"Dissertatio medica inauguralis in qua exhibetur hypothesis nova de febrium intermittentium causa\", in which he laid out his hypothesis that malaria arose only in areas with clay-rich soils. Although he failed to identify the true source of disease transmission, (i.e., the \"Anopheles\" mosquito), he did correctly predict that \"Artemisia annua\" (wormwood) would become a source of antimalarial medications.\nWithin two weeks he had completed his oral and practical examinations and was awarded a doctoral degree.\nThat summer Linnaeus reunited with Peter Artedi, a friend from Uppsala with whom he had once made a pact that should either of the two predecease the other, the survivor would finish the decedent's work. Ten weeks later, Artedi drowned in the canals of Amsterdam, leaving behind an unfinished manuscript on the classification of fish.\nPublishing of \".\nOne of the first scientists Linnaeus met in the Netherlands was Johan Frederik Gronovius, to whom Linnaeus showed one of the several manuscripts he had brought with him from Sweden. The manuscript described a new system for classifying plants. When Gronovius saw it, he was very impressed, and offered to help pay for the printing. With an additional monetary contribution by the Scottish doctor Isaac Lawson, the manuscript was published as \" (1735).\nLinnaeus became acquainted with one of the most respected physicians and botanists in the Netherlands, Herman Boerhaave, who tried to convince Linnaeus to make a career there. Boerhaave offered him a journey to South Africa and America, but Linnaeus declined, stating he would not stand the heat. Instead, Boerhaave convinced Linnaeus that he should visit the botanist Johannes Burman. After his visit, Burman, impressed with his guest's knowledge, decided Linnaeus should stay with him during the winter. During his stay, Linnaeus helped Burman with his '. Burman also helped Linnaeus with the books on which he was working: ' and \"\".\nGeorge Clifford, Philip Miller, and Johann Jacob Dillenius.\nIn August 1735, during Linnaeus's stay with Burman, he met George Clifford III, a director of the Dutch East India Company and the owner of a rich botanical garden at the estate of Hartekamp in Heemstede. Clifford was very impressed with Linnaeus's ability to classify plants, and invited him to become his physician and superintendent of his garden. Linnaeus had already agreed to stay with Burman over the winter, and could thus not accept immediately. However, Clifford offered to compensate Burman by offering him a copy of Sir Hans Sloane's \"Natural History of Jamaica\", a rare book, if he let Linnaeus stay with him, and Burman accepted. On 24 September 1735, Linnaeus moved to Hartekamp to become personal physician to Clifford, and curator of Clifford's herbarium. He was paid 1,000 florins a year, with free board and lodging. Though the agreement was only for a winter of that year, Linnaeus practically stayed there until 1738. It was here that he wrote a book \"Hortus Cliffortianus\", in the preface of which he described his experience as \"the happiest time of my life\". (A portion of Hartekamp was declared as public garden in April 1956 by the Heemstede local authority, and was named \"Linnaeushof\". It eventually became, as it is claimed, the biggest playground in Europe.)\nIn July 1736, Linnaeus travelled to England, at Clifford's expense. He went to London to visit Sir Hans Sloane, a collector of natural history, and to see his cabinet, as well as to visit the Chelsea Physic Garden and its keeper, Philip Miller. He taught Miller about his new system of subdividing plants, as described in \"\". At first, Miller was reluctant to use the new binomial nomenclature, preferring instead the classifications of Joseph Pitton de Tournefort and John Ray. Nevertheless, Linnaeus applauded Miller's \"Gardeners Dictionary\". The conservative Miller actually retained in his dictionary a number of pre-Linnaean binomial signifiers discarded by Linnaeus but which have been retained by modern botanists. He only fully changed to the Linnaean system in the edition of \"The Gardeners Dictionary\" of 1768. Miller ultimately was impressed, and from then on started to arrange the garden according to Linnaeus's system.\nLinnaeus also travelled to Oxford University to visit the botanist Johann Jacob Dillenius. He failed to make Dillenius publicly fully accept his new classification system, though the two men remained in correspondence for many years afterwards. Linnaeus dedicated his \"Critica Botanica\" to him, as \"opus botanicum quo absolutius mundus non-vidit\". Linnaeus would later name a genus of tropical tree Dillenia in his honour. He then returned to Hartekamp, bringing with him many specimens of rare plants. The next year, 1737, he published ', in which he described 935 genera of plants, and shortly thereafter he supplemented it with ', with another sixty (\"sexaginta\") genera.\nHis work at Hartekamp led to another book, \"\", a catalogue of the botanical holdings in the herbarium and botanical garden of Hartekamp. He wrote it in nine months (completed in July 1737), but it was not published until 1738. It contains the first use of the name \"Nepenthes\", which Linnaeus used to describe a genus of pitcher plants.\nLinnaeus stayed with Clifford at Hartekamp until 18 October 1737 (new style), when he left the house to return to Sweden. Illness and the kindness of Dutch friends obliged him to stay some months longer in Holland. In May 1738, he set out for Sweden again. On the way home, he stayed in Paris for about a month, visiting botanists such as Antoine de Jussieu. After his return, Linnaeus never again left Sweden.\nReturn to Sweden.\nWhen Linnaeus returned to Sweden on 28 June 1738, he went to Falun, where he entered into an engagement to Sara Elisabeth Mor\u00e6a. Three months later, he moved to Stockholm to find employment as a physician, and thus to make it possible to support a family. Once again, Linnaeus found a patron; he became acquainted with Count Carl Gustav Tessin, who helped him get work as a physician at the Admiralty. During this time in Stockholm, Linnaeus helped found the Royal Swedish Academy of Science; he became the first Praeses of the academy by drawing of lots.\nBecause his finances had improved and were now sufficient to support a family, he received permission to marry his fianc\u00e9e, Sara Elisabeth Mor\u00e6a. Their wedding was held 26 June 1739. Seventeen months later, Sara gave birth to their first son, Carl. Two years later, a daughter, Elisabeth Christina, was born, and the subsequent year Sara gave birth to Sara Magdalena, who died when 15 days old. Sara and Linnaeus would later have four other children: Lovisa, Sara Christina, Johannes and Sophia.\nIn May 1741, Linnaeus was appointed Professor of Medicine at Uppsala University, first with responsibility for medicine-related matters. Soon, he changed place with the other Professor of Medicine, Nils Ros\u00e9n, and thus was responsible for the Botanical Garden (which he would thoroughly reconstruct and expand), botany and natural history, instead. In October that same year, his wife and nine-month-old son followed him to live in Uppsala.\n\u00d6land and Gotland.\nTen days after he was appointed professor, he undertook an expedition to the island provinces of \u00d6land and Gotland with six students from the university to look for plants useful in medicine. They stayed on \u00d6land until 21 June, then sailed to Visby in Gotland. Linnaeus and the students stayed on Gotland for about a month, and then returned to Uppsala. During this expedition, they found 100 previously unrecorded plants. The observations from the expedition were later published in ', written in Swedish. Like ', it contained both zoological and botanical observations, as well as observations concerning the culture in \u00d6land and Gotland.\nDuring the summer of 1745, Linnaeus published two more books: ' and '. ' was a strictly botanical book, while ' was zoological. Anders Celsius had created the temperature scale named after him in 1742. Celsius's scale was originally inverted compared to the way it is used today, with water boiling at 0\u00a0\u00b0C and freezing at 100\u00a0\u00b0C. Linnaeus was the one who inverted the scale to its present usage, in 1745.\nV\u00e4sterg\u00f6tland.\nIn the summer of 1746, Linnaeus was once again commissioned by the Government to carry out an expedition, this time to the Swedish province of V\u00e4sterg\u00f6tland. He set out from Uppsala on 12 June and returned on 11 August. On the expedition his primary companion was Erik Gustaf Lidbeck, a student who had accompanied him on his previous journey. Linnaeus described his findings from the expedition in the book \"\", published the next year. After he returned from the journey, the Government decided Linnaeus should take on another expedition to the southernmost province Scania. This journey was postponed, as Linnaeus felt too busy.\nIn 1747, Linnaeus was given the title archiater, or chief physician, by the Swedish king Adolf Frederick\u2014a mark of great respect. The same year he was elected member of the Academy of Sciences in Berlin.\nScania.\nIn the spring of 1749, Linnaeus could finally journey to Scania, again commissioned by the government. With him he brought his student Olof S\u00f6derberg. On the way to Scania, he made his last visit to his brothers and sisters in Stenbrohult since his father had died the previous year. The expedition was similar to the previous journeys in most aspects, but this time he was also ordered to find the best place to grow walnut and Swedish whitebeam trees; these trees were used by the military to make rifles. While there, they also visited the Raml\u00f6sa mineral spa, where he remarked on the quality of its ferruginous water. The journey was successful, and Linnaeus's observations were published the next year in \"\".\nRector of Uppsala University.\nIn 1750, Linnaeus became rector of Uppsala University, starting a period where natural sciences were esteemed. Perhaps the most important contribution he made during his time at Uppsala was to teach; many of his students travelled to various places in the world to collect botanical samples. Linnaeus called the best of these students his \"apostles\". His lectures were normally very popular and were often held in the Botanical Garden. He tried to teach the students to think for themselves and not trust anybody, not even him. Even more popular than the lectures were the botanical excursions made every Saturday during summer, where Linnaeus and his students explored the flora and fauna in the vicinity of Uppsala.\n\"Philosophia Botanica\".\nLinnaeus published \"Philosophia Botanica\" in 1751. The book contained a complete survey of the taxonomy system he had been using in his earlier works. It also contained information of how to keep a journal on travels and how to maintain a botanical garden.\n\"Nutrix Noverca\".\nDuring Linnaeus's time it was normal for upper class women to have wet nurses for their babies. Linnaeus joined an ongoing campaign to end this practice in Sweden and promote breast-feeding by mothers. In 1752 Linnaeus published a thesis along with Frederick Lindberg, a physician student, based on their experiences. In the tradition of the period, this dissertation was essentially an idea of the presiding reviewer (\"prases\") expounded upon by the student. Linnaeus's dissertation was translated into French by J. E. Gilibert in 1770 as . Linnaeus suggested that children might absorb the personality of their wet nurse through the milk. He admired the child care practices of the Lapps and pointed out how healthy their babies were compared to those of Europeans who employed wet nurses. He compared the behaviour of wild animals and pointed out how none of them denied their newborns their breastmilk. It is thought that his activism played a role in his choice of the term \"Mammalia\" for the class of organisms.\n\"Species Plantarum\".\nLinnaeus published \"Species Plantarum\", the work which is now internationally accepted as the starting point of modern botanical nomenclature, in 1753. The first volume was issued on 24 May, the second volume followed on 16 August of the same year. The book contained 1,200 pages and was published in two volumes; it described over 7,300 species. The same year the king dubbed him knight of the Order of the Polar Star, the first civilian in Sweden to become a knight in this order. He was then seldom seen not wearing the order's insignia.\nEnnoblement.\nLinnaeus felt Uppsala was too noisy and unhealthy, so he bought two farms in 1758: Hammarby and S\u00e4vja. The next year, he bought a neighbouring farm, Edeby. He spent the summers with his family at Hammarby; initially it only had a small one-storey house, but in 1762 a new, larger main building was added. In Hammarby, Linnaeus made a garden where he could grow plants that could not be grown in the Botanical Garden in Uppsala. He began constructing a museum on a hill behind Hammarby in 1766, where he moved his library and collection of plants. A fire that destroyed about one third of Uppsala and had threatened his residence there necessitated the move.\nSince the initial release of ' in 1735, the book had been expanded and reprinted several times; the tenth edition was released in 1758. This edition established itself as the starting point for zoological nomenclature, the equivalent of '.\nThe Swedish King Adolf Frederick granted Linnaeus nobility in 1757, but he was not ennobled until 1761. With his ennoblement, he took the name Carl von Linn\u00e9 (Latinised as \"\"), 'Linn\u00e9' being a shortened and gallicised version of 'Linn\u00e6us', and the German nobiliary particle 'von' signifying his ennoblement. The noble family's coat of arms prominently features a twinflower, one of Linnaeus's favourite plants; it was given the scientific name \"Linnaea borealis\" in his honour by Gronovius. The shield in the coat of arms is divided into thirds: red, black and green for the three kingdoms of nature (animal, mineral and vegetable) in Linnaean classification; in the centre is an egg \"to denote Nature, which is continued and perpetuated \"in ovo\".\" At the bottom is a phrase in Latin, borrowed from the Aeneid, which reads \"Famam extendere factis\": we extend our fame by our deeds. Linnaeus inscribed this personal motto in books that were given to him by friends.\nAfter his ennoblement, Linnaeus continued teaching and writing. In total, he presided at 186 PhD ceremonies, with many of the dissertations written by himself. His reputation had spread over the world, and he corresponded with many different people. For example, Catherine II of Russia sent him seeds from her country. He also corresponded with Giovanni Antonio Scopoli, \"the Linnaeus of the Austrian Empire\", who was a doctor and a botanist in Idrija, Duchy of Carniola (nowadays Slovenia). Scopoli communicated all of his research, findings, and descriptions (for example of the olm and the dormouse, two little animals hitherto unknown to Linnaeus). Linnaeus greatly respected Scopoli and showed great interest in his work. He named a solanaceous genus, \"Scopolia\", the source of scopolamine, after him, but because of the great distance between them, they never met.\nFinal years.\nLinnaeus was relieved of his duties in the Royal Swedish Academy of Science in 1763, but continued his work there as usual for more than ten years after. In 1769 he was elected to the American Philosophical Society for his work. He stepped down as rector at Uppsala University in December 1772, mostly due to his declining health.\nLinnaeus's last years were troubled by illness. He had had a disease called the Uppsala fever in 1764, but survived due to the care of Ros\u00e9n. He developed sciatica in 1773, and the next year, he had a stroke which partially paralysed him. He had a second stroke in 1776, losing the use of his right side and leaving him bereft of his memory; while still able to admire his own writings, he could not recognise himself as their author.\nIn December 1777, he had another stroke which greatly weakened him, and eventually led to his death on 10 January 1778 in Hammarby. Despite his desire to be buried in Hammarby, he was buried in Uppsala Cathedral on 22 January.\nHis library and collections were left to his widow Sara and their children. Joseph Banks, an eminent botanist, wished to purchase the collection, but his son Carl refused the offer and instead moved the collection to Uppsala. In 1783 Carl died and Sara inherited the collection, having outlived both her husband and son. She tried to sell it to Banks, but he was no longer interested; instead an acquaintance of his agreed to buy the collection. The acquaintance was a 24-year-old medical student, James Edward Smith, who bought the whole collection: 14,000 plants, 3,198 insects, 1,564 shells, about 3,000 letters and 1,600 books. Smith founded the Linnean Society of London five years later.\nThe von Linn\u00e9 name ended with his son Carl, who never married. His other son, Johannes, had died aged 3. There are over two hundred descendants of Linnaeus through two of his daughters.\nApostles.\nDuring Linnaeus's time as Professor and Rector of Uppsala University, he taught many devoted students, 17 of whom he called \"apostles\". They were the most promising, most committed students, and all of them made botanical expeditions to various places in the world, often with his help. The amount of this help varied; sometimes he used his influence as Rector to grant his apostles a scholarship or a place on an expedition. To most of the apostles he gave instructions of what to look for on their journeys. Abroad, the apostles collected and organised new plants, animals and minerals according to Linnaeus's system. Most of them also gave some of their collection to Linnaeus when their journey was finished. Thanks to these students, the Linnaean system of taxonomy spread through the world without Linnaeus ever having to travel outside Sweden after his return from Holland. The British botanist William T. Stearn notes, without Linnaeus's new system, it would not have been possible for the apostles to collect and organise so many new specimens. Many of the apostles died during their expeditions.\nEarly expeditions.\nChristopher T\u00e4rnstr\u00f6m, the first apostle and a 43-year-old pastor with a wife and children, made his journey in 1746. He boarded a Swedish East India Company ship headed for China. T\u00e4rnstr\u00f6m never reached his destination, dying of a tropical fever on C\u00f4n S\u01a1n Island the same year. T\u00e4rnstr\u00f6m's widow blamed Linnaeus for making her children fatherless, causing Linnaeus to prefer sending out younger, unmarried students after T\u00e4rnstr\u00f6m. Six other apostles later died on their expeditions, including Pehr Forssk\u00e5l and Pehr L\u00f6fling.\nTwo years after T\u00e4rnstr\u00f6m's expedition, Finnish-born Pehr Kalm set out as the second apostle to North America. There he spent two-and-a-half years studying the flora and fauna of Pennsylvania, New York, New Jersey and Canada. Linnaeus was overjoyed when Kalm returned, bringing back with him many pressed flowers and seeds. At least 90 of the 700 North American species described in \"Species Plantarum\" had been brought back by Kalm.\nCook expeditions and Japan.\nDaniel Solander was living in Linnaeus's house during his time as a student in Uppsala. Linnaeus was very fond of him, promising Solander his eldest daughter's hand in marriage. On Linnaeus's recommendation, Solander travelled to England in 1760, where he met the English botanist Joseph Banks. With Banks, Solander joined James Cook on his expedition to Oceania on the \"Endeavour\" in 1768\u201371. Solander was not the only apostle to journey with James Cook; Anders Sparrman followed on the \"Resolution\" in 1772\u201375 bound for, among other places, Oceania and South America. Sparrman made many other expeditions, one of them to South Africa.\nPerhaps the most famous and successful apostle was Carl Peter Thunberg, who embarked on a nine-year expedition in 1770. He stayed in South Africa for three years, then travelled to Japan. All foreigners in Japan were forced to stay on the island of Dejima outside Nagasaki, so it was thus hard for Thunberg to study the flora. He did, however, manage to persuade some of the translators to bring him different plants, and he also found plants in the gardens of Dejima. He returned to Sweden in 1779, one year after Linnaeus's death.\nMajor publications.\n\"Systema Naturae\".\nThe first edition of \"\" was printed in the Netherlands in 1735. It was a twelve-page work. By the time it reached its 10th edition in 1758, it classified 4,400 species of animals and 7,700 species of plants. People from all over the world sent their specimens to Linnaeus to be included. By the time he started work on the 12th edition, Linnaeus needed a new invention\u2014the index card\u2014to track classifications.\nIn \"Systema Naturae\", the unwieldy names mostly used at the time, such as \"\", were supplemented with concise and now familiar \"binomials\", composed of the generic name, followed by a specific epithet\u2014in the case given, \"Physalis angulata\". These binomials could serve as a label to refer to the species. Higher taxa were constructed and arranged in a simple and orderly manner. Although the system, now known as binomial nomenclature, was partially developed by the Bauhin brothers (see Gaspard Bauhin and Johann Bauhin) almost 200 years earlier, Linnaeus was the first to use it consistently throughout the work, including in monospecific genera, and may be said to have popularised it within the scientific community.\nAfter the decline in Linnaeus's health in the early 1770s, publication of editions of \"Systema Naturae\" went in two different directions. Another Swedish scientist, Johan Andreas Murray, issued the \"Regnum Vegetabile\" section separately in 1774 as the \"Systema Vegetabilium\", rather confusingly labelled the 13th edition. Meanwhile, a 13th edition of the entire \"Systema\" appeared in parts between 1788 and 1793 under the editorship of Johann Friedrich Gmelin. It was through the \"Systema Vegetabilium\" that Linnaeus's work became widely known in England, following its translation from the Latin by the Lichfield Botanical Society as \"A System of Vegetables\" (1783\u20131785).\n\"Orbis eruditi judicium de Caroli Linnaei MD scriptis\".\n('Opinion of the learned world on the writings of Carl Linnaeus, Doctor') Published in 1740, this small octavo-sized pamphlet was presented to the State Library of New South Wales by the Linnean Society of NSW in 2018. This is considered among the rarest of all the writings of Linnaeus, and crucial to his career, securing him his appointment to a professorship of medicine at Uppsala University. From this position he laid the groundwork for his radical new theory of classifying and naming organisms for which he was considered the founder of modern taxonomy.\n' (or, more fully, ') was first published in 1753, as a two-volume work. Its prime importance is perhaps that it is the primary starting point of plant nomenclature as it exists today.\n\" was first published in 1737, delineating plant genera. Around 10 editions were published, not all of them by Linnaeus himself; the most important is the 1754 fifth edition. In it Linnaeus divided the plant Kingdom into 24 classes. One, Cryptogamia, included all the plants with concealed reproductive parts (algae, fungi, mosses and liverworts and ferns).\n' (1751) was a summary of Linnaeus's thinking on plant classification and nomenclature, and an elaboration of the work he had previously published in ' (1736) and ' (1737). Other publications forming part of his plan to reform the foundations of botany include his ' and ': all were printed in Holland (as were ' (1737) and \" (1735)), the \"Philosophia\" being simultaneously released in Stockholm.\nCollections.\nAt the end of his lifetime the Linnean collection in Uppsala was considered one of the finest collections of natural history objects in Sweden. Next to his own collection he had also built up a museum for the university of Uppsala, which was supplied by material donated by Carl Gyllenborg (in 1744\u20131745), crown-prince Adolf Fredrik (in 1745), Erik Petreus (in 1746), Claes Grill (in 1746), Magnus Lagerstr\u00f6m (in 1748 and 1750) and Jonas Alstr\u00f6mer (in 1749). The relation between the museum and the private collection was not formalised and the steady flow of material from Linnean pupils were incorporated to the private collection rather than to the museum. Linnaeus felt his work was reflecting the harmony of nature and he said in 1754 \"the earth is then nothing else but a museum of the all-wise creator's masterpieces, divided into three chambers\". He had turned his own estate into a microcosm of that 'world museum'.\nIn April 1766 parts of the town were destroyed by a fire and the Linnean private collection was subsequently moved to a barn outside the town, and shortly afterwards to a single-room stone building close to his country house at Hammarby near Uppsala. This resulted in a physical separation between the two collections; the museum collection remained in the botanical garden of the university. Some material which needed special care (alcohol specimens) or ample storage space was moved from the private collection to the museum.\nIn Hammarby the Linnean private collections suffered seriously from damp and the depredations by mice and insects. Carl von Linn\u00e9's son (Carl Linnaeus) inherited the collections in 1778 and retained them until his own death in 1783. Shortly after Carl von Linn\u00e9's death his son confirmed that mice had caused \"horrible damage\" to the plants and that also moths and mould had caused considerable damage. He tried to rescue them from the neglect they had suffered during his father's later years, and also added further specimens. This last activity however reduced rather than augmented the scientific value of the original material.\nIn 1784 the young medical student James Edward Smith purchased the entire specimen collection, library, manuscripts, and correspondence of Carl Linnaeus from his widow and daughter and transferred the collections to London. Not all material in Linn\u00e9's private collection was transported to England. Thirty-three fish specimens preserved in alcohol were not sent and were later lost.\nIn London Smith tended to neglect the zoological parts of the collection; he added some specimens and also gave some specimens away. Over the following centuries the Linnean collection in London suffered enormously at the hands of scientists who studied the collection, and in the process disturbed the original arrangement and labels, added specimens that did not belong to the original series and withdrew precious original type material.\nMuch material which had been intensively studied by Linn\u00e9 in his scientific career belonged to the collection of Queen Lovisa Ulrika (1720\u20131782) (in the Linnean publications referred to as \"Museum Ludovicae Ulricae\" or \"M. L. U.\"). This collection was donated by her grandson King Gustav IV Adolf (1778\u20131837) to the museum in Uppsala in 1804. Another important collection in this respect was that of her husband King Adolf Fredrik (1710\u20131771) (in the Linnean sources known as \"Museum Adolphi Friderici\" or \"Mus. Ad. Fr.\"), the wet parts (alcohol collection) of which were later donated to the Royal Swedish Academy of Sciences, and is today housed in the Swedish Museum of Natural History at Stockholm. The dry material was transferred to Uppsala.\nSystem of taxonomy.\nThe establishment of universally accepted conventions for the naming of organisms was Linnaeus's main contribution to taxonomy\u2014his work marks the starting point of consistent use of binomial nomenclature. During the 18th century expansion of natural history knowledge, Linnaeus also developed what became known as the \"Linnaean taxonomy\"; the system of scientific classification now widely used in the biological sciences. A previous zoologist Rumphius (1627\u20131702) had more or less approximated the Linnaean system and his material contributed to the later development of the binomial scientific classification by Linnaeus.\nThe Linnaean system classified nature within a nested hierarchy, starting with three kingdoms. Kingdoms were divided into classes and they, in turn, into orders, and thence into genera (\"singular:\" genus), which were divided into species (\"singular:\" species). Below the rank of species he sometimes recognised taxa of a lower (unnamed) rank; these have since acquired standardised names such as \"variety\" in botany and \"subspecies\" in zoology. Modern taxonomy includes a rank of family between order and genus and a rank of phylum between kingdom and class that were not present in Linnaeus's original system.\nLinnaeus's groupings were based upon shared physical characteristics, and not based upon differences. Of his higher groupings, only those for animals are still in use, and the groupings themselves have been significantly changed since their conception, as have the principles behind them. Nevertheless, Linnaeus is credited with establishing the idea of a hierarchical structure of classification which is based upon observable characteristics and intended to reflect natural relationships. While the underlying details concerning what are considered to be scientifically valid \"observable characteristics\" have changed with expanding knowledge (for example, DNA sequencing, unavailable in Linnaeus's time, has proven to be a tool of considerable utility for classifying living organisms and establishing their evolutionary relationships), the fundamental principle remains sound.\nHuman taxonomy.\nLinnaeus's system of taxonomy was especially noted as the first to include humans (\"Homo\") taxonomically grouped with apes (\"Simia\"), under the header of \"Anthropomorpha\".\nGerman biologist Ernst Haeckel speaking in 1907 noted this as the \"most important sign of Linnaeus's genius\".\nLinnaeus classified humans among the primates beginning with the first edition of \"\". During his time at Hartekamp, he had the opportunity to examine several monkeys and noted similarities between them and man. He pointed out both species basically have the same anatomy; except for speech, he found no other differences. Thus he placed man and monkeys under the same category, \"Anthropomorpha\", meaning \"manlike\". This classification received criticism from other biologists such as Johan Gottschalk Wallerius, Jacob Theodor Klein and Johann Georg Gmelin on the ground that it is illogical to describe man as human-like. In a letter to Gmelin from 1747, Linnaeus replied:\nThe theological concerns were twofold: first, putting man at the same level as monkeys or apes would lower the spiritually higher position that man was assumed to have in the great chain of being, and second, because the Bible says man was created in the image of God (theomorphism), if monkeys/apes and humans were not distinctly and separately designed, that would mean monkeys and apes were created in the image of God as well. This was something many could not accept. The conflict between world views that was caused by asserting man was a type of animal would simmer for a century until the much greater, and still ongoing, creation\u2013evolution controversy began in earnest with the publication of \"On the Origin of Species\" by Charles Darwin in 1859.\nAfter such criticism, Linnaeus felt he needed to explain himself more clearly. The 10th edition of ' introduced new terms, including \"Mammalia\" and \"Primates\", the latter of which would replace \"Anthropomorpha\" as well as giving humans the full binomial \"Homo sapiens\". The new classification received less criticism, but many natural historians still believed he had demoted humans from their former place of ruling over nature and not being a part of it. Linnaeus believed that man biologically belongs to the animal kingdom and had to be included in it. In his book ', he said, \"One should not vent one's wrath on animals, Theology decree that man has a soul and that the animals are mere 'automata mechanica,' but I believe they would be better advised that animals have a soul and that the difference is of nobility.\"\nLinnaeus added a second species to the genus \"Homo\" in \"\" based on a figure and description by Jacobus Bontius from a 1658 publication: \"Homo troglodytes\" (\"caveman\") and published a third in 1771: \"Homo lar\". Swedish historian Gunnar Broberg states that the new human species Linnaeus described were actually simians or native people clad in skins to frighten colonial settlers, whose appearance had been exaggerated in accounts to Linnaeus. For \"Homo troglodytes\" Linnaeus asked the Swedish East India Company to search for one, but they did not find any signs of its existence. \"Homo lar\" has since been reclassified as \"Hylobates lar\", the lar gibbon.\nIn the first edition of \"\", Linnaeus subdivided the human species into four varieties: \"Europ\u00e6us albesc[ens]\" (whitish European), \"Americanus rubesc[ens]\" (reddish American), \"Asiaticus fuscus\" (tawny Asian) and \"Africanus nigr[iculus]\" (blackish African).\nIn the tenth edition of Systema Naturae he further detailed phenotypical characteristics for each variety, based on the concept of the four temperaments from classical antiquity, and changed the description of Asians' skin tone to \"luridus\" (yellow). While Linnaeus believed that these varieties resulted from environmental differences between the four known continents, the Linnean Society acknowledges that his categorization's focus on skin color and later inclusion of cultural and behavioral traits cemented colonial stereotypes and provided the foundations for scientific racism. Additionally, Linnaeus created a wastebasket taxon \"monstrosus\" for \"wild and monstrous humans, unknown groups, and more or less abnormal people\".\nIn 1959, W. T. Stearn designated Linnaeus to be the lectotype of \"H. sapiens\".\nInfluences and economic beliefs.\nLinnaeus's applied science was inspired not only by the instrumental utilitarianism general to the early Enlightenment, but also by his adherence to the older economic doctrine of Cameralism. Additionally, Linnaeus was a state interventionist. He supported tariffs, levies, export bounties, quotas, embargoes, navigation acts, subsidised investment capital, ceilings on wages, cash grants, state-licensed producer monopolies, and cartels.\nCommemoration.\nAnniversaries of Linnaeus's birth, especially in centennial years, have been marked by major celebrations. Linnaeus has appeared on numerous Swedish postage stamps and banknotes. There are numerous statues of Linnaeus in countries around the world. The Linnean Society of London has awarded the Linnean Medal for excellence in botany or zoology since 1888. Following approval by the Riksdag of Sweden, V\u00e4xj\u00f6 University and Kalmar College merged on 1 January 2010 to become Linnaeus University. Other things named after Linnaeus include the twinflower genus \"Linnaea\", \"Linnaeosicyos\" (a monotypic genus in the family Cucurbitaceae), the crater Linn\u00e9 on the Earth's moon, a street in Cambridge, Massachusetts, and the cobalt sulfide mineral Linnaeite.\nCommentary.\nLinnaeus wrote a description of himself in his autobiography \"Egenh\u00e4ndiga anteckningar af Carl Linn\u00e6us om sig sjelf : med anm\u00e4rkningar och till\u00e4gg\", which was published by his student Adam Afzelius in 1823:\nAndrew Dickson White wrote in \"A History of the Warfare of Science with Theology in Christendom\" (1896):\nThe mathematical PageRank algorithm, applied to 24 multilingual Wikipedia editions in 2014, published in \"PLOS ONE\" in 2015, placed Carl Linnaeus at the top historical figure, above Jesus, Aristotle, Napoleon, and Adolf Hitler (in that order).\nIn the 21st century, Linn\u00e6us's taxonomy of human \"races\" has been problematised and discussed. Some critics claim that Linn\u00e6us was one of the forebears of the modern pseudoscientific notion of scientific racism, while others hold the view that while his classification was stereotyped, it did not imply that certain human \"races\" were superior to others.\nExternal links.\nBiographies\nResources"}
{"id": "5236", "revid": "20319727", "url": "https://en.wikipedia.org/wiki?curid=5236", "title": "Coast", "text": "A coastalso called the coastline, shoreline, or seashoreis the land next to the sea or the line that forms the boundary between the land and the ocean or a lake. Coasts are influenced by the topography of the surrounding landscape, as well as by water induced erosion, such as waves. The geological composition of rock and soil dictates the type of shore that is created. Earth contains roughly of coastline.\nCoasts are important zones in natural ecosystems, often home to a wide range of biodiversity. On land, they harbor important ecosystems such as freshwater or estuarine wetlands, which are important for bird populations and other terrestrial animals. In wave-protected areas, they harbor salt marshes, mangroves or seagrasses, all of which can provide nursery habitat for finfish, shellfish, and other aquatic animals. Rocky shores are usually found along exposed coasts and provide habitat for a wide range of sessile animals (e.g. mussels, starfish, barnacles) and various kinds of seaweeds.\nIn physical oceanography, a shore is the wider fringe that is geologically modified by the action of the body of water past and present, while the beach is at the edge of the shore, representing the intertidal zone where there is one. Along tropical coasts with clear, nutrient-poor water, coral reefs can often be found between depths of .\nAccording to an atlas prepared by the United Nations, about 44% of the human population lives within of the sea . Due to its importance in society and its high population concentrations, the coast is important for major parts of the global food and economic system, and they provide many ecosystem services to humankind. For example, important human activities happen in port cities. Coastal fisheries (commercial, recreational, and subsistence) and aquaculture are major economic activities and create jobs, livelihoods, and protein for the majority of coastal human populations. Other coastal spaces like beaches and seaside resorts generate large revenues through tourism.\nMarine coastal ecosystems can also provide protection against sea level rise and tsunamis. In many countries, mangroves are the primary source of wood for fuel (e.g. charcoal) and building material. Coastal ecosystems like mangroves and seagrasses have a much higher capacity for carbon sequestration than many terrestrial ecosystems, and as such can play a critical role in the near-future to help mitigate climate change effects by uptake of atmospheric anthropogenic carbon dioxide. \nHowever, the economic importance of coasts makes many of these communities vulnerable to climate change, which causes increases in extreme weather and sea level rise, as well as related issues like coastal erosion, saltwater intrusion, and coastal flooding. Other coastal issues, such as marine pollution, marine debris, coastal development, and marine ecosystem destruction, further complicate the human uses of the coast and threaten coastal ecosystems.\nThe interactive effects of climate change, habitat destruction, overfishing, and water pollution (especially eutrophication) have led to the demise of coastal ecosystem around the globe. This has resulted in population collapse of fisheries stocks, loss of biodiversity, increased invasion of alien species, and loss of healthy habitats. International attention to these issues has been captured in Sustainable Development Goal 14 \"Life Below Water\", which sets goals for international policy focused on preserving marine coastal ecosystems and supporting more sustainable economic practices for coastal communities. Likewise, the United Nations has declared 2021\u20132030 the UN Decade on Ecosystem Restoration, but restoration of coastal ecosystems has received insufficient attention.\nSince coasts are constantly changing, a coastline's exact perimeter cannot be determined; this measurement challenge is called the coastline paradox. The term \"coastal zone\" is used to refer to a region where interactions of sea and land processes occur. Both the terms \"coast\" and \"coastal\" are often used to describe a geographic location or region located on a coastline (e.g., New Zealand's West Coast, or the East, West, and Gulf Coast of the United States.) Coasts with a narrow continental shelf that are close to the open ocean are called \"pelagic\" \"coast\", while other coasts are more sheltered coast in a gulf or bay. A shore, on the other hand, may refer to parts of land adjoining any large body of water, including oceans (sea shore) and lakes (lake shore). \nSize.\nThe Earth has approximately of coastline. Coastal habitats, which extend to the margins of the continental shelves, make up about 7 percent of the Earth's oceans, but at least 85% of commercially harvested fish depend on coastal environments during at least part of their life cycle. about 2.86% of exclusive economic zones were part of marine protected areas.\nThe definition of coasts varies. Marine scientists think of the \"wet\" (aquatic or intertidal) vegetated habitats as being coastal ecosystems (including seagrass, salt marsh etc.) whilst some terrestrial scientists might only think of coastal ecosystems as purely terrestrial plants that live close to the seashore (see also estuaries and coastal ecosystems).\nWhile there is general agreement in the scientific community regarding the definition of coast, in the political sphere, the delineation of the extents of a coast differ according to jurisdiction. Government authorities in various countries may define coast differently for economic and social policy reasons. \nFormation.\nTides often determine the range over which sediment is deposited or eroded. Areas with high tidal ranges allow waves to reach farther up the shore, and areas with lower tidal ranges produce deposition at a smaller elevation interval. The tidal range is influenced by the size and shape of the coastline. Tides do not typically cause erosion by themselves; however, tidal bores can erode as the waves surge up the river estuaries from the ocean.\nGeologists classify coasts on the basis of tidal range into \"macrotidal coasts\" with a tidal range greater than ; \"mesotidal coasts\" with a tidal range of ; and \"microtidal coasts\" with a tidal range of less than . The distinction between macrotidal and mesotidal coasts is more important. Macrotidal coasts lack barrier islands and lagoons, and are characterized by funnel-shaped estuaries containing sand ridges aligned with tidal currents. Wave action is much more important for determining bedforms of sediments deposited along mesotidal and microtidal coasts than in macrotidal coasts.\nWaves erode coastline as they break on shore releasing their energy; the larger the wave the more energy it releases and the more sediment it moves. Coastlines with longer shores have more room for the waves to disperse their energy, while coasts with cliffs and short shore faces give little room for the wave energy to be dispersed. In these areas, the wave energy breaking against the cliffs is higher, and air and water are compressed into cracks in the rock, forcing the rock apart, breaking it down. Sediment deposited by waves comes from eroded cliff faces and is moved along the coastline by the waves. This forms an abrasion or cliffed coast.\nSediment deposited by rivers is the dominant influence on the amount of sediment located in the case of coastlines that have estuaries. Today, riverine deposition at the coast is often blocked by dams and other human regulatory devices, which remove the sediment from the stream by causing it to be deposited inland. Coral reefs are a provider of sediment for coastlines of tropical islands.\nLike the ocean which shapes them, coasts are a dynamic environment with constant change. The Earth's natural processes, particularly sea level rises, waves and various weather phenomena, have resulted in the erosion, accretion and reshaping of coasts as well as flooding and creation of continental shelves and drowned river valleys (rias).\nImportance for humans and ecosystems.\nHuman settlements.\nMore and more of the world's people live in coastal regions. According to a United Nations atlas, 44% of all people live within 150 km (93 mi) of the sea. Many major cities are on or near good harbors and have port facilities. Some landlocked places have achieved port status by building canals.\nNations defend their coasts against military invaders, smugglers and illegal migrants. Fixed coastal defenses have long been erected in many nations, and coastal countries typically have a navy and some form of coast guard.\nTourism.\nCoasts, especially those with beaches and warm water, attract tourists often leading to the development of seaside resort communities. In many island nations such as those of the Mediterranean, South Pacific Ocean and Caribbean, tourism is central to the economy. Coasts offer recreational activities such as swimming, fishing, surfing, boating, and sunbathing. \nGrowth management and coastal management can be a challenge for coastal local authorities who often struggle to provide the infrastructure required by new residents, and poor management practices of construction often leave these communities and infrastructure vulnerable to processes like coastal erosion and sea level rise. In many of these communities, management practices such as beach nourishment or when the coastal infrastructure is no longer financially sustainable, managed retreat to remove communities from the coast.\nTypes.\nEmergent coastline.\nAccording to one principle of classification, an emergent coastline is a coastline that has experienced a fall in sea level, because of either a global sea-level change, or local uplift. Emergent coastlines are identifiable by the coastal landforms, which are above the high tide mark, such as raised beaches. In contrast, a submergent coastline is one where the sea level has risen, due to a global sea-level change, local subsidence, or isostatic rebound. Submergent coastlines are identifiable by their submerged, or \"drowned\" landforms, such as rias (drowned valleys) and fjords\nConcordant coastline.\nAccording to the second principle of classification, a concordant coastline is a coastline where bands of different rock types run parallel to the shore. These rock types are usually of varying resistance, so the coastline forms distinctive landforms, such as coves. Discordant coastlines feature distinctive landforms because the rocks are eroded by the ocean waves. The less resistant rocks erode faster, creating inlets or bay; the more resistant rocks erode more slowly, remaining as headlands or outcroppings.\nHigh and low energy coasts.\nParts of a coastline can be categorised as high energy coast or low energy coast. The distinguishing characteristics of a high energy coast are that the average wave energy is relatively high so that erosion of small grained material tends to exceed deposition, and consequently landforms like cliffs, headlands and wave-cut terraces develop. Low energy coasts are generally sheltered from waves, or in regions where the average wind wave and swell conditions are relatively mild. Low energy coasts typically change slowly, and tend to be depositional environments. \nHigh energy coasts are exposed to the direct impact of waves and storms, and are generally erosional environments. High energy storm events can make large changes to a coastline, and can move significant amounts of sediment over a short period, sometimes changing a shoreline configuration.\nDestructive and constructive waves.\nSwash is the shoreward flow after the break, backwash is the water flow back down the beach. The relative strength of flow in the swash and backwash determines what size grains are deposited or eroded. This is dependent on how the wave breaks and the slope of the shore.\nDepending on the form of the breaking wave, its energy can carry granular material up the beach and deposit it, or erode it by carrying more material down the slope than up it. Steep waves that are close together and break with the surf plunging down onto the shore slope expend much of their energy lifting the sediment. The weak swash does not carry it far up the slope, and the strong backwash carries it further down the slope, where it either settles in deeper water or is carried along the shore by a longshore current induced by an angled approach of the wave-front to the shore. These waves which erode the beach are called destructive waves.\nLow waves that are further apart and break by spilling, expend more of their energy in the swash which carries particles up the beach, leaving less energy for the backwash to transport them downslope, with a net constrictive influence on the beach.\nRivieras.\n\"Riviera\" is an Italian word for \"shoreline\", ultimately derived from Latin (\"riverbank\"). It came to be applied as a proper name to the coast of the Ligurian Sea, in the form \"riviera ligure\", then shortened to \"riviera\". Historically, the Ligurian Riviera extended from Capo Corvo (Punta Bianca) south of Genoa, north and west into what is now French territory past Monaco and sometimes as far as Marseille. Today, this coast is divided into the Italian Riviera and the French Riviera, although the French use the term \"Riviera\" to refer to the Italian Riviera and call the French portion the \"C\u00f4te d'Azur\".\nAs a result of the fame of the Ligurian rivieras, the term came into English to refer to any shoreline, especially one that is sunny, topographically diverse and popular with tourists. Such places using the term include the Australian Riviera in Queensland and the Turkish Riviera along the Aegean Sea.\nLandforms.\nThe following articles describe some coastal landforms:\nCoastal waters.\n\"Coastal waters\" (or \"coastal seas\") is a rather general term used differently in different contexts, ranging geographically from the waters within a few kilometers of the coast, through to the entire continental shelf which may stretch for more than a hundred kilometers from land. Thus the term coastal waters is used in a slightly different way in discussions of legal and economic boundaries (see territorial waters and international waters) or when considering the geography of coastal landforms or the ecological systems operating through the continental shelf (marine coastal ecosystems). The research on coastal waters often divides into these separate areas too. \nThe dynamic fluid nature of the ocean means that all components of the whole ocean system are ultimately connected, although certain regional classifications are useful and relevant. The waters of the continental shelves represent such a region. The term \"coastal waters\" has been used in a wide variety of different ways in different contexts. In European Union environmental management it extends from the coast to just a few nautical miles while in the United States the US EPA considers this region to extend much further offshore. \n\"Coastal waters\" has specific meanings in the context of commercial coastal shipping, and somewhat different meanings in the context of naval littoral warfare. Oceanographers and marine biologists have yet other takes. Coastal waters have a wide range of marine habitats from enclosed estuaries to the open waters of the continental shelf. \nSimilarly, the term littoral zone has no single definition. It is the part of a sea, lake, or river that is close to the shore. In coastal environments, the littoral zone extends from the high water mark, which is rarely inundated, to shoreline areas that are permanently submerged.\nCoastal waters can be threatened by coastal eutrophication and harmful algal blooms.\nIn geology.\nThe identification of bodies of rock formed from sediments deposited in shoreline and nearshore environments (shoreline and nearshore \"facies\") is extremely important to geologists. These provide vital clues for reconstructing the geography of ancient continents (\"paleogeography\"). The locations of these beds show the extent of ancient seas at particular points in geological time, and provide clues to the magnitudes of tides in the distant past.\nSediments deposited in the shoreface are preserved as lenses of sandstone in which the upper part of the sandstone is coarser than the lower part (a \"coarsening upwards sequence\"). Geologists refer to these are \"parasequences\". Each records an episode of retreat of the ocean from the shoreline over a period of 10,000 to 1,000,000 years. These often show laminations reflecting various kinds of tidal cycles.\nSome of the best-studied shoreline deposits in the world are found along the former western shore of the Western Interior Seaway, a shallow sea that flooded central North America during the late Cretaceous Period (about 100 to 66 million years ago). These are beautifully exposed along the Book Cliffs of Utah and Colorado.\nGeologic processes.\nThe following articles describe the various geologic processes that affect a coastal zone:\nWildlife.\nAnimals.\nLarger animals that live in coastal areas include puffins, sea turtles and rockhopper penguins, among many others. Sea snails and various kinds of barnacles live on rocky coasts and scavenge on food deposited by the sea. Some coastal animals are used to humans in developed areas, such as dolphins and seagulls who eat food thrown for them by tourists. Since the coastal areas are all part of the littoral zone, there is a profusion of marine life found just off-coast, including sessile animals such as corals, sponges, starfish, mussels, seaweeds, fishes, and sea anemones.\nThere are many kinds of seabirds on various coasts. These include pelicans and cormorants, who join up with terns and oystercatchers to forage for fish and shellfish. There are sea lions on the coast of Wales and other countries.\nPlants.\nMany coastal areas are famous for their kelp beds. Kelp is a fast-growing seaweed that can grow up to half a meter a day in ideal conditions. Mangroves, seagrasses, macroalgal beds, and salt marsh are important coastal vegetation types in tropical and temperate environments respectively. Restinga is another type of coastal vegetation.\nThreats.\nCoasts also face many human-induced environmental impacts and coastal development hazards. The most important ones are: \nPollution.\nThe pollution of coastlines is connected to marine pollution which can occur from a number of sources: Marine debris (garbage and industrial debris); the transportation of petroleum in tankers, increasing the probability of large oil spills; small oil spills created by large and small vessels, which flush bilge water into the ocean.\nGlobal goals.\nInternational attention to address the threats of coasts has been captured in Sustainable Development Goal 14 \"Life Below Water\" which sets goals for international policy focused on preserving marine coastal ecosystems and supporting more sustainable economic practices for coastal communities. Likewise, the United Nations has declared 2021\u20132030 the UN Decade on Ecosystem Restoration, but restoration of coastal ecosystems has received insufficient attention."}
{"id": "5237", "revid": "44342193", "url": "https://en.wikipedia.org/wiki?curid=5237", "title": "Catatonia", "text": "Catatonia is a complex syndrome, most commonly seen in people with underlying mood disorders, such as major depressive disorder, or psychotic disorders, such as schizophrenia. People with catatonia have abnormal movement and behaviors, which vary from person to person and fluctuate in intensity within a single episode. People with catatonia appear withdrawn, meaning that they do not interact with the outside world and have difficulty processing information. They may be nearly motionless for days on end or perform repetitive purposeless movements. Two people may exhibit very different sets of behaviors and both still be diagnosed with catatonia. Treatment with benzodiazepines or ECT are most effective and lead to remission of symptoms in most cases.\nThere are different subtypes of catatonia, which represent groups of symptoms that commonly occur together. These include stuporous/akinetic catatonia, excited catatonia, malignant catatonia, and periodic catatonia.\nCatatonia has historically been related to schizophrenia (catatonic schizophrenia), but is most often seen in mood disorders. It is now known that catatonic symptoms are nonspecific and may be observed in other mental, neurological, and medical conditions.\nClassification.\nModern classifications.\nThe ICD-11 is the most common manual used globally to define and diagnose illness, including mental illness. It diagnoses catatonia in someone who has three different symptoms associated with catatonia at one time. These symptoms are called stupor, catalepsy, waxy flexibility, mutism, negativism, posturing, mannerisms, stereotypies, psychomotor agitation, grimacing, echolalia, and echopraxia. It divides catatonia into three groups based on the underlying cause; Catatonia associated with another mental disorder, catatonia induced by psychoactive substance, and secondary catatonia.\nThe DSM-5 is the most common manual used by mental health professionals in the United States to define and diagnose different mental illnesses. The DSM-5 defines catatonia as \u201ca syndrome characterized by lack of movement and communication, along with three or more of the following 12 behaviors; stupor, catalepsy, waxy flexibility, mutism, negativism, posturing, mannerism, stereotypy, agitation, grimacing, echolalia, or echopraxia.\u201d As a syndrome, catatonia can only occur in people with an existing illness. The DSM-5 divides catatonia into 3 diagnoses. The most common of the three diagnoses is Catatonia Associated with Another Mental Disorder. Around 20% of cases are caused by an underlying medical condition, and known as Catatonic Disorder Due to Another Medical Condition. When the underlying condition is unknown it is considered Unspecified Catatonia.\nSigns and symptoms.\nAs discussed previously, the ICD-11 and DSM-5 both require 3 or more of the symptoms defined in the table below in order to diagnose catatonia. However, each person can have a different set of symptoms that may worsen, improve, and change in appearance throughout a single episode. Symptoms may develop over hours or days to weeks.\nBecause most patients with catatonia have an underlying psychiatric illness, the majority will present with worsening depression, mania, or psychosis followed by catatonia symptoms. Even when unable to interact, It should not be assumed that patients presenting with catatonia are unaware of their surroundings as some patients can recall in detail their catatonic state and their actions.\nSubtypes.\nThere are several subtypes of catatonia which are used currently: stuporous catatonia, excited catatonia, malignant catatonia, and periodic catatonia. Subtypes are defined by the group of symptoms and associated features that a person is experiencing or displaying. Notably, while catatonia can be divided into various subtypes, the appearance of catatonia is often dynamic and the same individual may have different subtypes at different times.\nStuporous catatonia: This form of catatonia is characterized by immobility, mutism, and a lack of response to the world around them. They may appear frozen in one position for long periods of time unable to eat, drink, or speak.\nExcited catatonia: This form of catatonia is characterized by odd mannerisms and gestures, purposeless or inappropriate actions, excessive motor activity, restlessness, stereotypy, impulsivity, agitation, and combativeness. Speech and actions may be repetitive or mimic another person's. People in this state are extremely hyperactive and may have delusions and hallucinations.\nMalignant catatonia: This form of catatonia is life-threatening. It is characterized by fever, dramatic and rapid changes in blood pressure, increased heart rate and respiratory rate, and excessive sweating. Laboratory tests may be abnormal.\nPeriodic catatonia: This form of catatonia is characterized by only by a person having recurrent episodes of catatonia. Individuals will experience multiple episodes over time, without signs of catatonia in between episodes. Historically, the Wernicke-Kleist-Leonhard School considered periodic catatonia a distinct form of \"non-system schizophrenia\" characterized by recurrent acute phases with hyperkinetic and akinetic features and often psychotic symptoms, and the build-up of a residual state in between these acute phases, which is characterized by low-level catatonic features and aboulia of varying severity.\nCauses.\nCatatonia can only exist if a person has another underlying illness, and can be associated with a wide range of illnesses including psychiatric disorders, medical conditions, and substance use.\nPsychiatric conditions.\nMood disorders such as bipolar disorder and depression are the most common conditions underlying catatonia. Other psychiatric conditions that can cause catatonia include schizophrenia and other primary psychotic disorders, autism spectrum disorders, ADHD, and post-traumatic stress disorder. In autism, people tend to present with catatonia during periods of regression.\nPsychodynamic theorists have interpreted catatonia as a defense against the potentially destructive consequences of responsibility, and the passivity of the disorder provides relief.\nMedical conditions.\nCatatonia is also seen in many medical disorders, including encephalitis, meningitis, autoimmune disorders, focal neurological lesions (including strokes), alcohol withdrawal, abrupt or overly rapid benzodiazepine withdrawal, cerebrovascular disease, neoplasms, head injury, and some metabolic conditions (homocystinuria, diabetic ketoacidosis, hepatic encephalopathy, and hypercalcaemia).\nNeurological disorders.\nCatatonia can occur due to a number of neurological conditions. For instance, certain types of encephalitis can cause catatonia. Anti-NMDA receptor encephalitis is a form of autoimmune encephalitis which is known to cause catatonia in some people. Additionally, encephalitis has been reported to cause catatonia in people who have encephalitis due to HIV and herpes simplex virus (HSV). The research is limited, but some evidence suggests that people can develop catatonia after traumatic brain injury without a primary psychiatric disorder. Similarly, there are several case reports suggesting that people have experienced catatonia after a stroke, with some people having catatonia-associated symptoms that were unexplainable by their stroke itself, and which improved after treatment with benzodiazepines. Parkinson's disease can cause catatonia for some people by impairing their ability to produce and secrete dopamine, a neurotransmitter which is thought to contribute to motor dysfunction in people with catatonia.\nMetabolic and endocrine disorders.\nAbnormal thyroid function can cause catatonia when the thyroid overproduces or underproduces thyroid hormones. This is thought to occur due to thyroid hormones impact on metabolism including in the cells of the nervous system. Abnormal electrolyte levels have also been shown to cause catatonia in rare cases. Most notably, low levels of sodium in the blood can cause catatonia in some people.\nAutoimmune disorders.\nAs discussed previously, anti-NMDA receptor encephalitis is a form of autoimmune encephalitis which can cause catatonia. Additionally, autoimmune diseases that are not exclusively neurological can cause neurological and psychiatric symptoms including catatonia. For instance, systemic lupus erythematosus can cause catatonia and is thought to do by causing inflammation in the blood vessels of the brain or possibly by the body's own antibodies damaging neurons. \nInfectious diseases.\nCertain types of infections are known to cause catatonia either through directly impairing brain function or by making a person more likely to contract diseases that impair brain function. HIV and AIDS can cause catatonia, most likely by predisposing one to infections in the brain, including different types of viral encephalitis. \"Borrelia burgdorferi\" causes Lyme disease, which has been shown to cause catatonia by infecting the brain and causing encephalitis.\nPharmacological causes.\nUse of NMDA receptor antagonists including ketamine and phencyclidine (PCP) can lead to catatonia-like states. Information about these effects has improved scientific understanding of the role of glutamate in catatonia. High dose and chronic use of stimulants like cocaine and amphetamines can lead to cases of catatonia, typically associated with psychosis. This is thought to be due to changes in the function of circuits of the brain associated with dopamine release.\nPathogenesis.\nThe mechanisms in the brain that cause catatonia are poorly understood. Currently, there are two main categories of explanations for what may be happening in the brain to cause catatonia. The first is that there is disruption of normal neurotransmitter production or release in certain areas of the brain preventing normal function of those areas of the brain, leading to behavioral and motor symptoms associated with catatonia. The second claims that disruption of communication between different areas of the brain causes catatonia.\nNeurotransmitters.\nThe neurotransmitters that are most strongly associated with catatonia are GABA, dopamine, and glutamate. GABA is the main inhibitory neurotransmitter of the brain, meaning that it slows down the activity of the systems of the brain it acts on. In catatonia, people have low levels of GABA which causes them to be overly activated, especially in the areas of the brain that cause inhibition. This is thought to cause the behavioral symptoms associated with catatonia including withdrawal. Dopamine can increase or decrease the activity of the area of the brain it acts on depending on where in the brain it is. Dopamine is lower than normal in people with catatonia, which is thought to cause many of the motor symptoms, because dopamine is the main neurotransmitter which activates the parts of the brain responsible for movement. Glutamate is an excitatory neurotransmitter, meaning that it increases the activity of the areas of the brain it acts on. Notably, glutamate increases tells the neuron it acts on to fire, by binding to the NMDA receptor. People with anti-NMDA receptor encephalitis can develop catatonia because their own antibodies attack the NMDA receptor, which reduces the ability of the brain to activate different areas of the brain using glutamate.\nNeurological pathways.\nSeveral pathways in the brain have been studied which seem to contribute to catatonia when they are not functioning properly. However, these studies were unable to determine if the abnormalities they observed were the cause of catatonia or if the catatonia caused the abnormalities. Furthermore, it has also been hypothesized that pathways that connect the basal ganglia with the cortex and thalamus is involved in the development of catatonia.\nDiagnosis.\nThere is not yet a definitive consensus regarding diagnostic criteria of catatonia. In the fifth edition of the American Psychiatric Association's \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-5, 2013) and the eleventh edition of the World Health Organization's \"International Classification of Diseases (\"ICD-11, 2022), the classification is more homogeneous than in earlier editions. Prominent researchers in the field have other suggestions for diagnostic criteria. Still, diagnosing catatonia can be challenging. Evidence suggests that there is as high as a 15-day average delay to diagnosis for people with catatonia.\nDSM-5 classification\nThe DSM-5 does not classify catatonia as an independent disorder, but rather it classifies it as catatonia associated with another mental disorder, due to another medical condition, or as unspecified catatonia. \nCatatonia is diagnosed by the presence of three or more of the following 12 psychomotor symptoms in association with a mental disorder, medical condition, or unspecified:\nOther disorders (additional code 293.89 [F06.1] to indicate the presence of the co-morbid catatonia):\nIf catatonic symptoms are present but do not form the catatonic syndrome, a medication- or substance-induced aetiology should first be considered.\nICD-11 classification\nIn the ICD-11, catatonia is defined as a syndrome of primarily psychomotor disturbances that is characterized by the simultaneous occurrence of several symptoms such as stupor, catalepsy, waxy flexibility, mutism, negativism, posturing, mannerisms, stereotypies, psychomotor agitation, grimacing, echolalia, and echopraxia. Catatonia may occur in the context of specific mental disorders, including mood disorders, schizophrenia or other primary psychotic disorders, and neurodevelopmental disorders, and may be induced by psychoactive substances, including medications. Catatonia may also be caused by a medical condition not classified under mental, behavioral, or neurodevelopmental disorders.\nAssessment/physical.\nCatatonia is often overlooked and under-diagnosed. Patients with catatonia most commonly have an underlying psychiatric disorder. For this reason, physicians may overlook signs of catatonia due to the severity of the psychosis the patient is presenting with. Furthermore, the patient may not be presenting with the common signs of catatonia such as mutism and posturing. Additionally, the motor abnormalities seen in catatonia are also present in psychiatric disorders. For example, a patient with mania will show increased motor activity and may not be considered for a diagnosis of excited catatonia, even if symptoms are developing that are not associated with mania. One way in which physicians can differentiate between the two is to observe the motor abnormality. Patients with mania present with increased goal-directed activity. On the other hand, the increased activity in catatonia is not goal-directed and often repetitive.\nCatatonia is a clinical diagnosis and there is no specific laboratory test to diagnose it. However, certain testing can help determine what is causing the catatonia. An EEG will likely show diffuse slowing. If seizure activity is driving the syndrome, then an EEG would also be helpful in detecting this. CT or MRI will not show catatonia; however, they might reveal abnormalities that might be leading to the syndrome. Metabolic screens, inflammatory markers, or autoantibodies may reveal reversible medical causes of catatonia.\nVital signs should be frequently monitored as catatonia can progress to malignant catatonia which is life-threatening. Malignant catatonia is characterized by fever, hypertension, tachycardia, and tachypnea.\nRating scale.\nVarious rating scales for catatonia have been developed, however, their utility for clinical care has not been well established. The most commonly used scale is the Bush-Francis Catatonia Rating Scale (BFCRS) (external link is provided below). The scale is composed of 23 items with the first 14 items being used as the screening tool. If 2 of the 14 are positive, this prompts for further evaluation and completion of the remaining 9 items.\nA diagnosis can be supported by the lorazepam challenge or the zolpidem challenge. While proven useful in the past, barbiturates are no longer commonly used in psychiatry; thus the option of either benzodiazepines or ECT.\nLaboratory findings.\nCertain lab findings are common with malignant catatonia that are uncommon in other forms of catatonia. These lab findings include: leukocytosis, elevated creatine kinase, low serum iron. The signs and symptoms of malignant catatonia overlap significantly with neuroleptic malignant syndrome (NMS). Therefore, the results of laboratory tests need to be considered in the context of clinical history, review of medications, and physical exam findings.\nDifferential diagnosis.\nThe differential diagnosis of catatonia is extensive as signs and symptoms of catatonia may overlap significantly with those of other conditions. Therefore, a careful and detailed history, medication review, and physical exam are key to diagnosing catatonia and differentiating it from other conditions. Furthermore, some of these conditions can themselves lead to catatonia. The differential diagnosis is as follows:\nTreatment.\nTreating catatonia effectively requires treating the catatonia itself, treating the underlying condition, and helping them with their basic needs, like eating, drinking, and staying clean and safe, while they are withdrawn and incapable of caring for themselves.\nCatatonia-specific treatments.\nThe specifics of treating catatonia itself can vary from region to region, hospital to hospital, and individual to individual, but typically involves the use of benzodiazepines. In fact, in some cases it is unclear whether a person has catatonia or another condition which may present similarly. In these cases a \"benzodiazepine challenge\" is often done. During a \"benzodiazepine challenge\" a healthcare provider will give a moderate dose of a benzodiazepine to the patient and monitor them. If a person has catatonia they will often have improvements in their symptoms within 15 to 30 minutes. If the person does not improve within 30 minutes they are given a second dose and the process is repeated once more. If the person responds to either of the doses then they can be given benzodiazepines at a consistent dose and timing until their catatonia resolves. Depending on the person, a person may need to reduce their dosing slowly over time in order to prevent reoccurrence of their symptoms.\nECT is also commonly used to treat catatonia in people who do not improve with medication alone or whose symptoms reoccur whenever the dose of medications are reduced. ECT is usually administered with multiple sessions per week over two to four weeks. ECT has a success rate of 80% to 100%. ECT is effective for all subtypes of catatonia, however people who have catatonia with an underlying neurological condition show less improvement with ECT treatment.\nExcessive glutamate activity is believed to be involved in catatonia; when first-line treatment options fail, NMDA antagonists such as amantadine or memantine may be used. Amantadine may have an increased incidence of tolerance with prolonged use and can cause psychosis, due to its additional effects on the dopamine system. Memantine has a more targeted pharmacological profile for the glutamate system, reduced incidence of psychosis and may therefore be preferred for individuals who cannot tolerate amantadine. Topiramate is another treatment option for resistant catatonia; it produces its therapeutic effects by producing glutamate antagonism via modulation of AMPA receptors.\nNon-specific aspects of treatment.\nTreating the underlying condition.\nThere are many medications that are known to cause catatonia in some people including steroids, stimulants, anticonvulsants, neuroleptics or dopamine blockers. If a person has catatonia and is on these medications, they should be considered as a potential cause if another cause is not apparent and discontinued if possible.\nAntipsychotics are sometimes used in those with a co-existing psychosis, however they should be used with care as they may worsen catatonia and have a risk of neuroleptic malignant syndrome, a dangerous condition that can mimic catatonia and requires immediate discontinuation of the antipsychotic. There is evidence that clozapine works better than other antipsychotics to treat catatonia.\nSupportive care.\nSupportive care is required in those with catatonia. This includes monitoring vital signs and fluid status, and in those with chronic symptoms; maintaining nutrition and hydration, medications to prevent a blood clot, and measures to prevent the development of pressure ulcers.\nPrognosis.\nTwenty-five percent of psychiatric patients with catatonia will have more than one episode throughout their lives. Treatment response for patients with catatonia is 50\u201370%, with treatment failure being associated with a poor prognosis. Many of these patients will require long-term and continuous mental health care. The prognosis for people with catatonia due to schizophrenia is much worse compared to other causes. In cases of catatonia that develop into malignant catatonia, the mortality rate is as high as 20%.\nComplications.\nPatients may experience several complications from being in a catatonic state. The nature of these complications will depend on the type of catatonia being experienced by the patient. For example, patients presenting with withdrawn catatonia may have refusal to eat which will in turn lead to malnutrition and dehydration. Furthermore, if immobility is a symptom the patient is presenting with, then they may develop pressure ulcers, muscle contractions, and are at risk of developing deep vein thrombosis (DVT) and pulmonary embolus (PE). Patients with excited catatonia may be aggressive and violent, and physical trauma may result from this. Catatonia may progress to the malignant type which will present with autonomic instability and may be life-threatening. Other complications also include the development of pneumonia and neuroleptic malignant syndrome.\nEpidemiology.\nCatatonia has been historically studied in psychiatric patients. Catatonia is under-recognized because the features are often mistaken for other disorders including delirium or the negative symptoms of schizophrenia. The prevalence has been reported to be as high as 10% in those with acute psychiatric illnesses, and 9\u201330% in the setting of inpatient psychiatric care. The incidence of catatonia is 10.6 episodes per 100 000 person-years, which essentially means that in a group of 100,000 people, the group as a whole would experience 10 to 11 episodes of catatonia per year. Catatonia can occur at any age, but is most commonly seen in adolescence or young adulthood or in older adults with existing medical conditions. It occurs in males and females in approximately equal numbers. Around 20% of all catatonia cases can be attributed to a general medical condition. \nHistory.\nAncient history.\nThere have been reports of stupor-like and catatonia-like states in people throughout the history of psychiatry. In ancient Greece, the first physician to document stupor-like or catatonia-like states was Hippocrates, in his \"Aphorisms.\" He never defined the syndrome, but seemingly observed these states in people he was treating for melancholia. In ancient China, the first descriptions of people that appear in the Huangdi Neijing (The Yellow Emperor's Inner Canon), the book which forms the basis of Traditional Chinese Medicine. It is thought to have been compiled by many people over the course of centuries during the Warring States Period (475-221 BCE) and the early Han Dynasty (206 BCE-220 CE).\nModern history.\nThe term \u201ccatatonia\u201d was first used by German psychiatrist Karl Ludwig Kahlbaum in 1874, in his book \"Die Katatonie oder das Spannungsirresein\", which translates to \"Catatonia or Tension Insanity\". He viewed catatonia as its own illness, which would get worse over time in stages of mania, depression, and psychosis leading to dementia. This work heavily influenced another German psychiatrist, Emil Kraeplin, who was the first to classify catatonia as a syndrome. Kraeplin associated catatonia with a psychotic disorder called dementia praecox, which is no longer used as a diagnosis, but heavily informed the development of the concept of schizophrenia.\nKraeplin's work influenced two other notable German psychiatrists, Karl Leonhard and Max Fink, and their colleagues to expand the concept of catatonia as a syndrome which could occur in the setting of many mental illnesses, not just psychotic disorders. They also laid the groundwork to describe different subtypes of catatonia still used today, including Stuporous Catatonia, Excited Catatonia, Malignant Catatonia, and Periodic Catatonia. Additionally, Leonhard and his colleagues categorized catatonia as either systematic or unsystematic, based on whether or not symptoms happened according to consistent and predictable patterns. These ways of thinking shaped the way that psychologists and psychiatrists thought of catatonia well into the 20th century. In fact, catatonia was a subtype of schizophrenia as recently as the DSM-III, and was not revised to be able to be applied to mood disorders until 1994 with the release of the DSM-IV.\nIn the latter half of the 20th century, clinicians observed that catatonia occurred in various psychiatric and medical conditions, not exclusively in schizophrenia. Max Fink and colleagues advocated for recognizing catatonia as an independent syndrome, highlighting its frequent association with mood disorders and responsiveness to treatments like benzodiazepines and ECT.\nSociety and culture.\nPopular conceptions and origins.\nCatatonia, historically misunderstood, has been subject to shifting perceptions in society. As discussed previously, since the 19th century it was often linked exclusively to schizophrenia, perpetuating misconceptions. These historical misunderstandings have shaped the public opinions on catatonia. This has contributed to a lack of understanding about catatonia and its broader association with other mental disorders and medical conditions.\nPopular culture and media have played a significant role in shaping societal perceptions of catatonia. In many cases, media portrayals reduce it to a stereotypical \"frozen state,\" similar to a coma, failing to capture the complexity of symptoms like stupor, agitation, and mutism. Such oversimplifications contribute to public misperceptions and get in the way of people receiving the care they need."}
{"id": "5238", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=5238", "title": "CountriesY", "text": ""}
{"id": "5239", "revid": "37740198", "url": "https://en.wikipedia.org/wiki?curid=5239", "title": "Countably infinite", "text": ""}
{"id": "5242", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5242", "title": "Ciliates", "text": ""}
{"id": "5244", "revid": "12396222", "url": "https://en.wikipedia.org/wiki?curid=5244", "title": "Cipher", "text": "In cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption\u2014a series of well-defined steps that can be followed as a procedure. An alternative, less common term is \"encipherment\". To encipher or encode is to convert information into cipher or code. In common parlance, \"cipher\" is synonymous with \"code\", as they are both a set of steps that encrypt a message; however, the concepts are distinct in cryptography, especially classical cryptography.\nCodes generally substitute different length strings of characters in the output, while ciphers generally substitute the same number of characters as are input. A code maps one meaning with another. Words and phrases can be coded as letters or numbers. Codes typically have direct meaning from input to key. Codes primarily function to save time. Ciphers are algorithmic. The given input must follow the cipher's process to be solved. Ciphers are commonly used to encrypt written information.\nCodes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase. For example, \"UQJHSE\" could be the code for \"Proceed to the following coordinates.\" When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.\nThe operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a \"cryptovariable\"). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.\nMost modern ciphers can be categorized in several ways:\nEtymology.\nOriginating from the Arabic word for zero \u0635\u0641\u0631 (\u1e63ifr), the word \"cipher\" spread to Europe as part of the Arabic numeral system during the Middle Ages. The Roman numeral system lacked the concept of zero, and this limited advances in mathematics. In this transition, the word was adopted into Medieval Latin as cifra, and then into Middle French as cifre. This eventually led to the English word cipher (minority spelling cypher). One theory for how the term came to refer to encoding is that the concept of zero was confusing to Europeans, and so the term came to refer to a message or communication that was not easily understood. \nThe term \"cipher\" was later also used to refer to any Arabic digit, or to calculation using them, so encoding text in the form of Arabic numerals is literally converting the text to \"ciphers\".\nVersus codes.\nIn casual contexts, \"code\" and \"cipher\" can typically be used interchangeably; however, the technical usages of the words refer to different concepts. Codes contain meaning; words and phrases are assigned to numbers or symbols, creating a shorter message. \nAn example of this is the commercial telegraph code which was used to shorten long telegraph messages which resulted from entering into commercial contracts using exchanges of telegrams.\nAnother example is given by whole word ciphers, which allow the user to replace an entire word with a symbol or character, much like the way written Japanese utilizes Kanji (meaning Chinese characters in Japanese) characters to supplement the native Japanese characters representing syllables. An example using English language with Kanji could be to replace \"The quick brown fox jumps over the lazy dog\" by \"The quick brown \u72d0 jumps \u4e0a the lazy \u72ac\". Stenographers sometimes use specific symbols to abbreviate whole words.\nCiphers, on the other hand, work at a lower level: the level of individual letters, small groups of letters, or, in modern schemes, individual bits and blocks of bits. Some systems used both codes and ciphers in one system, using superencipherment to increase the security. In some cases the terms \"codes\" and \"ciphers\" are used synonymously with \"substitution\" and \"transposition\", respectively.\nHistorically, cryptography was split into a dichotomy of codes and ciphers, while coding had its own terminology analogous to that of ciphers: \"\"encoding\", \"codetext\", \"decoding\"\" and so on.\nHowever, codes have a variety of drawbacks, including susceptibility to cryptanalysis and the difficulty of managing a cumbersome codebook. Because of this, codes have fallen into disuse in modern cryptography, and ciphers are the dominant technique.\nTypes.\nThere are a variety of different types of encryption. Algorithms used earlier in the history of cryptography are substantially different from modern methods, and modern ciphers can be classified according to how they operate and whether they use one or two keys.\nHistorical.\nThe Caesar Cipher is one of the earliest known cryptographic systems. Julius Caesar used a cipher that shifts the letters in the alphabet in place by three and wrapping the remaining letters to the front to write to Marcus Tullius Cicero in approximately 50 BC.\nHistorical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include simple substitution ciphers (such as ROT13) and transposition ciphers (such as a Rail Fence Cipher). For example, \"GOOD DOG\" can be encrypted as \"PLLX XLP\" where \"L\" substitutes for \"O\", \"P\" for \"G\", and \"X\" for \"D\" in the message. Transposition of the letters \"GOOD DOG\" can result in \"DGOGDOO\". These simple ciphers and examples are easy to crack, even without plaintext-ciphertext pairs.\nIn the 1640s, the Parliamentarian commander, Edward Montagu, 2nd Earl of Manchester, developed ciphers to send coded messages to his allies during the English Civil War.\nSimple ciphers were replaced by polyalphabetic substitution ciphers (such as the Vigen\u00e8re) which changed the substitution alphabet for every letter. For example, \"GOOD DOG\" can be encrypted as \"PLSX TWF\" where \"L\", \"S\", and \"W\" substitute for \"O\". With even a small amount of known or estimated plaintext, simple polyalphabetic substitution ciphers and letter transposition ciphers designed for pen and paper encryption are easy to crack. It is possible to create a secure pen and paper cipher based on a one-time pad, but these have other disadvantages.\nDuring the early twentieth century, electro-mechanical machines were invented to do encryption and decryption using transposition, polyalphabetic substitution, and a kind of \"additive\" substitution. In rotor machines, several rotor disks provided polyalphabetic substitution, while plug boards provided another substitution. Keys were easily changed by changing the rotor disks and the plugboard wires. Although these encryption methods were more complex than previous schemes and required machines to encrypt and decrypt, other machines such as the British Bombe were invented to crack these encryption methods.\nModern.\nModern encryption methods can be divided by two criteria: by type of key used, and by type of input data.\nBy type of key used ciphers are divided into:\nIn a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. The design of AES (Advanced Encryption System) was beneficial because it aimed to overcome the flaws in the design of the DES (Data encryption standard). AES's designer's claim that the common means of modern cipher cryptanalytic attacks are ineffective against AES due to its design structure.[12] \nCiphers can be distinguished into two types by the type of input data:\nKey size and vulnerability.\nIn a pure mathematical attack, (i.e., lacking any other information to help break a cipher) two factors above all count:\nSince the desired effect is computational difficulty, in theory one would choose an algorithm and desired difficulty level, thus decide the key length accordingly.\nClaude Shannon proved, using information theory considerations, that any theoretically unbreakable cipher must have keys which are at least as long as the plaintext, and used only once: one-time pad."}
