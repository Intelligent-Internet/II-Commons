{"id": "5346", "revid": "448265", "url": "https://en.wikipedia.org/wiki?curid=5346", "title": "Colloid", "text": "A colloid is a mixture in which one substance consisting of microscopically dispersed insoluble particles is suspended throughout another substance. Some definitions specify that the particles must be dispersed in a liquid, while others extend the definition to include substances like aerosols and gels. The term colloidal suspension refers unambiguously to the overall mixture (although a narrower sense of the word \"suspension\" is distinguished from colloids by larger particle size). A colloid has a dispersed phase (the suspended particles) and a continuous phase (the medium of suspension). The dispersed phase particles have a diameter of approximately 1 nanometre to 1 micrometre.\nSome colloids are translucent because of the Tyndall effect, which is the scattering of light by particles in the colloid. Other colloids may be opaque or have a slight color.\nColloidal suspensions are the subject of interface and colloid science. This field of study began in 1845 by Francesco Selmi, who called them pseudosolutions, and expanded by Michael Faraday and Thomas Graham, who coined the term \"colloid\" in 1861.\nClassification.\nColloids can be classified as follows:\nHomogeneous mixtures with a dispersed phase in this size range may be called \"colloidal aerosols\", \"colloidal emulsions\", \"colloidal suspensions\", \"colloidal foams\", \"colloidal dispersions\", or \"hydrosols\".\nHydrocolloids.\nHydrocolloids describe certain chemicals (mostly polysaccharides and proteins) that are colloidally dispersible in water. Thus becoming effectively \"soluble\" they change the rheology of water by raising the viscosity and/or inducing gelation. They may provide other interactive effects with other chemicals, in some cases synergistic, in others antagonistic. Using these attributes hydrocolloids are very useful chemicals since in many areas of technology from foods through pharmaceuticals, personal care and industrial applications, they can provide stabilization, destabilization and separation, gelation, flow control, crystallization control and numerous other effects. Apart from uses of the soluble forms some of the hydrocolloids have additional useful functionality in a dry form if after solubilization they have the water removed - as in the formation of films for breath strips or sausage casings or indeed, wound dressing fibers, some being more compatible with skin than others. There are many different types of hydrocolloids each with differences in structure function and utility that generally are best suited to particular application areas in the control of rheology and the physical modification of form and texture. Some hydrocolloids like starch and casein are useful foods as well as rheology modifiers, others have limited nutritive value, usually providing a source of fiber.\nThe term hydrocolloids also refers to a type of dressing designed to lock moisture in the skin and help the natural healing process of skin to reduce scarring, itching and soreness.\nComponents.\nHydrocolloids contain some type of gel-forming agent, such as sodium carboxymethylcellulose (NaCMC) and gelatin. They are normally combined with some type of sealant, i.e. polyurethane to 'stick' to the skin.\nCompared with solution.\nA colloid has a dispersed phase and a continuous phase, whereas in a solution, the solute and solvent constitute only one phase. A solute in a solution are individual molecules or ions, whereas colloidal particles are bigger. For example, in a solution of salt in water, the sodium chloride (NaCl) crystal dissolves, and the Na+ and Cl\u2212 ions are surrounded by water molecules.\u00a0 However, in a colloid such as milk, the colloidal particles are globules of fat, rather than individual fat molecules. Because colloid is multiple phases, it has very different properties compared to fully mixed, continuous solution.\nInteraction between particles.\nThe following forces play an important role in the interaction of colloid particles:\nSedimentation velocity.\nThe Earth\u2019s gravitational field acts upon colloidal particles. Therefore, if the colloidal particles are denser than the medium of suspension, they will sediment (fall to the bottom), or if they are less dense, they will cream (float to the top). Larger particles also have a greater tendency to sediment because they have smaller Brownian motion to counteract this movement.\nThe sedimentation or creaming velocity is found by equating the Stokes drag force with the gravitational force:\nwhere\nand formula_5 is the sedimentation or creaming velocity.\nThe mass of the colloidal particle is found using:\nwhere\nand formula_9 is the difference in mass density between the colloidal particle and the suspension medium.\nBy rearranging, the sedimentation or creaming velocity is:\nThere is an upper size-limit for the diameter of colloidal particles because particles larger than 1 \u03bcm tend to sediment, and thus the substance would no longer be considered a colloidal suspension.\nThe colloidal particles are said to be in sedimentation equilibrium if the rate of sedimentation is equal to the rate of movement from Brownian motion.\nPreparation.\nThere are two principal ways to prepare colloids:\nStabilization.\nThe stability of a colloidal system is defined by particles remaining suspended in solution and depends on the interaction forces between the particles. These include electrostatic interactions and van der Waals forces, because they both contribute to the overall free energy of the system.\nA colloid is stable if the interaction energy due to attractive forces between the colloidal particles is less than kT, where k is the Boltzmann constant and T is the absolute temperature. If this is the case, then the colloidal particles will repel or only weakly attract each other, and the substance will remain a suspension.\nIf the interaction energy is greater than kT, the attractive forces will prevail, and the colloidal particles will begin to clump together. This process is referred to generally as aggregation, but is also referred to as flocculation, coagulation or precipitation. While these terms are often used interchangeably, for some definitions they have slightly different meanings. For example, coagulation can be used to describe irreversible, permanent aggregation where the forces holding the particles together are stronger than any external forces caused by stirring or mixing. Flocculation can be used to describe reversible aggregation involving weaker attractive forces, and the aggregate is usually called a \"floc\". The term precipitation is normally reserved for describing a phase change from a colloid dispersion to a solid (precipitate) when it is subjected to a perturbation. Aggregation causes sedimentation or creaming, therefore the colloid is unstable: if either of these processes occur the colloid will no longer be a suspension.\nElectrostatic stabilization and steric stabilization are the two main mechanisms for stabilization against aggregation.\nA combination of the two mechanisms is also possible (electrosteric stabilization).\nA method called gel network stabilization represents the principal way to produce colloids stable to both aggregation and sedimentation. The method consists in adding to the colloidal suspension a polymer able to form a gel network. Particle settling is hindered by the stiffness of the polymeric matrix where particles are trapped, and the long polymeric chains can provide a steric or electrosteric stabilization to dispersed particles. Examples of such substances are xanthan and guar gum.\nDestabilization.\nDestabilization can be accomplished by different methods:\nUnstable colloidal suspensions of low-volume fraction form clustered liquid suspensions, wherein individual clusters of particles sediment if they are more dense than the suspension medium, or cream if they are less dense. However, colloidal suspensions of higher-volume fraction form colloidal gels with viscoelastic properties. Viscoelastic colloidal gels, such as bentonite and toothpaste, flow like liquids under shear, but maintain their shape when shear is removed. It is for this reason that toothpaste can be squeezed from a toothpaste tube, but stays on the toothbrush after it is applied.\nMonitoring stability.\nThe most widely used technique to monitor the dispersion state of a product, and to identify and quantify destabilization phenomena, is multiple light scattering coupled with vertical scanning. This method, known as turbidimetry, is based on measuring the fraction of light that, after being sent through the sample, it backscattered by the colloidal particles. The backscattering intensity is directly proportional to the average particle size and volume fraction of the dispersed phase. Therefore, local changes in concentration caused by sedimentation or creaming, and clumping together of particles caused by aggregation, are detected and monitored. These phenomena are associated with unstable colloids.\nDynamic light scattering can be used to detect the size of a colloidal particle by measuring how fast they diffuse. This method involves directing laser light towards a colloid. The scattered light will form an interference pattern, and the fluctuation in light intensity in this pattern is caused by the Brownian motion of the particles. If the apparent size of the particles increases due to them clumping together via aggregation, it will result in slower Brownian motion. This technique can confirm that aggregation has occurred if the apparent particle size is determined to be beyond the typical size range for colloidal particles.\nAccelerating methods for shelf life prediction.\nThe kinetic process of destabilisation can be rather long (up to several months or years for some products). Thus, it is often required for the formulator to use further accelerating methods to reach reasonable development time for new product design. Thermal methods are the most commonly used and consist of increasing temperature to accelerate destabilisation (below critical temperatures of phase inversion or chemical degradation). Temperature affects not only viscosity, but also interfacial tension in the case of non-ionic surfactants or more generally interactions forces inside the system. Storing a dispersion at high temperatures enables to simulate real life conditions for a product (e.g. tube of sunscreen cream in a car in the summer), but also to accelerate destabilisation processes up to 200 times.\nMechanical acceleration including vibration, centrifugation and agitation are sometimes used. They subject the product to different forces that pushes the particles / droplets against one another, hence helping in the film drainage. Some emulsions would never coalesce in normal gravity, while they do under artificial gravity. Segregation of different populations of particles have been highlighted when using centrifugation and vibration.\nAs a model system for atoms.\nIn physics, colloids are an interesting model system for atoms. Micrometre-scale colloidal particles are large enough to be observed by optical techniques such as confocal microscopy. Many of the forces that govern the structure and behavior of matter, such as excluded volume interactions or electrostatic forces, govern the structure and behavior of colloidal suspensions. For example, the same techniques used to model ideal gases can be applied to model the behavior of a hard sphere colloidal suspension. Phase transitions in colloidal suspensions can be studied in real time using optical techniques, and are analogous to phase transitions in liquids. In many interesting cases optical fluidity is used to control colloid suspensions.\nCrystals.\nA colloidal crystal is a highly ordered array of particles that can be formed over a very long range (typically on the order of a few millimeters to one centimeter) and that appear analogous to their atomic or molecular counterparts. One of the finest natural examples of this ordering phenomenon can be found in precious opal, in which brilliant regions of pure spectral color result from close-packed domains of amorphous colloidal spheres of silicon dioxide (or silica, SiO2). These spherical particles precipitate in highly siliceous pools in Australia and elsewhere, and form these highly ordered arrays after years of sedimentation and compression under hydrostatic and gravitational forces. The periodic arrays of submicrometre spherical particles provide similar arrays of interstitial voids, which act as a natural diffraction grating for visible light waves, particularly when the interstitial spacing is of the same order of magnitude as the incident lightwave.\nThus, it has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations with interparticle separation distances, often being considerably greater than the individual particle diameter. In all of these cases in nature, the same brilliant iridescence (or play of colors) can be attributed to the diffraction and constructive interference of visible lightwaves that satisfy Bragg\u2019s law, in a matter analogous to the scattering of X-rays in crystalline solids.\nThe large number of experiments exploring the physics and chemistry of these so-called \"colloidal crystals\" has emerged as a result of the relatively simple methods that have evolved in the last 20 years for preparing synthetic monodisperse colloids (both polymer and mineral) and, through various mechanisms, implementing and preserving their long-range order formation.\nIn biology.\nColloidal phase separation is an important organising principle for compartmentalisation of both the cytoplasm and nucleus of cells into biomolecular condensates\u2014similar in importance to compartmentalisation via lipid bilayer membranes, a type of liquid crystal. The term biomolecular condensate has been used to refer to clusters of macromolecules that arise via liquid-liquid or liquid-solid phase separation within cells. Macromolecular crowding strongly enhances colloidal phase separation and formation of biomolecular condensates.\nIn the environment.\nColloidal particles can also serve as transport vector\nof diverse contaminants in the surface water (sea water, lakes, rivers, fresh water bodies) and in underground water circulating in fissured rocks\n(e.g. limestone, sandstone, granite). Radionuclides and heavy metals easily sorb onto colloids suspended in water. Various types of colloids are recognised: inorganic colloids (e.g. clay particles, silicates, iron oxy-hydroxides), organic colloids (humic and fulvic substances). When heavy metals or radionuclides form their own pure colloids, the term \"eigencolloid\" is used to designate pure phases, i.e., pure Tc(OH)4, U(OH)4, or Am(OH)3. Colloids have been suspected for the long-range transport of plutonium on the Nevada Nuclear Test Site. They have been the subject of detailed studies for many years. However, the mobility of inorganic colloids is very low in compacted bentonites and in deep clay formations\nbecause of the process of ultrafiltration occurring in dense clay membrane.\nThe question is less clear for small organic colloids often mixed in porewater with truly dissolved organic molecules.\nIn soil science, the colloidal fraction in soils consists of tiny clay and humus particles that are less than 1\u03bcm in diameter and carry either positive and/or negative electrostatic charges that vary depending on the chemical conditions of the soil sample, i.e. soil pH.\nIntravenous therapy.\nColloid solutions used in intravenous therapy belong to a major group of volume expanders, and can be used for intravenous fluid replacement. Colloids preserve a high colloid osmotic pressure in the blood, and therefore, they should theoretically preferentially increase the intravascular volume, whereas other types of volume expanders called crystalloids also increase the interstitial volume and intracellular volume. However, there is still controversy to the actual difference in efficacy by this difference, and much of the research related to this use of colloids is based on fraudulent research by Joachim Boldt. Another difference is that crystalloids generally are much cheaper than colloids."}
{"id": "5347", "revid": "41294494", "url": "https://en.wikipedia.org/wiki?curid=5347", "title": "Chinese", "text": " \nChinese may refer to:"}
{"id": "5350", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=5350", "title": "Riding shotgun", "text": "\"Riding shotgun\" was a phrase used to describe the bodyguard who rides alongside a stagecoach driver, typically armed with a break-action shotgun, called a coach gun, to ward off bandits or hostile Native Americans. In modern use, it refers to the practice of sitting alongside the driver in a moving vehicle. The coining of this phrase dates to 1905 at the latest.\nEtymology.\nThe expression \"riding shotgun\" is derived from \"shotgun messenger\", a colloquial term for \"express messenger\", when stagecoach travel was popular during the American Wild West and the Colonial period in Australia. The person rode alongside the driver. The first known use of the phrase \"riding shotgun\" was in the 1905 novel \"The Sunset Trail\" by Alfred Henry Lewis. \nIt was later used in print and especially film depiction of stagecoaches and wagons in the Old West in danger of being robbed or attacked by bandits. A special armed employee of the express service using the stage for transportation of bullion or cash would sit beside the driver, carrying a short shotgun (or alternatively a rifle), to provide an armed response in case of threat to the cargo, which was usually a strongbox. Absence of an armed person in that position often signaled that the stage was not carrying a strongbox, but only passengers.\nHistorical examples.\nTombstone, Arizona Territory.\nOn the evening of March 15, 1881, a Kinnear &amp; Company stagecoach carrying US$26,000 in silver bullion () was en route from the boom town of Tombstone, Arizona Territory to Benson, Arizona, the nearest freight terminal. Bob Paul, who had run for Pima County Sheriff and was contesting the election he lost due to ballot-stuffing, was temporarily working once again as the Wells Fargo shotgun messenger. He had taken the reins and driver's seat in Contention City because the usual driver, a well-known and popular man named Eli \"Budd\" Philpot, was ill. Philpot was riding shotgun.\nNear Drew's Station, just outside Contention City, a man stepped into the road and commanded them to \"Hold!\" Three cowboys attempted to rob the stage. Paul, in the driver's seat, fired his shotgun and emptied his revolver at the robbers, wounding a cowboy later identified as Bill Leonard in the groin. Philpot, riding shotgun, and passenger Peter Roerig, riding in the rear dickey seat, were both shot and killed. The horses spooked and Paul wasn't able to bring the stage under control for almost a mile, leaving the robbers with nothing. Paul, who normally rode shotgun, later said he thought the first shot killing Philpot had been meant for him.\nWhen Wyatt Earp first arrived in Tombstone in December 1879, he initially took a job as a stagecoach shotgun messenger for Wells Fargo, guarding shipments of silver bullion. When Earp was appointed Pima County Deputy Sheriff on July 27, 1881, his brother Morgan Earp took over his job.\nHistorical weapon.\nWhen Wells, Fargo &amp; Co. began regular stagecoach service from Tipton, Missouri to San Francisco, California in 1858, they issued shotguns to its drivers and guards for defense along the perilous 2,800 mile route. The guard was called a shotgun messenger and they were issued a Coach gun, typically a 10-gauge or 12-gauge, short, double-barreled shotgun.\nModern usage.\nThe term has been applied to an informal game, typically played by younger people. When 3 or more people are getting into a vehicle, the first person to say \"shotgun\" determines who rides beside the driver. Specific rules used vary. "}
{"id": "5355", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=5355", "title": "Cooking", "text": "Cooking, also known as cookery or professionally as the culinary arts, is the art, science and craft of using heat to make food more palatable, digestible, nutritious, or safe. Cooking techniques and ingredients vary widely, from grilling food over an open fire, to using electric stoves, to baking in various types of ovens, reflecting local conditions. Cooking is an aspect of all human societies and a cultural universal.\nTypes of cooking also depend on the skill levels and training of the cooks. Cooking is done both by people in their own dwellings and by professional cooks and chefs in restaurants and other food establishments.\nPreparing food with heat or fire is an activity unique to humans. Archeological evidence of cooking fires from at least 300,000 years ago exists, but some estimate that humans started cooking up to 2 million years ago.\nThe expansion of agriculture, commerce, trade, and transportation between civilizations in different regions offered cooks many new ingredients. New inventions and technologies, such as the invention of pottery for holding and boiling of water, expanded cooking techniques. Some modern cooks apply advanced scientific techniques to food preparation to further enhance the flavor of the dish served.\nHistory.\nPhylogenetic analysis suggests that early hominids may have adopted cooking 1 million to 2 million years ago. of burnt bone fragments and plant ashes from the Wonderwerk Cave in South Africa has provided evidence supporting control of fire by early humans 1 million years ago. In his seminal work \"\", Richard Wrangham suggested that evolution of bipedalism and a large cranial capacity meant that early \"Homo habilis\" regularly cooked food. However, unequivocal evidence in the archaeological record for the controlled use of fire begins at 400,000 BCE, long after \"Homo erectus\". Archaeological evidence from 300,000 years ago, in the form of ancient hearths, earth ovens, burnt animal bones, and flint, are found across Europe and the Middle East. The oldest evidence (via heated fish teeth from a deep cave) of controlled use of fire to cook food by archaic humans was dated to ~780,000 years ago. Anthropologists think that widespread cooking fires began about 250,000 years ago when hearths first appeared.\nRecently, the earliest hearths have been reported to be at least 790,000 years old.\nCommunication between the Old World and the New World in the Columbian Exchange influenced the history of cooking. The movement of foods across the Atlantic from the New World, such as potatoes, tomatoes, maize, beans, bell pepper, chili pepper, vanilla, pumpkin, cassava, avocado, peanut, pecan, cashew, pineapple, blueberry, sunflower, chocolate, gourds, green beans, and squash, had a profound effect on Old World cooking. The movement of foods across the Atlantic from the Old World, such as cattle, sheep, pigs, wheat, oats, barley, rice, apples, pears, peas, chickpeas, mustard, and carrots, similarly changed New World cooking.\nIn the 17th and 18th centuries, food was a classic marker of identity in Europe. In the 19th-century \"Age of Nationalism\", cuisine became a defining symbol of national identity.\nThe Industrial Revolution brought mass-production, mass-marketing, and standardization of food. Factories processed, preserved, canned, and packaged a wide variety of foods, and processed cereals quickly became a defining feature of the American breakfast. In the 1920s, freezing methods, cafeterias, and fast food restaurants emerged.\nIngredients.\nMost ingredients in cooking are derived from living organisms. Vegetables, fruits, grains and nuts as well as herbs and spices come from plants, while meat, eggs, and dairy products come from animals. Mushrooms and the yeast used in baking are kinds of fungi. Cooks also use water and minerals such as salt. Cooks can also use wine or spirits.\nNaturally occurring ingredients contain various amounts of molecules called \"proteins\", \"carbohydrates\" and \"fats\". They also contain water and minerals. Cooking involves a manipulation of the chemical properties of these molecules.\nCarbohydrates.\nCarbohydrates include the common sugar, sucrose (table sugar), a disaccharide, and such simple sugars as glucose (made by enzymatic splitting of sucrose) and fructose (from fruit), and starches from sources such as cereal flour, rice, arrowroot and potato.\nThe interaction of heat and carbohydrate is complex. Long-chain sugars such as starch tend to break down into more digestible simpler sugars. If the sugars are heated so that all water of crystallisation is driven off, caramelization starts, with the sugar undergoing thermal decomposition with the formation of carbon, and other breakdown products producing caramel. Similarly, the heating of sugars and proteins causes the Maillard reaction, a basic flavor-enhancing technique.\nAn emulsion of starch with fat or water can, when gently heated, provide thickening to the dish being cooked. In European cooking, a mixture of butter and flour called a roux is used to thicken liquids to make stews or sauces. In Asian cooking, a similar effect is obtained from a mixture of rice or corn starch and water. These techniques rely on the properties of starches to create simpler mucilaginous saccharides during cooking, which causes the familiar thickening of sauces. This thickening will break down, however, under additional heat.\nFats.\nTypes of fat include vegetable oils, animal products such as butter and lard, as well as fats from grains, including maize and flax oils. Fats are used in a number of ways in cooking and baking. To prepare stir fries, grilled cheese or pancakes, the pan or griddle is often coated with fat or oil. Fats are also used as an ingredient in baked goods such as cookies, cakes and pies. Fats can reach temperatures higher than the boiling point of water, and are often used to conduct high heat to other ingredients, such as in frying, deep frying or saut\u00e9ing. Fats are used to add flavor to food (e.g., butter or bacon fat), prevent food from sticking to pans and create a desirable texture.\nFats are one of the three main macronutrient groups in human diet, along with carbohydrates and proteins, and the main components of common food products like milk, butter, tallow, lard, salt pork, and cooking oils. They are a major and dense source of food energy for many animals and play important structural and metabolic functions, in most living beings, including energy storage, waterproofing, and thermal insulation. The human body can produce the fat it requires from other food ingredients, except for a few essential fatty acids that must be included in the diet. Dietary fats are also the carriers of some flavor and aroma ingredients and vitamins that are not water-soluble.\nProteins.\nEdible animal material, including muscle, offal, milk, eggs and egg whites, contains substantial amounts of protein. Almost all vegetable matter (in particular legumes and seeds) also includes proteins, although generally in smaller amounts. Mushrooms have high protein content. Any of these may be sources of essential amino acids. When proteins are heated they become denatured (unfolded) and change texture. In many cases, this causes the structure of the material to become softer or more friable \u2013 meat becomes \"cooked\" and is more friable and less flexible. In some cases, proteins can form more rigid structures, such as the coagulation of albumen in egg whites. The formation of a relatively rigid but flexible matrix from egg white provides an important component in baking cakes, and also underpins many desserts based on meringue.\nWater.\nCooking often involves water, and water-based liquids. These can be added in order to immerse the substances being cooked (this is typically done with water, stock or wine). Alternatively, the foods themselves can release water. A favorite method of adding flavor to dishes is to save the liquid for use in other recipes. Liquids are so important to cooking that the name of the cooking method used is often based on how the liquid is combined with the food, as in steaming, simmering, boiling, braising and blanching. Heating liquid in an open container results in rapidly increased evaporation, which concentrates the remaining flavor and ingredients; this is a critical component of both stewing and sauce making.\nVitamins and minerals.\nVitamins and minerals are required for normal metabolism; and what the body cannot manufacture itself must come from external sources. Vitamins come from several sources including fresh fruit and vegetables (Vitamin C), carrots, liver (Vitamin A), cereal bran, bread, liver (B vitamins), fish liver oil (Vitamin D) and fresh green vegetables (Vitamin K). Many minerals are also essential in small quantities including iron, calcium, magnesium, sodium chloride and sulfur; and in very small quantities copper, zinc and selenium. The micronutrients, minerals, and vitamins in fruit and vegetables may be destroyed or eluted by cooking. Vitamin C is especially prone to oxidation during cooking and may be completely destroyed by protracted cooking. The bioavailability of some vitamins such as thiamin, vitamin B6, niacin, folate, and carotenoids are increased with cooking by being freed from the food microstructure. Blanching or steaming vegetables is a way of minimizing vitamin and mineral loss in cooking.\nMethods.\nThere are many methods of cooking, most of which have been known since antiquity. These include baking, roasting, frying, grilling, barbecuing, smoking, boiling, steaming and braising. A more recent innovation is microwaving. Various methods use differing levels of heat and moisture and vary in cooking time. The method chosen greatly affects the result. Some major hot cooking techniques include:\nHealth and safety.\nIndoor air pollution.\nAs of 2021, over 2.6 billion people cook using open fires or inefficient stoves using kerosene, biomass, and coal as fuel. These cooking practices use fuels and technologies that produce high levels of household air pollution, causing 3.8 million premature deaths annually. Of these deaths, 27% are from pneumonia, 27% from ischaemic heart disease, 20% from chronic obstructive pulmonary disease, 18% from stroke, and 8% from lung cancer. Women and young children are disproportionately affected, since they spend the most time near the hearth.\nSecurity while cooking.\nHazards while cooking can include:\nTo prevent those injuries there are protections such as cooking clothing, anti-slip shoes, fire extinguisher and more.\nFood safety.\nCooking can prevent many foodborne illnesses that would otherwise occur if raw food is consumed. When heat is used in the preparation of food, it can kill or inactivate harmful organisms, such as bacteria and viruses, as well as various parasites such as tapeworms and \"Toxoplasma gondii\". Food poisoning and other illness from uncooked or poorly prepared food may be caused by bacteria such as of \"Escherichia coli\", \"Salmonella typhimurium\" and \"Campylobacter\", viruses such as noroviruses, and protozoa such as \"Entamoeba histolytica\". Bacteria, viruses and parasites may be introduced through salad, meat that is uncooked or done rare, and unboiled water.\nThe sterilizing effect of cooking depends on temperature, cooking time, and technique used. Some food spoilage bacteria such as \"Clostridium botulinum\" or \"Bacillus cereus\" can form spores that survive cooking or boiling, which then germinate and regrow after the food has cooled. This makes it unsafe to reheat cooked food more than once.\nCooking increases the digestibility of many foods which are inedible or poisonous when raw. For example, raw cereal grains are hard to digest, while kidney beans are toxic when raw or improperly cooked due to the presence of phytohaemagglutinin, which is inactivated by cooking for at least ten minutes at .\nFood safety depends on the safe preparation, handling, and storage of food. Food spoilage bacteria proliferate in the \"Danger zone\" temperature range from ; therefore, food should not be stored in this temperature range. Washing of hands and surfaces, especially when handling different meats, and keeping raw food separate from cooked food to avoid cross-contamination, are good practices in food preparation. Foods prepared on plastic cutting boards may be less likely to harbor bacteria than wooden ones. Washing and disinfecting cutting boards, especially after use with raw meat, poultry, or seafood, reduces the risk of contamination.\nEffects on nutritional content of food.\nProponents of raw foodism argue that cooking food increases the risk of some of the detrimental effects on food or health. They point out that during cooking of vegetables and fruit containing vitamin C, the vitamin elutes into the cooking water and becomes degraded through oxidation. Peeling vegetables can also substantially reduce the vitamin C content, especially in the case of potatoes where most vitamin C is in the skin. However, research has shown that in the specific case of carotenoids a greater proportion is absorbed from cooked vegetables than from raw vegetables.\nSulforaphane, a glucosinolate breakdown product, is present in vegetables such as broccoli, and is mostly destroyed when the vegetable is boiled. Although there has been some basic research on how sulforaphane might exert beneficial effects in vivo, there is no high-quality evidence for its efficacy against human diseases.\nThe United States Department of Agriculture has studied retention data for 16 vitamins, 8 minerals, and alcohol for approximately 290 foods across various cooking methods.\nCarcinogens and AGEs.\nIn a human epidemiological analysis by Richard Doll and Richard Peto in 1981, diet was estimated to cause a large percentage of cancers. Studies suggest that around 32% of cancer deaths may be avoidable by changes to the diet. Some of these cancers may be caused by carcinogens in food generated during the cooking process, although it is often difficult to identify the specific components in diet that serve to increase cancer risk.\nSeveral studies published since 1990 indicate that cooking meat at high temperature creates heterocyclic amines (HCA's), which are thought to increase cancer risk in humans. Researchers at the National Cancer Institute found that human subjects who ate beef rare or medium-rare had less than one third the risk of stomach cancer than those who ate beef medium-well or well-done. While avoiding meat or eating meat raw may be the only ways to avoid HCA's in meat fully, the National Cancer Institute states that cooking meat below creates \"negligible amounts\" of HCA's. Also, microwaving meat before cooking may reduce HCAs by 90% by reducing the time needed for the meat to be cooked at high heat. Nitrosamines are found in some food, and may be produced by some cooking processes from proteins or from nitrites used as food preservatives; cured meat such as bacon has been found to be carcinogenic, with links to colon cancer. Ascorbate, which is added to cured meat, however, reduces nitrosamine formation.\nBaking, grilling or broiling food, especially starchy foods, until a toasted crust is formed generates significant concentrations of acrylamide. This discovery in 2002 led to international health concerns. Subsequent research has however found that it is not likely that the acrylamides in burnt or well-cooked food cause cancer in humans; Cancer Research UK categorizes the idea that burnt food causes cancer as a \"myth\".\nCooking food at high temperature may create advanced glycation end-products (AGEs) that are believed to be involved in a number of diseases, including diabetes, chronic kidney disease, cancer and cardiovascular diseases, as well as in ageing. AGEs are a group of compounds that are formed between reducing sugars and amino acids via Maillard reaction. These compounds impart colors, tastes and smells that are specific to these food, but may also be deleterious to health. Dry heat (e.g. in roasting or grilling) can significantly increase the production of AGEs, as well as food rich in animal protein and fats. The production of AGEs during cooking can be significantly reduced by cooking in water or moist heat, reducing the cooking times and temperatures, as well as by first marinating the meat in acidic ingredients such as lemon juice and vinegar.\nScientific aspects.\nThe scientific study of cooking has become known as molecular gastronomy. This is a subdiscipline of food science concerning the physical and chemical transformations that occur during cooking.\nImportant contributions have been made by scientists, chefs and authors such as Herv\u00e9 This (chemist), Nicholas Kurti (physicist), Peter Barham (physicist), Harold McGee (author), Shirley Corriher (biochemist, author), Robert Wolke (chemist, author.) It is different for the application of scientific knowledge to cooking, that is \"molecular cooking\" (for the technique) or \"molecular cuisine\" (for a culinary style), for which chefs such as Raymond Blanc, Philippe and Christian Conticini, Ferran Adria, Heston Blumenthal, Pierre Gagnaire (chef).\nChemical processes central to cooking include hydrolysis (in particular beta elimination of pectins, during the thermal treatment of plant tissues), pyrolysis, and glycation reactions wrongly named Maillard reactions.\nCooking foods with heat depends on many factors: the specific heat of an object, thermal conductivity, and (perhaps most significantly) the difference in temperature between the two objects. Thermal diffusivity is the combination of specific heat, conductivity and density that determines how long it will take for the food to reach a certain temperature.\nHome-cooking and commercial cooking.\nHome cooking has traditionally been a process carried out informally in a home or around a communal fire, and can be enjoyed by all members of the family, although in many cultures women bear primary responsibility. Cooking is also often carried out outside of personal quarters, for example at restaurants, or schools. Bakeries were one of the earliest forms of cooking outside the home, and bakeries in the past often offered the cooking of pots of food provided by their customers as an additional service. In the present day, factory food preparation has become common, with many \"ready-to-eat\" as well as \"ready-to-cook\" foods being prepared and cooked in factories and home cooks using a mixture of scratch made, and factory made foods together to make a meal. The nutritional value of including more commercially prepared foods has been found to be inferior to home-made foods. Home-cooked meals tend to be healthier with fewer calories, and less saturated fat, cholesterol and sodium on a per calorie basis while providing more fiber, calcium, and iron. The ingredients are also directly sourced, so there is control over authenticity, taste, and nutritional value. The superior nutritional quality of home-cooking could therefore play a role in preventing chronic disease. Cohort studies following the elderly over 10 years show that adults who cook their own meals have significantly lower mortality, even when controlling for confounding variables.\n\"Home-cooking\" may be associated with comfort food, and some commercially produced foods and restaurant meals are presented through advertising or packaging as having been \"home-cooked\", regardless of their actual origin. This trend began in the 1920s and is attributed to people in urban areas of the U.S. wanting homestyle food even though their schedules and smaller kitchens made cooking harder."}
{"id": "5356", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5356", "title": "Cancer Coast", "text": ""}
{"id": "5359", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=5359", "title": "Collectable card game", "text": ""}
{"id": "5360", "revid": "23105605", "url": "https://en.wikipedia.org/wiki?curid=5360", "title": "Card game", "text": "A card game is any game that uses playing cards as the primary device with which the game is played, whether the cards are of a traditional design or specifically created for the game (proprietary). Countless card games exist, including families of related games (such as poker). A small number of card games played with traditional decks have formally standardized rules with international tournaments being held, but most are folk games whose rules may vary by region, culture, location or from circle to circle.\nTraditional card games are played with a \"deck\" or \"pack\" of playing cards which are identical in size and shape. Each card has two sides, the \"face\" and the \"back\". Normally the backs of the cards are indistinguishable. The faces of the cards may all be unique, or there can be duplicates. The composition of a deck is known to each player. In some cases several decks are shuffled together to form a single \"pack\" or \"shoe\". Modern card games usually have bespoke decks, often with a vast amount of cards, and can include number or action cards. This type of game is generally regarded as part of the board game hobby.\nGames using playing cards exploit the fact that cards are individually identifiable from one side only, so that each player knows only the cards they hold and not those held by anyone else. For this reason card games are often characterized as games of chance or \"imperfect information\"\u2014as distinct from games of strategy or perfect information, where the current position is fully visible to all players throughout the game. Many games that are not generally placed in the family of card games do in fact use cards for some aspect of their play.\nSome games that are placed in the card game genre involve a board. The distinction is that the play in a card game chiefly depends on the use of the cards by players (the board is a guide for scorekeeping or for card placement), while board games (the principal non-card game genre to use cards) generally focus on the players' positions on the board, and use the cards for some secondary purpose.\nHistory.\n14th and 15th centuries.\nThe earliest European mention of playing cards appears in 1371 in a Catalan language rhyme dictionary. This suggests that cards may have been \"reasonably well known\" in Catalonia (now part of Spain) at that time, perhaps introduced as a result of maritime trade with the Mamluk rulers of Egypt. It is not until 1408 that the first card game is described in a document about the exploits of two card sharps; although it is evidently very simple, the game is not named. In fact the earliest games to be mentioned by name are Gleek, Ronfa and Condemnade, the latter being the game played by the aforementioned card cheats. All three are recorded during the 15th century, along with Karn\u00f6ffel, first mentioned in 1426 and which is still played in several forms today, including Bruus, Kn\u00fcffeln, Kaiserspiel and Styrivolt.\nSince the arrival of trick-taking games in Europe in the late 14th century, there have only been two major innovations. The first was the introduction of trump cards with the power to beat all cards in other suits. Such cards were initially called \"trionfi\" and first appeared with the advent of Tarot cards in which there is a separate, permanent trump suit comprising a number of picture cards. The first known example of such cards was ordered by the Duke of Milan around 1420 and included 16 trumps with images of Greek and Roman gods. Thus games played with Tarot cards appeared very early on and spread to most parts of Europe with the notable exceptions of the British Isles, the Iberian Peninsula, and the Balkans. However, we do not know the rules of the early Tarot games; the earliest detailed description in any language being those published by the Abb\u00e9 de Marolles in Nevers in 1637.\nThe concept of trumps was sufficiently powerful that it was soon transferred to games played with far cheaper ordinary packs of cards, as opposed to expensive Tarot cards. The first of these was Triomphe, the name simply being the French equivalent of the Italian \"trionfi\". Although not testified before 1538, its first rules were written by a Spaniard who left his native country for Milan in 1509 never to return; thus the game may date to the late 15th century.\nOthers games that may well date to the 15th century are Pochen \u2013 the game of \"Bocken\" or \"Boeckels\" being attested in Strasbourg in 1441 \u2013 and Thirty-One, which is first mentioned in a French translation of a 1440 sermon by the Italian, Saint Bernadine, the name actually referring to two different card games: one like Pontoon and one like Commerce.\n16th century.\nIn the 16th century printed documents replace handwritten sources and card games become a popular topic with preachers, autobiographists and writers in general. A key source of the games in vogue in France and Europe at that time is Fran\u00e7ois Rabelais, whose fictional character \"Gargantua\" played no less than 30 card games, many of which are recognisable. They include: Aluette, B\u00eate, Cent, Coquimbert, Coucou, Flush or Flux, G\u00e9 (Pairs), Gleek, Lansquenet, Piquet, Post and Pair, Primero, Ronfa, Triomphe, Sequence, Speculation, Tarot and Trente-et-Un; possibly Rams, Mouche and Brandeln as well. Girolamo Cardano also provides invaluable information including the earliest rules of Trappola. Among the most popular were the games of Flusso and Primiera, which originated in Italy and spread throughout Europe, becoming known in England as Flush and Primero.\nIn Britain the earliest known European fishing game was recorded in 1522. Another first was Losing Loadum, noted by Florio in 1591, which is the earliest known English point-trick game. In Scotland, the game of Mawe, testified in the 1550s, evolved from a country game into one played at the royal Scottish court, becoming a favorite of James VI. The ancestor of Cribbage \u2013 a game called Noddy \u2013 is mentioned for the first time in 1589, \"Noddy\" being the Knave turned for trump at the start of play.\n17th century.\nThe 17th century saw an upsurge in the number of new games being reported as well as the first sets of rules, those for Piquet appearing in 1632 and Reversis in 1634. The first French games compendium, \"La Maison Acad\u00e9mique\", appeared in 1654 and it was followed in 1674 by Charles Cotton's \"The Compleat Gamester\", although an earlier manuscript of games by Francis Willughby was written sometime between 1665 and 1670. Cotton records the first rules for the classic English games of Cribbage, a descendant of Noddy, and Whist, a development of English Trump or Ruff ('ruff' then meaning 'rob') in which four players were dealt 12 cards each and the dealer 'robbed' from the remaining stock of 4 cards.\nPiquet was a two-player, trick-taking game that originated in France, probably in the 16th century and was initially played with 36 cards before, around 1690, the pack reduced to the 32 cards that gives the Piquet pack its name. Reversis is a reverse game in which players avoid taking tricks and appears to be an Italian invention that came to France around 1600 and spread rapidly to other countries in Europe.\nIn the mid-17th century, a certain game named after Cardinal Mazarin, prime minister to King Louis XIV, became very popular at the French royal court. Called Hoc Mazarin, it had three phases, the final one of which evolved into a much simpler game called Manille that was renamed Com\u00e8te on the appearance of Halley's Comet in 1682. In Com\u00e8te the aim is to be first to shed all one's hand cards to sequences laid out in rows on the table. However, there are certain cards known as 'stops' or \"hocs\": cards that end a sequence and give the one who played it the advantage of being able to start a new sequence. This concept spread to other 17th and 18th century games including Poque, Comete, Emprunt, Manille, Nain Jaune and Lindor, all except Emprunt being still played in some form today.\nIt was the 17th century that saw the second of the two great innovations being introduced into trick-taking games: the concept of bidding. This first emerged in the Spanish game of Ombre, an evolution of Triomphe that \"in its time, was the most successful card game ever invented.\" Ombre's origins are unclear and obfuscated by the existence of a game called Homme or B\u00eate in France, \"ombre\" and \"homme\" being respectively Spanish and French for 'man'. In Ombre, the player who won the bidding became the \"Man\" and played alone against the other two. The game spread rapidly across Europe, spawning variants for different numbers of players and known as Quadrille, Quintille, M\u00e9diateur and Solo. Quadrille went on to become highly fashionable in England during the 18th century and is mentioned several times, for example, in Jane Austen's \"Pride and Prejudice\".\nThe first rules of any game in the German language were those for R\u00fcmpffen published in 1608 and later expanded in several subsequent editions. In addition, the first German games compendium, \"Palamedes Redivivus\" appeared in 1678, containing the rules for Hoick (Hoc), Ombre, Picquet (sic), R\u00fcmpffen and Thurnspiel.\n18th century.\nThe evolution of card games continued apace, with notable national games emerging like Briscola and Tressette (Italy), Schafkopf (Bavaria), Jass (Switzerland), Mariage, the ancestor of Austria's Schnapsen and Germany's Sixty-Six, and Tapp Tarock, the progenitor of most modern central European Tarot games. Whist spread to the continent becoming very popular in the north and west. In France, Comet appeared, a game that later evolved into Nain Jaune and the Victorian game of Pope Joan.\nTypes.\nCard games may be classified in different ways: by their objective, by the equipment used (e.g. number of cards and type of suits), by country of origin or by mechanism (how the game is played). Parlett and McLeod predominantly group cards games by mechanism of which there are five categories: outplay, card exchange, hand comparison, layout and a miscellaneous category that includes combat and compendium games. These are described in the following sections.\nOutplay games.\nEasily the largest category of games in which players have a hand of cards and must play them out to the table. Play ends when players have played all their cards.\nTrick-taking games.\nTrick-taking games are the largest category of outplay games. Players typically receive an equal number of cards and a trick involves each player playing a card face up to the table \u2013 the rules of play dictating what cards may be played and who wins the trick.\nThere are two main types of trick-taking game with different objectives. Both are based on the play of multiple tricks, in each of which each player plays a single card from their hand, and based on the values of played cards one player wins or \"takes\" the trick. In plain-trick games the aim is to win a number of tricks, a specific trick or as many tricks as possible, without regard to the actual cards. In point-trick games, the number of tricks is immaterial; what counts is the value, in points, of the cards captured.\nPlain-trick games.\nMany common Anglo-American games fall into the category of plain-trick games. The usual objective is to take the most tricks, but variations taking all tricks, making as few tricks (or penalty cards) as possible or taking an exact number of tricks. Bridge, Whist and Spades are popular examples. Hearts, Black Lady and Black Maria are examples of reverse games in which the aim is to avoid certain cards. Plain-trick games may be divided into the following 11 groups:\nPoint-trick games.\nPoint-trick games are all European or of European origin and include the Tarot card games. Individual cards have specific point values and the objective is usually to amass the majority of points by taking tricks, especially those with higher value cards. There are around nine main groups:\nBeating games.\nIn beating games the idea is to beat the card just played if possible, otherwise it must be picked up, either alone or together with other cards, and added to the hand. In many beating games the objective is to shed all one's cards, in which case they are also \"shedding games\". Well known examples include Crazy Eights, Mau Mau, Durak, and Skitgubbe.\nAdding games.\nThis is a small group whose ancestor is Noddy, now extinct, but which generated the far more interesting games of Costly Colours and Cribbage. Players play in turn and add the values of the cards as they go. The aim is to reach or avoid certain totals and also to score for certain combinations.\nFishing games.\nIn fishing games, cards from the hand are played against cards in a layout on the table, capturing table cards if they match. Fishing games are popular in many nations, including China, where there are many diverse fishing games. Scopa is considered one of the national card games of Italy. Cassino is the only fishing game to be widely played in English-speaking countries. Zwicker has been described as a \"simpler and jollier version of Cassino\", played in Germany. Tablanet (tabli\u0107) is a fishing-style game popular in Balkans.\nMatching games.\nThe object of a matching (or sometimes \"melding\") game is to acquire particular groups of matching cards before an opponent can do so. In Rummy, this is done through drawing and discarding, and the groups are called melds. Mahjong is a very similar game played with tiles instead of cards. Non-Rummy examples of match-type games generally fall into the \"fishing\" genre and include the children's games Go Fish and Old Maid.\nWar group.\nIn games of the war group, also called \"catch and collect games\" or \"accumulating games\", the object is to acquire all cards in the deck. Examples include most War type games, and games involving slapping a discard pile such as Slapjack. Egyptian Ratscrew has both of these features.\nClimbing games.\nClimbing games are an Oriental family in which the idea is to play a higher card or combination of cards that the one just played. Alternatively a player must pass or may choose to pass even if able to beat. The sole Western example is the game of President, which is probably derived from an Asian game.\nCard exchange games.\nCard exchange games form another large category in which players exchange a card or cards from their hands with table cards or with other players with the aim, typically, of collecting specific cards or card combinations. Games of the rummy family are the best known.\nDraw and discard group.\nIn these games players draw a card from stock, make a move if possible or desired, and then discard a card to a discard pile. Almost all the games of this group are in the rummy family, but Golf is a non-rummy example.\nCommerce group.\nAs the name might suggest, players exchange hand cards with a common pool of cards on the table. Examples include Schwimmen, Kemps, James Bond and Whisky Poker. They originated in the old European games of Thirty-One and Commerce.\nCuckoo group.\nA very old round game played in different forms in different countries. Players are dealt just one card and may try and swap it with a neighbor to avoid having the lowest card or, sometimes, certain penalty cards. The old French game is Coucou and its later English cousin is Ranter Go Round, also called Chase the Ace and Screw Your Neighbour.\nA family of such games played with special cards includes Italian Cuc\u00f9, Scandinavian Gnav, Austrian Hexenspiel and German Vogelspiel.\nQuartet group.\nGames involving collecting sets of cards, the best known of which is Happy Families. Highly successful is its German equivalent, Quartett, which may be played with a Skat pack, but is much more commonly played with proprietary packs.\nCard passing group.\nGames involving passing cards to your neighbors. The classic game is Old Maid which may, however, be derived from German Black Peter and related to the French game of Vieux Gar\u00e7on. Pig, with its variations of Donkey and Spoons, is also popular.\nLayout games.\nPatience or solitaire games.\nMost patience or card solitaire games are designed to be played by one player, but some are designed for two or more players to compete.\nSingle player patiences or solitaires.\nPatience games originated in northern Europe and were designed for a single player, hence its subsequent North American name of solitaire. Most games begin with a specific layout of cards, called a tableau, and the object is then either to construct a more elaborate final layout, or to clear the tableau and/or the draw pile or stock by moving all cards to one or more discard or foundation piles.\nCompetitive patiences.\nIn competitive patiences, two or more players compete to be first to complete a patience or solitaire-like tableau. Some use a common layout; in others each player has a separate layout. Popular examples include Spite and Malice, Racing Demon or Nerts, Spit, Speed and Russian Bank.\nConnecting games.\nThe most common of these is Card Dominoes also known as Fan Tan or Parliament in which the idea is to build the four suits in sequence from a central card (the 7 in 52-card games or the Unter in 32-card packs). The winner is the first out and the loser the last left in holding cards.\nHand comparison games.\nHand comparison games, also called comparing card games, are mostly gambling games that use cards. Players lay their initial stakes, are dealt cards, may or may not be able to exchange or add to them, and may or may not be able to raise their stakes, and the outcome is decided by some form of comparison of card values or combinations. The main groups are vying and banking games. A smaller mainly Oriental group are partition games in which players divide their hands before comparing.\nVying games.\nVying games, are those in which players bet or \"vie\" on who has the best hand. The player with the best combination of hand cards in a \"showdown\", or the player able to bluff the others into folding, wins the hand. Easily the best known of the group around the world is Poker, which itself is a family of games with over 100 variants. Other examples include English Brag and the old Basque game of Mus. Most may be classified as gambling games and, while they may involve skill in terms of bluffing and memorizing and assessing odds, they involve little or no card playing skill.\nPoker games.\nPoker is a family of gambling games in which players bet into a pool, called the pot, the value of which changes as the game progresses that the value of the hand they carry will beat all others according to the ranking system. Variants largely differ on how cards are dealt and the methods by which players can improve a hand. For many reasons, including its age and its popularity among Western militaries, it is one of the most universally known card games in existence.\nBanking games.\nThese are gambling games played for money or chips in which players compete, not against one another, but against a banker. They are commonly played in casinos, but many have become domesticized, played at home for sweets, matchsticks or points. In casino games, the banker will have a 'house advantage' that ensures a profit for the casino. Popular casino games include Blackjack and Baccarat, while Pontoon is a cousin of Blackjack that emerged from the trenches of the First World War to become a popular British family game.\nMiscellaneous games.\nThese games do not fit into any of the foregoing categories. The only traditional games in this group are the compendium games, which date back at least 200 years, and Speculation, a 19th century trading game.\nCompendium games.\nCompendium games consist of a sequence of different contracts played in succession. A common pattern is for a number of reverse deals to be played, in which the aim is to avoid certain cards, followed by a final contract which is a domino-type game. Examples include: Barbu, Herzeln, Lorum and Rosbiratschka. In other games, such as Quodlibet and Rumpel, there is a range of widely varying contracts.\nCombat games.\nA new genre not recorded before 1970, most of which use proprietary cards of the collectible card game type (see below). The earliest example is Cuttle and the best known is .\nCard games by objective.\nAnother broad way of classifying card games is by objective. There are four main types as well as a handful of games that have miscellaneous objectives.\nCapturing games.\nIn these games the objective is to capture cards or to avoid capturing them. These break down into the following:\nShedding games.\nIn a shedding game, also called an accumulating game, players start with a hand of cards, and the object of the game is to be the first player to discard all cards from one's hand. Common shedding games include Crazy Eights (commercialized by Mattel as Uno) and Daihinmin. Similar games are Switch, Mau Mau or Whot!. Some matching-type games are also shedding-type games; some variants of Rummy such as Paskahousu, Phase 10, Rummikub, the bluffing game I Doubt It, and the children's games Musta Maija and Old Maid, fall into both categories.\nCombination games.\nIn many games, the aim is to form combinations of cards: by addition, by matching sets or forming sequences. All Rummy games are based on the last two principles, although in the basic variants, the end objective is to shed cards which makes them shedding games (see above). However, meld scoring variants such as Canasta or Romm\u00e9 are true combination games.\nComparing games.\nComparing card games are those where hand values are compared to determine the winner, also known as \"vying\" or \"showdown\" games. Poker, blackjack, mus, and baccarat are examples of comparing card games. As seen, nearly all of these games are designed as gambling games.\nDrinking games.\nDrinking card games are drinking games using cards, in which the object in playing the game is either to drink or to force others to drink. Many games are ordinary card games with the establishment of \"drinking rules\"; President, for instance, is virtually identical to Daihinmin but with additional rules governing drinking. Poker can also be played using a number of drinks as the wager. Another game often played as a drinking game is Toepen, quite popular in the Netherlands. Some card games are designed specifically to be played as drinking games.\nProprietary games.\nThese are card games played with a dedicated deck. Many other card games have been designed and published on a commercial or amateur basis. In a few cases, the game uses the standard 52-card deck, but the object is unique. In Eleusis, for example, players play single cards, and are told whether the play was legal or illegal, in an attempt to discover the underlying rules made up by the dealer.\nMost of these games however typically use a specially made deck of cards designed specifically for the game (or variations of it). The decks are thus usually proprietary, but may be created by the game's players. Uno, Phase 10, Set, and 1000 Blank White Cards are popular dedicated-deck card games; 1000 Blank White Cards is unique in that the cards for the game are designed by the players of the game while playing it; there is no commercially available deck advertised as such.\nCollectible card games (CCGs).\nCollectible card games (CCG) are proprietary playing card games. CCGs are games of strategy between two or more players. Each player has their own deck constructed from a very large pool of unique cards in the commercial market. The cards have different effects, costs, and art. New card sets are released periodically and sold as starter decks or booster packs. Obtaining the different cards makes the game a collectible card game, and cards are sold or traded on the secondary market. \"\", \"Pok\u00e9mon\", and \"Yu-Gi-Oh!\" are well-known collectible card games.\nLiving card games (LCGs).\nLiving card games (LCGs) are similar to collectible card games (CCGs), with their most distinguishing feature being a fixed distribution method, which breaks away from the traditional collectible card game format. While new cards for CCGs are usually sold in the form of starter decks or booster packs (the latter being often randomized), LCGs thrive on a model that requires players to acquire one core set in order to play the game, which players can further customize by acquiring extra sets or expansions featuring new content in the form of cards or scenarios. No randomization is involved in the process, thus players that get the same sets or expansions will get the exact same content. The term was popularized by Fantasy Flight Games (FFG) and mainly applies to its products, however some tabletop gaming companies can be seen using a very similar model.\nSimulation card games.\nA deck of either customized dedicated cards or a standard deck of playing cards with assigned meanings is used to simulate the actions of another activity, for example card football.\nFictional card games.\nMany games, including card games, are fabricated by science fiction authors and screenwriters to distance a culture depicted in the story from present-day Western culture. They are commonly used as filler to depict background activities in an atmosphere like a bar or rec room, but sometimes the drama revolves around the play of the game. Some of these games become real card games as the holder of the intellectual property develops and markets a suitable deck and ruleset for the game, while others lack sufficient descriptions of rules, or depend on cards or other hardware that are infeasible or physically impossible.\nTypical structure of card games.\nNumber and association of players.\nAny specific card game imposes restrictions on the number of players. The most significant dividing lines run between one-player games and two-player games, and between two-player games and multi-player games. Card games for one player are known as \"solitaire\" or \"patience\" card games. (See list of solitaire card games.) Generally speaking, they are in many ways special and atypical, although some of them have given rise to two- or multi-player games such as Spite and Malice.\nIn card games for two players, usually not all cards are distributed to the players, as they would otherwise have perfect information about the game state. Two-player games have always been immensely popular and include some of the most significant card games such as piquet, bezique, sixty-six, klaberjass, gin rummy and cribbage. Many multi-player games started as two-player games that were adapted to a greater number of players. For such adaptations a number of non-obvious choices must be made beginning with the choice of a game orientation.\nOne way of extending a two-player game to more players is by building two teams of equal size. A common case is four players in two fixed partnerships, sitting crosswise as in whist and contract bridge. Partners sit opposite to each other and cannot see each other's hands. If communication between the partners is allowed at all, then it is usually restricted to a specific list of permitted signs and signals. 17th-century French partnership games such as triomphe were special in that partners sat next to each other and were allowed to communicate freely so long as they did not exchange cards or play out of order.\nAnother way of extending a two-player game to more players is as a \"cut-throat\" or \"individual\" game, in which all players play for themselves, and win or lose alone. Most such card games are \"round games\", i.e. they can be played by any number of players starting from two or three, so long as there are enough cards for all.\nFor some of the most interesting games such as ombre, tarot and skat, the associations between players change from hand to hand. Ultimately players all play on their own, but for each hand, some game mechanism divides the players into two teams. Most typically these are \"solo games\", i.e. games in which one player becomes the soloist and has to achieve some objective against the others, who form a team and win or lose all their points jointly. But in games for more than three players, there may also be a mechanism that selects two players who then have to play against the others.\nDirection of play.\nThe players of a card game normally form a circle around a table or other space that can hold cards. The \"game orientation\" or \"direction of play\", which is only relevant for three or more players, can be either clockwise or counterclockwise. It is the direction in which various roles in the game proceed. (In real-time card games, there may be no need for a direction of play.) Most regions have a traditional direction of play, such as:\nEurope is roughly divided into a clockwise area in the north and a counterclockwise area in the south. The boundary runs between England, Ireland, Netherlands, Germany, Austria (mostly), Slovakia, Ukraine and Russia (clockwise) and France, Switzerland, Spain, Italy, Slovenia, Balkans, Hungary, Romania, Bulgaria, Greece and Turkey (counterclockwise).\nGames that originate in a region with a strong preference are often initially played in the original direction, even in regions that prefer the opposite direction. For games that have official rules and are played in tournaments, the direction of play is often prescribed in those rules.\nDetermining who deals.\nMost games have some form of asymmetry between players. The roles of players are normally expressed in terms of the \"dealer\", i.e. the player whose task it is to shuffle the cards and distribute them to the players. Being the dealer can be a (minor or major) advantage or disadvantage, depending on the game. Therefore, after each played hand, the deal normally passes to the next player according to the game orientation.\nAs it can still be an advantage or disadvantage to be the first dealer, there are some standard methods for determining who is the first dealer. A common method is by cutting, which works as follows. One player shuffles the deck and places it on the table. Each player lifts a packet of cards from the top, reveals its bottom card, and returns it to the deck. The player who reveals the highest (or lowest) card becomes dealer. In the case of a tie, the process is repeated by the tied players. For some games such as whist this process of cutting is part of the official rules, and the hierarchy of cards for the purpose of cutting (which need not be the same as that used otherwise in the game) is also specified. But in general, any method can be used, such as tossing a coin in case of a two-player game, drawing cards until one player draws an ace, or rolling dice.\nHands, rounds and games.\nA \"hand\", also called a \"deal\", is a unit of the game that begins with the dealer shuffling and dealing the cards as described below, and ends with the players scoring and the next dealer being determined. The set of cards that each player receives and holds in his or her hands is also known as that player's hand.\nThe hand is over when the players have finished playing their hands. Most often this occurs when one player (or all) has no cards left. The player who sits after the dealer in the direction of play is known as eldest hand (or in two-player games as elder hand) or forehand. A \"game round\" consists of as many hands as there are players. After each hand, the deal is passed on in the direction of play, i.e. the previous eldest hand becomes the new dealer. Normally players score points after each hand. A game may consist of a fixed number of rounds. Alternatively it can be played for a fixed number of points. In this case it is over with the hand in which a player reaches the target score.\nShuffling.\nShuffling is the process of bringing the cards of a pack into a random order. There are a large number of techniques with various advantages and disadvantages. \"Riffle shuffling\" is a method in which the deck is divided into two roughly equal-sized halves that are bent and then released, so that the cards interlace. Repeating this process several times randomizes the deck well, but the method is harder to learn than some others and may damage the cards. The \"overhand shuffle\" and the \"Hindu shuffle\" are two techniques that work by taking batches of cards from the top of the deck and reassembling them in the opposite order. They are easier to learn but must be repeated more to sufficiently randomize the deck. A method suitable for small children consists in spreading the cards on a large surface and moving them around before picking up the deck again. This is also the most common method for shuffling tiles such as dominoes.\nFor casino games that are played for large sums it is vital that the cards be properly randomized, but for many games this is less critical, and in fact player experience can suffer when the cards are shuffled too well. The official skat rules stipulate that the cards are \"shuffled well\", but according to a decision of the German skat court, a one-handed player should ask another player to do the shuffling, rather than use a shuffling machine, as it would shuffle the cards \"too\" well. French belote rules go so far as to prescribe that the deck never be shuffled between hands.\nDealing.\nThe dealer takes all of the cards in the pack, arranges them so that they are in a uniform stack, and shuffles them. In strict play, the dealer then offers the deck to the previous player (in the sense of the game direction) for \"cutting\". If the deal is clockwise, this is the player to the dealer's right; if counterclockwise, it is the player to the dealer's left. The invitation to cut is made by placing the pack, face downward, on the table near the player who is to cut: who then lifts the upper portion of the pack clear of the lower portion and places it alongside. (Normally the two portions have about equal size. Strict rules often indicate that each portion must contain a certain minimum number of cards, such as three or five.) The formerly lower portion is then replaced on top of the formerly upper portion. Instead of cutting, one may also knock on the deck to indicate that one trusts the dealer to have shuffled fairly.\nThe actual \"deal\" (distribution of cards) is done in the direction of play, beginning with eldest hand. The dealer holds the pack, face down, in one hand, and removes cards from the top of it with his or her other hand to distribute to the players, placing them face down on the table in front of the players to whom they are dealt. The cards may be dealt one at a time, or in batches of more than one card; and either the entire pack or a determined number of cards are dealt out. The undealt cards, if any, are left face down in the middle of the table, forming the \"stock\" (also called the talon, widow, skat or kitty depending on the game and region).\nThroughout the shuffle, cut, and deal, the dealer should prevent the players from seeing the faces of any of the cards. The players should not try to see any of the faces. Should a player accidentally see a card, other than one's own, proper etiquette would be to admit this. It is also dishonest to try to see cards as they are dealt, or to take advantage of having seen a card. Should a card accidentally become exposed, (visible to all), any player can demand a redeal (all the cards are gathered up, and the shuffle, cut, and deal are repeated) or that the card be replaced randomly into the deck (\"burning\" it) and a replacement dealt from the top to the player who was to receive the revealed card.\nWhen the deal is complete, all players pick up their cards, or \"hand\", and hold them in such a way that the faces can be seen by the holder of the cards but not the other players, or vice versa depending on the game. It is helpful to fan one's cards out so that if they have corner indices all their values can be seen at once. In most games, it is also useful to sort one's hand, rearranging the cards in a way appropriate to the game. For example, in a trick-taking game it may be easier to have all one's cards of the same suit together, whereas in a rummy game one might sort them by rank or by potential combinations.\nSignalling.\nNormally communication between partners about tactics or the cards in their hands is forbidden. However, in a small number of games communication and/or signaling is permitted and very much part of the play. Most of these games are very old and, often, have rules of play that allow any card to be played at any time. Such games include:\nRules.\nA new card game starts in a small way, either as someone's invention, or as a modification of an existing game. Those playing it may agree to change the rules as they wish. The rules that they agree on become the \"house rules\" under which they play the game. A set of house rules may be accepted as valid by a group of players wherever they play, as it may also be accepted as governing all play within a particular house, caf\u00e9, or club.\nWhen a game becomes sufficiently popular, so that people often play it with strangers, there is a need for a generally accepted set of rules. This need is often met when a particular set of house rules becomes generally recognized. For example, when Whist became popular in 18th-century England, players in the Portland Club agreed on a set of house rules for use on its premises. Players in some other clubs then agreed to follow the \"Portland Club\" rules, rather than go to the trouble of codifying and printing their own sets of rules. The Portland Club rules eventually became generally accepted throughout England and Western cultures.\nThere is nothing static or \"official\" about this process. For the majority of games, there is no one set of universal rules by which the game is played, and the most common ruleset is no more or less than that. Many widely played card games, such as Canasta and Pinochle, have no official regulating body. The most common ruleset is often determined by the most popular distribution of rulebooks for card games. Perhaps the original compilation of popular playing card games was collected by Edmund Hoyle, a self-made authority on many popular parlor games. The U.S. Playing Card Company now owns the eponymous Hoyle brand, and publishes a series of rulebooks for various families of card games that have largely standardized the games' rules in countries and languages where the rulebooks are widely distributed. However, players are free to, and often do, invent \"house rules\" to supplement or even largely replace the \"standard\" rules.\nIf there is a sense in which a card game can have an official set of rules, it is when that card game has an \"official\" governing body. For example, the rules of tournament bridge are governed by the World Bridge Federation, and by local bodies in various countries such as the American Contract Bridge League in the U.S., and the English Bridge Union in England. The rules of skat are governed by The International Skat Players Association and, in Germany, by the \"Deutscher Skatverband\" which publishes the \"Skatordnung\". The rules of French tarot are governed by the F\u00e9d\u00e9ration Fran\u00e7aise de Tarot. The rules of Schafkopf are laid down by the \"Schafkopfschule\" in Munich. Even in these cases, the rules must only be followed at games sanctioned by these governing bodies or where the tournament organisers specify them. Players in informal settings are free to implement agreed supplemental or substitute rules. For example, in Schafkopf there are numerous local variants sometimes known as \"impure\" Schafkopf and specified by assuming the official rules and describing the additions e.g. \"with Geier and Bettel, tariff 5/10 cents\".\nRule infractions.\nAn infraction is any action which is against the rules of the game, such as playing a card when it is not one's turn to play or the accidental exposure of a card, informally known as \"bleeding.\"\nIn many official sets of rules for card games, the rules specifying the penalties for various infractions occupy more pages than the rules specifying how to play correctly. This is tedious but necessary for games that are played seriously. Players who intend to play a card game at a high level generally ensure before beginning that all agree on the penalties to be used. When playing privately, this will normally be a question of agreeing house rules. In a tournament, there will probably be a tournament director who will enforce the rules when required and arbitrate in cases of doubt.\nIf a player breaks the rules of a game deliberately, this is cheating. The rest of this section is therefore about accidental infractions, caused by ignorance, clumsiness, inattention, etc.\nAs the same game is played repeatedly among a group of players, precedents build up about how a particular infraction of the rules should be handled. For example, \"Sheila just led a card when it wasn't her turn. Last week when Jo did that, we agreed ... etc.\" Sets of such precedents tend to become established among groups of players, and to be regarded as part of the house rules. Sets of house rules may become formalized, as described in the previous section. Therefore, for some games, there is a \"proper\" way of handling infractions of the rules. But for many games, without governing bodies, there is no standard way of handling infractions.\nIn many circumstances, there is no need for special rules dealing with what happens after an infraction. As a general principle, the person who broke a rule should not benefit from it, and the other players should not lose by it. An exception to this may be made in games with fixed partnerships, in which it may be felt that the partner(s) of the person who broke a rule should also not benefit. The penalty for an accidental infraction should be as mild as reasonable, consistent with there being a possible benefit to the person responsible.\nPlaying cards.\nThe oldest surviving reference to the card game in world history is from the 9th century China, when the \"Collection of Miscellanea at Duyang\", written by Tang-dynasty writer Su E, described Princess (daughter of Emperor Yizong of Tang) playing the \"leaf game\" with members of the Wei clan (the family of the princess's husband) in 868 . The Song dynasty statesman and historian Ouyang Xiu has noted that paper playing cards arose in connection to an earlier development in the book format from scrolls to pages.\nPlaying cards first appeared in Europe in the last quarter of the 14th century. The earliest European references speak of a Saracen or Moorish game called \"naib\", and in fact an almost complete Mamluk Egyptian deck of 52 cards in a distinct oriental design has survived from around the same time, with the four suits \"swords\", \"polo sticks\", \"cups\" and \"coins\" and the ranks \"king\", \"governor\", \"second governor\", and \"ten\" to \"one\".\nThe 1430s in Italy saw the invention of the tarot deck, a full Latin-suited deck augmented by suitless cards with painted motifs that played a special role as trumps. Tarot card games are still played with (subsets of) these decks in parts of Central Europe. A full tarot deck contains 14 cards in each suit; low cards labeled 1\u201310, and court cards (jack), (cavalier/knight), (queen), and (king), plus the fool or excuse card, and 21 trump cards. In the 18th century the card images of the traditional Italian tarot decks became popular in cartomancy and evolved into \"esoteric\" decks used primarily for the purpose; today most tarot decks sold in North America are the occult type, and are closely associated with fortune telling. In Europe, \"playing tarot\" decks remain popular for games, and have evolved since the 18th century to use regional suits (spades, hearts, diamonds and clubs in France; leaves, hearts, bells and acorns in Germany) as well as other familiar aspects of the English-pattern pack such as corner card indices and \"stamped\" card symbols for non-court cards. Decks differ regionally based on the number of cards needed to play the games; the French tarot consists of the \"full\" 78 cards, while Germanic, Spanish and Italian Tarot variants remove certain values (usually low suited cards) from the deck, creating a deck with as few as 32 cards.\nThe French suits were introduced around 1480 and, in France, mostly replaced the earlier Latin suits of \"swords\", \"clubs\", \"cups\" and \"coins\". (which are still common in Spanish- and Portuguese-speaking countries as well as in some northern regions of Italy) The suit symbols, being very simple and single-color, could be stamped onto the playing cards to create a deck, thus only requiring special full-color card art for the court cards. This drastically simplifies the production of a deck of cards versus the traditional Italian deck, which used unique full-color art for each card in the deck. The French suits became popular in English playing cards in the 16th century (despite historic animosity between France and England), and from there were introduced to British colonies including North America. The rise of Western culture has led to the near-universal popularity and availability of French-suited playing cards even in areas with their own regional card art.\nIn Japan, a distinct 48-card hanafuda deck is popular. It is derived from 16th-century Portuguese decks, after undergoing a long evolution driven by laws enacted by the Tokugawa shogunate attempting to ban the use of playing cards\nThe best-known deck internationally is the English pattern of the 52-card French deck, also called the International or Anglo-American pattern, used for such games as poker and contract bridge. It contains one card for each unique combination of thirteen \"ranks\" and the four French \"suits\" \"spades\", \"hearts\", \"diamonds\", and \"clubs\". The ranks (from highest to lowest in bridge and poker) are \"ace\", \"king\", \"queen\", \"jack\" (or \"knave\"), and the numbers from \"ten\" down to \"two\" (or \"deuce\"). The trump cards and \"knight\" cards from the French playing tarot are not included.\nOriginally the term \"knave\" was more common than \"jack\"; the card had been called a jack as part of the terminology of all-fours since the 17th century, but the word was considered vulgar. (Note the exclamation by Estella in Charles Dickens's novel \"Great Expectations\": \"He calls the knaves, Jacks, this boy!\") However, because the card abbreviation for knave (\"Kn\") was so close to that of the king, it was very easy to confuse them, especially after suits and rankings were moved to the corners of the card in order to enable people to fan them in one hand and still see all the values. (The earliest known deck to place suits and rankings in the corner of the card is from 1693, but these cards did not become common until after 1864 when Hart reintroduced them along with the knave-to-jack change.) However, books of card games published in the third quarter of the 19th century evidently still referred to the \"knave\", and the term with this definition is still recognized in the United Kingdom.\nIn the 17th century, a French, five-trick, gambling game called B\u00eate became popular and spread to Germany, where it was called La Bete and England where it was named Beast. It was a derivative of Triomphe and was the first card game in history to introduce the concept of bidding.\nChinese handmade mother-of-pearl gaming counters were used in scoring and bidding of card games in the West during the approximate period of 1700\u20131840. The gaming counters would bear an engraving such as a coat of arms or a monogram to identify a family or individual. Many of the gaming counters also depict Chinese scenes, flowers or animals. Queen Charlotte is one prominent British individual who is known to have played with the Chinese gaming counters. Card games such as Ombre, Quadrille and Pope Joan were popular at the time and required counters for scoring. The production of counters declined after Whist, with its different scoring method, became the most popular card game in the West.\nBased on the association of card games and gambling, Pope Benedict XIV banned card games on October 17, 1750."}
{"id": "5361", "revid": "1263324663", "url": "https://en.wikipedia.org/wiki?curid=5361", "title": "Cross-stitch", "text": "Cross-stitch is a form of sewing and a popular form of counted-thread embroidery in which X-shaped stitches (called cross stitches) in a tiled, raster-like pattern are used to form a picture. The stitcher counts the threads on a piece of evenweave fabric (such as linen) in each direction so that the stitches are of uniform size and appearance. This form of cross-stitch is also called counted cross-stitch in order to distinguish it from other forms of cross-stitch. Sometimes cross-stitch is done on designs printed on the fabric (stamped cross-stitch); the stitcher simply stitches over the printed pattern. Cross-stitch is often executed on easily countable fabric called aida cloth, whose weave creates a plainly visible grid of squares with holes for the needle at each corner.\nFabrics used in cross-stitch include linen, aida cloth, and mixed-content fabrics called 'evenweave' such as jobelan. All cross-stitch fabrics are technically \"evenweave\" as the term refers to the fact that the fabric is woven to make sure that there are the same number of threads per inch in both the warp and the weft (i.e. vertically and horizontally). Fabrics are categorized by threads per inch (referred to as 'count'), which can range from 11 to 40 count.\nCounted cross-stitch projects are worked from a gridded pattern called a chart and can be used on any count fabric; the count of the fabric and the number of threads per stitch determine the size of the finished stitching. For example, if a given design is stitched on a 28 count cross-stitch fabric with each cross worked over two threads, the finished stitching size is the same as it would be on a 14 count aida cloth fabric with each cross worked over one square. These methods are referred to as \"2 over 2\" (2 embroidery threads used to stitch over 2 fabric threads) and \"1 over 1\" (1 embroidery thread used to stitch over 1 fabric thread or square), respectively. There are different methods of stitching a pattern, including the cross-country method where one colour is stitched at a time, or the parking method where one block of fabric is stitched at a time and the end of the thread is \"parked\" at the next point the same colour occurs in the pattern.\nHistory.\nCross-stitch can be found all over the world since the Middle Ages. Many folk museums show examples of clothing decorated with cross-stitch, especially from continental Europe and Asia.\nThe cross-stitch sampler is called that because it was generally stitched by a young girl to learn how to stitch and to record alphabet and other patterns to be used in her household sewing. These samples of her stitching could be referred back to over the years. Often, motifs and initials were stitched on household items to identify their owner, or simply to decorate the otherwise-plain cloth. The earliest known cross stitch sampler made in the United States is currently housed at Pilgrim Hall in Plymouth, Massachusetts. The sampler was created by Loara Standish, daughter of Captain Myles Standish and pioneer of the Leviathan stitch, circa 1653.\nTraditionally, cross-stitch was used to embellish items like household linens, tablecloths, dishcloths, and doilies (only a small portion of which would actually be embroidered, such as a border). Although there are many cross-stitchers who still employ it in this fashion, it is now increasingly popular to work the pattern on pieces of fabric and hang them on the wall for decoration. Cross-stitch is also often used to make greeting cards, pillow tops, or as inserts for box tops, coasters and trivets.\nMulticoloured, shaded, painting-like patterns as we know them today are a fairly modern development, deriving from similar shaded patterns of Berlin wool work of the mid-nineteenth century. Besides designs created expressly for cross-stitch, there are software programs that convert a photograph or a fine art image into a chart suitable for stitching. One example of this is in the cross-stitched reproduction of the Sistine Chapel charted and stitched by Joanna Lopianowski-Roberts.\nThere are many cross-stitching \"guilds\" and groups across the United States and Europe which offer classes, collaborate on large projects, stitch for charity, and provide other ways for local cross-stitchers to get to know one another. Individually owned local needlework shops (LNS) often have stitching nights at their shops, or host weekend stitching retreats.\nToday, cotton floss is the most common embroidery thread. It is a thread made of mercerized cotton, composed of six strands that are only loosely twisted together and easily separable. While there are other manufacturers, the two most-commonly used (and oldest) brands are DMC and Anchor, both of which have been manufacturing embroidery floss since the 1800s.\nOther materials used are pearl (or perle) cotton, Danish flower thread, silk and Rayon. Different wool threads, metallic threads or other novelty threads are also used, sometimes for the whole work, but often for accents and embellishments. Hand-dyed cross-stitch floss is created just as the name implies\u2014it is dyed by hand. Because of this, there are variations in the amount of color throughout the thread. Some variations can be subtle, while some can be a huge contrast. Some also have more than one color per thread.\nCross-stitch is widely used in traditional Palestinian dressmaking. Palestinian cross stitch is called tatreez. In 2021, tatreez was added to the UNESCO List of the Intangible Cultural Heritage of Humanity.\nRelated stitches and forms of embroidery.\nThe cross-stitch can be executed partially such as in quarter-, half-, and three-quarter-stitches. A single straight stitch, done in the form of backstitching, is often used as an outline, to add detail or definition.\nThere are many stitches which are related structurally to cross-stitch. The best known are Italian cross-stitch (as seen in Assisi embroidery), long-armed cross-stitch, and Montenegrin stitch. Italian cross-stitch and Montenegrin stitch are reversible, meaning the work looks the same on both sides. These styles have a slightly different look than ordinary cross-stitch. These more difficult stitches are rarely used in mainstream embroidery, but they are still used to recreate historical pieces of embroidery or by the creative and adventurous stitcher. The double cross-stitch, also known as a Leviathan stitch or Smyrna cross-stitch, combines a cross-stitch with an upright cross-stitch.\nBerlin wool work and similar petit point stitchery resembles the heavily shaded, opulent styles of cross-stitch, and sometimes also used charted patterns on paper.\nCross-stitch is often combined with other popular forms of embroidery, such as Hardanger embroidery or blackwork embroidery. Cross-stitch may also be combined with other work, such as canvaswork or drawn thread work. Beadwork and other embellishments such as paillettes, charms, small buttons and specialty threads of various kinds may also be used. Cross stitch can often be used in needlepoint.\nTwenty first century cross stitch trends.\nCross-stitch has become increasingly popular with the younger generation of Europe in recent years. Retailers such as John Lewis experienced a 17% rise in sales of haberdashery products between 2009 and 2010. Hobbycraft, a chain of stores selling craft supplies, also enjoyed an 11% increase in sales over the year to February 22, 2009 primarily attributed to the needlework sector. The cross stitch market has continued to grow and market research firm Mintel reported a 12% rise in women doing some sort of needlecraft as a hobby between 2015 and 2017. London department store Liberty's, reported double-digit growth in the fabric and haberdashery departments in 2017 and the store increased its range by 25%.\nKnitting and cross-stitching have become more popular hobbies for a younger market, in contrast to its traditional reputation as a hobby for retirees. Sewing and craft groups such as Stitch and Bitch London have resurrected the idea of the traditional craft club. At Clothes Show Live 2010 there was a new area called \"Sknitch\" promoting modern sewing, knitting and embroidery.\nIn a departure from the traditional designs associated with cross-stitch, there is a current trend for more postmodern or tongue-in-cheek designs featuring retro images or contemporary sayings. It is linked to a concept known as 'subversive cross-stitch', which involves more risque designs, often fusing the traditional sampler style with sayings designed to shock or be incongruous with the old-fashioned image of cross-stitch.\nStitching designs on other materials can be accomplished by using waste canvas. This is a temporary gridded canvas similar to regular canvas used for embroidery that is held together by a water-soluble glue, which is removed after completion of stitch design. Soluble canvas serves a similar purpose and is entirely dissolved in water after finishing a design. Other crafters have taken to cross-stitching on all manner of gridded objects as well including old kitchen strainers or chain-link fences.\nWhile cross stitch is traditionally a women's craft, it is growing in popularity among men.\nCross-stitch and feminism.\nIn the 21st century, an emphasis on feminist design has emerged within cross-stitch communities. Some cross-stitchers have commented on the way that the practice of embroidery makes them feel connected to the women who practised it before them. There is a push for all embroidery, including cross-stitch, to be respected as a significant art form.\nCross-stitch and computers.\nThe development of computer technology has also affected such a seemingly conservative craft as cross-stitch. With the help of computer visualization algorithms, it is now possible to create embroidery designs using a photograph or any other picture. Visualisation uses a drawing on a graphical grid, representing colors and / or symbols, which gives the user an indication of the possible use of colors, the position of those colors, and the type of stitch used, such as full cross or quarter stitch.\nFlosstube.\nAn increasingly popular activity for cross-stitchers is to watch and make YouTube videos detailing their hobby. Flosstubers, as they are known, typically cover WIPs (Works in Progress), FOs (Finished Objects), and Haul (new patterns, thread, and fabric, as well as cross-stitching accessories, such as needle minders). Other accessories include but are not limited to: Floss organizers, thread conditioner, pin cushions, aida cloth or plastic canvas, and embroidery needles."}
{"id": "5362", "revid": "46051904", "url": "https://en.wikipedia.org/wiki?curid=5362", "title": "Casino game", "text": "Games available in most casinos are commonly called casino games. In a casino game, the players gamble cash or casino chips on various possible random outcomes or combinations of outcomes. Casino games are also available in online casinos, where permitted by law. Casino games can also be played outside of casinos for entertainment purposes, like in parties or in school competitions, on machines that simulate gambling.\nCategories.\nThere are three general categories of casino games: gaming machines, table games, and random number games. Gaming machines, such as slot machines and pachinko, are usually played by one player at a time and do not require the involvement of casino employees. Tables games, such as blackjack or craps, involve one or more players who are competing against the house (the casino itself) rather than each other. Table games are usually conducted by casino employees known as croupiers or dealers. Random number games are based on the selection of random numbers, either from a computerized random number generator or from other gaming equipment. Random number games may be played at a table or through the purchase of paper tickets or cards, such as keno or bingo.\nSome casino games combine multiple of the above aspects; for example, roulette is a table game conducted by a dealer, that involves random numbers. Casinos may also offer other types of gaming, such as hosting poker games or tournaments where players compete against each other.\nCommon casino games.\nGames commonly found at casinos include table games, gaming machines and random number games.\nTable games.\nIn the United States, 'table game' is the term used for games of chance such as blackjack, craps, roulette, and baccarat that are played against the casino and operated by one or more live croupiers, as opposed to those played on a mechanical device like a slot machine or against other players instead of the casino, such as standard poker.\nTable games are popularly played in casinos and involve some form of legal gambling, but they are also played privately under varying house rules. The term has significance in that some jurisdictions permit casinos to have only slots and no table games. In some states, this law has resulted in casinos employing electronic table games, such as roulette, blackjack, and craps.\nTable games found in casinos include:\nGaming machines.\nGaming machines found in casinos include:\nRandom numbers games.\nRandom numbers games found in casinos include:\nHouse advantage.\nCasino games typically provide a predictable long-term advantage to the casino, or \"house\", while offering the players the possibility of a short-term gain that in some cases can be large. Some casino games have a skill element, where the players' decisions have an impact on the results. Players possessing sufficient skills to eliminate the inherent long-term disadvantage (the \"house edge\" or vigorish) in a casino game are referred to as advantage players.\nThe players' disadvantage is a result of the casino not paying winning wagers according to the game's \"true odds\", which are the payouts that would be expected considering the odds of a wager either winning or losing. For example, if a game is played by wagering on the number that would result from the roll of one die, the true odds would be 6 times the amount wagered since there is a 1 in 6 chance of any single number appearing, assuming that the player gets the original amount wagered back. However, the casino may only pay 4 times the amount wagered for a winning wager.\nThe house edge, or vigorish, is defined as the casino profit expressed as a percentage of the player's original bet. (In games such as blackjack or Spanish 21, the final bet may be several times the original bet, if the player doubles and splits.)\nIn American roulette, there are two \"zeroes\" (0, 00) and 36 non-zero numbers (18 red and 18 black). This leads to a higher house edge compared to European roulette. The chances of a player, who bets 1 unit on red, winning are 18/38 and his chances of losing 1 unit are 20/38. The player's expected value is EV = (18/38 \u00d7 1) + (20/38 \u00d7 (\u22121)) = 18/38 \u2212 20/38 = \u22122/38 = \u22125.26%. Therefore, the house edge is 5.26%. After 10 spins, betting 1 unit per spin, the average house profit will be 10 \u00d7 1 \u00d7 5.26% = 0.53 units. European roulette wheels have only one \"zero\" and therefore the house advantage (ignoring the en prison rule) is equal to 1/37 = 2.7%.\nThe house edge of casino games varies greatly with the game, with some games having an edge as low as 0.3%. Keno can have house edges of up to 25%, slot machines having up to\u00a015%.\nThe calculation of the roulette house edge is a trivial exercise; for other games, this is not usually the case. Combinatorial analysis and/or computer simulation is necessary to complete the task.\nIn games that have a skill element, such as blackjack or Spanish 21, the house edge is defined as the house advantage from optimal play (without the use of advanced techniques such as card counting), on the first hand of the shoe (the container that holds the cards). The set of optimal plays for all possible hands is known as \"basic strategy\" and is highly dependent on the specific rules and even the number of decks used.\nTraditionally, the majority of casinos have refused to reveal the house edge information for their slots games, and due to the unknown number of symbols and weightings of the reels, in most cases, it is much more difficult to calculate the house edge than in other casino games. However, due to some online properties revealing this information and some independent research conducted by Michael Shackleford in the offline sector, this pattern is slowly changing.\nIn games where players are not competing against the house, such as poker, the casino usually earns money via a commission, known as a \"rake\".\nStandard deviation.\nThe luck factor in a casino game is quantified using standard deviations (SD). The standard deviation of a simple game like roulette can be calculated using the binomial distribution. In the binomial distribution, SD = formula_1, where \"n\" = number of rounds played, \"p\" = probability of winning, and \"q\" = probability of losing. The binomial distribution assumes a result of 1 unit for a win, and 0 units for a loss, rather than \u22121 units for a loss, which doubles the range of possible outcomes. Furthermore, if we flat bet at 10 units per round instead of 1 unit, the range of possible outcomes increases 10 fold.\nFor example, after 10 rounds at 1 unit per round, the standard deviation will be 2 \u00d7 1 \u00d7 formula_3 = 3.16 units. After 10 rounds, the expected loss will be 10 \u00d7 1 \u00d7 5.26% = 0.53. As you can see, standard deviation is many times the magnitude of the expected loss.\nThe standard deviation for pai gow poker is the lowest out of all common casino games. Many casino games, particularly slot machines, have extremely high standard deviations. The bigger size of the potential payouts, the more the standard deviation may increase.\nAs the number of rounds increases, eventually, the expected loss will exceed the standard deviation, many times over. From the formula, we can see that the standard deviation is proportional to the square root of the number of rounds played, while the expected loss is proportional to the number of rounds played. As the number of rounds increases, the expected loss increases at a much faster rate. This is why it is impossible for a gambler to win in the long term. It is the high ratio of short-term standard deviation to expected loss that fools gamblers into thinking that they can win.\nIt is important for a casino to know both the house edge and variance for all of their games. The house edge tells them what kind of profit they will make as a percentage of turnover, and the variance tells them how much they need in the way of cash reserves. The mathematicians and computer programmers that do this kind of work are called gaming mathematicians and gaming analysts. Casinos do not have in-house expertise in this field, so they outsource their requirements to experts in the gaming analysis field."}
{"id": "5363", "revid": "45584915", "url": "https://en.wikipedia.org/wiki?curid=5363", "title": "Video game", "text": "A video game, sometimes further qualified as a computer game, is an electronic game that involves interaction with a user interface or input device (such as a joystick, controller, keyboard, or motion sensing device) to generate visual feedback from a display device, most commonly shown in a video format on a television set, computer monitor, flat-panel display or touchscreen on handheld devices, or a virtual reality headset. Most modern video games are audiovisual, with audio complement delivered through speakers or headphones, and sometimes also with other types of sensory feedback (e.g., haptic technology that provides tactile sensations). Some video games also allow microphone and webcam inputs for in-game chatting and livestreaming.\nVideo games are typically categorized according to their hardware platform, which traditionally includes arcade video games, console games, and computer games (which includes LAN games, online games, and browser games). More recently, the video game industry has expanded onto mobile gaming through mobile devices (such as smartphones and tablet computers), virtual and augmented reality systems, and remote cloud gaming. Video games are also classified into a wide range of genres based on their style of gameplay and target audience.\nThe first video game prototypes in the 1950s and 1960s were simple extensions of electronic games using video-like output from large, room-sized mainframe computers. The first consumer video game was the arcade video game \"Computer Space\" in 1971, which took inspiration from the earlier 1962 computer game \"Spacewar!\". In 1972 came the now-iconic video game \"Pong\" and the first home console, the Magnavox Odyssey. The industry grew quickly during the \"golden age\" of arcade video games from the late 1970s to early 1980s but suffered from the crash of the North American video game market in 1983 due to loss of publishing control and saturation of the market. Following the crash, the industry matured, was dominated by Japanese companies such as Nintendo, Sega, and Sony, and established practices and methods around the development and distribution of video games to prevent a similar crash in the future, many of which continue to be followed. In the 2000s, the core industry centered on \"AAA\" games, leaving little room for riskier experimental games. Coupled with the availability of the Internet and digital distribution, this gave room for independent video game development (or \"indie games\") to gain prominence into the 2010s. Since then, the commercial importance of the video game industry has been increasing. The emerging Asian markets and proliferation of smartphone games in particular are altering player demographics towards casual gaming and increasing monetization by incorporating games as a service.\nToday, video game development requires numerous skills, vision, teamwork, and liaisons between different parties, including developers, publishers, distributors, retailers, hardware manufacturers, and other marketers, to successfully bring a game to its consumers. , the global video game market had estimated annual revenues of across hardware, software, and services, which is three times the size of the global music industry and four times that of the film industry in 2019, making it a formidable heavyweight across the modern entertainment industry. The video game market is also a major influence behind the electronics industry, where personal computer component, console, and peripheral sales, as well as consumer demands for better game performance, have been powerful driving factors for hardware design and innovation.\nOrigins.\nEarly video games use interactive electronic devices with various display formats. The earliest example is from 1947\u2014a \"cathode-ray tube amusement device\" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992. Inspired by radar display technology, it consists of an analog device allowing a user to control the parabolic arc of a dot on the screen to simulate a missile being fired at targets, which are paper drawings fixed to the screen. Other early examples include Christopher Strachey's draughts game, the Nimrod computer at the 1951 Festival of Britain; \"OXO\", a tic-tac-toe computer game by Alexander S. Douglas for the EDSAC in 1952; \"Tennis for Two\", an electronic interactive game engineered by William Higinbotham in 1958; and \"Spacewar!\", written by Massachusetts Institute of Technology students Martin Graetz, Steve Russell, and Wayne Wiitanen's on a DEC PDP-1 computer in 1962. Each game has different means of display: NIMROD has a panel of lights to play the game of Nim, OXO has a graphical display to play tic-tac-toe, \"Tennis for Two\" has an oscilloscope to display a side view of a tennis court, and \"Spacewar!\" has the DEC PDP-1's vector display to have two spaceships battle each other.\nThese inventions laid the foundation for modern video games. In 1966, while working at Sanders Associates, Ralph H. Baer devised a system to play a basic table tennis game on a television screen. With the company's approval, Baer created the prototype known as the \"Brown Box\". Sanders patented Baer's innovations and licensed them to Magnavox, which commercialized the technology as the first home video game console, the Magnavox Odyssey, released in 1972. Separately, Nolan Bushnell and Ted Dabney, inspired by seeing \"Spacewar!\" running at Stanford University, devised a similar version running in a smaller coin-operated arcade cabinet using a less expensive computer. This was released as \"Computer Space\", the first arcade video game, in 1971. Bushnell and Dabney went on to form Atari, Inc., and with Allan Alcorn, created their second arcade game in 1972, the hit ping pong-style \"Pong\", which was directly inspired by the table tennis game on the Odyssey. Atari made a home version of \"Pong\", which was released by Christmas 1975. The success of the Odyssey and \"Pong\", both as an arcade game and home machine, launched the video game industry. Both Baer and Bushnell have been titled \"Father of Video Games\" for their contributions.\nTerminology.\nThe term \"video game\" was developed to distinguish this class of electronic games that were played on some type of video display rather than on a teletype printer, audio speaker, or similar device. This also distinguished from many handheld electronic games like \"Merlin\" which commonly used LED lights for indicators but did not use these in combination for imaging purposes.\n\"Computer game\" may also be used as a descriptor, as all these types of games essentially require the use of a computer processor, and in some cases, it is used interchangeably with \"video game\". Particularly in the United Kingdom and Western Europe, this is common due to the historic relevance of domestically produced microcomputers. Other terms used include digital game, for example, by the Australian Bureau of Statistics. However, the term \"computer game\" can also be used to more specifically refer to games played primarily on personal computers or other types of flexible hardware systems (also known as PC game), as a way to distinguish them from console games, arcade games, or mobile games. Other terms such as \"television game\", \"telegame\", or \"TV game\" had been used in the 1970s and early 1980s, particularly for home gaming consoles that rely on connection to a television set. However, these terms were also used interchangeably with \"video game\" in the 1970s, primarily due to \"video\" and \"television\" being synonymous. In Japan, where consoles like the Odyssey were first imported and then made within the country by the large television manufacturers such as Toshiba and Sharp Corporation, such games are known as \"TV games\", \"TV geemu\", or \"terebi geemu\". The term \"TV game\" is still commonly used into the 21st century. \"Electronic game\" may also be used to refer to video games, but this also incorporates devices like early handheld electronic games that lack any video output.\nThe first appearance of the term \"video game\" emerged around 1973. The Oxford English Dictionary cited a 10 November 1973 \"BusinessWeek\" article as the first printed use of the term. Though Bushnell believed the term came from a vending magazine review of \"Computer Space\" in 1971, a review of the major vending magazines \"Vending Times\" and \"Cashbox\" showed that the term may have come even earlier, appearing first in a letter dated July 10, 1972. In the letter, Bushnell uses the term \"video game\" twice. Per video game historian Keith Smith, the sudden appearance suggested that the term had been proposed and readily adopted by those in the field. Around March 1973, Ed Adlum, who ran \"Cashbox\"s coin-operated section until 1972 and then later founded \"RePlay Magazine\", covering the coin-op amusement field, in 1975, used the term in an article in March 1973. In a September 1982 issue of \"RePlay\", Adlum is credited with first naming these games as \"video games\": \"RePlay's Eddie Adlum worked at 'Cash Box' when 'TV games' first came out. The personalities in those days were Bushnell, his sales manager Pat Karns, and a handful of other 'TV game' manufacturers like Henry Leyser and the McEwan brothers. It seemed awkward to call their products 'TV games', so borrowing a word from \"Billboard\"s description of movie jukeboxes, Adlum started to refer to this new breed of amusement machine as 'video games.' The phrase stuck.\" Adlum explained in 1985 that up until the early 1970s, amusement arcades typically had non-video arcade games such as pinball machines and electro-mechanical games. With the arrival of video games in arcades during the early 1970s, there was initially some confusion in the arcade industry over what term should be used to describe the new games. He \"wrestled with descriptions of this type of game,\" alternating between \"TV game\" and \"television game\" but \"finally woke up one day\" and said, \"What the hell... video game!\"\nDefinition.\nWhile many games readily fall into a clear, well-understood definition of video games, new genres and innovations in game development have raised the question of what are the essential factors of a video game that separate the medium from other forms of entertainment.\nThe introduction of interactive films in the 1980s with games like \"Dragon's Lair\", featured games with full motion video played off a form of media but only limited user interaction. This had required a means to distinguish these games from more traditional board games that happen to also use external media, such as the \"Clue VCR Mystery Game\" which required players to watch VCR clips between turns. To distinguish between these two, video games are considered to require some interactivity that affects the visual display.\nMost video games tend to feature some type of victory or winning conditions, such as a scoring mechanism or a final boss fight. The introduction of walking simulators (adventure games that allow for exploration but lack any objectives) like \"Gone Home\", and empathy games (video games that tend to focus on emotion) like \"That Dragon, Cancer\" brought the idea of games that did not have any such type of winning condition and raising the question of whether these were actually games. These are still commonly justified as video games as they provide a game world that the player can interact with by some means.\nThe lack of any industry definition for a video game by 2021 was an issue during the case \"Epic Games v. Apple\" which dealt with video games offered on Apple's iOS App Store. Among concerns raised were games like \"Fortnite Creative\" and \"Roblox\" which created metaverses of interactive experiences, and whether the larger game and the individual experiences themselves were games or not in relation to fees that Apple charged for the App Store. Judge Yvonne Gonzalez Rogers, recognizing that there was yet an industry standard definition for a video game, established for her ruling that \"At a bare minimum, video games appear to require some level of interactivity or involvement between the player and the medium\" compared to passive entertainment like film, music, and television, and \"videogames are also generally graphically rendered or animated, as opposed to being recorded live or via motion capture as in films or television\". Rogers still concluded that what is a video game \"appears highly eclectic and diverse\".\nVideo game terminology.\nThe gameplay experience varies radically between video games, but many common elements exist. Most games will launch into a title screen and give the player a chance to review options such as the number of players before starting a game. Most games are divided into levels which the player must work the avatar through, scoring points, collecting power-ups to boost the avatar's innate attributes, all while either using special attacks to defeat enemies or moves to avoid them. This information is relayed to the player through a type of on-screen user interface such as a heads-up display atop the rendering of the game itself. Taking damage will deplete their avatar's health, and if that falls to zero or if the avatar otherwise falls into an impossible-to-escape location, the player will lose one of their lives. Should they lose all their lives without gaining an extra life or \"1-UP\", then the player will reach the \"game over\" screen. Many levels as well as the game's finale end with a type of boss character the player must defeat to continue on. In some games, intermediate points between levels will offer save points where the player can create a saved game on storage media to restart the game should they lose all their lives or need to stop the game and restart at a later time. These also may be in the form of a passage that can be written down and reentered at the title screen.\nProduct flaws include software bugs which can manifest as glitches which may be exploited by the player; this is often the foundation of speedrunning a video game. These bugs, along with cheat codes, Easter eggs, and other hidden secrets that were intentionally added to the game can also be exploited. On some consoles, cheat cartridges allow players to execute these cheat codes, and user-developed trainers allow similar bypassing for computer software games. Both of which might make the game easier, give the player additional power-ups, or change the appearance of the game.\nComponents.\nTo distinguish from electronic games, a video game is generally considered to require a platform, the hardware which contains computing elements, to process player interaction from some type of input device and displays the results to a video output display.\nPlatform.\nVideo games require a platform, a specific combination of electronic components or computer hardware and associated software, to operate. The term system is also commonly used. These platforms may include multiple brandsheld by platform holders, such as Nintendo or Sony, seeking to gain larger market shares. Games are typically designed to be played on one or a limited number of platforms, and exclusivity to a platform or brand is used by platform holders as a competitive edge in the video game market. However, games may be developed for alternative platforms than intended, which are described as ports or conversions. These also may be remasters - where most of the original game's source code is reused and art assets, models, and game levels are updated for modern systems \u2013 and remakes, where in addition to asset improvements, significant reworking of the original game and possibly from scratch is performed.\nThe list below is not exhaustive and excludes other electronic devices capable of playing video games such as PDAs and graphing calculators.\nGame media.\nEarly arcade games, home consoles, and handheld games were dedicated hardware units with the game's logic built into the electronic componentry of the hardware. Since then, most video game platforms are considered programmable, having means to read and play multiple games distributed on different types of media or formats. Physical formats include ROM cartridges, magnetic storage including magnetic-tape data storage and floppy discs, optical media formats including CD-ROM and DVDs, and flash memory cards. Furthermore digital distribution over the Internet or other communication methods as well as cloud gaming alleviate the need for any physical media. In some cases, the media serves as the direct read-only memory for the game, or it may be the form of installation media that is used to write the main assets to the player's platform's local storage for faster loading periods and later updates.\nGames can be extended with new content and software patches through either expansion packs which are typically available as physical media, or as downloadable content nominally available via digital distribution. These can be offered freely or can be used to monetize a game following its initial release. Several games offer players the ability to create user-generated content to share with others to play. Other games, mostly those on personal computers, can be extended with user-created modifications or mods that alter or add onto the game; these often are unofficial and were developed by players from reverse engineering of the game, but other games provide official support for modding the game.\nInput device.\nVideo game can use several types of input devices to translate human actions to a game. Most common are the use of game controllers like gamepads and joysticks for most consoles, and as accessories for personal computer systems along keyboard and mouse controls. Common controls on the most recent controllers include face buttons, shoulder triggers, analog sticks, and directional pads (\"d-pads\"). Consoles typically include standard controllers which are shipped or bundled with the console itself, while peripheral controllers are available as a separate purchase from the console manufacturer or third-party vendors. Similar control sets are built into handheld consoles and onto arcade cabinets. Newer technology improvements have incorporated additional technology into the controller or the game platform, such as touchscreens and motion detection sensors that give more options for how the player interacts with the game. Specialized controllers may be used for certain genres of games, including racing wheels, light guns and dance pads. Digital cameras and motion detection can capture movements of the player as input into the game, which can, in some cases, effectively eliminate the control, and on other systems such as virtual reality, are used to enhance immersion into the game.\nDisplay and output.\nBy definition, all video games are intended to output graphics to an external video display, such as cathode-ray tube televisions, newer liquid-crystal display (LCD) televisions and built-in screens, projectors or computer monitors, depending on the type of platform the game is played on. Features such as color depth, refresh rate, frame rate, and screen resolution are a combination of the limitations of the game platform and display device and the program efficiency of the game itself. The game's output can range from fixed displays using LED or LCD elements, text-based games, two-dimensional and three-dimensional graphics, and augmented reality displays.\nThe game's graphics are often accompanied by sound produced by internal speakers on the game platform or external speakers attached to the platform, as directed by the game's programming. This often will include sound effects tied to the player's actions to provide audio feedback, as well as background music for the game.\nSome platforms support additional feedback mechanics to the player that a game can take advantage of. This is most commonly haptic technology built into the game controller, such as causing the controller to shake in the player's hands to simulate a shaking earthquake occurring in game.\nClassifications.\nVideo games are frequently classified by a number of factors related to how one plays them.\nGenre.\nA video game, like most other forms of media, may be categorized into genres. However, unlike film or television which use visual or narrative elements, video games are generally categorized into genres based on their gameplay interaction, since this is the primary means which one interacts with a video game. The narrative setting does not impact gameplay; a shooter game is still a shooter game, regardless of whether it takes place in a fantasy world or in outer space. An exception is the horror game genre, used for games that are based on narrative elements of horror fiction, the supernatural, and psychological horror.\nGenre names are normally self-describing in terms of the type of gameplay, such as action game, role playing game, or shoot 'em up, though some genres have derivations from influential works that have defined that genre, such as roguelikes from \"Rogue\", Grand Theft Auto clones from \"Grand Theft Auto III\", and battle royale games from the film \"Battle Royale\". The names may shift over time as players, developers and the media come up with new terms; for example, first-person shooters were originally called \"Doom clones\" based on the 1993 game. A hierarchy of game genres exist, with top-level genres like \"shooter game\" and \"action game\" that broadly capture the game's main gameplay style, and several subgenres of specific implementation, such as within the shooter game first-person shooter and third-person shooter. Some cross-genre types also exist that fall until multiple top-level genres such as action-adventure game.\nMode.\nA video game's mode describes how many players can use the game at the same type. This is primarily distinguished by single-player video games and multiplayer video games. Within the latter category, multiplayer games can be played in a variety of ways, including locally at the same device, on separate devices connected through a local network such as LAN parties, or online via separate Internet connections. Most multiplayer games are based on competitive gameplay, but many offer cooperative and team-based options as well as asymmetric gameplay. Online games use server structures that can also enable massively multiplayer online games (MMOs) to support hundreds of players at the same time.\nA small number of video games are zero-player games, in which the player has very limited interaction with the game itself. These are most commonly simulation games where the player may establish a starting state and then let the game proceed on its own, watching the results as a passive observer, such as with many computerized simulations of Conway's Game of Life.\nTypes.\nMost video games are intended for entertainment purposes. Different game types include:\nContent rating.\nVideo games can be subject to national and international content rating requirements. Like with film content ratings, video game ratings typing identify the target age group that the national or regional ratings board believes is appropriate for the player, ranging from all-ages, to a teenager-or-older, to mature, to the infrequent adult-only games. Most content review is based on the level of violence, both in the type of violence and how graphic it may be represented, and sexual content, but other themes such as drug and alcohol use and gambling that can influence children may also be identified. A primary identifier based on a minimum age is used by nearly all systems, along with additional descriptors to identify specific content that players and parents should be aware of.\nThe regulations vary from country to country but generally are voluntary systems upheld by vendor practices, with penalty and fines issued by the ratings body on the video game publisher for misuse of the ratings. Among the major content rating systems include:\nAdditionally, the major content system provides have worked to create the International Age Rating Coalition (IARC), a means to streamline and align the content ratings system between different region, so that a publisher would only need to complete the content ratings review for one provider, and use the IARC transition to affirm the content rating for all other regions.\nCertain nations have even more restrictive rules related to political or ideological content. Within Germany, until 2018, the Unterhaltungssoftware Selbstkontrolle (\"Entertainment Software Self-Regulation\") would refuse to classify, and thus allow sale, of any game depicting Nazi imagery, and thus often requiring developers to replace such imagery with fictional ones. This ruling was relaxed in 2018 to allow for such imagery for \"social adequacy\" purposes that applied to other works of art. China's video game segment is mostly isolated from the rest of the world due to the government's censorship, and all games published there must adhere to strict government review, disallowing content such as smearing the image of the Chinese Communist Party. Foreign games published in China often require modification by developers and publishers to meet these requirements.\nDevelopment.\nVideo game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred to, primarily include programmers and graphic designers. Over the years, this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.\nIn the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the \"one-man shop\" is gone, as this is still sometimes found in the casual gaming and handheld markets, where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some PDAs).\nVideo games are programmed like any other piece of computer software. Prior to the mid-1970s, arcade and home consoles were programmed by assembling discrete electro-mechanical components on circuit boards, which limited games to relatively simple logic. By 1975, low-cost microprocessors were available at volume to be used for video game hardware, which allowed game developers to program more detailed games, widening the scope of what was possible. Ongoing improvements in computer hardware technology have expanded what has become possible to create in video games, coupled with convergence of common hardware between console, computer, and arcade platforms to simplify the development process. Today, game developers have a number of commercial and open source tools available for use to make games, often which are across multiple platforms to support portability, or may still opt to create their own for more specialized features and direct control of the game. Today, many games are built around a game engine that handles the bulk of the game's logic, gameplay, and rendering. These engines can be augmented with specialized engines for specific features, such as a physics engine that simulates the physics of objects in real-time. A variety of middleware exists to help developers access other features, such as playback of videos within games, network-oriented code for games that communicate via online services, matchmaking for online games, and similar features. These features can be used from a developer's programming language of choice, or they may opt to also use game development kits that minimize the amount of direct programming they have to do but can also limit the amount of customization they can add into a game. Like all software, video games usually undergo quality testing before release to assure there are no bugs or glitches in the product, though frequently developers will release patches and updates.\nWith the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need the best talent, while publishers reduce costs to maintain profitability on their investment. Typically, a video game console development team ranges from 5 to 50 people, and some exceed 100. In May 2009, \"Assassin's Creed II\" was reported to have a development staff of 450. The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games, and the release of unfinished products.\nWhile amateur and hobbyist game programming had existed since the late 1970s with the introduction of home computers, a newer trend since the mid-2000s is indie game development. Indie games are made by small teams outside any direct publisher control, their games being smaller in scope than those from the larger \"AAA\" game studios, and are often experiments in gameplay and art style. Indie game development is aided by the larger availability of digital distribution, including the newer mobile gaming market, and readily-available and low-cost development tools for these platforms.\nGame theory and studies.\nAlthough departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls \"Cyberdrama\". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from \"Star Trek\", arguing for the video game as a medium in which the player is allowed to become another person, and to act out in another world. This image of video games received early widespread popular support, and forms the basis of films such as \"Tron\", \"eXistenZ\" and \"The Last Starfighter\".\nLudologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game \"Tomb Raider\", saying that \"the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it.\" Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.\nWhile many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term \"emergent narrative\" has been used to describe how, in a simulated environment, storyline can be created simply by \"what happens to the player.\" However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game.\nIntellectual property for video games.\nMost commonly, video games are protected by copyright, though both patents and trademarks have been used as well.\nThough local copyright regulations vary to the degree of protection, video games qualify as copyrighted visual-audio works, and enjoy cross-country protection under the Berne Convention. This typically only applies to the underlying code, as well as to the artistic aspects of the game such as its writing, art assets, and music. Gameplay itself is generally not considered copyrightable; in the United States among other countries, video games are considered to fall into the idea\u2013expression distinction in that it is how the game is presented and expressed to the player that can be copyrighted, but not the underlying principles of the game.\nBecause gameplay is normally ineligible for copyright, gameplay ideas in popular games are often replicated and built upon in other games. At times, this repurposing of gameplay can be seen as beneficial and a fundamental part of how the industry has grown by building on the ideas of others. For example \"Doom\" (1993) and \"Grand Theft Auto III\" (2001) introduced gameplay that created popular new game genres, the first-person shooter and the \"Grand Theft Auto\" clone, respectively, in the few years after their release. However, at times and more frequently at the onset of the industry, developers would intentionally create video game clones of successful games and game hardware with few changes, which led to the flooded arcade and dedicated home console market around 1978. Cloning is also a major issue with countries that do not have strong intellectual property protection laws, such as within China. The lax oversight by China's government and the difficulty for foreign companies to take Chinese entities to court had enabled China to support a large grey market of cloned hardware and software systems. The industry remains challenged to distinguish between creating new games based on refinements of past successful games to create a new type of gameplay, and intentionally creating a clone of a game that may simply swap out art assets.\nIndustry.\nHistory.\nThe early history of the video game industry, following the first game hardware releases and through 1983, had little structure. Video games quickly took off during the golden age of arcade video games from the late 1970s to early 1980s, but the newfound industry was mainly composed of game developers with little business experience. This led to numerous companies forming simply to create clones of popular games to try to capitalize on the market. Due to loss of publishing control and oversaturation of the market, the North American home video game market crashed in 1983, dropping from revenues of around in 1983 to by 1985. Many of the North American companies created in the prior years closed down. Japan's growing game industry was briefly shocked by this crash but had sufficient longevity to withstand the short-term effects, and Nintendo helped to revitalize the industry with the release of the Nintendo Entertainment System in North America in 1985. Along with it, Nintendo established a number of core industrial practices to prevent unlicensed game development and control game distribution on their platform, methods that continue to be used by console manufacturers today.\nThe industry remained more conservative following the 1983 crash, forming around the concept of publisher-developer dichotomies, and by the 2000s, leading to the industry centralizing around low-risk, triple-A games and studios with large development budgets of at least or more. The advent of the Internet brought digital distribution as a viable means to distribute games, and contributed to the growth of more riskier, experimental independent game development as an alternative to triple-A games in the late 2000s and which has continued to grow as a significant portion of the video game industry.\nIndustry roles.\nVideo games have a large network effect that draw on many different sectors that tie into the larger video game industry. While video game developers are a significant portion of the industry, other key participants in the market include:\nMajor regional markets.\nThe industry itself grew out from both the United States and Japan in the 1970s and 1980s before having a larger worldwide contribution. Today, the video game industry is predominantly led by major companies in North America (primarily the United States and Canada), Europe, and southeast Asia including Japan, South Korea, and China. Hardware production remains an area dominated by Asian companies either directly involved in hardware design or part of the production process, but digital distribution and indie game development of the late 2000s has allowed game developers to flourish nearly anywhere and diversify the field.\nGame sales.\nAccording to the market research firm Newzoo, the global video game industry drew estimated revenues of over in 2020. Mobile games accounted for the bulk of this, with a 48% share of the market, followed by console games at 28% and personal computer games at 23%.\nSales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more handheld games than console games and especially PC games, with a strong preference for games catering to local tastes. Another key difference is that, though having declined in the West, arcade games remain an important sector of the Japanese gaming industry. In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China.\nEffects on society.\nCulture.\nVideo game culture is a worldwide new media subculture formed around video games and game playing. As computer and video games have increased in popularity over time, they have had a significant influence on popular culture. Video game culture has also evolved over time hand in hand with internet culture as well as the increasing popularity of mobile games. Many people who play video games identify as gamers, which can mean anything from someone who enjoys games to someone who is passionate about it. As video games become more social with multiplayer and online capability, gamers find themselves in growing social networks. Gaming can both be entertainment as well as competition, as a new trend known as electronic sports is becoming more widely accepted. In the 2010s, video games and discussions of video game trends and topics can be seen in social media, politics, television, film and music. The COVID-19 pandemic during 2020\u20132021 gave further visibility to video games as a pastime to enjoy with friends and family online as a means of social distancing.\nArt.\nSince the mid-2000s there has been debate whether video games qualify as art, primarily as the form's interactivity interfered with the artistic intent of the work and that they are designed for commercial appeal. A significant debate on the matter came after film critic Roger Ebert published an essay \"Video Games can never be art\", which challenged the industry to prove him and other critics wrong. The view that video games were an art form was cemented in 2011 when the U.S. Supreme Court ruled in the landmark case \"Brown v. Entertainment Merchants Association\" that video games were a protected form of speech with artistic merit. Since then, video game developers have come to use the form more for artistic expression, including the development of art games, and the cultural heritage of video games as works of arts, beyond their technical capabilities, have been part of major museum exhibits, including \"The Art of Video Games\" at the Smithsonian American Art Museum and toured at other museums from 2012 to 2016.\nVideo games will inspire sequels and other video games within the same franchise, but also have influenced works outside of the video game medium. Numerous television shows (both animated and live-action), films, comics and novels have been created based on existing video game franchises. Because video games are an interactive medium there has been trouble in converting them to these passive forms of media, and typically such works have been critically panned or treated as children's media. For example, until 2019, no video game film had ever been received a \"Fresh\" rating on Rotten Tomatoes, but the releases of \"Detective Pikachu\" (2019) and \"Sonic the Hedgehog\" (2020), both receiving \"Fresh\" ratings, shows signs of the film industry having found an approach to adapt video games for the large screen. That said, some early video game-based films have been highly successful at the box office, such as 1995's \"Mortal Kombat\" and 2001's \"\".\nMore recently since the 2000s, there has also become a larger appreciation of video game music, which ranges from chiptunes composed for limited sound-output devices on early computers and consoles, to fully-scored compositions for most modern games. Such music has frequently served as a platform for covers and remixes, and concerts featuring video game soundtracks performed by bands or orchestras, such as \"Video Games Live\", have also become popular. Video games also frequently incorporate licensed music, particularly in the area of rhythm games, furthering the depth of which video games and music can work together.\nFurther, video games can serve as a virtual environment under full control of a producer to create new works. With the capability to render 3D actors and settings in real-time, a new type of work machinima (short for \"machine cinema\") grew out from using video game engines to craft narratives. As video game engines gain higher fidelity, they have also become part of the tools used in more traditional filmmaking. Unreal Engine has been used as a backbone by Industrial Light &amp; Magic for their StageCraft technology for shows like \"The Mandalorian\".\nSeparately, video games are also frequently used as part of the promotion and marketing for other media, such as for films, anime, and comics. However, these licensed games in the 1990s and 2000s often had a reputation for poor quality, developed without any input from the intellectual property rights owners, and several of them are considered among lists of games with notably negative reception, such as \"Superman 64\". More recently, with these licensed games being developed by triple-A studios or through studios directly connected to the licensed property owner, there has been a significant improvement in the quality of these games, with an early trendsetting example of \"\".\nBeneficial uses.\nBesides their entertainment value, appropriately-designed video games have been seen to provide value in education across several ages and comprehension levels. Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system. It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits. Students are found to be \"learning by doing\" while playing video games while fostering creative thinking.\nVideo games are also believed to be beneficial to the mind and body. It has been shown that action video game players have better hand\u2013eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers. Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects. A 2018 systematic review found evidence that video gaming training had positive effects on cognitive and emotional skills in the adult population, especially with young adults. A 2019 systematic review also added support for the claim that video games are beneficial to the brain, although the beneficial effects of video gaming on the brain differed by video games types.\nOrganisers of video gaming events, such as the organisers of the \"D-Lux\" video game festival in Dumfries, Scotland, have emphasised the positive aspects video games can have on mental health. Organisers, mental health workers and mental health nurses at the event emphasised the relationships and friendships that can be built around video games and how playing games can help people learn about others as a precursor to discussing the person's mental health. A study in 2020 from Oxford University also suggested that playing video games can be a benefit to a person's mental health. The report of 3,274 gamers, all over the age of 18, focused on the games and and used actual play-time data. The report found that those that played more games tended to report greater \"wellbeing\". Also in 2020, computer science professor Regan Mandryk of the University of Saskatchewan said her research also showed that video games can have health benefits such as reducing stress and improving mental health. The university's research studied all age groups \u2013 \"from pre-literate children through to older adults living in long term care homes\" \u2013 with a main focus on 18 to 55-year-olds.\nA study of gamers attitudes towards gaming which was reported about in 2018 found that millennials use video games as a key strategy for coping with stress. In the study of 1,000 gamers, 55% said that it \"helps them to unwind and relieve stress ... and half said they see the value in gaming as a method of escapism to help them deal with daily work pressures\".\nControversies.\nVideo games have caused controversy since the 1970s. Parents and children's advocates regularly raise concerns that violent video games can influence young players into performing those violent acts in real life, and events such as the Columbine High School massacre in 1999 in which some claimed the perpetrators specifically alluded to using video games to plot out their attack, raised further fears. Medical experts and mental health professionals have also raised concerned that video games may be addictive, and the World Health Organization has included \"gaming disorder\" in the 11th revision of its International Statistical Classification of Diseases. Other health experts, including the American Psychiatric Association, have stated that there is insufficient evidence that video games can create violent tendencies or lead to addictive behavior, though agree that video games typically use a compulsion loop in their core design that can create dopamine that can help reinforce the desire to continue to play through that compulsion loop and potentially lead into violent or addictive behavior. Even with case law establishing that video games qualify as a protected art form, there has been pressure on the video game industry to keep their products in check to avoid over-excessive violence particularly for games aimed at younger children. The potential addictive behavior around games, coupled with increased used of post-sale monetization of video games, has also raised concern among parents, advocates, and government officials about gambling tendencies that may come from video games, such as controversy around the use of loot boxes in many high-profile games.\nNumerous other controversies around video games and its industry have arisen over the years, among the more notable incidents include the 1993 United States Congressional hearings on violent games like \"Mortal Kombat\" which lead to the formation of the ESRB ratings system, numerous legal actions taken by attorney Jack Thompson over violent games such as \"Grand Theft Auto III\" and \"Manhunt\" from 2003 to 2007, the outrage over the \"No Russian\" level from \"\" in 2009 which allowed the player to shoot a number of innocent non-player characters at an airport, and the Gamergate harassment campaign in 2014 that highlighted misogyny from a portion of the player demographic. The industry as a whole has also dealt with issues related to gender, racial, and LGBTQ+ discrimination and mischaracterization of these minority groups in video games. A further issue in the industry is related to working conditions, as development studios and publishers frequently use \"crunch time\", required extended working hours, in the weeks and months ahead of a game's release to assure on-time delivery.\nCollecting and preservation.\nPlayers of video games often maintain collections of games. More recently there has been interest in retrogaming, focusing on games from the first decades. Games in retail packaging in good shape have become collectors items for the early days of the industry, with some rare publications having gone for over . Separately, there is also concern about the preservation of video games, as both game media and the hardware to play them degrade over time. Further, many of the game developers and publishers from the first decades no longer exist, so records of their games have disappeared. Archivists and preservations have worked within the scope of copyright law to save these games as part of the cultural history of the industry.\nThere are many video game museums around the world, including the National Videogame Museum in Frisco, Texas, which serves as the largest museum wholly dedicated to the display and preservation of the industry's most important artifacts. Europe hosts video game museums such as the Computer Games Museum in Berlin and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg. The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games. The Video Game Museum of Rome is also dedicated to preserving video games and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a exhibit which allows guests to play their way through the history of video games. The Smithsonian Institution in Washington, DC has three video games on permanent display: \"Pac-Man\", \"Dragon's Lair\", and \"Pong\".\nThe Museum of Modern Art has added a total of 20 video games and one video game console to its permanent Architecture and Design Collection since 2012. In 2012, the Smithsonian American Art Museum ran an exhibition on \"The Art of Video Games\". However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum."}
{"id": "5365", "revid": "42316941", "url": "https://en.wikipedia.org/wiki?curid=5365", "title": "Christianity/Fish", "text": ""}
{"id": "5367", "revid": "28979433", "url": "https://en.wikipedia.org/wiki?curid=5367", "title": "Cambrian", "text": "The Cambrian ( ) is the first geological period of the Paleozoic Era, and the Phanerozoic Eon. The Cambrian lasted 51.95 million years from the end of the preceding Ediacaran period 538.8 Ma (million years ago) to the beginning of the Ordovician Period 486.85 Ma.\nMost of the continents lay in the southern hemisphere surrounded by the vast Panthalassa Ocean. The assembly of Gondwana during the Ediacaran and early Cambrian led to the development of new convergent plate boundaries and continental-margin arc magmatism along its margins that helped drive up global temperatures. Laurentia lay across the equator, separated from Gondwana by the opening Iapetus Ocean.\nThe Cambrian was a time of greenhouse climate conditions, with high levels of atmospheric carbon dioxide and low levels of oxygen in the atmosphere and seas. Upwellings of anoxic deep ocean waters into shallow marine environments led to extinction events, whilst periods of raised oxygenation led to increased biodiversity.\nThe Cambrian marked a profound change in life on Earth; prior to the Period, the majority of living organisms were small, unicellular and poorly preserved. Complex, multicellular organisms gradually became more common during the Ediacaran, but it was not until the Cambrian that the rapid diversification of lifeforms, known as the Cambrian explosion, produced the first representatives of most modern animal phyla. The Period is also unique in its unusually high proportion of lagerst\u00e4tte deposits, sites of exceptional preservation where \"soft\" parts of organisms are preserved as well as their more resistant shells.\nBy the end of the Cambrian, myriapods, arachnids, and hexapods started adapting to the land, along with the first plants.\nEtymology and history.\nThe term \"Cambrian\" is derived from the Latin version of \"Cymru\", the Welsh name for Wales, where rocks of this age were first studied. It was named by Adam Sedgwick in 1835, who divided it into three groups; the Lower, Middle, and Upper. He defined the boundary between the Cambrian and the overlying Silurian, together with Roderick Murchison, in their joint paper \"On the Silurian and Cambrian Systems, Exhibiting the Order in which the Older Sedimentary Strata Succeed each other in England and Wales\". This early agreement did not last.\nDue to the scarcity of fossils, Sedgwick used rock types to identify Cambrian strata. He was also slow in publishing further work. The clear fossil record of the Silurian, however, allowed Murchison to correlate rocks of a similar age across Europe and Russia, and on these he published extensively. As increasing numbers of fossils were identified in older rocks, he extended the base of the Silurian downwards into the Sedgwick's \"Upper Cambrian\", claiming all fossilised strata for \"his\" Silurian series. Matters were complicated further when, in 1852, fieldwork carried out by Sedgwick and others revealed an unconformity within the Silurian, with a clear difference in fauna between the two. This allowed Sedgwick to now claim a large section of the Silurian for \"his\" Cambrian and gave the Cambrian an identifiable fossil record. The dispute between the two geologists and their supporters, over the boundary between the Cambrian and Silurian, would extend beyond the life times of both Sedgwick and Murchison. It was not resolved until 1879, when Charles Lapworth proposed the disputed strata belong to its own system, which he named the Ordovician.\nThe term \"Cambrian\" for the oldest period of the Paleozoic was officially agreed in 1960, at the 21st International Geological Congress. It only includes Sedgwick's \"Lower Cambrian series\", but its base has been extended into much older rocks.\nGeology.\nStratigraphy.\nSystems, series and stages can be defined globally or regionally. For global stratigraphic correlation, the ICS ratify rock units based on a Global Boundary Stratotype Section and Point (GSSP) from a single formation (a stratotype) identifying the lower boundary of the unit. Currently the boundaries of the Cambrian System, three series and six stages are defined by global stratotype sections and points.\nEdiacaran-Cambrian boundary.\nThe lower boundary of the Cambrian was originally held to represent the first appearance of complex life, represented by trilobites. The recognition of small shelly fossils before the first trilobites, and Ediacara biota substantially earlier, has led to calls for a more precisely defined base to the Cambrian Period.\nDespite the long recognition of its distinction from younger Ordovician rocks and older Precambrian rocks, it was not until 1994 that the Cambrian system/period was internationally ratified. After decades of careful consideration, a continuous sedimentary sequence at Fortune Head, Newfoundland was settled upon as a formal base of the Cambrian Period, which was to be correlated worldwide by the earliest appearance of \"Treptichnus pedum\". Discovery of this fossil a few metres below the GSSP led to the refinement of this statement, and it is the \"T. pedum\" ichnofossil assemblage that is now formally used to correlate the base of the Cambrian.\nThis formal designation allowed radiometric dates to be obtained from samples across the globe that corresponded to the base of the Cambrian. An early date of 570 Ma quickly gained favour, though the methods used to obtain this number are now considered to be unsuitable and inaccurate. A more precise analysis using modern radiometric dating yields a date of 538.8 \u00b1 0.6 Ma. The ash horizon in Oman from which this date was recovered corresponds to a marked fall in the abundance of carbon-13 that correlates to equivalent excursions elsewhere in the world, and to the disappearance of distinctive Ediacaran fossils (\"Namacalathus\", \"Cloudina\"). Nevertheless, there are arguments that the dated horizon in Oman does not correspond to the Ediacaran-Cambrian boundary, but represents a facies change from marine to evaporite-dominated strata \u2013 which would mean that dates from other sections, ranging from 544 to 542 Ma, are more suitable.\nTerreneuvian.\nThe Terreneuvian is the lowermost series/epoch of the Cambrian, lasting from 538.8 \u00b1 0.6 Ma to c. 521 Ma. It is divided into two stages: the Fortunian stage, 538.8 \u00b1 0.6 Ma to c. 529 Ma; and the unnamed Stage 2, c. 529 Ma to c. 521 Ma. The name Terreneuvian was ratified by the International Union of Geological Sciences (IUGS) in 2007, replacing the previous \"Cambrian Series 1\". The GSSP defining its base is at Fortune Head on the Burin Peninsula, eastern Newfoundland, Canada (see Ediacaran - Cambrian boundary above). The Terreneuvian is the only series in the Cambrian to contain no trilobite fossils. Its lower part is characterised by complex, sediment-penetrating Phanerozoic-type trace fossils, and its upper part by small shelly fossils.\nCambrian Series 2.\nThe second series/epoch of the Cambrian is currently unnamed and known as Cambrian Series 2. It lasted from c. 521 Ma to c. 506.5 Ma. Its two stages are also unnamed and known as Cambrian Stage 3, c. 521 Ma to c. 514.5 Ma, and Cambrian Stage 4, c. 514.5 Ma to c. 506.5 Ma. The base of Series 2 does not yet have a GSSP, but it is expected to be defined in strata marking the first appearance of trilobites in Gondwana. There was a rapid diversification of metazoans during this epoch, but their restricted geographic distribution, particularly of the trilobites and archaeocyaths, have made global correlations difficult, hence ongoing efforts to establish a GSSP.\nMiaolingian.\nThe Miaolingian is the third series/epoch of the Cambrian, lasting from c. 506.5 Ma to c. 497 Ma, and roughly identical to the middle Cambrian in older literature. It is divided into three stages: the Wuliuan c. 506.5 Ma to 504.5 Ma; the Drumian c. 504.5 Ma to c. 500.5 Ma; and the Guzhangian c. 500.5 Ma to c. 497 Ma. The name replaces Cambrian Series 3 and was ratified by the IUGS in 2018. It is named after the Miaoling Mountains in southeastern Guizhou Province, South China, where the GSSP marking its base is found. This is defined by the first appearance of the oryctocephalid trilobite \"Oryctocephalus indicus\". Secondary markers for the base of the Miaolingian include the appearance of many acritarchs forms, a global marine transgression, and the disappearance of the polymerid trilobites, \"Bathynotus\" or \"Ovatoryctocara.\" Unlike the Terreneuvian and Series 2, all the stages of the Miaolingian are defined by GSSPs\".\"\nThe olenellids, eodiscids, and most redlichiids trilobites went extinct at the boundary between Series 2 and the Miaolingian. This is considered the oldest mass extinction of trilobites.\nFurongian.\nThe Furongian, c. 497 Ma to 486.85 \u00b1 1.5 Ma, is the fourth and uppermost series/epoch of the Cambrian. The name was ratified by the IUGS in 2003 and replaces Cambrian Series 4 and the traditional \"Upper Cambrian\". The GSSP for the base of the Furongian is in the Wuling Mountains, in northwestern Hunan Province, China. It coincides with the first appearance of the agnostoid trilobite \"Glyptagnostus reticulatus\", and is near the beginning of a large positive \u03b413C isotopic excursion.\nThe Furongian is divided into three stages: the Paibian, c. 497 Ma to c. 494 Ma, and the Jiangshanian c. 494.2 Ma to c. 491 Ma, which have defined GSSPs; and the unnamed Cambrian Stage 10, c. 491 Ma to 486.85 \u00b1 1.5 Ma.\nCambrian\u2013Ordovician boundary.\nThe GSSP for the Cambrian\u2013Ordovician boundary is at Green Point, western Newfoundland, Canada, and is dated at 486.85 Ma. It is defined by the appearance of the conodont \"Iapetognathus fluctivagus\". Where these conodonts are not found the appearance of planktonic graptolites or the trilobite \"Jujuyaspis borealis\" can be used. The boundary also corresponds with the peak of the largest positive variation in the \u03b413C curve during the boundary time interval and with a global marine transgression.\nImpact structures.\nMajor meteorite impact structures include: the early Cambrian (c. 535 Ma) Neugrund crater in the Gulf of Finland, Estonia, a complex meteorite crater about 20\u00a0km in diameter, with two inner ridges of about 7\u00a0km and 6\u00a0km diameter, and an outer ridge of 8\u00a0km that formed as the result of an impact of an asteroid 1\u00a0km in diameter; the 5\u00a0km diameter Gardnos crater (500\u00b110 Ma) in Buskerud, Norway, where post-impact sediments indicate the impact occurred in a shallow marine environment with rock avalanches and debris flows occurring as the crater rim was breached not long after impact; the 24\u00a0km diameter Presqu'ile crater (500 Ma or younger) Quebec, Canada; the 19\u00a0km diameter Glikson crater (c. 508 Ma) in Western Australia; the 5\u00a0km diameter Mizarai crater (500\u00b110 Ma) in Lithuania; and the 3.2\u00a0km diameter Newporte structure (c. 500 Ma or slightly younger) in North Dakota, U.S.A.\nPaleogeography.\nReconstructing the position of the continents during the Cambrian is based on palaeomagnetic, palaeobiogeographic, tectonic, geological and palaeoclimatic data. However, these have different levels of uncertainty and can produce contradictory locations for the major continents. This, together with the ongoing debate around the existence of the Neoproterozoic supercontinent of Pannotia, means that while most models agree the continents lay in the southern hemisphere, with the vast Panthalassa Ocean covering most of northern hemisphere, the exact distribution and timing of the movements of the Cambrian continents varies between models.\nMost models show Gondwana stretching from the south polar region to north of the equator. Early in the Cambrian, the south pole corresponded with the western South American sector and as Gondwana rotated anti-clockwise, by the middle of the Cambrian, the south pole lay in the northwest African region.\nLaurentia lay across the equator, separated from Gondwana by the Iapetus Ocean. Proponents of Pannotia have Laurentia and Baltica close to the Amazonia region of Gondwana with a narrow Iapetus Ocean that only began to open once Gondwana was fully assembled c. 520 Ma. Those not in favour of the existence of Pannotia show the Iapetus opening during the Late Neoproterozoic, with up to c. 6,500\u00a0km (c. 4038 miles) between Laurentia and West Gondwana at the beginning of the Cambrian.\nOf the smaller continents, Baltica lay between Laurentia and Gondwana, the Ran Ocean (an arm of the Iapetus) opening between it and Gondwana. Siberia lay close to the western margin of Gondwana and to the north of Baltica. Annamia and South China formed a single continent situated off north central Gondwana. The location of North China is unclear. It may have lain along the northeast Indian sector of Gondwana or already have been a separate continent.\nLaurentia.\nDuring the Cambrian, Laurentia lay across or close to the equator. \u00a0It drifted south and rotated c. 20\u00b0 anticlockwise during the middle Cambrian, before drifting north again in the late Cambrian.\nAfter the Late Neoproterozoic (or mid-Cambrian) rifting of Laurentia from Gondwana and the subsequent opening of the Iapetus Ocean, Laurentia was largely surrounded by passive margins with much of the continent covered by shallow seas.\nAs Laurentia separated from Gondwana, a sliver of continental terrane rifted from Laurentia with the narrow Taconic seaway opening between them. The remains of this terrane are now found in southern Scotland, Ireland, and Newfoundland. Intra-oceanic subduction either to the southeast of this terrane in the Iapetus, or to its northwest in the Taconic seaway, resulted in the formation of an island arc. This accreted to the terrane in the late Cambrian, triggering southeast-dipping subduction beneath the terrane itself and consequent closure of the marginal seaway. The terrane collided with Laurentia in the Early Ordovician.\nTowards the end of the early Cambrian, rifting along Laurentia's southeastern margin led to the separation of Cuyania (now part of Argentina) from the Ouachita embayment with a new ocean established that continued to widen through the Cambrian and Early Ordovician.\nGondwana.\nGondwana was a massive continent, three times the size of any of the other Cambrian continents. Its continental land area extended from the south pole to north of the equator. Around it were extensive shallow seas and numerous smaller land areas.\nThe cratons that formed Gondwana came together during the Neoproterozoic to early Cambrian. A narrow ocean separated Amazonia from Gondwana until c. 530 Ma and the Arequipa-Antofalla block united with the South American sector of Gondwana in the early Cambrian. The Kuunga Orogeny between northern (Congo Craton, Madagascar and India) and southern Gondwana (Kalahari Craton and East Antarctica), which began c. 570 Ma, continued with parts of northern Gondwana over-riding southern Gondwana and was accompanied by metamorphism and the intrusion of granites.\nSubduction zones, active since the Neoproterozoic, extended around much of Gondwana's margins, from northwest Africa southwards round South America, South Africa, East Antarctica, and the eastern edge of West Australia. Shorter subduction zones existed north of Arabia and India.\nThe Famatinian continental arc stretched from central Peru in the north to central Argentina in the south. Subduction beneath this proto-Andean margin began by the late Cambrian.\nAlong the northern margin of Gondwana, between northern Africa and the Armorican Terranes of southern Europe, the continental arc of the Cadomian Orogeny continued from the Neoproterozoic in response to the oblique subduction of the Iapetus Ocean. This subduction extended west along the Gondwanan margin and by c. 530 Ma may have evolved into a major transform fault system.\nAt c. 511 Ma the continental flood basalts of the Kalkarindji large igneous province (LIP) began to erupt. These covered an area of &gt; 2.1 \u00d7 106 km2 across northern, central and Western Australia regions of Gondwana making it one of the largest, as well as the earliest, LIPs of the Phanerozoic. The timing of the eruptions suggests they played a role in the early to middle Cambrian mass extinction.\nGanderia, East and West Avalonia, Carolinia and Meguma Terranes.\nThe terranes of Ganderia, East and West Avalonia, Carolinia and Meguma lay in polar regions during the early Cambrian, and high-to-mid southern latitudes by the mid to late Cambrian. They are commonly shown as an island arc-transform fault system along the northwestern margin of Gondwana north of northwest Africa and Amazonia, which rifted from Gondwana during the Ordovician. However, some models show these terranes as part of a single independent microcontinent, Greater Avalonia, lying to the west of Baltica and aligned with its eastern (Timanide) margin, with the Iapetus to the north and the Ran Ocean to the south.\nBaltica.\nDuring the Cambrian, Baltica rotated more than 60\u00b0 anti-clockwise and began to drift northwards. This rotation was accommodated by major strike-slip movements in the Ran Ocean between it and Gondwana.\nBaltica lay at mid-to-high southerly latitudes, separated from Laurentia by the Iapetus and from Gondwana by the Ran Ocean. It was composed of two continents, Fennoscandia and Sarmatia, separated by shallow seas. The sediments deposited in these unconformably overlay Precambrian basement rocks. The lack of coarse-grained sediments indicates low lying topography across the centre of the craton.\nAlong Baltica's northeastern margin subduction and arc magmatism associated with the Ediacaran Timanian Orogeny was coming to an end. In this region the early to middle Cambrian was a time of non-deposition and followed by late Cambrian rifting and sedimentation.\nIts southeastern margin was also a convergent boundary, with the accretion of island arcs and microcontinents to the craton, although the details are unclear.\nSiberia.\nSiberia began the Cambrian close to western Gondwana and north of Baltica. It drifted northwestwards to close to the equator as the \u00c6gir Ocean opened between it and Baltica. Much of the continent was covered by shallow seas with extensive archaeocyathan reefs. The then northern third of the continent (present day south; Siberia has rotated 180\u00b0 since the Cambrian) adjacent to its convergent margin was mountainous.\nFrom the Late Neoproterozoic to the Ordovician, a series of island arcs accreted to Siberia's then northeastern margin, accompanied by extensive arc and back-arc volcanism. These now form the Altai-Sayan terranes. Some models show a convergent plate margin extending from Greater Avalonia, through the Timanide margin of Baltica, forming the Kipchak island arc offshore of southeastern Siberia and curving round to become part of the Altai-Sayan convergent margin.\nAlong the then western margin, Late Neoproterozoic to early Cambrian rifting was followed by the development of a passive margin.\nTo the then north, Siberia was separated from the Central Mongolian terrane by the narrow and slowly opening Mongol-Okhotsk Ocean. The Central Mongolian terrane's northern margin with the Panthalassa was convergent, whilst its southern margin facing the Mongol-Okhotsk Ocean was passive.\nCentral Asia.\nDuring the Cambrian, the terranes that would form Kazakhstania later in the Paleozoic were a series of island arc and accretionary complexes that lay along an intra-oceanic convergent plate margin to the south of North China.\nTo the south of these the Tarim microcontinent lay between Gondwana and Siberia. Its northern margin was passive for much of the Paleozoic, with thick sequences of platform carbonates and fluvial to marine sediments resting unconformably on Precambrian basement. Along its southeast margin was the Altyn Cambro\u2013Ordovician accretionary complex, whilst to the southwest a subduction zone was closing the narrow seaway between the North West Kunlun region of Tarim and the South West Kunlun terrane.\nNorth China.\nNorth China lay at equatorial to tropical latitudes during the early Cambrian, although its exact position is unknown. Much of the craton was covered by shallow seas, with land in the northwest and southeast.\nNorthern North China was a passive margin until the onset of subduction and the development of the Bainaimiao arc in the late Cambrian. To its south was a convergent margin with a southwest dipping subduction zone, beyond which lay the North Qinling terrane (now part of the Qinling Orogenic Belt).\nSouth China and Annamia.\nSouth China and Annamia formed a single continent. Strike-slip movement between it and Gondwana accommodated its steady drift northwards from offshore the Indian sector of Gondwana to near the western Australian sector. This northward drift is evidenced by the progressive increase in limestones and increasing faunal diversity.\nThe northern margin South China, including the South Qinling block, was a passive margin.\nAlong the southeastern margin, lower Cambrian volcanics indicate the accretion of an island arc along the Song Ma suture zone. Also, early in the Cambrian, the eastern margin of South China changed from passive to active, with the development of oceanic volcanic island arcs that now form part of the Japanese terrane.\nClimate.\nThe distribution of climate-indicating sediments, including the wide latitudinal distribution of tropical carbonate platforms, archaeocyathan reefs and bauxites, and arid zone evaporites and calcrete deposits, show the Cambrian was a time of greenhouse climate conditions. During the late Cambrian the distribution of trilobite provinces also indicate only a moderate pole-to-equator temperature gradient. There is evidence of glaciation at high latitudes on Avalonia. However, it is unclear whether these sediments are early Cambrian or actually late Neoproterozoic in age.\nCalculations of global average temperatures (GAT) vary depending on which techniques are used. Whilst some measurements show GAT over c. models that combine multiple sources give GAT of c. in the Terreneuvian increasing to c. for the rest of the Cambrian. The warm climate was linked to elevated atmospheric carbon dioxide levels. Assembly of Gondwana led to the reorganisation of the tectonic plates with the development of new convergent plate margins and continental-margin arc magmatism that helped drive climatic warming. The eruptions of the Kalkarindji LIP basalts during Stage 4 and into the early Miaolingian, also released large quantities of carbon dioxide, methane and sulphur dioxide into the atmosphere leading to rapid climatic changes and elevated sea surface temperatures.\nThere is uncertainty around the maximum sea surface temperatures. These are calculated using \u03b418O values from marine rocks, and there is an ongoing debate about the levels \u03b418O in Cambrian seawater relative to the rest of the Phanerozoic. Estimates for tropical sea surface temperatures vary from c. , to c. . Modern average tropical sea surface temperatures are .\nAtmospheric oxygen levels rose steadily rising from the Neoproterozoic due to the increase in photosynthesising organisms. Cambrian levels varied between c. 3% and 14% (present day levels are c. 21%). Low levels of atmospheric oxygen and the warm climate resulted in lower dissolved oxygen concentrations in marine waters and widespread anoxia in deep ocean waters.\nThere is a complex relationship between oxygen levels, the biogeochemistry of ocean waters, and the evolution of life. Newly evolved burrowing organisms exposed anoxic sediments to the overlying oxygenated seawater. This bioturbation decreased the burial rates of organic carbon and sulphur, which over time reduced atmospheric and oceanic oxygen levels, leading to widespread anoxic conditions. Periods of higher rates of continental weathering led to increased delivery of nutrients to the oceans, boosting productivity of phytoplankton and stimulating metazoan evolution. However, rapid increases in nutrient supply led to eutrophication, where rapid growth in phytoplankton numbers result in the depletion of oxygen in the surrounding waters.\nPulses of increased oxygen levels are linked to increased biodiversity; raised oxygen levels supported the increasing metabolic demands of organisms, and increased ecological niches by expanding habitable areas of seafloor. Conversely, incursions of oxygen-deficient water, due to changes in sea level, ocean circulation, upwellings from deeper waters and/or biological productivity, produced anoxic conditions that limited habitable areas, reduced ecological niches and resulted in extinction events both regional and global.\nOverall, these dynamic, fluctuating environments, with global and regional anoxic incursions resulting in extinction events, and periods of increased oceanic oxygenation stimulating biodiversity, drove evolutionary innovation.\nGeochemistry.\nDuring the Cambrian, variations in isotope ratios were more frequent and more pronounced than later in the Phanerozoic, with at least 10 carbon isotope (\u03b413C) excursions (significant variations in global isotope ratios) recognised. These excursions record changes in the biogeochemistry of the oceans and atmosphere, which are due to processes such as the global rates of continental arc magmatism, rates of weathering and nutrients levels entering the marine environment, sea level changes, and biological factors including the impact of burrowing fauna on oxygen levels.\nIsotope excursions.\nBase of Cambrian.\nThe basal Cambrian \u03b413C excursion (BACE), together with low \u03b4238U and raised \u03b434S indicates a period of widespread shallow marine anoxia, which occurs at the same time as the extinction off the Ediacaran acritarchs. It was followed by the rapid appearance and diversification of bilaterian animals.\nCambrian Stages 2 and 3.\nDuring the early Cambrian, 87Sr/86Sr rose in response to enhanced continental weathering. This increased the input of nutrients into the oceans and led to higher burial rates of organic matter. Over long timescales, the extra oxygen released by organic carbon burial is balanced by a decrease in the rates of pyrite (FeS2) burial (a process which also releases oxygen), leading to stable levels of oxygen in the atmosphere. However, during the early Cambrian, a series of linked \u03b413C and \u03b434S excursions indicate high burial rates of both organic carbon and pyrite in biologically productive yet anoxic ocean floor waters. The oxygen-rich waters produced by these processes spread from the deep ocean into shallow marine environments, extending the habitable regions of the seafloor. These pulses of oxygen are associated with the radiation of the small shelly fossils and the Cambrian arthropod radiation isotope excursion (CARE). The increase in oxygenated waters in the deep ocean ultimately reduced the levels of organic carbon and pyrite burial, leading to a decrease in oxygen production and the re-establishment of anoxic conditions. This cycle was repeated several times during the early Cambrian.\nCambrian Stage 4 to early Miaolingian.\nThe beginning of the eruptions of the Kalkarindji LIP basalts during Stage 4 and the early Miaolingian released large quantities of carbon dioxide, methane and sulphur dioxide into the atmosphere. The changes these wrought are reflected by three large and rapid \u03b413C excursions. Increased temperatures led to a global sea level rise that flooded continental shelves and interiors with anoxic waters from the deeper ocean and drowned carbonate platforms of archaeocyathan reefs, resulting in the widespread accumulation of black organic-rich shales. Known as the Sinsk anoxic extinction event, this triggered the first major extinction of the Phanerozoic, the 513 \u2013 508 Ma Botoman-Toyonian Extinction (BTE), which included the loss of the archaeocyathids and hyoliths and saw a major drop in biodiversity. The rise in sea levels is also evidenced by a global decrease in 87Sr/86Sr. The flooding of continental areas decreased the rates of continental weathering, reducing the input of 87Sr to the oceans and lowering the 87Sr/86Sr of seawater.\nThe base of the Miaolingian is marked by the Redlichiid\u2013Olenellid extinction carbon isotope event (ROECE), which coincides with the main phase of Kalkarindji volcanism.\nDuring the Miaolingian, orogenic events along the Australian-Antarctic margin of Gondwana led to an increase in weathering and an influx of nutrients into the ocean, raising the level of productivity and organic carbon burial. These can be seen in the steady increase in 87Sr/86Sr and \u03b413C.\nEarly Furongian.\nContinued erosion of the deeper levels of the Gondwanan mountain belts led to a peak in 87Sr/86Sr and linked positive \u03b413C and \u03b434S excursions, known as the Steptoean positive carbon isotope excursion (SPICE). This indicates similar geochemical conditions to Stages 2 and 3 of the early Cambrian existed, with the expansion of seafloor anoxia enhancing the burial rates of organic matter and pyrite. This increase in the extent of anoxic seafloor conditions led to the extinction of the marjumiid and damesellid trilobites, whilst the increase in oxygen levels that followed helped drive the radiation of plankton.\n87Sr/86Sr fell sharply near the top of the Jiangshanian Stage, and through Stage 10 as the Gondwanan mountains were eroded down and rates of weathering decreased.\nMagnesium/calcium isotope ratios in seawater.\nThe mineralogy of inorganic marine carbonates has varied through the Phanerozoic, controlled by the Mg2+/Ca2+ values of seawater. High Mg2+/Ca2+ result in calcium carbonate precipitation dominated by aragonite and high-magnesium calcite, known as aragonite seas, and low ratios result in calcite seas where low-magnesium calcite is the primary calcium carbonate precipitate. The shells and skeletons of biomineralising organisms reflect the dominant form of calcite.\nDuring the late Ediacaran to early Cambrian increasing oxygen levels led to a decrease in ocean acidity and an increase in the concentration of calcium in sea water. However, there was not a simple transition from aragonite to calcite seas, rather a protracted and variable change through the Cambrian. Aragonite and high-magnesium precipitation continued from the Ediacaran into Cambrian Stage 2. Low-magnesium calcite skeletal hard parts appear in Cambrian Age 2, but inorganic precipitation of aragonite also occurred at this time. Mixed aragonite\u2013calcite seas continued through the middle and late Cambrian, with fully calcite seas not established until the early Ordovician.\nThese variations and slow decrease in Mg2+/Ca2+ of seawater were due to low oxygen levels, high continental weathering rates and the geochemistry of the Cambrian seas. In conditions of low oxygen and high iron levels, iron substitutes for magnesium in authigenic clay minerals deposited on the ocean floor, slowing the removal rates of magnesium from seawater. The enrichment of ocean waters in silica, prior to the radiation of siliceous organisms, and the limited bioturbation of the anoxic ocean floor increased the rates of deposition, relative to the rest of the Phanerozoic, of these clays. This, together with the high input of magnesium into the oceans via enhanced continental weathering, delayed the reduction in Mg2+/Ca2+ and facilitated continued aragonite precipitation.\nThe conditions that favoured the deposition of authigenic clays were also ideal for the formation of \"lagerst\u00e4tten\", with the minerals in the clays replacing the soft body parts of Cambrian organisms.\nFlora.\nThe Cambrian flora was little different from the Ediacaran. The principal taxa were the marine macroalgae \"Fuxianospira\", \"Sinocylindra\", and \"Marpolia\". No calcareous macroalgae are known from the period.\nNo land plant (embryophyte) fossils are known from the Cambrian. However, biofilms and microbial mats were well developed on Cambrian tidal flats and beaches 500 mya, and microbes forming microbial Earth ecosystems, comparable with modern soil crust of desert regions, contributing to soil formation. Although molecular clock estimates suggest terrestrial plants may have first emerged during the Middle or Late Cambrian, the consequent large-scale removal of the greenhouse gas CO2 from the atmosphere through sequestration did not begin until the Ordovician.\nOceanic life.\nThe Cambrian explosion was a period of rapid multicellular growth. Most animal life during the Cambrian was aquatic. Trilobites were once assumed to be the dominant life form at that time, but this has proven to be incorrect. Arthropods were by far the most dominant animals in the ocean, but trilobites were only a minor part of the total arthropod diversity. What made them so apparently abundant was their heavy armor reinforced by calcium carbonate (CaCO3), which fossilized far more easily than the fragile chitinous exoskeletons of other arthropods, leaving numerous preserved remains.\nThe period marked a steep change in the diversity and composition of Earth's biosphere. The Ediacaran biota suffered a mass extinction at the start of the Cambrian Period, which corresponded with an increase in the abundance and complexity of burrowing behaviour. This behaviour had a profound and irreversible effect on the substrate which transformed the seabed ecosystems. Before the Cambrian, the sea floor was covered by microbial mats. By the end of the Cambrian, burrowing animals had destroyed the mats in many areas through bioturbation. As a consequence, many of those organisms that were dependent on the mats became extinct, while the other species adapted to the changed environment that now offered new ecological niches. Around the same time there was a seemingly rapid appearance of representatives of all the mineralized phyla, including the Bryozoa, which were once thought to have only appeared in the Lower Ordovician. However, many of those phyla were represented only by stem-group forms; and since mineralized phyla generally have a benthic origin, they may not be a good proxy for (more abundant) non-mineralized phyla.\nWhile the early Cambrian showed such diversification that it has been named the Cambrian Explosion, this changed later in the period, when there occurred a sharp drop in biodiversity. About 515 Ma, the number of species going extinct exceeded the number of new species appearing. Five million years later, the number of genera had dropped from an earlier peak of about 600 to just 450. Also, the speciation rate in many groups was reduced to between a fifth and a third of previous levels. 500 Ma, oxygen levels fell dramatically in the oceans, leading to hypoxia, while the level of poisonous hydrogen sulfide simultaneously increased, causing another extinction. The later half of Cambrian was surprisingly barren and showed evidence of several rapid extinction events; the stromatolites which had been replaced by reef building sponges known as Archaeocyatha, returned once more as the archaeocyathids became extinct. This declining trend did not change until the Great Ordovician Biodiversification Event.\nSome Cambrian organisms ventured onto land, producing the trace fossils \"Protichnites\" and \"Climactichnites\". Fossil evidence suggests that euthycarcinoids, an extinct group of arthropods, produced at least some of the \"Protichnites\". Fossils of the track-maker of \"Climactichnites\" have not been found; however, fossil trackways and resting traces suggest a large, slug-like mollusc.\nIn contrast to later periods, the Cambrian fauna was somewhat restricted; free-floating organisms were rare, with the majority living on or close to the sea floor; and mineralizing animals were rarer than in future periods, in part due to the unfavourable ocean chemistry.\nMany modes of preservation are unique to the Cambrian, and some preserve soft body parts, resulting in an abundance of . These include Sirius Passet, the Sinsk Algal Lens, the Maotianshan Shales, the Emu Bay Shale, and the Burgess Shale.\nSymbol.\nThe United States Federal Geographic Data Committee uses a \"barred capital C\" character to represent the Cambrian Period.\nThe Unicode character is ."}
{"id": "5369", "revid": "1179321356", "url": "https://en.wikipedia.org/wiki?curid=5369", "title": "Contraception", "text": ""}
{"id": "5370", "revid": "1246516266", "url": "https://en.wikipedia.org/wiki?curid=5370", "title": "Theory of categories", "text": "In ontology, the theory of categories concerns itself with the \"categories of being\": the highest \"genera\" or \"kinds of entities\". To investigate the categories of being, or simply categories, is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction. Various systems of categories have been proposed, they often include categories for substances, properties, relations, states of affairs or events. A representative question within the theory of categories might articulate itself, for example, in a query like, \"Are universals prior to particulars?\"\nEarly development.\nThe process of abstraction required to discover the number and names of the categories of being has been undertaken by many philosophers since Aristotle and involves the careful inspection of each concept to ensure that there is no higher category or categories under which that concept could be subsumed. The scholars of the twelfth and thirteenth centuries developed Aristotle's ideas. For example, Gilbert of Poitiers divides Aristotle's ten categories into two sets, primary and secondary, according to whether they inhere in the subject or not:\nFurthermore, following Porphyry\u2019s likening of the classificatory hierarchy to a tree, they concluded that the major classes could be subdivided to form subclasses, for example, Substance could be divided into Genus and Species, and Quality could be subdivided into Property and Accident, depending on whether the property was necessary or contingent. An alternative line of development was taken by Plotinus in the second century who by a process of abstraction reduced Aristotle's list of ten categories to five: Substance, Relation, Quantity, Motion and Quality. Plotinus further suggested that the latter three categories of his list, namely Quantity, Motion and Quality correspond to three different kinds of relation and that these three categories could therefore be subsumed under the category of Relation. This was to lead to the supposition that there were only two categories at the top of the hierarchical tree, namely Substance and Relation. Many supposed that relations only exist in the mind. Substance and Relation, then, are closely commutative with Matter and Mind--this is expressed most clearly in the dualism of Ren\u00e9 Descartes.\nAristotle.\nOne of Aristotle\u2019s early interests lay in the classification of the natural world, how for example the genus \"animal\" could be first divided into \"two-footed animal\" and then into \"wingless, two-footed animal\". He realised that the distinctions were being made according to the qualities the animal possesses, the quantity of its parts and the kind of motion that it exhibits. To fully complete the proposition \"this animal is\u00a0...\" Aristotle stated in his work on the Categories that there were ten kinds of predicate where\u00a0...\n\"...\u00a0each signifies either substance or quantity or quality or relation or where or when or being-in-a-position or having or acting or being acted upon\".\nHe realised that predicates could be simple or complex. The simple kinds consist of a subject and a predicate linked together by the \"categorical\" or inherent type of relation. For Aristotle the more complex kinds were limited to propositions where the predicate is compounded of two of the above categories for example \"this is a horse running\". More complex kinds of proposition were only discovered after Aristotle by the Stoic, Chrysippus, who developed the \"hypothetical\" and \"disjunctive\" types of syllogism and these were terms which were to be developed through the Middle Ages and were to reappear in Kant's system of categories.\n\"Category\" came into use with Aristotle's essay \"Categories\", in which he discussed univocal and equivocal terms, predication, and ten categories:\nPlotinus.\nPlotinus in writing his \"Enneads\" around AD 250 recorded that \"Philosophy at a very early age investigated the number and character of the existents\u00a0... some found ten, others less\u00a0... to some the genera were the first principles, to others only a generic classification of existents.\" He realised that some categories were reducible to others saying \"Why are not Beauty, Goodness and the virtues, Knowledge and Intelligence included among the primary genera?\" He concluded that such transcendental categories and even the categories of Aristotle were in some way posterior to the three Eleatic categories first recorded in Plato's dialogue \"Parmenides\" and which comprised the following three coupled terms: \nPlotinus called these \"the hearth of reality\" deriving from them not only the three categories of Quantity, Motion and Quality but also what came to be known as \"the three moments of the Neoplatonic world process\":\nPlotinus likened the three to the centre, the radii and the circumference of a circle, and clearly thought that the principles underlying the categories were the first principles of creation. \"From a single root all being multiplies.\" Similar ideas were to be introduced into Early Christian thought by, for example, Gregory of Nazianzus who summed it up saying \"Therefore, Unity, having from all eternity arrived by motion at duality, came to rest in Trinity.\"\nModern development.\nKant and Hegel accused the Aristotelian table of categories of being \", derived arbitrarily and in bulk from experience, without any systematic necessity.\nThe early modern dualism, which has been described above, of Mind and Matter or Subject and Relation, as reflected in the writings of Descartes underwent a substantial revision in the late 18th century. The first objections to this stance were formulated in the eighteenth century by Immanuel Kant who realised that we can say nothing about Substance except through the relation of the subject to other things. \nFor example: In the sentence \"This is a house\" the substantive subject \"house\" only gains meaning in relation to human use patterns or to other similar houses. The category of Substance disappears from Kant's tables, and under the heading of Relation, Kant lists \"inter alia\" the three relationship types of Disjunction, Causality and Inherence. The three older concepts of Quantity, Motion and Quality, as Peirce discovered, could be subsumed under these three broader headings in that Quantity relates to the subject through the relation of Disjunction; Motion relates to the subject through the relation of Causality; and Quality relates to the subject through the relation of Inherence. Sets of three continued to play an important part in the nineteenth century development of the categories, most notably in G.W.F. Hegel's extensive tabulation of categories, and in C.S. Peirce's categories set out in his work on the logic of relations. One of Peirce's contributions was to call the three primary categories Firstness, Secondness and Thirdness which both emphasises their general nature, and avoids the confusion of having the same name for both the category itself and for a concept within that category.\nIn a separate development, and building on the notion of primary and secondary categories introduced by the Scholastics, Kant introduced the idea that secondary or \"derivative\" categories could be derived from the primary categories through the combination of one primary category with another. This would result in the formation of three secondary categories: the first, \"Community\" was an example that Kant gave of such a derivative category; the second, \"Modality\", introduced by Kant, was a term which Hegel, in developing Kant's dialectical method, showed could also be seen as a derivative category; and the third, \"Spirit\" or \"Will\" were terms that Hegel and Schopenhauer were developing separately for use in their own systems. Karl Jaspers in the twentieth century, in his development of existential categories, brought the three together, allowing for differences in terminology, as Substantiality, Communication and Will. This pattern of three primary and three secondary categories was used most notably in the nineteenth century by Peter Mark Roget to form the six headings of his Thesaurus of English Words and Phrases. The headings used were the three objective categories of Abstract Relation, Space (including Motion) and Matter and the three subjective categories of Intellect, Feeling and Volition, and he found that under these six headings all the words of the English language, and hence any possible predicate, could be assembled.\nKant.\nIn the \"Critique of Pure Reason\" (1781), Immanuel Kant argued that the categories are part of our own mental structure and consist of a set of \"a priori\" concepts through which we interpret the world around us. These concepts correspond to twelve logical functions of the understanding which we use to make judgements and there are therefore two tables given in the \"Critique\", one of the Judgements and a corresponding one for the Categories. To give an example, the logical function behind our reasoning from ground to consequence (based on the Hypothetical relation) underlies our understanding of the world in terms of cause and effect (the Causal relation). In each table the number twelve arises from, firstly, an initial division into two: the Mathematical and the Dynamical; a second division of each of these headings into a further two: Quantity and Quality, and Relation and Modality respectively; and, thirdly, each of these then divides into a further three subheadings as follows.\nTable of Judgements\nMathematical\nDynamical\nTable of Categories\nMathematical\nDynamical\nCriticism of Kant's system followed, firstly, by Arthur Schopenhauer, who amongst other things was unhappy with the term \"Community\", and declared that the tables \"do open violence to truth, treating it as nature was treated by old-fashioned gardeners\", and secondly, by W.T.Stace who in his book \"The Philosophy of Hegel\" suggested that in order to make Kant's structure completely symmetrical a third category would need to be added to the Mathematical and the Dynamical. This, he said, Hegel was to do with his category of concept.\nHegel.\nG.W.F. Hegel in his \"Science of Logic\" (1812) attempted to provide a more comprehensive system of categories than Kant and developed a structure that was almost entirely triadic. So important were the categories to Hegel that he claimed the first principle of the world, which he called the \"absolute\", is \"a system of categories\u00a0 the categories must be the reason of which the world is a consequent\".\nUsing his own logical method of sublation, later called the Hegelian dialectic, reasoning from the abstract through the negative to the concrete, he arrived at a hierarchy of some 270 categories, as explained by W.\u00a0T.\u00a0Stace. The three very highest categories were \"logic\", \"nature\" and \"spirit\". The three highest categories of \"logic\", however, he called \"being\", \"essence\", and \"notion\" which he explained as follows:\nSchopenhauer's category that corresponded with \"notion\" was that of \"idea\", which in his \"Four-Fold Root of Sufficient Reason\" he complemented with the category of the \"will\". The title of his major work was \"The World as Will and Idea\". The two other complementary categories, reflecting one of Hegel's initial divisions, were those of Being and Becoming. At around the same time, Goethe was developing his colour theories in the of 1810, and introduced similar principles of combination and complementation, symbolising, for Goethe, \"the primordial relations which belong both to nature and vision\". Hegel in his \"Science of Logic\" accordingly asks us to see his system not as a tree but as a circle.\nTwentieth-century development.\nIn the twentieth century the primacy of the division between the subjective and the objective, or between mind and matter, was disputed by, among others, Bertrand Russell and Gilbert Ryle. Philosophy began to move away from the metaphysics of categorisation towards the linguistic problem of trying to differentiate between, and define, the words being used. Ludwig Wittgenstein\u2019s conclusion was that there were no clear definitions which we can give to words and categories but only a \"halo\" or \"corona\" of related meanings radiating around each term. Gilbert Ryle thought the problem could be seen in terms of dealing with \"a galaxy of ideas\" rather than a single idea, and suggested that category mistakes are made when a concept (e.g. \"university\"), understood as falling under one category (e.g. abstract idea), is used as though it falls under another (e.g. physical object). With regard to the visual analogies being used, Peirce and Lewis, just like Plotinus earlier, likened the terms of propositions to points, and the relations between the terms to lines. Peirce, taking this further, talked of univalent, bivalent and trivalent relations linking predicates to their subject and it is just the number and types of relation linking subject and predicate that determine the category into which a predicate might fall. Primary categories contain concepts where there is one dominant kind of relation to the subject. Secondary categories contain concepts where there are two dominant kinds of relation. Examples of the latter were given by Heidegger in his two propositions \"the house is on the creek\" where the two dominant relations are spatial location (Disjunction) and cultural association (Inherence), and \"the house is eighteenth century\" where the two relations are temporal location (Causality) and cultural quality (Inherence). A third example may be inferred from Kant in the proposition \"the house is impressive or sublime\" where the two relations are spatial or mathematical disposition (Disjunction) and dynamic or motive power (Causality). Both Peirce and Wittgenstein introduced the analogy of colour theory in order to illustrate the shades of meanings of words. Primary categories, like primary colours, are analytical representing the furthest we can go in terms of analysis and abstraction and include Quantity, Motion and Quality. Secondary categories, like secondary colours, are synthetic and include concepts such as Substance, Community and Spirit. \nApart from these, the categorial scheme of Alfred North Whitehead and his Process Philosophy, alongside Nicolai Hartmann and his Critical Realism, remain one of the most detailed and advanced systems in categorial research in metaphysics.\nPeirce.\nCharles Sanders Peirce, who had read Kant and Hegel closely, and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: Firstness, Secondness, and Thirdness, which he repeatedly invoked in his subsequent writings. Like Hegel, C.S. Peirce attempted to develop a system of categories from a single indisputable principle, in Peirce's case the notion that in the first instance he could only be aware of his own ideas. \n \"It seems that the true categories of consciousness are first, feeling\u00a0... second, a sense of resistance\u00a0... and third, synthetic consciousness, or thought\".\n Elsewhere he called the three primary categories: Quality, Reaction and Meaning, and even Firstness, Secondness and Thirdness, saying, \"perhaps it is not right to call these categories conceptions, they are so intangible that they are rather tones or tints upon conceptions\": \nAlthough Peirce's three categories correspond to the three concepts of relation given in Kant's tables, the sequence is now reversed and follows that given by Hegel, and indeed before Hegel of the three moments of the world-process given by Plotinus. Later, Peirce gave a mathematical reason for there being three categories in that although monadic, dyadic and triadic nodes are irreducible, every node of a higher valency is reducible to a \"compound of triadic relations\". Ferdinand de Saussure, who was developing \"semiology\" in France just as Peirce was developing \"semiotics\" in the US, likened each term of a proposition to \"the centre of a constellation, the point where other coordinate terms, the sum of which is indefinite, converge\".\nOthers.\nEdmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.\nFor Gilbert Ryle (1949), a category (in particular a \"category mistake\") is an important semantic concept, but one having only loose affinities to an ontological category.\nContemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956\u201365), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006)."}
{"id": "5371", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=5371", "title": "Concrete", "text": "Concrete is a composite material composed of aggregate bonded together with a fluid cement that cures to a solid over time. Concrete is the second-most-used substance in the world after water, and is the most widely used building material. Its usage worldwide, ton for ton, is twice that of steel, wood, plastics, and aluminium combined.\nWhen aggregate is mixed with dry Portland cement and water, the mixture forms a fluid slurry that is easily poured and molded into shape. The cement reacts with the water through a process called concrete hydration that hardens it over several hours to form a hard matrix that binds the materials together into a durable stone-like material that has many uses. This time allows concrete to not only be cast in forms, but also to have a variety of tooled processes performed. The hydration process is exothermic, which means ambient temperature plays a significant role in how long it takes concrete to set. Often, additives (such as pozzolans or superplasticizers) are included in the mixture to improve the physical properties of the wet mix, delay or accelerate the curing time, or otherwise change the finished material. Most concrete is poured with reinforcing materials (such as steel rebar) embedded to provide tensile strength, yielding reinforced concrete.\nIn the past, lime-based cement binders, such as lime putty, were often used but sometimes with other hydraulic cements, (water resistant) such as a calcium aluminate cement or with Portland cement to form Portland cement concrete (named for its visual resemblance to Portland stone). Many other non-cementitious types of concrete exist with other methods of binding aggregate together, including asphalt concrete with a bitumen binder, which is frequently used for road surfaces, and polymer concretes that use polymers as a binder. Concrete is distinct from mortar. Whereas concrete is itself a building material, mortar is a bonding agent that typically holds bricks, tiles and other masonry units together. Grout is another material associated with concrete and cement. It does not contain coarse aggregates and is usually either pourable or thixotropic, and is used to fill gaps between masonry components or coarse aggregate which has already been put in place. Some methods of concrete manufacture and repair involve pumping grout into the gaps to make up a solid mass \"in situ\".\nEtymology.\nThe word concrete comes from the Latin word \" (meaning compact or condensed), the perfect passive participle of \", from \"-\" (together) and \"\" (to grow).\nHistory.\nAncient times.\nConcrete floors were found in the royal palace of Tiryns, Greece, which dates roughly to 1400 to 1200 BC. Lime mortars were used in Greece, such as in Crete and Cyprus, in 800 BC. The Assyrian Jerwan Aqueduct (688 BC) made use of waterproof concrete. Concrete was used for construction in many ancient structures.\nMayan concrete at the ruins of Uxmal (AD 850\u2013925) is referenced in \"Incidents of Travel in the Yucat\u00e1n\" by John L. Stephens. \"The roof is flat and had been covered with cement\". \"The floors were cement, in some places hard, but, by long exposure, broken, and now crumbling under the feet.\" \"But throughout the wall was solid, and consisting of large stones imbedded in mortar, almost as hard as rock.\"\nSmall-scale production of concrete-like materials was pioneered by the Nabatean traders who occupied and controlled a series of oases and developed a small empire in the regions of southern Syria and northern Jordan from the 4th century BC. They discovered the advantages of hydraulic lime, with some self-cementing properties, by 700 BC. They built kilns to supply mortar for the construction of rubble masonry houses, concrete floors, and underground waterproof cisterns. They kept the cisterns secret as these enabled the Nabataeans to thrive in the desert. Some of these structures survive to this day.\nIn the Ancient Egyptian and later Roman eras, builders discovered that adding volcanic ash to lime allowed the mix to set underwater. They discovered the pozzolanic reaction.\nClassical era.\nThe Romans used concrete extensively from 300 BC to AD 476. During the Roman Empire, Roman concrete (or \"opus caementicium\") was made from quicklime, pozzolana and an aggregate of pumice. Its widespread use in many Roman structures, a key event in the history of architecture termed the Roman architectural revolution, freed Roman construction from the restrictions of stone and brick materials. It enabled revolutionary new designs in terms of both structural complexity and dimension. The Colosseum in Rome was built largely of concrete, and the Pantheon has the world's largest unreinforced concrete dome.\nConcrete, as the Romans knew it, was a new and revolutionary material. Laid in the shape of arches, vaults and domes, it quickly hardened into a rigid mass, free from many of the internal thrusts and strains that troubled the builders of similar structures in stone or brick.\nModern tests show that \"opus caementicium\" had as much compressive strength as modern Portland-cement concrete (c. ). However, due to the absence of reinforcement, its tensile strength was far lower than modern reinforced concrete, and its mode of application also differed:\nModern structural concrete differs from Roman concrete in two important details. First, its mix consistency is fluid and homogeneous, allowing it to be poured into forms rather than requiring hand-layering together with the placement of aggregate, which, in Roman practice, often consisted of rubble. Second, integral reinforcing steel gives modern concrete assemblies great strength in tension, whereas Roman concrete could depend only upon the strength of the concrete bonding to resist tension.\nThe long-term durability of Roman concrete structures has been found to be due to its use of pyroclastic (volcanic) rock and ash, whereby the crystallization of str\u00e4tlingite (a specific and complex calcium aluminosilicate hydrate) and the coalescence of this and similar calcium\u2013aluminium-silicate\u2013hydrate cementing binders helped give the concrete a greater degree of fracture resistance even in seismically active environments. Roman concrete is significantly more resistant to erosion by seawater than modern concrete; it used pyroclastic materials which react with seawater to form Al-tobermorite crystals over time. The use of hot mixing and the presence of lime clasts are thought to give the concrete a self-healing ability, where cracks that form become filled with calcite that prevents the crack from spreading.\nThe widespread use of concrete in many Roman structures ensured that many survive to the present day. The Baths of Caracalla in Rome are just one example. Many Roman aqueducts and bridges, such as the magnificent Pont du Gard in southern France, have masonry cladding on a concrete core, as does the dome of the Pantheon.\nMiddle Ages.\nAfter the Roman Empire, the use of burned lime and pozzolana was greatly reduced. Low kiln temperatures in the burning of lime, lack of pozzolana, and poor mixing all contributed to a decline in the quality of concrete and mortar. From the 11th century, the increased use of stone in church and castle construction led to an increased demand for mortar. Quality began to improve in the 12th century through better grinding and sieving. Medieval lime mortars and concretes were non-hydraulic and were used for binding masonry, \"hearting\" (binding rubble masonry cores) and foundations. Bartholomaeus Anglicus in his \"De proprietatibus rerum\" (1240) describes the making of mortar. In an English translation from 1397, it reads \"lyme ... is a stone brent; by medlynge thereof with sonde and water sement is made\". From the 14th century, the quality of mortar was again excellent, but only from the 17th century was pozzolana commonly added.\nThe \"Canal du Midi\" was built using concrete in 1670.\nIndustrial era.\nPerhaps the greatest step forward in the modern use of concrete was Smeaton's Tower, built by British engineer John Smeaton in Devon, England, between 1756 and 1759. This third Eddystone Lighthouse pioneered the use of hydraulic lime in concrete, using pebbles and powdered brick as aggregate.\nA method for producing Portland cement was developed in England and patented by Joseph Aspdin in 1824. Aspdin chose the name for its similarity to Portland stone, which was quarried on the Isle of Portland in Dorset, England. His son William continued developments into the 1840s, earning him recognition for the development of \"modern\" Portland cement.\nReinforced concrete was invented in 1849 by Joseph Monier. and the first reinforced concrete house was built by Fran\u00e7ois Coignet in 1853.\nThe first concrete reinforced bridge was designed and built by Joseph Monier in 1875.\nPrestressed concrete and post-tensioned concrete were pioneered by Eug\u00e8ne Freyssinet, a French structural and civil engineer. Concrete components or structures are compressed by tendon cables during, or after, their fabrication in order to strengthen them against tensile forces developing when put in service. Freyssinet patented the technique on 2 October 1928.\nComposition.\nConcrete is an artificial composite material, comprising a matrix of cementitious binder (typically Portland cement paste or asphalt) and a dispersed phase or \"filler\" of aggregate (typically a rocky material, loose stones, and sand). The binder \"glues\" the filler together to form a synthetic conglomerate. Many types of concrete are available, determined by the formulations of binders and the types of aggregate used to suit the application of the engineered material. These variables determine strength and density, as well as chemical and thermal resistance of the finished product.\nConstruction aggregates consist of large chunks of material in a concrete mix, generally a coarse gravel or crushed rocks such as limestone, or granite, along with finer materials such as sand.\nCement paste, most commonly made of Portland cement, is the most prevalent kind of concrete binder. For cementitious binders, water is mixed with the dry cement powder and aggregate, which produces a semi-liquid slurry (paste) that can be shaped, typically by pouring it into a form. The concrete solidifies and hardens through a chemical process called hydration. The water reacts with the cement, which bonds the other components together, creating a robust, stone-like material. Other cementitious materials, such as fly ash and slag cement, are sometimes added\u2014either pre-blended with the cement or directly as a concrete component\u2014and become a part of the binder for the aggregate. Fly ash and slag can enhance some properties of concrete such as fresh properties and durability. Alternatively, other materials can also be used as a concrete binder: the most prevalent substitute is asphalt, which is used as the binder in asphalt concrete.\nAdmixtures are added to modify the cure rate or properties of the material. Mineral admixtures use recycled materials as concrete ingredients. Conspicuous materials include fly ash, a by-product of coal-fired power plants; ground granulated blast furnace slag, a by-product of steelmaking; and silica fume, a by-product of industrial electric arc furnaces.\nStructures employing Portland cement concrete usually include steel reinforcement because this type of concrete can be formulated with high compressive strength, but always has lower tensile strength. Therefore, it is usually reinforced with materials that are strong in tension, typically steel rebar.\nThe \"mix design\" depends on the type of structure being built, how the concrete is mixed and delivered, and how it is placed to form the structure.\nCement.\nPortland cement is the most common type of cement in general usage. It is a basic ingredient of concrete, mortar, and many plasters. It consists of a mixture of calcium silicates (alite, belite), aluminates and ferrites\u2014compounds, which will react with water. Portland cement and similar materials are made by heating limestone (a source of calcium) with clay or shale (a source of silicon, aluminium and iron) and grinding this product (called \"clinker\") with a source of sulfate (most commonly gypsum).\nCement kilns are extremely large, complex, and inherently dusty industrial installations. Of the various ingredients used to produce a given quantity of concrete, the cement is the most energetically expensive. Even complex and efficient kilns require 3.3 to 3.6 gigajoules of energy to produce a ton of clinker and then grind it into cement. Many kilns can be fueled with difficult-to-dispose-of wastes, the most common being used tires. The extremely high temperatures and long periods of time at those temperatures allows cement kilns to efficiently and completely burn even difficult-to-use fuels. The five major compounds of calcium silicates and aluminates comprising Portland cement range from 5 to 50% in weight.\nCuring.\nCombining water with a cementitious material forms a cement paste by the process of hydration. The cement paste glues the aggregate together, fills voids within it, and makes it flow more freely.\nAs stated by Abrams' law, a lower water-to-cement ratio yields a stronger, more durable concrete, whereas more water gives a freer-flowing concrete with a higher slump. The hydration of cement involves many concurrent reactions. The process involves polymerization, the interlinking of the silicates and aluminate components as well as their bonding to sand and gravel particles to form a solid mass. One illustrative conversion is the hydration of tricalcium silicate:\nThe hydration (curing) of cement is irreversible.\nAggregates.\nFine and coarse aggregates make up the bulk of a concrete mixture. Sand, natural gravel, and crushed stone are used mainly for this purpose. Recycled aggregates (from construction, demolition, and excavation waste) are increasingly used as partial replacements for natural aggregates, while a number of manufactured aggregates, including air-cooled blast furnace slag and bottom ash are also permitted.\nThe size distribution of the aggregate determines how much binder is required. Aggregate with a very even size distribution has the biggest gaps whereas adding aggregate with smaller particles tends to fill these gaps. The binder must fill the gaps between the aggregate as well as paste the surfaces of the aggregate together, and is typically the most expensive component. Thus, variation in sizes of the aggregate reduces the cost of concrete. The aggregate is nearly always stronger than the binder, so its use does not negatively affect the strength of the concrete.\nRedistribution of aggregates after compaction often creates non-homogeneity due to the influence of vibration. This can lead to strength gradients.\nDecorative stones such as quartzite, small river stones or crushed glass are sometimes added to the surface of concrete for a decorative \"exposed aggregate\" finish, popular among landscape designers.\nAdmixtures.\nAdmixtures are materials in the form of powder or fluids that are added to the concrete to give it certain characteristics not obtainable with plain concrete mixes. Admixtures are defined as additions \"made as the concrete mix is being prepared\". The most common admixtures are retarders and accelerators. In normal use, admixture dosages are less than 5% by mass of cement and are added to the concrete at the time of batching/mixing. (See below.) The common types of admixtures are as follows:\nMineral admixtures and blended cements.\nInorganic materials that have pozzolanic or latent hydraulic properties, these very fine-grained materials are added to the concrete mix to improve the properties of concrete (mineral admixtures), or as a replacement for Portland cement (blended cements). Products which incorporate limestone, fly ash, blast furnace slag, and other useful materials with pozzolanic properties into the mix, are being tested and used. These developments are ever growing in relevance to minimize the impacts caused by cement use, notorious for being one of the largest producers (at about 5 to 10%) of global greenhouse gas emissions. The use of alternative materials also is capable of lowering costs, improving concrete properties, and recycling wastes, the latest being relevant for circular economy aspects of the construction industry, whose demand is ever growing with greater impacts on raw material extraction, waste generation and landfill practices.\nProduction.\nConcrete production is the process of mixing together the various ingredients\u2014water, aggregate, cement, and any additives\u2014to produce concrete. Concrete production is time-sensitive. Once the ingredients are mixed, workers must put the concrete in place before it hardens. In modern usage, most concrete production takes place in a large type of industrial facility called a concrete plant, or often a batch plant. The usual method of placement is casting in formwork, which holds the mix in shape until it has set enough to hold its shape unaided.\nConcrete plants come in two main types, ready-mix plants and central mix plants. A ready-mix plant blends all of the solid ingredients, while a central mix does the same but adds water. A central-mix plant offers more precise control of the concrete quality. Central mix plants must be close to the work site where the concrete will be used, since hydration begins at the plant.\nA concrete plant consists of large hoppers for storage of various ingredients like cement, storage for bulk ingredients like aggregate and water, mechanisms for the addition of various additives and amendments, machinery to accurately weigh, move, and mix some or all of those ingredients, and facilities to dispense the mixed concrete, often to a concrete mixer truck.\nModern concrete is usually prepared as a viscous fluid, so that it may be poured into forms. The forms are containers that define the desired shape. Concrete formwork can be prepared in several ways, such as slip forming and steel plate construction. Alternatively, concrete can be mixed into dryer, non-fluid forms and used in factory settings to manufacture precast concrete products.\nInterruption in pouring the concrete can cause the initially placed material to begin to set before the next batch is added on top. This creates a horizontal plane of weakness called a \"cold joint\" between the two batches. Once the mix is where it should be, the curing process must be controlled to ensure that the concrete attains the desired attributes. During concrete preparation, various technical details may affect the quality and nature of the product.\nDesign mix.\n\"Design mix\" ratios are decided by an engineer after analyzing the properties of the specific ingredients being used. Instead of using a 'nominal mix' of 1 part cement, 2 parts sand, and 4 parts aggregate, a civil engineer will custom-design a concrete mix to exactly meet the requirements of the site and conditions, setting material ratios and often designing an admixture package to fine-tune the properties or increase the performance envelope of the mix. Design-mix concrete can have very broad specifications that cannot be met with more basic nominal mixes, but the involvement of the engineer often increases the cost of the concrete mix.\nConcrete mixes are primarily divided into nominal mix, standard mix and design mix.\nNominal mix ratios are given in volume of formula_1. Nominal mixes are a simple, fast way of getting a basic idea of the properties of the finished concrete without having to perform testing in advance.\nVarious governing bodies (such as British Standards) define nominal mix ratios into a number of grades, usually ranging from lower compressive strength to higher compressive strength. The grades usually indicate the 28-day cure strength.\nMixing.\nThorough mixing is essential to produce uniform, high-quality concrete.\n has shown that the mixing of cement and water into a paste before combining these materials with aggregates can increase the compressive strength of the resulting concrete. The paste is generally mixed in a , shear-type mixer at a w/c (water to cement ratio) of 0.30 to 0.45 by mass. The cement paste premix may include admixtures such as accelerators or retarders, superplasticizers, pigments, or silica fume. The premixed paste is then blended with aggregates and any remaining batch water and final mixing is completed in conventional concrete mixing equipment.\nSample analysis\u2014workability.\nWorkability is the ability of a fresh (plastic) concrete mix to fill the form/mold properly with the desired work (pouring, pumping, spreading, tamping, vibration) and without reducing the concrete's quality. Workability depends on water content, aggregate (shape and size distribution), cementitious content and age (level of hydration) and can be modified by adding chemical admixtures, like superplasticizer. Raising the water content or adding chemical admixtures increases concrete workability. Excessive water leads to increased bleeding or segregation of aggregates (when the cement and aggregates start to separate), with the resulting concrete having reduced quality. Changes in gradation can also affect workability of the concrete, although a wide range of gradation can be used for various applications. An undesirable gradation can mean using a large aggregate that is too large for the size of the formwork, or which has too few smaller aggregate grades to serve to fill the gaps between the larger grades, or using too little or too much sand for the same reason, or using too little water, or too much cement, or even using jagged crushed stone instead of smoother round aggregate such as pebbles. Any combination of these factors and others may result in a mix which is too harsh, i.e., which does not flow or spread out smoothly, is difficult to get into the formwork, and which is difficult to surface finish.\nWorkability can be measured by the concrete slump test, a simple measure of the plasticity of a fresh batch of concrete following the ASTM C 143 or EN 12350-2 test standards. Slump is normally measured by filling an \"Abrams cone\" with a sample from a fresh batch of concrete. The cone is placed with the wide end down onto a level, non-absorptive surface. It is then filled in three layers of equal volume, with each layer being tamped with a steel rod to consolidate the layer. When the cone is carefully lifted off, the enclosed material slumps a certain amount, owing to gravity. A relatively dry sample slumps very little, having a slump value of one or two inches (25 or 50\u00a0mm) out of . A relatively wet concrete sample may slump as much as eight inches. Workability can also be measured by the flow table test.\nSlump can be increased by addition of chemical admixtures such as plasticizer or superplasticizer without changing the water-cement ratio. Some other admixtures, especially air-entraining admixture, can increase the slump of a mix.\nHigh-flow concrete, like self-consolidating concrete, is tested by other flow-measuring methods. One of these methods includes placing the cone on the narrow end and observing how the mix flows through the cone while it is gradually lifted.\nAfter mixing, concrete is a fluid and can be pumped to the location where needed.\nCuring.\nMaintaining optimal conditions for cement hydration.\nConcrete must be kept moist during curing in order to achieve optimal strength and durability. During curing hydration occurs, allowing calcium-silicate hydrate (C-S-H) to form. Over 90% of a mix's final strength is typically reached within four weeks, with the remaining 10% achieved over years or even decades. The conversion of calcium hydroxide in the concrete into calcium carbonate from absorption of CO2 over several decades further strengthens the concrete and makes it more resistant to damage. This carbonation reaction, however, lowers the pH of the cement pore solution and can corrode the reinforcement bars.\nHydration and hardening of concrete during the first three days is critical. Abnormally fast drying and shrinkage due to factors such as evaporation from wind during placement may lead to increased tensile stresses at a time when it has not yet gained sufficient strength, resulting in greater shrinkage cracking. The early strength of the concrete can be increased if it is kept damp during the curing process. Minimizing stress prior to curing minimizes cracking. High-early-strength concrete is designed to hydrate faster, often by increased use of cement that increases shrinkage and cracking. The strength of concrete changes (increases) for up to three years. It depends on cross-section dimension of elements and conditions of structure exploitation. Addition of short-cut polymer fibers can improve (reduce) shrinkage-induced stresses during curing and increase early and ultimate compression strength.\nProperly curing concrete leads to increased strength and lower permeability and avoids cracking where the surface dries out prematurely. Care must also be taken to avoid freezing or overheating due to the exothermic setting of cement. Improper curing can cause spalling, reduced strength, poor abrasion resistance and cracking.\nCuring techniques avoiding water loss by evaporation.\nDuring the curing period, concrete is ideally maintained at controlled temperature and humidity. To ensure full hydration during curing, concrete slabs are often sprayed with \"curing compounds\" that create a water-retaining film over the concrete. Typical films are made of wax or related hydrophobic compounds. After the concrete is sufficiently cured, the film is allowed to abrade from the concrete through normal use.\nTraditional conditions for curing involve spraying or ponding the concrete surface with water. The adjacent picture shows one of many ways to achieve this, ponding\u2014submerging setting concrete in water and wrapping in plastic to prevent dehydration. Additional common curing methods include wet burlap and plastic sheeting covering the fresh concrete.\nFor higher-strength applications, accelerated curing techniques may be applied to the concrete. A common technique involves heating the poured concrete with steam, which serves to both keep it damp and raise the temperature so that the hydration process proceeds more quickly and more thoroughly.\nAlternative types.\nAsphalt.\n\"Asphalt concrete\" (commonly called \"asphalt\", \"blacktop\", or \"pavement\" in North America, and \"tarmac\", \"bitumen macadam\", or \"rolled asphalt\" in the United Kingdom and the Republic of Ireland) is a composite material commonly used to surface roads, parking lots, airports, as well as the core of embankment dams. Asphalt mixtures have been used in pavement construction since the beginning of the twentieth century. It consists of mineral aggregate bound together with asphalt, laid in layers, and compacted. The process was refined and enhanced by Belgian inventor and U.S. immigrant Edward De Smedt.\nThe terms \"asphalt\" (or \"asphaltic\") \"concrete\", \"bituminous asphalt concrete\", and \"bituminous mixture\" are typically used only in engineering and construction documents, which define concrete as any composite material composed of mineral aggregate adhered with a binder. The abbreviation, \"AC\", is sometimes used for \"asphalt concrete\" but can also denote \"asphalt content\" or \"asphalt cement\", referring to the liquid asphalt portion of the composite material.\nGraphene enhanced concrete.\nGraphene enhanced concretes are standard designs of concrete mixes, except that during the cement-mixing or production process, a small amount of chemically engineered graphene is added. These enhanced graphene concretes are designed around the concrete application.\nMicrobial.\nBacteria such as \"Bacillus pasteurii\", \"Bacillus pseudofirmus\", \"Bacillus cohnii\", \"Sporosarcina pasteuri\", and \"Arthrobacter crystallopoietes\" increase the compression strength of concrete through their biomass. However some forms of bacteria can also be concrete-destroying. Bacillus sp. CT-5. can reduce corrosion of reinforcement in reinforced concrete by up to four times. \"Sporosarcina pasteurii\" reduces water and chloride permeability. \"B. pasteurii\" increases resistance to acid. \"Bacillus pasteurii\" and \"B. sphaericuscan\" induce calcium carbonate precipitation in the surface of cracks, adding compression strength.\nNanoconcrete.\nNanoconcrete (also spelled \"nano concrete\"' or \"nano-concrete\") is a class of materials that contains Portland cement particles that are no greater than 100 \u03bcm and particles of silica no greater than 500 \u03bcm, which fill voids that would otherwise occur in normal concrete, thereby substantially increasing the material's strength. It is widely used in foot and highway bridges where high flexural and compressive strength are indicated.\nPervious.\nPervious concrete is a mix of specially graded coarse aggregate, cement, water, and little-to-no fine aggregates. This concrete is also known as \"no-fines\" or porous concrete. Mixing the ingredients in a carefully controlled process creates a paste that coats and bonds the aggregate particles. The hardened concrete contains interconnected air voids totaling approximately 15 to 25 percent. Water runs through the voids in the pavement to the soil underneath. Air entrainment admixtures are often used in freeze-thaw climates to minimize the possibility of frost damage. Pervious concrete also permits rainwater to filter through roads and parking lots, to recharge aquifers, instead of contributing to runoff and flooding.\nPolymer.\nPolymer concretes are mixtures of aggregate and any of various polymers and may be reinforced. The cement is costlier than lime-based cements, but polymer concretes nevertheless have advantages; they have significant tensile strength even without reinforcement, and they are largely impervious to water. Polymer concretes are frequently used for the repair and construction of other applications, such as drains.\nPlant fibers.\nPlant fibers and particles can be used in a concrete mix or as a reinforcement. These materials can increase ductility but the lignocellulosic particles hydrolyze during concrete curing as a result of alkaline environment and elevated temperatures Such process, that is difficult to measure, can affect the properties of the resulting concrete.\nSulfur concrete.\nSulfur concrete is a special concrete that uses sulfur as a binder and does not require cement or water.\nVolcanic.\nVolcanic concrete substitutes volcanic rock for the limestone that is burned to form clinker. It consumes a similar amount of energy, but does not directly emit carbon as a byproduct. Volcanic rock/ash are used as supplementary cementitious materials in concrete to improve the resistance to sulfate, chloride and alkali silica reaction due to pore refinement. Also, they are generally cost effective in comparison to other aggregates, good for semi and light weight concretes, and good for thermal and acoustic insulation.\nPyroclastic materials, such as pumice, scoria, and ashes are formed from cooling magma during explosive volcanic eruptions. They are used as supplementary cementitious materials (SCM) or as aggregates for cements and concretes. They have been extensively used since ancient times to produce materials for building applications. For example, pumice and other volcanic glasses were added as a natural pozzolanic material for mortars and plasters during the construction of the Villa San Marco in the Roman period (89 BC \u2013 79 AD), which remain one of the best-preserved otium villae of the Bay of Naples in Italy.\nWaste light.\nWaste light is a form of polymer modified concrete. The specific polymer admixture allows the replacement of all the traditional aggregates (gravel, sand, stone) by any mixture of solid waste materials in the grain size of 3\u201310\u00a0mm to form a low-compressive-strength (3\u201320\u00a0N/mm2) product for road and building construction. One cubic meter of waste light concrete contains 1.1\u20131.3\u00a0m3 of shredded waste and no other aggregates.\nRecycled Aggregate Concrete (RAC).\nRecycled aggregate concretes are standard concrete mixes with the addition or substitution of natural aggregates with recycled aggregates sourced from construction and demolition wastes, disused pre-cast concretes or masonry. In most cases, recycled aggregate concrete results in higher water absorption levels by capillary action and permeation, which are the prominent determiners of the strength and durability of the resulting concrete. The increase in water absorption levels is mainly caused by the porous adhered mortar that exists in the recycled aggregates. Accordingly, recycled concrete aggregates that have been washed to reduce the quantity of mortar adhered to aggregates show lower water absorption levels compared to untreated recycled aggregates.\nThe quality of the recycled aggregate concrete is determined by several factors, including the size, the number of replacement cycles, and the moisture levels of the recycled aggregates. When the recycled concrete aggregates are crushed into coarser fractures, the mixed concrete shows better permeability levels, resulting in an overall increase in strength. In contrast, recycled masonry aggregates provide better qualities when crushed in finer fractures. With each generation of recycled concrete, the resulting compressive strength decreases.\nProperties.\nConcrete has relatively high compressive strength, but much lower tensile strength. Therefore, it is usually reinforced with materials that are strong in tension (often steel). The elasticity of concrete is relatively constant at low stress levels but starts decreasing at higher stress levels as matrix cracking develops. Concrete has a very low coefficient of thermal expansion and shrinks as it matures. All concrete structures crack to some extent, due to shrinkage and tension. Concrete that is subjected to long-duration forces is prone to creep.\nTests can be performed to ensure that the properties of concrete correspond to specifications for the application.\nThe ingredients affect the strengths of the material. Concrete strength values are usually specified as the lower-bound compressive strength of either a cylindrical or cubic specimen as determined by standard test procedures.\nThe strengths of concrete is dictated by its function. Very low-strength\u2014 or less\u2014concrete may be used when the concrete must be lightweight. Lightweight concrete is often achieved by adding air, foams, or lightweight aggregates, with the side effect that the strength is reduced. For most routine uses, concrete is often used. concrete is readily commercially available as a more durable, although more expensive, option. Higher-strength concrete is often used for larger civil projects. Strengths above are often used for specific building elements. For example, the lower floor columns of high-rise concrete buildings may use concrete of or more, to keep the size of the columns small. Bridges may use long beams of high-strength concrete to lower the number of spans required. Occasionally, other structural needs may require high-strength concrete. If a structure must be very rigid, concrete of very high strength may be specified, even much stronger than is required to bear the service loads. Strengths as high as have been used commercially for these reasons.\nEnergy efficiency.\nThe cement produced for making concrete accounts for about 8% of worldwide emissions per year (compared to, \"e.g.\", global aviation at 1.9%). The two largest sources of are produced by the cement manufacturing process, arising from (1) the decarbonation reaction of limestone in the cement kiln (T \u2248 950\u00a0\u00b0C), and (2) from the combustion of fossil fuel to reach the sintering temperature (T \u2248 1450\u00a0\u00b0C) of cement clinker in the kiln. The energy required for extracting, crushing, and mixing the raw materials (construction aggregates used in the concrete production, and also limestone and clay feeding the cement kiln) is lower. Energy requirement for transportation of ready-mix concrete is also lower because it is produced nearby the construction site from local resources, typically manufactured within 100 kilometers of the job site. The overall embodied energy of concrete at roughly 1 to 1.5 megajoules per kilogram is therefore lower than for many structural and construction materials.\nOnce in place, concrete offers a great energy efficiency over the lifetime of a building. Concrete walls leak air far less than those made of wood frames. Air leakage accounts for a large percentage of energy loss from a home. The thermal mass properties of concrete increase the efficiency of both residential and commercial buildings. By storing and releasing the energy needed for heating or cooling, concrete's thermal mass delivers year-round benefits by reducing temperature swings inside and minimizing heating and cooling costs. While insulation reduces energy loss through the building envelope, thermal mass uses walls to store and release energy. Modern concrete wall systems use both external insulation and thermal mass to create an energy-efficient building. Insulating concrete forms (ICFs) are hollow blocks or panels made of either insulating foam or rastra that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.\nFire safety.\nConcrete buildings are more resistant to fire than those constructed using steel frames, since concrete has lower heat conductivity than steel and can thus last longer under the same fire conditions. Concrete is sometimes used as a fire protection for steel frames, for the same effect as above. Concrete as a fire shield, for example Fondu fyre, can also be used in extreme environments like a missile launch pad.\nOptions for non-combustible construction include floors, ceilings and roofs made of cast-in-place and hollow-core precast concrete. For walls, concrete masonry technology and Insulating Concrete Forms (ICFs) are additional options. ICFs are hollow blocks or panels made of fireproof insulating foam that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.\nConcrete also provides good resistance against externally applied forces such as high winds, hurricanes, and tornadoes owing to its lateral stiffness, which results in minimal horizontal movement. However, this stiffness can work against certain types of concrete structures, particularly where a relatively higher flexing structure is required to resist more extreme forces.\nEarthquake safety.\nAs discussed above, concrete is very strong in compression, but weak in tension. Larger earthquakes can generate very large shear loads on structures. These shear loads subject the structure to both tensile and compressional loads. Concrete structures without reinforcement, like other unreinforced masonry structures, can fail during severe earthquake shaking. Unreinforced masonry structures constitute one of the largest earthquake risks globally. These risks can be reduced through seismic retrofitting of at-risk buildings, (e.g. school buildings in Istanbul, Turkey).\nConstruction.\nConcrete is one of the most durable building materials. It provides superior fire resistance compared with wooden construction and gains strength over time. Structures made of concrete can have a long service life. Concrete is used more than any other artificial material in the world. As of 2006, about 7.5 billion cubic meters of concrete are made each year, more than one cubic meter for every person on Earth.\nReinforced.\nThe use of reinforcement, in the form of iron was introduced in the 1850s by French industrialist Fran\u00e7ois Coignet, and it was not until the 1880s that German civil engineer G. A. Wayss used steel as reinforcement. Concrete is a relatively brittle material that is strong under compression but less in tension. Plain, unreinforced concrete is unsuitable for many structures as it is relatively poor at withstanding stresses induced by vibrations, wind loading, and so on. Hence, to increase its overall strength, steel rods, wires, mesh or cables can be embedded in concrete before it is set. This reinforcement, often known as rebar, resists tensile forces.\nReinforced concrete (RC) is a versatile composite and one of the most widely used materials in modern construction. It is made up of different constituent materials with very different properties that complement each other. In the case of reinforced concrete, the component materials are almost always concrete and steel. These two materials form a strong bond together and are able to resist a variety of applied forces, effectively acting as a single structural element.\nReinforced concrete can be precast or cast-in-place (in situ) concrete, and is used in a wide range of applications such as; slab, wall, beam, column, foundation, and frame construction. Reinforcement is generally placed in areas of the concrete that are likely to be subject to tension, such as the lower portion of beams. Usually, there is a minimum of 50\u00a0mm cover, both above and below the steel reinforcement, to resist spalling and corrosion which can lead to structural instability. Other types of non-steel reinforcement, such as Fibre-reinforced concretes are used for specialized applications, predominately as a means of controlling cracking.\nPrecast.\nPrecast concrete is concrete which is cast in one place for use elsewhere and is a mobile material. The largest part of precast production is carried out in the works of specialist suppliers, although in some instances, due to economic and geographical factors, scale of product or difficulty of access, the elements are cast on or adjacent to the construction site. Precasting offers considerable advantages because it is carried out in a controlled environment, protected from the elements, but the downside of this is the contribution to greenhouse gas emission from transportation to the construction site.\nAdvantages to be achieved by employing precast concrete:\nMass structures.\nDue to cement's exothermic chemical reaction while setting up, large concrete structures such as dams, navigation locks, large mat foundations, and large breakwaters generate excessive heat during hydration and associated expansion. To mitigate these effects, \"post-cooling\" is commonly applied during construction. An early example at Hoover Dam used a network of pipes between vertical concrete placements to circulate cooling water during the curing process to avoid damaging overheating. Similar systems are still used; depending on volume of the pour, the concrete mix used, and ambient air temperature, the cooling process may last for many months after the concrete is placed. Various methods also are used to pre-cool the concrete mix in mass concrete structures.\nAnother approach to mass concrete structures that minimizes cement's thermal by-product is the use of roller-compacted concrete, which uses a dry mix which has a much lower cooling requirement than conventional wet placement. It is deposited in thick layers as a semi-dry material then roller compacted into a dense, strong mass.\nSurface finishes.\nRaw concrete surfaces tend to be porous and have a relatively uninteresting appearance. Many finishes can be applied to improve the appearance and preserve the surface against staining, water penetration, and freezing.\nExamples of improved appearance include stamped concrete where the wet concrete has a pattern impressed on the surface, to give a paved, cobbled or brick-like effect, and may be accompanied with coloration. Another popular effect for flooring and table tops is polished concrete where the concrete is polished optically flat with diamond abrasives and sealed with polymers or other sealants.\nOther finishes can be achieved with chiseling, or more conventional techniques such as painting or covering it with other materials.\nThe proper treatment of the surface of concrete, and therefore its characteristics, is an important stage in the construction and renovation of architectural structures.\nPrestressed.\nPrestressed concrete is a form of reinforced concrete that builds in compressive stresses during construction to oppose tensile stresses experienced in use. This can greatly reduce the weight of beams or slabs, by\nbetter distributing the stresses in the structure to make optimal use of the reinforcement. For example, a horizontal beam tends to sag. Prestressed reinforcement along the bottom of the beam counteracts this.\nIn pre-tensioned concrete, the prestressing is achieved by using steel or polymer tendons or bars that are subjected to a tensile force prior to casting, or for post-tensioned concrete, after casting.\nThere are two different systems being used:\nMore than of highways in the United States are paved with this material. Reinforced concrete, prestressed concrete and precast concrete are the most widely used types of concrete functional extensions in modern days. For more information see Brutalist architecture.\nPlacement.\nOnce mixed, concrete is typically transported to the place where it is intended to become a structural item. Various methods of transportation and placement are used depending on the distances involve, quantity needed, and other details of application. Large amounts are often transported by truck, poured free under gravity or through a tremie, or pumped through a pipe. Smaller amounts may be carried in a skip (a metal container which can be tilted or opened to release the contents, usually transported by crane or hoist), or wheelbarrow, or carried in toggle bags for manual placement underwater.\nCold weather placement.\nExtreme weather conditions (extreme heat or cold; windy conditions, and humidity variations) can significantly alter the quality of concrete. Many precautions are observed in cold weather placement. Low temperatures significantly slow the chemical reactions involved in hydration of cement, thus affecting the strength development. Preventing freezing is the most important precaution, as formation of ice crystals can cause damage to the crystalline structure of the hydrated cement paste. If the surface of the concrete pour is insulated from the outside temperatures, the heat of hydration will prevent freezing.\nThe American Concrete Institute (ACI) definition of cold weather placement, ACI 306, is:\nIn Canada, where temperatures tend to be much lower during the cold season, the following criteria are used by CSA A23.1:\nThe minimum strength before exposing concrete to extreme cold is . CSA A 23.1 specified a compressive strength of 7.0\u00a0MPa to be considered safe for exposure to freezing.\nUnderwater placement.\nConcrete may be placed and cured underwater. Care must be taken in the placement method to prevent washing out the cement. Underwater placement methods include the tremie, pumping, skip placement, manual placement using toggle bags, and bagwork.\nA tremie is a vertical, or near-vertical, pipe with a hopper at the top used to pour concrete underwater in a way that avoids washout of cement from the mix due to turbulent water contact with the concrete while it is flowing. This produces a more reliable strength of the product. The method is generally used for placing small quantities and for repairs. Wet concrete is loaded into a reusable canvas bag and squeezed out at the required place by the diver. Care must be taken to avoid washout of the cement and fines.\n is the manual placement by divers of woven cloth bags containing dry mix, followed by piercing the bags with steel rebar pins to tie the bags together after every two or three layers, and create a path for hydration to induce curing, which can typically take about 6 to 12 hours for initial hardening and full hardening by the next day. Bagwork concrete will generally reach full strength within 28 days. Each bag must be pierced by at least one, and preferably up to four pins. Bagwork is a simple and convenient method of underwater concrete placement which does not require pumps, plant, or formwork, and which can minimise environmental effects from dispersing cement in the water. Prefilled bags are available, which are sealed to prevent premature hydration if stored in suitable dry conditions. The bags may be biodegradable.\n is an alternative method of forming a concrete mass underwater, where the forms are filled with coarse aggregate and the voids then completely filled from the bottom by displacing the water with pumped grout.\nRoads.\nConcrete roads are more fuel efficient to drive on, more reflective and last significantly longer than other paving surfaces, yet have a much smaller market share than other paving solutions. Modern-paving methods and design practices have changed the economics of concrete paving, so that a well-designed and placed concrete pavement will be less expensive on initial costs and significantly less expensive over the life cycle. Another major benefit is that pervious concrete can be used, which eliminates the need to place storm drains near the road, and reducing the need for slightly sloped roadway to help rainwater to run off. No longer requiring discarding rainwater through use of drains also means that less electricity is needed (more pumping is otherwise needed in the water-distribution system), and no rainwater gets polluted as it no longer mixes with polluted water. Rather, it is immediately absorbed by the ground.\nTube forest.\nCement molded into a forest of tubular structures can be 5.6 times more resistant to cracking/failure than standard concrete. The approach mimics mammalian cortical bone that features elliptical, hollow osteons suspended in an organic matrix, connected by relatively weak \"cement lines\". Cement lines provide a preferable in-plane crack path. This design fails via a \"stepwise toughening mechanism\". Cracks are contained within the tube, reducing spreading, by dissipating energy at each tube/step.\nEnvironment, health and safety.\nThe manufacture and use of concrete produce a wide range of environmental, economic and social impacts.\nHealth and safety.\nGrinding of concrete can produce hazardous dust. Exposure to cement dust can lead to issues such as silicosis, kidney disease, skin irritation and similar effects. The U.S. National Institute for Occupational Safety and Health in the United States recommends attaching local exhaust ventilation shrouds to electric concrete grinders to control the spread of this dust. In addition, the Occupational Safety and Health Administration (OSHA) has placed more stringent regulations on companies whose workers regularly come into contact with silica dust. An updated silica rule, which OSHA put into effect 23 September 2017 for construction companies, restricted the amount of breathable crystalline silica workers could legally come into contact with to 50 micro grams per cubic meter of air per 8-hour workday. That same rule went into effect 23 June 2018 for general industry, hydraulic fracturing and maritime. That deadline was extended to 23 June 2021 for engineering controls in the hydraulic fracturing industry. Companies which fail to meet the tightened safety regulations can face financial charges and extensive penalties. The presence of some substances in concrete, including useful and unwanted additives, can cause health concerns due to toxicity and radioactivity. Fresh concrete (before curing is complete) is highly alkaline and must be handled with proper protective equipment.\nCement.\nA major component of concrete is cement, a fine powder used mainly to bind sand and coarser aggregates together in concrete. Although a variety of cement types exist, the most common is \"Portland cement\", which is produced by mixing clinker with smaller quantities of other additives such as gypsum and ground limestone. The production of clinker, the main constituent of cement, is responsible for the bulk of the sector's greenhouse gas emissions, including both energy intensity and process emissions.\nThe cement industry is one of the three primary producers of carbon dioxide, a major greenhouse gas \u2013 the other two being energy production and transportation industries. On average, every tonne of cement produced releases one tonne of CO2 into the atmosphere. Pioneer cement manufacturers have claimed to reach lower carbon intensities, with 590\u00a0kg of CO2eq per tonne of cement produced. The emissions are due to combustion and calcination processes, which roughly account for 40% and 60% of the greenhouse gases, respectively. Considering that cement is only a fraction of the constituents of concrete, it is estimated that a tonne of concrete is responsible for emitting about 100\u2013200\u00a0kg of CO2. Every year more than 10 billion tonnes of concrete are used worldwide. In the coming years, large quantities of concrete will continue to be used, and the mitigation of CO2 emissions from the sector will be even more critical.\nConcrete is used to create hard surfaces that contribute to surface runoff, which can cause heavy soil erosion, water pollution, and flooding, but conversely can be used to divert, dam, and control flooding. Concrete dust released by building demolition and natural disasters can be a major source of dangerous air pollution. Concrete is a contributor to the urban heat island effect, though less so than asphalt.\nClimate change mitigation.\nReducing the cement clinker content might have positive effects on the environmental life-cycle assessment of concrete. Some research work on reducing the cement clinker content in concrete has already been carried out. However, there exist different research strategies. Often replacement of some clinker for large amounts of slag or fly ash was investigated based on conventional concrete technology. This could lead to a waste of scarce raw materials such as slag and fly ash. The aim of other research activities is the efficient use of cement and reactive materials like slag and fly ash in concrete based on a modified mix design approach.\nThe embodied carbon of a precast concrete facade can be reduced by 50% when using the presented fiber reinforced high performance concrete in place of typical reinforced concrete cladding. Studies have been conducted about commercialization of low-carbon concretes. Life cycle assessment (LCA) of low-carbon concrete was investigated according to the ground granulated blast-furnace slag (GGBS) and fly ash (FA) replacement ratios. Global warming potential (GWP) of GGBS decreased by 1.1\u00a0kg CO2 eq/m3, while FA decreased by 17.3\u00a0kg CO2 eq/m3 when the mineral admixture replacement ratio was increased by 10%. This study also compared the compressive strength properties of binary blended low-carbon concrete according to the replacement ratios, and the applicable range of mixing proportions was derived.\nClimate change adaptation.\nHigh-performance building materials will be particularly important for enhancing resilience, including for flood defenses and critical-infrastructure protection. Risks to infrastructure and cities posed by extreme weather events are especially serious for those places exposed to flood and hurricane damage, but also where residents need protection from extreme summer temperatures. Traditional concrete can come under strain when exposed to humidity and higher concentrations of atmospheric CO2. While concrete is likely to remain important in applications where the environment is challenging, novel, smarter and more adaptable materials are also needed.\nRecycling.\nThere have been concerns about the recycling of painted concrete due to possible lead content. Studies have indicated that recycled concrete exhibits lower strength and durability compared to concrete produced using natural aggregates. This deficiency can be addressed by incorporating supplementary materials such as fly ash into the mixture.\nWorld records.\nThe world record for the largest concrete pour in a single project is the Three Gorges Dam in Hubei Province, China by the Three Gorges Corporation. The amount of concrete used in the construction of the dam is estimated at 16 million cubic meters over 17 years. The previous record was 12.3 million cubic meters held by Itaipu hydropower station in Brazil.\nThe world record for concrete pumping was set on 7 August 2009 during the construction of the Parbati Hydroelectric Project, near the village of Suind, Himachal Pradesh, India, when the concrete mix was pumped through a vertical height of .\nThe Polavaram dam works in Andhra Pradesh on 6 January 2019 entered the Guinness World Records by pouring 32,100 cubic metres of concrete in 24 hours. The world record for the largest continuously poured concrete raft was achieved in August 2007 in Abu Dhabi by contracting firm Al Habtoor-CCC Joint Venture and the concrete supplier is Unibeton Ready Mix. The pour (a part of the foundation for the Abu Dhabi's Landmark Tower) was 16,000 cubic meters of concrete poured within a two-day period. The previous record, 13,200 cubic meters poured in 54 hours despite a severe tropical storm requiring the site to be covered with tarpaulins to allow work to continue, was achieved in 1992 by joint Japanese and South Korean consortiums Hazama Corporation and the Samsung C&amp;T Corporation for the construction of the Petronas Towers in Kuala Lumpur, Malaysia.\nThe world record for largest continuously poured concrete floor was completed 8 November 1997, in Louisville, Kentucky by design-build firm EXXCEL Project Management. The monolithic placement consisted of of concrete placed in 30 hours, finished to a flatness tolerance of FF 54.60 and a levelness tolerance of FL 43.83. This surpassed the previous record by 50% in total volume and 7.5% in total area.\nThe record for the largest continuously placed underwater concrete pour was completed 18 October 2010, in New Orleans, Louisiana by contractor C. J. Mahan Construction Company, LLC of Grove City, Ohio. The placement consisted of 10,251 cubic yards of concrete placed in 58.5 hours using two concrete pumps and two dedicated concrete batch plants. Upon curing, this placement allows the cofferdam to be dewatered approximately below sea level to allow the construction of the Inner Harbor Navigation Canal Sill &amp; Monolith Project to be completed in the dry."}
{"id": "5373", "revid": "1259341064", "url": "https://en.wikipedia.org/wiki?curid=5373", "title": "Coitus interruptus", "text": "Coitus interruptus, also known as withdrawal, pulling out or the pull-out method, is an act of birth control during penetrative sexual intercourse, whereby the penis is withdrawn from a vagina prior to ejaculation so that the ejaculate (semen) may be directed away in an effort to avoid insemination.\nThis method was used by an estimated 38 million couples worldwide in 1991. \"Coitus interruptus\" does not protect against sexually transmitted infections (STIs).\nHistory.\nPerhaps the oldest description of the use of the withdrawal method to avoid pregnancy is the story of Onan in the Torah and the Bible. This text is believed to have been written over 2,500 years ago. Societies in the ancient civilizations of Greece and Rome preferred small families and are known to have practiced a variety of birth control methods. There are references that have led historians to believe withdrawal was sometimes used as birth control. However, these societies viewed birth control as a woman's responsibility, and the only well-documented contraception methods were female-controlled devices (both possibly effective, such as pessaries, and ineffective, such as amulets).\nAfter the decline of the Roman Empire in the 5th century AD, contraceptive practices fell out of use in Europe; the use of contraceptive pessaries, for example, is not documented again until the 15th century. If withdrawal was used during the Roman Empire, knowledge of the art may have been lost during its decline.\nFrom the 18th century until the development of modern methods, withdrawal was one of the most popular methods of birth-control in Europe, North America, and elsewhere.\nEffects.\nLike many methods of birth control, reliable effect is achieved only by correct and consistent use. Observed failure rates of withdrawal vary depending on the population being studied: American studies have found actual failure rates of 15\u201328% per year. One US study, based on self-reported data from the 2006\u20132010 cycle of the National Survey of Family Growth, found significant differences in failure rate based on parity status. Women with 0 previous births had a 12-month failure rate of only 8.4%, which then increased to 20.4% for those with 1 prior birth and again to 27.7% for those with 2 or more.\nAn analysis of Demographic and Health Surveys in 43 developing countries between 1990 and 2013 found a median 12-month failure rate across subregions of 13.4%, with a range of 7.8\u201317.1%. Individual countries within the subregions were even more varied. A large scale study of women in England and Scotland during 1968\u20131974 to determine the efficacy of various contraceptive methods found a failure rate of 6.7 per 100 woman-years of use. This was a \u201ctypical use\u201d failure rate, including user failure to use the method correctly. In comparison, the combined oral contraceptive pill has an actual use failure rate of 2\u20138%, while intrauterine devices (IUDs) have an actual use failure rate of 0.1\u20130.8%. Condoms have an actual use failure rate of 10\u201318%. However, some authors suggest that actual effectiveness of withdrawal could be similar to the effectiveness of condoms; this area needs further research. (See Comparison of birth control methods.)\nFor couples that use \"coitus interruptus\" consistently and correctly at every act of intercourse, the failure rate is 4% per year. This rate is derived from an educated guess based on a modest chance of sperm in the pre-ejaculate. In comparison, the pill has a perfect-use failure rate of 0.3%, IUDs a rate of 0.1\u20130.6%, and internal condoms a rate of 2%.\nIt has been suggested that the pre-ejaculate (\"Cowper's fluid\") emitted by the penis prior to ejaculation may contain spermatozoa (sperm cells), which would compromise the effectiveness of the method. However, several small studies have failed to find any viable sperm in the fluid. While no large conclusive studies have been done, it is believed by some that the cause of method (correct-use) failure is the pre-ejaculate fluid picking up sperm from a previous ejaculation. For this reason, it is recommended that the male partner urinate between ejaculations, to clear the urethra of sperm, and wash any ejaculate from objects that might come near the woman's vulva (such as hands and penis).\nHowever, recent research suggests that this might not be accurate. A contrary, yet non-generalizable study that found mixed evidence, including individual cases of a high sperm concentration, was published in March 2011. A noted limitation to these previous studies' findings is that pre-ejaculate samples were analyzed after the critical two-minute point. That is, looking for motile sperm in small amounts of pre-ejaculate via microscope after two minutes \u2013 when the sample has most likely dried \u2013 makes examination and evaluation \"extremely difficult\". Thus, in March 2011 a team of researchers assembled 27 male volunteers and analyzed their pre-ejaculate samples within two minutes after producing them. The researchers found that 11 of the 27 men (41%) produced pre-ejaculatory samples that contained sperm, and 10 of these samples (37%) contained a \"fair amount\" of motile sperm (in other words, as few as 1 million to as many as 35 million). This study therefore recommends, in order to minimize unintended pregnancy and disease transmission, the use of condoms from the first moment of genital contact. As a point of reference, a study showed that, of couples who conceived within a year of trying, only 2.5% included a male partner with a total sperm count (per ejaculate) of 23 million sperm or less. However, across a wide range of observed values, total sperm count (as with other identified semen and sperm characteristics) has weak power to predict which couples are at risk of pregnancy. Regardless, this study introduced the concept that some men may consistently have sperm in their pre-ejaculate, due to a \"leakage,\" while others may not.\nSimilarly, another robust study performed in 2016 found motile sperm in the pre-ejaculate of 16.7% (7/42) healthy men. What more, this study attempted to exclude contamination of sperm from ejaculate by drying the pre-ejaculate specimens to reveal a fern-like pattern, characteristics of true pre-ejaculate. All pre-ejaculate specimens were examined within an hour of production and then dried; all pre-ejaculate specimens were found to be true pre-ejaculate. It is widely believed that urinating after an ejaculation will flush the urethra of remaining sperm. However, some of the subjects in the March 2011 study who produced sperm in their pre-ejaculate did urinate (sometimes more than once) before producing their sample. Therefore, some males can release the pre-ejaculate fluid containing sperm without a previous ejaculation.\nAdvantages.\nThe advantage of \"coitus interruptus\" is that it can be used by people who have objections to, or do not have access to, other forms of contraception. Some people prefer it so they can avoid possible adverse effects of hormonal contraceptives or so that they can have a full experience and be able to \"feel\" their partner. Other reasons for the popularity of this method are its anecdotal increase in male sexual deftness, it has no direct monetary cost, requires no artificial devices, has no physical side effects, can be practiced without a prescription or medical consultation, and provides no barriers to stimulation.\nDisadvantages.\nCompared to the other common reversible methods of contraception such as IUDs, hormonal contraceptives, and male condoms, \"coitus interruptus\" is less effective at preventing pregnancy. As a result, it is also less cost-effective than many more effective methods: although the method itself has no direct cost, users have a greater chance of incurring the risks and expenses of either child-birth or abortion. Only models that assume all couples practice perfect use of the method find cost savings associated with the choice of withdrawal as a birth control method.\nThe method is largely ineffective in the prevention of sexually transmitted infections (STIs), like HIV, since pre-ejaculate may carry viral particles or bacteria which may infect the partner if this fluid comes in contact with mucous membranes. However, a reduction in the volume of bodily fluids exchanged during intercourse may reduce the likelihood of disease transmission compared to using no method due to the smaller number of pathogens present.\nPrevalence.\nBased on data from surveys conducted during the late 1990s, 3% of women of childbearing age worldwide rely on withdrawal as their primary method of contraception. Regional popularity of the method varies widely, from a low of 1% in Africa to 16% in Western Asia.\nIn the United States, according to the National Survey of Family Growth (NSFG) in 2014, 8.1% of reproductive-aged women reported using withdrawal as a primary contraceptive method. This was a significant increase from 2012 when 4.8% of women reported the use of withdrawal as their most effective method. However, when withdrawal is used in addition to or in rotation with another contraceptive method, the percentage of women using withdrawal jumps from 5% for sole use and 11% for any withdrawal use in 2002, and for adolescents from 7.1% of sole withdrawal use to 14.6% of any withdrawal use in 2006\u20132008.\nWhen asked if withdrawal was used at least once in the past month by women, use of withdrawal increased from 13% as sole use to 33% ever use in the past month. These increases are even more pronounced for adolescents 15 to 19 years old and young women 20 to 24 years old Similarly, the NSFG reports that 9.8% of unmarried men who have had sexual intercourse in the last three months in 2002 used withdrawal, which then increased to 14.5% in 2006\u20132010, and then to 18.8% in 2011\u20132015. The use of withdrawal varied by the unmarried man's age and cohabiting status, but not by ethnicity or race. The use of withdrawal decreased significantly with increasing age groups, ranging from 26.2% among men aged 15\u201319 to 12% among men aged 35\u201344. The use of withdrawal was significantly higher for never-married men (23.0%) compared with formerly married (16.3%) and cohabiting (13.0%) men.\nFor 1998, about 18% of married men in Turkey reported using withdrawal as a contraceptive method."}
{"id": "5374", "revid": "9036255", "url": "https://en.wikipedia.org/wiki?curid=5374", "title": "Condom", "text": "A condom is a sheath-shaped barrier device used during sexual intercourse to reduce the probability of pregnancy or a sexually transmitted infection (STI). There are both external condoms, also called male condoms, and internal (female) condoms.\nThe external condom is rolled onto an erect penis before intercourse and works by forming a physical barrier which limits skin-to-skin contact, exposure to fluids, and blocks semen from entering the body of a sexual partner. External condoms are typically made from latex and, less commonly, from polyurethane, polyisoprene, or lamb intestine. External condoms have the advantages of ease of use, ease of access, and few side effects. Individuals with latex allergy should use condoms made from a material other than latex, such as polyurethane. Internal condoms are typically made from polyurethane and may be used multiple times.\nWith proper use\u2014and use at every act of intercourse\u2014women whose partners use external condoms experience a 2% per-year pregnancy rate. With typical use, the rate of pregnancy is 18% per-year. Their use greatly decreases the risk of gonorrhea, chlamydia, trichomoniasis, hepatitis B, and HIV/AIDS. To a lesser extent, they also protect against genital herpes, human papillomavirus (HPV), and syphilis.\nCondoms as a method of preventing STIs have been used since at least 1564. Rubber condoms became available in 1855, followed by latex condoms in the 1920s. It is on the World Health Organization's List of Essential Medicines. As of 2019, globally around 21% of those using birth control use the condom, making it the second-most common method after female sterilization (24%). Rates of condom use are highest in East and Southeast Asia, Europe and North America. \nMedical uses.\nBirth control.\nThe effectiveness of condoms, as of most forms of contraception, can be assessed two ways. \"Perfect use\" or \"method\" effectiveness rates only include people who use condoms properly and consistently. \"Actual use\", or \"typical use\" effectiveness rates are of all condom users, including those who use condoms incorrectly or do not use condoms at every act of intercourse. Rates are generally presented for the first year of use. Most commonly the Pearl Index is used to calculate effectiveness rates, but some studies use decrement tables.\nThe typical use pregnancy rate among condom users varies depending on the population being studied, ranging from 10 to 18% per year. The perfect use pregnancy rate of condoms is 2% per year. Condoms may be combined with other forms of contraception (such as spermicide) for greater protection.\nSexually transmitted infections.\nCondoms are widely recommended for the prevention of sexually transmitted infections (STIs). They have been shown to be effective in reducing infection rates in both men and women. While not perfect, the condom is effective at reducing the transmission of organisms that cause AIDS, genital herpes, cervical cancer, genital warts, syphilis, chlamydia, gonorrhea, and other diseases. Condoms are often recommended as an adjunct to more effective birth control methods (such as IUD) in situations where STI protection is also desired.\nFor this reason, condoms are frequently used by those in the swinging community.\nAccording to a 2000 report by the National Institutes of Health (NIH), consistent use of latex condoms reduces the risk of HIV transmission by approximately 85% relative to risk when unprotected, putting the seroconversion rate (infection rate) at 0.9 per 100 person-years with condom, down from 6.7 per 100 person-years. Analysis published in 2007 from the University of Texas Medical Branchand the World Health Organization found similar risk reductions of 80\u201395%.\nThe 2000 NIH review concluded that condom use significantly reduces the risk of gonorrhea for men. A 2006 study reports that proper condom use decreases the risk of transmission of human papillomavirus (HPV) to women by approximately 70%. Another study in the same year found consistent condom use was effective at reducing transmission of herpes simplex virus-2, also known as genital herpes, in both men and women.\nAlthough a condom is effective in limiting exposure, some disease transmission may occur even with a condom. Infectious areas of the genitals, especially when symptoms are present, may not be covered by a condom, and as a result, some diseases like HPV and herpes may be transmitted by direct contact. The primary effectiveness issue with using condoms to prevent STIs, however, is inconsistent use.\nCondoms may also be useful in treating potentially precancerous cervical changes. Exposure to human papillomavirus, even in individuals already infected with the virus, appears to increase the risk of precancerous changes. The use of condoms helps promote regression of these changes. In addition, researchers in the UK suggest that a hormone in semen can aggravate existing cervical cancer, condom use during sex can prevent exposure to the hormone.\nCauses of failure.\nCondoms may slip off the penis after ejaculation, break due to improper application or physical damage (such as tears caused when opening the package), or break or slip due to latex degradation (typically from usage past the expiration date, improper storage, or exposure to oils). The rate of breakage is between 0.4% and 2.3%, while the rate of slippage is between 0.6% and 1.3%. Even if no breakage or slippage is observed, 1\u20133% of women will test positive for semen residue after intercourse with a condom. Failure rates are higher for anal sex, and until 2022, condoms were only approved by the FDA for vaginal sex. The One Male Condom received FDA approval for anal sex on 23 February 2022.\nDifferent modes of condom failure result in different levels of semen exposure. If a failure occurs during application, the damaged condom may be disposed of and a new condom applied before intercourse begins\u00a0\u2013 such failures generally pose no risk to the user. One study found that semen exposure from a broken condom was about half that of unprotected intercourse; semen exposure from a slipped condom was about one-fifth that of unprotected intercourse.\nStandard condoms will fit almost any penis, with varying degrees of comfort or risk of slippage. Many condom manufacturers offer \"snug\" or \"magnum\" sizes. Some manufacturers also offer custom sized-to-fit condoms, with claims that they are more reliable and offer improved sensation/comfort. Some studies have associated larger penises and smaller condoms with increased breakage and decreased slippage rates (and vice versa), but other studies have been inconclusive.\nIt is recommended for condoms manufacturers to avoid very thick or very thin condoms, because they are both considered less effective. Some authors encourage users to choose thinner condoms \"for greater durability, sensation, and comfort\", but others warn that \"the thinner the condom, the smaller the force required to break it\".\nExperienced condom users are significantly less likely to have a condom slip or break compared to first-time users, although users who experience one slippage or breakage are more likely to suffer a second such failure. An article in \"Population Reports\" suggests that education on condom use reduces behaviors that increase the risk of breakage and slippage. A Family Health International publication also offers the view that education can reduce the risk of breakage and slippage, but emphasizes that more research needs to be done to determine all of the causes of breakage and slippage.\nAmong people who intend condoms to be their form of birth control, pregnancy may occur when the user has sex without a condom. The person may have run out of condoms, or be traveling and not have a condom with them, or dislike the feel of condoms and decide to \"take a chance\". This behavior is the primary cause of typical use failure (as opposed to method or perfect use failure).\nAnother possible cause of condom failure is sabotage. One motive is to have a child against a partner's wishes or consent. Some commercial sex workers from Nigeria reported clients sabotaging condoms in retaliation for being coerced into condom use. Using a fine needle to make several pinholes at the tip of the condom is believed to significantly impact on their effectiveness. Cases of such condom sabotage have occurred.\nSide effects.\nThe use of latex condoms by people with an allergy to latex can cause allergic symptoms, such as skin irritation. In people with severe latex allergies, using a latex condom can potentially be life-threatening. Repeated use of latex condoms can also cause the development of a latex allergy in some people. Irritation may also occur due to spermicides that may be present.\nUse.\nExternal condoms are usually packaged inside a foil or plastic wrapper, in a rolled-up form, and are designed to be applied to the tip of the penis and then unrolled over the erect penis. It is important that the closed end or the teat of the condom is pinched when the condom is placed on the tip of the penis. This will ensure that air is not trapped inside the condom which could cause it to burst during intercourse. In addition, this leaves space for the semen to collect which reduces the risk of it being forced out of the base of the device. Most condoms have a teat end for this purpose. Soon after ejaculating, the male should withdraw from his partner's body. The condom should then be carefully removed from the penis away from the other partner. It is recommended that the condom be wrapped in tissue or tied in a knot, then disposed of in a trash receptacle. Condoms are used to reduce the likelihood of pregnancy during intercourse and to reduce the likelihood of contracting sexually transmitted infections (STIs). Condoms are also used during fellatio to reduce the likelihood of contracting STIs.\nSome couples find that putting on a condom interrupts sex, although others incorporate condom application as part of their foreplay. Some men and women find the physical barrier of a condom dulls sensation. Advantages of dulled sensation can include prolonged erection and delayed ejaculation; disadvantages might include a loss of some sexual excitement. Advocates of condom use also cite their advantages of being inexpensive, easy to use, and having few side effects.\nAdult film industry.\nIn 2012 proponents gathered 372,000 voter signatures through a citizens' initiative in Los Angeles County to put Measure B on the 2012 ballot. As a result, Measure B, a law requiring the use of condoms in the production of pornographic films, was passed. This requirement has received much criticism and is said by some to be counter-productive, merely forcing companies that make pornographic films to relocate to other places without this requirement. Producers claim that condom use depresses sales.\nSex education.\nCondoms are often used in sex education programs, because they have the capability to reduce the chances of pregnancy and the spread of some sexually transmitted infections when used correctly. A recent American Psychological Association (APA) press release supported the inclusion of information about condoms in sex education, saying \"comprehensive sexuality education programs\u00a0... discuss the appropriate use of condoms\", and \"promote condom use for those who are sexually active.\"\nIn the United States, teaching about condoms in public schools is opposed by some religious organizations. Planned Parenthood, which advocates family planning and sex education, argues that no studies have shown abstinence-only programs to result in delayed intercourse, and cites surveys showing that 76% of American parents want their children to receive comprehensive sexuality education including condom use.\nInfertility treatment.\nCommon procedures in infertility treatment such as semen analysis and intrauterine insemination (IUI) require collection of semen samples. These are most commonly obtained through masturbation, but an alternative to masturbation is use of a special \"collection condom\" to collect semen during sexual intercourse.\nCollection condoms are made from silicone or polyurethane, as latex is somewhat harmful to sperm. Some religions prohibit masturbation entirely. Also, compared with samples obtained from masturbation, semen samples from collection condoms have higher total sperm counts, sperm motility, and percentage of sperm with normal morphology. For this reason, they are believed to give more accurate results when used for semen analysis, and to improve the chances of pregnancy when used in procedures such as intracervical or intrauterine insemination. Adherents of religions that prohibit contraception, such as Catholicism, may use collection condoms with holes pricked in them.\nFor fertility treatments, a collection condom may be used to collect semen during sexual intercourse where the semen is provided by the woman's partner. Private sperm donors may also use a collection condom to obtain samples through masturbation or by sexual intercourse with a partner and will transfer the ejaculate from the collection condom to a specially designed container. The sperm is transported in such containers, in the case of a donor, to a recipient woman to be used for insemination, and in the case of a woman's partner, to a fertility clinic for processing and use. However, transportation may reduce the fecundity of the sperm. Collection condoms may also be used where semen is produced at a sperm bank or fertility clinic.\n\"Condom therapy\" is sometimes prescribed to infertile couples when the female has high levels of antisperm antibodies. The theory is that preventing exposure to her partner's semen will lower her level of antisperm antibodies, and thus increase her chances of pregnancy when condom therapy is discontinued. However, condom therapy has not been shown to increase subsequent pregnancy rates.\nOther uses.\nCondoms excel as multipurpose containers and barriers because they are waterproof, elastic, durable, and (for military and espionage uses) will not arouse suspicion if found.\nOngoing military utilization began during World War II, and includes covering the muzzles of rifle barrels to prevent fouling, the waterproofing of firing assemblies in underwater demolitions, and storage of corrosive materials and garrotes by paramilitary agencies.\nCondoms have also been used to smuggle alcohol, cocaine, heroin, and other drugs across borders and into prisons by filling the condom with drugs, tying it in a knot and then either swallowing it or inserting it into the rectum. These methods are very dangerous and potentially lethal; if the condom breaks, the drugs inside become absorbed into the bloodstream and can cause an overdose.\nMedically, condoms can be used to cover endovaginal ultrasound probes, or in field chest needle decompressions they can be used to make a one-way valve.\nCondoms have also been used to protect scientific samples from the environment, and to waterproof microphones for underwater recording.\nTypes.\nMost condoms have a reservoir tip or teat end, making it easier to accommodate the man's ejaculate. Condoms come in different sizes and shapes.\nThey also come in a variety of surfaces intended to stimulate the user's partner. Condoms are usually supplied with a lubricant coating to facilitate penetration, while flavored condoms are principally used for oral sex. As mentioned above, most condoms are made of latex, but polyurethane and lambskin condoms also exist.\nInternal condom.\nExternal condoms have a tight ring to form a seal around the penis, while internal condoms usually have a large stiff ring to prevent them from slipping into the body orifice. The Female Health Company produced an internal condom that was initially made of polyurethane, but newer versions are made of nitrile rubber. Medtech Products produces an internal condom made of latex.\nMaterials.\nNatural latex.\nLatex has outstanding elastic properties: Its tensile strength exceeds 30\u00a0MPa, and latex condoms may be stretched in excess of 800% before breaking. In 1990 the ISO set standards for condom production (ISO 4074, Natural latex rubber condoms), and the EU followed suit with its CEN standard (Directive 93/42/EEC concerning medical devices). Every latex condom is tested for holes with an electric current. If the condom passes, it is rolled and packaged. In addition, a portion of each batch of condoms is subject to water leak and air burst testing.\nWhile the advantages of latex have made it the most popular condom material, it does have some drawbacks. Latex condoms are damaged when used with oil-based substances as lubricants, such as petroleum jelly, cooking oil, baby oil, mineral oil, skin lotions, suntan lotions, cold creams, butter or margarine. Contact with oil makes latex condoms more likely to break or slip off due to loss of elasticity caused by the oils. Additionally, latex allergy precludes use of latex condoms and is one of the principal reasons for the use of other materials. In May 2009, the U.S. Food and Drug Administration (FDA) granted approval for the production of condoms composed of Vytex, latex that has been treated to remove 90% of the proteins responsible for allergic reactions. An allergen-free condom made of synthetic latex (polyisoprene) is also available.\nSynthetic.\nThe most common non-latex condoms are made from polyurethane. Condoms may also be made from other synthetic materials, such as AT-10 resin, and most polyisoprene.\nPolyurethane condoms tend to be the same width and thickness as latex condoms, with most polyurethane condoms between 0.04\u00a0mm and 0.07\u00a0mm thick.\nPolyurethane can be considered better than latex in several ways: it conducts heat better than latex, is not as sensitive to temperature and ultraviolet light (and so has less rigid storage requirements and a longer shelf life), can be used with oil-based lubricants, is less allergenic than latex, and does not have an odor. Polyurethane condoms have gained FDA approval for sale in the United States as an effective method of contraception and HIV prevention, and under laboratory conditions have been shown to be just as effective as latex for these purposes.\nHowever, polyurethane condoms are less elastic than latex ones, and may be more likely to slip or break than latex, lose their shape or bunch up more than latex, and are more expensive.\nPolyisoprene is a synthetic version of natural rubber latex. While significantly more expensive, it has the advantages of latex (such as being softer and more elastic than polyurethane condoms) without the protein which is responsible for latex allergies. Unlike polyurethane condoms, they cannot be used with an oil-based lubricant.\nLambskin.\nCondoms made from sheep intestines, labeled \"lambskin\", are also available. Although they are generally effective as a contraceptive by blocking sperm, studies have found that they are less effective than latex in preventing the transmission of sexually transmitted infections because of pores in the material. This is because intestines, by their nature, are porous, permeable membranes, and while sperm are too large to pass through the pores, viruses\u2014such as HIV, herpes, and genital warts\u2014are small enough to pass.\nAs a result of laboratory data on condom porosity, in 1989, the FDA began requiring lambskin condom manufacturers to indicate that the products were not to be used for the prevention of sexually transmitted infections. The FDA cautions that while lambskin condoms \"provide good birth control and a varying degree of protection against some, but not all, sexually transmitted diseases\", people do not know what STIs a partner might have, and thus cannot assume that a lambskin condom will protect them.\nWhile lambskin condoms avoid triggering latex allergies, polyurethane condoms do as well, while also protecting more reliably against STIs. As slaughter by-products, lambskin condoms are also not vegetarian. Pharmacist advice prepared by the \"Canadian Pharmaceutical Journal\" says that lambskin condoms \"are generally not recommended\" due to limited STI prevention. An article in \"Adolescent Medicine\" advises that they \"should be used only for pregnancy prevention\".\nSpermicide.\nSome latex condoms are lubricated at the manufacturer with a small amount of a nonoxynol-9, a spermicidal chemical. According to \"Consumer Reports\", condoms lubricated with spermicide have no additional benefit in preventing pregnancy, have a shorter shelf life, and may cause urinary tract infections in women. In contrast, application of separately packaged spermicide \"is\" believed to increase the contraceptive efficacy of condoms.\nNonoxynol-9 was once believed to offer additional protection against STIs (including HIV) but recent studies have shown that, with frequent use, nonoxynol-9 may increase the risk of HIV transmission. The World Health Organization says that spermicidally lubricated condoms should no longer be promoted. However, it recommends using a nonoxynol-9 lubricated condom over no condom at all. , nine condom manufacturers have stopped manufacturing condoms with nonoxynol-9 and Planned Parenthood has discontinued the distribution of condoms so lubricated.\nRibbed and studded.\nTextured condoms include studded and ribbed condoms which can provide extra sensations to both partners. The studs or ribs can be located on the inside, outside, or both; alternatively, they are located in specific sections to provide directed stimulation to either the G-spot or frenulum. Many textured condoms which advertise \"mutual pleasure\" also are bulb-shaped at the top, to provide extra stimulation to the penis. Some women experience irritation during vaginal intercourse with studded condoms.\nOther.\nThe anti-rape condom is another variation designed to be worn by women. It is designed to cause pain to the attacker, hopefully allowing the victim a chance to escape.\nA collection condom is used to collect semen for fertility treatments or sperm analysis. These condoms are designed to maximize sperm life.\nIn February 2022, the U.S. Food and Drug Administration (FDA) approved the first condoms specifically indicated to help reduce transmission of sexually transmitted infections (STIs) during anal intercourse.\nPrevalence.\nThe prevalence of condom use varies greatly between countries. Most surveys of contraceptive use are among married women, or women in informal unions. Japan has the highest rate of condom usage in the world: in that country, condoms account for almost 80% of contraceptive use by married women. On average, in developed countries, condoms are the most popular method of birth control: 28% of married contraceptive users rely on condoms. In the average less-developed country, condoms are less common: only 6\u20138% of married contraceptive users choose condoms.\nHistory.\nBefore the 19th century.\nWhether condoms were used in ancient civilizations is debated by archaeologists and historians. In ancient Egypt, Greece, and Rome, pregnancy prevention was generally seen as a woman's responsibility, and the only well documented contraception methods were female-controlled devices. In Asia before the 15th century, some use of glans condoms (devices covering only the head of the penis) is recorded. Condoms seem to have been used for contraception, and to have been known only by members of the upper classes. In China, glans condoms may have been made of oiled silk paper, or of lamb intestines. In Japan, condoms called \"Kabuto-gata\" (\u7532\u5f62) were made of tortoise shell or animal horn.\nIn 16th-century Italy, anatomist and physician Gabriele Falloppio wrote a treatise on syphilis. The earliest documented strain of syphilis, first appearing in Europe in a 1490s outbreak, caused severe symptoms and often death within a few months of contracting the disease. Falloppio's treatise is the earliest uncontested description of condom use: it describes linen sheaths soaked in a chemical solution and allowed to dry before use. The cloths he described were sized to cover the glans of the penis, and were held on with a ribbon. Falloppio claimed that an experimental trial of the linen sheath demonstrated protection against syphilis.\nAfter this, the use of penis coverings to protect from disease is described in a wide variety of literature throughout Europe. The first indication that these devices were used for birth control, rather than disease prevention, is the 1605 theological publication \"De iustitia et iure\" (On justice and law) by Catholic theologian Leonardus Lessius, who condemned them as immoral. In 1666, the English Birth Rate Commission attributed a recent downward fertility rate to use of \"condons\", the first documented use of that word or any similar spelling. Other early spellings include \"condam\" and \"quondam\", from which the Italian derivation \"guantone\" has been suggested, from \"guanto\", \"a glove\".\nIn addition to linen, condoms during the Renaissance were made out of intestines and bladder. In the late 16th century, Dutch traders introduced condoms made from \"fine leather\" to Japan. Unlike the horn condoms used previously, these leather condoms covered the entire penis.\nCasanova in the 18th century was one of the first reported using \"assurance caps\" to prevent impregnating his mistresses.\nFrom at least the 18th century, condom use was opposed in some legal, religious, and medical circles for essentially the same reasons that are given today: condoms reduce the likelihood of pregnancy, which some thought immoral or undesirable for the nation; they do not provide full protection against sexually transmitted infections, while belief in their protective powers was thought to encourage sexual promiscuity; and, they are not used consistently due to inconvenience, expense, or loss of sensation.\nDespite some opposition, the condom market grew rapidly. In the 18th century, condoms were available in a variety of qualities and sizes, made from either linen treated with chemicals, or \"skin\" (bladder or intestine softened by treatment with sulfur and lye). They were sold at pubs, barbershops, chemist shops, open-air markets, and at the theater throughout Europe and Russia. They later spread to America, although in every place there were generally used only by the middle and upper classes, due to both expense and lack of sex education.\n1800 through 1920s.\nThe early 19th century saw contraceptives promoted to the poorer classes for the first time. Writers on contraception tended to prefer other birth control methods to the condom. By the late 19th century, many feminists expressed distrust of the condom as a contraceptive, as its use was controlled and decided upon by men alone. They advocated instead for methods controlled by women, such as diaphragms and spermicidal douches. Other writers cited both the expense of condoms and their unreliability (they were often riddled with holes and often fell off or tore). Still, they discussed condoms as a good option for some and the only contraceptive that protects from disease.\nMany countries passed laws impeding the manufacture and promotion of contraceptives. In spite of these restrictions, condoms were promoted by traveling lecturers and in newspaper advertisements, using euphemisms in places where such ads were illegal. Instructions on how to make condoms at home were distributed in the United States and Europe. Despite social and legal opposition, at the end of the 19th century the condom was the Western world's most popular birth control method.\nBeginning in the second half of the 19th century, American rates of sexually transmitted infections skyrocketed. Causes cited by historians include the effects of the American Civil War and the ignorance of prevention methods promoted by the Comstock laws. To fight the growing epidemic, sex education classes were introduced to public schools for the first time, teaching about venereal diseases and how they were transmitted. They generally taught abstinence was the only way to avoid sexually transmitted infections. Condoms were not promoted for disease prevention because the medical community and moral watchdogs considered STIs to be punishment for sexual misbehavior. The stigma against people with these diseases was so significant that many hospitals refused to treat people with syphilis.\nThe German military was the first to promote condom use among its soldiers in the later 19th century. Early 20th century experiments by the American military concluded that providing condoms to soldiers significantly lowered rates of sexually transmitted infections. During World War I, the United States and (at the beginning of the war only) Britain were the only countries with soldiers in Europe who did not provide condoms and promote their use.\nIn the decades after World War I, there remained social and legal obstacles to condom use throughout the U.S. and Europe. Founder of psychoanalysis Sigmund Freud opposed all methods of birth control because their failure rates were too high. Freud was especially opposed to the condom because he thought it cut down on sexual pleasure. Some feminists continued to oppose male-controlled contraceptives such as condoms. In 1920 the Church of England's Lambeth Conference condemned all \"unnatural means of conception avoidance\". The Bishop of London, Arthur Winnington-Ingram, complained of the huge number of condoms discarded in alleyways and parks, especially after weekends and holidays.\nHowever, European militaries continued to provide condoms to their members for disease protection, even in countries where they were illegal for the general population. Through the 1920s, catchy names and slick packaging became an increasingly important marketing technique for many consumer items, including condoms and cigarettes. Quality testing became more common, involving filling each condom with air followed by one of several methods intended to detect loss of pressure. Worldwide, condom sales doubled in the 1920s.\nRubber and manufacturing advances.\nIn 1839, Charles Goodyear discovered a way of processing natural rubber, which is too stiff when cold and too soft when warm, in such a way as to make it elastic. This proved to have advantages for the manufacture of condoms; unlike the sheep's gut condoms, they could stretch and did not tear quickly when used. The rubber vulcanization process was patented by Goodyear in 1844. The first rubber condom was produced in 1855. The earliest rubber condoms had a seam and were as thick as a bicycle inner tube. Besides this type, small rubber condoms covering only the glans were often used in England and the United States. There was more risk of losing them and if the rubber ring was too tight, it would constrict the penis. This type of condom was the original \"capote\" (French for condom), perhaps because of its resemblance to a woman's bonnet worn at that time, also called a capote.\nFor many decades, rubber condoms were manufactured by wrapping strips of raw rubber around penis-shaped molds, then dipping the wrapped molds in a chemical solution to cure the rubber. In 1912, Polish-born inventor Julius Fromm developed a new, improved manufacturing technique for condoms: dipping glass molds into a raw rubber solution. Called \"cement dipping\", this method required adding gasoline or benzene to the rubber to make it liquid.\nAround 1920 patent lawyer and vice-president of the United States Rubber Company Ernest Hopkinson invented a new technique of converting latex into rubber without a coagulant (demulsifier), which featured using water as a solvent and warm air to dry the solution, as well as optionally preserving liquid latex with ammonia. Condoms made this way, commonly called \"latex\" ones, required less labor to produce than cement-dipped rubber condoms, which had to be smoothed by rubbing and trimming. The use of water to suspend the rubber instead of gasoline and benzene eliminated the fire hazard previously associated with all condom factories. Latex condoms also performed better for the consumer: they were stronger and thinner than rubber condoms, and had a shelf life of five years (compared to three months for rubber).\nUntil the twenties, all condoms were individually hand-dipped by semi-skilled workers. Throughout the decade of the 1920s, advances in the automation of the condom assembly line were made. The first fully automated line was patented in 1930. Major condom manufacturers bought or leased conveyor systems, and small manufacturers were driven out of business. The skin condom, now significantly more expensive than the latex variety, became restricted to a niche high-end market.\n1930 to present.\nIn 1930 the Anglican Church's sanctioned the use of birth control by married couples. In 1931 the Federal Council of Churches in the U.S. issued a similar statement. The Roman Catholic Church responded by issuing the encyclical \"Casti connubii\" affirming its opposition to all contraceptives, a stance it has never reversed. In the 1930s, legal restrictions on condoms began to be relaxed. However, during this period Fascist Italy and Nazi Germany increased restrictions on condoms (limited sales as disease preventatives were still allowed). During the Depression, condom lines by Schmid gained in popularity. Schmid still used the cement-dipping method of manufacture which had two advantages over the latex variety. Firstly, cement-dipped condoms could be safely used with oil-based lubricants. Secondly, while less comfortable, these older-style rubber condoms could be reused and so were more economical, a valued feature in hard times. More attention was brought to quality issues in the 1930s, and the U.S. Food and Drug Administration began to regulate the quality of condoms sold in the United States.\nThroughout World War II, condoms were not only distributed to male U.S. military members, but also heavily promoted with films, posters, and lectures. European and Asian militaries on both sides of the conflict also provided condoms to their troops throughout the war, even Germany which outlawed all civilian use of condoms in 1941. In part because condoms were readily available, soldiers found a number of non-sexual uses for the devices, many of which continue to this day. After the war, condom sales continued to grow. From 1955 to 1965, 42% of Americans of reproductive age relied on condoms for birth control. In Britain from 1950 to 1960, 60% of married couples used condoms. The birth control pill became the world's most popular method of birth control in the years after its 1960 d\u00e9but, but condoms remained a strong second. The U.S. Agency for International Development pushed condom use in developing countries to help solve the \"world population crises\": by 1970 hundreds of millions of condoms were being used each year in India alone.(This number has grown in recent decades: in 2004, the government of India purchased 1.9 billion condoms for distribution at family planning clinics.)\nIn the 1960s and 1970s quality regulations tightened, and more legal barriers to condom use were removed. In Ireland, legal condom sales were allowed for the first time in 1978. Advertising, however was one area that continued to have legal restrictions. In the late 1950s, the American National Association of Broadcasters banned condom advertisements from national television; this policy remained in place until 1979.\nAfter it was discovered in the early 1980s that AIDS can be a sexually transmitted infection, the use of condoms was encouraged to prevent transmission of HIV. Despite opposition by some political, religious, and other figures, national condom promotion campaigns occurred in the U.S. and Europe. These campaigns increased condom use significantly.\nDue to increased demand and greater social acceptance, condoms began to be sold in a wider variety of retail outlets, including in supermarkets and in discount department stores such as Walmart. Condom sales increased every year until 1994, when media attention to the AIDS pandemic began to decline. The phenomenon of decreasing use of condoms as disease preventatives has been called \"prevention fatigue\" or \"condom fatigue\". Observers have cited condom fatigue in both Europe and North America. As one response, manufacturers have changed the tone of their advertisements from scary to humorous.\nNew developments continued to occur in the condom market, with the first polyurethane condom\u2014branded Avanti and produced by the manufacturer of Durex\u2014introduced in the 1990s. Worldwide condom use is expected to continue to grow: one study predicted that developing nations would need 18.6 billion condoms by 2015. , condoms are available inside prisons in Canada, most of the European Union, Australia, Brazil, Indonesia, South Africa, and the US states of Vermont (on 17 September 2013, the Californian Senate approved a bill for condom distribution inside the state's prisons, but the bill was not yet law at the time of approval).\nThe global condom market was estimated at US$9.2 billion in 2020.\nEtymology and other terms.\nThe term \"condom\" first appears in the early 18th century: early forms include \"condum\" (1706 and 1717), \"condon\" (1708) and \"cundum\" (1744). The word's etymology is unknown. In popular tradition, the invention and naming of the condom came to be attributed to an associate of England's King Charles II, one \"Dr. Condom\" or \"Earl of Condom\". There is however no evidence of the existence of such a person, and condoms had been used for over one hundred years before King Charles II acceded to the throne in 1660.\nA variety of unproven Latin etymologies have been proposed, including (receptacle), (house), and (scabbard or case). It has also been speculated to be from the Italian word \"guantone\", derived from \"guanto\", meaning glove. William E. Kruck wrote an article in 1981 concluding that, \"As for the word 'condom', I need state only that its origin remains completely unknown, and there ends this search for an etymology.\" Modern dictionaries may also list the etymology as \"unknown\".\nOther terms are also commonly used to describe condoms. In North America condoms are also commonly known as \"prophylactics\", or \"rubbers\". In Britain they may be called \"French letters\" or \"rubber johnnies\". Additionally, condoms may be referred to using the manufacturer's name.\nSociety and culture.\nSome moral and scientific criticism of condoms exists despite the many benefits of condoms agreed on by scientific consensus and sexual health experts.\nCondom usage is typically recommended for new couples who have yet to develop full trust in their partner with regard to STIs. Established couples on the other hand have few concerns about STIs, and can use other methods of birth control such as the pill, which does not act as a barrier to intimate sexual contact. Note that the polar debate with regard to condom usage is attenuated by the target group the argument is directed. Notably the age category and stable partner question are factors, as well as the distinction between heterosexual and homosexuals, who have different kinds of sex and have different risk consequences and factors.\nAmong the prime objections to condom usage is the blocking of erotic sensation, or the intimacy that barrier-free sex provides. As the condom is held tightly to the skin of the penis, it diminishes the delivery of stimulation through rubbing and friction. Condom proponents claim this has the benefit of making sex last longer, by diminishing sensation and delaying male ejaculation. Those who promote condom-free heterosexual sex (slang: \"bareback\") claim that the condom puts a barrier between partners, diminishing what is normally a highly sensual, intimate, and spiritual connection between partners.\nReligious.\nThe United Church of Christ (UCC), a Reformed denomination of the Congregationalist tradition, promotes the distribution of condoms in churches and faith-based educational settings. Michael Shuenemeyer, a UCC minister, has stated that \"The practice of safer sex is a matter of life and death. People of faith make condoms available because we have chosen life so that we and our children may live.\"\nOn the other hand, the Roman Catholic Church opposes all kinds of sexual acts outside of marriage, as well as any sexual act in which the chance of successful conception has been reduced by direct and intentional acts (for example, surgery to prevent conception) or foreign objects (for example, condoms).\nThe use of condoms to prevent STI transmission is not specifically addressed by Catholic doctrine, and is currently a topic of debate among theologians and high-ranking Catholic authorities. A few, such as Belgian Cardinal Godfried Danneels, believe the Catholic Church should actively support condoms used to prevent disease, especially serious diseases such as AIDS. However, the majority view\u2014including all statements from the Vatican\u2014is that condom-promotion programs encourage promiscuity, thereby actually increasing STI transmission. This view was most recently reiterated in 2009 by Pope Benedict XVI.\nThe Roman Catholic Church is the largest organized body of any world religion. The church has hundreds of programs dedicated to fighting the AIDS epidemic in Africa, but its opposition to condom use in these programs has been highly controversial.\nIn a November 2011 interview, Pope Benedict XVI discussed for the first time the use of condoms to prevent STI transmission. He said that the use of a condom can be justified in a few individual cases if the purpose is to reduce the risk of an HIV infection. He gave as an example male prostitutes. There was some confusion at first whether the statement applied only to homosexual prostitutes and thus not to heterosexual intercourse at all. However, Federico Lombardi, spokesman for the Vatican, clarified that it applied to heterosexual and transsexual prostitutes, whether male or female, as well. He did, however, also clarify that the Vatican's principles on sexuality and contraception had not been changed.\nScientific and environmental.\nMore generally, some scientific researchers have expressed objective concern over certain ingredients sometimes added to condoms, notably talc and nitrosamines. Dry dusting powders are applied to latex condoms before packaging to prevent the condom from sticking to itself when rolled up. Previously, talc was used by most manufacturers, but cornstarch is currently the most popular dusting powder. Although rare during normal use, talc is known to be potentially irritant to mucous membranes (such as in the vagina). Cornstarch is generally believed to be safe; however, some researchers have raised concerns over its use as well.\nNitrosamines, which are potentially carcinogenic in humans, are believed to be present in a substance used to improve elasticity in latex condoms. A 2001 review stated that humans regularly receive 1,000 to 10,000 times greater nitrosamine exposure from food and tobacco than from condom use and concluded that the risk of cancer from condom use is very low. However, a 2004 study in Germany detected nitrosamines in 29 out of 32 condom brands tested, and concluded that exposure from condoms might exceed the exposure from food by 1.5- to 3-fold.\nIn addition, the large-scale use of disposable condoms has resulted in concerns over their environmental impact via littering and in landfills, where they can eventually wind up in wildlife environments if not incinerated or otherwise permanently disposed of first. Polyurethane condoms in particular, given they are a form of plastic, are not biodegradable, and latex condoms take a very long time to break down. Experts, such as AVERT, recommend condoms be disposed of in a garbage receptacle, as flushing them down the toilet (which some people do) may cause plumbing blockages and other problems. Furthermore, the plastic and foil wrappers condoms are packaged in are also not biodegradable. However, the benefits condoms offer are widely considered to offset their small landfill mass. Frequent condom or wrapper disposal in public areas such as a parks have been seen as a persistent litter problem.\nWhile biodegradable, latex condoms damage the environment when disposed of improperly. According to the Ocean Conservancy, condoms, along with certain other types of trash, cover the coral reefs and smother sea grass and other bottom dwellers. The United States Environmental Protection Agency also has expressed concerns that many animals might mistake the litter for food.\nCultural barriers to use.\nIn much of the Western world, the introduction of the pill in the 1960s was associated with a decline in condom use. In Japan, oral contraceptives were not approved for use until September 1999, and even then access was more restricted than in other industrialized nations. Perhaps because of this restricted access to hormonal contraception, Japan has the highest rate of condom usage in the world: in 2008, 80% of contraceptive users relied on condoms.\nCultural attitudes toward gender roles, contraception, and sexual activity vary greatly around the world, and range from extremely conservative to extremely liberal. But in places where condoms are misunderstood, mischaracterised, demonised, or looked upon with overall cultural disapproval, the prevalence of condom use is directly affected. In less-developed countries and among less-educated populations, misperceptions about how disease transmission and conception work negatively affect the use of condoms; additionally, in cultures with more traditional gender roles, women may feel uncomfortable demanding that their partners use condoms.\nAs an example, Latino immigrants in the United States often face cultural barriers to condom use. A study on female HIV prevention published in the \"Journal of Sex Health Research\" asserts that Latino women often lack the attitudes needed to negotiate safe sex due to traditional gender-role norms in the Latino community, and may be afraid to bring up the subject of condom use with their partners. Women who participated in the study often reported that because of the general machismo subtly encouraged in Latino culture, their male partners would be angry or possibly violent at the woman's suggestion that they use condoms. A similar phenomenon has been noted in a survey of low-income American black women; the women in this study also reported a fear of violence at the suggestion to their male partners that condoms be used.\nA telephone survey conducted by Rand Corporation and Oregon State University, and published in the \"Journal of Acquired Immune Deficiency Syndromes\" showed that belief in AIDS conspiracy theories among United States black men is linked to rates of condom use. As conspiracy beliefs about AIDS grow in a given sector of these black men, consistent condom use drops in that same sector. Female use of condoms was not similarly affected.\nIn the African continent, condom promotion in some areas has been impeded by anti-condom campaigns by some Muslim and Catholic clerics. Among the Maasai in Tanzania, condom use is hampered by an aversion to \"wasting\" sperm, which is given sociocultural importance beyond reproduction. Sperm is believed to be an \"elixir\" to women and to have beneficial health effects. Maasai women believe that, after conceiving a child, they must have sexual intercourse repeatedly so that the additional sperm aids the child's development. Frequent condom use is also considered by some Maasai to cause impotence. Some women in Africa believe that condoms are \"for prostitutes\" and that respectable women should not use them. A few clerics even promote the lie that condoms are deliberately laced with HIV. In the United States, possession of many condoms has been used by police to accuse women of engaging in prostitution. The Presidential Advisory Council on HIV/AIDS has condemned this practice and there are efforts to end it.\nMiddle-Eastern couples who have not had children, because of the strong desire and social pressure to establish fertility as soon as possible within marriage, rarely use condoms.\nIn 2017, India restricted TV advertisements for condoms to between the hours of 10 pm to 6 am. Family planning advocates were against this, saying it was liable to \"undo decades of progress on sexual and reproductive health\".\nMajor manufacturers.\nOne analyst described the size of the condom market as something that \"boggles the mind\". Numerous small manufacturers, nonprofit groups, and government-run manufacturing plants exist around the world. Within the condom market, there are several major contributors, among them both for-profit businesses and philanthropic organizations. Most large manufacturers have ties to the business that reach back to the end of the 19th century.\nResearch.\nA spray-on condom made of latex is intended to be easier to apply and more successful in preventing the transmission of diseases. , the spray-on condom was not going to market because the drying time could not be reduced below two to three minutes.\nThe Invisible Condom, developed at Universit\u00e9 Laval in Quebec, Canada, is a gel that hardens upon increased temperature after insertion into the vagina or rectum. In the lab, it has been shown to effectively block HIV and herpes simplex virus. The barrier breaks down and liquefies after several hours. , the invisible condom is in the clinical trial phase, and has not yet been approved for use.\nAlso developed in 2005 is a condom treated with an erectogenic compound. The drug-treated condom is intended to help the wearer maintain an erection, which should also help reduce slippage. If approved, the condom would be marketed under the Durex brand. , it was still in clinical trials. In 2009, Ansell Healthcare, the makers of Lifestyle condoms, introduced the X2 condom lubricated with \"Excite Gel\" which contains the amino acid L-arginine and is intended to improve the strength of the erectile response.\nIn March 2013, philanthropist Bill Gates offered US$100,000 grants through his foundation for a condom design that \"significantly preserves or enhances pleasure\" to encourage more males to adopt the use of condoms for safer sex. The grant information stated: \"The primary drawback from the male perspective is that condoms decrease pleasure as compared to no condom, creating a trade-off that many men find unacceptable, particularly given that the decisions about use must be made just prior to intercourse. Is it possible to develop a product without this stigma, or better, one that is felt to enhance pleasure?\" In November of the same year, 11 research teams were selected to receive the grant money."}
{"id": "5375", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=5375", "title": "Country code", "text": "A country code is a short alphanumeric identification code for countries and dependent areas. Its primary use is in data processing and communications. Several identification systems have been developed.\nThe term \"country code\" frequently refers to ISO 3166-1 alpha-2, as well as the telephone country code, which is embodied in the E.164 recommendation by the International Telecommunication Union (ITU).\nISO 3166-1.\nThe standard ISO 3166-1 defines short identification codes for most countries and dependent areas:\nThe two-letter codes are used as the basis for other codes and applications, for example,\nOther applications are defined in ISO 3166-1 alpha-2.\nITU country codes.\nIn telecommunications, a \"country code\", or \"international subscriber dialing\" (ISD) code, is a telephone number prefix used in international direct dialing (IDD) and for destination routing of telephone calls to a country other than the caller's. A country or region with an autonomous telephone administration must apply for membership in the International Telecommunication Union (ITU) to participate in the international public switched telephone network (PSTN). County codes are defined by the ITU-T section of the ITU in standards E.123 and E.164.\nCountry codes constitute the international telephone numbering plan, and are dialed only when calling a telephone number in another country. They are dialed before the national telephone number. International calls require at least one additional prefix to be dialing before the country code, to connect the call to international circuits, the international call prefix. When printing telephone numbers this is indicated by a plus-sign (\"+\") in front of a complete international telephone number, per recommendation E164 by the ITU.\nHistory.\nThe International Telecommunications Union (ITU) created country codes for international dialing, first introduced in 1960 for Europe and expanded globally in 1964. Numbers were typically allocated by landmass and then subdivided by the capacity of each network at the time. France, the United Kingdom, the USA and USSR obtained preferential numbers due to their dominance in telecommunications at the time whilst China was able to ensure that Taiwan was officially left unlisted whilst being allocated the code \"886\". \nOther country codes.\nThe developers of ISO 3166 intended that in time it would replace other coding systems.\nOther codings.\nCountry identities may be encoded in the following coding systems:\nLists of country codes by country.\n -\n -\n -\n -\n -\n -\n -"}
{"id": "5376", "revid": "39317500", "url": "https://en.wikipedia.org/wiki?curid=5376", "title": "Cladistics", "text": "Cladistics ( ; from Ancient Greek 'branch') is an approach to biological classification in which organisms are categorized in groups (\"clades\") based on hypotheses of most recent common ancestry. The evidence for hypothesized relationships is typically shared derived characteristics (synapomorphies) that are not present in more distant groups and ancestors. However, from an empirical perspective, common ancestors are inferences based on a cladistic hypothesis of relationships of taxa whose character states can be observed. Theoretically, a last common ancestor and all its descendants constitute a (minimal) clade. Importantly, all descendants stay in their overarching ancestral clade. For example, if the terms \"worms\" or \"fishes\" were used within a \"strict\" cladistic framework, these terms would include humans. Many of these terms are normally used paraphyletically, outside of cladistics, e.g. as a 'grade', which are fruitless to precisely delineate, especially when including extinct species. Radiation results in the generation of new subclades by bifurcation, but in practice sexual hybridization may blur very closely related groupings.\nAs a hypothesis, a clade can be rejected only if some groupings were explicitly excluded. It may then be found that the excluded group did actually descend from the last common ancestor of the group, and thus emerged within the group. (\"Evolved from\" is misleading, because in cladistics all descendants stay in the ancestral group). To keep only valid clades, upon finding that the group is paraphyletic this way, either such excluded groups should be granted to the clade, or the group should be abolished.\nBranches down to the divergence to the next significant (e.g. extant) sister are considered stem-groupings of the clade, but in principle each level stands on its own, to be assigned a unique name. For a fully bifurcated tree, adding a group to a tree also adds an additional (named) clade, and a new level on that branch. Specifically, also extinct groups are always put on a side-branch, not distinguishing whether an actual ancestor of other groupings was found.\nThe techniques and nomenclature of cladistics have been applied to disciplines other than biology. (See phylogenetic nomenclature.)\nCladistics findings are posing a difficulty for taxonomy, where the rank and (genus-)naming of established groupings may turn out to be inconsistent.\nCladistics is now the most commonly used method to classify organisms.\nHistory.\nThe original methods used in cladistic analysis and the school of taxonomy derived from the work of the German entomologist Willi Hennig, who referred to it as phylogenetic systematics (also the title of his 1966 book); but the terms \"cladistics\" and \"clade\" were popularized by other researchers. Cladistics in the original sense refers to a particular set of methods used in phylogenetic analysis, although it is now sometimes used to refer to the whole field.\nWhat is now called the cladistic method appeared as early as 1901 with a work by Peter Chalmers Mitchell for birds and subsequently by Robert John Tillyard (for insects) in 1921, and W. Zimmermann (for plants) in 1943. The term \"clade\" was introduced in 1958 by Julian Huxley after having been coined by Lucien Cu\u00e9not in 1940, \"cladogenesis\" in 1958, \"cladistic\" by Arthur Cain and Harrison in 1960, \"cladist\" (for an adherent of Hennig's school) by Ernst Mayr in 1965, and \"cladistics\" in 1966. Hennig referred to his own approach as \"phylogenetic systematics\". From the time of his original formulation until the end of the 1970s, cladistics competed as an analytical and philosophical approach to systematics with phenetics and so-called evolutionary taxonomy. Phenetics was championed at this time by the numerical taxonomists Peter Sneath and Robert Sokal, and evolutionary taxonomy by Ernst Mayr.\nOriginally conceived, if only in essence, by Willi Hennig in a book published in 1950, cladistics did not flourish until its translation into English in 1966 (Lewin 1997). Today, cladistics is the most popular method for inferring phylogenetic trees from morphological data.\nIn the 1990s, the development of effective polymerase chain reaction techniques allowed the application of cladistic methods to biochemical and molecular genetic traits of organisms, vastly expanding the amount of data available for phylogenetics. At the same time, cladistics rapidly became popular in evolutionary biology, because computers made it possible to process large quantities of data about organisms and their characteristics.\nMethodology.\nThe cladistic method interprets each shared character state transformation as a potential piece of evidence for grouping. Synapomorphies (shared, derived character states) are viewed as evidence of grouping, while symplesiomorphies (shared ancestral character states) are not. The outcome of a cladistic analysis is a cladogram \u2013 a tree-shaped diagram (dendrogram) that is interpreted to represent the best hypothesis of phylogenetic relationships. Although traditionally such cladograms were generated largely on the basis of morphological characters and originally calculated by hand, genetic sequencing data and computational phylogenetics are now commonly used in phylogenetic analyses, and the parsimony criterion has been abandoned by many phylogeneticists in favor of more \"sophisticated\" but less parsimonious evolutionary models of character state transformation. Cladists contend that these models are unjustified because there is no evidence that they recover more \"true\" or \"correct\" results from actual empirical data sets \nEvery cladogram is based on a particular dataset analyzed with a particular method. Datasets are tables consisting of molecular, morphological, ethological and/or other characters and a list of operational taxonomic units (OTUs), which may be genes, individuals, populations, species, or larger taxa that are presumed to be monophyletic and therefore to form, all together, one large clade; phylogenetic analysis infers the branching pattern within that clade. Different datasets and different methods, not to mention violations of the mentioned assumptions, often result in different cladograms. Only scientific investigation can show which is more likely to be correct.\nUntil recently, for example, cladograms like the following have generally been accepted as accurate representations of the ancestral relations among turtles, lizards, crocodilians, and birds:\nIf this phylogenetic hypothesis is correct, then the last common ancestor of turtles and birds, at the branch near the lived earlier than the last common ancestor of lizards and birds, near the . Most molecular evidence, however, produces cladograms more like this:\nIf this is accurate, then the last common ancestor of turtles and birds lived later than the last common ancestor of lizards and birds. Since the cladograms show two mutually exclusive hypotheses to describe the evolutionary history, at most one of them is correct.\nThe cladogram to the right represents the current universally accepted hypothesis that all primates, including strepsirrhines like the lemurs and lorises, had a common ancestor all of whose descendants are or were primates, and so form a clade; the name Primates is therefore recognized for this clade. Within the primates, all anthropoids (monkeys, apes, and humans) are hypothesized to have had a common ancestor all of whose descendants are or were anthropoids, so they form the clade called Anthropoidea. The \"prosimians\", on the other hand, form a paraphyletic taxon. The name Prosimii is not used in phylogenetic nomenclature, which names only clades; the \"prosimians\" are instead divided between the clades Strepsirhini and Haplorhini, where the latter contains Tarsiiformes and Anthropoidea.\nLemurs and tarsiers may have looked closely related to humans, in the sense of being close on the evolutionary tree to humans. However, from the perspective of a tarsier, humans and lemurs would have looked close, in the exact same sense. Cladistics forces a neutral perspective, treating all branches (extant or extinct) in the same manner. It also forces one to try to make statements, and honestly take into account findings, about the exact historic relationships between the groups.\nTerminology for character states.\nThe following terms, coined by Hennig, are used to identify shared or distinct character states among groups:\nThe terms plesiomorphy and apomorphy are relative; their application depends on the position of a group within a tree. For example, when trying to decide whether the tetrapods form a clade, an important question is whether having four limbs is a synapomorphy of the earliest taxa to be included within Tetrapoda: did all the earliest members of the Tetrapoda inherit four limbs from a common ancestor, whereas all other vertebrates did not, or at least not homologously? By contrast, for a group within the tetrapods, such as birds, having four limbs is a plesiomorphy. Using these two terms allows a greater precision in the discussion of homology, in particular allowing clear expression of the hierarchical relationships among different homologous features.\nIt can be difficult to decide whether a character state is in fact the same and thus can be classified as a synapomorphy, which may identify a monophyletic group, or whether it only appears to be the same and is thus a homoplasy, which cannot identify such a group. There is a danger of circular reasoning: assumptions about the shape of a phylogenetic tree are used to justify decisions about character states, which are then used as evidence for the shape of the tree. Phylogenetics uses various forms of parsimony to decide such questions; the conclusions reached often depend on the dataset and the methods. Such is the nature of empirical science, and for this reason, most cladists refer to their cladograms as hypotheses of relationship. Cladograms that are supported by a large number and variety of different kinds of characters are viewed as more robust than those based on more limited evidence.\nTerminology for taxa.\nMono-, para- and polyphyletic taxa can be understood based on the shape of the tree (as done above), as well as based on their character states. These are compared in the table below.\nCriticism.\nCladistics, either generally or in specific applications, has been criticized from its beginnings. Decisions as to whether particular character states are homologous, a precondition of their being synapomorphies, have been challenged as involving circular reasoning and subjective judgements. Of course, the potential unreliability of evidence is a problem for any systematic method, or for that matter, for any empirical scientific endeavor at all.\nTransformed cladistics arose in the late 1970s in an attempt to resolve some of these problems by removing a priori assumptions about phylogeny from cladistic analysis, but it has remained unpopular.\nIssues.\nAncestors.\nThe cladistic method does not identify fossil species as actual ancestors of a clade. Instead, fossil taxa are identified as belonging to separate extinct branches. While a fossil species could be the actual ancestor of a clade, there is no way to know that. Therefore, a more conservative hypothesis is that the fossil taxon is related to other fossil and extant taxa, as implied by the pattern of shared apomorphic features.\nExtinction status.\nAn otherwise extinct group with any extant descendants, is not considered (literally) extinct, and for instance does not have a date of extinction.\nHybridization, interbreeding.\nAnything having to do with biology and sex is complicated and messy, and cladistics is no exception. Many species reproduce sexually, and are capable of interbreeding for millions of years. Worse, during such a period, many branches may have radiated, and it may take hundreds of millions of years for them to have whittled down to just two. Only then one can theoretically assign proper last common ancestors of groupings which do not inadvertently include earlier branches. The process of true cladistic bifurcation can thus take a much more extended time than one is usually aware of. In practice, for recent radiations, cladistically guided findings only give a coarse impression of the complexity. A more detailed account will give details about fractions of introgressions between groupings, and even geographic variations thereof. This has been used as an argument for the use of paraphyletic groupings, but typically other reasons are quoted.\nHorizontal gene transfer.\nHorizontal gene transfer is the mobility of genetic info between different organisms that can have immediate or delayed effects for the reciprocal host. There are several processes in nature which can cause horizontal gene transfer. This does typically not directly interfere with ancestry of the organism, but can complicate the determination of that ancestry. On another level, one can map the horizontal gene transfer processes, by determining the phylogeny of the individual genes using cladistics.\nNaming stability.\nIf there is unclarity in mutual relationships, there are a lot of possible trees. Assigning names to each possible clade may not be prudent. Furthermore, established names are discarded in cladistics, or alternatively carry connotations which may no longer hold, such as when additional groups are found to have emerged in them. Naming changes are the direct result of changes in the recognition of mutual relationships, which often is still in flux, especially for extinct species. Hanging on to older naming and/or connotations is counter-productive, as they typically do not reflect actual mutual relationships precisely at all. E.g. Archaea, Asgard archaea, protists, slime molds, worms, invertebrata, fishes, reptilia, monkeys, \"Ardipithecus\", \"Australopithecus\", \"Homo erectus\" all contain \"Homo sapiens\" cladistically, in their \"sensu lato\" meaning. For originally extinct stem groups, \"sensu lato\" generally means generously keeping previously included groups, which then may come to include even living species. A pruned \"sensu stricto\" meaning is often adopted instead, but the group would need to be restricted to a single branch on the stem. Other branches then get their own name and level. This is commensurate to the fact that more senior stem branches are in fact closer related to the resulting group than the more basal stem branches; that those stem branches only may have lived for a short time does not affect that assessment in cladistics.\nIn disciplines other than biology.\nThe comparisons used to acquire data on which cladograms can be based are not limited to the field of biology. Any group of individuals or classes that are hypothesized to have a common ancestor, and to which a set of common characteristics may or may not apply, can be compared pairwise. Cladograms can be used to depict the hypothetical descent relationships within groups of items in many different academic realms. The only requirement is that the items have characteristics that can be identified and measured.\nAnthropology and archaeology: Cladistic methods have been used to reconstruct the development of cultures or artifacts using groups of cultural traits or artifact features.\nComparative mythology and folktale use cladistic methods to reconstruct the protoversion of many myths. Mythological phylogenies constructed with mythemes clearly support low horizontal transmissions (borrowings), historical (sometimes Palaeolithic) diffusions and punctuated evolution. They also are a powerful way to test hypotheses about cross-cultural relationships among folktales.\nLiterature: Cladistic methods have been used in the classification of the surviving manuscripts of the \"Canterbury Tales\", and the manuscripts of the Sanskrit \"Charaka Samhita\".\nHistorical linguistics: Cladistic methods have been used to reconstruct the phylogeny of languages using linguistic features. This is similar to the traditional comparative method of historical linguistics, but is more explicit in its use of parsimony and allows much faster analysis of large datasets (computational phylogenetics).\nTextual criticism or stemmatics: Cladistic methods have been used to reconstruct the phylogeny of manuscripts of the same work (and reconstruct the lost original) using distinctive copying errors as apomorphies. This differs from traditional historical-comparative linguistics in enabling the editor to evaluate and place in genetic relationship large groups of manuscripts with large numbers of variants that would be impossible to handle manually. It also enables parsimony analysis of contaminated traditions of transmission that would be impossible to evaluate manually in a reasonable period of time.\nAstrophysics infers the history of relationships between galaxies to create branching diagram hypotheses of galaxy diversification."}
{"id": "5377", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=5377", "title": "Calendar", "text": "A calendar is a system of organizing days. This is done by giving names to periods of time, typically days, weeks, months and years. A date is the designation of a single and specific day within such a system. A calendar is also a physical record (often paper) of such a system. A calendar can also mean a list of planned events, such as a court calendar, or a partly or fully chronological list of documents, such as a calendar of wills.\nPeriods in a calendar (such as years and months) are usually, though not necessarily, synchronized with the cycle of the sun or the moon. The most common type of pre-modern calendar was the lunisolar calendar, a lunar calendar that occasionally adds one intercalary month to remain synchronized with the solar year over the long term.\nEtymology.\nThe term \"calendar\" is taken from , the term for the first day of the month in the Roman calendar, related to the verb 'to call out', referring to the \"calling\" of the new moon when it was first seen. Latin meant 'account book, register' (as accounts were settled and debts were collected on the calends of each month). The Latin term was adopted in Old French as and from there in Middle English as by the 13th\u00a0century (the spelling \"calendar\" is early modern).\nHistory.\nThe course of the Sun and the Moon are the most salient regularly recurring natural events useful for timekeeping, and in pre-modern societies around the world lunation and the year were most commonly used as time units. Nevertheless, the Roman calendar contained remnants of a very ancient pre-Etruscan 10-month solar year.\nThe first recorded physical calendars, dependent on the development of writing in the Ancient Near East, are the Bronze Age Egyptian and Sumerian calendars.\nDuring the Vedic period India developed a sophisticated timekeeping methodology and calendars for Vedic rituals. According to Yukio Ohashi, the Vedanga calendar in ancient India was based on astronomical studies during the Vedic Period and was not derived from other cultures.\nA large number of calendar systems in the Ancient Near East were based on the Babylonian calendar dating from the Iron Age, among them the calendar system of the Persian Empire, which in turn gave rise to the Zoroastrian calendar and the Hebrew calendar.\nA great number of Hellenic calendars were developed in Classical Greece, and during the Hellenistic period they gave rise to the ancient Roman calendar and to various Hindu calendars.\nCalendars in antiquity were lunisolar, depending on the introduction of intercalary months to align the solar and the lunar years. This was mostly based on observation, but there may have been early attempts to model the pattern of intercalation algorithmically, as evidenced in the fragmentary 2nd-century Coligny calendar.\nThe Roman calendar was reformed by Julius Caesar in 46\u00a0BC. His \"Julian\" calendar was no longer dependent on the observation of the new moon, but followed an algorithm of introducing a leap day every four years. This created a dissociation of the calendar month from lunation. The Gregorian calendar, introduced in 1582, corrected most of the remaining difference between the Julian calendar and the solar year.\nThe Islamic calendar is based on the prohibition of intercalation (\"nasi\"') by Muhammad, in Islamic tradition dated to a sermon given on 9 Dhu al-Hijjah AH\u00a010 (Julian date: 6 March 632). This resulted in an observation-based lunar calendar that shifts relative to the seasons of the solar year.\nThere have been several modern proposals for reform of the modern calendar, such as the World Calendar, the International Fixed Calendar, the Holocene calendar, and the Hanke\u2013Henry Permanent Calendar. Such ideas are promoted from time to time, but have failed to gain traction because of the loss of continuity and the massive upheaval that implementing them would involve, as well as their effect on cycles of religious activity.\nSystems.\nA full calendar system has a different calendar date for every day. Thus the week cycle is by itself not a full calendar system; neither is a system to name the days within a year without a system for identifying the years.\nThe simplest calendar system just counts time periods from a reference date. This applies for the Julian day or Unix Time. Virtually the only possible variation is using a different reference date, in particular, one less distant in the past to make the numbers smaller. Computations in these systems are just a matter of addition and subtraction.\nOther calendars have one (or multiple) larger units of time.\nCalendars that contain one level of cycles:\nCalendars with two levels of cycles:\nCycles can be synchronized with periodic phenomena:\nVery commonly a calendar includes more than one type of cycle or has both cyclic and non-cyclic elements.\nMost calendars incorporate more complex cycles. For example, the vast majority of them track years, months, weeks and days. The seven-day week is practically universal, though its use varies. It has run uninterrupted for millennia.\nSolar.\nSolar calendars assign a \"date\" to each solar day. A day may consist of the period between sunrise and sunset, with a following period of night, or it may be a period between successive events such as two sunsets. The length of the interval between two such successive events may be allowed to vary slightly during the year, or it may be averaged into a mean solar day. Other types of calendar may also use a solar day.\nThe Egyptians appear to have been the first to develop a solar calendar, using as a fixed point the annual sunrise reappearance of the Dog Star\u2014Sirius, or Sothis\u2014in the eastern sky, which coincided with the annual flooding of the Nile River. They built a calendar with 365 days, divided into 12 months of 30 days each, with 5 extra days at the end of the year. However, they did not include the extra bit of time in each year, and this caused their calendar to slowly become inaccurate.\nLunar.\nNot all calendars use the solar year as a unit. A lunar calendar is one in which days are numbered within each lunar phase cycle. Because the length of the lunar month is not an even fraction of the length of the tropical year, a purely lunar calendar quickly drifts against the seasons, which do not vary much near the equator. It does, however, stay constant with respect to other phenomena, notably tides. An example is the Islamic calendar.\nAlexander Marshack, in a controversial reading, believed that marks on a bone baton () represented a lunar calendar. Other marked bones may also represent lunar calendars. Similarly, Michael Rappenglueck believes that marks on a 15,000-year-old cave painting represent a lunar calendar.\nLunisolar.\nA lunisolar calendar is a lunar calendar that compensates by adding an extra month as needed to realign the months with the seasons. Prominent examples of lunisolar calendar are Hindu calendar and Buddhist calendar that are popular in South Asia and Southeast Asia. Another example is the Hebrew calendar, which uses a 19-year cycle.\nSubdivisions.\nNearly all calendar systems group consecutive days into \"months\" and also into \"years\". In a \"solar calendar\" a \"year\" approximates Earth's tropical year (that is, the time it takes for a complete cycle of seasons), traditionally used to facilitate the planning of agricultural activities. In a \"lunar calendar\", the \"month\" approximates the cycle of the moon phase. Consecutive days may be grouped into other periods such as the week.\nBecause the number of days in the \"tropical year\" is not a whole number, a solar calendar must have a different number of days in different years. This may be handled, for example, by adding an extra day in leap years. The same applies to months in a lunar calendar and also the number of months in a year in a lunisolar calendar. This is generally known as intercalation. Even if a calendar is solar, but not lunar, the year cannot be divided entirely into months that never vary in length.\nCultures may define other units of time, such as the week, for the purpose of scheduling regular activities that do not easily coincide with months or years. Many cultures use different baselines for their calendars' starting years. Historically, several countries have based their calendars on regnal years, a calendar based on the reign of their current sovereign. For example, the year 2006 in Japan is year 18 Heisei, with Heisei being the era name of Emperor Akihito.\nOther types.\nArithmetical and astronomical.\nAn \"astronomical calendar\" is based on ongoing observation; examples are the religious Islamic calendar and the old religious Jewish calendar in the time of the Second Temple. Such a calendar is also referred to as an \"observation-based\" calendar. The advantage of such a calendar is that it is perfectly and perpetually accurate. The disadvantage is that working out when a particular date would occur is difficult.\nAn \"arithmetic calendar\" is one that is based on a strict set of rules; an example is the current Jewish calendar. Such a calendar is also referred to as a \"rule-based\" calendar. The advantage of such a calendar is the ease of calculating when a particular date occurs. The disadvantage is imperfect accuracy. Furthermore, even if the calendar is very accurate, its accuracy diminishes slowly over time, owing to changes in Earth's rotation. This limits the lifetime of an accurate arithmetic calendar to a few thousand years. After then, the rules would need to be modified from observations made since the invention of the calendar.\nOther variants.\nThe early Roman calendar, created during the reign of Romulus, lumped the 61 days of the winter period together as simply \"winter\". Over time, this period became January and February; through further changes over time (including the creation of the Julian calendar) this calendar became the modern Gregorian calendar, introduced in the 1570s.\nUsage.\nThe primary practical use of a calendar is to identify days: to be informed about or to agree on a future event and to record an event that has happened. Days may be significant for agricultural, civil, religious, or social reasons. For example, a calendar provides a way to determine when to start planting or harvesting, which days are religious or civil holidays, which days mark the beginning and end of business accounting periods, and which days have legal significance, such as the day taxes are due or a contract expires. Also, a calendar may, by identifying a day, provide other useful information about the day such as its season.\nCalendars are also used as part of a complete timekeeping system: date and time of day together specify a moment in time. In the modern world, timekeepers can show time, date, and weekday. Some may also show the lunar phase.\nGregorian.\nThe Gregorian calendar is the \"de facto\" international standard and is used almost everywhere in the world for civil purposes. The widely used solar aspect is a cycle of leap days in a 400-year cycle designed to keep the duration of the year aligned with the solar year. There is a lunar aspect which approximates the position of the moon during the year, and is used in the calculation of the date of Easter. Each Gregorian year has either 365 or 366 days (the leap day being inserted as 29 February), amounting to an average Gregorian year of 365.2425 days (compared to a solar year of 365.2422 days).\nThe Gregorian calendar was introduced in 1582 as a refinement to the Julian calendar, that had been in use throughout the European Middle Ages, amounting to a 0.002% correction in the length of the year. During the Early Modern period, its adoption was mostly limited to Roman Catholic nations, but by the 19th century it had become widely adopted for the sake of convenience in international trade. The last European country to adopt it was Greece, in 1923.\nThe calendar epoch used by the Gregorian calendar is inherited from the medieval convention established by Dionysius Exiguus and associated with the Julian calendar. The year number is variously given as AD (for \"Anno Domini\") or CE (for \"Common Era\" or \"Christian Era\").\nReligious.\nThe most important use of pre-modern calendars is keeping track of the liturgical year and the observation of religious feast days.\nWhile the Gregorian calendar is itself historically motivated to the calculation of the Easter date, it is now in worldwide secular use as the \"de facto\" standard. Alongside the use of the Gregorian calendar for secular matters, there remain several calendars in use for religious purposes.\nWestern Christian liturgical calendars are based on the cycle of the Roman Rite of the Catholic Church, and generally include the liturgical seasons of Advent, Christmas, Ordinary Time (Time after Epiphany), Lent, Easter, and Ordinary Time (Time after Pentecost). Some Christian calendars do not include Ordinary Time and every day falls into a denominated season.\nThe Eastern Orthodox Church employs the use of 2 liturgical calendars; the Julian calendar (often called the Old Calendar) and the Revised Julian Calendar (often called the New Calendar). The Revised Julian Calendar is nearly the same as the Gregorian calendar, with the addition that years divisible by 100 are not leap years, except that years with remainders of 200 or 600 when divided by 900 remain leap years, e.g. 2000 and 2400 as in the Gregorian calendar.\nThe Islamic calendar or Hijri calendar is a lunar calendar consisting of 12 lunar months in a year of 354 or 355 days. It is used to date events in most of the Muslim countries (concurrently with the Gregorian calendar) and used by Muslims everywhere to determine the proper day on which to celebrate Islamic holy days and festivals. Its epoch is the Hijra (corresponding to AD\u00a0622). With an annual drift of 11 or 12 days, the seasonal relation is repeated approximately every 33 Islamic years.\nVarious Hindu calendars remain in use in the Indian subcontinent, including the Nepali calendars, Bengali calendar, Malayalam calendar, Tamil calendar, Vikrama Samvat used in Northern India, and Shalivahana calendar in the Deccan states.\nThe Buddhist calendar and the traditional lunisolar calendars of Cambodia, Laos, Myanmar, Sri Lanka and Thailand are also based on an older version of the Hindu calendar.\nMost of the Hindu calendars are inherited from a system first enunciated in Vedanga Jyotisha of Lagadha, standardized in the \"S\u016brya Siddh\u0101nta\" and subsequently reformed by astronomers such as \u0100ryabha\u1e6da (AD\u00a0499), Var\u0101hamihira (6th century) and Bh\u0101skara II (12th century).\nThe Hebrew calendar is used by Jews worldwide for religious and cultural affairs, also influences civil matters in Israel (such as national holidays) and can be used business dealings (such as for the dating of cheques).\nFollowers of the Bah\u00e1\u02bc\u00ed Faith use the Bah\u00e1\u02bc\u00ed calendar. The Bah\u00e1\u02bc\u00ed Calendar, also known as the Badi Calendar was first established by the Bab in the Kitab-i-Asma. The Bah\u00e1\u02bc\u00ed Calendar is also purely a solar calendar and comprises 19 months each having nineteen days.\nNational.\nThe Chinese, Hebrew, Hindu, and Julian calendars are widely used for religious and social purposes.\nThe Iranian (Persian) calendar is used in Iran and some parts of Afghanistan. The Assyrian calendar is in use by the members of the Assyrian community in the Middle East (mainly Iraq, Syria, Turkey, and Iran) and the diaspora. The first year of the calendar is exactly 4750 years prior to the start of the Gregorian calendar. The Ethiopian calendar or Ethiopic calendar is the principal calendar used in Ethiopia and Eritrea, with the Oromo calendar also in use in some areas. In neighboring Somalia, the Somali calendar co-exists alongside the Gregorian and Islamic calendars. In Thailand, where the Thai solar calendar is used, the months and days have adopted the western standard, although the years are still based on the traditional Buddhist calendar.\nFiscal.\nA fiscal calendar generally means the accounting year of a government or a business. It is used for budgeting, keeping accounts, and taxation. It is a set of 12 months that may start at any date in a year. The US government's fiscal year starts on 1 October and ends on 30 September. The government of India's fiscal year starts on 1 April and ends on 31 March. Small traditional businesses in India start the fiscal year on Diwali festival and end the day before the next year's Diwali festival.\nIn accounting (and particularly accounting software), a fiscal calendar (such as a 4/4/5 calendar) fixes each month at a specific number of weeks to facilitate comparisons from month to month and year to year. January always has exactly 4 weeks (Sunday through Saturday), February has 4 weeks, March has 5 weeks, etc. Note that this calendar will normally need to add a 53rd week to every 5th or 6th year, which might be added to December or might not be, depending on how the organization uses those dates. There exists an international standard way to do this (the ISO week). The ISO week starts on a Monday and ends on a Sunday. Week 1 is always the week that contains 4 January in the Gregorian calendar.\nFormats.\nThe term \"calendar\" applies not only to a given scheme of timekeeping but also to a specific record or device displaying such a scheme, for example, an appointment book in the form of a pocket calendar (or personal organizer), desktop calendar, a wall calendar, etc.\nIn a paper calendar, one or two sheets can show a single day, a week, a month, or a year. If a sheet is for a single day, it easily shows the date and the weekday. If a sheet is for multiple days it shows a conversion table to convert from weekday to date and back. With a special pointing device, or by crossing out past days, it may indicate the current date and weekday. This is the most common usage of the word.\nIn the US Sunday is considered the first day of the week and so appears on the far left and Saturday the last day of the week appearing on the far right. In Britain, the weekend may appear at the end of the week so the first day is Monday and the last day is Sunday. The US calendar display is also used in Britain.\nIt is common to display the Gregorian calendar in separate monthly grids of seven columns (from Monday to Sunday, or Sunday to Saturday depending on which day is considered to start the week\u00a0\u2013 this varies according to country) and five to six rows (or rarely, four rows when the month of February contains 28 days in common years beginning on the first day of the week), with the day of the month numbered in each cell, beginning with 1. The sixth row is sometimes eliminated by marking 23/30 and 24/31 together as necessary.\nWhen working with weeks rather than months, a continuous format is sometimes more convenient, where no blank cells are inserted to ensure that the first day of a new month begins on a fresh row.\nSoftware.\nCalendaring software provides users with an electronic version of a calendar, and may additionally provide an appointment book, address book, or contact list.\nCalendaring is a standard feature of many PDAs, EDAs, and smartphones. The software may be a local package designed for individual use (e.g., Lightning extension for Mozilla Thunderbird, Microsoft Outlook without Exchange Server, or Windows Calendar) or maybe a networked package that allows for the sharing of information between users (e.g., Mozilla Sunbird, Windows Live Calendar, Google Calendar, or Microsoft Outlook with Exchange Server)."}
{"id": "5378", "revid": "3391", "url": "https://en.wikipedia.org/wiki?curid=5378", "title": "Physical cosmology", "text": "Physical cosmology is a branch of cosmology concerned with the study of cosmological models. A cosmological model, or simply cosmology, provides a description of the largest-scale structures and dynamics of the universe and allows study of fundamental questions about its origin, structure, evolution, and ultimate fate. Cosmology as a science originated with the Copernican principle, which implies that celestial bodies obey identical physical laws to those on Earth, and Newtonian mechanics, which first allowed those physical laws to be understood.\nPhysical cosmology, as it is now understood, began in 1915 with the development of Albert Einstein's general theory of relativity, followed by major observational discoveries in the 1920s: first, Edwin Hubble discovered that the universe contains a huge number of external galaxies beyond the Milky Way; then, work by Vesto Slipher and others showed that the universe is expanding. These advances made it possible to speculate about the origin of the universe, and allowed the establishment of the Big Bang theory, by Georges Lema\u00eetre, as the leading cosmological model. A few researchers still advocate a handful of alternative cosmologies; however, most cosmologists agree that the Big Bang theory best explains the observations.\nDramatic advances in observational cosmology since the 1990s, including the cosmic microwave background, distant supernovae and galaxy redshift surveys, have led to the development of a standard model of cosmology. This model requires the universe to contain large amounts of dark matter and dark energy whose nature is currently not well understood, but the model gives detailed predictions that are in excellent agreement with many diverse observations.\nCosmology draws heavily on the work of many disparate areas of research in theoretical and applied physics. Areas relevant to cosmology include particle physics experiments and theory, theoretical and observational astrophysics, general relativity, quantum mechanics, and plasma physics.\nSubject history.\nModern cosmology developed along tandem tracks of theory and observation. In 1916, Albert Einstein published his theory of general relativity, which provided a unified description of gravity as a geometric property of space and time. At the time, Einstein believed in a static universe, but found that his original formulation of the theory did not permit it. This is because masses distributed throughout the universe gravitationally attract, and move toward each other over time. However, he realized that his equations permitted the introduction of a constant term which could counteract the attractive force of gravity on the cosmic scale. Einstein published his first paper on relativistic cosmology in 1917, in which he added this \"cosmological constant\" to his field equations in order to force them to model a static universe. The Einstein model describes a static universe; space is finite and unbounded (analogous to the surface of a sphere, which has a finite area but no edges). However, this so-called Einstein model is unstable to small perturbations\u2014it will eventually start to expand or contract. It was later realized that Einstein's model was just one of a larger set of possibilities, all of which were consistent with general relativity and the cosmological principle. The cosmological solutions of general relativity were found by Alexander Friedmann in the early 1920s. His equations describe the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker universe, which may expand or contract, and whose geometry may be open, flat, or closed.\nIn the 1910s, Vesto Slipher (and later Carl Wilhelm Wirtz) interpreted the red shift of spiral nebulae as a Doppler shift that indicated they were receding from Earth. However, it is difficult to determine the distance to astronomical objects. One way is to compare the physical size of an object to its angular size, but a physical size must be assumed in order to do this. Another method is to measure the brightness of an object and assume an intrinsic luminosity, from which the distance may be determined using the inverse-square law. Due to the difficulty of using these methods, they did not realize that the nebulae were actually galaxies outside our own Milky Way, nor did they speculate about the cosmological implications. In 1927, the Belgian Roman Catholic priest Georges Lema\u00eetre independently derived the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker equations and proposed, on the basis of the recession of spiral nebulae, that the universe began with the \"explosion\" of a \"primeval atom\"\u2014which was later called the Big Bang. In 1929, Edwin Hubble provided an observational basis for Lema\u00eetre's theory. Hubble showed that the spiral nebulae were galaxies by determining their distances using measurements of the brightness of Cepheid variable stars. He discovered a relationship between the redshift of a galaxy and its distance. He interpreted this as evidence that the galaxies are receding from Earth in every direction at speeds proportional to their distance from Earth. This fact is now known as Hubble's law, though the numerical factor Hubble found relating recessional velocity and distance was off by a factor of ten, due to not knowing about the types of Cepheid variables.\nGiven the cosmological principle, Hubble's law suggested that the universe was expanding. Two primary explanations were proposed for the expansion. One was Lema\u00eetre's Big Bang theory, advocated and developed by George Gamow. The other explanation was Fred Hoyle's steady state model in which new matter is created as the galaxies move away from each other. In this model, the universe is roughly the same at any point in time.\nFor a number of years, support for these theories was evenly divided. However, the observational evidence began to support the idea that the universe evolved from a hot dense state. The discovery of the cosmic microwave background in 1965 lent strong support to the Big Bang model, and since the precise measurements of the cosmic microwave background by the Cosmic Background Explorer in the early 1990s, few cosmologists have seriously proposed other theories of the origin and evolution of the cosmos. One consequence of this is that in standard general relativity, the universe began with a singularity, as demonstrated by Roger Penrose and Stephen Hawking in the 1960s.\nAn alternative view to extend the Big Bang model, suggesting the universe had no beginning or singularity and the age of the universe is infinite, has been presented.\nIn September 2023, astrophysicists questioned the overall current view of the universe, in the form of the Standard Model of Cosmology, based on the latest James Webb Space Telescope studies.\nEnergy of the cosmos.\nThe lightest chemical elements, primarily hydrogen and helium, were created during the Big Bang through the process of nucleosynthesis. In a sequence of stellar nucleosynthesis reactions, smaller atomic nuclei are then combined into larger atomic nuclei, ultimately forming stable iron group elements such as iron and nickel, which have the highest nuclear binding energies. The net process results in a \"later energy release\", meaning subsequent to the Big Bang. Such reactions of nuclear particles can lead to \"sudden energy releases\" from cataclysmic variable stars such as novae. Gravitational collapse of matter into black holes also powers the most energetic processes, generally seen in the nuclear regions of galaxies, forming \"quasars\" and \"active galaxies\".\nCosmologists cannot explain all cosmic phenomena exactly, such as those related to the accelerating expansion of the universe, using conventional forms of energy. Instead, cosmologists propose a new form of energy called dark energy that permeates all space. One hypothesis is that dark energy is just the vacuum energy, a component of empty space that is associated with the virtual particles that exist due to the uncertainty principle.\nThere is no clear way to define the total energy in the universe using the most widely accepted theory of gravity, general relativity. Therefore, it remains controversial whether the total energy is conserved in an expanding universe. For instance, each photon that travels through intergalactic space loses energy due to the redshift effect. This energy is not transferred to any other system, so seems to be permanently lost. On the other hand, some cosmologists insist that energy is conserved in some sense; this follows the law of conservation of energy.\nDifferent forms of energy may dominate the cosmos\u2014relativistic particles which are referred to as radiation, or non-relativistic particles referred to as matter. Relativistic particles are particles whose rest mass is zero or negligible compared to their kinetic energy, and so move at the speed of light or very close to it; non-relativistic particles have much higher rest mass than their energy and so move much slower than the speed of light.\nAs the universe expands, both matter and radiation become diluted. However, the energy densities of radiation and matter dilute at different rates. As a particular volume expands, mass-energy density is changed only by the increase in volume, but the energy density of radiation is changed both by the increase in volume and by the increase in the wavelength of the photons that make it up. Thus the energy of radiation becomes a smaller part of the universe's total energy than that of matter as it expands. The very early universe is said to have been 'radiation dominated' and radiation controlled the deceleration of expansion. Later, as the average energy per photon becomes roughly 10 eV and lower, matter dictates the rate of deceleration and the universe is said to be 'matter dominated'. The intermediate case is not treated well analytically. As the expansion of the universe continues, matter dilutes even further and the cosmological constant becomes dominant, leading to an acceleration in the universe's expansion.\nHistory of the universe.\nThe history of the universe is a central issue in cosmology. The history of the universe is divided into different periods called epochs, according to the dominant forces and processes in each period. The standard cosmological model is known as the Lambda-CDM model.\nEquations of motion.\nWithin the standard cosmological model, the equations of motion governing the universe as a whole are derived from general relativity with a small, positive cosmological constant. The solution is an expanding universe; due to this expansion, the radiation and matter in the universe cool and become diluted. At first, the expansion is slowed down by gravitation attracting the radiation and matter in the universe. However, as these become diluted, the cosmological constant becomes more dominant and the expansion of the universe starts to accelerate rather than decelerate. In our universe this happened billions of years ago.\nParticle physics in cosmology.\nDuring the earliest moments of the universe, the average energy density was very high, making knowledge of particle physics critical to understanding this environment. Hence, scattering processes and decay of unstable elementary particles are important for cosmological models of this period.\nAs a rule of thumb, a scattering or a decay process is cosmologically important in a certain epoch if the time scale describing that process is smaller than, or comparable to, the time scale of the expansion of the universe. The time scale that describes the expansion of the universe is formula_1 with formula_2 being the Hubble parameter, which varies with time. The expansion timescale formula_1 is roughly equal to the age of the universe at each point in time.\nTimeline of the Big Bang.\nObservations suggest that the universe began around 13.8 billion years ago. Since then, the evolution of the universe has passed through three phases. The very early universe, which is still poorly understood, was the split second in which the universe was so hot that particles had energies higher than those currently accessible in particle accelerators on Earth. Therefore, while the basic features of this epoch have been worked out in the Big Bang theory, the details are largely based on educated guesses.\nFollowing this, in the early universe, the evolution of the universe proceeded according to known high energy physics. This is when the first protons, electrons and neutrons formed, then nuclei and finally atoms. With the formation of neutral hydrogen, the cosmic microwave background was emitted. Finally, the epoch of structure formation began, when matter started to aggregate into the first stars and quasars, and ultimately galaxies, clusters of galaxies and superclusters formed. The future of the universe is not yet firmly known, but according to the \u039bCDM model it will continue expanding forever.\nAreas of study.\nBelow, some of the most active areas of inquiry in cosmology are described, in roughly chronological order. This does not include all of the Big Bang cosmology, which is presented in \"Timeline of the Big Bang.\"\nVery early universe.\nThe early, hot universe appears to be well explained by the Big Bang from roughly 10\u221233 seconds onwards, but there are several problems. One is that there is no compelling reason, using current particle physics, for the universe to be flat, homogeneous, and isotropic \"(see the cosmological principle)\". Moreover, grand unified theories of particle physics suggest that there should be magnetic monopoles in the universe, which have not been found. These problems are resolved by a brief period of cosmic inflation, which drives the universe to flatness, smooths out anisotropies and inhomogeneities to the observed level, and exponentially dilutes the monopoles. The physical model behind cosmic inflation is extremely simple, but it has not yet been confirmed by particle physics, and there are difficult problems reconciling inflation and quantum field theory. Some cosmologists think that string theory and brane cosmology will provide an alternative to inflation.\nAnother major problem in cosmology is what caused the universe to contain far more matter than antimatter. Cosmologists can observationally deduce that the universe is not split into regions of matter and antimatter. If it were, there would be X-rays and gamma rays produced as a result of annihilation, but this is not observed. Therefore, some process in the early universe must have created a small excess of matter over antimatter, and this (currently not understood) process is called \"baryogenesis\". Three required conditions for baryogenesis were derived by Andrei Sakharov in 1967, and requires a violation of the particle physics symmetry, called CP-symmetry, between matter and antimatter. However, particle accelerators measure too small a violation of CP-symmetry to account for the baryon asymmetry. Cosmologists and particle physicists look for additional violations of the CP-symmetry in the early universe that might account for the baryon asymmetry.\nBoth the problems of baryogenesis and cosmic inflation are very closely related to particle physics, and their resolution might come from high energy theory and experiment, rather than through observations of the universe.\nBig Bang Theory.\nBig Bang nucleosynthesis is the theory of the formation of the elements in the early universe. It finished when the universe was about three minutes old and its temperature dropped below that at which nuclear fusion could occur. Big Bang nucleosynthesis had a brief period during which it could operate, so only the very lightest elements were produced. Starting from hydrogen ions (protons), it principally produced deuterium, helium-4, and lithium. Other elements were produced in only trace abundances. The basic theory of nucleosynthesis was developed in 1948 by George Gamow, Ralph Asher Alpher, and Robert Herman. It was used for many years as a probe of physics at the time of the Big Bang, as the theory of Big Bang nucleosynthesis connects the abundances of primordial light elements with the features of the early universe. Specifically, it can be used to test the equivalence principle, to probe dark matter, and test neutrino physics. Some cosmologists have proposed that Big Bang nucleosynthesis suggests there is a fourth \"sterile\" species of neutrino.\nStandard model of Big Bang cosmology.\nThe \u039bCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek \u039b), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology.\nCosmic microwave background.\nThe cosmic microwave background is radiation left over from decoupling after the epoch of recombination when neutral atoms first formed. At this point, radiation produced in the Big Bang stopped Thomson scattering from charged ions. The radiation, first observed in 1965 by Arno Penzias and Robert Woodrow Wilson, has a perfect thermal black-body spectrum. It has a temperature of 2.7 kelvins today and is isotropic to one part in 105. Cosmological perturbation theory, which describes the evolution of slight inhomogeneities in the early universe, has allowed cosmologists to precisely calculate the angular power spectrum of the radiation, and it has been measured by the recent satellite experiments (COBE and WMAP) and many ground and balloon-based experiments (such as Degree Angular Scale Interferometer, Cosmic Background Imager, and Boomerang). One of the goals of these efforts is to measure the basic parameters of the Lambda-CDM model with increasing accuracy, as well as to test the predictions of the Big Bang model and look for new physics. The results of measurements made by WMAP, for example, have placed limits on the neutrino masses.\nNewer experiments, such as QUIET and the Atacama Cosmology Telescope, are trying to measure the polarization of the cosmic microwave background. These measurements are expected to provide further confirmation of the theory as well as information about cosmic inflation, and the so-called secondary anisotropies, such as the Sunyaev-Zel'dovich effect and Sachs-Wolfe effect, which are caused by interaction between galaxies and clusters with the cosmic microwave background.\nOn 17 March 2014, astronomers of the BICEP2 Collaboration announced the apparent detection of \"B\"-mode polarization of the CMB, considered to be evidence of primordial gravitational waves that are predicted by the theory of inflation to occur during the earliest phase of the Big Bang. However, later that year the Planck collaboration provided a more accurate measurement of cosmic dust, concluding that the B-mode signal from dust is the same strength as that reported from BICEP2. On 30 January 2015, a joint analysis of BICEP2 and Planck data was published and the European Space Agency announced that the signal can be entirely attributed to interstellar dust in the Milky Way.\nFormation and evolution of large-scale structure.\nUnderstanding the formation and evolution of the largest and earliest structures (i.e., quasars, galaxies, clusters and superclusters) is one of the largest efforts in cosmology. Cosmologists study a model of hierarchical structure formation in which structures form from the bottom up, with smaller objects forming first, while the largest objects, such as superclusters, are still assembling. One way to study structure in the universe is to survey the visible galaxies, in order to construct a three-dimensional picture of the galaxies in the universe and measure the matter power spectrum. This is the approach of the \"Sloan Digital Sky Survey\" and the 2dF Galaxy Redshift Survey.\nAnother tool for understanding structure formation is simulations, which cosmologists use to study the gravitational aggregation of matter in the universe, as it clusters into filaments, superclusters and voids. Most simulations contain only non-baryonic cold dark matter, which should suffice to understand the universe on the largest scales, as there is much more dark matter in the universe than visible, baryonic matter. More advanced simulations are starting to include baryons and study the formation of individual galaxies. Cosmologists study these simulations to see if they agree with the galaxy surveys, and to understand any discrepancy.\nOther, complementary observations to measure the distribution of matter in the distant universe and to probe reionization include:\nThese will help cosmologists settle the question of when and how structure formed in the universe.\nDark matter.\nEvidence from Big Bang nucleosynthesis, the cosmic microwave background, structure formation, and galaxy rotation curves suggests that about 23% of the mass of the universe consists of non-baryonic dark matter, whereas only 4% consists of visible, baryonic matter. The gravitational effects of dark matter are well understood, as it behaves like a cold, non-radiative fluid that forms haloes around galaxies. Dark matter has never been detected in the laboratory, and the particle physics nature of dark matter remains completely unknown. Without observational constraints, there are a number of candidates, such as a stable supersymmetric particle, a weakly interacting massive particle, a gravitationally-interacting massive particle, an axion, and a massive compact halo object. Alternatives to the dark matter hypothesis include a modification of gravity at small accelerations (MOND) or an effect from brane cosmology. TeVeS is a version of MOND that can explain gravitational lensing.\nDark energy.\nIf the universe is flat, there must be an additional component making up 73% (in addition to the 23% dark matter and 4% baryons) of the energy density of the universe. This is called dark energy. In order not to interfere with Big Bang nucleosynthesis and the cosmic microwave background, it must not cluster in haloes like baryons and dark matter. There is strong observational evidence for dark energy, as the total energy density of the universe is known through constraints on the flatness of the universe, but the amount of clustering matter is tightly measured, and is much less than this. The case for dark energy was strengthened in 1999, when measurements demonstrated that the expansion of the universe has begun to gradually accelerate.\nApart from its density and its clustering properties, nothing is known about dark energy. \"Quantum field theory\" predicts a cosmological constant (CC) much like dark energy, but 120 orders of magnitude larger than that observed. Steven Weinberg and a number of string theorists \"(see string landscape)\" have invoked the 'weak anthropic principle': i.e. the reason that physicists observe a universe with such a small cosmological constant is that no physicists (or any life) could exist in a universe with a larger cosmological constant. Many cosmologists find this an unsatisfying explanation: perhaps because while the weak anthropic principle is self-evident (given that living observers exist, there must be at least one universe with a cosmological constant (CC) which allows for life to exist) it does not attempt to explain the context of that universe. For example, the weak anthropic principle alone does not distinguish between:\nOther possible explanations for dark energy include quintessence or a modification of gravity on the largest scales. The effect on cosmology of the dark energy that these models describe is given by the dark energy's equation of state, which varies depending upon the theory. The nature of dark energy is one of the most challenging problems in cosmology.\nA better understanding of dark energy is likely to solve the problem of the ultimate fate of the universe. In the current cosmological epoch, the accelerated expansion due to dark energy is preventing structures larger than superclusters from forming. It is not known whether the acceleration will continue indefinitely, perhaps even increasing until a big rip, or whether it will eventually reverse, lead to a Big Freeze, or follow some other scenario.\nGravitational waves.\nGravitational waves are ripples in the curvature of spacetime that propagate as waves at the speed of light, generated in certain gravitational interactions that propagate outward from their source. Gravitational-wave astronomy is an emerging branch of observational astronomy which aims to use gravitational waves to collect observational data about sources of detectable gravitational waves such as binary star systems composed of white dwarfs, neutron stars, and black holes; and events such as supernovae, and the formation of the early universe shortly after the Big Bang.\nIn 2016, the LIGO Scientific Collaboration and Virgo Collaboration teams announced that they had made the first observation of gravitational waves, originating from a pair of merging black holes using the Advanced LIGO detectors. On 15 June 2016, a second detection of gravitational waves from coalescing black holes was announced. Besides LIGO, many other gravitational-wave observatories (detectors) are under construction.\nOther areas of inquiry.\nCosmologists also study:"}
{"id": "5382", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=5382", "title": "Cosmic inflation", "text": "In physical cosmology, cosmic inflation, cosmological inflation, or just inflation, is a theory of exponential expansion of space in the very early universe. Following the inflationary period, the universe continued to expand, but at a slower rate. The re-acceleration of this slowing expansion due to dark energy began after the universe was already over 7.7\u00a0billion years old (5.4\u00a0billion years ago).\nInflation theory was developed in the late 1970s and early 1980s, with notable contributions by several theoretical physicists, including Alexei Starobinsky at Landau Institute for Theoretical Physics, Alan Guth at Cornell University, and Andrei Linde at Lebedev Physical Institute. Starobinsky, Guth, and Linde won the 2014 Kavli Prize \"for pioneering the theory of cosmic inflation\". It was developed further in the early 1980s. It explains the origin of the large-scale structure of the cosmos. Quantum fluctuations in the microscopic inflationary region, magnified to cosmic size, become the seeds for the growth of structure in the Universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the universe is flat, and why no magnetic monopoles have been observed.\nThe detailed particle physics mechanism responsible for inflation is unknown. A number of inflation model predictions have been confirmed by observation; for example temperature anisotropies observed by the COBE satellite in 1992 exhibit nearly scale-invariant spectra as predicted by the inflationary paradigm and WMAP results also show strong evidence for inflation. However, some scientists dissent from this position. The hypothetical field thought to be responsible for inflation is called the inflaton.\nIn 2002, three of the original architects of the theory were recognized for their major contributions; physicists Alan Guth of M.I.T., Andrei Linde of Stanford, and Paul Steinhardt of Princeton shared the Dirac Prize \"for development of the concept of inflation in cosmology\". In 2012, Guth and Linde were awarded the Breakthrough Prize in Fundamental Physics for their invention and development of inflationary cosmology.\nOverview.\nAround 1930, Edwin Hubble discovered that light from remote galaxies was redshifted; the more remote, the more shifted. This implies that the galaxies are receding from the Earth, with more distant galaxies receding more rapidly, such that galaxies also recede from each other. This expansion of the universe was previously predicted by Alexander Friedmann and Georges Lema\u00eetre from the theory of general relativity. It can be understood as a consequence of an initial impulse, which sent the contents of the universe flying apart at such a rate that their mutual gravitational attraction has not reversed their increasing separation.\nInflation may have provided this initial impulse. According to the Friedmann equations that describe the dynamics of an expanding universe, a fluid with sufficiently negative pressure exerts gravitational repulsion in the cosmological context. A field in a positive-energy false vacuum state could represent such a fluid, and the resulting repulsion would set the universe into exponential expansion. This \"inflation\" phase was originally proposed by Alan Guth in 1979 because the exponential expansion could dilute exotic relics, such as magnetic monopoles, that were predicted by grand unified theories at the time. This would explain why such relics were not seen. It was quickly realized that such accelerated expansion would resolve the horizon problem and the flatness problem. These problems arise from the notion that to look like it does \"today\", the Universe must have started from very finely tuned, or \"special\", initial conditions at the Big Bang.\nTheory.\nAn expanding universe generally has a cosmological horizon, which, by analogy with the more familiar horizon caused by the curvature of Earth's surface, marks the boundary of the part of the Universe that an observer can see. Light (or other radiation) emitted by objects beyond the cosmological horizon in an accelerating universe never reaches the observer, because the space in between the observer and the object is expanding too rapidly.\nThe observable universe is one \"causal patch\" of a much larger unobservable universe; other parts of the Universe cannot communicate with Earth yet. These parts of the Universe are outside our current cosmological horizon, which is believed to be 46 billion light years in all directions from Earth. In the standard hot big bang model, without inflation, the cosmological horizon moves out, bringing new regions into view. Yet as a local observer sees such a region for the first time, it looks no different from any other region of space the local observer has already seen: Its background radiation is at nearly the same temperature as the background radiation of other regions, and its space-time curvature is evolving lock-step with the others. This presents a mystery: how did these new regions know what temperature and curvature they were supposed to have? They could not have learned it by getting signals, because they were not previously in communication with our past light cone.\nInflation answers this question by postulating that all the regions come from an earlier era with a big vacuum energy, or cosmological constant. A space with a cosmological constant is qualitatively different: instead of moving outward, the cosmological horizon stays put. For any one observer, the distance to the cosmological horizon is constant. With exponentially expanding space, two nearby observers are separated very quickly; so much so, that the distance between them quickly exceeds the limits of communication. The spatial slices are expanding very fast to cover huge volumes. Things are constantly moving beyond the cosmological horizon, which is a fixed distance away, and everything becomes homogeneous.\nAs the inflationary field slowly relaxes to the vacuum, the cosmological constant goes to zero and space begins to expand normally. The new regions that come into view during the normal expansion phase are exactly the same regions that were pushed out of the horizon during inflation, and so they are at nearly the same temperature and curvature, because they come from the same originally small patch of space.\nThe theory of inflation thus explains why the temperatures and curvatures of different regions are so nearly equal. It also predicts that the total curvature of a space-slice at constant global time is zero. This prediction implies that the total ordinary matter, dark matter and residual vacuum energy in the Universe have to add up to the critical density, and the evidence supports this. More strikingly, inflation allows physicists to calculate the minute differences in temperature of different regions from quantum fluctuations during the inflationary era, and many of these quantitative predictions have been confirmed.\nSpace expands.\nIn a space that expands exponentially (or nearly exponentially) with time, any pair of free-floating objects that are initially at rest will move apart from each other at an accelerating rate, at least as long as they are not bound together by any force. From the point of view of one such object, the spacetime is something like an inside-out Schwarzschild black hole\u2014each object is surrounded by a spherical event horizon. Once the other object has fallen through this horizon it can never return, and even light signals it sends will never reach the first object (at least so long as the space continues to expand exponentially).\nIn the approximation that the expansion is exactly exponential, the horizon is static and remains a fixed physical distance away. This patch of an inflating universe can be described by the following metric:\nThis exponentially expanding spacetime is called a de Sitter space, and to sustain it there must be a cosmological constant, a vacuum energy density that is constant in space and time and proportional to \u039b in the above metric. For the case of exactly exponential expansion, the vacuum energy has a negative pressure \"p\" equal in magnitude to its energy density \"\u03c1\"; the equation of state is \"p=\u2212\u03c1\".\nInflation is typically not an exactly exponential expansion, but rather quasi- or near-exponential. In such a universe the horizon will slowly grow with time as the vacuum energy density gradually decreases.\nFew inhomogeneities remain.\nBecause the accelerating expansion of space stretches out any initial variations in density or temperature to very large length scales, an essential feature of inflation is that it smooths out inhomogeneities and anisotropies, and reduces the curvature of space. This pushes the Universe into a very simple state in which it is completely dominated by the inflaton field and the only significant inhomogeneities are tiny quantum fluctuations. Inflation also dilutes exotic heavy particles, such as the magnetic monopoles predicted by many extensions to the Standard Model of particle physics. If the Universe was only hot enough to form such particles \"before\" a period of inflation, they would not be observed in nature, as they would be so rare that it is quite likely that there are none in the observable universe. Together, these effects are called the inflationary \"no-hair theorem\" by analogy with the no hair theorem for black holes.\nThe \"no-hair\" theorem works essentially because the cosmological horizon is no different from a black-hole horizon, except for not testable disagreements about what is on the other side. The interpretation of the no-hair theorem is that the Universe (observable and unobservable) expands by an enormous factor during inflation. In an expanding universe, energy densities generally fall, or get diluted, as the volume of the Universe increases. For example, the density of ordinary \"cold\" matter (dust) declines as the inverse of the volume: when linear dimensions double, the energy density declines by a factor of eight; the radiation energy density declines even more rapidly as the Universe expands since the wavelength of each photon is stretched (redshifted), in addition to the photons being dispersed by the expansion. When linear dimensions are doubled, the energy density in radiation falls by a factor of sixteen (see the solution of the energy density continuity equation for an ultra-relativistic fluid). During inflation, the energy density in the inflaton field is roughly constant. However, the energy density in everything else, including inhomogeneities, curvature, anisotropies, exotic particles, and standard-model particles is falling, and through sufficient inflation these all become negligible. This leaves the Universe flat and symmetric, and (apart from the homogeneous inflaton field) mostly empty, at the moment inflation ends and reheating begins.\nReheating.\nInflation is a period of supercooled expansion, when the temperature drops by a factor of 100,000 or so. (The exact drop is model-dependent, but in the first models it was typically from \u00a0K down to \u00a0K.) This relatively low temperature is maintained during the inflationary phase. When inflation ends, the temperature returns to the pre-inflationary temperature; this is called \"reheating\" or thermalization because the large potential energy of the inflaton field decays into particles and fills the Universe with Standard Model particles, including electromagnetic radiation, starting the radiation dominated phase of the Universe. Because the nature of the inflaton field is not known, this process is still poorly understood, although it is believed to take place through a parametric resonance.\nMotivations.\nInflation tries to resolve several problems in Big Bang cosmology that were discovered in the 1970s. Inflation was first proposed by Alan Guth in 1979 while investigating the problem of why no magnetic monopoles are seen today; he found that a positive-energy false vacuum would, according to general relativity, generate an exponential expansion of space. It was quickly realised that such an expansion would resolve many other long-standing problems. These problems arise from the observation that to look like it does \"today\", the Universe would have to have started from very finely tuned, or \"special\" initial conditions at the Big Bang. Inflation attempts to resolve these problems by providing a dynamical mechanism that drives the Universe to this special state, thus making a universe like ours much more likely in the context of the Big Bang theory.\nHorizon problem.\nThe horizon problem is the problem of determining why the universe appears statistically homogeneous and isotropic in accordance with the cosmological principle. For example, molecules in a canister of gas are distributed homogeneously and isotropically because they are in thermal equilibrium: gas throughout the canister has had enough time to interact to dissipate inhomogeneities and anisotropies. The situation is quite different in the big bang model without inflation, because gravitational expansion does not give the early universe enough time to equilibrate. In a big bang with only the matter and radiation known in the Standard Model, two widely separated regions of the observable universe cannot have equilibrated because they move apart from each other faster than the speed of light and thus have never come into causal contact. In the early Universe, it was not possible to send a light signal between the two regions. Because they have had no interaction, it is difficult to explain why they have the same temperature (are thermally equilibrated). Historically, proposed solutions included the \"Phoenix universe\" of Georges Lema\u00eetre, the related oscillatory universe of Richard Chase Tolman, and the Mixmaster universe of Charles Misner. Lema\u00eetre and Tolman proposed that a universe undergoing a number of cycles of contraction and expansion could come into thermal equilibrium. Their models failed, however, because of the buildup of entropy over several cycles. Misner made the (ultimately incorrect) conjecture that the Mixmaster mechanism, which made the Universe \"more\" chaotic, could lead to statistical homogeneity and isotropy.\nFlatness problem.\nThe flatness problem is sometimes called one of the Dicke coincidences (along with the cosmological constant problem). It became known in the 1960s that the density of matter in the Universe was comparable to the critical density necessary for a flat universe (that is, a universe whose large-scale geometry is the usual Euclidean geometry, rather than a non-Euclidean hyperbolic or spherical geometry).\nTherefore, regardless of the shape of the universe, the contribution of spatial curvature to the expansion of the Universe could not be much greater than the contribution of matter. But as the Universe expands, the curvature redshifts away more slowly than matter and radiation. Extrapolated into the past, this presents a fine-tuning problem because the contribution of curvature to the Universe must be exponentially small (sixteen orders of magnitude less than the density of radiation at Big Bang nucleosynthesis, for example). Observations of the cosmic microwave background have demonstrated that the Universe is flat to within a few percent.\nMagnetic-monopole problem.\nStable magnetic monopoles are a problem for Grand Unified Theories, which propose that at high temperatures (such as in the early universe), the electromagnetic force, strong, and weak nuclear forces are not actually fundamental forces but arise due to spontaneous symmetry breaking from a single gauge theory. These theories predict a number of heavy, stable particles that have not been observed in nature. The most notorious is the magnetic monopole, a kind of stable, heavy \"charge\" of magnetic field.\nMonopoles are predicted to be copiously produced following Grand Unified Theories at high temperature, and they should have persisted to the present day, to such an extent that they would become the primary constituent of the Universe. Not only is that not the case, but all searches for them have failed, placing stringent limits on the density of relic magnetic monopoles in the Universe.\nA period of inflation that occurs below the temperature where magnetic monopoles can be produced would offer a possible resolution of this problem: Monopoles would be separated from each other as the Universe around them expands, potentially lowering their observed density by many orders of magnitude. Though, as cosmologist Martin Rees has written,\nHistory.\nPrecursors.\nIn the early days of general relativity, Albert Einstein introduced the cosmological constant to allow a static solution, which was a three-dimensional sphere with a uniform density of matter. Later, Willem de Sitter found a highly symmetric inflating universe, which described a universe with a cosmological constant that is otherwise empty. It was discovered that Einstein's universe is unstable, and that small fluctuations cause it to collapse or turn into a de Sitter universe.\nIn 1965, Erast Gliner proposed a unique assumption regarding the early Universe's pressure in the context of the Einstein\u2013Friedmann equations. According to his idea, the pressure was negatively proportional to the energy density. This relationship between pressure and energy density served as the initial theoretical prediction of dark energy.\nIn the early 1970s, Yakov Zeldovich noticed the flatness and horizon problems of Big Bang cosmology; before his work, cosmology was presumed to be symmetrical on purely philosophical grounds. In the Soviet Union, this and other considerations led Vladimir Belinski and Isaak Khalatnikov to analyze the chaotic BKL singularity in general relativity. Misner's Mixmaster universe attempted to use this chaotic behavior to solve the cosmological problems, with limited success.\nFalse vacuum.\nIn the late 1970s, Sidney Coleman applied the instanton techniques developed by Alexander Polyakov and collaborators to study the fate of the false vacuum in quantum field theory. Like a metastable phase in statistical mechanics\u2014water below the freezing temperature or above the boiling point\u2014a quantum field would need to nucleate a large enough bubble of the new vacuum, the new phase, in order to make a transition. Coleman found the most likely decay pathway for vacuum decay and calculated the inverse lifetime per unit volume. He eventually noted that gravitational effects would be significant, but he did not calculate these effects and did not apply the results to cosmology.\nThe universe could have been spontaneously created from nothing (no space, time, nor matter) by quantum fluctuations of metastable false vacuum causing an expanding bubble of true vacuum.\nThe Causal Universe of Brout Englert and Gunzig.\nIn 1978 and 1979, Robert Brout, Fran\u00e7ois Englert and Edgard Gunzig suggested that the universe could originate from a fluctuation of Minkowski space which would be followed by a period in which the geometry would resemble De Sitter space.\nThis initial period would then evolve into the standard expanding universe. They noted that their proposal makes the universe causal, as there are neither particle nor event horizons in their model. \nStarobinsky inflation.\nIn the Soviet Union, Alexei Starobinsky noted that quantum corrections to general relativity should be important for the early universe. These generically lead to curvature-squared corrections to the Einstein\u2013Hilbert action and a form of modified gravity. The solution to Einstein's equations in the presence of curvature squared terms, when the curvatures are large, leads to an effective cosmological constant. Therefore, he proposed that the early universe went through an inflationary de Sitter era. This resolved the cosmology problems and led to specific predictions for the corrections to the microwave background radiation, corrections that were then calculated in detail. Starobinsky used the action \nwhich corresponds to the potential\nin the Einstein frame. This results in the observables: formula_4\nMonopole problem.\nIn 1978, Zeldovich noted the magnetic monopole problem, which was an unambiguous quantitative version of the horizon problem, this time in a subfield of particle physics, which led to several speculative attempts to resolve it. In 1980, Alan Guth realized that false vacuum decay in the early universe would solve the problem, leading him to propose a scalar-driven inflation. Starobinsky's and Guth's scenarios both predicted an initial de Sitter phase, differing only in mechanistic details.\nEarly inflationary models.\nGuth proposed inflation in January 1981 to explain the nonexistence of magnetic monopoles; it was Guth who coined the term \"inflation\". At the same time, Starobinsky argued that quantum corrections to gravity would replace the supposed initial singularity of the Universe with an exponentially expanding de Sitter phase. In October 1980, Demosthenes Kazanas suggested that exponential expansion could eliminate the particle horizon and perhaps solve the horizon problem, while Katsuhiko Sato suggested that an exponential expansion could eliminate domain walls (another kind of exotic relic). In 1981, Einhorn and Sato published a model similar to Guth's and showed that it would resolve the puzzle of the magnetic monopole abundance in Grand Unified Theories. Like Guth, they concluded that such a model not only required fine tuning of the cosmological constant, but also would likely lead to a much too granular universe, i.e., to large density variations resulting from bubble wall collisions.\nGuth proposed that as the early universe cooled, it was trapped in a false vacuum with a high energy density, which is much like a cosmological constant. As the very early universe cooled it was trapped in a metastable state (it was supercooled), which it could only decay out of through the process of bubble nucleation via quantum tunneling. Bubbles of true vacuum spontaneously form in the sea of false vacuum and rapidly begin expanding at the speed of light. Guth recognized that this model was problematic because the model did not reheat properly: when the bubbles nucleated, they did not generate radiation. Radiation could only be generated in collisions between bubble walls. But if inflation lasted long enough to solve the initial conditions problems, collisions between bubbles became exceedingly rare. In any one causal patch it is likely that only one bubble would nucleate.\nSlow-roll inflation.\nThe bubble collision problem was solved by Andrei Linde and independently by Andreas Albrecht and Paul Steinhardt in a model named \"new inflation\" or \"slow-roll inflation\" (Guth's model then became known as \"old inflation\"). In this model, instead of tunneling out of a false vacuum state, inflation occurred by a scalar field rolling down a potential energy hill. When the field rolls very slowly compared to the expansion of the Universe, inflation occurs. However, when the hill becomes steeper, inflation ends and reheating can occur.\nEffects of asymmetries.\nEventually, it was shown that new inflation does not produce a perfectly symmetric universe, but that quantum fluctuations in the inflaton are created. These fluctuations form the primordial seeds for all structure created in the later universe. These fluctuations were first calculated by Viatcheslav Mukhanov and G. V. Chibisov in analyzing Starobinsky's similar model. In the context of inflation, they were worked out independently of the work of Mukhanov and Chibisov at the three-week 1982 Nuffield Workshop on the Very Early Universe at Cambridge University. The fluctuations were calculated by four groups working separately over the course of the workshop: Stephen Hawking; Starobinsky; Alan Guth and So-Young Pi; and James Bardeen, Paul Steinhardt and Michael Turner.\nObservational status.\nInflation is a mechanism for realizing the cosmological principle, which is the basis of the standard model of physical cosmology: it accounts for the homogeneity and isotropy of the observable universe. In addition, it accounts for the observed flatness and absence of magnetic monopoles. Since Guth's early work, each of these observations has received further confirmation, most impressively by the detailed observations of the cosmic microwave background made by the Planck spacecraft. This analysis shows that the Universe is flat to within percent, and that it is homogeneous and isotropic to one part in 100,000.\nInflation predicts that the structures visible in the Universe today formed through the gravitational collapse of perturbations that were formed as quantum mechanical fluctuations in the inflationary epoch. The detailed form of the spectrum of perturbations, called a nearly-scale-invariant Gaussian random field is very specific and has only two free parameters. One is the amplitude of the spectrum and the \"spectral index\", which measures the slight deviation from scale invariance predicted by inflation (perfect scale invariance corresponds to the idealized de Sitter universe).\nThe other free parameter is the tensor to scalar ratio. The simplest inflation models, those without fine-tuning, predict a tensor to scalar ratio near 0.1\u00a0.\nInflation predicts that the observed perturbations should be in thermal equilibrium with each other (these are called \"adiabatic\" or \"isentropic\" perturbations). This structure for the perturbations has been confirmed by the Planck spacecraft, WMAP spacecraft and other cosmic microwave background (CMB) experiments, and galaxy surveys, especially the ongoing Sloan Digital Sky Survey. These experiments have shown that the one part in 100,000\u00a0inhomogeneities observed have exactly the form predicted by theory. There is evidence for a slight deviation from scale invariance. The \"spectral index\", is one for a scale-invariant Harrison\u2013Zel'dovich spectrum. The simplest inflation models predict that is between 0.92 and 0.98\u00a0. This is the range that is possible without fine-tuning of the parameters related to energy. From Planck data it can be inferred that =0.968 \u00b1 0.006, and a tensor to scalar ratio that is less than 0.11\u00a0. These are considered an important confirmation of the theory of inflation.\nVarious inflation theories have been proposed that make radically different predictions, but they generally have much more fine-tuning than should be necessary. As a physical model, however, inflation is most valuable in that it robustly predicts the initial conditions of the Universe based on only two adjustable parameters: the spectral index (that can only change in a small range) and the amplitude of the perturbations. Except in contrived models, this is true regardless of how inflation is realized in particle physics.\nOccasionally, effects are observed that appear to contradict the simplest models of inflation. The first-year WMAP data suggested that the spectrum might not be nearly scale-invariant, but might instead have a slight curvature. However, the third-year data revealed that the effect was a statistical anomaly. Another effect remarked upon since the first cosmic microwave background satellite, the Cosmic Background Explorer is that the amplitude of the quadrupole moment of the CMB is unexpectedly low and the other low multipoles appear to be preferentially aligned with the ecliptic plane. Some have claimed that this is a signature of non-Gaussianity and thus contradicts the simplest models of inflation. Others have suggested that the effect may be due to other new physics, foreground contamination, or even publication bias.\nAn experimental program is underway to further test inflation with more precise CMB measurements. In particular, high precision measurements of the so-called \"B-modes\" of the polarization of the background radiation could provide evidence of the gravitational radiation produced by inflation, and could also show whether the energy scale of inflation predicted by the simplest models (~ GeV) is correct. In March 2014, the BICEP2 team announced B-mode CMB polarization confirming inflation had been demonstrated. The team announced the tensor-to-scalar power ratio was between 0.15 and 0.27 (rejecting the null hypothesis; is expected to be 0 in the absence of inflation). However, on 19 June 2014, lowered confidence in confirming the findings was reported; on 19 September 2014, a further reduction in confidence was reported and, on 30 January 2015, even less confidence yet was reported. By 2018, additional data suggested, with 95% confidence, that formula_5 is 0.06 or lower: Consistent with the null hypothesis, but still also consistent with many remaining models of inflation.\nOther potentially corroborating measurements are expected from the Planck spacecraft, although it is unclear if the signal will be visible, or if contamination from foreground sources will interfere.\nOther forthcoming measurements, such as those of 21 centimeter radiation (radiation emitted and absorbed from neutral hydrogen before the first stars formed), may measure the power spectrum with even greater resolution than the CMB and galaxy surveys, although it is not known if these measurements will be possible or if interference with radio sources on Earth and in the galaxy will be too great.\nTheoretical status.\nIn Guth's early proposal, it was thought that the inflaton was the Higgs field, the field that explains the mass of the elementary particles. It is now believed by some that the inflaton cannot be the Higgs field. One problem of this identification is the current tension with experimental data at the electroweak scale. Other models of inflation relied on the properties of Grand Unified Theories.\nFine-tuning problem.\nOne of the most severe challenges for inflation arises from the need for fine tuning. In new inflation, the \"slow-roll conditions\" must be satisfied for inflation to occur. The slow-roll conditions say that the inflaton potential must be flat (compared to the large vacuum energy) and that the inflaton particles must have a small mass.\nNew inflation requires the Universe to have a scalar field with an especially flat potential and special initial conditions. However, explanations for these fine-tunings have been proposed. For example, classically scale invariant field theories, where scale invariance is broken by quantum effects, provide an explanation of the flatness of inflationary potentials, as long as the theory can be studied through perturbation theory.\nLinde proposed a theory known as \"chaotic inflation\" in which he suggested that the conditions for inflation were actually satisfied quite generically. Inflation will occur in virtually any universe that begins in a chaotic, high energy state that has a scalar field with unbounded potential energy.\nHowever, in his model, the inflaton field necessarily takes values larger than one Planck unit: For this reason, these are often called \"large field\" models and the competing new inflation models are called \"small field\" models. In this situation, the predictions of effective field theory are thought to be invalid, as renormalization should cause large corrections that could prevent inflation.\nThis problem has not yet been resolved and some cosmologists argue that the small field models, in which inflation can occur at a much lower energy scale, are better models.\nWhile inflation depends on quantum field theory (and the semiclassical approximation to quantum gravity) in an important way, it has not been completely reconciled with these theories.\nBrandenberger commented on fine-tuning in another situation.\nThe amplitude of the primordial inhomogeneities produced in inflation is directly tied to the energy scale of inflation. This scale is suggested to be around GeV or times the Planck energy. The natural scale is na\u00efvely the Planck scale so this small value could be seen as another form of fine-tuning (called a hierarchy problem): The energy density given by the scalar potential is down by compared to the Planck density. This is not usually considered to be a critical problem, however, because the scale of inflation corresponds naturally to the scale of gauge unification.\nEternal inflation.\nIn many models, the inflationary phase of the Universe's expansion lasts forever in at least some regions of the Universe. This occurs because inflating regions expand very rapidly, reproducing themselves. Unless the rate of decay to the non-inflating phase is sufficiently fast, new inflating regions are produced more rapidly than non-inflating regions. In such models, most of the volume of the Universe is continuously inflating at any given time.\nAll models of eternal inflation produce an infinite, hypothetical multiverse, typically a fractal. The multiverse theory has created significant dissension in the scientific community about the viability of the inflationary model.\nPaul Steinhardt, one of the original architects of the inflationary model, introduced the first example of eternal inflation in 1983. He showed that the inflation could proceed forever by producing bubbles of non-inflating space filled with hot matter and radiation surrounded by empty space that continues to inflate. The bubbles could not grow fast enough to keep up with the inflation. Later that same year, Alexander Vilenkin showed that eternal inflation is generic.\nAlthough new inflation is classically rolling down the potential, quantum fluctuations can sometimes lift it to previous levels. These regions in which the inflaton fluctuates upwards, expand much faster than regions in which the inflaton has a lower potential energy, and tend to dominate in terms of physical volume. It has been shown that any inflationary theory with an unbounded potential is eternal. There are well-known theorems that this steady state cannot continue forever into the past. Inflationary spacetime, which is similar to de Sitter space, is incomplete without a contracting region. However, unlike de Sitter space, fluctuations in a contracting inflationary space collapse to form a gravitational singularity, a point where densities become infinite. Therefore, it is necessary to have a theory for the Universe's initial conditions.\nIn eternal inflation, regions with inflation have an exponentially growing volume, while regions that are not inflating do not. This suggests that the volume of the inflating part of the Universe in the global picture is always unimaginably larger than the part that has stopped inflating, even though inflation eventually ends as seen by any single pre-inflationary observer. Scientists disagree about how to assign a probability distribution to this hypothetical anthropic landscape. If the probability of different regions is counted by volume, one should expect that inflation will never end or applying boundary conditions that a local observer exists to observe it, that inflation will end as late as possible.\nSome physicists believe this paradox can be resolved by weighting observers by their pre-inflationary volume. Others believe that there is no resolution to the paradox and that the multiverse is a critical flaw in the inflationary paradigm. Paul Steinhardt, who first introduced the eternal inflationary model, later became one of its most vocal critics for this reason.\nInitial conditions.\nSome physicists have tried to avoid the initial conditions problem by proposing models for an eternally inflating universe with no origin. These models propose that while the Universe, on the largest scales, expands exponentially it was, is and always will be, spatially infinite and has existed, and will exist, forever.\nOther proposals attempt to describe the ex nihilo creation of the Universe based on quantum cosmology and the following inflation. Vilenkin put forth one such scenario. Hartle and Hawking offered the no-boundary proposal for the initial creation of the Universe in which inflation comes about naturally.\nGuth described the inflationary universe as the \"ultimate free lunch\": new universes, similar to our own, are continually produced in a vast inflating background. Gravitational interactions, in this case, circumvent (but do not violate) the first law of thermodynamics (energy conservation) and the second law of thermodynamics (entropy and the arrow of time problem). However, while there is consensus that this solves the initial conditions problem, some have disputed this, as it is much more likely that the Universe came about by a quantum fluctuation. Don Page was an outspoken critic of inflation because of this anomaly. He stressed that the thermodynamic arrow of time necessitates low entropy initial conditions, which would be highly unlikely. According to them, rather than solving this problem, the inflation theory aggravates it \u2013 the reheating at the end of the inflation era increases entropy, making it necessary for the initial state of the Universe to be even more orderly than in other Big Bang theories with no inflation phase.\nHawking and Page later found ambiguous results when they attempted to compute the probability of inflation in the Hartle\u2013Hawking initial state. Other authors have argued that, since inflation is eternal, the probability doesn't matter as long as it is not precisely zero: once it starts, inflation perpetuates itself and quickly dominates the Universe. However, Albrecht and Lorenzo Sorbo argued that the probability of an inflationary cosmos, consistent with today's observations, emerging by a random fluctuation from some pre-existent state is much higher than that of a non-inflationary cosmos. This is because the \"seed\" amount of non-gravitational energy required for the inflationary cosmos is so much less than that for a non-inflationary alternative, which outweighs any entropic considerations.\nAnother problem that has occasionally been mentioned is the trans-Planckian problem or trans-Planckian effects. Since the energy scale of inflation and the Planck scale are relatively close, some of the quantum fluctuations that have made up the structure in our universe were smaller than the Planck length before inflation. Therefore, there ought to be corrections from Planck-scale physics, in particular the unknown quantum theory of gravity. Some disagreement remains about the magnitude of this effect: about whether it is just on the threshold of detectability or completely undetectable.\nHybrid inflation.\nAnother kind of inflation, called \"hybrid inflation\", is an extension of new inflation. It introduces additional scalar fields, so that while one of the scalar fields is responsible for normal slow roll inflation, another triggers the end of inflation: when inflation has continued for sufficiently long, it becomes favorable to the second field to decay into a much lower energy state.\nIn hybrid inflation, one scalar field is responsible for most of the energy density (thus determining the rate of expansion), while another is responsible for the slow roll (thus determining the period of inflation and its termination). Thus fluctuations in the former inflaton would not affect inflation termination, while fluctuations in the latter would not affect the rate of expansion. Therefore, hybrid inflation is not eternal. When the second (slow-rolling) inflaton reaches the bottom of its potential, it changes the location of the minimum of the first inflaton's potential, which leads to a fast roll of the inflaton down its potential, leading to termination of inflation.\nRelation to dark energy.\nDark energy is broadly similar to inflation and is thought to be causing the expansion of the present-day universe to accelerate. However, the energy scale of dark energy is much lower, \u00a0GeV, roughly 27 orders of magnitude less than the scale of inflation.\nInflation and string cosmology.\nThe discovery of flux compactifications opened the way for reconciling inflation and string theory. \"Brane inflation\" suggests that inflation arises from the motion of D-branes in the compactified geometry, usually towards a stack of anti-D-branes. This theory, governed by the \"Dirac\u2013Born\u2013Infeld action\", is different from ordinary inflation. The dynamics are not completely understood. It appears that special conditions are necessary since inflation occurs in tunneling between two vacua in the string landscape. The process of tunneling between two vacua is a form of old inflation, but new inflation must then occur by some other mechanism.\nInflation and loop quantum gravity.\nWhen investigating the effects the theory of loop quantum gravity would have on cosmology, a loop quantum cosmology model has evolved that provides a possible mechanism for cosmological inflation. Loop quantum gravity assumes a quantized spacetime. If the energy density is larger than can be held by the quantized spacetime, it is thought to bounce back.\nAlternatives and adjuncts.\nOther models have been advanced that are claimed to explain some or all of the observations addressed by inflation.\nBig bounce.\nThe big bounce hypothesis attempts to replace the cosmic singularity with a cosmic contraction and bounce, thereby explaining the initial conditions that led to the big bang. The flatness and horizon problems are naturally solved in the Einstein\u2013Cartan\u2013Sciama\u2013Kibble theory of gravity, without needing an exotic form of matter or free parameters. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. The minimal coupling between torsion and Dirac spinors generates a spin-spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction averts the unphysical Big Bang singularity, replacing it with a cusp-like bounce at a finite minimum scale factor, before which the Universe was contracting. The rapid expansion immediately after the Big Bounce explains why the present Universe at largest scales appears spatially flat, homogeneous and isotropic. As the density of the Universe decreases, the effects of torsion weaken and the Universe smoothly enters the radiation-dominated era.\nEkpyrotic and cyclic models.\nThe ekpyrotic and cyclic models are also considered adjuncts to inflation. These models solve the horizon problem through an expanding epoch well \"before\" the Big Bang, and then generate the required spectrum of primordial density perturbations during a contracting phase leading to a Big Crunch. The Universe passes through the Big Crunch and emerges in a hot Big Bang phase. In this sense they are reminiscent of Richard Chace Tolman's oscillatory universe; in Tolman's model, however, the total age of the Universe is necessarily finite, while in these models this is not necessarily so. Whether the correct spectrum of density fluctuations can be produced, and whether the Universe can successfully navigate the Big Bang/Big Crunch transition, remains a topic of controversy and current research. Ekpyrotic models avoid the magnetic monopole problem as long as the temperature at the Big Crunch/Big Bang transition remains below the Grand Unified Scale, as this is the temperature required to produce magnetic monopoles in the first place. As things stand, there is no evidence of any 'slowing down' of the expansion, but this is not surprising as each cycle is expected to last on the order of a trillion years.\nString gas cosmology.\nString theory requires that, in addition to the three observable spatial dimensions, additional dimensions exist that are curled up or compactified (see also Kaluza\u2013Klein theory). Extra dimensions appear as a frequent component of supergravity models and other approaches to quantum gravity. This raised the contingent question of why four space-time dimensions became large and the rest became unobservably small. An attempt to address this question, called \"string gas cosmology\", was proposed by Robert Brandenberger and Cumrun Vafa. This model focuses on the dynamics of the early universe considered as a hot gas of strings. Brandenberger and Vafa show that a dimension of spacetime can only expand if the strings that wind around it can efficiently annihilate each other. Each string is a one-dimensional object, and the largest number of dimensions in which two strings will generically intersect (and, presumably, annihilate) is three. Therefore, the most likely number of non-compact (large) spatial dimensions is three. Current work on this model centers on whether it can succeed in stabilizing the size of the compactified dimensions and produce the correct spectrum of primordial density perturbations. The original model did not \"solve the entropy and flatness problems of standard cosmology\", although Brandenburger and coauthors later argued that these problems can be eliminated by implementing string gas cosmology in the context of a bouncing-universe scenario.\nVarying \"c\".\nCosmological models employing a variable speed of light have been proposed to resolve the horizon problem of and provide an alternative to cosmic inflation. In the VSL models, the fundamental constant \"c\", denoting the speed of light in vacuum, is greater in the early universe than its present value, effectively increasing the particle horizon at the time of decoupling sufficiently to account for the observed isotropy of the CMB.\nCriticisms.\nSince its introduction by Alan Guth in 1980, the inflationary paradigm has become widely accepted. Nevertheless, many physicists, mathematicians, and philosophers of science have voiced criticisms, claiming untestable predictions and a lack of serious empirical support. In 1999, John Earman and Jes\u00fas Moster\u00edn published a thorough critical review of inflationary cosmology, concluding,\nAs pointed out by Roger Penrose from 1986 on, in order to work, inflation requires extremely specific initial conditions of its own, so that the problem (or pseudo-problem) of initial conditions is not solved:\nThe problem of specific or \"fine-tuned\" initial conditions would not have been solved; it would have gotten worse. At a conference in 2015, Penrose said that\nA recurrent criticism of inflation is that the invoked inflaton field does not correspond to any known physical field, and that its potential energy curve seems to be an ad hoc contrivance to accommodate almost any data obtainable. Paul Steinhardt, one of the founding fathers of inflationary cosmology, calls 'bad inflation' a period of accelerated expansion whose outcome conflicts with observations, and 'good inflation' one compatible with them:\nTogether with Anna Ijjas and Abraham Loeb, he wrote articles claiming that the inflationary paradigm is in trouble in view of the data from the Planck satellite.\nCounter-arguments were presented by Alan Guth, David Kaiser, and Yasunori Nomura and by Linde, saying that"}
{"id": "5385", "revid": "43007828", "url": "https://en.wikipedia.org/wiki?curid=5385", "title": "Candela", "text": "The candela (symbol: cd) is the unit of luminous intensity in the International System of Units (SI). It measures luminous power per unit solid angle emitted by a light source in a particular direction. Luminous intensity is analogous to radiant intensity, but instead of simply adding up the contributions of every wavelength of light in the source's spectrum, the contribution of each wavelength is weighted by the luminous efficiency function, the model of the sensitivity of the human eye to different wavelengths, standardized by the CIE and ISO. A common wax candle emits light with a luminous intensity of roughly one candela. If emission in some directions is blocked by an opaque barrier, the emission would still be approximately one candela in the directions that are not obscured.\nThe word \"candela\" is Latin for \"candle\". The old name \"candle\" is still sometimes used, as in \"foot-candle\" and the modern definition of \"candlepower\".\nDefinition.\nThe 26th General Conference on Weights and Measures (CGPM) redefined the candela in 2018. The new definition, which took effect on 20 May 2019, is:\nThe candela [...] is defined by taking the fixed numerical value of the luminous efficacy of monochromatic radiation of frequency , \"K\"cd, to be 683 when expressed in the unit lm\u00a0W\u22121, which is equal to , or , where the kilogram, metre and second are defined in terms of \"h\", \"c\" and \u0394\"\u03bd\"Cs.\nExplanation.\nThe frequency chosen is in the visible spectrum near green, corresponding to a wavelength of about 555\u00a0nanometres. The human eye, when adapted for bright conditions, is most sensitive near this frequency. Under these conditions, photopic vision dominates the visual perception of our eyes over the scotopic vision. At other frequencies, more radiant intensity is required to achieve the same luminous intensity, according to the frequency response of the human eye. The luminous intensity for light of a particular wavelength \"\u03bb\" is given by\nformula_1\nwhere is the luminous intensity, is the radiant intensity and formula_2 is the photopic luminous efficiency function. If more than one wavelength is present (as is usually the case), one must integrate over the spectrum of wavelengths to get the total luminous intensity.\nHistory.\nPrior to 1948, various standards for luminous intensity were in use in a number of countries. These were typically based on the brightness of the flame from a \"standard candle\" of defined composition, or the brightness of an incandescent filament of specific design. One of the best-known of these was the English standard of candlepower. One candlepower was the light produced by a pure spermaceti candle weighing one sixth of a pound and burning at a rate of 120\u00a0grains per hour. Germany, Austria and Scandinavia used the Hefnerkerze, a unit based on the output of a Hefner lamp.\nA better standard for luminous intensity was needed. In 1884, Jules Violle had proposed a standard based on the light emitted by 1\u00a0cm2 of platinum at its melting point (or freezing point). The resulting unit of intensity, called the \"violle\", was roughly equal to 60 English candlepower. Platinum was convenient for this purpose because it had a high enough melting point, was not prone to oxidation, and could be obtained in pure form. Violle showed that the intensity emitted by pure platinum was strictly dependent on its temperature, and so platinum at its melting point should have a consistent luminous intensity. \nIn practice, realizing a standard based on Violle's proposal turned out to be more difficult than expected. Impurities on the surface of the platinum could directly affect its emissivity, and in addition impurities could affect the luminous intensity by altering the melting point. Over the following half century various scientists tried to make a practical intensity standard based on incandescent platinum. The successful approach was to suspend a hollow shell of thorium dioxide with a small hole in it in a bath of molten platinum. The shell (cavity) serves as a black body, producing black-body radiation that depends on the temperature and is not sensitive to details of how the device is constructed.\nIn 1937, the \"Commission Internationale de l'\u00c9clairage\" (International Commission on Illumination) and the CIPM proposed a \"new candle\" based on this concept, with value chosen to make it similar to the earlier unit candlepower. The decision was promulgated by the CIPM in 1946:\nThe value of the new candle is such that the brightness of the full radiator at the temperature of solidification of platinum is 60\u00a0new candles per square centimetre.\nIt was then ratified in 1948 by the 9th\u00a0CGPM which adopted a new name for this unit, the \"candela\". In 1967 the 13th CGPM removed the term \"new candle\" and gave an amended version of the candela definition, specifying the atmospheric pressure applied to the freezing platinum:\nThe candela is the luminous intensity, in the perpendicular direction, of a surface of square\u00a0metre of a black body at the temperature of freezing platinum under a pressure of \u00a0newtons per square\u00a0metre.\nIn 1979, because of the difficulties in realizing a Planck radiator at high temperatures and the new possibilities offered by radiometry, the 16th\u00a0CGPM adopted a new definition of the candela:\nThe candela is the luminous intensity, in a given direction, of a source that emits monochromatic radiation of frequency and that has a radiant intensity in that direction of \u00a0watt per steradian.\nThe definition describes how to produce a light source that (by definition) emits one candela, but does not specify the luminous efficiency function for weighting radiation at other frequencies. Such a source could then be used to calibrate instruments designed to measure luminous intensity with reference to a specified luminous efficiency function. An appendix to the SI Brochure makes it clear that the luminous efficiency function is not uniquely specified, but must be selected to fully define the candela.\nThe arbitrary (1/683) term was chosen so that the new definition would precisely match the old definition. Although the candela is now defined in terms of the second (an SI base unit) and the watt (a derived SI unit), the candela remains a base unit of the SI system, by definition.\nThe 26th CGPM approved the modern definition of the candela in 2018 as part of the 2019 revision of the SI, which redefined the SI base units in terms of fundamental physical constants.\nSI photometric light units.\nRelationships between luminous intensity, luminous flux, and illuminance.\nIf a source emits a known luminous intensity (in candelas) in a well-defined cone, the total luminous flux in lumens is given by\nformula_4\nwhere is the \"radiation angle\" of the lamp\u2014the full vertex angle of the emission cone. For example, a lamp that emits 590\u00a0cd with a radiation angle of 40\u00b0 emits about 224\u00a0lumens. See MR16 for emission angles of some common lamps.\nIf the source emits light uniformly in all directions, the flux can be found by multiplying the intensity by 4: a uniform 1\u00a0candela source emits 4 lumens (approximately 12.566\u00a0lumens).\nFor the purpose of measuring illumination, the candela is not a practical unit, as it only applies to idealized point light sources, each approximated by a source small compared to the distance from which its luminous radiation is measured, also assuming that it is done so in the absence of other light sources. What gets directly measured by a light meter is incident light on a sensor of finite area, i.e. illuminance in lm/m2 (lux). However, if designing illumination from many point light sources, like light bulbs, of known approximate omnidirectionally uniform intensities, the contributions to illuminance from incoherent light being additive, it is mathematically estimated as follows. If is the position of the \"i\"th source of uniform intensity , and is the unit vector normal to the illuminated elemental opaque area being measured, and provided that all light sources lie in the same half-space divided by the plane of this area,\nformula_5\nIn the case of a single point light source of intensity \"Iv\", at a distance \"r\" and normally incident, this reduces to\nformula_6\nSI multiples.\nLike other SI units, the candela can also be modified by adding a metric prefix that multiplies it by a power of 10, for example millicandela (mcd) for 10\u22123 candela."}
{"id": "5387", "revid": "1260885373", "url": "https://en.wikipedia.org/wiki?curid=5387", "title": "Condensed matter physics", "text": "Condensed matter physics is the field of physics that deals with the macroscopic and microscopic physical properties of matter, especially the solid and liquid phases, that arise from electromagnetic forces between atoms and electrons. More generally, the subject deals with condensed phases of matter: systems of many constituents with strong interactions among them. More exotic condensed phases include the superconducting phase exhibited by certain materials at extremely low cryogenic temperatures, the ferromagnetic and antiferromagnetic phases of spins on crystal lattices of atoms, the Bose\u2013Einstein condensates found in ultracold atomic systems, and liquid crystals. Condensed matter physicists seek to understand the behavior of these phases by experiments to measure various material properties, and by applying the physical laws of quantum mechanics, electromagnetism, statistical mechanics, and other physics theories to develop mathematical models and predict the properties of extremely large groups of atoms.\nThe diversity of systems and phenomena available for study makes condensed matter physics the most active field of contemporary physics: one third of all American physicists self-identify as condensed matter physicists, and the Division of Condensed Matter Physics is the largest division of the American Physical Society. These include solid state and soft matter physicists, who study quantum and non-quantum physical properties of matter respectively. Both types study a great range of materials, providing many research, funding and employment opportunities. The field overlaps with chemistry, materials science, engineering and nanotechnology, and relates closely to atomic physics and biophysics. The theoretical physics of condensed matter shares important concepts and methods with that of particle physics and nuclear physics.\nA variety of topics in physics such as crystallography, metallurgy, elasticity, magnetism, etc., were treated as distinct areas until the 1940s, when they were grouped together as \"solid-state physics\". Around the 1960s, the study of physical properties of liquids was added to this list, forming the basis for the more comprehensive specialty of condensed matter physics. The Bell Telephone Laboratories was one of the first institutes to conduct a research program in condensed matter physics. According to the founding director of the Max Planck Institute for Solid State Research, physics professor Manuel Cardona, it was Albert Einstein who created the modern field of condensed matter physics starting with his seminal 1905 article on the photoelectric effect and photoluminescence which opened the fields of photoelectron spectroscopy and photoluminescence spectroscopy, and later his 1907 article on the specific heat of solids which introduced, for the first time, the effect of lattice vibrations on the thermodynamic properties of crystals, in particular the specific heat. Deputy Director of the Yale Quantum Institute A. Douglas Stone makes a similar priority case for Einstein in his work on the synthetic history of quantum mechanics.\nEtymology.\nAccording to physicist Philip Warren Anderson, the use of the term \"condensed matter\" to designate a field of study was coined by him and Volker Heine, when they changed the name of their group at the Cavendish Laboratories, Cambridge, from \"Solid state theory\" to \"Theory of Condensed Matter\" in 1967, as they felt it better included their interest in liquids, nuclear matter, and so on. Although Anderson and Heine helped popularize the name \"condensed matter\", it had been used in Europe for some years, most prominently in the Springer-Verlag journal \"Physics of Condensed Matter\", launched in 1963. The name \"condensed matter physics\" emphasized the commonality of scientific problems encountered by physicists working on solids, liquids, plasmas, and other complex matter, whereas \"solid state physics\" was often associated with restricted industrial applications of metals and semiconductors. In the 1960s and 70s, some physicists felt the more comprehensive name better fit the funding environment and Cold War politics of the time.\nReferences to \"condensed\" states can be traced to earlier sources. For example, in the introduction to his 1947 book \"Kinetic Theory of Liquids\", Yakov Frenkel proposed that \"The kinetic theory of liquids must accordingly be developed as a generalization and extension of the kinetic theory of solid bodies. As a matter of fact, it would be more correct to unify them under the title of 'condensed bodies.\nHistory.\nClassical physics.\nOne of the first studies of condensed states of matter was by English chemist Humphry Davy, in the first decades of the nineteenth century. Davy observed that of the forty chemical elements known at the time, twenty-six had metallic properties such as lustre, ductility and high electrical and thermal conductivity. This indicated that the atoms in John Dalton's atomic theory were not indivisible as Dalton claimed, but had inner structure. Davy further claimed that elements that were then believed to be gases, such as nitrogen and hydrogen could be liquefied under the right conditions and would then behave as metals.\nIn 1823, Michael Faraday, then an assistant in Davy's lab, successfully liquefied chlorine and went on to liquefy all known gaseous elements, except for nitrogen, hydrogen, and oxygen. Shortly after, in 1869, Irish chemist Thomas Andrews studied the phase transition from a liquid to a gas and coined the term critical point to describe the condition where a gas and a liquid were indistinguishable as phases, and Dutch physicist Johannes van der Waals supplied the theoretical framework which allowed the prediction of critical behavior based on measurements at much higher temperatures. By 1908, James Dewar and Heike Kamerlingh Onnes were successfully able to liquefy hydrogen and the then newly discovered helium respectively.\nPaul Drude in 1900 proposed the first theoretical model for a classical electron moving through a metallic solid. Drude's model described properties of metals in terms of a gas of free electrons, and was the first microscopic model to explain empirical observations such as the Wiedemann\u2013Franz law. However, despite the success of Drude's model, it had one notable problem: it was unable to correctly explain the electronic contribution to the specific heat and magnetic properties of metals, and the temperature dependence of resistivity at low temperatures.\nIn 1911, three years after helium was first liquefied, Onnes working at University of Leiden discovered superconductivity in mercury, when he observed the electrical resistivity of mercury to vanish at temperatures below a certain value. The phenomenon completely surprised the best theoretical physicists of the time, and it remained unexplained for several decades. Albert Einstein, in 1922, said regarding contemporary theories of superconductivity that \"with our far-reaching ignorance of the quantum mechanics of composite systems we are very far from being able to compose a theory out of these vague ideas.\"\nAdvent of quantum mechanics.\nDrude's classical model was augmented by Wolfgang Pauli, Arnold Sommerfeld, Felix Bloch and other physicists. Pauli realized that the free electrons in metal must obey the Fermi\u2013Dirac statistics. Using this idea, he developed the theory of paramagnetism in 1926. Shortly after, Sommerfeld incorporated the Fermi\u2013Dirac statistics into the free electron model and made it better to explain the heat capacity. Two years later, Bloch used quantum mechanics to describe the motion of an electron in a periodic lattice.\nThe mathematics of crystal structures developed by Auguste Bravais, Yevgraf Fyodorov and others was used to classify crystals by their symmetry group, and tables of crystal structures were the basis for the series \"International Tables of Crystallography\", first published in 1935. Band structure calculations were first used in 1930 to predict the properties of new materials, and in 1947 John Bardeen, Walter Brattain and William Shockley developed the first semiconductor-based transistor, heralding a revolution in electronics.\nIn 1879, Edwin Herbert Hall working at the Johns Hopkins University discovered that a voltage developed across conductors which was transverse to both an electric current in the conductor and a magnetic field applied perpendicular to the current. This phenomenon, arising due to the nature of charge carriers in the conductor, came to be termed the Hall effect, but it was not properly explained at the time because the electron was not experimentally discovered until 18 years later. After the advent of quantum mechanics, Lev Landau in 1930 developed the theory of Landau quantization and laid the foundation for a theoretical explanation of the quantum Hall effect which was discovered half a century later.\nMagnetism as a property of matter has been known in China since 4000 BC. However, the first modern studies of magnetism only started with the development of electrodynamics by Faraday, Maxwell and others in the nineteenth century, which included classifying materials as ferromagnetic, paramagnetic and diamagnetic based on their response to magnetization. Pierre Curie studied the dependence of magnetization on temperature and discovered the Curie point phase transition in ferromagnetic materials. In 1906, Pierre Weiss introduced the concept of magnetic domains to explain the main properties of ferromagnets. The first attempt at a microscopic description of magnetism was by Wilhelm Lenz and Ernst Ising through the Ising model that described magnetic materials as consisting of a periodic lattice of spins that collectively acquired magnetization. The Ising model was solved exactly to show that spontaneous magnetization can occur in one dimension and it is possible in higher-dimensional lattices. Further research such as by Bloch on spin waves and N\u00e9el on antiferromagnetism led to developing new magnetic materials with applications to magnetic storage devices.\nModern many-body physics.\nThe Sommerfeld model and spin models for ferromagnetism illustrated the successful application of quantum mechanics to condensed matter problems in the 1930s. However, there still were several unsolved problems, most notably the description of superconductivity and the Kondo effect. After World War II, several ideas from quantum field theory were applied to condensed matter problems. These included recognition of collective excitation modes of solids and the important notion of a quasiparticle. Soviet physicist Lev Landau used the idea for the Fermi liquid theory wherein low energy properties of interacting fermion systems were given in terms of what are now termed Landau-quasiparticles. Landau also developed a mean-field theory for continuous phase transitions, which described ordered phases as spontaneous breakdown of symmetry. The theory also introduced the notion of an order parameter to distinguish between ordered phases. Eventually in 1956, John Bardeen, Leon Cooper and Robert Schrieffer developed the so-called BCS theory of superconductivity, based on the discovery that arbitrarily small attraction between two electrons of opposite spin mediated by phonons in the lattice can give rise to a bound state called a Cooper pair.\nThe study of phase transitions and the critical behavior of observables, termed critical phenomena, was a major field of interest in the 1960s. Leo Kadanoff, Benjamin Widom and Michael Fisher developed the ideas of critical exponents and widom scaling. These ideas were unified by Kenneth G. Wilson in 1972, under the formalism of the renormalization group in the context of quantum field theory.\nThe quantum Hall effect was discovered by Klaus von Klitzing, Dorda and Pepper in 1980 when they observed the Hall conductance to be integer multiples of a fundamental constant formula_1.(see figure) The effect was observed to be independent of parameters such as system size and impurities. In 1981, theorist Robert Laughlin proposed a theory explaining the unanticipated precision of the integral plateau. It also implied that the Hall conductance is proportional to a topological invariant, called Chern number, whose relevance for the band structure of solids was formulated by David J. Thouless and collaborators. Shortly after, in 1982, Horst St\u00f6rmer and Daniel Tsui observed the fractional quantum Hall effect where the conductance was now a rational multiple of the constant formula_1. Laughlin, in 1983, realized that this was a consequence of quasiparticle interaction in the Hall states and formulated a variational method solution, named the Laughlin wavefunction. The study of topological properties of the fractional Hall effect remains an active field of research. Decades later, the aforementioned topological band theory advanced by David J. Thouless and collaborators was further expanded leading to the discovery of topological insulators. \nIn 1986, Karl M\u00fcller and Johannes Bednorz discovered the first high temperature superconductor, La2-xBaxCuO4, which is superconducting at temperatures as high as 39 kelvin. It was realized that the high temperature superconductors are examples of strongly correlated materials where the electron\u2013electron interactions play an important role. A satisfactory theoretical description of high-temperature superconductors is still not known and the field of strongly correlated materials continues to be an active research topic.\nIn 2012, several groups released preprints which suggest that samarium hexaboride has the properties of a topological insulator in accord with the earlier theoretical predictions. Since samarium hexaboride is an established Kondo insulator, i.e. a strongly correlated electron material, it is expected that the existence of a topological Dirac surface state in this material would lead to a topological insulator with strong electronic correlations.\nTheoretical.\nTheoretical condensed matter physics involves the use of theoretical models to understand properties of states of matter. These include models to study the electronic properties of solids, such as the Drude model, the band structure and the density functional theory. Theoretical models have also been developed to study the physics of phase transitions, such as the Ginzburg\u2013Landau theory, critical exponents and the use of mathematical methods of quantum field theory and the renormalization group. Modern theoretical studies involve the use of numerical computation of electronic structure and mathematical tools to understand phenomena such as high-temperature superconductivity, topological phases, and gauge symmetries.\nEmergence.\nTheoretical understanding of condensed matter physics is closely related to the notion of emergence, wherein complex assemblies of particles behave in ways dramatically different from their individual constituents. For example, a range of phenomena related to high temperature superconductivity are understood poorly, although the microscopic physics of individual electrons and lattices is well known. Similarly, models of condensed matter systems have been studied where collective excitations behave like photons and electrons, thereby describing electromagnetism as an emergent phenomenon. Emergent properties can also occur at the interface between materials: one example is the lanthanum aluminate-strontium titanate interface, where two band-insulators are joined to create conductivity and superconductivity.\nElectronic theory of solids.\nThe metallic state has historically been an important building block for studying properties of solids. The first theoretical description of metals was given by Paul Drude in 1900 with the Drude model, which explained electrical and thermal properties by describing a metal as an ideal gas of then-newly discovered electrons. He was able to derive the empirical Wiedemann-Franz law and get results in close agreement with the experiments. This classical model was then improved by Arnold Sommerfeld who incorporated the Fermi\u2013Dirac statistics of electrons and was able to explain the anomalous behavior of the specific heat of metals in the Wiedemann\u2013Franz law. In 1912, The structure of crystalline solids was studied by Max von Laue and Paul Knipping, when they observed the X-ray diffraction pattern of crystals, and concluded that crystals get their structure from periodic lattices of atoms. In 1928, Swiss physicist Felix Bloch provided a wave function solution to the Schr\u00f6dinger equation with a periodic potential, known as Bloch's theorem.\nCalculating electronic properties of metals by solving the many-body wavefunction is often computationally hard, and hence, approximation methods are needed to obtain meaningful predictions. The Thomas\u2013Fermi theory, developed in the 1920s, was used to estimate system energy and electronic density by treating the local electron density as a variational parameter. Later in the 1930s, Douglas Hartree, Vladimir Fock and John Slater developed the so-called Hartree\u2013Fock wavefunction as an improvement over the Thomas\u2013Fermi model. The Hartree\u2013Fock method accounted for exchange statistics of single particle electron wavefunctions. In general, it is very difficult to solve the Hartree\u2013Fock equation. Only the free electron gas case can be solved exactly. Finally in 1964\u201365, Walter Kohn, Pierre Hohenberg and Lu Jeu Sham proposed the density functional theory (DFT) which gave realistic descriptions for bulk and surface properties of metals. The density functional theory has been widely used since the 1970s for band structure calculations of variety of solids.\nSymmetry breaking.\nSome states of matter exhibit \"symmetry breaking\", where the relevant laws of physics possess some form of symmetry that is broken. A common example is crystalline solids, which break continuous translational symmetry. Other examples include magnetized ferromagnets, which break rotational symmetry, and more exotic states such as the ground state of a BCS superconductor, that breaks U(1) phase rotational symmetry.\nGoldstone's theorem in quantum field theory states that in a system with broken continuous symmetry, there may exist excitations with arbitrarily low energy, called the Goldstone bosons. For example, in crystalline solids, these correspond to phonons, which are quantized versions of lattice vibrations.\nPhase transition.\nPhase transition refers to the change of phase of a system, which is brought about by change in an external parameter such as temperature, pressure, or molar composition. In a single-component system, a classical phase transition occurs at a temperature (at a specific pressure) where there is an abrupt change in the order of the system. For example, when ice melts and becomes water, the ordered hexagonal crystal structure of ice is modified to a hydrogen bonded, mobile arrangement of water molecules.\nIn quantum phase transitions, the temperature is set to absolute zero, and the non-thermal control parameter, such as pressure or magnetic field, causes the phase transitions when order is destroyed by quantum fluctuations originating from the Heisenberg uncertainty principle. Here, the different quantum phases of the system refer to distinct ground states of the Hamiltonian matrix. Understanding the behavior of quantum phase transition is important in the difficult tasks of explaining the properties of rare-earth magnetic insulators, high-temperature superconductors, and other substances.\nTwo classes of phase transitions occur: \"first-order transitions\" and \"second-order\" or \"continuous transitions\". For the latter, the two phases involved do not co-exist at the transition temperature, also called the critical point. Near the critical point, systems undergo critical behavior, wherein several of their properties such as correlation length, specific heat, and magnetic susceptibility diverge exponentially. These critical phenomena present serious challenges to physicists because normal macroscopic laws are no longer valid in the region, and novel ideas and methods must be invented to find the new laws that can describe the system.\nThe simplest theory that can describe continuous phase transitions is the Ginzburg\u2013Landau theory, which works in the so-called mean-field approximation. However, it can only roughly explain continuous phase transition for ferroelectrics and type I superconductors which involves long range microscopic interactions. For other types of systems that involves short range interactions near the critical point, a better theory is needed.\nNear the critical point, the fluctuations happen over broad range of size scales while the feature of the whole system is scale invariant. Renormalization group methods successively average out the shortest wavelength fluctuations in stages while retaining their effects into the next stage. Thus, the changes of a physical system as viewed at different size scales can be investigated systematically. The methods, together with powerful computer simulation, contribute greatly to the explanation of the critical phenomena associated with continuous phase transition.\nExperimental.\nExperimental condensed matter physics involves the use of experimental probes to try to discover new properties of materials. Such probes include effects of electric and magnetic fields, measuring response functions, transport properties and thermometry. Commonly used experimental methods include spectroscopy, with probes such as X-rays, infrared light and inelastic neutron scattering; study of thermal response, such as specific heat and measuring transport via thermal and heat conduction.\nScattering.\nSeveral condensed matter experiments involve scattering of an experimental probe, such as X-ray, optical photons, neutrons, etc., on constituents of a material. The choice of scattering probe depends on the observation energy scale of interest. Visible light has energy on the scale of 1 electron volt (eV) and is used as a scattering probe to measure variations in material properties such as the dielectric constant and refractive index. X-rays have energies of the order of 10 keV and hence are able to probe atomic length scales, and are used to measure variations in electron charge density and crystal structure.\nNeutrons can also probe atomic length scales and are used to study the scattering off nuclei and electron spins and magnetization (as neutrons have spin but no charge). Coulomb and Mott scattering measurements can be made by using electron beams as scattering probes. Similarly, positron annihilation can be used as an indirect measurement of local electron density. Laser spectroscopy is an excellent tool for studying the microscopic properties of a medium, for example, to study forbidden transitions in media with nonlinear optical spectroscopy. \nExternal magnetic fields.\nIn experimental condensed matter physics, external magnetic fields act as thermodynamic variables that control the state, phase transitions and properties of material systems. Nuclear magnetic resonance (NMR) is a method by which external magnetic fields are used to find resonance modes of individual nuclei, thus giving information about the atomic, molecular, and bond structure of their environment. NMR experiments can be made in magnetic fields with strengths up to 60 tesla. Higher magnetic fields can improve the quality of NMR measurement data. Quantum oscillations is another experimental method where high magnetic fields are used to study material properties such as the geometry of the Fermi surface. High magnetic fields will be useful in experimental testing of the various theoretical predictions such as the quantized magnetoelectric effect, image magnetic monopole, and the half-integer quantum Hall effect.\nMagnetic resonance spectroscopy.\nThe local structure, as well as the structure of the nearest neighbour atoms, can be investigated in condensed matter with magnetic resonance methods, such as electron paramagnetic resonance (EPR) and nuclear magnetic resonance (NMR), which are very sensitive to the details of the surrounding of nuclei and electrons by means of the hyperfine coupling. Both localized electrons and specific stable or unstable isotopes of the nuclei become the probe of these hyperfine interactions), which couple the electron or nuclear spin to the local electric and magnetic fields. These methods are suitable to study defects, diffusion, phase transitions and magnetic order. Common experimental methods include NMR, nuclear quadrupole resonance (NQR), implanted radioactive probes as in the case of muon spin spectroscopy (formula_3SR), M\u00f6ssbauer spectroscopy, formula_4NMR and perturbed angular correlation (PAC). PAC is especially ideal for the study of phase changes at extreme temperatures above 2000\u00a0\u00b0C due to the temperature independence of the method.\nCold atomic gases.\nUltracold atom trapping in optical lattices is an experimental tool commonly used in condensed matter physics, and in atomic, molecular, and optical physics. The method involves using optical lasers to form an interference pattern, which acts as a \"lattice\", in which ions or atoms can be placed at very low temperatures. Cold atoms in optical lattices are used as \"quantum simulators\", that is, they act as controllable systems that can model behavior of more complicated systems, such as frustrated magnets. In particular, they are used to engineer one-, two- and three-dimensional lattices for a Hubbard model with pre-specified parameters, and to study phase transitions for antiferromagnetic and spin liquid ordering.\nIn 1995, a gas of rubidium atoms cooled down to a temperature of 170 nK was used to experimentally realize the Bose\u2013Einstein condensate, a novel state of matter originally predicted by S. N. Bose and Albert Einstein, wherein a large number of atoms occupy one quantum state.\nApplications.\nResearch in condensed matter physics has given rise to several device applications, such as the development of the semiconductor transistor, laser technology, magnetic storage, liquid crystals, optical fibres and several phenomena studied in the context of nanotechnology. Methods such as scanning-tunneling microscopy can be used to control processes at the nanometer scale, and have given rise to the study of nanofabrication. Such molecular machines were developed for example by Nobel laureates in chemistry Ben Feringa, Jean-Pierre Sauvage and Fraser Stoddart. Feringa and his team developed multiple molecular machines such as the molecular car, molecular windmill and many more.\nIn quantum computation, information is represented by quantum bits, or qubits. The qubits may decohere quickly before useful computation is completed. This serious problem must be solved before quantum computing may be realized. To solve this problem, several promising approaches are proposed in condensed matter physics, including Josephson junction qubits, spintronic qubits using the spin orientation of magnetic materials, and the topological non-Abelian anyons from fractional quantum Hall effect states.\nCondensed matter physics also has important uses for biomedicine. For example, magnetic resonance imaging is widely used in medical imaging of soft tissue and other physiological features which cannot be viewed with traditional x-ray imaging."}
{"id": "5388", "revid": "44514456", "url": "https://en.wikipedia.org/wiki?curid=5388", "title": "Cultural anthropology", "text": "Cultural anthropology is a branch of anthropology focused on the study of cultural variation among humans. It is in contrast to social anthropology, which perceives cultural variation as a subset of a posited anthropological constant. The term sociocultural anthropology includes both cultural and social anthropology traditions.\nAnthropologists have pointed out that through culture, people can adapt to their environment in non-genetic ways, so people living in different environments will often have different cultures. Much of anthropological theory has originated in an appreciation of and interest in the tension between the local (particular cultures) and the global (a universal human nature, or the web of connections between people in distinct places/circumstances).\nCultural anthropology has a rich methodology, including participant observation (often called fieldwork because it requires the anthropologist spending an extended period of time at the research location), interviews, and surveys.\nHistory.\nModern anthropology emerged in the 19th century coinciding with significant developments in the Western world. These changes sparked a renewed interest in understanding humankind, particularly, its origins, unity, and plurality. However, it was in the 20th century that cultural anthropology began to adopt a more pluralistic perspective on cultures and societies.\nThe rise of cultural anthropology took place within the context of the late 19th century, when questions regarding which cultures were \"primitive\" and which were \"civilized\" occupied the mind of not only Freud, but many others. Colonialism and its processes increasingly brought European thinkers into direct or indirect contact with \"primitive others\". The first generation of cultural anthropologists were interested in the relative status of various humans, some of whom had modern advanced technologies, while others lacked anything but face-to-face communication techniques and still lived a Paleolithic lifestyle.\nTheoretical foundations.\nThe concept of culture.\nOne of the earliest articulations of the anthropological meaning of the term \"culture\" came from Sir Edward Tylor: \"Culture, or civilization, taken in its broad, ethnographic sense, is that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society.\" The term \"civilization\" later gave way to definitions given by V. Gordon Childe, with culture forming an umbrella term and civilization becoming a particular kind of culture.\nAccording to Kay Milton, former director of anthropology research at Queens University Belfast, culture can be general or specific. This means culture can be something applied to all human beings or it can be specific to a certain group of people such as African American culture or Irish American culture. Specific cultures are structured systems which means they are organized very specifically and adding or taking away any element from that system may disrupt it.\nThe critique of evolutionism.\nAnthropology is concerned with the lives of people in different parts of the world, particularly in relation to the discourse of beliefs and practices. In addressing this question, ethnologists in the 19th century divided into two schools of thought. Some, like Grafton Elliot Smith, argued that different groups must have learned from one another somehow, however indirectly; in other words, they argued that cultural traits spread from one place to another, or \"diffused\".\nOther ethnologists argued that different groups had the capability of creating similar beliefs and practices independently. Some of those who advocated \"independent invention\", like Lewis Henry Morgan, additionally supposed that similarities meant that different groups had passed through the same stages of cultural evolution (See also classical social evolutionism). Morgan, in particular, acknowledged that certain forms of society and culture could not possibly have arisen before others. For example, industrial farming could not have been invented before simple farming, and metallurgy could not have developed without previous non-smelting processes involving metals (such as simple ground collection or mining). Morgan, like other 19th century social evolutionists, believed there was a more or less orderly progression from the primitive to the civilized.\n20th-century anthropologists largely reject the notion that all human societies must pass through the same stages in the same order, on the grounds that such a notion does not fit the empirical facts. Some 20th-century ethnologists, like Julian Steward, have instead argued that such similarities reflected similar adaptations to similar environments. Although 19th-century ethnologists saw \"diffusion\" and \"independent invention\" as mutually exclusive and competing theories, most ethnographers quickly reached a consensus that both processes occur, and that both can plausibly account for cross-cultural similarities. But these ethnographers also pointed out the superficiality of many such similarities. They noted that even traits that spread through diffusion often were given different meanings and function from one society to another. Analyses of large human concentrations in big cities, in multidisciplinary studies by Ronald Daus, show how new methods may be applied to the understanding of man living in a global world and how it was caused by the action of extra-European nations, so highlighting the role of Ethics in modern anthropology.\nAccordingly, most of these anthropologists showed less interest in comparing cultures, generalizing about human nature, or discovering universal laws of cultural development, than in understanding particular cultures in those cultures' own terms. Such ethnographers and their students promoted the idea of \"cultural relativism\", the view that one can only understand another person's beliefs and behaviors in the context of the culture in which they live or lived.\nOthers, such as Claude L\u00e9vi-Strauss (who was influenced both by American cultural anthropology and by French Durkheimian sociology), have argued that apparently similar patterns of development reflect fundamental similarities in the structure of human thought (see structuralism). By the mid-20th century, the number of examples of people skipping stages, such as going from hunter-gatherers to post-industrial service occupations in one generation, were so numerous that 19th-century evolutionism was effectively disproved.\nCultural relativism.\nCultural relativism is a principle that was established as axiomatic in anthropological research by Franz Boas and later popularized by his students. Boas first articulated the idea in 1887: \"...civilization is not something absolute, but ... is relative, and ... our ideas and conceptions are true only so far as our civilization goes.\" Although Boas did not coin the term, it became common among anthropologists after Boas' death in 1942, to express their synthesis of a number of ideas Boas had developed. Boas believed that the sweep of cultures, to be found in connection with any sub-species, is so vast and pervasive that there cannot be a relationship between culture and race. Cultural relativism involves specific epistemological and methodological claims. Whether or not these claims require a specific ethical stance is a matter of debate. This principle should not be confused with moral relativism.\nCultural relativism was in part a response to Western ethnocentrism. Ethnocentrism may take obvious forms, in which one consciously believes that one's people's arts are the most beautiful, values the most virtuous, and beliefs the most truthful. Boas, originally trained in physics and geography, and heavily influenced by the thought of Kant, Herder, and von Humboldt, argued that one's culture may mediate and thus limit one's perceptions in less obvious ways. This understanding of culture confronts anthropologists with two problems: first, how to escape the unconscious bonds of one's own culture, which inevitably bias our perceptions of and reactions to the world, and second, how to make sense of an unfamiliar culture. The principle of cultural relativism thus forced anthropologists to develop innovative methods and heuristic strategies.\nBoas and his students realized that if they were to conduct scientific research in other cultures, they would need to employ methods that would help them escape the limits of their own ethnocentrism. One such method is that of ethnography. This method advocates living with people of another culture for an extended period of time to learn the local language and be enculturated, at least partially, into that culture. In this context, cultural relativism is of fundamental methodological importance, because it calls attention to the importance of the local context in understanding the meaning of particular human beliefs and activities. Thus, in 1948 Virginia Heyer wrote, \"Cultural relativity, to phrase it in starkest abstraction, states the relativity of the part to the whole. The part gains its cultural significance by its place in the whole, and cannot retain its integrity in a different situation.\"\nComparison with social anthropology.\nThe rubric \"cultural\" anthropology is generally applied to ethnographic works that are holistic in approach, are oriented to the ways in which culture affects individual experience or aim to provide a rounded view of the knowledge, customs, and institutions of a people. \"Social\" anthropology is a term applied to ethnographic works that attempt to isolate a particular system of social relations such as those that comprise domestic life, economy, law, politics, or religion, give analytical priority to the organizational bases of social life, and attend to cultural phenomena as somewhat secondary to the main issues of social scientific inquiry.\nParallel with the rise of cultural anthropology in the United States, social anthropology developed as an academic discipline in Britain and in France.\nFoundational thinkers.\nLewis Henry Morgan.\nLewis Henry Morgan (1818\u20131881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from \"savagery\", to \"barbarism\", to \"civilization\". Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.\nFranz Boas, founder of the modern discipline.\nFranz Boas (1858\u20131942) established academic anthropology in the United States in opposition to Morgan's evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature.\nInfluenced by the German tradition, Boas argued that the world was full of distinct \"cultures,\" rather than societies whose evolution could be measured by the extent of \"civilization\" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible.\nIn doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas. Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called \"Four Field Approach\" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture.\nKroeber, Mead, and Benedict.\nBoas used his positions at Columbia University and the American Museum of Natural History (AMNH) to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir, and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages.\nThe publication of Alfred Kroeber's textbook \"Anthropology\" (1923) marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up.\nThough such works as Mead's \"Coming of Age in Samoa\" (1928) and Benedict's \"The Chrysanthemum and the Sword\" (1946) remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined in favor of Ralph Linton, and Mead was limited to her offices at the AMNH.\nWolf, Sahlins, Mintz, and political economy.\nIn the 1950s and mid-1960s anthropology tended increasingly to model itself after the natural sciences. Some anthropologists, such as Lloyd Fallers and Clifford Geertz, focused on processes of modernization by which newly independent states could develop. Others, such as Julian Steward and Leslie White, focused on how societies evolve and fit their ecological niche\u2014an approach popularized by Marvin Harris.\nEconomic anthropology as influenced by Karl Polanyi and practiced by Marshall Sahlins and George Dalton challenged standard neoclassical economics to take account of cultural and social factors and employed Marxian analysis into anthropological study. In England, British Social Anthropology's paradigm began to fragment as Max Gluckman and Peter Worsley experimented with Marxism and authors such as Rodney Needham and Edmund Leach incorporated L\u00e9vi-Strauss's structuralism into their work. Structuralism also influenced a number of developments in the 1960s and 1970s, including cognitive anthropology and componential analysis.\nIn keeping with the times, much of anthropology became politicized through the Algerian War of Independence and opposition to the Vietnam War; Marxism became an increasingly popular theoretical approach in the discipline. By the 1970s the authors of volumes such as \"Reinventing Anthropology\" worried about anthropology's relevance.\nSince the 1980s issues of power, such as those examined in Eric Wolf's \"Europe and the People Without History\", have been central to the discipline. In the 1980s books like \"Anthropology and the Colonial Encounter\" pondered anthropology's ties to colonial inequality, while the immense popularity of theorists such as Antonio Gramsci and Michel Foucault moved issues of power and hegemony into the spotlight. Gender and sexuality became popular topics, as did the relationship between history and anthropology, influenced by Marshall Sahlins, who drew on L\u00e9vi-Strauss and Fernand Braudel to examine the relationship between symbolic meaning, sociocultural structure, and individual agency in the processes of historical transformation. Jean and John Comaroff produced a whole generation of anthropologists at the University of Chicago that focused on these themes. Also influential in these issues were Nietzsche, Heidegger, the critical theory of the Frankfurt School, Derrida and Lacan.\nGeertz, Schneider, and interpretive anthropology.\nMany anthropologists reacted against the renewed emphasis on materialism and scientific modelling derived from Marx by emphasizing the importance of the concept of culture. Authors such as David Schneider, Clifford Geertz, and Marshall Sahlins developed a more fleshed-out concept of culture as a web of meaning or signification, which proved very popular within and beyond the discipline. Geertz was to state:\nGeertz's interpretive method involved what he called \"thick description\". The cultural symbols of rituals, political and economic action, and of kinship, are \"read\" by the anthropologist as if they are a document in a foreign language. The interpretation of those symbols must be re-framed for their anthropological audience, i.e. transformed from the \"experience-near\" but foreign concepts of the other culture, into the \"experience-distant\" theoretical concepts of the anthropologist. These interpretations must then be reflected back to its originators, and its adequacy as a translation fine-tuned in a repeated way, a process called the hermeneutic circle. Geertz applied his method in a number of areas, creating programs of study that were very productive. His analysis of \"religion as a cultural system\" was particularly influential outside of anthropology. David Schnieder's cultural analysis of American kinship has proven equally influential. Schneider demonstrated that the American folk-cultural emphasis on \"blood connections\" had an undue influence on anthropological kinship theories, and that kinship is not a biological characteristic, but a cultural relationship established on very different terms in different societies.\nProminent British symbolic anthropologists include Victor Turner and Mary Douglas.\nThe post-modern turn.\nIn the late 1980s and 1990s authors such as James Clifford pondered ethnographic authority, in particular how and why anthropological knowledge was possible and authoritative. They were reflecting trends in research and discourse initiated by feminists in the academy, although they excused themselves from commenting specifically on those pioneering critics. Nevertheless, key aspects of feminist theory and methods became \"de rigueur\" as part of the 'post-modern moment' in anthropology: Ethnographies became more interpretative and reflexive, explicitly addressing the author's methodology; cultural, gendered, and racial positioning; and their influence on the ethnographic analysis. This was part of a more general trend of postmodernism that was popular contemporaneously. Currently anthropologists pay attention to a wide variety of issues pertaining to the contemporary world, including globalization, medicine and biotechnology, indigenous rights, virtual communities, and the anthropology of industrialized societies.\nMethods.\nModern cultural anthropology has its origins in, and developed in reaction to, 19th century ethnology, which involves the organized comparison of human societies. Scholars like E.B. Tylor and J.G. Frazer in England worked mostly with materials collected by others\u2014usually missionaries, traders, explorers, or colonial officials\u2014earning them the moniker of \"arm-chair anthropologists\".\nParticipant observation.\nParticipant observation is one of the principal research methods of cultural anthropology. It relies on the assumption that the best way to understand a group of people is to interact with them closely over a long period of time. The method originated in the field research of social anthropologists, especially Bronislaw Malinowski in Britain, the students of Franz Boas in the United States, and in the later urban research of the Chicago School of Sociology. Historically, the group of people being studied was a small, non-Western society. However, today it may be a specific corporation, a church group, a sports team, or a small town. There are no restrictions as to what the subject of participant observation can be, as long as the group of people is studied intimately by the observing anthropologist over a long period of time. This allows the anthropologist to develop trusting relationships with the subjects of study and receive an inside perspective on the culture, which helps him or her to give a richer description when writing about the culture later. Observable details (like daily time allotment) and more hidden details (like taboo behavior) are more easily observed and interpreted over a longer period of time, and researchers can discover discrepancies between what participants say\u2014and often believe\u2014should happen (the formal system) and what actually does happen, or between different aspects of the formal system; in contrast, a one-time survey of people's answers to a set of questions might be quite consistent, but is less likely to show conflicts between different aspects of the social system or between conscious representations and behavior.\nInteractions between an ethnographer and a cultural informant must go both ways. Just as an ethnographer may be naive or curious about a culture, the members of that culture may be curious about the ethnographer. To establish connections that will eventually lead to a better understanding of the cultural context of a situation, an anthropologist must be open to becoming part of the group, and willing to develop meaningful relationships with its members. One way to do this is to find a small area of common experience between an anthropologist and their subjects, and then to expand from this common ground into the larger area of difference. Once a single connection has been established, it becomes easier to integrate into the community, and it is more likely that accurate and complete information is being shared with the anthropologist.\nBefore participant observation can begin, an anthropologist must choose both a location and a focus of study. This focus may change once the anthropologist is actively observing the chosen group of people, but having an idea of what one wants to study before beginning fieldwork allows an anthropologist to spend time researching background information on their topic. It can also be helpful to know what previous research has been conducted in one's chosen location or on similar topics, and if the participant observation takes place in a location where the spoken language is not one the anthropologist is familiar with, they will usually also learn that language. This allows the anthropologist to become better established in the community. The lack of need for a translator makes communication more direct, and allows the anthropologist to give a richer, more contextualized representation of what they witness. In addition, participant observation often requires permits from governments and research institutions in the area of study, and always needs some form of funding.\nThe majority of participant observation is based on conversation. This can take the form of casual, friendly dialogue, or can also be a series of more structured interviews. A combination of the two is often used, sometimes along with photography, mapping, artifact collection, and various other methods. In some cases, ethnographers also turn to structured observation, in which an anthropologist's observations are directed by a specific set of questions they are trying to answer. In the case of structured observation, an observer might be required to record the order of a series of events, or describe a certain part of the surrounding environment. While the anthropologist still makes an effort to become integrated into the group they are studying, and still participates in the events as they observe, structured observation is more directed and specific than participant observation in general. This helps to standardize the method of study when ethnographic data is being compared across several groups or is needed to fulfill a specific purpose, such as research for a governmental policy decision.\nOne common criticism of participant observation is its lack of objectivity. Because each anthropologist has their own background and set of experiences, each individual is likely to interpret the same culture in a different way. Who the ethnographer is has a lot to do with what they will eventually write about a culture, because each researcher is influenced by their own perspective. This is considered a problem especially when anthropologists write in the ethnographic present, a present tense which makes a culture seem stuck in time, and ignores the fact that it may have interacted with other cultures or gradually evolved since the anthropologist made observations. To avoid this, past ethnographers have advocated for strict training, or for anthropologists working in teams. However, these approaches have not generally been successful, and modern ethnographers often choose to include their personal experiences and possible biases in their writing instead.\nParticipant observation has also raised ethical questions, since an anthropologist is in control of what they report about a culture. In terms of representation, an anthropologist has greater power than their subjects of study, and this has drawn criticism of participant observation in general. Additionally, anthropologists have struggled with the effect their presence has on a culture. Simply by being present, a researcher causes changes in a culture, and anthropologists continue to question whether or not it is appropriate to influence the cultures they study, or possible to avoid having influence.\nEthnography.\nIn the 20th century, most cultural and social anthropologists turned to the crafting of ethnographies. An ethnography is a piece of writing about a people, at a particular place and time. Typically, the anthropologist lives among people in another society for a period of time, simultaneously participating in and observing the social and cultural life of the group.\nNumerous other ethnographic techniques have resulted in ethnographic writing or details being preserved, as cultural anthropologists also curate materials, spend long hours in libraries, churches and schools poring over records, investigate graveyards, and decipher ancient scripts. A typical ethnography will also include information about physical geography, climate and habitat. It is meant to be a holistic piece of writing about the people in question, and today often includes the longest possible timeline of past events that the ethnographer can obtain through primary and secondary research.\nBronis\u0142aw Malinowski developed the ethnographic method, and Franz Boas taught it in the United States. Boas' students such as Alfred L. Kroeber, Ruth Benedict and Margaret Mead drew on his conception of culture and cultural relativism to develop cultural anthropology in the United States. Simultaneously, Malinowski and A.R. Radcliffe Brown's students were developing social anthropology in the United Kingdom. Whereas cultural anthropology focused on symbols and values, social anthropology focused on social groups and institutions. Today socio-cultural anthropologists attend to all these elements.\nIn the early 20th century, socio-cultural anthropology developed in different forms in Europe and in the United States. European \"social anthropologists\" focused on observed social behaviors and on \"social structure\", that is, on relationships among social roles (for example, husband and wife, or parent and child) and social institutions (for example, religion, economy, and politics).\nAmerican \"cultural anthropologists\" focused on the ways people expressed their view of themselves and their world, especially in symbolic forms, such as art and myths. These two approaches frequently converged and generally complemented one another. For example, kinship and leadership function both as symbolic systems and as social institutions. Today almost all socio-cultural anthropologists refer to the work of both sets of predecessors and have an equal interest in what people do and in what people say.\nCross-cultural comparison.\nOne means by which anthropologists combat ethnocentrism is to engage in the process of cross-cultural comparison. It is important to test so-called \"human universals\" against the ethnographic record. Monogamy, for example, is frequently touted as a universal human trait, yet comparative study shows that it is not. The Human Relations Area Files, Inc. (HRAF) is a research agency based at Yale University. Since 1949, its mission has been to encourage and facilitate worldwide comparative studies of human culture, society, and behavior in the past and present. The name came from the Institute of Human Relations, an interdisciplinary program/building at Yale at the time. The Institute of Human Relations had sponsored HRAF's precursor, the \"Cross-Cultural Survey\" (see George Peter Murdock), as part of an effort to develop an integrated science of human behavior and culture. The two eHRAF databases on the Web are expanded and updated annually. \"eHRAF World Cultures\" includes materials on cultures, past and present, and covers nearly 400 cultures. The second database, \"eHRAF Archaeology\", covers major archaeological traditions and many more sub-traditions and sites around the world.\nComparison across cultures includes the industrialized (or de-industrialized) West. Cultures in the more traditional standard cross-cultural sample of small-scale societies are:\nMulti-sited ethnography.\nEthnography dominates socio-cultural anthropology. Nevertheless, many contemporary socio-cultural anthropologists have rejected earlier models of ethnography as treating local cultures as bounded and isolated. These anthropologists continue to concern themselves with the distinct ways people in different locales experience and understand their lives, but they often argue that one cannot understand these particular ways of life solely from a local perspective; they instead combine a focus on the local with an effort to grasp larger political, economic, and cultural frameworks that impact local lived realities. Notable proponents of this approach include Arjun Appadurai, James Clifford, George Marcus, Sidney Mintz, Michael Taussig, Eric Wolf and Ronald Daus.\nA growing trend in anthropological research and analysis is the use of multi-sited ethnography, discussed in George Marcus' article, \"Ethnography In/Of the World System: the Emergence of Multi-Sited Ethnography\". Looking at culture as embedded in macro-constructions of a global social order, multi-sited ethnography uses traditional methodology in various locations both spatially and temporally. Through this methodology, greater insight can be gained when examining the impact of world-systems on local and global communities.\nAlso emerging in multi-sited ethnography are greater interdisciplinary approaches to fieldwork, bringing in methods from cultural studies, media studies, science and technology studies, and others. In multi-sited ethnography, research tracks a subject across spatial and temporal boundaries. For example, a multi-sited ethnography may follow a \"thing\", such as a particular commodity, as it is transported through the networks of global capitalism.\nMulti-sited ethnography may also follow ethnic groups in diaspora, stories or rumours that appear in multiple locations and in multiple time periods, metaphors that appear in multiple ethnographic locations, or the biographies of individual people or groups as they move through space and time. It may also follow conflicts that transcend boundaries. An example of multi-sited ethnography is Nancy Scheper-Hughes' work on the international black market for the trade of human organs. In this research, she follows organs as they are transferred through various legal and illegal networks of capitalism, as well as the rumours and urban legends that circulate in impoverished communities about child kidnapping and organ theft.\nSociocultural anthropologists have increasingly turned their investigative eye on to \"Western\" culture. For example, Philippe Bourgois won the Margaret Mead Award in 1997 for \"In Search of Respect\", a study of the entrepreneurs in a Harlem crack-den. Also growing more popular are ethnographies of professional communities, such as laboratory researchers, Wall Street investors, law firms, or information technology (IT) computer employees.\nTopics.\nKinship and family.\nKinship refers to the anthropological study of the ways in which humans form and maintain relationships with one another and how those relationships operate within and define social organization.\nResearch in kinship studies often crosses over into different anthropological subfields including medical, feminist, and public anthropology. This is likely due to its fundamental concepts, as articulated by linguistic anthropologist Patrick McConvell: Throughout history, kinship studies have primarily focused on the topics of marriage, descent, and procreation. Anthropologists have written extensively on the variations within marriage across cultures and its legitimacy as a human institution. There are stark differences between communities in terms of marital practice and value, leaving much room for anthropological fieldwork. For instance, the Nuer of Sudan and the Brahmans of Nepal practice polygyny, where one man has several marriages to two or more women. The Nyar of India and Nyimba of Tibet and Nepal practice polyandry, where one woman is often married to two or more men. The marital practice found in most cultures, however, is monogamy, where one woman is married to one man. Anthropologists also study different marital taboos across cultures, most commonly the incest taboo of marriage within sibling and parent-child relationships. It has been found that all cultures have an incest taboo to some degree, but the taboo shifts between cultures when the marriage extends beyond the nuclear family unit.\nThere are similar foundational differences where the act of procreation is concerned. Although anthropologists have found that biology is acknowledged in every cultural relationship to procreation, there are differences in the ways in which cultures assess the constructs of parenthood. For example, in the Nuyoo municipality of Oaxaca, Mexico, it is believed that a child can have partible maternity and partible paternity. In this case, a child would have multiple biological mothers in the case that it is born of one woman and then breastfed by another. A child would have multiple biological fathers in the case that the mother had sex with multiple men, following the commonplace belief in Nuyoo culture that pregnancy must be preceded by sex with multiple men in order have the necessary accumulation of semen.\nLate twentieth-century shifts in interest.\nIn the twenty-first century, Western ideas of kinship have evolved beyond the traditional assumptions of the nuclear family, raising anthropological questions of consanguinity, lineage, and normative marital expectation. The shift can be traced back to the 1960s, with the reassessment of kinship's basic principles offered by Edmund Leach, Rodney Neeham, David Schneider, and others. Instead of relying on narrow ideas of Western normalcy, kinship studies increasingly catered to \"more ethnographic voices, human agency, intersecting power structures, and historical context\". The study of kinship evolved to accommodate for the fact that it cannot be separated from its institutional roots and must pay respect to the society in which it lives, including that society's contradictions, hierarchies, and individual experiences of those within it. This shift was progressed further by the emergence of second-wave feminism in the early 1970s, which introduced ideas of marital oppression, sexual autonomy, and domestic subordination. Other themes that emerged during this time included the frequent comparisons between Eastern and Western kinship systems and the increasing amount of attention paid to anthropologists' own societies, a swift turn from the focus that had traditionally been paid to largely \"foreign\", non-Western communities.\nKinship studies began to gain mainstream recognition in the late 1990s with the surging popularity of feminist anthropology, particularly with its work related to biological anthropology and the intersectional critique of gender relations. At this time, there was the arrival of \"Third World feminism\", a movement that argued kinship studies could not examine the gender relations of developing countries in isolation and must pay respect to racial and economic nuance as well. This critique became relevant, for instance, in the anthropological study of Jamaica: race and class were seen as the primary obstacles to Jamaican liberation from economic imperialism, and gender as an identity was largely ignored. Third World feminism aimed to combat this in the early twenty-first century by promoting these categories as coexisting factors. In Jamaica, marriage as an institution is often substituted for a series of partners, as poor women cannot rely on regular financial contributions in a climate of economic instability. In addition, there is a common practice of Jamaican women artificially lightening their skin tones in order to secure economic survival. These anthropological findings, according to Third World feminism, cannot see gender, racial, or class differences as separate entities, and instead must acknowledge that they interact together to produce unique individual experiences.\nRise of reproductive anthropology.\nKinship studies have also experienced a rise in the interest of reproductive anthropology with the advancement of assisted reproductive technologies (ARTs), including in vitro fertilization (IVF). These advancements have led to new dimensions of anthropological research, as they challenge the Western standard of biogenetically based kinship, relatedness, and parenthood. According to anthropologists Maria C. Inhorn and Daphna Birenbaum-Carmeli, \"ARTs have pluralized notions of relatedness and led to a more dynamic notion of \"kinning\" namely, kinship as a process, as something under construction, rather than a natural given\". With this technology, questions of kinship have emerged over the difference between biological and genetic relatedness, as gestational surrogates can provide a biological environment for the embryo while the genetic ties remain with a third party. If genetic, surrogate, and adoptive maternities are involved, anthropologists have acknowledged that there can be the possibility for three \"biological\" mothers to a single child. With ARTs, there are also anthropological questions concerning the intersections between wealth and fertility: ARTs are generally only available to those in the highest income bracket, meaning the infertile poor are inherently devalued in the system. There have also been issues of reproductive tourism and bodily commodification, as individuals seek economic security through hormonal stimulation and egg harvesting, which are potentially harmful procedures. With IVF, specifically, there have been many questions of embryotic value and the status of life, particularly as it relates to the manufacturing of stem cells, testing, and research.\nCurrent issues in kinship studies, such as adoption, have revealed and challenged the Western cultural disposition towards the genetic, \"blood\" tie. Western biases against single parent homes have also been explored through similar anthropological research, uncovering that a household with a single parent experiences \"greater levels of scrutiny and [is] routinely seen as the 'other' of the nuclear, patriarchal family\". The power dynamics in reproduction, when explored through a comparative analysis of \"conventional\" and \"unconventional\" families, have been used to dissect the Western assumptions of child bearing and child rearing in contemporary kinship studies.\nCritiques of kinship studies.\nKinship, as an anthropological field of inquiry, has been heavily criticized across the discipline. One critique is that, as its inception, the framework of kinship studies was far too structured and formulaic, relying on dense language and stringent rules. Another critique, explored at length by American anthropologist David Schneider, argues that kinship has been limited by its inherent Western ethnocentrism. Schneider proposes that kinship is not a field that can be applied cross-culturally, as the theory itself relies on European assumptions of normalcy. He states in the widely circulated 1984 book \"A critique of the study of kinship\" that \"[K]inship has been defined by European social scientists, and European social scientists use their own folk culture as the source of many, if not all of their ways of formulating and understanding the world about them\". However, this critique has been challenged by the argument that it is linguistics, not cultural divergence, that has allowed for a European bias, and that the bias can be lifted by centering the methodology on fundamental human concepts. Polish anthropologist Anna Wierzbicka argues that \"mother\" and \"father\" are examples of such fundamental human concepts and can only be Westernized when conflated with English concepts such as \"parent\" and \"sibling\".\nA more recent critique of kinship studies is its solipsistic focus on privileged, Western human relations and its promotion of normative ideals of human exceptionalism. In \"Critical Kinship Studies\", social psychologists Elizabeth Peel and Damien Riggs argue for a move beyond this human-centered framework, opting instead to explore kinship through a \"posthumanist\" vantage point where anthropologists focus on the intersecting relationships of human animals, non-human animals, technologies and practices.\nInstitutional anthropology.\nThe role of anthropology in institutions has expanded significantly since the end of the 20th century. Much of this development can be attributed to the rise in anthropologists working outside of academia and the increasing importance of globalization in both institutions and the field of anthropology. Anthropologists can be employed by institutions such as for-profit business, nonprofit organizations, and governments. For instance, cultural anthropologists are commonly employed by the United States federal government.\nThe two types of institutions defined in the field of anthropology are total institutions and social institutions. Total institutions are places that comprehensively coordinate the actions of people within them, and examples of total institutions include prisons, convents, and hospitals. Social institutions, on the other hand, are constructs that regulate individuals' day-to-day lives, such as kinship, religion, and economics. Anthropology of institutions may analyze labor unions, businesses ranging from small enterprises to corporations, government, medical organizations, education, prisons, and financial institutions. Nongovernmental organizations have garnered particular interest in the field of institutional anthropology because they are capable of fulfilling roles previously ignored by governments, or previously realized by families or local groups, in an attempt to mitigate social problems.\nThe types and methods of scholarship performed in the anthropology of institutions can take a number of forms. Institutional anthropologists may study the relationship between organizations or between an organization and other parts of society. Institutional anthropology may also focus on the inner workings of an institution, such as the relationships, hierarchies and cultures formed, and the ways that these elements are transmitted and maintained, transformed, or abandoned over time. Additionally, some anthropology of institutions examines the specific design of institutions and their corresponding strength. More specifically, anthropologists may analyze specific events within an institution, perform semiotic investigations, or analyze the mechanisms by which knowledge and culture are organized and dispersed.\nIn all manifestations of institutional anthropology, participant observation is critical to understanding the intricacies of the way an institution works and the consequences of actions taken by individuals within it. Simultaneously, anthropology of institutions extends beyond examination of the commonplace involvement of individuals in institutions to discover how and why the organizational principles evolved in the manner that they did.\nCommon considerations taken by anthropologists in studying institutions include the physical location at which a researcher places themselves, as important interactions often take place in private, and the fact that the members of an institution are often being examined in their workplace and may not have much idle time to discuss the details of their everyday endeavors. The ability of individuals to present the workings of an institution in a particular light or frame must additionally be taken into account when using interviews and document analysis to understand an institution, as the involvement of an anthropologist may be met with distrust when information being released to the public is not directly controlled by the institution and could potentially be damaging."}
{"id": "5390", "revid": "6036800", "url": "https://en.wikipedia.org/wiki?curid=5390", "title": "Conversion of units", "text": "Conversion of units is the conversion of the unit of measurement in which a quantity is expressed, typically through a multiplicative conversion factor that changes the unit without changing the quantity. This is also often loosely taken to include replacement of a quantity with a corresponding quantity that describes the same physical property.\nUnit conversion is often easier within a metric system such as the SI than in others, due to the system's coherence and its metric prefixes that act as power-of-10 multipliers. \nOverview.\nThe definition and choice of units in which to express a quantity may depend on the specific situation and the intended purpose. This may be governed by regulation, contract, technical specifications or other published standards. Engineering judgment may include such factors as:\nFor some purposes, conversions from one system of units to another are needed to be exact, without increasing or decreasing the precision of the expressed quantity. An \"adaptive conversion\" may not produce an exactly equivalent expression. Nominal values are sometimes allowed and used.\nFactor\u2013label method.\nThe factor\u2013label method, also known as the unit\u2013factor method or the unity bracket method, is a widely used technique for unit conversions that uses the rules of algebra.\nThe factor\u2013label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to metres per second by using a sequence of conversion factors as shown below:\nformula_1\nEach conversion factor is chosen based on the relationship between one of the original units and one of the desired units (or some intermediary unit), before being rearranged to create a factor that cancels out the original unit. For example, as \"mile\" is the numerator in the original fraction and , \"mile\" will need to be the denominator in the conversion factor. Dividing both sides of the equation by 1 mile yields , which when simplified results in the dimensionless . Because of the identity property of multiplication, multiplying any quantity (physical or not) by the dimensionless 1 does not change that quantity. Once this and the conversion factor for seconds per hour have been multiplied by the original fraction to cancel out the units \"mile\" and \"hour\", 10 miles per hour converts to 4.4704 metres per second.\nAs a more complex example, the concentration of nitrogen oxides (NO\"x\") in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (g/h) of NO\"x\" by using the following information as shown below:\nAfter cancelling any dimensional units that appear both in the numerators and the denominators of the fractions in the above equation, the NO\"x\" concentration of 10\u00a0ppmv converts to mass flow rate of 24.63\u00a0grams per hour.\nChecking equations that involve dimensions.\nThe factor\u2013label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.\nFor example, check the universal gas law equation of , when:\nformula_3\nAs can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units. Dimensional analysis can be used as a tool to construct equations that relate non-associated physico-chemical properties. The equations may reveal undiscovered or overlooked properties of matter, in the form of left-over dimensions \u2013 dimensional adjusters \u2013 that can then be assigned physical significance. It is important to point out that such 'mathematical manipulation' is neither without prior precedent, nor without considerable scientific significance. Indeed, the Planck constant, a fundamental physical constant, was 'discovered' as a purely mathematical abstraction or representation that built on the Rayleigh\u2013Jeans law for preventing the ultraviolet catastrophe. It was assigned and ascended to its quantum physical significance either in tandem or post mathematical dimensional adjustment \u2013 not earlier.\nLimitations.\nThe factor\u2013label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0 (ratio scale in Stevens's typology). Most conversions fit this paradigm. An example for which it cannot be used is the conversion between the Celsius scale and the Kelvin scale (or the Fahrenheit scale). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (, rather than a linear transform ) between them.\nFor example, the freezing point of water is 0\u00a0\u00b0C and 32\u00a0\u00b0F, and a 5\u00a0\u00b0C change is the same as a 9\u00a0\u00b0F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32\u00a0\u00b0F (the offset from the point of reference), divides by 9\u00a0\u00b0F and multiplies by 5\u00a0\u00b0C (scales by the ratio of units), and adds 0\u00a0\u00b0C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100\u00a0\u00b0C and 212\u00a0\u00b0F, which yields the same formula.\nHence, to convert the numerical quantity value of a temperature \"T\"[F] in degrees Fahrenheit to a numerical quantity value \"T\"[C] in degrees Celsius, this formula may be used:\nTo convert \"T\"[C] in degrees Celsius to \"T\"[F] in degrees Fahrenheit, this formula may be used:\nExample.\nStarting with:\nreplace the original unit with its meaning in terms of the desired unit , e.g. if , then:\nNow and are both numerical values, so just calculate their product.\nOr, which is just mathematically the same thing, multiply \"Z\" by unity, the product is still \"Z\":\nFor example, you have an expression for a physical value \"Z\" involving the unit \"feet per second\" () and you want it in terms of the unit \"miles per hour\" ():\nOr as an example using the metric system, you have a value of fuel economy in the unit \"litres per 100 kilometres\" and you want it in terms of the unit \"microlitres per metre\":\nCalculation involving non-SI Units.\nIn the cases where non-SI units are used, the numerical calculation of a formula can be done by first working out the factor, and then plug in the numerical values of the given/known quantities.\nFor example, in the study of Bose\u2013Einstein condensate, atomic mass is usually given in daltons, instead of kilograms, and chemical potential is often given in the Boltzmann constant times nanokelvin. The condensate's healing length is given by:\nformula_9\nFor a 23Na condensate with chemical potential of (the Boltzmann constant times) 128\u00a0nK, the calculation of healing length (in micrometres) can be done in two steps:\nCalculate the factor.\nAssume that , this gives\nformula_10\nwhich is our factor.\nCalculate the numbers.\nNow, make use of the fact that . With , .\nThis method is especially useful for programming and/or making a worksheet, where input quantities are taking multiple different values; For example, with the factor calculated above, it is very easy to see that the healing length of 174Yb with chemical potential 20.3\u00a0nK is \nSoftware tools.\nThere are many conversion tools. They are found in the function libraries of applications such as spreadsheets databases, in calculators, and in macro packages and plugins for many other applications such as the mathematical, scientific and technical applications.\nThere are many standalone applications that offer the thousands of the various units with conversions. For example, the free software movement offers a command line utility GNU units for GNU and Windows. The Unified Code for Units of Measure is also a popular option."}
{"id": "5391", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=5391", "title": "City", "text": "A city is a human settlement of a substantial size. The term \"city\" has different meanings around the world and in some places the settlement can be very small. Even where the term is limited to larger settlements, there is no universally agreed definition of the lower boundary for their size. In a narrower sense, a city can be defined as a permanent and densely populated place with administratively defined boundaries whose members work primarily on non-agricultural tasks. Cities generally have extensive systems for housing, transportation, sanitation, utilities, land use, production of goods, and communication. Their density facilitates interaction between people, government organizations, and businesses, sometimes benefiting different parties in the process, such as improving the efficiency of goods and service distribution.\nHistorically, city dwellers have been a small proportion of humanity overall, but following two centuries of unprecedented and rapid urbanization, more than half of the world population now lives in cities, which has had profound consequences for global sustainability. Present-day cities usually form the core of larger metropolitan areas and urban areas\u2014creating numerous commuters traveling toward city centres for employment, entertainment, and education. However, in a world of intensifying globalization, all cities are to varying degrees also connected globally beyond these regions. This increased influence means that cities also have significant influences on global issues, such as sustainable development, climate change, and global health. Because of these major influences on global issues, the international community has prioritized investment in sustainable cities through Sustainable Development Goal 11. Due to the efficiency of transportation and the smaller land consumption, dense cities hold the potential to have a smaller ecological footprint per inhabitant than more sparsely populated areas. Therefore, compact cities are often referred to as a crucial element in fighting climate change. However, this concentration can also have some significant negative consequences, such as forming urban heat islands, concentrating pollution, and stressing water supplies and other resources.\nMeaning.\nA city can be distinguished from other human settlements by its relatively great size, but also by its functions and its special symbolic status, which may be conferred by a central authority. The term can also refer either to the physical streets and buildings of the city or to the collection of people who dwell there and can be used in a general sense to mean urban rather than rural territory.\nNational censuses use a variety of definitions \u2013 invoking factors such as population, population density, number of dwellings, economic function, and infrastructure \u2013 to classify populations as urban. Typical working definitions for small-city populations start at around 100,000 people. Common population definitions for an urban area (city or town) range between 1,500 and 50,000 people, with most U.S. states using a minimum between 1,500 and 5,000 inhabitants. Some jurisdictions set no such minima. In the United Kingdom, city status is awarded by the Crown and then remains permanent. (Historically, the qualifying factor was the presence of a cathedral, resulting in some very small cities such as Wells, with a population of 12,000 , and St Davids, with a population of 1,841 .) According to the \"functional definition\", a city is not distinguished by size alone, but also by the role it plays within a larger political context. Cities serve as administrative, commercial, religious, and cultural hubs for their larger surrounding areas.\nThe presence of a\u00a0literate elite\u00a0is often associated with cities because of the cultural diversities present in a city. A typical city has professional administrators, regulations, and some form of taxation (food and other necessities or means to trade for them) to support the government workers. (This arrangement contrasts with the more typically horizontal relationships in a tribe or village accomplishing common goals through informal agreements between neighbors, or the leadership of a chief.) The governments may be based on heredity, religion, military power, work systems such as canal-building, food distribution, land-ownership, agriculture, commerce, manufacturing, finance, or a combination of these. Societies that live in cities are often called civilizations.\nThe \"degree of urbanization\" is a modern metric to help define what comprises a city: \"a population of at least 50,000 inhabitants in contiguous dense grid cells (&gt;1,500 inhabitants per square kilometer)\". This metric was \"devised over years by the European Commission, OECD, World Bank and others, and endorsed in March [2021] by the United Nations ... largely for the purpose of international statistical comparison\".\nEtymology.\nThe word \"city\" and the related \"civilization\" come from the Latin root , originally meaning 'citizenship' or 'community member' and eventually coming to correspond with , meaning 'city' in a more physical sense. The Roman \"civitas\" was closely linked with the Greek \u2014another common root appearing in English words such as \"metropolis\".\nIn toponymic terminology, names of individual cities and towns are called \"astionyms\" (from Ancient Greek 'city or town' and 'name').\nGeography.\nUrban geography deals both with cities in their larger context and with their internal structure. Cities are estimated to cover about 3% of the land surface of the Earth.\nSite.\nTown siting has varied through history according to natural, technological, economic, and military contexts. Access to water has long been a major factor in city placement and growth, and despite exceptions enabled by the advent of rail transport in the nineteenth century, through the present most of the world's urban population lives near the coast or on a river.\nUrban areas as a rule cannot produce their own food and therefore must develop some relationship with a hinterland that sustains them. Only in special cases such as mining towns which play a vital role in long-distance trade, are cities disconnected from the countryside which feeds them. Thus, centrality within a productive region influences siting, as economic forces would, in theory, favor the creation of marketplaces in optimal mutually reachable locations.\nCenter.\nThe vast majority of cities have a central area containing buildings with special economic, political, and religious significance. Archaeologists refer to this area by the Greek term temenos or if fortified as a citadel. These spaces historically reflect and amplify the city's centrality and importance to its wider sphere of influence. Today cities have a city center or downtown, sometimes coincident with a central business district.\nPublic space.\nCities typically have public spaces where anyone can go. These include privately owned spaces open to the public as well as forms of public land such as public domain and the commons. Western philosophy since the time of the Greek agora has considered physical public space as the substrate of the symbolic public sphere. Public art adorns (or disfigures) public spaces. Parks and other natural sites within cities provide residents with relief from the hardness and regularity of typical built environments. Urban green spaces are another component of public space that provides the benefit of mitigating the urban heat island effect, especially in cities that are in warmer climates. These spaces prevent carbon imbalances, extreme habitat losses, electricity and water consumption, and human health risks.\nInternal structure.\nThe urban structure generally follows one or more basic patterns: geomorphic, radial, concentric, rectilinear, and curvilinear. The physical environment generally constrains the form in which a city is built. If located on a mountainside, urban structures may rely on terraces and winding roads. It may be adapted to its means of subsistence (e.g. agriculture or fishing). And it may be set up for optimal defense given the surrounding landscape. Beyond these \"geomorphic\" features, cities can develop internal patterns, due to natural growth or to city planning.\nIn a radial structure, main roads converge on a central point. This form could evolve from successive growth over a long time, with concentric traces of town walls and citadels marking older city boundaries. In more recent history, such forms were supplemented by ring roads moving traffic around the outskirts of a town. Dutch cities such as Amsterdam and Haarlem are structured as a central square surrounded by concentric canals marking every expansion. In cities such as Moscow, this pattern is still clearly visible.\nA system of rectilinear city streets and land plots, known as the grid plan, has been used for millennia in Asia, Europe, and the Americas. The Indus Valley Civilization built Mohenjo-Daro, Harappa, and other cities on a grid pattern, using ancient principles described by Kautilya, and aligned with the compass points. The ancient Greek city of Priene exemplifies a grid plan with specialized districts used across the Hellenistic Mediterranean.\nUrban areas.\nThe urban-type settlement extends far beyond the traditional boundaries of the city proper in a form of development sometimes described critically as urban sprawl. Decentralization and dispersal of city functions (commercial, industrial, residential, cultural, political) has transformed the very meaning of the term and has challenged geographers seeking to classify territories according to an urban-rural binary.\nMetropolitan areas include suburbs and exurbs organized around the needs of commuters, and sometimes edge cities characterized by a degree of economic and political independence. (In the US these are grouped into metropolitan statistical areas for purposes of demography and marketing.) Some cities are now part of a continuous urban landscape called urban agglomeration, conurbation, or megalopolis (exemplified by the BosWash corridor of the Northeastern United States.)\nHistory.\nThe emergence of cities from proto-urban settlements, such as \u00c7atalh\u00f6y\u00fck, is a non-linear development that demonstrates the varied experiences of early urbanization.\nThe cities of Jericho, Aleppo, Byblos, Faiyum, Yerevan, Athens, Matera, Damascus, and Argos are among those laying claim to the longest continual inhabitation.\nCities, characterized by population density, symbolic function, and urban planning, have existed for thousands of years. In the conventional view, civilization and the city were both followed by the development of agriculture, which enabled the production of surplus food and thus a social division of labor (with concomitant social stratification) and trade. Early cities often featured granaries, sometimes within a temple. A minority viewpoint considers that cities may have arisen without agriculture, due to alternative means of subsistence (fishing), to use as communal seasonal shelters, to their value as bases for defensive and offensive military organization, or to their inherent economic function. Cities played a crucial role in the establishment of political power over an area, and ancient leaders such as Alexander the Great founded and created them with zeal.\nAncient times.\nJericho and \u00c7atalh\u00f6y\u00fck, dated to the eighth millennium BC, are among the earliest proto-cities known to archaeologists. However, the Mesopotamian city of Uruk from the mid-fourth millennium BC (ancient Iraq) is considered by most archaeologists to be the first true city, innovating many characteristics for cities to follow, with its name attributed to the Uruk period.\nIn the fourth and third millennium BC, complex civilizations flourished in the river valleys of Mesopotamia, India, China, and Egypt. Excavations in these areas have found the ruins of cities geared variously towards trade, politics, or religion. Some had large, dense populations, but others carried out urban activities in the realms of politics or religion without having large associated populations.\nAmong the early Old World cities, Mohenjo-daro of the Indus Valley civilization in present-day Pakistan, existing from about 2600 BC, was one of the largest, with a population of 50,000 or more and a sophisticated sanitation system. China's planned cities were constructed according to sacred principles to act as celestial microcosms.\nThe Ancient Egyptian cities known physically by archaeologists are not extensive. They include (known by their Arab names) El Lahun, a workers' town associated with the pyramid of Senusret II, and the religious city Amarna built by Akhenaten and abandoned. These sites appear planned in a highly regimented and stratified fashion, with a minimalistic grid of rooms for the workers and increasingly more elaborate housing available for higher classes.\nIn Mesopotamia, the civilization of Sumer, followed by Assyria and Babylon, gave rise to numerous cities, governed by kings and fostered multiple languages written in cuneiform. The Phoenician trading empire, flourishing around the turn of the first millennium BC, encompassed numerous cities extending from Tyre, Cydon, and Byblos to Carthage and C\u00e1diz.\nIn the following centuries, independent city-states of Greece, especially Athens, developed the \"polis\", an association of male landowning citizens who collectively constituted the city. The agora, meaning \"gathering place\" or \"assembly\", was the center of the athletic, artistic, spiritual, and political life of the polis. Rome was the first city that surpassed one million inhabitants. Under the authority of its empire, Rome transformed and founded many cities (), and with them brought its principles of urban architecture, design, and society.\nIn the ancient Americas, early urban traditions developed in the Andes and Mesoamerica. In the Andes, the first urban centers developed in the Norte Chico civilization, Chavin and Moche cultures, followed by major cities in the Huari, Chimu, and Inca cultures. The Norte Chico civilization included as many as 30 major population centers in what is now the Norte Chico region of north-central coastal Peru. It is the oldest known civilization in the Americas, flourishing between the 30th and 18th centuries BC. Mesoamerica saw the rise of early urbanism in several cultural regions, beginning with the Olmec and spreading to the Preclassic Maya, the Zapotec of Oaxaca, and Teotihuacan in central Mexico. Later cultures such as the Aztec, Andean civilizations, Mayan, Mississippians, and Pueblo peoples drew on these earlier urban traditions. Many of their ancient cities continue to be inhabited, including major metropolitan cities such as Mexico City, in the same location as Tenochtitlan; while ancient continuously inhabited Pueblos are near modern urban areas in New Mexico, such as Acoma Pueblo near the Albuquerque metropolitan area and Taos Pueblo near Taos; while others like Lima are located nearby ancient Peruvian sites such as Pachacamac.\nFrom 1600 BC, Dhar Tichitt, in the south of present-day Mauritania, presented characteristics suggestive of an incipient form of urbanism. The second place to show urban characteristics in West Africa was Dia, in present-day Mali, from 800 BC. Both Dhar Tichitt and Dia were founded by the same people: the Soninke, who would later also found the Ghana Empire.\nAnother ancient site, Jenn\u00e9-Jeno, in what is today Mali, has been dated to the third century BCE. According to Roderick and Susan McIntosh, Jenn\u00e9-Jeno did not fit into traditional Western conceptions of urbanity as it lacked monumental architecture and a distinctive elite social class, but it should indeed be considered a city based on a functional redefinition of urban development. In particular, Jenn\u00e9-Jeno featured settlement mounds arranged according to a horizontal, rather than vertical, power hierarchy, and served as a center of specialized production and exhibited functional interdependence with the surrounding hinterland.\nMore recently, scholars have concluded that the civilization of Djenne-Djenno was likely established by the Mande progenitors of the Bozo people. Their habitation of the site spanned the period from 3rd century BCE to 13th century CE. Archaeological evidence from Jenn\u00e9-Jeno, specifically the presence of non-West African glass beads dated from the third century BCE to the fourth century CE, indicates that pre-Arabic trade contacts probably existed between Jenn\u00e9-Jeno and North Africa.\nAdditionally, other early urban centers in West Africa, dated to around 500 CE, include Awdaghust, Kumbi Saleh, the ancient capital of Ghana, and , a center located on a trade route between Egypt and Gao.\nMiddle Ages.\nThe dissolution of the Roman Empire in the West was connected with profound changes in urban fabric of western Europe. In places where Roman administration quickly weakened urbanism went through a profound crisis, even if it continued to remain an important symbolic factor. In regions like Italy or Spain cities diminished in size but nevertheless continued to play a key role in both the economy and government. Late antique cities in the East were also undergoing intense transformations, with increased political participation of the crowds and demographical fluctuations. Christian communities and their doctrinal differences increasingly shaped the urban fabric. The locus of power shifted to Constantinople and to the ascendant Islamic civilization with its major cities Baghdad, Cairo, and C\u00f3rdoba. From the 9th through the end of the 12th century, Constantinople, the capital of the Eastern Roman Empire, was the largest and wealthiest city in Europe, with a population approaching 1 million. The Ottoman Empire gradually gained control over many cities in the Mediterranean area, including Constantinople in 1453.\nIn the Holy Roman Empire, beginning in the 12th century, free imperial cities such as Nuremberg, Strasbourg, Frankfurt, Basel, Z\u00fcrich, and Nijmegen became a privileged elite among towns having won self-governance from their local lord or having been granted self-governance by the emperor and being placed under his immediate protection. By 1480, these cities, as far as still part of the empire, became part of the Imperial Estates governing the empire with the emperor through the Imperial Diet.\nBy the 13th and 14th centuries, some cities become powerful states, taking surrounding areas under their control or establishing extensive maritime empires. In Italy, medieval communes developed into city-states including the Republic of Venice and the Republic of Genoa. In Northern Europe, cities including L\u00fcbeck and Bruges formed the Hanseatic League for collective defense and commerce. Their power was later challenged and eclipsed by the Dutch commercial cities of Ghent, Ypres, and Amsterdam. Similar phenomena existed elsewhere, as in the case of Sakai, which enjoyed considerable autonomy in late medieval Japan.\nIn the first millennium AD, the Khmer capital of Angkor in Cambodia grew into the most extensive preindustrial settlement in the world by area, covering over 1,000\u00a0km2 and possibly supporting up to one million people.\nWest Africa already had cities before the Common Era, but the consolidation of Trans-Saharan trade in the Middle Ages multiplied the number of cities in the region, as well as making some of them very populous, notably Gao (72,000 inhabitants in 800 AD), Oyo-Ile (50,000 inhabitants in 1400 AD, and may have reached up to 140,000 inhabitants in the 18th century), Ile-If\u1eb9\u0300 (70,000 to 105,000 inhabitants in the 14th and 15th centuries), Niani (50,000 inhabitants in 1400 AD) and Timbuktu (100,000 inhabitants in 1450 AD).\nEarly modern.\nIn the West, nation-states became the dominant unit of political organization following the Peace of Westphalia in the seventeenth century. Western Europe's larger capitals (London and Paris) benefited from the growth of commerce following the emergence of an Atlantic trade. However, most towns remained small.\nDuring the Spanish colonization of the Americas, the old Roman city concept was extensively used. Cities were founded in the middle of the newly conquered territories and were bound to several laws regarding administration, finances, and urbanism.\nIndustrial age.\nThe growth of the modern industry from the late 18th century onward led to massive urbanization and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. England led the way as London became the capital of a world empire and cities across the country grew in locations strategic for manufacturing. In the United States from 1860 to 1910, the introduction of railroads reduced transportation costs, and large manufacturing centers began to emerge, fueling migration from rural to city areas.\nSome industrialized cities were confronted with health challenges associated with overcrowding, occupational hazards of industry, contaminated water and air, poor sanitation, and communicable diseases such as typhoid and cholera. Factories and slums emerged as regular features of the urban landscape.\nPost-industrial age.\nIn the second half of the 20th century, deindustrialization (or \"economic restructuring\") in the West led to poverty, homelessness, and urban decay in formerly prosperous cities. America's \"Steel Belt\" became a \"Rust Belt\" and cities such as Detroit, Michigan, and Gary, Indiana began to shrink, contrary to the global trend of massive urban expansion. Such cities have shifted with varying success into the service economy and public-private partnerships, with concomitant gentrification, uneven revitalization efforts, and selective cultural development. Under the Great Leap Forward and subsequent five-year plans continuing today, China has undergone concomitant urbanization and industrialization and become the world's leading manufacturer.\nAmidst these economic changes, high technology and instantaneous telecommunication enable select cities to become centers of the knowledge economy. A new smart city paradigm, supported by institutions such as the RAND Corporation and IBM, is bringing computerized surveillance, data analysis, and governance to bear on cities and city dwellers. Some companies are building brand-new master-planned cities from scratch on greenfield sites.\nUrbanization.\nUrbanization is the process of migration from rural to urban areas, driven by various political, economic, and cultural factors. Until the 18th century, an equilibrium existed between the rural agricultural population and towns featuring markets and small-scale manufacturing. With the agricultural and industrial revolutions urban population began its unprecedented growth, both through migration and demographic expansion. In England, the proportion of the population living in cities jumped from 17% in 1801 to 72% in 1891. In 1900, 15% of the world's population lived in cities. The cultural appeal of cities also plays a role in attracting residents.\nUrbanization rapidly spread across Europe and the Americas and since the 1950s has taken hold in Asia and Africa as well. The Population Division of the United Nations Department of Economic and Social Affairs reported in 2014 that for the first time, more than half of the world population lives in cities.\nLatin America is the most urban continent, with four-fifths of its population living in cities, including one-fifth of the population said to live in shantytowns (favelas, poblaciones callampas, etc.). Batam, Indonesia, Mogadishu, Somalia, Xiamen, China, and Niamey, Niger, are considered among the world's fastest-growing cities, with annual growth rates of 5\u20138%. In general, the more developed countries of the \"Global North\" remain more urbanized than the less developed countries of the \"Global South\"\u2014but the difference continues to shrink because urbanization is happening faster in the latter group. Asia is home to by far the greatest absolute number of city-dwellers: over two billion and counting. The UN predicts an additional 2.5 billion city dwellers (and 300 million fewer country dwellers) worldwide by 2050, with 90% of urban population expansion occurring in Asia and Africa.\nMegacities, cities with populations in the multi-millions, have proliferated into the dozens, arising especially in Asia, Africa, and Latin America. Economic globalization fuels the growth of these cities, as new torrents of foreign capital arrange for rapid industrialization, as well as the relocation of major businesses from Europe and North America, attracting immigrants from near and far. A deep gulf divides the rich and poor in these cities, which usually contain a super-wealthy elite living in gated communities and large masses of people living in substandard housing with inadequate infrastructure and otherwise poor conditions.\nCities around the world have expanded physically as they grow in population, with increases in their surface extent, with the creation of high-rise buildings for residential and commercial use, and with development underground.\nUrbanization can create rapid demand for water resources management, as formerly good sources of freshwater become overused and polluted, and the volume of sewage begins to exceed manageable levels.\nGovernment.\nThe local government of cities takes different forms including prominently the municipality (especially in England, in the United States, India, and former British colonies; legally, the municipal corporation; \"municipio\" in Spain and Portugal, and, along with \"municipalidad\", in most former parts of the Spanish and Portuguese empires) and the \"commune\" (in France and Chile; or \"comune\" in Italy).\nThe chief official of the city is very often called the \"mayor\". Whatever their true degree of political authority, the mayor typically acts as the figurehead or personification of their city.\nLegal conflicts and issues arise more frequently in cities than elsewhere due to the bare fact of their greater density. Modern city governments thoroughly regulate everyday life in many dimensions, including public and personal health, transport, burial, resource use and extraction, recreation, and the nature and use of buildings. Technologies, techniques, and laws governing these areas\u2014developed in cities\u2014have become ubiquitous in many areas.\nMunicipal officials may be appointed from a higher level of government or elected locally.\nMunicipal services.\nCities typically provide municipal services such as education, through school systems; policing, through police departments; and firefighting, through fire departments; as well as the city's basic infrastructure. These are provided more or less routinely, in a more or less equal fashion. Responsibility for administration usually falls on the city government, but some services may be operated by a higher level of government, while others may be privately run. Armies may assume responsibility for policing cities in states of domestic turmoil such as America's King assassination riots of 1968.\nFinance.\nThe traditional basis for municipal finance is local property tax levied on real estate within the city. Local government can also collect revenue for services, or by leasing land that it owns. However, financing municipal services, as well as urban renewal and other development projects, is a perennial problem, which cities address through appeals to higher governments, arrangements with the private sector, and techniques such as privatization (selling services into the private sector), corporatization (formation of quasi-private municipally-owned corporations), and financialization (packaging city assets into tradeable financial public contracts and other related rights). This situation has become acute in deindustrialized cities and in cases where businesses and wealthier citizens have moved outside of city limits and therefore beyond the reach of taxation. Cities in search of ready cash increasingly resort to the municipal bond, essentially a loan with interest and a repayment date. City governments have also begun to use tax increment financing, in which a development project is financed by loans based on future tax revenues which it is expected to yield. Under these circumstances, creditors and consequently city governments place a high importance on city credit ratings.\nGovernance.\nGovernance includes government but refers to a wider domain of social control functions implemented by many actors including non-governmental organizations. The impact of globalization and the role of multinational corporations in local governments worldwide, has led to a shift in perspective on urban governance, away from the \"urban regime theory\" in which a coalition of local interests functionally govern, toward a theory of outside economic control, widely associated in academics with the philosophy of neoliberalism. In the neoliberal model of governance, public utilities are privatized, the industry is deregulated, and corporations gain the status of governing actors\u2014as indicated by the power they wield in public-private partnerships and over business improvement districts, and in the expectation of self-regulation through corporate social responsibility. The biggest investors and real estate developers act as the city's de facto urban planners.\nThe related concept of good governance places more emphasis on the state, with the purpose of assessing urban governments for their suitability for development assistance. The concepts of governance and good governance are especially invoked in emergent megacities, where international organizations consider existing governments inadequate for their large populations.\nUrban planning.\nUrban planning, the application of forethought to city design, involves optimizing land use, transportation, utilities, and other basic systems, in order to achieve certain objectives. Urban planners and scholars have proposed overlapping theories as ideals for how plans should be formed. Planning tools, beyond the original design of the city itself, include public capital investment in infrastructure and land-use controls such as zoning. The continuous process of comprehensive planning involves identifying general objectives as well as collecting data to evaluate progress and inform future decisions.\nGovernment is legally the final authority on planning but in practice, the process involves both public and private elements. The legal principle of eminent domain is used by the government to divest citizens of their property in cases where its use is required for a project. Planning often involves tradeoffs\u2014decisions in which some stand to gain and some to lose\u2014and thus is closely connected to the prevailing political situation.\nThe history of urban planning dates to some of the earliest known cities, especially in the Indus Valley and Mesoamerican civilizations, which built their cities on grids and apparently zoned different areas for different purposes. The effects of planning, ubiquitous in today's world, can be seen most clearly in the layout of planned communities, fully designed prior to construction, often with consideration for interlocking physical, economic, and cultural systems.\nSociety.\nSocial structure.\nUrban society is typically stratified. Spatially, cities are formally or informally segregated along ethnic, economic, and racial lines. People living relatively close together may live, work, and play in separate areas, and associate with different people, forming ethnic or lifestyle enclaves or, in areas of concentrated poverty, ghettoes. While in the US and elsewhere poverty became associated with the inner city, in France it has become associated with the \"banlieues\", areas of urban development that surround the city proper. Meanwhile, across Europe and North America, the racially white majority is empirically the most segregated group. Suburbs in the West, and, increasingly, gated communities and other forms of \"privatopia\" around the world, allow local elites to self-segregate into secure and exclusive neighborhoods.\nLandless urban workers, contrasted with peasants and known as the proletariat, form a growing stratum of society in the age of urbanization. In Marxist doctrine, the proletariat will inevitably revolt against the bourgeoisie as their ranks swell with disenfranchised and disaffected people lacking all stake in the status quo. The global urban proletariat of today, however, generally lacks the status of factory workers which in the nineteenth century provided access to the means of production.\nEconomics.\nHistorically, cities rely on rural areas for intensive farming to yield surplus crops, in exchange for which they provide money, political administration, manufactured goods, and culture. Urban economics tends to analyze larger agglomerations, stretching beyond city limits, in order to reach a more complete understanding of the local labor market.\nAs hubs of trade, cities have long been home to retail commerce and consumption through the interface of shopping. In the 20th century, department stores using new techniques of advertising, public relations, decoration, and design, transformed urban shopping areas into fantasy worlds encouraging self-expression and escape through consumerism.\nIn general, the density of cities expedites commerce and facilitates knowledge spillovers, helping people and firms exchange information and generate new ideas. A thicker labor market allows for better skill matching between firms and individuals. Population density enables also sharing of common infrastructure and production facilities; however, in very dense cities, increased crowding and waiting times may lead to some negative effects.\nAlthough manufacturing fueled the growth of cities, many now rely on a tertiary or service economy. The services in question range from tourism, hospitality, entertainment, and housekeeping to grey-collar work in law, financial consulting, and administration.\nAccording to a scientific model of cities by Professor Geoffrey West, with the doubling of a city's size, salaries per capita will generally increase by 15%.\nCulture and communications.\nCities are typically hubs for education and the arts, supporting universities, museums, temples, and other cultural institutions. They feature impressive displays of architecture ranging from small to enormous and ornate to brutal; skyscrapers, providing thousands of offices or homes within a small footprint, and visible from miles away, have become iconic urban features. Cultural elites tend to live in cities, bound together by shared cultural capital, and themselves play some role in governance. By virtue of their status as centers of culture and literacy, cities can be described as the locus of civilization, human history, and social change.\nDensity makes for effective mass communication and transmission of news, through heralds, printed proclamations, newspapers, and digital media. These communication networks, though still using cities as hubs, penetrate extensively into all populated areas. In the age of rapid communication and transportation, commentators have described urban culture as nearly ubiquitous or as no longer meaningful.\nToday, a city's promotion of its cultural activities dovetails with place branding and city marketing, public diplomacy techniques used to inform development strategy; attract businesses, investors, residents, and tourists; and to create shared identity and sense of place within the metropolitan area. Physical inscriptions, plaques, and monuments on display physically transmit a historical context for urban places. Some cities, such as Jerusalem, Mecca, and Rome have indelible religious status and for hundreds of years have attracted pilgrims. Patriotic tourists visit Agra to see the Taj Mahal, or New York City to visit the World Trade Center. Elvis lovers visit Memphis to pay their respects at Graceland. Place brands (which include place satisfaction and place loyalty) have great economic value (comparable to the value of commodity brands) because of their influence on the decision-making process of people thinking about doing business in\u2014\"purchasing\" (the brand of)\u2014a city.\nBread and circuses among other forms of cultural appeal, attract and entertain the masses. Sports also play a major role in city branding and local identity formation. Cities go to considerable lengths in competing to host the Olympic Games, which bring global attention and tourism. Paris, a city known for its cultural history, was the site of the most recent Olympics in the summer of 2024.\nWarfare.\nCities play a crucial strategic role in warfare due to their economic, demographic, symbolic, and political centrality. For the same reasons, they are targets in asymmetric warfare. Many cities throughout history were founded under military auspices, a great many have incorporated fortifications, and military principles continue to influence urban design. Indeed, war may have served as the social rationale and economic basis for the very earliest cities.\nPowers engaged in geopolitical conflict have established fortified settlements as part of military strategies, as in the case of garrison towns, America's Strategic Hamlet Program during the Vietnam War, and Israeli settlements in Palestine. While occupying the Philippines, the US Army ordered local people to concentrate in cities and towns, in order to isolate committed insurgents and battle freely against them in the countryside.\nDuring World War II, national governments on occasion declared certain cities open, effectively surrendering them to an advancing enemy in order to avoid damage and bloodshed. Urban warfare proved decisive, however, in the Battle of Stalingrad, where Soviet forces repulsed German occupiers, with extreme casualties and destruction. In an era of low-intensity conflict and rapid urbanization, cities have become sites of long-term conflict waged both by foreign occupiers and by local governments against insurgency. Such warfare, known as counterinsurgency, involves techniques of surveillance and psychological warfare as well as close combat, and functionally extends modern urban crime prevention, which already uses concepts such as defensible space.\nAlthough capture is the more common objective, warfare has in some cases spelled complete destruction for a city. Mesopotamian tablets and ruins attest to such destruction, as does the Latin motto \"Carthago delenda est\". Since the atomic bombings of Hiroshima and Nagasaki and throughout the Cold War, nuclear strategists continued to contemplate the use of \"counter-value\" targeting: crippling an enemy by annihilating its valuable cities, rather than aiming primarily at its military forces.\nInfrastructure.\nUrban infrastructure involves various physical networks and spaces necessary for transportation, water use, energy, recreation, and public functions. Infrastructure carries a high initial cost in fixed capital but lower marginal costs and thus positive economies of scale. Because of the higher barriers to entry, these networks have been classified as natural monopolies, meaning that economic logic favors control of each network by a single organization, public or private.\nInfrastructure in general plays a vital role in a city's capacity for economic activity and expansion, underpinning the very survival of the city's inhabitants, as well as technological, commercial, industrial, and social activities. Structurally, many infrastructure systems take the form of networks with redundant links and multiple pathways, so that the system as a whole continue to operate even if parts of it fail. The particulars of a city's infrastructure systems have historical path dependence because new development must build from what exists already.\nMegaprojects such as the construction of airports, power plants, and railways require large upfront investments and thus tend to require funding from the national government or the private sector. Privatization may also extend to all levels of infrastructure construction and maintenance.\nUrban infrastructure ideally serves all residents equally but in practice may prove uneven\u2014with, in some cities, clear first-class and second-class alternatives.\nUtilities.\nPublic utilities (literally, useful things with general availability) include basic and essential infrastructure networks, chiefly concerned with the supply of water, electricity, and telecommunications capability to the populace.\nSanitation, necessary for good health in crowded conditions, requires water supply and waste management as well as individual hygiene. Urban water systems include principally a water supply network and a network (sewerage system) for sewage and stormwater. Historically, either local governments or private companies have administered urban water supply, with a tendency toward government water supply in the 20th century and a tendency toward private operation at the turn of the twenty-first. The market for private water services is dominated by two French companies, Veolia Water (formerly Vivendi) and Engie (formerly Suez), said to hold 70% of all water contracts worldwide.\nModern urban life relies heavily on the energy transmitted through electricity for the operation of electric machines (from household appliances to industrial machines to now-ubiquitous electronic systems used in communications, business, and government) and for traffic lights, street lights, and indoor lighting. Cities rely to a lesser extent on hydrocarbon fuels such as gasoline and natural gas for transportation, heating, and cooking. Telecommunications infrastructure such as telephone lines and coaxial cables also traverse cities, forming dense networks for mass and point-to-point communications.\nTransportation.\nBecause cities rely on specialization and an economic system based on wage labor, their inhabitants must have the ability to regularly travel between home, work, commerce, and entertainment. City dwellers travel by foot or by wheel on roads and walkways, or use special rapid transit systems based on underground, overground, and elevated rail. Cities also rely on long-distance transportation (truck, rail, and airplane) for economic connections with other cities and rural areas.\nCity streets historically were the domain of horses and their riders and pedestrians, who only sometimes had sidewalks and special walking areas reserved for them. In the West, bicycles or (velocipedes), efficient human-powered machines for short- and medium-distance travel, enjoyed a period of popularity at the beginning of the twentieth century before the rise of automobiles. Soon after, they gained a more lasting foothold in Asian and African cities under European influence. In Western cities, industrializing, expanding, and electrifying public transit systems, and especially streetcars enabled urban expansion as new residential neighborhoods sprung up along transit lines and workers rode to and from work downtown.\nSince the mid-20th century, cities have relied heavily on motor vehicle transportation, with major implications for their layout, environment, and aesthetics. (This transformation occurred most dramatically in the US\u2014where corporate and governmental policies favored automobile transport systems\u2014and to a lesser extent in Europe.) The rise of personal cars accompanied the expansion of urban economic areas into much larger metropolises, subsequently creating ubiquitous traffic issues with the accompanying construction of new highways, wider streets, and alternative walkways for pedestrians. However, severe traffic jams still occur regularly in cities around the world, as private car ownership and urbanization continue to increase, overwhelming existing urban street networks.\nThe urban bus system, the world's most common form of public transport, uses a network of scheduled routes to move people through the city, alongside cars, on the roads. The economic function itself also became more decentralized as concentration became impractical and employers relocated to more car-friendly locations (including edge cities). Some cities have introduced bus rapid transit systems which include exclusive bus lanes and other methods for prioritizing bus traffic over private cars. Many big American cities still operate conventional public transit by rail, as exemplified by the ever-popular New York City Subway system. Rapid transit is widely used in Europe and has increased in Latin America and Asia.\nWalking and cycling (\"non-motorized transport\") enjoy increasing favor (more pedestrian zones and bike lanes) in American and Asian urban transportation planning, under the influence of such trends as the Healthy Cities movement, the drive for sustainable development, and the idea of a carfree city. Techniques such as road space rationing and road use charges have been introduced to limit urban car traffic.\nHousing.\nThe housing of residents presents one of the major challenges every city must face. Adequate housing entails not only physical shelters but also the physical systems necessary to sustain life and economic activity.\nHomeownership represents status and a modicum of economic security, compared to renting which may consume much of the income of low-wage urban workers. Homelessness, or lack of housing, is a challenge currently faced by millions of people in countries rich and poor. Because cities generally have higher population densities than rural areas, city dwellers are more likely to reside in apartments and less likely to live in a single-family home.\nEcology.\nUrban ecosystems, influenced as they are by the density of human buildings and activities, differ considerably from those of their rural surroundings. Anthropogenic buildings and waste, as well as cultivation in gardens, create physical and chemical environments which have no equivalents in the wilderness, in some cases enabling exceptional biodiversity. They provide homes not only for immigrant humans but also for immigrant plants, bringing about interactions between species that never previously encountered each other. They introduce frequent disturbances (construction, walking) to plant and animal habitats, creating opportunities for recolonization and thus favoring young ecosystems with r-selected species dominant. On the whole, urban ecosystems are less complex and productive than others, due to the diminished absolute amount of biological interactions.\nTypical urban fauna includes insects (especially ants), rodents (mice, rats), and birds, as well as cats and dogs (domesticated and feral). Large predators are scarce. However, in North America, large predators such as coyotes and other large animals like white-tailed deer persist.\nCities generate considerable ecological footprints, locally and at longer distances, due to concentrated populations and technological activities. From one perspective, cities are not ecologically sustainable due to their resource needs. From another, proper management may be able to ameliorate a city's ill effects. Air pollution arises from various forms of combustion, including fireplaces, wood or coal-burning stoves, other heating systems, and internal combustion engines. Industrialized cities, and today third-world megacities, are notorious for veils of smog (industrial haze) that envelop them, posing a chronic threat to the health of their millions of inhabitants. Urban soil contains higher concentrations of heavy metals (especially lead, copper, and nickel) and has lower pH than soil in the comparable wilderness.\nModern cities are known for creating their own microclimates, due to concrete, asphalt, and other artificial surfaces, which heat up in sunlight and channel rainwater into underground ducts. The temperature in New York City exceeds nearby rural temperatures by an average of 2\u20133\u00a0\u00b0C and at times 5\u201310\u00a0\u00b0C differences have been recorded. This effect varies nonlinearly with population changes (independently of the city's physical size). Aerial particulates increase rainfall by 5\u201310%. Thus, urban areas experience unique climates, with earlier flowering and later leaf dropping than in nearby countries.\nPoor and working-class people face disproportionate exposure to environmental risks (known as environmental racism when intersecting also with racial segregation). For example, within the urban microclimate, less-vegetated poor neighborhoods bear more of the heat (but have fewer means of coping with it).\nOne of the main methods of improving the urban ecology is including in the cities more urban green spaces: parks, gardens, lawns, and trees. These areas improve the health and well-being of the human, animal, and plant populations of the cities. Well-maintained urban trees can provide many social, ecological, and physical benefits to the residents of the city.\nA study published in \"Scientific Reports\" in 2019 found that people who spent at least two hours per week in nature were 23 percent more likely to be satisfied with their life and were 59 percent more likely to be in good health than those who had zero exposure. The study used data from almost 20,000 people in the UK. Benefits increased for up to 300 minutes of exposure. The benefits are applied to men and women of all ages, as well as across different ethnicities, socioeconomic statuses, and even those with long-term illnesses and disabilities. People who did not get at least two hours \u2013 even if they surpassed an hour per week \u2013 did not get the benefits. The study is the latest addition to a compelling body of evidence for the health benefits of nature. Many doctors already give nature prescriptions to their patients. The study did not count time spent in a person's own yard or garden as time in nature, but the majority of nature visits in the study took place within two miles of home. \"Even visiting local urban green spaces seems to be a good thing,\" Dr. White said in a press release. \"Two hours a week is hopefully a realistic target for many people, especially given that it can be spread over an entire week to get the benefit.\"\nWorld city system.\nAs the world becomes more closely linked through economics, politics, technology, and culture (a process called globalization), cities have come to play a leading role in transnational affairs, exceeding the limitations of international relations conducted by national governments. This phenomenon, resurgent today, can be traced back to the Silk Road, Phoenicia, and the Greek city-states, through the Hanseatic League and other alliances of cities. Today the information economy based on high-speed internet infrastructure enables instantaneous telecommunication around the world, effectively eliminating the distance between cities for the purposes of the international markets and other high-level elements of the world economy, as well as personal communications and mass media.\nGlobal city.\nA global city, also known as a world city, is a prominent centre of trade, banking, finance, innovation, and markets. Saskia Sassen used the term \"global city\" in her 1991 work, \"The Global City: New York, London, Tokyo\" to refer to a city's power, status, and cosmopolitanism, rather than to its size. Following this view of cities, it is possible to rank the world's cities hierarchically. Global cities form the capstone of the global hierarchy, exerting command and control through their economic and political influence. Global cities may have reached their status due to early transition to post-industrialism or through inertia which has enabled them to maintain their dominance from the industrial era. This type of ranking exemplifies an emerging discourse in which cities, considered variations on the same ideal type, \"must\" compete with each other globally to achieve prosperity.\nCritics of the notion point to the different realms of power and interchange. The term \"global city\" is heavily influenced by economic factors and, thus, may not account for places that are otherwise significant. Paul James, for example argues that the term is \"reductive and skewed\" in its focus on financial systems.\nMultinational corporations and banks make their headquarters in global cities and conduct much of their business within this context. American firms dominate the international markets for law and engineering and maintain branches in the biggest foreign global cities.\nLarge cities have a great divide between populations of both ends of the financial spectrum. Regulations on immigration promote the exploitation of low- and high-skilled immigrant workers from poor areas. During employment, migrant workers may be subject to unfair working conditions, including working overtime, low wages, and lack of safety in workplaces.\nTransnational activity.\nCities increasingly participate in world political activities independently of their enclosing nation-states. Early examples of this phenomenon are the sister city relationship and the promotion of multi-level governance within the European Union as a technique for European integration. Cities including Hamburg, Prague, Amsterdam, The Hague, and City of London maintain their own embassies to the European Union at Brussels.\nNew urban dwellers are increasingly transmigrants, keeping one foot each (through telecommunications if not travel) in their old and their new homes.\nGlobal governance.\nCities participate in global governance by various means including membership in global networks which transmit norms and regulations. At the general, global level, United Cities and Local Governments (UCLG) is a significant umbrella organization for cities; regionally and nationally, Eurocities, Asian Network of Major Cities 21, the Federation of Canadian Municipalities the National League of Cities, and the United States Conference of Mayors play similar roles. UCLG took responsibility for creating Agenda 21 for culture, a program for cultural policies promoting sustainable development, and has organized various conferences and reports for its furtherance.\nNetworks have become especially prevalent in the arena of environmentalism and specifically climate change following the adoption of Agenda 21. Environmental city networks include the C40 Cities Climate Leadership Group, the United Nations Global Compact Cities Programme, the Carbon Neutral Cities Alliance (CNCA), the Covenant of Mayors and the Compact of Mayors, ICLEI \u2013 Local Governments for Sustainability, and the Transition Towns network.\nCities with world political status as meeting places for advocacy groups, non-governmental organizations, lobbyists, educational institutions, intelligence agencies, military contractors, information technology firms, and other groups with a stake in world policymaking. They are consequently also sites for symbolic protest.\nSouth Africa has one of the highest rate of protests in the world. Pretoria, a city in South Africa, had a rally where five thousand people took part in order to advocate for increasing wages to afford living costs.\nUnited Nations System.\nThe United Nations System has been involved in a series of events and declarations dealing with the development of cities during this period of rapid urbanization.\nUN-Habitat coordinates the U.N. urban agenda, working with the UN Environmental Programme, the UN Development Programme, the Office of the High Commissioner for Human Rights, the World Health Organization, and the World Bank.\nThe World Bank, a U.N. specialized agency, has been a primary force in promoting the Habitat conferences, and since the first Habitat conference has used their declarations as a framework for issuing loans for urban infrastructure. The bank's structural adjustment programs contributed to urbanization in the Third World by creating incentives to move to cities. The World Bank and UN-Habitat in 1999 jointly established the Cities Alliance (based at the World Bank headquarters in Washington, D.C.) to guide policymaking, knowledge sharing, and grant distribution around the issue of urban poverty. (UN-Habitat plays an advisory role in evaluating the quality of a locality's governance.) The Bank's policies have tended to focus on bolstering real estate markets through credit and technical assistance.\nThe United Nations Educational, Scientific and Cultural Organization, UNESCO has increasingly focused on cities as key sites for influencing cultural governance. It has developed various city networks including the International Coalition of Cities against Racism and the Creative Cities Network. UNESCO's capacity to select World Heritage Sites gives the organization significant influence over cultural capital, tourism, and historic preservation funding.\nRepresentation in culture.\nCities figure prominently in traditional Western culture, appearing in the Bible in both evil and holy forms, symbolized by Babylon and Jerusalem. Cain and Nimrod are the first city builders in the Book of Genesis. In Sumerian mythology Gilgamesh built the walls of Uruk.\nCities can be perceived in terms of extremes or opposites: at once liberating and oppressive, wealthy and poor, organized and chaotic. The name anti-urbanism refers to various types of ideological opposition to cities, whether because of their culture or their political relationship with the country. Such opposition may result from identification of cities with oppression and the ruling elite. This and other political ideologies strongly influence narratives and themes in discourse about cities. In turn, cities symbolize their home societies.\nWriters, painters, and filmmakers have produced innumerable works of art concerning the urban experience. Classical and medieval literature includes a genre of \"descriptiones\" which treat of city features and history. Modern authors such as Charles Dickens and James Joyce are famous for evocative descriptions of their home cities. Fritz Lang conceived the idea for his influential 1927 film \"Metropolis\" while visiting Times Square and marveling at the nighttime neon lighting. Other early cinematic representations of cities in the twentieth century generally depicted them as technologically efficient spaces with smoothly functioning systems of automobile transport. By the 1960s, however, traffic congestion began to appear in such films as \"The Fast Lady\" (1962) and \"Playtime\" (1967).\nLiterature, film, and other forms of popular culture have supplied visions of future cities both utopian and dystopian. The prospect of expanding, communicating, and increasingly interdependent world cities has given rise to images such as Nylonkong (New York, London, Hong Kong) and visions of a single world-encompassing ecumenopolis."}
{"id": "5392", "revid": "9092818", "url": "https://en.wikipedia.org/wiki?curid=5392", "title": "Cilantro", "text": ""}
{"id": "5393", "revid": "1135346512", "url": "https://en.wikipedia.org/wiki?curid=5393", "title": "Coriander (plant)", "text": ""}
{"id": "5394", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=5394", "title": "Chervil", "text": "Chervil (; Anthriscus cerefolium), sometimes called French parsley or garden chervil (to distinguish it from similar plants also called chervil), is a delicate annual herb related to parsley. It was formerly called myrhis due to its volatile oil with an aroma similar to the resinous substance myrrh. It is commonly used to season mild-flavoured dishes and is a constituent of the French herb mixture .\nName.\nThe name \"chervil\" is from Anglo-Norman, from Latin or , meaning \"leaves of joy\"; the Latin is formed, as from an Ancient Greek word ().\nDescription.\nThe plants grow to , with tripinnate leaves that may be curly. The small white flowers form small umbels, across. The fruit is about 1\u00a0cm long, oblong-ovoid with a slender, ridged beak.\nDistribution and habitat.\nA member of the Apiaceae, chervil is native to the Caucasus but was spread by the Romans through most of Europe, where it is now naturalised. It is also grown frequently in the United States, where it sometimes escapes cultivation. Such escape can be recognized, however, as garden chervil is distinguished from all other Anthriscus species growing in North America (i.e., \"A. caucalis\" and \"A. sylvestris\") by its having lanceolate-linear bracteoles and a fruit with a relatively long beak.\nCultivation.\nTransplanting chervil can be difficult, due to the long taproot. It prefers a cool and moist location; otherwise, it rapidly goes to seed (also known as bolting). It is usually grown as a cool-season crop, like lettuce, and should be planted in early spring and late fall or in a winter greenhouse. Regular harvesting of leaves also helps to prevent bolting. If plants bolt despite precautions, the plant can be periodically re-sown throughout the growing season, thus producing fresh plants as older plants bolt and go out of production.\nChervil grows to a height of , and a width of .\nUses.\nCulinary.\nChervil is used, particularly in France, to season poultry, seafood, young spring vegetables (such as carrots), soups, and sauces. More delicate than parsley, it has a faint taste of liquorice or aniseed.\nChervil is one of the four traditional French , along with tarragon, chives, and parsley, which are essential to French cooking. Unlike the more pungent, robust herbs such as thyme and rosemary, which can take prolonged cooking, the are added at the last minute, to salads, omelettes, and soups.\nChemical constituents.\nEssential oil obtained via water distillation of wild Turkish Anthriscus cerefolium was analyzed by gas chromatography - mass spectrometry identifying 4 compounds: methyl chavicol (83.10%), 1-allyl-2,4-dimethoxybenzene (15.15%), undecane (1.75%) and \u03b2-pinene (&lt;0.01%).\nHorticulture.\nAccording to some, slugs are attracted to chervil and the plant is sometimes used to bait them.\nHealth.\nChervil has had various uses in folk medicine. It was claimed to be useful as a digestive aid, for lowering high blood pressure, and, infused with vinegar, for curing hiccups. Besides its digestive properties, it is used as a mild stimulant.\nChervil has also been implicated in \"strimmer dermatitis\", another name for phytophotodermatitis, due to spray from weed trimmers and similar forms of contact. Other plants in the family Apiaceae can have similar effects."}
{"id": "5395", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=5395", "title": "Chives", "text": "Chives, scientific name Allium schoenoprasum, is a species of flowering plant in the family Amaryllidaceae.\nA perennial plant, \"A.\u00a0schoenoprasum\" is widespread in nature across much of Eurasia and North America. It is the only species of \"Allium\" native to both the New and the Old Worlds.\nThe leaves and flowers are edible. Chives are a commonly used herb and vegetable with a variety of culinary uses. They are also used to repel insects.\nDescription.\nChives are a bulb-forming herbaceous perennial plant, growing to tall. The bulbs are slender, conical, long and broad, and grow in dense clusters from the roots. The scapes (or stems) are hollow and tubular, up to long and across, with a soft texture, although, prior to the emergence of a flower, they may appear stiffer than usual. The grass-like leaves, which are shorter than the scapes, are also hollow and tubular, or terete (round in cross-section).\nThe flowers are pale purple, and star-shaped with six petals, wide, and produced in a dense inflorescence of 10\u201330 together; before opening, the inflorescence is surrounded by a papery bract. The seeds are produced in a small, three-valved capsule, maturing in summer. The herb flowers from April to May in the southern parts of its habitat zones and in June in the northern parts.\nChives are the only species of \"Allium\" native to both the New and the Old Worlds. Sometimes, the plants found in North America are classified as \"A. schoenoprasum\" var. \"sibiricum\", although this is disputed. Differences between specimens are significant. One example was found in northern Maine growing solitary, instead of in clumps, also exhibiting dingy grey flowers.\nSimilar species.\nClose relatives of chives include common onions, garlic, shallot, leek, scallion, and Chinese onion.\nThe terete hollow leaves distinguish the plant from \"Allium tuberosum\" (garlic chives).\nTaxonomy.\nIt was formally described by the Swedish botanist Carl Linnaeus in his seminal publication \"Species Plantarum\" in 1753.\nThe name of the species derives from the Greek \u03c3\u03c7\u03bf\u03af\u03bd\u03bf\u03c2, \"skho\u00ednos\" (sedge or rush) and \u03c0\u03c1\u03ac\u03c3\u03bf\u03bd, \"pr\u00e1son\" (leek). Its English name, chives, derives from the French word \"cive\", from \"cepa\", the Latin word for onion. In the Middle Ages, it was known as 'rush leek'.\nSeveral subspecies have been proposed, but are not accepted by Plants of the World Online, , which sinks them into two subspecies:\nVarieties have also been proposed, including \"A.\u00a0schoenoprasum\" var. \"sibiricum\". The Flora of North America notes that the species is very variable, and considers recognition of varieties as \"unsound\".\nDistribution and habitat.\nChives are native to temperate areas of Europe, Asia and North America. \nRange.\nChives have a wide natural range across much of the Northern Hemisphere.\nIn Asia it is native from the Ural Mountains in Russia to Kamchatka in the far east. It grows natively in the Korean peninsula, but only the islands of Hokkaido and Honshu in Japan. Likewise its natural range in China only extends to Xinjiang and Inner Mongolia, though it is also found in adjacent Mongolia. It is native to all the nations of the Caucasus. However, in Central Asia it is only found in Kazakhstan and Kyrgyzstan. To the south its range also extends to Afghanistan, Iran, Iraq, Pakistan, and the Western Himalayas in India.\nIt is native to all parts of Europe with the exception of Sicily, Sardinia, the island of Cyprus, Iceland, Crimea, and Hungary and other offshore islands. It also is not native to Belgium and Ireland, but it grows there as an introduced plant.\nIn North America it is native to Alaska and almost every province of Canada, but has been introduced to the island of Newfoundland. In the United States the certain native range in the lower 48 is in two separated areas. In the west its range is in Washington, Oregon, Idaho, Montana, Wyoming, and Colorado. In the east it extends from Minnesota, eastward through Wisconsin, Michigan, Ohio, Pennsylvania, and New Jersey. Then northward into New York and all of New England. The Plants of the World Online database lists it as introduced to Illinois and Maryland and the USDA Natural Resources Conservation Service PLANTS database additionally lists it as growing in Nevada, Utah, Missouri, and Virginia without information on if it is native or introduced to those states.\nIn other areas of the Americas chives grow as an introduced plant in Mexico, Honduras, Costa Rica, Cuba, Jamaica, Hispaniola, Trinidad, Colombia, Bolivia, and the southern part of Argentina in Tierra del Fuego.\nEcology.\nChives are repulsive to most insects due to their sulfur compounds, but their flowers attract bees, and they are at times kept to increase desired insect life.\nThe plant provides a great deal of nectar for pollinators. It was rated in the top 10 for most nectar production (nectar per unit cover per year) in a United Kingdom plants survey conducted by the AgriLand project which is supported by the UK Insect Pollinators Initiative.\nCultivation.\nChives have been cultivated in Europe since the Middle Ages (from the fifth until the 15th centuries), although their usage dates back 5,000 years.\nChives are cultivated both for their culinary uses and for their ornamental value; the violet flowers are often used in ornamental dry bouquets.\nChives thrive in well-drained soil, rich in organic matter, with a pH of 6\u20137 and full sun. They can be grown from seed and mature in summer, or early the following spring. Typically, chives need to be germinated at a temperature of and kept moist. They can also be planted under a cloche or germinated indoors in cooler climates, then planted out later. After at least four weeks, the young shoots should be ready to be planted out. They are also easily propagated by division.\nIn cold regions, chives die back to the underground bulbs in winter, with the new leaves appearing in early spring. Chives starting to look old can be cut back to about 2\u20135\u00a0cm. When harvesting, the needed number of stalks should be cut to the base. During the growing season, the plant continually regrows leaves, allowing for a continuous harvest.\nChives are susceptible to damage by leek moth larvae, which bore into the leaves or bulbs of the plant.\nUses.\nCulinary arts.\nChives are grown for their scapes and leaves, which are used for culinary purposes as a flavoring herb, and provide a somewhat milder onion-like flavor than those of other \"Allium\" species. The edible flowers are used in salads, or used to make blossom vinegars. Both the scapes and the unopened, immature flower buds are diced and used as an ingredient for omelettes, fish, potatoes, soups, and many other dishes. \nChives have a wide variety of culinary uses, such as in traditional dishes in France, Sweden, and elsewhere. In his 1806 book \"Attempt at a Flora\" (\"F\u00f6rs\u00f6k til en flora\"), Anders Jahan Retzius describes how chives are used with pancakes, soups, fish, and sandwiches. They are also an ingredient of the \"gr\u00e4ddfil\" sauce with the traditional herring dish served at Swedish midsummer celebrations. The flowers may also be used to garnish dishes.\nIn Poland and Germany, chives are served with quark. Chives are one of the \"fines herbes\" of French cuisine, the others being tarragon, chervil and parsley. Chives can be found fresh at most markets year-round, making them readily available; they can also be dry-frozen without much impairment to the taste, giving home growers the opportunity to store large quantities harvested from their own gardens.\nUses in plant cultivation.\nRetzius also describes how farmers would plant chives between the rocks making up the borders of their flowerbeds, to keep the plants free from pests (such as Japanese beetles). The growing plant repels unwanted insect life, and the juice of the leaves can be used for the same purpose, as well as fighting fungal infections, mildew, and scab.\nIn culture.\nIn Europe, chives were sometimes referred to as \"rush leeks\".\nIt was mentioned in 80 A.D. by Marcus Valerius Martialis in his \"Epigrams\".\nThe Romans believed chives could relieve the pain from sunburn or a sore throat. They believed eating chives could increase blood pressure and act as a diuretic.\nRomani have used chives in fortune telling. Bunches of dried chives hung around a house were believed to ward off disease and evil.\nIn the 19th century, Dutch farmers fed cattle on the herb to give a different taste to their milk."}
{"id": "5396", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=5396", "title": "ChrisMorris", "text": ""}
{"id": "5397", "revid": "1778754", "url": "https://en.wikipedia.org/wiki?curid=5397", "title": "Chris Morris (satirist)", "text": "Christopher J. Morris (born 15 June 1962) is an English comedian, radio presenter, actor and filmmaker. Known for his deadpan, dark humour, surrealism and controversial subject matter, he has been praised by the British Film Institute for his \"uncompromising, moralistic drive\".\nMorris teamed up with his radio producer Armando Iannucci in the early 1990s to create \"On the Hour\", a satire of news programmes. A television spin off, \"The Day Today\", launched the career of comedian Steve Coogan and was hailed as one of the most important satirical shows of the 1990s. Morris further developed the satirical news format with \"Brass Eye\", which lampooned celebrities whilst focusing on themes such as crime and drugs. For some, the apotheosis of Morris' career was a \"Brass Eye\" special dealing with the moral panic surrounding paedophilia. It became one of the most complained-about programmes in British television history, leading the \"Daily Mail\" to describe him as \"the most loathed man on TV\".\nMorris's similarly controversial postmodern sketch comedy and ambient music radio show \"Blue Jam\" gained a cult following. It was adapted into the TV series \"Jam\", hailed as \"the most radical and original television programme broadcast in years\", and Morris won the BAFTA Award for Best Short Film after expanding a \"Blue Jam\" sketch into \"My Wrongs 8245\u20138249 &amp; 117\" starring Paddy Considine. \"Nathan Barley\", a sitcom written in collaboration with then little-known Charlie Brooker that satirised hipsters, had low ratings but success with its DVD release. Morris joined the cast of sitcom \"The IT Crowd\", his first project in which he did not have writing or producing input.\nIn 2010, Morris directed his feature-length film, \"Four Lions\", satirising Islamic terrorism through a group of inept British Muslims. Reception was largely positive, earning Morris his second BAFTA Film Award, this time for Outstanding Debut. He has directed four episodes of Iannucci's political comedy \"Veep\" and appeared onscreen in \"The Double\" and \"Stewart Lee's Comedy Vehicle\" also serving as script-editor for the latter. His second feature-length film, \"The Day Shall Come\", was released in 2019. \nEarly life.\nChristopher J. Morris was born on 15 June 1962 in Colchester, Essex, the son of Rosemary Parrington and Paul Michael Morris. His father was a GP. Morris has a large red birthmark almost completely covering the left side of his face and neck, which he disguises with makeup when acting. He grew up in a Victorian farmhouse in the village of Buckden, Cambridgeshire, which he described as \"very dull\". He has two younger brothers, including theatre director Tom Morris. From an early age, he was a prankster and had a passion for radio. From the age of 10, he was educated at the independent Jesuit boarding school Stonyhurst College in Stonyhurst, Lancashire. He went to study zoology at the University of Bristol, where he gained a .\nCareer.\nRadio.\nOn graduating, Morris pursued a career as a musician in various bands, for which he played the bass guitar. He then went to work for Radio West, a local radio station in Bristol. He then took up a news traineeship with BBC Radio Cambridgeshire, where he took advantage of access to editing and recording equipment to create elaborate spoofs and parodies. He also spent time in early 1987 hosting a 2\u20134pm afternoon show and finally ended up presenting Saturday morning show \"I.T.\"\nIn July 1987, he moved on to BBC Radio Bristol to present his own show, \"No Known Cure\", broadcast on Saturday and Sunday mornings. The show was surreal and satirical, with odd interviews conducted with unsuspecting members of the public. He was fired from Bristol in 1990 after \"talking over the news bulletins and making silly noises\". In 1988 he also joined, from its launch, Greater London Radio (GLR). He presented \"The Chris Morris Show\" on GLR until 1993, when one show got suspended after a sketch was broadcast involving a child \"outing\" celebrities.\nIn 1991, Morris joined Armando Iannucci's spoof news project \"On the Hour\". Broadcast on BBC Radio 4, it saw him work alongside Iannucci, Steve Coogan, Stewart Lee, Richard Herring and Rebecca Front. In 1992, Morris hosted Danny Baker's Radio 5 Morning Edition show for a week whilst Baker was on holiday. In 1994, Morris began a weekly evening show, the \"Chris Morris Music Show\", on BBC Radio 1 alongside Peter Baynham and 'man with a mobile phone' Paul Garner. In the shows, Morris perfected the spoof interview style that would become a central component of his \"Brass Eye\" programme. In the same year, Morris teamed up with Peter Cook (as Sir Arthur Streeb-Greebling) in a series of improvised conversations for BBC Radio 3 entitled \"Why Bother?\".\nMove into television and film.\nIn 1994, a BBC Two television series based on \"On the Hour\" was broadcast under the name \"The Day Today\". \"The Day Today\" made a star of Morris, and marked the television debut of Steve Coogan's Alan Partridge character. The programme ended on a high after just one series, with Morris winning the 1994 British Comedy Award for Best Newcomer for his lead role as the Paxmanesque news anchor.\nIn 1996, Morris appeared on the daytime programme \"The Time, The Place\", posing as an academic, Thurston Lowe, in a discussion entitled \"Are British Men Lousy Lovers?\", but was found out when a producer alerted the show's host, John Stapleton.\nIn 1997, the black humour which had featured in \"On the Hour\" and \"The Day Today\" became more prominent in \"Brass Eye\", another spoof of current affairs television documentary, shown on Channel 4. All three series satirised and exaggerated issues expected of news shows. The second episode of \"Brass Eye,\" for example, satirised drugs and the political rhetoric surrounding them. To help convey the satire, Morris invented a fictional drug by the name of \"cake\". In the episode, British celebrities and politicians describe the supposed symptoms in detail; David Amess mentioned the fictional drug at Parliament. In 2001, Morris satirised the moral panic regarding paedophilia in the most controversial episode of \"Brass Eye\", \"Paedogeddon\". Channel 4 apologised for the episode after receiving criticism from tabloids and around 3,000 complaints from viewers, which, at the time, was the most for an episode of British television. \nFrom 1997 to 1999, Morris created \"Blue Jam\" for BBC Radio 1, a surreal, taboo-breaking radio show set to an ambient soundtrack. In 2000, this was followed by \"Jam\", a television reworking. Morris released a 'remix' version of this, entitled \"Jaaaaam\".\nIn 2002, Morris ventured into film, directing the short \"My Wrongs #8245\u20138249 &amp; 117\", adapted from a \"Blue Jam\" monologue about a man led astray by a sinister talking dog. It was the first film project of Warp Films, a branch of Warp Records. In 2002 it won the BAFTA for best short film. In 2005 Morris worked on a sitcom entitled \"Nathan Barley\", based on the character created by Charlie Brooker for his website TVGoHome (Morris had contributed to TVGoHome on occasion, under the pseudonym 'Sid Peach'). Co-written by Brooker and Morris, the series was broadcast on Channel 4 in early 2005.\n\"The IT Crowd\" and \"Comedy Vehicle\".\nMorris appeared in \"The IT Crowd\", a Channel 4 sitcom which focuses on the information technology department of the fictional company Reynholm Industries. The series was written and directed by Graham Linehan (with whom Morris collaborated on \"The Day Today\", \"Brass Eye\" and \"Jam\") and produced by Ash Atalla. Morris played Denholm Reynholm, the eccentric managing director of the company. This marked the first time Morris had acted in a substantial role in a project which he has not developed himself. Morris's character was killed off during episode two of the second series. His character made a brief return in the first episode of the third series.\nIn November 2007, Morris wrote an article for \"The Observer\" in response to Ronan Bennett's article published six days earlier in \"The Guardian\". Bennett's article, \"Shame on us\", accused the novelist Martin Amis of racism. Morris's response, \"The absurd world of Martin Amis\", was also highly critical of Amis; although he did not accede to Bennett's accusation of racism, Morris likened Amis to the Muslim cleric Abu Hamza (who was jailed for inciting racial hatred in 2006), suggesting that both men employ \"mock erudition, vitriol and decontextualised quotes from the Qu'ran\" to incite hatred.\nMorris served as script editor for the 2009 series \"Stewart Lee's Comedy Vehicle\", working with former colleagues Stewart Lee, Kevin Eldon and Armando Iannucci. He maintained this role for the second (2011) and third series (2014), also appearing as a mock interviewer dubbed the \"hostile interrogator\" in the third and fourth series.\n\"Four Lions\", \"Veep\", and other appearances.\nMorris completed his debut feature film \"Four Lions\" in late 2009, a satire based on a group of Islamist terrorists in Sheffield.\nIt premiered at the Sundance Film Festival in January 2010 and was short-listed for the festival's World Cinema Narrative prize. The film (working title \"Boilerhouse\") was picked up by Film Four. Morris told \"The Sunday Times\" that the film sought to do for Islamic terrorism what \"Dad's Army\", the classic BBC comedy, did for the Nazis by showing them as \"scary but also ridiculous\".\nIn 2012, Morris directed the seventh and penultimate episode of the first season of \"Veep\", an Armando Iannucci-devised American version of \"The Thick of It\". In 2013, he returned to direct two episodes for the second season of \"Veep\", and a further episode for season three in 2014.\nIn 2013, Morris appeared briefly in Richard Ayoade's \"The Double\", a black comedy film based on the Fyodor Dostoyevsky novella of the same name. Morris had previously worked with Ayoade on \"Nathan Barley\" and \"The IT Crowd\".\nIn February 2014, Morris made a surprise appearance at the beginning of a Stewart Lee live show, introducing the comedian with fictional anecdotes about their work together. The following month, Morris appeared in the third series of \"Stewart Lee's Comedy Vehicle\" as a \"hostile interrogator\", a role previously occupied by Armando Iannucci.\nIn December 2014, it was announced that a short radio collaboration with Noel Fielding and Richard Ayoade would be broadcast on BBC Radio 6. According to Fielding, the work had been in progress since around 2006. However, in January 2015 it was decided, 'in consultation with [Morris]', that the project was not yet complete, and so the intended broadcast did not go ahead.\n\"The Day Shall Come\".\nA statement released by Film4 in February 2016 made reference to funding what would be Morris's second feature film. In November 2017 it was reported that Morris had shot the movie, starring Anna Kendrick, in the Dominican Republic but the title was not made public. It was later reported in January 2018 that Jim Gaffigan and Rupert Friend had joined the cast of the still-untitled film, and that the plot would revolve around an FBI hostage situation gone wrong. The completed film, titled \"The Day Shall Come\", had its world premiere at South by Southwest on 11 March 2019.\nMusic.\nMorris often co-writes and performs incidental music for his television shows, notably with \"Jam\" and the 'extended remix' version, \"Jaaaaam\". In the early 1990s Morris contributed a Pixies parody track entitled \"Motherbanger\" to a flexi-disc given away with an edition of Select music magazine. Morris supplied sketches for British band Saint Etienne's 1993 single \"You're in a Bad Way\" (the sketch 'Spongbake' appears at the end of the 4th track on the CD single).\nIn 2000, Morris collaborated by mail with Amon Tobin to create the track \"Bad Sex\", which was released as a B-side on the Tobin single \"Slowly\".\nAnglo-French band Stereolab's song \"Nothing to Do with Me\" from their 2001 album \"Sound-Dust\" featured various lines from Chris Morris sketches as lyrics.\nStyle.\nRamsey Ess of \"Vulture\" described Morris's comedy style as \"crass\" and \"shocking\", but noted an \"underlying morality\" and integrity, as well as the humour being Morris's priority.\nRecognition.\nIn 2003, Morris was listed in \"The Observer\" as one of the 50 funniest acts in British comedy. In 2005, Channel 4 aired a show called \"The Comedian's Comedian\" in which foremost writers and performers of comedy ranked their 50 favourite acts. Morris was at number eleven. Morris won the BAFTA for outstanding debut with his film \"Four Lions\". Adeel Akhtar and Nigel Lindsay collected the award in his absence. Lindsay stated that Morris had sent him a text message before they collected the award reading, 'Doused in petrol, Zippo at the ready'. In June 2012 Morris was placed at number 16 in the Top 100 People in UK Comedy.\nIn 2010, a biography, \"Disgusting Bliss: The Brass Eye of Chris Morris\", was published. Written by Lucian Randall, the book depicted Morris as \"brilliant but uncompromising\", and a \"frantic-minded perfectionist\".\nIn November 2014, BBC Radio 4 Extra broadcast a three-hour retrospective of Morris's radio career called 'Raw Meat Radio', which was presented by Mary Anne Hobbs and featured interviews with Armando Iannucci, Peter Baynham, Paul Garner, and others.\nAwards.\nMorris won the Best TV Comedy Newcomer award from the British Comedy Awards in 1994 for his performance in \"The Day Today\". He has won two BAFTA awards: the BAFTA Award for Best Short Film in 2002 for \"My Wrongs #8245\u20138249 &amp; 117\", and the BAFTA Award for Outstanding Debut by a British director, writer or producer in 2011 for \"Four Lions\".\nPersonal life.\nMorris and his wife, actress-turned-literary agent Jo Unwin, live in the Brixton district of London. The pair met in 1984 at the Edinburgh Festival, when he was playing bass guitar for the Cambridge Footlights Revue and she was in a comedy troupe called the Millies. They have two sons."}
{"id": "5399", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=5399", "title": "Colorado", "text": "Colorado is a state in the Western United States. It is one of the Mountain states, sharing the Four Corners region with Arizona, New Mexico, and Utah. It is also bordered by Wyoming to the north, Nebraska to the northeast, Kansas to the east, and Oklahoma to the southeast. Colorado is noted for its landscape of mountains, forests, high plains, mesas, canyons, plateaus, rivers, and desert lands. It encompasses most of the Southern Rocky Mountains, as well as the northeastern portion of the Colorado Plateau and the western edge of the Great Plains. Colorado is the eighth-largest U.S. state by area and the 21st by population. The United States Census Bureau estimated the population of Colorado to be 5,957,493 as of July 1, 2024, a 3.2% increase from the 2020 United States census.\nThe region has been inhabited by Native Americans and their ancestors for at least 13,500 years and possibly much longer. The eastern edge of the Rocky Mountains was a major migration route for early peoples who spread throughout the Americas. In 1848, much of the Nuevo M\u00e9xico region was annexed to the United States with the Treaty of Guadalupe Hidalgo. The Pike's Peak Gold Rush of 1858\u20131862 created an influx of settlers. On February 28, 1861, U.S. President James Buchanan signed an act creating the Territory of Colorado, and on August 1, 1876, President Ulysses S. Grant signed Proclamation 230, admitting Colorado to the Union as the 38th state. The Spanish adjective \"colorado\" means \"colored red\" or \"ruddy\". Colorado is nicknamed the \"Centennial State\" because it became a state 100 years (and four weeks) after the signing of the United States Declaration of Independence.\nDenver is the capital, the most populous city, and the center of the Front Range Urban Corridor. Colorado Springs is the second-most populous city of the state. Residents of the state are known as Coloradans, although the antiquated \"Coloradoan\" is occasionally used. Major parts of the economy include government and defense, mining, agriculture, tourism, and increasingly other kinds of manufacturing. With increasing temperatures and decreasing water availability, Colorado's agriculture forestry and tourism economies are expected to be heavily affected by climate change.\nHistory.\nThe region that is today the State of Colorado has been inhabited by Native Americans and their Paleoamerican ancestors for at least 13,500 years and possibly more than 37,000 years. The eastern edge of the Rocky Mountains was a major migration route that was important to the spread of early peoples throughout the Americas. The Lindenmeier site in Larimer County contains artifacts dating from approximately 8720 BCE. The Ancient Pueblo peoples lived in the valleys and mesas of the Colorado Plateau in far southwestern Colorado. The Ute Nation inhabited the mountain valleys of the Southern Rocky Mountains and the Western Rocky Mountains, even as far east as the Front Range of the present day. The Apache and the Comanche also inhabited the Eastern and Southeastern parts of the state. In the 17th century, the Arapaho and Cheyenne moved west from the Great Lakes region to hunt across the High Plains of Colorado and Wyoming.\nThe Spanish Empire claimed Colorado as part of Nuevo M\u00e9xico. The U.S. acquired the territorial claim to the eastern Rocky Mountains with the Louisiana Purchase from France in 1803. This U.S. claim conflicted with the claim by Spain to the upper Arkansas River Basin. In 1806, Zebulon Pike led a U.S. Army reconnaissance expedition into the disputed region. Colonel Pike and his troops were arrested by Spanish cavalrymen in the San Luis Valley the following February, taken to Chihuahua, and expelled from Mexico the following July.\nThe U.S. relinquished its claim to all land south and west of the Arkansas River and south of 42nd parallel north and west of the 100th meridian west as part of its purchase of Florida from Spain with the Adams-On\u00eds Treaty of 1819. The treaty took effect on February 22, 1821. Having settled its border with Spain, the U.S. admitted the southeastern portion of the Territory of Missouri to the Union as the state of Missouri on August 10, 1821. The remainder of Missouri Territory, including what would become northeastern Colorado, became an unorganized territory and remained so for 33 years over the question of slavery. After 11 years of war, Spain finally recognized the independence of Mexico with the Treaty of C\u00f3rdoba signed on August 24, 1821. Mexico eventually ratified the Adams\u2013On\u00eds Treaty in 1831. The Texian Revolt of 1835\u201336 fomented a dispute between the U.S. and Mexico which eventually erupted into the Mexican\u2013American War in 1846. Mexico surrendered its northern territory to the U.S. with the Treaty of Guadalupe Hidalgo after the war in 1848; this included much of the western and southern areas of Colorado.\nMost American settlers first traveled to Colorado through the Santa Fe Trail, which connected the U.S. to Santa Fe and the Camino Real de Tierra Adentro southward. Others were traveling overland west to the Oregon Country, the new goldfields of California, or the new Mormon settlements of the State of Deseret in the Salt Lake Valley, avoided the rugged Southern Rocky Mountains, and instead followed the North Platte River and Sweetwater River to South Pass (Wyoming), the lowest crossing of the Continental Divide between the Southern Rocky Mountains and the Central Rocky Mountains. In 1849, the Mormons of the Salt Lake Valley organized the extralegal State of Deseret, claiming the entire Great Basin and all lands drained by the rivers Green, Grand, and Colorado. The federal government of the U.S. flatly refused to recognize the new Mormon government because it was theocratic and sanctioned plural marriage. Instead, the Compromise of 1850 divided the Mexican Cession and the northwestern claims of Texas into a new state and two new territories, the state of California, the Territory of New Mexico, and the Territory of Utah. On April 9, 1851, Hispano settlers from the area of Taos settled the village of San Luis, then in the New Mexico Territory, as Colorado's first permanent Euro-American settlement, further cementing the traditions of New Mexican cuisine and New Mexico music in the developing Southern Rocky Mountain Front.\nIn 1854, Senator Stephen A. Douglas persuaded the U.S. Congress to divide the unorganized territory east of the Continental Divide into two new organized territories, the Territory of Kansas and the Territory of Nebraska, and an unorganized southern region known as the Indian Territory. Each new territory was to decide the fate of slavery within its boundaries, but this compromise merely served to fuel animosity between free soil and pro-slavery factions.\nThe gold seekers organized the Provisional Government of the Territory of Jefferson on August 24, 1859, but this new territory failed to secure approval from the Congress of the United States embroiled in the debate over slavery. The election of Abraham Lincoln for the President of the United States on November 6, 1860, led to the secession of nine southern slave states and the threat of civil war among the states. Seeking to augment the political power of the Union states, the Republican Party-dominated Congress quickly admitted the eastern portion of the Territory of Kansas into the Union as the free State of Kansas on January 29, 1861, leaving the western portion of the Kansas Territory, and its gold-mining areas, as unorganized territory.\nTerritory act.\nThirty days later on February 28, 1861, outgoing U.S. President James Buchanan signed an Act of Congress organizing the free Territory of Colorado. The original boundaries of Colorado remain unchanged except for government survey amendments. In 1776, Spanish priest Silvestre V\u00e9lez de Escalante recorded that Native Americans in the area knew the river as \"\" for the red-brown silt that the river carried from the mountains. In 1859, a U.S. Army topographic expedition led by Captain John Macomb located the confluence of the Green River with the Grand River in what is now Canyonlands National Park in Utah. The Macomb party designated the confluence as the source of the Colorado River.\nOn April 12, 1861, South Carolina artillery opened fire on Fort Sumter to start the American Civil War. While many gold seekers held sympathies for the Confederacy, the vast majority remained fiercely loyal to the Union cause.\nIn 1862, a force of Texas cavalry invaded the Territory of New Mexico and captured Santa Fe on March 10. The object of this Western Campaign was to seize or disrupt Colorado and California's gold fields and seize Pacific Ocean ports for the Confederacy. A hastily organized force of Colorado volunteers force-marched from Denver City, Colorado Territory, to Glorieta Pass, New Mexico Territory, in an attempt to block the Texans. On March 28, the Coloradans and local New Mexico volunteers stopped the Texans at the Battle of Glorieta Pass, destroyed their cannon and supply wagons, and dispersed 500 of their horses and mules. The Texans were forced to retreat to Santa Fe. Having lost the supplies for their campaign and finding little support in New Mexico, the Texans abandoned Santa Fe and returned to San Antonio in defeat. The Confederacy made no further attempts to seize the Southwestern United States.\nIn 1864, Territorial Governor John Evans appointed the Reverend John Chivington as Colonel of the Colorado Volunteers with orders to protect white settlers from Cheyenne and Arapaho warriors who were accused of stealing cattle. Colonel Chivington ordered his troops to attack a band of Cheyenne and Arapaho encamped along Sand Creek. Chivington reported that his troops killed more than 500 warriors. The militia returned to Denver City in triumph, but several officers reported that the so-called battle was a blatant massacre of Indians at peace, that most of the dead were women and children, and that the bodies of the dead had been hideously mutilated and desecrated. Three U.S. Army inquiries condemned the action, and incoming President Andrew Johnson asked Governor Evans for his resignation, but none of the perpetrators was ever punished. This event is now known as the Sand Creek massacre.\nIn the midst and aftermath of the Civil War, many discouraged prospectors returned to their homes, but a few stayed and developed mines, mills, farms, ranches, roads, and towns in Colorado Territory. On September 14, 1864, James Huff discovered silver near Argentine Pass, the first of many silver strikes. In 1867, the Union Pacific Railroad laid its tracks west to Weir, now Julesburg, in the northeast corner of the Territory. The Union Pacific linked up with the Central Pacific Railroad at Promontory Summit, Utah, on May 10, 1869, to form the First transcontinental railroad. The Denver Pacific Railway reached Denver in June of the following year, and the Kansas Pacific arrived two months later to forge the second line across the continent. In 1872, rich veins of silver were discovered in the San Juan Mountains on the Ute Indian reservation in southwestern Colorado. The Ute people were removed from the San Juan Mountains the following year.\nStatehood.\nThe United States Congress passed an enabling act on March 3, 1875, specifying the requirements for the Territory of Colorado to become a state. On August 1, 1876 (four weeks after the Centennial of the United States), U.S. President Ulysses S. Grant signed a proclamation admitting Colorado to the Union as the 38th state and earning it the moniker \"Centennial State\".\nThe discovery of a major silver lode near Leadville in 1878 triggered the Colorado Silver Boom. The Sherman Silver Purchase Act of 1890 invigorated silver mining, and Colorado's last, but greatest, gold strike at Cripple Creek a few months later lured a new generation of gold seekers. Colorado women were granted the right to vote on November 7, 1893, making Colorado the second state to grant universal suffrage and the first one by a popular vote (of Colorado men). The repeal of the Sherman Silver Purchase Act in 1893 led to a staggering collapse of the mining and agricultural economy of Colorado, but the state slowly and steadily recovered. Between the 1880s and 1930s, Denver's floriculture industry developed into a major industry in Colorado. This period became known locally as the Carnation Gold Rush.\nTwentieth and twenty-first centuries.\nPoor labor conditions and discontent among miners resulted in several major clashes between strikers and the Colorado National Guard, including the 1903\u20131904 Western Federation of Miners Strike and Colorado Coalfield War, the latter of which included the Ludlow massacre that killed a dozen women and children. Both the 1913\u20131914 Coalfield War and the Denver streetcar strike of 1920 resulted in federal troops intervening to end the violence. In 1927, the 1927-28 Colorado coal strike occurred and was ultimately successful in winning a dollar a day increase in wages. During it however the Columbine Mine massacre resulted in six dead strikers following a confrontation with Colorado Rangers. In a separate incident in Trinidad the mayor was accused of deputizing members of the KKK against the striking workers. More than 5,000 Colorado miners\u2014many immigrants\u2014are estimated to have died in accidents since records were first formally collected following an 1884 accident in Crested Butte that killed 59.\nIn 1924, the Ku Klux Klan Colorado Realm achieved dominance in Colorado politics. With peak membership levels, the Second Klan levied significant control over both the local and state Democrat and Republican parties, particularly in the governor's office and city governments of Denver, Ca\u00f1on City, and Durango. A particularly strong element of the Klan controlled the Denver Police. Cross burnings became semi-regular occurrences in cities such as Florence and Pueblo. The Klan targeted African-Americans, Catholics, Eastern European immigrants, and other non-White Protestant groups. Efforts by non-Klan lawmen and lawyers including Philip Van Cise led to a rapid decline in the organization's power, with membership waning significantly by the end of the 1920s.\nColorado became the first western state to host a major political convention when the Democratic Party met in Denver in 1908. By the U.S. census in 1930, the population of Colorado first exceeded one million residents. Colorado suffered greatly through the Great Depression and the Dust Bowl of the 1930s, but a major wave of immigration following World War II boosted Colorado's fortune. Tourism became a mainstay of the state economy, and high technology became an important economic engine. The United States Census Bureau estimated that the population of Colorado exceeded five million in 2009.\nOn September 11, 1957, a plutonium fire occurred at the Rocky Flats Plant, which resulted in the significant plutonium contamination of surrounding populated areas.\nFrom the 1940s and 1970s, many protest movements gained momentum in Colorado, predominantly in Denver. This included the Chicano Movement, a civil rights, and social movement of Mexican Americans emphasizing a Chicano identity that is widely considered to have begun in Denver. The National Chicano Youth Liberation Conference was held in Colorado in March 1969.\nIn 1967, Colorado was the first state to loosen restrictions on abortion when governor John Love signed a law allowing abortions in cases of rape, incest, or threats to the woman's mental or physical health. Many states followed Colorado's lead in loosening abortion laws in the 1960s and 1970s.\nSince the late 1990s, Colorado has been the site of multiple major mass shootings, including the infamous Columbine High School massacre in 1999 which made international news, where two gunmen killed 12 students and one teacher, before committing suicide. The incident has spawned many copycat incidents. On July 20, 2012, a gunman killed 12 people in a movie theater in Aurora. The state responded with tighter restrictions on firearms, including introducing a limit on magazine capacity. On March 22, 2021, a gunman killed 10 people, including a police officer, in a King Soopers supermarket in Boulder. In an instance of anti-LGBT violence, a gunman killed 5 people at a nightclub in Colorado Springs during the night of November 19\u201320, 2022.\nFour warships of the U.S. Navy have been named the USS \"Colorado\". The first USS \"Colorado\" was named for the Colorado River and served in the Civil War and later the Asiatic Squadron, where it was attacked during the 1871 Korean Expedition. The later three ships were named in honor of the state, including an armored cruiser and the battleship USS \"Colorado\", the latter of which was the lead ship of her class and served in World War II in the Pacific beginning in 1941. At the time of the attack on Pearl Harbor, the battleship USS \"Colorado\" was located at the naval base in San Diego, California, and thus went unscathed. The most recent vessel to bear the name USS \"Colorado\" is Virginia-class submarine USS \"Colorado\" (SSN-788), which was commissioned in 2018.\nGeography.\nColorado is notable for its diverse geography, which includes alpine mountains, high plains, deserts with huge sand dunes, and deep canyons. In 1861, the United States Congress defined the boundaries of the new Territory of Colorado exclusively by lines of latitude and longitude, stretching from 37\u00b0N to 41\u00b0N latitude, and from 102\u00b002\u203248\u2033W to 109\u00b002\u203248\u2033W longitude (25\u00b0W to 32\u00b0W from the Washington Meridian). After years of government surveys, the borders of Colorado were officially defined by 697 boundary markers and 697 straight boundary lines. Colorado, Wyoming, and Utah are the only states that have their borders defined solely by straight boundary lines with no natural features. The southwest corner of Colorado is the Four Corners Monument at 36\u00b059\u203256\u2033N, 109\u00b02\u203243\u2033W. The Four Corners Monument, located at the place where Colorado, New Mexico, Arizona, and Utah meet, is the only place in the United States where four states meet.\nPlains.\nApproximately half of Colorado is flat and rolling land. East of the Rocky Mountains is the Colorado Eastern Plains of the High Plains, the section of the Great Plains within Colorado at elevations ranging from roughly . The Colorado plains are mostly prairies but also include deciduous forests, buttes, and canyons. Precipitation averages annually.\nEastern Colorado is presently mainly farmland and rangeland, along with small farming villages and towns. Corn, wheat, hay, soybeans, and oats are all typical crops. Most villages and towns in this region boast both a water tower and a grain elevator. Irrigation water is available from both surface and subterranean sources. Surface water sources include the South Platte, the Arkansas River, and a few other streams. Subterranean water is generally accessed through artesian wells. Heavy usage of these wells for irrigation purposes caused underground water reserves to decline in the region. Eastern Colorado also hosts a considerable amount and range of livestock, such as cattle ranches and hog farms.\nFront Range.\nRoughly 70% of Colorado's population resides along the eastern edge of the Rocky Mountains in the Front Range Urban Corridor between Cheyenne, Wyoming, and Pueblo, Colorado. This region is partially protected from prevailing storms that blow in from the Pacific Ocean region by the high Rockies in the middle of Colorado. The \"Front Range\" includes Denver, Boulder, Fort Collins, Loveland, Castle Rock, Colorado Springs, Pueblo, Greeley, and other townships and municipalities in between. On the other side of the Rockies, the significant population centers in western Colorado (which is known as \"The Western Slope\") are the cities of Grand Junction, Durango, and Montrose.\nMountains.\nTo the west of the Great Plains of Colorado rises the eastern slope of the Rocky Mountains. Notable peaks of the Rocky Mountains include Longs Peak, Mount Blue Sky, Pikes Peak, and the Spanish Peaks near Walsenburg, in southern Colorado. This area drains to the east and the southeast, ultimately either via the Mississippi River or the Rio Grande into the Gulf of Mexico.\nThe Rocky Mountains within Colorado contain 53 true peaks and 58 named peaks that are or higher in elevation above sea level, known as fourteeners. These mountains are largely covered with trees such as conifers and aspens up to the tree line, at an elevation of about in southern Colorado to about in northern Colorado. Above this tree line, only alpine vegetation grows. \nMuch of the alpine snow melts by mid-August except for a few snow-capped peaks and a few small glaciers. The Colorado Mineral Belt, stretching from the San Juan Mountains in the southwest to Boulder and Central City on the front range, contains most of the historic gold- and silver-mining districts of Colorado. The 30 highest major summits of the Rocky Mountains of North America are all within the state.\nThe summit of Mount Elbert at elevation in Lake County is the highest point in Colorado and the Rocky Mountains of North America. Colorado is the only U.S. state that lies entirely above 1,000 meters elevation. The point where the Arikaree River flows out of Yuma County, Colorado, and into Cheyenne County, Kansas, is the lowest in Colorado at elevation. This point, which is the highest low elevation point of any state, is higher than the high elevation points of 18 states and the District of Columbia.\nContinental Divide.\nThe Continental Divide of the Americas extends along the crest of the Rocky Mountains. The area of Colorado to the west of the Continental Divide is called the Western Slope of Colorado. West of the Continental Divide, water flows to the southwest via the Colorado River and the Green River towards the Gulf of California.\nWithin the interior of the Rocky Mountains are several large parks which are high broad basins. In the north, on the east side of the Continental Divide is the North Park of Colorado. The North Park is drained by the North Platte River, which flows north into Wyoming and Nebraska. Just to the south of North Park, but on the western side of the Continental Divide, is the Middle Park of Colorado, which is drained by the Colorado River. The South Park of Colorado is the region of the headwaters of the South Platte River.\nSouth Central region.\nIn south-central Colorado is the large San Luis Valley, where the headwaters of the Rio Grande are located. The northern part of the valley is the San Luis Closed Basin, an endorheic basin that helped created the Great Sand Dunes. The valley sits between the Sangre de Cristo Mountains and San Juan Mountains. The Rio Grande drains due south into New Mexico, Texas, and Mexico. Across the Sangre de Cristo Range to the east of the San Luis Valley lies the Wet Mountain Valley. These basins, particularly the San Luis Valley, lie along the Rio Grande rift, a major geological formation of the Rocky Mountains, and its branches.\nWestern Slope.\nThe Western Slope of Colorado includes the western face of the Rocky Mountains and all of the area to the western border. This area includes several terrains and climates from alpine mountains to arid deserts. The Western Slope includes many ski resort towns in the Rocky Mountains and towns west to Utah. It is less populous than the Front Range but includes a large number of national parks and monuments.\nThe northwestern corner of Colorado is a sparsely populated region, and it contains part of the noted Dinosaur National Monument, which not only is a paleontological area, but is also a scenic area of rocky hills, canyons, arid desert, and streambeds. Here, the Green River briefly crosses over into Colorado.\nThe Western Slope of Colorado is drained by the Colorado River and its tributaries (primarily the Gunnison River, Green River, and the San Juan River). The Colorado River flows through Glenwood Canyon, and then through an arid valley made up of desert from Rifle to Parachute, through the desert canyon of De Beque Canyon, and into the arid desert of Grand Valley, where the city of Grand Junction is located.\nAlso prominent is the Grand Mesa, which lies to the southeast of Grand Junction; the high San Juan Mountains, a rugged mountain range; and to the north and west of the San Juan Mountains, the Colorado Plateau.\nGrand Junction, Colorado, at the confluence of the Colorado and Gunnison Rivers, is the largest city on the Western Slope. Grand Junction and Durango are the only major centers of television broadcasting west of the Continental Divide in Colorado, though most mountain resort communities publish daily newspapers. Grand Junction is located at the juncture of Interstate 70 and US 50, the only major highways in western Colorado. Grand Junction is also along the major railroad of the Western Slope, the Union Pacific. This railroad also provides the tracks for Amtrak's \"California Zephyr\" passenger train, which crosses the Rocky Mountains between Denver and Grand Junction.\nThe Western Slope includes multiple notable destinations in the Colorado Rocky Mountains, including Glenwood Springs, with its resort hot springs, and the ski resorts of Aspen, Breckenridge, Vail, Crested Butte, Steamboat Springs, and Telluride.\nHigher education in and near the Western Slope can be found at Colorado Mesa University in Grand Junction, Western Colorado University in Gunnison, Fort Lewis College in Durango, and Colorado Mountain College in Glenwood Springs and Steamboat Springs.\nThe Four Corners Monument in the southwest corner of Colorado marks the common boundary of Colorado, New Mexico, Arizona, and Utah; the only such place in the United States.\nClimate.\nThe climate of Colorado is more complex than states outside of the Mountain States region. Unlike most other states, southern Colorado is not always warmer than northern Colorado. Most of Colorado is made up of mountains, foothills, high plains, and desert lands. Mountains and surrounding valleys greatly affect the local climate. Northeast, east, and southeast Colorado are mostly the high plains, while Northern Colorado is a mix of high plains, foothills, and mountains. Northwest and west Colorado are predominantly mountainous, with some desert lands mixed in. Southwest and southern Colorado are a complex mixture of desert and mountain areas.\nEastern Plains.\nThe climate of the Eastern Plains is semi-arid (K\u00f6ppen climate classification: \"BSk\") with low humidity and moderate precipitation, usually from annually, although many areas near the rivers are semi-humid climate. The area is known for its abundant sunshine and cool, clear nights, which give this area a great average diurnal temperature range. The difference between the highs of the days and the lows of the nights can be considerable as warmth dissipates to space during clear nights, the heat radiation not being trapped by clouds. The Front Range urban corridor, where most of the population of Colorado resides, lies in a pronounced precipitation shadow as a result of being on the lee side of the Rocky Mountains.\nIn summer, this area can have many days above 95\u00a0\u00b0F (35\u00a0\u00b0C) and often 100\u00a0\u00b0F (38\u00a0\u00b0C). On the plains, the winter lows usually range from 25 to \u221210 \u00b0F (\u22124 to \u221223\u00a0\u00b0C). About 75% of the precipitation falls within the growing season, from April to September, but this area is very prone to droughts. Most of the precipitation comes from thunderstorms, which can be severe, and from major snowstorms that occur in the winter and early spring. Otherwise, winters tend to be mostly dry and cold.\nIn much of the region, March is the snowiest month. April and May are normally the rainiest months, while April is the wettest month overall. The Front Range cities closer to the mountains tend to be warmer in the winter due to Chinook winds which warms the area, sometimes bringing temperatures of 70 \u00b0F (21\u00a0\u00b0C) or higher in the winter. The average July temperature is 55\u00a0\u00b0F (13\u00a0\u00b0C) in the morning and 90\u00a0\u00b0F (32\u00a0\u00b0C) in the afternoon. The average January temperature is 18\u00a0\u00b0F (\u22128\u00a0\u00b0C) in the morning and 48\u00a0\u00b0F (9\u00a0\u00b0C) in the afternoon, although variation between consecutive days can be 40\u00a0\u00b0F (22 \u00b0C).\nFront Range foothills.\nJust west of the plains and into the foothills, there is a wide variety of climate types. Locations merely a few miles apart can experience entirely different weather depending on the topography. Most valleys have a semi-arid climate, not unlike the eastern plains, which transitions to an alpine climate at the highest elevations. Microclimates also exist in local areas that run nearly the entire spectrum of climates, including subtropical highland (\"Cfb/Cwb\"), humid subtropical (\"Cfa\"), humid continental (\"Dfa/Dfb\"), Mediterranean (\"Csa/Csb\") and subarctic (\"Dfc\").\nExtreme weather.\nExtreme weather changes are common in Colorado, although a significant portion of the extreme weather occurs in the least populated areas of the state. Thunderstorms are common east of the Continental Divide in the spring and summer, yet are usually brief. Hail is a common sight in the mountains east of the Divide and across the eastern Plains, especially the northeast part of the state. Hail is the most commonly reported warm-season severe weather hazard, and occasionally causes human injuries, as well as significant property damage. The eastern Plains are subject to some of the biggest hail storms in North America. Notable examples are the severe hailstorms that hit Denver on July 11, 1990, and May 8, 2017, the latter being the costliest ever in the state.\nThe Eastern Plains are part of the extreme western portion of Tornado Alley; some damaging tornadoes in the Eastern Plains include the 1990 Limon F3 tornado and the 2008 Windsor EF3 tornado, which devastated a small town. Portions of the eastern Plains see especially frequent tornadoes, both those spawned from mesocyclones in supercell thunderstorms and from less intense landspouts, such as within the Denver convergence vorticity zone (DCVZ).\nThe Plains are also susceptible to occasional floods and particularly severe flash floods, which are caused both by thunderstorms and by the rapid melting of snow in the mountains during warm weather. Notable examples include the 1965 Denver Flood, the Big Thompson River flooding of 1976 and the 2013 Colorado floods. Hot weather is common during summers in Denver. The city's record in 1901 for the number of consecutive days above 90\u00a0\u00b0F (32\u00a0\u00b0C) was broken during the summer of 2008. The new record of 24 consecutive days surpassed the previous record by almost a week.\nMuch of Colorado is very dry, with the state averaging only of precipitation per year statewide. The state rarely experiences a time when some portion is not in some degree of drought. The lack of precipitation contributes to the severity of wildfires in the state, such as the Hayman Fire of 2002. Other notable fires include the Fourmile Canyon Fire of 2010, the Waldo Canyon Fire and High Park Fire of June 2012, and the Black Forest Fire of June 2013. Even these fires were exceeded in severity by the Pine Gulch Fire, Cameron Peak Fire, and East Troublesome Fire in 2020, all being the three largest fires in Colorado history (see 2020 Colorado wildfires). And the Marshall Fire which started on December 30, 2021, while not the largest in state history, was the most destructive ever in terms of property loss (see Marshall Fire).\nHowever, some of the mountainous regions of Colorado receive a huge amount of moisture from winter snowfalls. The spring melts of these snows often cause great waterflows in the Yampa River, the Colorado River, the Rio Grande, the Arkansas River, the North Platte River, and the South Platte River.\nWater flowing out of the Colorado Rocky Mountains is a very significant source of water for the farms, towns, and cities of the southwest states of New Mexico, Arizona, Utah, and Nevada, as well as the Midwest, such as Nebraska and Kansas, and the southern states of Oklahoma and Texas. A significant amount of water is also diverted for use in California; occasionally (formerly naturally and consistently), the flow of water reaches northern Mexico.\nRecords.\nThe highest official ambient air temperature ever recorded in Colorado was on July 20, 2019, at John Martin Dam. The lowest official air temperature was on February 1, 1985, at Maybell.\nEarthquakes.\nDespite its mountainous terrain, Colorado experiences less seismic activity than states like California and Alaska. There are over 90 potentially active faults, and since 1867, Colorado has experienced 700 recorded earthquakes of magnitude 2.5 or higher. The U.S. National Earthquake Information Center is located in Golden.\nOn August 22, 2011, a 5.3 magnitude earthquake occurred west-southwest of the city of Trinidad. There were no casualties and only a small amount of damage was reported. It was the second-largest earthquake in Colorado's history, the largest being a magnitude 6.6 earthquake, recorded in 1882. Four minor earthquakes rattled Colorado on August 24, 2018, ranging from magnitude 2.9 to 4.3. , there were 525 recorded earthquakes in Colorado since 1973, a majority of which range 2\u00a0to 3.5 on the Richter scale.\nFauna.\nA process of extirpation by trapping and poisoning of the gray wolf (\"Canis lupus\") from Colorado in the 1930s saw the last wild wolf in the state shot in 1945. A wolf pack recolonized Moffat County, Colorado in northwestern Colorado in 2019. Cattle farmers have expressed concern that a returning wolf population potentially threatens their herds. Coloradans voted to reintroduce gray wolves in 2020, with the state committing to a plan to have a population in the state by 2022 and permitting non-lethal methods of driving off wolves attacking livestock and pets.\nWhile there is fossil evidence of Harrington's mountain goat in Colorado between at least 800,000 years ago and its extinction with megafauna roughly 11,000 years ago, the mountain goat is not native to Colorado but was instead introduced to the state over time during the interval between 1947 and 1972. Despite being an artificially-introduced species, the state declared mountain goats a native species in 1993. In 2013, 2014, and 2019, an unknown illness killed nearly all mountain goat kids, leading to a Colorado Parks and Wildlife investigation.\nThe native population of pronghorn in Colorado has varied wildly over the last century, reaching a low of only 15,000 individuals during the 1960s. However, conservation efforts succeeded in bringing the stable population back up to roughly 66,000 by 2013. The population was estimated to have reached 85,000 by 2019 and had increasingly more run-ins with the increased suburban housing along the eastern Front Range. State wildlife officials suggested that landowners would need to modify fencing to allow the greater number of pronghorns to move unabated through the newly developed land. Pronghorns are most readily found in the northern and eastern portions of the state, with some populations also in the western San Juan Mountains.\nCommon wildlife found in the mountains of Colorado include mule deer, southwestern red squirrel, golden-mantled ground squirrel, yellow-bellied marmot, moose, American pika, and red fox, all at exceptionally high numbers, though moose are not native to the state. The foothills include deer, fox squirrel, desert cottontail, mountain cottontail, and coyote. The prairies are home to black-tailed prairie dog, the endangered swift fox, American badger, and white-tailed jackrabbit.\nGovernment.\nState government.\nLike the federal government and all other U.S. states, Colorado's state constitution provides for three branches of government: the legislative, the executive, and the judicial branches.\nThe Governor of Colorado heads the state's executive branch. The current governor is Jared Polis, a Democrat. Colorado's other statewide elected executive officers are the Lieutenant Governor of Colorado (elected on a ticket with the Governor), Secretary of State of Colorado, Colorado State Treasurer, and Attorney General of Colorado, all of whom serve four-year terms.\nThe seven-member Colorado Supreme Court is the state's highest court. The Colorado Court of Appeals, with 22 judges, sits in divisions of three judges each. Colorado is divided into 22 judicial districts, each of which has a district court and a county court with limited jurisdiction. The state also has specialized water courts, which sit in seven distinct divisions around the state and which decide matters relating to water rights and the use and administration of water.\nThe state legislative body is the Colorado General Assembly, which is made up of two houses \u2013 the House of Representatives and the Senate. The House has 65 members and the Senate has 35. , the Democratic Party holds a 23 to 12 majority in the Senate and a 46 to 19 majority in the House.\nMost Coloradans are native to other states (nearly 60% according to the 2000 census), and this is illustrated by the fact that the state did not have a native-born governor from 1975 (when John David Vanderhoof left office) until 2007, when Bill Ritter took office; his election the previous year marked the first electoral victory for a native-born Coloradan in a gubernatorial race since 1958 (Vanderhoof had ascended from the Lieutenant Governorship when John Arthur Love was given a position in Richard Nixon's administration in 1973).\nTax is collected by the Colorado Department of Revenue.\nPolitics.\nColorado was once considered a swing state, but has become a relatively safe blue state in both state and federal elections. In presidential elections, it had not been won until 2020 by double digits since 1984 and has backed the winning candidate in 9 of the last 11 elections. Coloradans have elected 17 Democrats and 12 Republicans to the governorship in the last 100 years.\nIn presidential politics, Colorado was considered a reliably Republican state during the post-World War II era, voting for the Democratic candidate only in 1948, 1964, and 1992. However, it became a competitive swing state in the 1990s. Since the mid-2000s, it has swung heavily to the Democrats, voting for Barack Obama in 2008 and 2012, Hillary Clinton in 2016, Joe Biden in 2020, and Kamala Harris in 2024.\nColorado politics exhibits a contrast between conservative cities such as Colorado Springs and Grand Junction, and liberal cities such as Boulder and Denver. Democrats are strongest in metropolitan Denver, the college towns of Fort Collins and Boulder, southern Colorado (including Pueblo), and several western ski resort counties. The Republicans are strongest in the Eastern Plains, Colorado Springs, Greeley, and far Western Colorado near Grand Junction.\nColorado is represented by two members of the United States Senate:\nColorado is represented by eight members of the United States House of Representatives:\nIn a 2020 study, Colorado was ranked as the seventh easiest state for citizens to vote in.\nSignificant initiatives and legislation enacted in Colorado.\nColorado was the first state in the union to enact, by voter referendum, a law extending suffrage to women. That initiative was approved by the state's voters on November 7, 1893.\nOn the November 8, 1932, ballot, Colorado approved the repeal of alcohol prohibition more than a year before the Twenty-first Amendment to the United States Constitution was ratified.\nColorado has banned, via C.R.S. section 12-6-302, the sale of motor vehicles on Sunday since at least 1953.\nIn 1972, Colorado voters rejected a referendum proposal to fund the 1976 Winter Olympics, which had been scheduled to be held in the state. Denver had been chosen by the International Olympic Committee as the host city on May 12, 1970.\nIn 1992, by a margin of 53 to 47 percent, Colorado voters approved an amendment to the state constitution (Amendment 2) that would have prevented any city, town, or county in the state from taking any legislative, executive, or judicial action to recognize homosexuals or bisexuals as a protected class. In 1996, in a 6\u20133 ruling in \"Romer v. Evans\", the U.S. Supreme Court found that preventing protected status based upon homosexuality or bisexuality did not satisfy the Equal Protection Clause.\nIn 2006, voters passed Amendment 43, which banned same-sex marriage in Colorado. That initiative was nullified by the U.S. Supreme Court's 2015 decision in \"Obergefell v. Hodges\". In 2024, Colorado residents voted to establish an explicit right to abortion in Colorado's state constitution and to repeal Amendment 43's defunct marriage ban.\nIn 2012, voters amended the state constitution protecting the \"personal use\" of marijuana for adults, establishing a framework to regulate cannabis like alcohol. The first recreational marijuana shops in Colorado, and by extension the United States, opened their doors on January 1, 2014.\nOn 30 October 2019, Colorado became the first state to accept digital ID via its myColorado app. The state-issued digital identifications will be considered valid when Real ID enforcement begins in 2025, in line with the Real ID Act of 2005. By November 2022 The Colorado Governor's Office of Information Technology announced that the myColorado app had over 1 million users.\nOn December 19, 2023, the Colorado Supreme Court ruled that Donald Trump was disqualified from the 2024 United States presidential election in part due to his alleged incitement of the January 6 United States Capitol attack. On March 4, 2024, the United States Supreme Court overruled the Colorado decision.\nCounties.\nThe State of Colorado is divided into 64 counties. Two of these counties, the City and County of Broomfield and the City and County of Denver, have consolidated city and county governments. Counties are important units of government in Colorado since there are no civil townships or other minor civil divisions.\nThe most populous county in Colorado is El Paso County, the home of the City of Colorado Springs. The second most populous county is the City and County of Denver, the state capital. Five of the 64 counties now have more than 500,000 residents, while 12 have fewer than 5,000 residents. The ten most populous Colorado counties are all located in the Front Range Urban Corridor. Mesa County is the most populous county on the Colorado Western Slope.\nMunicipalities.\nColorado has 273 active incorporated municipalities, comprising 198 towns, 73 cities, and two consolidated city and county governments. At the 2020 United States census, 4,299,942 of the 5,773,714 Colorado residents (74.47%) lived in one of these municipalities. Another 714,417 residents (12.37%) lived in one of the 210 census-designated places, while the remaining 759,355 residents (13.15%) lived in the many rural and mountainous areas of the state.\nColorado municipalities operate under one of five types of municipal governing authority. Colorado currently has two consolidated city and county governments, 61 home rule cities, 12 statutory cities, 35 home rule towns, 161 statutory towns, and one territorial charter municipality.\nThe most populous municipality is the City and County of Denver. Colorado has 12 municipalities with more than 100,000 residents, and 17 with fewer than 100 residents. The 16 most populous Colorado municipalities are all located in the Front Range Urban Corridor. The City of Grand Junction is the most populous municipality on the Colorado Western Slope. The Town of Carbonate has had no year-round population since the 1890 census due to its severe winter weather and difficult access.\nUnincorporated communities.\nIn addition to its 272 municipalities, Colorado has 210 unincorporated census-designated places (CDPs) and many other small communities. The most populous unincorporated community in Colorado is Highlands Ranch south of Denver. The seven most populous CDPs are located in the Front Range Urban Corridor. The Clifton CDP is the most populous CDP on the Colorado Western Slope.\nSpecial districts.\nColorado has more than 4,000 special districts, most with property tax authority. These districts may provide schools, law enforcement, fire protection, water, sewage, drainage, irrigation, transportation, recreation, infrastructure, cultural facilities, business support, redevelopment, or other services.\nSome of these districts have the authority to levy sales tax as well as property tax and use fees. This has led to a hodgepodge of sales tax and property tax rates in Colorado. There are some street intersections in Colorado with a different sales tax rate on each corner, sometimes substantially different.\nSome of the more notable Colorado districts are:\nStatistical areas.\nMost recently on March 6, 2020, the Office of Management and Budget defined 21 statistical areas for Colorado comprising four combined statistical areas, seven metropolitan statistical areas, and ten micropolitan statistical areas.\nThe most populous of the seven metropolitan statistical areas in Colorado is the 10-county Denver-Aurora-Lakewood, CO Metropolitan Statistical Area with a population of 2,963,821 at the 2020 United States census, an increase of +15.29% since the 2010 census.\nThe more extensive 12-county Denver-Aurora, CO Combined Statistical Area had a population of 3,623,560 at the 2020 census, an increase of +17.23% since the 2010 census.\nThe most populous extended metropolitan region in Rocky Mountain Region is the 18-county Front Range Urban Corridor along the northeast face of the Southern Rocky Mountains. This region with Denver at its center had a population of 5,055,344 at the 2020 census, an increase of +16.65% since the 2010 census.\nDemographics.\nThe United States Census Bureau estimated the population of Colorado on July 1, 2023, at 5,877,610, a 1.80% increase since the 2020 United States census.\nColoradan Hispanics and Latinos (of any race and heritage) made up 20.7% of the population. According to the 2000 census, the largest ancestry groups in Colorado are German (22%), Mexican (18%), Irish (12%), and English (12%). Persons reporting German ancestry are especially numerous in the Front Range, the Rockies (west-central counties), and Eastern parts/High Plains.\nColorado has a high proportion of Hispanic, mostly Mexican-American, citizens in Metropolitan Denver, Colorado Springs, as well as the smaller cities of Greeley and Pueblo, and elsewhere. Southern, Southwestern, and Southeastern Colorado have a large number of Hispanos, the descendants of the early settlers of colonial Spanish origin. In 1940, the U.S. Census Bureau reported Colorado's population as 8.2% Hispanic and 90.3% non-Hispanic White. The Hispanic population of Colorado has continued to grow quickly over the past decades. By 2019, Hispanics made up 22% of Colorado's population, and Non-Hispanic Whites made up 70%. Spoken English in Colorado has many Spanish idioms.\nColorado also has some large African-American communities located in Denver, in the neighborhoods of Montbello, Five Points, Whittier, and many other East Denver areas. The state has sizable numbers of Asian-Americans of Mongolian, Chinese, Filipino, Korean, Southeast Asian, and Japanese descent. The highest population of Asian Americans can be found on the south and southeast side of Denver, as well as some on Denver's southwest side. The Denver metropolitan area is considered more liberal and diverse than much of the state when it comes to political issues and environmental concerns.\nThe population of Native Americans in the state is small. Native Americans are concentrated in metropolitan Denver and the southwestern corner of Colorado, where there are two Ute reservations.\nThe majority of Colorado's immigrants are from Mexico, India, China, Vietnam, Korea, Germany and Canada.\nThere were a total of 70,331 births in Colorado in 2006. (Birth rate of 14.6 per thousand.) In 2007, non-Hispanic Whites were involved in 59.1% of all births. Some 14.06% of those births involved a non-Hispanic White person and someone of a different race, most often with a couple including one Hispanic. A birth where at least one Hispanic person was involved counted for 43% of the births in Colorado. As of the 2010 census, Colorado has the seventh highest percentage of Hispanics (20.7%) in the U.S. behind New Mexico (46.3%), California (37.6%), Texas (37.6%), Arizona (29.6%), Nevada (26.5%), and Florida (22.5%). Per the 2000 census, the Hispanic population is estimated to be 918,899, or approximately 20% of the state's total population. Colorado has the 5th-largest population of Mexican-Americans, behind California, Texas, Arizona, and Illinois. In percentages, Colorado has the 6th-highest percentage of Mexican-Americans, behind New Mexico, California, Texas, Arizona, and Nevada.\nBirth data.\nIn 2011, 46% of Colorado's population younger than the age of one were minorities, meaning that they had at least one parent who was not non-Hispanic White.\n\"Note: Births in table do not add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.\"\nIn 2017, Colorado recorded the second-lowest fertility rate in the United States outside of New England, after Oregon, at 1.63 children per woman. Significant contributing factors to the decline in pregnancies were theTitle X Family Planning Program and an intrauterine device grant from Warren Buffett's family.\nLanguage.\nThe English language, the official language of the state, is the most commonly spoken language in Colorado. The second most commonly spoken language in the state is the Spanish language. The Colorado River Numic language, also known as the Ute dialect, is still spoken in Colorado.\nReligion.\nMajor religious affiliations of the people of Colorado as of 2014 were 64% Christian, of whom there are 44% Protestant, 16% Roman Catholic, 3% Mormon, and 1% Eastern Orthodox. Other religious breakdowns according to the Pew Research Center were 1% Judaism, 1% Muslim, 1% Buddhist, and 4% other. Secular Coloradans made up 29% of the population. In 2020, according to the Public Religion Research Institute, Christianity was 66% of the population. Judaism was also reported to have increased in this separate study, forming 2% of the religious landscape, while the religiously unaffiliated were reported to form 28% of the population in this separate study. In 2022, the same organization reported 61% was Christian (39% Protestant, 19% Catholic, 2% Mormon, 1% Eastern Orthodox), 2% New Age, 1% Jewish, 1% Hindu, and 34% religiously unaffiliated.\nAccording to the Association of Religion Data Archives, the largest Christian denominations by the number of adherents in 2010 were the Catholic Church with 811,630; multi-denominational Evangelical Protestants with 229,981; and the Church of Jesus Christ of Latter-day Saints with 151,433. In 2020, the Association of Religion Data Archives determined the largest Christian denominations were Catholics (873,236), non/multi/inter-denominational Protestants (406,798), and Mormons (150,509). Throughout its non-Christian population, there were 12,500 Hindus, 7,101 Hindu Yogis, and 17,369 Buddhists at the 2020 study.\nOur Lady of Guadalupe Catholic Church was the first permanent Catholic parish in modern-day Colorado and was constructed by Spanish colonists from New Mexico in modern-day Conejos. Latin Church Catholics are served by three dioceses: the Archdiocese of Denver and the Dioceses of Colorado Springs and Pueblo.\nThe first permanent settlement by members of the Church of Jesus Christ of Latter-day Saints in Colorado arrived from Mississippi and initially camped along the Arkansas River just east of the present-day site of Pueblo.\nHealth.\nColorado is generally considered among the healthiest states by behavioral and healthcare researchers. Among the positive contributing factors is the state's well-known outdoor recreation opportunities and initiatives. However, there is a stratification of health metrics with wealthier counties such as Douglas and Pitkin performing significantly better relative to southern, less wealthy counties such as Huerfano and Las Animas.\nObesity.\nAccording to several studies, Coloradans have the lowest rates of obesity of any state in the US. , 24% of the population was considered medically obese, and while the lowest in the nation, the percentage had increased from 17% in 2004.\nLife expectancy.\nAccording to a report in the Journal of the American Medical Association, residents of Colorado had a 2014 life expectancy of 80.21 years, the longest of any U.S. state.\nHomelessness.\nAccording to HUD's 2022 Annual Homeless Assessment Report, there were an estimated 10,397 homeless people in Colorado.\nEconomy.\nIn 2019 the total employment was 2,473,192. The number of employer establishments is 174,258.\nThe total state product in 2015 was $318.6 billion. Median Annual Household Income in 2016 was $70,666, 8th in the nation. Per capita personal income in 2010 was $51,940, ranking Colorado 11th in the nation. The state's economy broadened from its mid-19th-century roots in mining when irrigated agriculture developed, and by the late 19th century, raising livestock had become important. Early industry was based on the extraction and processing of minerals and agricultural products. Current agricultural products are cattle, wheat, dairy products, corn, and hay.\nThe federal government operates several federal facilities in the state, including NORAD (North American Aerospace Defense Command), United States Air Force Academy, Schriever Air Force Base located approximately 10 miles (16 kilometers) east of Peterson Air Force Base, and Fort Carson, both located in Colorado Springs within El Paso County; NOAA, the National Renewable Energy Laboratory (NREL) in Golden, and the National Institute of Standards and Technology in Boulder; U.S. Geological Survey and other government agencies at the Denver Federal Center near Lakewood; the Denver Mint, Buckley Space Force Base, the Tenth Circuit Court of Appeals, and the Byron G. Rogers Federal Building and United States Courthouse in Denver; and a federal Supermax Prison and other federal prisons near Ca\u00f1on City. In addition to these and other federal agencies, Colorado has abundant National Forest land and four National Parks that contribute to federal ownership of of land in Colorado, or 37% of the total area of the state.\nIn the second half of the 20th century, the industrial and service sectors expanded greatly. The state's economy is diversified and is notable for its concentration on scientific research and high-technology industries. Other industries include food processing, transportation equipment, machinery, chemical products, the extraction of metals such as gold (see Gold mining in Colorado), silver, and molybdenum. Colorado now also has the largest annual production of beer in any state. Denver is an important financial center.\nThe state's diverse geography and majestic mountains attract millions of tourists every year, including 85.2\u00a0million in 2018. Tourism contributes greatly to Colorado's economy, with tourists generating $22.3 billion in 2018.\nSeveral nationally known brand names have originated in Colorado factories and laboratories. From Denver came the forerunner of telecommunications giant Qwest in 1879, Samsonite luggage in 1910, Gates belts and hoses in 1911, and Russell Stover Candies in 1923. Kuner canned vegetables began in Brighton in 1864. From Golden came Coors beer in 1873, CoorsTek industrial ceramics in 1920, and Jolly Rancher candy in 1949. CF&amp;I railroad rails, wire, nails, and pipe debuted in Pueblo in 1892. Holly Sugar was first milled from beets in Holly in 1905, and later moved its headquarters to Colorado Springs. The present-day Swift packed meat of Greeley evolved from Monfort of Colorado, Inc., established in 1930. Estes model rockets were launched in Penrose in 1958. Fort Collins has been the home of Woodward Governor Company's motor controllers (governors) since 1870, and Waterpik dental water jets and showerheads since 1962. Celestial Seasonings herbal teas have been made in Boulder since 1969. Rocky Mountain Chocolate Factory made its first candy in Durango in 1981.\nColorado has a flat 4.63% income tax, regardless of income level. On 3 November 2020 voters authorized an initiative to lower that income tax rate to 4.55 percent. Unlike most states, which calculate taxes based on federal \"adjusted gross income\", Colorado taxes are based on \"taxable income\"\u2014income after federal exemptions and federal itemized (or standard) deductions. Colorado's state sales tax is 2.9% on retail sales. When state revenues exceed state constitutional limits, according to Colorado's Taxpayer Bill of Rights legislation, full-year Colorado residents can claim a sales tax refund on their individual state income tax return. Many counties and cities charge their own rates, in addition to the base state rate. There are also certain county and special district taxes that may apply.\nReal estate and personal business property are taxable in Colorado. The state's senior property tax exemption was temporarily suspended by the Colorado Legislature in 2003. The tax break was scheduled to return for the assessment year 2006, payable in 2007.\n, the state's unemployment rate was 4.2%.\nThe West Virginia teachers' strike in 2018 inspired teachers in other states, including Colorado, to take similar action.\nAgriculture.\nCorn is grown in the Eastern Plains of Colorado. Arid conditions and drought negatively impacted yields in 2020 and 2022.\nNatural resources.\nColorado has significant hydrocarbon resources. According to the Energy Information Administration, Colorado hosts seven of the largest natural gas fields in the United States, and two of the largest oil fields. Conventional and unconventional natural gas output from several Colorado basins typically accounts for more than five percent of annual U.S. natural gas production. Colorado's oil shale deposits hold an estimated of oil\u2014nearly as much oil as the entire world's proven oil reserves. Substantial deposits of bituminous, subbituminous, and lignite coal are found in the state.\nUranium mining in Colorado goes back to 1872, when pitchblende ore was taken from gold mines near Central City, Colorado. Not counting byproduct uranium from phosphate, Colorado is considered to have the third-largest uranium reserves of any U.S. state, behind Wyoming and New Mexico. When Colorado and Utah dominated radium mining from 1910 to 1922, uranium and vanadium were the byproducts (giving towns like present-day Superfund site Uravan their names). Uranium price increases from 2001 to 2007 prompted several companies to revive uranium mining in Colorado. During the 1940s certain communities\u2013including Naturita and Paradox\u2013earned the moniker of \"yellowcake towns\" from their relationship with uranium mining. Price drops and financing problems in late 2008 forced these companies to cancel or scale back the uranium-mining project. As of 2016, there were no major uranium mining operations in the state, though plans existed to restart production.\nElectricity generation.\nColorado's high Rocky Mountain ridges and eastern plains offer wind power potential, and geologic activity in the mountain areas provides the potential for geothermal power development. Much of the state is sunny and could produce solar power. Major rivers flowing from the Rocky Mountains offer hydroelectric power resources.\nCulture.\nArts and film.\nSeveral film productions have been shot on location in Colorado, especially prominent Westerns like \"True Grit\", \"The Searchers\", \"City Slickers,\" \"Butch Cassidy and the Sundance Kid, and My Life With the Walter Boys\". Several historic military forts, railways with trains still operating, and mining ghost towns have been used and transformed for historical accuracy in well-known films. There are also several scenic highways and mountain passes that helped to feature the open road in films such as \"Vanishing Point\", \"Bingo\" and \"Starman\". Some Colorado landmarks have been featured in films, such as The Stanley Hotel in \"Dumb and Dumber\" and \"The Shining\" and the Sculptured House in \"Sleeper\". In 2015, \"Furious 7\" was to film driving sequences on Pikes Peak Highway in Colorado. The TV adult-animated series \"South Park\" takes place in central Colorado in the titular town. Additionally, The TV series \"Good Luck Charlie\" was set, but not filmed, in Denver, Colorado. The Colorado Office of Film and Television has noted that more than 400 films have been shot in Colorado.\nThere are also several established film festivals in Colorado, including Aspen Filmfest and Aspen Shortsfest, Boulder International Film Festival, Castle Rock Film Festival, Denver Film Festival, Festivus Film Festival, Mile High Horror Film Festival, Moondance International Film Festival, Mountainfilm in Telluride, Rocky Mountain Women's Film Festival, and Telluride Film Festival.\nMany notable writers have lived or spent extended periods in Colorado. Beat Generation writers Jack Kerouac and Neal Cassady lived in and around Denver for several years each. Irish playwright Oscar Wilde visited Colorado on his tour of the United States in 1882, writing in his 1906 \"Impressions of America\" that Leadville was \"the richest city in the world. It has also got the reputation of being the roughest, and every man carries a revolver.\"\nCuisine.\nColorado is known for its Southwest and Rocky Mountain cuisine, with Mexican restaurants found throughout the state.\nBoulder was named America's Foodiest Town 2010 by Bon App\u00e9tit. Boulder, and Colorado in general, is home to several national food and beverage companies, top-tier restaurants and farmers' markets. Boulder also has more Master Sommeliers per capita than any other city, including San Francisco and New York. Denver is known for steak, but now has a diverse culinary scene with many restaurants.\nPolidori Sausage is a brand of pork products available in supermarkets, which originated in Colorado, in the early 20th century.\nThe Food &amp; Wine Classic is held annually each June in Aspen. Aspen also has a reputation as the culinary capital of the Rocky Mountain region.\nWine and beer.\nColorado wines include varietals that have attracted favorable notice from outside the state. With wines made from traditional \"Vitis vinifera\" grapes along with wines made from cherries, peaches, plums, and honey, Colorado wines have won top national and international awards for their quality. Colorado's grape growing regions contain the highest elevation vineyards in the United States, with most viticulture in the state practiced between above sea level. The mountain climate ensures warm summer days and cool nights. Colorado is home to two designated American Viticultural Areas of the Grand Valley AVA and the West Elks AVA, where most of the vineyards in the state are located. However, an increasing number of wineries are located along the Front Range. In 2018, Wine Enthusiast Magazine named Colorado's Grand Valley AVA in Mesa County, Colorado, as one of the Top Ten wine travel destinations in the world.\nColorado is home to many nationally praised microbreweries, including New Belgium Brewing Company, Odell Brewing Company, Great Divide Brewing Company, and Bristol Brewing Company. The area of northern Colorado near and between the cities of Denver, Boulder, and Fort Collins is known as the \"Napa Valley of Beer\" due to its high density of craft breweries.\nMarijuana and hemp.\nColorado is open to cannabis (marijuana) tourism. With the adoption of the 64th state amendment in 2012, Colorado became the first state in the union to legalize marijuana for medicinal (2000), industrial (referring to hemp, 2012), and recreational (2012) use. Colorado's marijuana industry sold $1.31 billion worth of marijuana in 2016 and $1.26\u00a0billion in the first three-quarters of 2017. The state generated tax, fee, and license revenue of $194 million in 2016 on legal marijuana sales. Colorado regulates hemp as any part of the plant with less than 0.3% THC.\nOn April 4, 2014, Senate Bill 14\u2013184 addressing oversight of Colorado's industrial hemp program was first introduced, ultimately being signed into law by Governor John Hickenlooper on May 31, 2014.\nMedicinal use.\nOn November 7, 2000, 54% of Colorado voters passed Amendment 20, which amends the Colorado State constitution to allow the medical use of marijuana. A patient's medical use of marijuana, within the following limits, is lawful:\nCurrently, Colorado has listed \"eight medical conditions for which patients can use marijuana\u2014cancer, glaucoma, HIV/AIDS, muscle spasms, seizures, severe pain, severe nausea and cachexia, or dramatic weight loss and muscle atrophy\". While governor, John Hickenlooper allocated about half of the state's $13\u00a0million \"Medical Marijuana Program Cash Fund\" to medical research in the 2014 budget. By 2018, the Medical Marijuana Program Cash Fund was the \"largest pool of pot money in the state\" and was used to fund programs including research into pediatric applications for controlling autism symptoms.\nRecreational use.\nOn November 6, 2012, voters amended the state constitution to protect \"personal use\" of marijuana for adults, establishing a framework to regulate marijuana in a manner similar to alcohol. The first recreational marijuana shops in Colorado, and by extension the United States, opened their doors on January 1, 2014.\nTransportation.\nColorado's primary mode of transportation (in terms of passengers) is its highway system. Interstate\u00a025 (I-25) is the primary north\u2013south highway in the state, connecting Pueblo, Colorado Springs, Denver, and Fort Collins, and extending north to Wyoming and south to New Mexico. I-70 is the primary east\u2013west corridor. It connects Grand Junction and the mountain communities with Denver and enters Utah and Kansas. The state is home to a network of US and Colorado highways that provide access to all principal areas of the state. Many smaller communities are connected to this network only via county roads.\nDenver International Airport (DIA) is the third-busiest domestic U.S. and international airport in the world by passenger traffic. DIA handles by far the largest volume of commercial air traffic in Colorado and is the busiest U.S. hub airport between Chicago and the Pacific coast, making Denver the most important airport for connecting passenger traffic in the western United States.\nPublic transportation bus services are offered both intra-city and inter-city\u2014including the Denver metro area's RTD services. The Regional Transportation District (RTD) operates the popular RTD Bus &amp; Rail transit system in the Denver Metropolitan Area. the RTD rail system had 170 light-rail vehicles, serving of track. In addition to local public transit, intercity bus service is provided by Burlington Trailways, Bustang, Express Arrow, and Greyhound Lines.\nAmtrak operates two passenger rail lines in Colorado, the \"California Zephyr\" and \"Southwest Chief\". Colorado's contribution to world railroad history was forged principally by the Denver and Rio Grande Western Railroad which began in 1870 and wrote the book on mountain railroading. In 1988 the \"Rio Grande\" was acquired, but was merged into, the Southern Pacific Railroad by their joint owner Philip Anschutz. On September 11, 1996, Anschutz sold the combined company to the Union Pacific Railroad, creating the largest railroad network in the United States. The Anschutz sale was partly in response to the earlier merger of Burlington Northern and Santa Fe which formed the large Burlington Northern and Santa Fe Railway (BNSF), Union Pacific's principal competitor in western U.S. railroading. Both Union Pacific and BNSF have extensive freight operations in Colorado.\nColorado's freight railroad network consists of 2,688 miles of Class I trackage. It is integral to the U.S. economy, being a critical artery for the movement of energy, agriculture, mining, and industrial commodities as well as general freight and manufactured products between the East and Midwest and the Pacific coast states.\nIn August 2014, Colorado began to issue driver licenses to aliens not lawfully in the United States who lived in Colorado. In September 2014, KCNC reported that 524 non-citizens were issued Colorado driver licenses that are normally issued to U.S. citizens living in Colorado.\nEducation.\nThe first institution of higher education in the Colorado Territory was the Colorado Seminary, opened on November 16, 1864, by the Methodist Episcopal Church. The seminary closed in 1867 but reopened in 1880 as the University of Denver. In 1870, the Bishop George Maxwell Randall of the Episcopal Church's Missionary District of Colorado and Parts Adjacent opened the first of what become the Colorado University Schools which would include the Territorial School of Mines opened in 1873 and sold to the Colorado Territory in 1874. These schools were initially run by the Episcopal Church. An 1861 territorial act called for the creation of a public university in Boulder, though it would not be until 1876 that the University of Colorado was founded. The 1876 act also renamed Territorial School of Mines as the Colorado School of Mines. An 1870 territorial act created the Agricultural College of Colorado which opened in 1879. The college was renamed the Colorado State College of Agriculture and Mechanic Arts in 1935, and became Colorado State University in 1957.\nThe first Catholic college in Colorado was the Jesuit Sacred Heart College, which was founded in New Mexico in 1877, moved to Morrison in 1884, and to Denver in 1887. The college was renamed Regis College in 1921 and Regis University in 1991. On April 1, 1924, armed students patrolled the campus after a burning cross was found, the climax of tensions between Regis College and the locally-powerful Ku Klux Klan.\nFollowing a 1950 assessment by the Service Academy Board, it was determined that there was a need to supplement the U.S. Military and Naval Academies with a third school that would provide commissioned officers for the newly independent Air Force. On April 1, 1954, President Dwight Eisenhower signed a law that moved for the creation of a U.S. Air Force Academy. Later that year, Colorado Springs was selected to host the new institution. From its establishment in 1955, until the construction of appropriate facilities in Colorado Springs was completed and opened in 1958, the Air Force Academy operated out of Lowry Air Force Base in Denver. With the opening of the Colorado Springs facility, the cadets moved to the new campus, though not in the full-kit march that some urban and campus legends suggest. The first class of Space Force officers from the Air Force Academy commissioned on April 18, 2020.\nIndigenous People.\nThe two Native American reservations remaining in Colorado are the Southern Ute Indian Reservation (1873; Ute dialect: \"Kapuuta-wa Moghwachi N\u00fauchi-u\") and Ute Mountain Ute Indian Reservation (1940; Ute dialect: \"W\u0289gama N\u00fauchi\").\nThe two abolished Indian reservations in Colorado were the Cheyenne and Arapaho Indian Reservation (1851\u20131870) and Ute Indian Reservation (1855\u20131873).\nMilitary installations.\nThe major military installations in Colorado include:\nFormer military posts in Colorado include:\nProtected areas.\nColorado is home to:\nSports.\nColorado has five major professional sports leagues, all based in the Denver metropolitan area. Colorado is the least populous state with a franchise in each of the major professional sports leagues.\nThe Colorado Springs Snow Sox professional baseball team is based in Colorado Springs. The team is a member of the Pecos League, an independent baseball league which is not affiliated with Major or Minor League Baseball.\nThe Pikes Peak International Hill Climb is a major hill climbing motor race held on the Pikes Peak Highway.\nThe Cherry Hills Country Club has hosted several professional golf tournaments, including the U.S. Open, U.S. Senior Open, U.S. Women's Open, PGA Championship and BMW Championship.\nCollege athletics.\nThe following universities and colleges participate in the National Collegiate Athletic Association Division\u00a0I. "}
{"id": "5400", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5400", "title": "Countries of the World", "text": ""}
{"id": "5401", "revid": "45172657", "url": "https://en.wikipedia.org/wiki?curid=5401", "title": "Carboniferous", "text": "The Carboniferous ( ) is a geologic period and system of the Paleozoic era that spans 60 million years from the end of the Devonian Period Ma (million years ago) to the beginning of the Permian Period, Ma. It is the fifth and penultimate period of the Paleozoic era and the fifth period of the Phanerozoic eon. In North America, the Carboniferous is often treated as two separate geological periods, the earlier Mississippian and the later Pennsylvanian.\nThe name \"Carboniferous\" means \"coal-bearing\", from the Latin (\"coal\") and (\"bear, carry\"), and refers to the many coal beds formed globally during that time. The first of the modern \"system\" names, it was coined by geologists William Conybeare and William Phillips in 1822, based on a study of the British rock succession.\nCarboniferous is the period during which both terrestrial animal and land plant life was well established. Stegocephalia (four-limbed vertebrates including true tetrapods), whose forerunners (tetrapodomorphs) had evolved from lobe-finned fish during the preceding Devonian period, became pentadactylous during the Carboniferous. The period is sometimes called the Age of Amphibians because of the diversification of early amphibians such as the temnospondyls, which became dominant land vertebrates, as well as the first appearance of amniotes including synapsids (the clade to which modern mammals belong) and sauropsids (which include modern reptiles and birds) during the late Carboniferous. Land arthropods such as arachnids (e.g. trigonotarbids and \"Pulmonoscorpius\"), myriapods (e.g. \"Arthropleura\") and especially insects (particularly flying insects) also underwent a major evolutionary radiation during the late Carboniferous. Vast swaths of forests and swamps covered the land, which eventually became the coal beds characteristic of the Carboniferous stratigraphy evident today.\nThe later half of the period experienced glaciations, low sea level, and mountain building as the continents collided to form Pangaea. A minor marine and terrestrial extinction event, the Carboniferous rainforest collapse, occurred at the end of the period, caused by climate change. Atmospheric oxygen levels, originally thought to be consistently higher than today throughout the Carboniferous, have been shown to be more variable, increasing from low levels at the beginning of the Period to highs of 25\u201330%.\nEtymology and history.\nThe development of a Carboniferous chronostratigraphic timescale began in the late 18th century. The term \"Carboniferous\" was first used as an adjective by Irish geologist Richard Kirwan in 1799 and later used in a heading entitled \"Coal-measures or Carboniferous Strata\" by John Farey Sr. in 1811. Four units were originally ascribed to the Carboniferous, in ascending order, the Old Red Sandstone, Carboniferous Limestone, Millstone Grit and the Coal Measures. These four units were placed into a formalised Carboniferous unit by William Conybeare and William Phillips in 1822 and then into the Carboniferous System by Phillips in 1835. The Old Red Sandstone was later considered Devonian in age.\nThe similarity in successions between the British Isles and Western Europe led to the development of a common European timescale with the Carboniferous System divided into the lower Dinantian, dominated by carbonate deposition and the upper Silesian with mainly siliciclastic deposition. The Dinantian was divided into the Tournaisian and Vis\u00e9an stages. The Silesian was divided into the Namurian, Westphalian and Stephanian stages. The Tournaisian is the same length as the International Commission on Stratigraphy (ICS) stage, but the Vis\u00e9an is longer, extending into the lower Serpukhovian.\nNorth American geologists recognised a similar stratigraphy but divided it into two systems rather than one. These are the lower carbonate-rich sequence of the Mississippian System and the upper siliciclastic and coal-rich sequence of the Pennsylvanian. The United States Geological Survey officially recognised these two systems in 1953. In Russia, in the 1840s British and Russian geologists divided the Carboniferous into the Lower, Middle and Upper series based on Russian sequences. In the 1890s these became the Dinantian, Moscovian and Uralian stages. The Serpukivian was proposed as part of the Lower Carboniferous, and the Upper Carboniferous was divided into the Moscovian and Gzhelian. The Bashkirian was added in 1934.\nIn 1975, the ICS formally ratified the Carboniferous System, with the Mississippian and Pennsylvanian subsystems from the North American timescale, the Tournaisian and Visean stages from the Western European and the Serpukhovian, Bashkirian, Moscovian, Kasimovian and Gzhelian from the Russian. With the formal ratification of the Carboniferous System, the Dinantian, Silesian, Namurian, Westphalian and Stephanian became redundant terms, although the latter three are still in common use in Western Europe.\nGeology.\nStratigraphy.\nStages can be defined globally or regionally. For global stratigraphic correlation, the ICS ratify global stages based on a Global Boundary Stratotype Section and Point (GSSP) from a single formation (a stratotype) identifying the lower boundary of the stage. Only the boundaries of the Carboniferous System and three of the stage bases are defined by global stratotype sections and points because of the complexity of the geology. The ICS subdivisions from youngest to oldest are as follows:\nMississippian.\nThe Mississippian was proposed by Alexander Winchell in 1870 named after the extensive exposure of lower Carboniferous limestone in the upper Mississippi River valley. During the Mississippian, there was a marine connection between the Paleo-Tethys and Panthalassa through the Rheic Ocean resulting in the near worldwide distribution of marine faunas and so allowing widespread correlations using marine biostratigraphy. However, there are few Mississippian volcanic rocks, and so obtaining radiometric dates is difficult.\nThe Tournaisian Stage is named after the Belgian city of Tournai. It was introduced in scientific literature by Belgian geologist Andr\u00e9 Dumont in 1832. The GSSP for the base of the Carboniferous System, Mississippian Subsystem and Tournaisian Stage is located at the La Serre section in Montagne Noire, southern France. It is defined by the first appearance of the conodont \"Siphonodella sulcata\" within the evolutionary lineage from \"Siphonodella praesulcata\" to \"Siphonodella sulcata\". This was ratified by the ICS in 1990. However, in 2006 further study revealed the presence of \"Siphonodella sulcata\" below the boundary, and the presence of \"Siphonodella\" \"praesulcata\" and \"Siphonodella sulcata\" together above a local unconformity. This means the evolution of one species to the other, the definition of the boundary, is not seen at the La Serre site making precise correlation difficult.The Vis\u00e9an Stage was introduced by Andr\u00e9 Dumont in 1832 and is named after the city of Vis\u00e9, Li\u00e8ge Province, Belgium. In 1967, the base of the Visean was officially defined as the first black limestone in the Leffe facies at the Bastion Section in the Dinant Basin. These changes are now thought to be ecologically driven rather than caused by evolutionary change, and so this has not been used as the location for the GSSP. Instead, the GSSP for the base of the Visean is located in Bed 83 of the sequence of dark grey limestones and shales at the Pengchong section, Guangxi, southern China. It is defined by the first appearance of the fusulinid \"Eoparastaffella simplex\" in the evolutionary lineage \"Eoparastaffella ovalis \u2013 Eoparastaffella simplex\" and was ratified in 2009.\nThe Serpukhovian Stage was proposed in 1890 by Russian stratigrapher Sergei Nikitin. It is named after the city of Serpukhov, near Moscow and currently lacks a defined GSSP. The Visean-Serpukhovian boundary coincides with a major period of glaciation. The resulting sea level fall and climatic changes led to the loss of connections between marine basins and endemism of marine fauna across the Russian margin. This means changes in biota are environmental rather than evolutionary making wider correlation difficult. Work is underway in the Urals and Nashui, Guizhou Province, southwestern China for a suitable site for the GSSP with the proposed definition for the base of the Serpukhovian as the first appearance of conodont \"Lochriea ziegleri.\"\nPennsylvanian.\nThe Pennsylvanian was proposed by J.J.Stevenson in 1888, named after the widespread coal-rich strata found across the state of Pennsylvania. The closure of the Rheic Ocean and formation of Pangea during the Pennsylvanian, together with widespread glaciation across Gondwana led to major climate and sea level changes, which restricted marine fauna to particular geographic areas thereby reducing widespread biostratigraphic correlations. Extensive volcanic events associated with the assembling of Pangea means more radiometric dating is possible relative to the Mississippian.\nThe Bashkirian Stage was proposed by Russian stratigrapher Sofia Semikhatova in 1934. It was named after Bashkiria, the then Russian name of the republic of Bashkortostan in the southern Ural Mountains of Russia. The GSSP for the base of the Pennsylvanian Subsystem and Bashkirian Stage is located at Arrow Canyon in Nevada, US and was ratified in 1996. It is defined by the first appearance of the conodont \"Declinognathodus noduliferus\". Arrow Canyon lay in a shallow, tropical seaway which stretched from Southern California to Alaska. The boundary is within a cyclothem sequence of transgressive limestones and fine sandstones, and regressive mudstones and brecciated limestones.\nThe Moscovian Stage is named after shallow marine limestones and colourful clays found around Moscow, Russia. It was first introduced by Sergei Nikitin in 1890. The Moscovian currently lacks a defined GSSP. The fusulinid \"Aljutovella aljutovica\" can be used to define the base of the Moscovian across the northern and eastern margins of Pangea, however, it is restricted in geographic area, which means it cannot be used for global correlations. The first appearance of the conodonts \"Declinognathodus donetzianus\" or \"Idiognathoides postsulcatus\" have been proposed as a boundary marking species and potential sites in the Urals and Nashui, Guizhou Province, southwestern China are being considered.\nThe Kasimovian is the first stage in the Upper Pennsylvanian. It is named after the Russian city of Kasimov, and was originally included as part of Nikitin's 1890 definition of the Moscovian. It was first recognised as a distinct unit by A.P. Ivanov in 1926, who named it the \"Tiguliferina\" Horizon after a type of brachiopod. The boundary of the Kasimovian covers a period of globally low sea level, which has resulted in disconformities within many sequences of this age. This has created difficulties in finding suitable marine fauna that can used to correlate boundaries worldwide. The Kasimovian currently lacks a defined GSSP; potential sites in the southern Urals, southwest USA and Nashui, Guizhou Province, southwestern China are being considered.\nThe Gzhelian is named after the Russian village of Gzhel, near Ramenskoye, not far from Moscow. The name and type locality were defined by Sergei Nikitin in 1890. The Gzhelian currently lacks a defined GSSP. The first appearance of the fusulinid \"Rauserites rossicus\" and \"Rauserites\" \"stuckenbergi\" can be used in the Boreal Sea and Paleo-Tethyan regions but not eastern Pangea or Panthalassa margins. Potential sites in the Urals and Nashui, Guizhou Province, southwestern China for the GSSP are being considered.\nThe GSSP for the base of the Permian is located in the Aidaralash River valley near Aqt\u00f6be, Kazakhstan and was ratified in 1996. The beginning of the stage is defined by the first appearance of the conodont \"Streptognathodus postfusus.\"\nCyclothems.\nA cyclothem is a succession of non-marine and marine sedimentary rocks, deposited during a single sedimentary cycle, with an erosional surface at its base. Whilst individual cyclothems are often only metres to a few tens of metres thick, cyclothem sequences can be many hundreds to thousands of metres thick and contain tens to hundreds of individual cyclothems. Cyclothems were deposited along continental shelves where the very gentle gradient of the shelves meant even small changes in sea level led to large advances or retreats of the sea. Cyclothem lithologies vary from mudrock and carbonate-dominated to coarse siliciclastic sediment-dominated sequences depending on the paleo-topography, climate and supply of sediments to the shelf.\nThe main period of cyclothem deposition occurred during the Late Paleozoic Ice Age from the Late Mississippian to early Permian, when the waxing and waning of ice sheets led to rapid changes in eustatic sea level. The growth of ice sheets led global sea levels to fall as water was locked away in glaciers. Falling sea levels exposed large tracts of the continental shelves across which river systems eroded channels and valleys and vegetation broke down the surface to form soils. The non-marine sediments deposited on this erosional surface form the base of the cyclothem. As sea levels began to rise, the rivers flowed through increasingly water-logged landscapes of swamps and lakes. Peat mires developed in these wet and oxygen-poor conditions, leading to coal formation. With continuing sea level rise, coastlines migrated landward and deltas, lagoons and esturaries developed; their sediments deposited over the peat mires. As fully marine conditions were established, limestones succeeded these marginal marine deposits. The limestones were in turn overlain by deep water black shales as maximum sea levels were reached.\nIdeally, this sequence would be reversed as sea levels began to fall again; however, sea level falls tend to be protracted, whilst sea level rises are rapid, ice sheets grow slowly but melt quickly. Therefore, the majority of a cyclothem sequence occurred during falling sea levels, when rates of erosion were high, meaning they were often periods of non-deposition. Erosion during sea level falls could also result in the full or partial removal of previous cyclothem sequences. Individual cyclothems are generally less than 10 m thick because the speed at which sea level rose gave only limited time for sediments to accumulate.\nDuring the Pennsylvanian, cyclothems were deposited in shallow, epicontinental seas across the tropical regions of Laurussia (present day western and central US, Europe, Russia and central Asia) and the North and South China cratons. The rapid sea levels fluctuations they represent correlate with the glacial cycles of the Late Paleozoic Ice Age. The advance and retreat of ice sheets across Gondwana followed a 100 kyr Milankovitch cycle, and so each cyclothem represents a cycle of sea level fall and rise over a 100 kyr period.\nCoal formation.\nCoal forms when organic matter builds up in waterlogged, anoxic swamps, known as peat mires, and is then buried, compressing the peat into coal. The majority of Earth's coal deposits were formed during the late Carboniferous and early Permian. The plants from which they formed contributed to changes in the Carboniferous Earth's atmosphere.\nDuring the Pennsylvanian, vast amounts of organic debris accumulated in the peat mires that formed across the low-lying, humid equatorial wetlands of the foreland basins of the Central Pangean Mountains in Laurussia, and around the margins of the North and South China cratons. During glacial periods, low sea levels exposed large areas of the continental shelves. Major river channels, up to several kilometres wide, stretched across these shelves feeding a network of smaller channels, lakes and peat mires. These wetlands were then buried by sediment as sea levels rose during interglacials. Continued crustal subsidence of the foreland basins and continental margins allowed this accumulation and burial of peat deposits to continue over millions of years resulting in the formation of thick and widespread coal formations. During the warm interglacials, smaller coal swamps with plants adapted to the temperate conditions formed on the Siberian craton and the western Australian region of Gondwana.\nThere is ongoing debate as to why this peak in the formation of Earth's coal deposits occurred during the Carboniferous. The first theory, known as the delayed fungal evolution hypothesis, is that a delay between the development of trees with the wood fibre lignin and the subsequent evolution of lignin-degrading fungi gave a period of time where vast amounts of lignin-based organic material could accumulate. Genetic analysis of basidiomycete fungi, which have enzymes capable of breaking down lignin, supports this theory by suggesting this fungi evolved in the Permian. However, significant Mesozoic and Cenozoic coal deposits formed after lignin-digesting fungi had become well established, and fungal degradation of lignin may have already evolved by the end of the Devonian, even if the specific enzymes used by basidiomycetes had not. The second theory is that the geographical setting and climate of the Carboniferous were unique in Earth's history: the co-occurrence of the position of the continents across the humid equatorial zone, high biological productivity, and the low-lying, water-logged and slowly subsiding sedimentary basins that allowed the thick accumulation of peat were sufficient to account for the peak in coal formation.\nPalaeogeography.\nDuring the Carboniferous, there was an increased rate in tectonic plate movements as the supercontinent Pangea assembled. The continents themselves formed a near circle around the opening Paleo-Tethys Ocean, with the massive Panthalassic Ocean beyond. Gondwana covered the south polar region. To its northwest was Laurussia. These two continents slowly collided to form the core of Pangea. To the north of Laurussia lay Siberia and Amuria. To the east of Siberia, Kazakhstania, North China and South China formed the northern margin of the Paleo-Tethys, with Annamia laying to the south.\nVariscan-Alleghanian-Ouachita orogeny.\nThe Central Pangean Mountains were formed during the Variscan-Alleghanian-Ouachita orogeny. Today their remains stretch over 10,000\u00a0km from the Gulf of Mexico in the west to Turkey in the east. The orogeny was caused by a series of continental collisions between Laurussia, Gondwana and the Armorican terrane assemblage (much of modern-day Central and Western Europe including Iberia) as the Rheic Ocean closed and Pangea formed. This mountain building process began in the Middle Devonian and continued into the early Permian.\nThe Armorican terranes rifted away from Gondwana during the Late Ordovician. As they drifted northwards the Rheic Ocean closed in front of them, and they began to collide with southeastern Laurussia in the Middle Devonian. The resulting Variscan orogeny involved a complex series of oblique collisions with associated metamorphism, igneous activity, and large-scale deformation between these terranes and Laurussia, which continued into the Carboniferous.\nDuring the mid Carboniferous, the South American sector of Gondwana collided obliquely with Laurussia's southern margin resulting in the Ouachita orogeny. The major strike-slip faulting that occurred between Laurussia and Gondwana extended eastwards into the Appalachian Mountains where early deformation in the Alleghanian orogeny was predominantly strike-slip. As the West African sector of Gondwana collided with Laurussia during the Late Pennsylvanian, deformation along the Alleghanian orogen became northwesterly-directed compression.\nUralian orogeny.\nThe Uralian orogeny is a north\u2013south trending fold and thrust belt that forms the western edge of the Central Asian Orogenic Belt. The Uralian orogeny began in the Late Devonian and continued, with some hiatuses, into the Jurassic. From the Late Devonian to early Carboniferous, the Magnitogorsk island arc, which lay between Kazakhstania and Laurussia in the Ural Ocean, collided with the passive margin of northeastern Laurussia (Baltica craton). The suture zone between the former island arc complex and the continental margin formed the Main Uralian Fault, a major structure that runs for more than 2,000\u00a0km along the orogen. Accretion of the island arc was complete by the Tournaisian, but subduction of the Ural Ocean between Kazakhstania and Laurussia continued until the Bashkirian when the ocean finally closed and continental collision began. Significant strike-slip movement along this zone indicates the collision was oblique. Deformation continued into the Permian and during the late Carboniferous and Permian the region was extensively intruded by granites.\nLaurussia.\nThe Laurussian continent was formed by the collision between Laurentia, Baltica and Avalonia during the Devonian. At the beginning of the Carboniferous, some models show it at the equator, while others place it further south. In either case, the continent drifted northwards, reaching low latitudes in the northern hemisphere by the end of the Period. The Central Pangean Mountain drew in moist air from the Paleo-Tethys Ocean resulting in heavy precipitation and a tropical wetland environment. Extensive coal deposits developed within the cyclothem sequences that dominated the Pennsylvanian sedimentary basins associated with the growing orogenic belt.\nSubduction of the Panthalassic oceanic plate along its western margin resulted in the Antler orogeny in the Late Devonian to Early Mississippian. Further north along the margin, slab roll-back, beginning in the Early Mississippian, led to the rifting of the Yukon\u2013Tanana terrane and the opening of the Slide Mountain Ocean. Along the northern margin of Laurussia, orogenic collapse of the Late Devonian to Early Mississippian Innuitian orogeny led to the development of the Sverdrup Basin.\nGondwana.\nMuch of Gondwana lay in the southern polar region during the Carboniferous. As the plate moved, the South Pole drifted from southern Africa in the early Carboniferous to eastern Antarctica by the end of the period. Glacial deposits are widespread across Gondwana and indicate multiple ice centres and long-distance movement of ice. The northern to northeastern margin of Gondwana (northeast Africa, Arabia, India and northeastern West Australia) was a passive margin along the southern edge of the Paleo-Tethys with cyclothem deposition including, during more temperate intervals, coal swamps in Western Australia. The Mexican terranes along the northwestern Gondwana margin, were affected by the subduction of the Rheic Ocean. However, they lay to west of the Ouachita orogeny and were not impacted by continental collision but became part of the active margin of the Pacific. The Moroccan margin was affected by periods of widespread dextral strike-slip deformation, magmatism and metamorphism associated with the Variscan orogeny.\nTowards the end of the Carboniferous, extension and rifting across the northern margin of Gondwana led to the breaking away of the Cimmerian terrane during the early Permian and the opening of the Neo-Tethys Ocean. Along the southeastern and southern margin of Gondwana (eastern Australia and Antarctica), northward subduction of Panthalassa continued. Changes in the relative motion of the plates resulted in the early Carboniferous Kanimblan Orogeny. Continental arc magmatism continued into the late Carboniferous and extended round to connect with the developing proto-Andean subduction zone along the western South American margin of Gondwana.\nSiberia and Amuria.\nShallow seas covered much of the Siberian craton in the early Carboniferous. These retreated as sea levels fell in the Pennsylvanian and as the continent drifted north into more temperate zones extensive coal deposits formed in the Kuznetsk Basin. The northwest to eastern margins of Siberia were passive margins along the Mongol-Okhotsk Ocean on the far side of which lay Amuria. From the mid Carboniferous, subduction zones with associated magmatic arcs developed along both margins of the ocean.\nThe southwestern margin of Siberia was the site of a long lasting and complex accretionary orogen. The Devonian to early Carboniferous Siberian and South Chinese Altai accretionary complexes developed above an east-dipping subduction zone, whilst further south, the Zharma-Saur arc formed along the northeastern margin of Kazakhstania. By the late Carboniferous, all these complexes had accreted to the Siberian craton as shown by the intrusion of post-orogenic granites across the region. As Kazakhstania had already accreted to Laurussia, Siberia was effectively part of Pangea by 310 Ma, although major strike-slip movements continued between it and Laurussia into the Permian.\nCentral and East Asia.\nThe Kazakhstanian microcontinent is composed of a series of Devonian and older accretionary complexes. It was strongly deformed during the Carboniferous as its western margin collided with Laurussia during the Uralian orogen and its northeastern margin collided with Siberia. Continuing strike-slip motion between Laurussia and Siberia led the formerly elongate microcontinent to bend into an orocline.\nDuring the Carboniferous, the Tarim craton lay along the northwestern edge of North China. Subduction along the Kazakhstanian margin of the Turkestan Ocean resulted in collision between northern Tarim and Kazakhstania during the mid Carboniferous as the ocean closed. The South Tian Shan fold and thrust belt, which extends over 2,000\u00a0km from Uzbekistan to northwest China, is the remains of this accretionary complex and forms the suture between Kazakhstania and Tarim. A continental magmatic arc above a south-dipping subduction zone lay along the northern North China margin, consuming the Paleoasian Ocean. Northward subduction of the Paleo-Tethys beneath the southern margins of North China and Tarim continued during the Carboniferous, with the South Qinling block accreted to North China during the mid to late Carboniferous. No sediments are preserved from the early Carboniferous in North China. However, bauxite deposits immediately above the regional mid Carboniferous unconformity indicate warm tropical conditions and are overlain by cyclothems including extensive coals.\nSouth China and Annamia (Southeast Asia) rifted from Gondwana during the Devonian. During the Carboniferous, they were separated from each other and North China by the Paleoasian Ocean with the Paleo-Tethys to the southwest and Panthalassa to the northeast. Cyclothem sediments with coal and evaporites were deposited across the passive margins that surrounded both continents.\nClimate.\nThe Carboniferous climate was dominated by the Late Paleozoic Ice Age (LPIA), the most extensive and longest icehouse period of the Phanerozoic, which lasted from the Late Devonian to the Permian (365 Ma-253 Ma). Temperatures began to drop during the late Devonian with a short-lived glaciation in the late Famennian through Devonian\u2013Carboniferous boundary, before the Early Tournaisian Warm Interval. Following this, a reduction in atmospheric CO2 levels, caused by the increased burial of organic matter and widespread ocean anoxia led to climate cooling and glaciation across the south polar region. During the Visean Warm Interval glaciers nearly vanished retreating to the proto-Andes in Bolivia and western Argentina and the Pan-African mountain ranges in southeastern Brazil and southwest Africa.\nThe main phase of the LPIA (c. 335\u2013290 Ma) began in the late Visean, as the climate cooled and atmospheric CO2 levels dropped. Its onset was accompanied by a global fall in sea level and widespread multimillion-year unconformities. This main phase consisted of a series of discrete several million-year-long glacial periods during which ice expanded out from up to 30 ice centres that stretched across mid- to high latitudes of Gondwana in eastern Australia, northwestern Argentina, southern Brazil, and central and Southern Africa.\nIsotope records indicate this drop in CO2 levels was triggered by tectonic factors with increased weathering of the growing Central Pangean Mountains and the influence of the mountains on precipitation and surface water flow. Closure of the oceanic gateway between the Rheic and Tethys oceans in the early Bashkirian also contributed to climate cooling by changing ocean circulation and heat flow patterns.\nWarmer periods with reduced ice volume within the Bashkirian, the late Moscovian and the latest Kasimovian to mid-Gzhelian are inferred from the disappearance of glacial sediments, the appearance of deglaciation deposits and rises in sea levels.\nIn the early Kasimovian there was short-lived (&lt;1 million years) intense period of glaciation, with atmospheric CO2 concentration levels dropping as low as 180 ppm. This ended suddenly as a rapid increase in CO2 concentrations to c. 600 ppm resulted in a warmer climate. This rapid rise in CO2 may have been due to a peak in pyroclastic volcanism and/or a reduction in burial of terrestrial organic matter.\nThe LPIA peaked across the Carboniferous-Permian boundary. Widespread glacial deposits are found across South America, western and central Africa, Antarctica, Australia, Tasmania, the Arabian Peninsula, India, and the Cimmerian blocks, indicating trans-continental ice sheets across southern Gondwana that reached to sea-level. In response to the uplift and erosion of the more mafic basement rocks of the Central Pangea Mountains at this time, CO2 levels dropped as low as 175 ppm and remained under 400 ppm for 10 Ma.\nTemperatures.\nTemperatures across the Carboniferous reflect the phases of the LPIA. At the extremes, during the Permo-Carboniferous Glacial Maximum (299\u2013293 Ma) the global average temperature (GAT) was c. 13\u00a0\u00b0C (55\u00a0\u00b0F), the average temperature in the tropics c. 24\u00a0\u00b0C (75\u00a0\u00b0F) and in polar regions c. -23\u00a0\u00b0C (-10\u00a0\u00b0F), whilst during the Early Tournaisian Warm Interval (358\u2013353 Ma) the GAT was c. 22\u00a0\u00b0C (72\u00a0\u00b0F), the tropics c. 30\u00a0\u00b0C (86\u00a0\u00b0F) and polar regions c. 1.5\u00a0\u00b0C (35\u00a0\u00b0F). Overall, for the Ice Age the GAT was c. 17\u00a0\u00b0C (62\u00a0\u00b0F), with tropical temperatures c. 26\u00a0\u00b0C and polar temperatures c. -9.0\u00a0\u00b0C (16\u00a0\u00b0F).\nAtmospheric oxygen levels.\nThere are a variety of methods for reconstructing past atmospheric oxygen levels, including the charcoal record, halite gas inclusions, burial rates of organic carbon and pyrite, carbon isotopes of organic material, isotope mass balance and forward modelling. Depending on the preservation of source material, some techniques represent moments in time (e.g. halite gas inclusions), whilst others have a wider time range (e.g. the charcoal record and pyrite). Results from these different methods for the Carboniferous vary. For example: the increasing occurrence of charcoal produced by wildfires from the Late Devonian into the Carboniferous indicates increasing oxygen levels, with calculations showing oxygen levels above 21% for most of the Carboniferous; halite gas inclusions from sediments dated 337\u2013335 Ma give estimates for the Visean of c. 15.3%, although with large uncertainties; and, pyrite records suggest levels of c. 15% early in the Carboniferous, to over 25% during the Pennsylvanian, before dropping back below 20% towards the end. However, whilst exact numbers vary, all models show an overall increase in atmospheric oxygen levels from a low of between 15\u201320% at the beginning of the Carboniferous to highs of 25\u201330% during the Period. This was not a steady rise, but included peaks and troughs reflecting the dynamic climate conditions of the time. How the atmospheric oxygen concentrations influenced the large body size of arthropods and other fauna and flora during the Carboniferous is also a subject of ongoing debate.\nEffects of climate on sedimentation.\nThe changing climate was reflected in regional-scale changes in sedimentation patterns. In the relatively warm waters of the Early to Middle Mississippian, carbonate production occurred to depth across the gently dipping continental slopes of Laurussia and North and South China (carbonate ramp architecture) and evaporites formed around the coastal regions of Laurussia, Kazakhstania, and northern Gondwana.\nFrom the late Visean, the cooling climate restricted carbonate production to depths of less than c. 10 m forming carbonate shelves with flat-tops and steep sides. By the Moscovian, the waxing and waning of the ice sheets led to cyclothem deposition with mixed carbonate-siliciclastic sequences deposited on continental platforms and shelves.\nSeasonal melting of glaciers resulted in near freezing waters around the margins of Gondwana. This is evidenced by the occurrence of glendonite (a pseudomorph of ikaite; a form of calcite deposited in glacial waters) in fine-grained, shallow marine sediments.\nThe glacial grinding and erosion of siliciclastic rocks across Gondwana and the Central Pangaean Mountains produced vast amounts of silt-sized sediment. Redistributed by the wind, this formed widespread deposits of loess across equatorial Pangea.\nEffects of climate on biodiversity.\nThe main phase of the LPIA was considered a crisis for marine biodiversity with the loss of many genera, followed by low biodiversity. However, recent studies of marine life suggest the rapid climate and environmental changes that accompanied the onset of the main glacial phase resulted in an adaptive radiation with a rapid increase in the number of species.\nThe oscillating climate conditions also led to repeated restructuring of Laurasian tropical forests between wetlands and seasonally dry ecosystems, and the appearance and diversification of tetrapods species. There was a major restructuring of wetland forests during the Kasimovian glacial interval, with the loss of arborescent (tree-like) lycopisids and other wetland groups, and a general decline in biodiversity. These events are attributed to the drop in CO2 levels below 400 ppm. Although referred to as the Carboniferous rainforest collapse, this was a complex replacement of one type of rainforest by another, not a complete disappearance of rainforest vegetation.\nAcross the Carboniferous\u2013Permian boundary interval, a rapid drop in CO2 levels and increasingly arid conditions at low-latitudes led to a permanent shift to seasonally dry woodland vegetation. Tetrapods acquired new terrestrial adaptations and there was a radiation of dryland-adapted amniotes.\nGeochemistry.\nAs the continents assembled to form Pangea, the growth of the Central Pangean Mountains led to increased weathering and carbonate sedimentation on the ocean floor, whilst the distribution of continents across the paleo-tropics meant vast areas of land were available for the spread of tropical rainforests. Together these two factors significantly increased CO2 drawdown from the atmosphere, lowering global temperatures, increasing ocean pH and triggering the Late Paleozoic Ice Age. The growth of the supercontinent also changed seafloor spreading rates and led to a decrease in the length and volume of mid-ocean ridge systems.\nMagnesium/calcium isotope ratios in seawater.\nDuring the early Carboniferous, the Mg2+/Ca2+ ratio in seawater began to rise and by the Middle Mississippian aragonite seas had replaced calcite seas. The concentration of calcium in seawater is largely controlled by ocean pH, and as this increased the calcium concentration was reduced. At the same time, the increase in weathering, increased the amount of magnesium entering the marine environment. As magnesium is removed from seawater and calcium added along mid-ocean ridges where seawater reacts with the newly formed lithosphere, the reduction in length of mid-ocean ridge systems increased the Mg2+/Ca2+ ratio further. The Mg2+/Ca2+ ratio of the seas also affects the ability of organisms to biomineralize. The Carboniferous aragonite seas favoured those that secreted aragonite and the dominant reef builders of the time were aragonitic sponges and corals.\nStrontium isotopic composition of seawater.\nThe strontium isotopic composition (87Sr/86Sr) of seawater represents a mix of strontium derived from continental weathering which is rich in 87Sr and from mantle sources e.g. mid-ocean ridges, which are relatively depleted in 87Sr. 87Sr/86Sr ratios above 0.7075 indicate continental weathering is the main source of 87Sr, whilst ratios below indicate mantle-derived sources are the principal contributor.\n87Sr/86Sr values varied through the Carboniferous, although they remained above 0.775, indicating continental weathering dominated as the source of 87Sr throughout. The 87Sr/86Sr during the Tournaisian was c. 0.70840, it decreased through the Visean to 0.70771 before increasing during the Serpukhovian to the lowermost Gzhelian where it plateaued at 0.70827, before decreasing again to 0.70814 at the Carboniferous-Permian boundary. These variations reflect the changing influence of weathering and sediment supply to the oceans of the growing Central Pangean Mountains. By the Serpukhovian basement rocks, such as granite, had been uplifted and exposed to weathering. The decline towards the end of the Carboniferous is interpreted as a decrease in continental weathering due to the more arid conditions.\nOxygen and carbon isotope ratios in seawater.\nUnlike Mg2+/Ca2+ and 87Sr/86Sr isotope ratios, which are consistent across the world's oceans at any one time, \u03b418O and \u03b413C preserved in the fossil record can be affected by regional factors. Carboniferous \u03b418O and \u03b413C records show regional differences between the South China open-water setting and the epicontinental seas of Laurussia. These differences are due to variations in seawater salinity and evaporation between epicontinental seas relative to the more open waters. However, large scale trends can still be determined. \u03b413C rose rapidly from c. 0 to 1\u2030 (parts per thousand) to c. 5 to 7\u2030 in the Early Mississippian and remained high for the duration of the Late Paleozoic Ice Age (c. 3\u20136\u2030) into the early Permian. Similarly from the Early Mississippian there was a long-term increase in \u03b418O values as the climate cooled.\nBoth \u03b413C and \u03b418O records show significant global isotope changes (known as excursions) during the Carboniferous. The mid-Tournaisian positive \u03b413C and \u03b418O excursions lasted between 6 and 10 million years and were also accompanied by c. 6\u2030 positive excursion in organic matter \u03b415N values, a negative excursion in carbonate \u03b4238U and a positive excursion in carbonate-associated sulphate \u03b434S. These changes in seawater geochemistry are interpreted as a decrease in atmospheric CO2 due to increased organic matter burial and widespread ocean anoxia triggering climate cooling and onset of glaciation.\nThe Mississippian-Pennsylvanian boundary positive \u03b418O excursion occurred at the same time as global sea level falls and widespread glacial deposits across southern Gondwana, indicating climate cooling and ice build-up. The rise in 87Sr/86Sr just before the \u03b418O excursion suggests climate cooling in this case was caused by increased continental weathering of the growing Central Pangean Mountains and the influence of the orogeny on precipitation and surface water flow rather than increased burial of organic matter. \u03b413C values show more regional variation, and it is unclear whether there is a positive \u03b413C excursion or a readjustment from previous lower values.\nDuring the early Kasimovian there was a short (&lt;1myr), intense glacial period, which came to a sudden end as atmospheric CO2 concentrations rapidly rose. There was a steady increase in arid conditions across tropical regions and a major reduction in the extent of tropical rainforests, as shown by the widespread loss of coal deposits from this time. The resulting reduction in productivity and burial of organic matter led to increasing atmospheric CO2 levels, which were recorded by a negative \u03b413C excursion and an accompanying, but smaller decrease in \u03b418O values.\nLife.\nPlants.\nEarly Carboniferous land plants, some of which were preserved in coal balls, were very similar to those of the preceding Late Devonian, but new groups also appeared at this time. The main early Carboniferous plants were the Equisetales (horse-tails), Sphenophyllales (scrambling plants), Lycopodiales (club mosses), Lepidodendrales (scale trees), Filicales (ferns), Medullosales (informally included in the \"seed ferns\", an assemblage of a number of early gymnosperm groups) and the Cordaitales. These continued to dominate throughout the period, but during the late Carboniferous, several other groups, Cycadophyta (cycads), the Callistophytales (another group of \"seed ferns\"), and the Voltziales, appeared.\nThe Carboniferous lycophytes of the order Lepidodendrales, which are cousins (but not ancestors) of the tiny club-moss of today, were huge trees with trunks 30 meters high and up to 1.5 meters in diameter. These included \"Lepidodendron\" (with its cone called Lepidostrobus), \"Anabathra\", \"Lepidophloios\" and \"Sigillaria\". The roots of several of these forms are known as Stigmaria. Unlike present-day trees, their secondary growth took place in the cortex, which also provided stability, instead of the xylem. The Cladoxylopsids were large trees, that were ancestors of ferns, first arising in the Carboniferous.\nThe fronds of some Carboniferous ferns are almost identical with those of living species. Probably many species were epiphytic. Fossil ferns and \"seed ferns\" include \"Pecopteris\", \"Cyclopteris\", \"Neuropteris\", \"Alethopteris\", and \"Sphenopteris\"; \"Megaphyton\" and \"Caulopteris\" were tree ferns.\nThe Equisetales included the common giant form \"Calamites\", with a trunk diameter of 30 to and a height of up to . \"Sphenophyllum\" was a slender climbing plant with whorls of leaves, which was probably related both to the calamites and the lycopods.\n\"Cordaites\", a tall plant (6 to over 30 meters) with strap-like leaves, was related to the cycads and conifers; the catkin-like reproductive organs, which bore ovules/seeds, is called \"Cardiocarpus\". These plants were thought to live in swamps. True coniferous trees (\"Walchia\", of the order Voltziales) appear later in the Carboniferous, and preferred higher drier ground.\nMarine invertebrates.\nIn the oceans the marine invertebrate groups are the Foraminifera, corals, Bryozoa, Ostracoda, brachiopods, ammonoids, hederelloids, microconchids and echinoderms (especially crinoids). The diversity of brachiopods and fusilinid foraminiferans, surged beginning in the Visean, continuing through the end of the Carboniferous, although cephalopod and nektonic conodont diversity declined. This evolutionary radiation was known as the Carboniferous-Earliest Permian Biodiversification Event. For the first time foraminifera took a prominent part in the marine faunas. The large spindle-shaped genus Fusulina and its relatives were abundant in what is now Russia, China, Japan, North America; other important genera include \"Valvulina\", \"Endothyra\", \"Archaediscus\", and \"Saccammina\" (the latter common in Britain and Belgium). Some Carboniferous genera are still extant. The first true priapulids appeared during this period.\nThe microscopic shells of radiolarians are found in cherts of this age in the Culm of Devon and Cornwall, and in Russia, Germany and elsewhere. Sponges are known from spicules and anchor ropes, and include various forms such as the Calcispongea \"Cotyliscus\" and \"Girtycoelia\", the demosponge \"Chaetetes\", and the genus of unusual colonial glass sponges \"Titusvillia\". Both reef-building and solitary corals diversify and flourish; these include both rugose (for example, \"Caninia\", \"Corwenia\", \"Neozaphrentis\"), heterocorals, and tabulate (for example, \"Chladochonus\", \"Michelinia\") forms. Conularids were well represented by \"Conularia\"\nBryozoa are abundant in some regions; the fenestellids including \"Fenestella\", \"Polypora\", and \"Archimedes\", so named because it is in the shape of an Archimedean screw. Brachiopods are also abundant; they include productids, some of which reached very large for brachiopods size and had very thick shells (for example, the -wide \"Gigantoproductus\"), while others like \"Chonetes\" were more conservative in form. Athyridids, spiriferids, rhynchonellids, and terebratulids are also very common. Inarticulate forms include \"Discina\" and \"Crania\". Some species and genera had a very wide distribution with only minor variations.\nAnnelids such as \"Serpulites\" are common fossils in some horizons. Among the mollusca, the bivalves continue to increase in numbers and importance. Typical genera include \"Aviculopecten\", \"Posidonomya\", \"Nucula\", \"Carbonicola\", \"Edmondia\", and \"Modiola\". Gastropods are also numerous, including the genera \"Murchisonia\", \"Euomphalus\", \"Naticopsis\". Nautiloid cephalopods are represented by tightly coiled nautilids, with straight-shelled and curved-shelled forms becoming increasingly rare. Goniatite ammonoids such as Aenigmatoceras are common.\nTrilobites are rarer than in previous periods, on a steady trend towards extinction, represented only by the proetid group. Ostracoda, a class of crustaceans, were abundant as representatives of the meiobenthos; genera included \"Amphissites\", \"Bairdia\", \"Beyrichiopsis\", \"Cavellina\", \"Coryellina\", \"Cribroconcha\", \"Hollinella\", \"Kirkbya\", \"Knoxiella\", and \"Libumella\". Crinoids were highly numerous during the Carboniferous, though they suffered a gradual decline in diversity during the Middle Mississippian. Dense submarine thickets of long-stemmed crinoids appear to have flourished in shallow seas, and their remains were consolidated into thick beds of rock. Prominent genera include \"Cyathocrinus\", \"Woodocrinus\", and \"Actinocrinus\". Echinoids such as \"Archaeocidaris\" and \"Palaeechinus\" were also present. The blastoids, which included the Pentreinitidae and Codasteridae and superficially resembled crinoids in the possession of long stalks attached to the seabed, attain their maximum development at this time.\nFreshwater and lagoonal invertebrates.\nFreshwater Carboniferous invertebrates include various bivalve molluscs that lived in brackish or fresh water, such as \"Anthraconaia\", \"Naiadites\", and \"Carbonicola\"; diverse crustaceans such as \"Candona\", \"Carbonita\", \"Darwinula\", \"Estheria\", \"Acanthocaris\", \"Dithyrocaris\", and \"Anthrapalaemon\". The eurypterids were also diverse, and are represented by such genera as \"Adelophthalmus\", \"Megarachne\" (originally misinterpreted as a giant spider, hence its name) and the specialised very large \"Hibbertopterus\". Many of these were amphibious. Frequently a temporary return of marine conditions resulted in marine or brackish water genera such as \"Lingula\", \"Orbiculoidea\", and \"Productus\" being found in the thin beds known as marine bands.\nTerrestrial invertebrates.\nFossil remains of air-breathing insects, myriapods, and arachnids are known from the Carboniferous. Their diversity when they do appear, however, shows that these arthropods were both well-developed and numerous. Some arthropods grew to large sizes with the up to millipede-like \"Arthropleura\" being the largest-known land invertebrate of all time. Among the insect groups are the huge predatory Protodonata (griffinflies), among which was \"Meganeura\", a giant dragonfly-like insect and with a wingspan of ca. \u2014the largest flying insect ever to roam the planet. Further groups are the Syntonopterodea (relatives of present-day mayflies), the abundant and often large sap-sucking Palaeodictyopteroidea, the diverse herbivorous Protorthoptera, and numerous basal Dictyoptera (ancestors of cockroaches).\nMany insects have been obtained from the coalfields of Saarbr\u00fccken and Commentry, and from the hollow trunks of fossil trees in Nova Scotia. Some British coalfields have yielded good specimens: \"Archaeoptilus\", from the Derbyshire coalfield, had a large wing with preserved part, and some specimens (\"Brodia\") still exhibit traces of brilliant wing colors. In the Nova Scotian tree trunks land snails (\"Archaeozonites\", \"Dendropupa\") have been found.\nFish.\nMany fish inhabited the Carboniferous seas; predominantly Elasmobranchs (sharks and their relatives). These included some, like \"Psammodus\", with crushing pavement-like teeth adapted for grinding the shells of brachiopods, crustaceans, and other marine organisms. Other groups of elasmobranchs, like the ctenacanthiformes grew to large sizes, with some genera like \"Saivodus\" reaching around 6\u20139 meters (20\u201330 feet). Other fish had piercing teeth, such as the Symmoriida; some, the petalodonts, had peculiar cycloid cutting teeth. Most of the other cartilaginous fish were marine, but others like the Xenacanthida, and several genera like \"Bandringa\" invaded fresh waters of the coal swamps. Among the bony fish, the Palaeonisciformes found in coastal waters also appear to have migrated to rivers. Sarcopterygian fish were also prominent, and one group, the Rhizodonts, reached very large size.\nMost species of Carboniferous marine fish have been described largely from teeth, fin spines and dermal ossicles, with smaller freshwater fish preserved whole. Freshwater fish were abundant, and include the genera \"Ctenodus\", \"Uronemus\", \"Acanthodes\", \"Cheirodus\", and \"Gyracanthus\". Chondrichthyes (especially holocephalans like the Stethacanthids) underwent a major evolutionary radiation during the Carboniferous. It is believed that this evolutionary radiation occurred because the decline of the placoderms at the end of the Devonian caused many environmental niches to become unoccupied and allowed new organisms to evolve and fill these niches. As a result of the evolutionary radiation Carboniferous holocephalans assumed a wide variety of bizarre shapes including \"Stethacanthus\" which possessed a flat brush-like dorsal fin with a patch of denticles on its top. \"Stethacanthus\" unusual fin may have been used in mating rituals.\nOther groups like the eugeneodonts filled in the niches left by large predatory placoderms. These fish were unique as they only possessed one row of teeth in their upper or lower jaws in the form of elaborate tooth whorls. The first members of the helicoprionidae, a family eugeneodonts that were characterized by the presence of one circular tooth whorl in the lower jaw, appeared during the early Carboniferous. Perhaps the most bizarre radiation of holocephalans at this time was that of the iniopterygiformes, an order of holocephalans that greatly resembled modern day flying fish that could have also \"flown\" in the water with their massive, elongated pectoral fins. They were further characterized by their large eye sockets, club-like structures on their tails, and spines on the tips of their fins.\nTetrapods.\nCarboniferous amphibians were diverse and common by the middle of the period, more so than they are today; some were as long as 6 meters, and those fully terrestrial as adults had scaly skin. They included basal tetrapod groups classified in early books under the Labyrinthodontia. These had a long body, a head covered with bony plates, and generally weak or undeveloped limbs. The largest were over 2 meters long. They were accompanied by an assemblage of smaller amphibians included under the Lepospondyli, often only about long. Some Carboniferous amphibians were aquatic and lived in rivers (\"Loxomma\", \"Eogyrinus\", \"Proterogyrinus\"); others may have been semi-aquatic (\"Ophiderpeton\", \"Amphibamus\", \"Hyloplesion\") or terrestrial (\"Dendrerpeton\", \"Tuditanus\", \"Anthracosaurus\").\nThe Carboniferous rainforest collapse slowed the evolution of amphibians who could not survive as well in the cooler, drier conditions. Amniotes, however, prospered because of specific key adaptations. One of the greatest evolutionary innovations of the Carboniferous was the amniote egg, which allowed the laying of eggs in a dry environment, as well as keratinized scales and claws, allowing for the further exploitation of the land by certain tetrapods. These included the earliest sauropsid reptiles (\"Hylonomus\"), and the earliest known synapsid (\"Archaeothyris\"). Synapsids quickly became huge and diversified in the Permian, only for their dominance to stop during the Mesozoic. Sauropsids (reptiles, and also, later, birds) also diversified but remained small until the Mesozoic, during which they dominated the land, as well as the water and sky, only for their dominance to stop during the Cenozoic.\nReptiles underwent a major evolutionary radiation in response to the drier climate that preceded the rainforest collapse. By the end of the Carboniferous amniotes had already diversified into a number of groups, including several families of synapsid pelycosaurs, protorothyridids, captorhinids, saurians and araeoscelids.\nFungi.\nAs plants and animals were growing in size and abundance in this time, land fungi diversified further. Marine fungi still occupied the oceans. All modern classes of fungi were present in the late Carboniferous.\nExtinction events.\nRomer's gap.\nThe first 15 million years of the Carboniferous had very limited terrestrial fossils. While it has long been debated whether the gap is a result of fossilisation or relates to an actual event, recent work indicates there was a drop in atmospheric oxygen levels, indicating some sort of ecological collapse. The gap saw the demise of the Devonian fish-like ichthyostegalian labyrinthodonts and the rise of the more advanced temnospondylian and reptiliomorphan amphibians that so typify the Carboniferous terrestrial vertebrate fauna.\nCarboniferous rainforest collapse.\nBefore the end of the Carboniferous, an extinction event occurred. On land this event is referred to as the Carboniferous rainforest collapse. Vast tropical rainforests collapsed suddenly as the climate changed from hot and humid to cool and arid. This was likely caused by intense glaciation and a drop in sea levels. The new climatic conditions were not favorable to the growth of rainforest and the animals within them. Rainforests shrank into isolated islands, surrounded by seasonally dry habitats. Towering lycopsid forests with a heterogeneous mixture of vegetation were replaced by much less diverse tree fern dominated flora.\nAmphibians, the dominant vertebrates at the time, fared poorly through this event with large losses in biodiversity; reptiles continued to diversify through key adaptations that let them survive in the drier habitat, specifically the hard-shelled egg and scales, both of which retain water better than their amphibian counterparts."}
{"id": "5403", "revid": "48537787", "url": "https://en.wikipedia.org/wiki?curid=5403", "title": "Comoros", "text": "The Comoros, officially the Union of the Comoros, is an archipelagic country made up of three islands in Southeastern Africa, located at the northern end of the Mozambique Channel in the Indian Ocean. Its capital and largest city is Moroni. The religion of the majority of the population, and the official state religion, is Sunni Islam. Comoros proclaimed its independence from France on 6 July 1975. The Comoros is the only country of the Arab League which is entirely in the Southern Hemisphere. It is a member state of the African Union, the \"Organisation internationale de la Francophonie\", the Organisation of Islamic Co-operation, and the Indian Ocean Commission. The country has three official languages: Shikomori, French and Arabic.\nAt , the Comoros is the third-smallest African country by area. In 2019, its population was estimated to be 850,886. The sovereign state consists of three major islands and numerous smaller islands, all of the volcanic Comoro Islands with the exception of Mayotte. Mayotte voted against independence from France in a referendum in 1974, and continues to be administered by France as an overseas department. France has vetoed a United Nations Security Council resolution that would have affirmed Comorian sovereignty over the island. Mayotte became an overseas department and a region of France in 2011 following a referendum which was passed overwhelmingly.\nThe Comoros were likely first settled by Austronesian/Malagasy peoples, Bantu speakers from East Africa, and seafaring Arab traders. From 1500 the Sultanate of Anjouan dominated the islands, with Grande Comore split between several sultans. It became part of the French colonial empire during the 19th century, before its independence in 1975. It has experienced more than 20 coups or attempted coups, with various heads of state assassinated. Along with this constant political instability, it has one of the highest levels of income inequality of any nation, and ranks in the medium quartile on the Human Development Index. Between 2009 and 2014, about 19% of the population lived below the international poverty line of US$1.90 a day by purchasing power parity.\nEtymology.\nThe name \"Comoros\" derives from the Arabic word \"qamar\" (\"moon\").\nHistory.\nSettlement.\nAccording to mythology, a jinni (spirit) dropped a jewel, which formed a great circular inferno. This became the Karthala volcano, which created the island of Ngazidja (Grande Comore). King Solomon is also said to have visited the island accompanied by his queen Bilqis.\nThe first attested human inhabitants of the Comoro Islands are now thought to have been Austronesian settlers travelling by boat from islands in Southeast Asia. These people arrived in the area no later than the eighth century AD, the date of the earliest known archaeological site, found on Mayotte, although settlement beginning in the first century has been postulated.\nSubsequent settlers came from the east coast of Africa, the Arabian Peninsula and the Persian Gulf, the Malay Archipelago, and Madagascar. Bantu-speaking settlers were present on the islands from the beginnings of settlement , probably brought to the islands as slaves.\nDevelopment of the Comoros is divided into phases. The earliest reliably recorded phase is the Dembeni phase (eighth to tenth centuries), during which there were several small settlements on each island. From the eleventh to the fifteenth centuries, trade with the island of Madagascar and merchants from the Swahili coast and the Middle East flourished, more villages were founded and existing villages grew. Settlers from the Arabian peninsula, particularly Hadhramaut, arrived during this period.\nMedieval Comoros.\nAccording to legend, in 632, upon hearing of Islam, islanders are said to have dispatched an emissary, Mtswa-Mwindza, to Mecca\u2014but by the time he arrived there, the Islamic prophet Muhammad had died. Nonetheless, after a stay in Mecca, he returned to Ngazidja, where he built a mosque in his home town of Ntsaweni, and led the gradual conversion of the islanders to Islam.\nIn 933, the Comoros was referred to by Omani sailors as the Perfume Islands.\nAmong the earliest accounts of East Africa, the works of Al-Masudi describe early Islamic trade routes, and how the coast and islands were frequently visited by Muslims including Persian and Arab merchants and sailors in search of coral, ambergris, ivory, tortoiseshell, gold and slaves for the Arabic slave trade. They also brought Islam to the people of the Zanj including the Comoros. As the importance of the Comoros grew along the East African coast, both small and large mosques were constructed. The Comoros are part of the Swahili cultural and economic complex and the islands became a major hub of trade and an important location in a network of trading towns that included Kilwa, in present-day Tanzania, Sofala (an outlet for Zimbabwean gold), in Mozambique, and Mombasa in Kenya.\nThe Portuguese arrived in the Indian Ocean at the end of the 15th century and the first Portuguese visit to the islands seems to have been that of Vasco da Gama's second fleet in 1503. For much of the 16th century the islands provided provisions to the Portuguese fort at Mozambique and although there was no formal attempt by the Portuguese crown to take possession, a number of Portuguese traders settled and married local women.\nBy the end of the 16th century local rulers on the African mainland were beginning to push back and, with the support of the Omani Sultan Saif bin Sultan they began to defeat the Dutch and the Portuguese. One of his successors, Said bin Sultan, increased Omani Arab influence in the region, moving his administration to nearby Zanzibar, which came under Omani rule. Nevertheless, the Comoros remained independent, and although the three smaller islands were usually politically unified, the largest island, Ngazidja, was divided into a number of autonomous kingdoms (\"ntsi\").\nThe islands were well placed to meet the needs of Europeans, initially supplying the Portuguese in Mozambique, then ships, particularly the English, on the route to India, and, later, slaves to the plantation islands in the Mascarenes.\nEuropean contact and French colonisation.\nIn the last decade of the 18th century, Malagasy warriors, mostly Betsimisaraka and Sakalava, started raiding the Comoros for slaves and the islands were devastated as crops were destroyed and the people were slaughtered, taken into captivity or fled to the African mainland: it is said that by the time the raids finally ended in the second decade of the 19th century only one man remained on Mwali. The islands were repopulated by slaves from the mainland, who were traded to the French in Mayotte and the Mascarenes. On the Comoros, it was estimated in 1865 that as much as 40% of the population consisted of slaves.\nFrance first established colonial rule in the Comoros by taking possession of Mayotte in 1841 when the Sakalava usurper sultan (also known as Tsy Levalo) signed the Treaty of April 1841, which ceded the island to the French authorities. After its annexation, France attempted to convert Mayotte into a sugar plantation colony.\nMeanwhile, Ndzwani (or Johanna as it was known to the British) continued to serve as a way station for English merchants sailing to India and the Far East, as well as American whalers, although the British gradually abandoned it following their possession of Mauritius in 1814, and by the time the Suez Canal opened in 1869 there was no longer any significant supply trade at Ndzwani. Local commodities exported by the Comoros were, in addition to slaves, coconuts, timber, cattle and tortoiseshell. British and American settlers, as well as the island's sultan, established a plantation-based economy that used about one-third of the land for export crops. In addition to sugar on Mayotte, ylang-ylang and other perfume plants, vanilla, cloves, coffee, cocoa beans, and sisal were introduced.\nIn 1886, Mwali was placed under French protection by its Sultan Mardjani Abdou Cheikh. That same year, Sultan Said Ali of Bambao, one of the sultanates on Ngazidja, placed the island under French protection in exchange for French support of his claim to the entire island, which he retained until his abdication in 1910. In 1908 the four islands were unified under a single administration (\"Colonie de Mayotte et d\u00e9pendances\") and placed under the authority of the French colonial Governor-General of Madagascar. In 1909, Sultan Said Muhamed of Ndzwani abdicated in favour of French rule and in 1912 the protectorates were abolished and the islands administered as a single colony. Two years later the colony was abolished and the islands became a province of the colony of Madagascar.\nAgreement was reached with France in 1973 for the Comoros to become independent in 1978, despite the deputies of Mayotte voting for increased integration with France. A referendum was held on all four of the islands. Three voted for independence by large margins, while Mayotte voted against. On 6 July 1975, however, the Comorian parliament passed a unilateral resolution declaring independence. Ahmed Abdallah proclaimed the independence of the Comorian State (\"\u00c9tat comorien\"; \u062f\u0648\u0644\u0629 \u0627\u0644\u0642\u0645\u0631) and became its first president. France did not recognise the new state until 31 December, and retained control of Mayotte.\nIndependence (1975).\nThe next 30 years were a period of political turmoil. On 3 August 1975, less than one month after independence, president Ahmed Abdallah was removed from office in an armed coup and replaced with United National Front of the Comoros (FNUK) member Said Mohamed Jaffar. Months later, in January 1976, Jaffar was ousted in favour of his Minister of Defence Ali Soilihi.\nThe population of Mayotte voted against independence from France in three referendums during this period. The first, held on all the islands on 22 December 1974, won 63.8% support for maintaining ties with France on Mayotte; the second, held in February 1976, confirmed that vote with an overwhelming 99.4%, while the third, in April 1976, confirmed that the people of Mayotte wished to remain a French territory. The three remaining islands, ruled by President Soilihi, instituted a number of socialist and isolationist policies that soon strained relations with France. On 13 May 1978, Bob Denard, once again commissioned by the French intelligence service (SDECE), returned to overthrow President Soilihi and reinstate Abdallah with the support of the French, Rhodesian and South African governments. Ali Soilihi was captured and executed a few weeks later.\nIn contrast to Soilihi, Abdallah's presidency was marked by authoritarian rule and increased adherence to traditional Islam and the country was renamed the Federal Islamic Republic of the Comoros (\"R\u00e9publique F\u00e9d\u00e9rale Islamique des Comores\"; \u062c\u0645\u0647\u0648\u0631\u064a\u0629 \u0627\u0644\u0642\u0645\u0631 \u0627\u0644\u0625\u062a\u062d\u0627\u062f\u064a\u0629 \u0627\u0644\u0625\u0633\u0644\u0627\u0645\u064a\u0629). Bob Denard served as Abdallah's first advisor; nicknamed the \"Viceroy of the Comoros,\" he was sometimes considered the real strongman of the regime. Very close to South Africa, which financed his \"presidential guard,\" he allowed Paris to circumvent the international embargo on the apartheid regime via Moroni. He also set up from the archipelago a permanent mercenary corps, called upon to intervene at the request of Paris or Pretoria in conflicts in Africa. Abdallah continued as president until 1989 when, fearing a probable coup, he signed a decree ordering the Presidential Guard, led by Bob Denard, to disarm the armed forces. Shortly after the signing of the decree, Abdallah was allegedly shot dead in his office by a disgruntled military officer, though later sources claim an antitank missile was launched into his bedroom and killed him. Although Denard was also injured, it is suspected that Abdallah's killer was a soldier under his command.\nA few days later, Bob Denard was evacuated to South Africa by French paratroopers. Said Mohamed Djohar, Soilihi's older half-brother, then became president, and served until September 1995, when Bob Denard returned and attempted another coup. This time France intervened with paratroopers and forced Denard to surrender. The French removed Djohar to Reunion, and the Paris-backed Mohamed Taki Abdoulkarim became president by election. He led the country from 1996, during a time of labour crises, government suppression, and secessionist conflicts, until his death in November 1998. He was succeeded by Interim President Tadjidine Ben Said Massounde.\nThe islands of Ndzwani and Mwali declared their independence from the Comoros in 1997, in an attempt to restore French rule. But France rejected their request, leading to bloody confrontations between federal troops and rebels. In April 1999, Colonel Azali Assoumani, Army Chief of Staff, seized power in a bloodless coup, overthrowing the Interim President Massounde, citing weak leadership in the face of the crisis. This was the Comoros' 18th coup, or attempted coup d'\u00e9tat since independence in 1975.\nAssoumani failed to consolidate power and reestablish control over the islands, which was the subject of international criticism. The African Union, under the auspices of President Thabo Mbeki of South Africa, imposed sanctions on Ndzwani to help broker negotiations and effect reconciliation. Under the terms of the Fomboni Accords, signed in December 2001 by the leaders of all three islands, the official name of the country was changed to the Union of the Comoros; the new state was to be highly decentralised and the central union government would devolve most powers to the new island governments, each led by a president. The Union president, although elected by national elections, would be chosen in rotation from each of the islands every five years.\nAssoumani stepped down in 2002 to run in the democratic election of the President of the Comoros, which he won. Under ongoing international pressure, as a military ruler who had originally come to power by force, and was not always democratic while in office, Assoumani led the Comoros through constitutional changes that enabled new elections. A \"Loi des comp\u00e9tences\" law was passed in early 2005 that defines the responsibilities of each governmental body, and is in the process of implementation. The elections in 2006 were won by Ahmed Abdallah Mohamed Sambi, a Sunni Muslim cleric nicknamed the \"Ayatollah\" for his time spent studying Islam in Iran. Assoumani honoured the election results, thus allowing the first peaceful and democratic exchange of power for the archipelago.\nColonel Mohammed Bacar, a French-trained former gendarme elected President of Ndzwani in 2001, refused to step down at the end of his five-year mandate. He staged a vote in June 2007 to confirm his leadership that was rejected as illegal by the Comoros federal government and the African Union. On 25 March 2008 hundreds of soldiers from the African Union and the Comoros seized rebel-held Ndzwani, generally welcomed by the population: there have been reports of hundreds, if not thousands, of people tortured during Bacar's tenure.\nSome rebels were killed and injured, but there are no official figures. At least 11 civilians were wounded. Some officials were imprisoned. Bacar fled in a speedboat to Mayotte to seek asylum. Anti-French protests followed in the Comoros (see 2008 invasion of Anjouan). Bacar was eventually granted asylum in Benin.\nSince independence from France, the Comoros experienced more than 20 coups or attempted coups.\nFollowing elections in late 2010, former Vice-president Ikililou Dhoinine was inaugurated as president on 26 May 2011. Dhoinine is the first President of the Comoros from the island of Mwali. Following the 2016 elections, Azali Assoumani, from Ngazidja, became president for a third term. In 2018 Assoumani held a referendum on constitutional reform that would permit a president to serve two terms. The amendments passed, although the vote was widely contested and boycotted by the opposition, and in April 2019, and to widespread opposition, Assoumani was re-elected president to serve the first of potentially two five-year terms. In January 2020, the legislative elections in Comoros were dominated by President Azali Assoumani's party, the Convention for the Renewal of the Comoros, CRC. It took an overwhelming majority in the parliament.\nIn 2021, Comoros signed and ratified the Treaty on the Prohibition of Nuclear Weapons, making it a nuclear-weapon-free state. and in 2023, Comoros was invited as a non-member guest to the G7 summit in Hiroshima. On 18 February 2023 the Comoros assumed the presidency of the African Union.\nIn January 2024, President Azali Assoumani was re-elected with 63% of the vote in the disputed presidential election.\nGeography.\nThe Comoros is formed by Ngazidja (Grande Comore), Mwali (Moh\u00e9li) and Ndzwani (Anjouan), three major islands in the Comoros Archipelago, as well as many minor islets. The islands are officially known by their Comorian language names, though international sources still use their French names (given in parentheses above). The capital and largest city, Moroni, is located on Ngazidja and the most densely populated city is Anjouan. The archipelago is situated in the Indian Ocean, in the Mozambique Channel, between the African coast (nearest to Mozambique and Tanzania) and Madagascar, with no land borders.\nAt , it is one of the smallest countries in the world. The Comoros also has claim to of territorial seas. The interiors of the islands vary from steep mountains to low hills.\nThe areas and populations (at the 2017 Census) of the main islands are as follows:\nNgazidja is the largest of the Comoros Archipelago, with an area of 1,024\u00a0km2. It is also the most recent island, and therefore has rocky soil. The island's two volcanoes, Karthala (active) and La Grille (dormant), and the lack of good harbours are distinctive characteristics of its terrain. Mwali, with its capital at Fomboni, is the smallest of the four major islands. Ndzwani, whose capital is Mutsamudu, has a distinctive triangular shape caused by three mountain chains \u2013 Shisiwani, Nioumakele and Jimilime \u2013 emanating from a central peak, ().\nThe islands of the Comoros Archipelago were formed by volcanic activity. Mount Karthala, an active shield volcano located on Ngazidja, is the country's highest point, at . It contains the Comoros' largest patch of disappearing rainforest. Karthala is currently one of the most active volcanoes in the world, with a minor eruption in May 2006, and prior eruptions as recently as April 2005 and 1991. In the 2005 eruption, which lasted from 17 to 19 April, 40,000 citizens were evacuated, and the crater lake in the volcano's caldera was destroyed.\nThe Comoros also lays claim to the \"\u00celes \u00c9parses\" or \"\u00celes \u00e9parses de l'oc\u00e9an indien\" (Scattered Islands in the Indian Ocean) \u2013 Glorioso Islands, comprising Grande Glorieuse, \u00cele du Lys, Wreck Rock, South Rock, (three islets) and three unnamed islets \u2013 one of France's overseas districts. The Glorioso Islands were administered by the colonial Comoros before 1975, and are therefore sometimes considered part of the Comoros Archipelago. Banc du Geyser, a former island in the Comoros Archipelago, now submerged, is geographically located in the \"\u00celes \u00c9parses\", but was annexed by Madagascar in 1976 as an unclaimed territory. The Comoros and France each still view the Banc du Geyser as part of the Glorioso Islands and, thus, part of its particular exclusive economic zone.\nClimate.\nThe climate is generally tropical and mild, and the two major seasons are distinguishable by their raininess. The temperature reaches an average of in March, the hottest month in the rainy season (called [meaning north monsoon], which runs from November to April), and an average low of in the cool, dry season (kusi (meaning south monsoon), which proceeds from May to October). The islands are rarely subject to cyclones.\nBiodiversity.\nThe Comoros constitute an ecoregion in their own right, Comoros forests. It had a 2018 Forest Landscape Integrity Index mean score of 7.69/10, ranking it 33rd globally out of 172 countries.\nIn December 1952 a specimen of the West Indian Ocean coelacanth fish was re-discovered off the Comoros coast. The 66 million-year-old species was thought to have been long extinct until its first recorded appearance in 1938 off the South African coast. Between 1938 and 1975, 84 specimens were caught and recorded.\nProtected areas.\nThere are six national parks in the Comoros \u2013 Karthala, Coelacanth, and Mitsamiouli Ndroudi on Grande Comore, Mount Ntringui and Shisiwani on Anjouan, and Moh\u00e9li National Park on Moh\u00e9li. Karthala and Mount Ntrigui national parks cover the highest peaks on the respective islands, and Coelacanth, Mitsamiouli Ndroudi, and Shisiwani are marine national parks that protect the island's coastal waters and fringing reefs. Moh\u00e9li National Park includes both terrestrial and marine areas.\nGovernment.\nPolitics of the Comoros takes place in a framework of a unitary presidential republic, whereby the President of the Comoros is both head of state and head of government, and of a multi-party system. The Constitution of the Union of the Comoros was ratified by referendum on 23 December 2001, and the islands' constitutions and executives were elected in the following months. It had previously been considered a military dictatorship, and the transfer of power from Azali Assoumani to Ahmed Abdallah Mohamed Sambi in May 2006 was a watershed moment as it was the first peaceful transfer in Comorian history.\nExecutive power is exercised by the government. Legislative power is vested in both the government and parliament. The preamble of the constitution guarantees an Islamic inspiration in governance, a commitment to human rights, and several specific enumerated rights, democracy, \"a common destiny\" for all Comorians. Each of the islands (according to Title II of the Constitution) has a great amount of autonomy in the Union, including having their own constitutions (or Fundamental Law), president, and Parliament. The presidency and Assembly of the Union are distinct from each of the islands' governments. Up to a referendum on 30.7.2018 (62.7% participation, 92.34% for the amendment according to the Comorian government) the presidency of the Union rotated between the islands. \nLegal system.\nThe Comorian legal system rests on Islamic law, an inherited French (Napoleonic Code) legal code, and customary law (mila na ntsi). Village elders, kadis or civilian courts settle most disputes. The judiciary is independent of the legislative and the executive. The Supreme Court acts as a Constitutional Council in resolving constitutional questions and supervising presidential elections. As High Court of Justice, the Supreme Court also arbitrates in cases where the government is accused of malpractice. The Supreme Court consists of two members selected by the president, two elected by the Federal Assembly, and one by the council of each island.\nPolitical culture.\nAround 80 percent of the central government's annual budget is spent on the country's complex administrative system which provides for a semi-autonomous government and president for each of the three islands and a rotating presidency for the overarching Union government. A referendum took place on 16 May 2009 to decide whether to cut down the government's unwieldy political bureaucracy. 52.7% of those eligible voted, and 93.8% of votes were cast in approval of the referendum. Following the implementation of the changes, each island's president became a governor and the ministers became councillors.\nForeign relations.\nIn November 1975, the Comoros became the 143rd member of the United Nations. The new nation was defined as comprising the entire archipelago, although the citizens of Mayotte chose to become French citizens and keep their island as a French territory.\nThe Comoros has repeatedly pressed its claim to Mayotte before the United Nations General Assembly, which adopted a series of resolutions under the caption \"Question of the Comorian Island of Mayotte\", opining that Mayotte belongs to the Comoros under the principle that the territorial integrity of colonial territories should be preserved upon independence. As a practical matter, however, these resolutions have little effect and there is no foreseeable likelihood that Mayotte will become \"de facto\" part of the Comoros without its people's consent. More recently, the Assembly has maintained this item on its agenda but deferred it from year to year without taking action. Other bodies, including the Organisation of African Unity, the Movement of Non-Aligned Countries and the Organisation of Islamic Cooperation, have similarly questioned French sovereignty over Mayotte. To close the debate and to avoid being integrated by force in the Union of the Comoros, the population of Mayotte overwhelmingly chose to become an overseas department and a region of France in a 2009 referendum. The new status was effective on 31 March 2011 and Mayotte has been recognised as an outermost region by the European Union on 1 January 2014. This decision legally integrates Mayotte in the French Republic.\nThe Comoros is a member of the United Nations, the African Union, the Arab League, the World Bank, the International Monetary Fund, the Indian Ocean Commission and the African Development Bank. On 10 April 2008, the Comoros became the 179th nation to accept the Kyoto Protocol to the United Nations Framework Convention on Climate Change. The Comoros signed the UN treaty on the Prohibition of Nuclear Weapons. Azali Assoumani, President of the Comoros and Chair of the African Union, attended the 2023 Russia\u2013Africa Summit in Saint Petersburg.\nIn May 2013 the Union of the Comoros became known for filing a referral to the Office of the Prosecutor of the International Criminal Court (ICC) regarding the events of \"the 31 May 2010 Israeli raid on the Humanitarian Aid Flotilla bound for [the] Gaza Strip\". In November 2014 the ICC Prosecutor eventually decided that the events did constitute war crimes but did not meet the gravity standards of bringing the case before ICC.\nThe emigration rate of skilled workers was about 21.2% in 2000.\nMilitary.\nThe military resources of the Comoros consist of a small standing army and a 500-member police force, as well as a 500-member defence force. A defence treaty with France provides naval resources for protection of territorial waters, training of Comorian military personnel, and air surveillance. France maintains the presence of a few senior officers in the Comoros at government request, as well as a small maritime base and a Foreign Legion Detachment (DLEM) on Mayotte.\nOnce the new government was installed in May\u2013June 2011, an expert mission from UNREC (Lom\u00e9) came to the Comoros and produced guidelines for the elaboration of a national security policy, which were discussed by different actors, notably the national defence authorities and civil society. By the end of the programme in end March 2012, a normative framework agreed upon by all entities involved in SSR will have been established. This will then have to be adopted by Parliament and implemented by the authorities.\nHuman rights.\nBoth male and female same-sex sexual acts are illegal in Comoros. Such acts are punished with up to five years' imprisonment.\nEconomy.\nThe level of poverty in the Comoros is high, but \"judging by the international poverty threshold of $1.9 per person per day, only two out of every ten Comorians could be classified as poor, a rate that places the Comoros ahead of other low-income countries and 30 percentage points ahead of other countries in Sub-Saharan Africa.\" Poverty declined by about 10% between 2014 and 2018, and living conditions generally improved. Economic inequality remains widespread, with a major gap between rural and urban areas. Remittances through the sizable Comorian diaspora form a substantial part of the country's GDP and have contributed to decreases in poverty and increases in living standards.\nAccording to ILO's ILOSTAT statistical database, between 1991 and 2019 the unemployment rate as a percent of the total labor force ranged from 4.38% to 4.3%. An October 2005 paper by the Comoros Ministry of Planning and Regional Development, however, reported that \"registered unemployment rate is 14.3 percent, distributed very unevenly among and within the islands, but with marked incidence in urban areas.\"\nIn 2019, more than 56% of the labor force was employed in agriculture, with 29% employed in industry and 14% employed in services. The islands' agricultural sector is based on the export of spices, including vanilla, cinnamon, and cloves, and thus susceptible to price fluctuations in the volatile world commodity market for these goods. The Comoros is the world's largest producer of ylang-ylang, a plant whose extracted essential oil is used in the perfume industry; some 80% of the world's supply comes from the Comoros.\nHigh population densities, as much as 1000 per square kilometre in the densest agricultural zones, for what is still a mostly rural, agricultural economy may lead to an environmental crisis in the near future, especially considering the high rate of population growth. In 2004 the Comoros' real GDP growth was a low 1.9% and real GDP per capita continued to decline. These declines are explained by factors including declining investment, drops in consumption, rising inflation, and an increase in trade imbalance due in part to lowered cash crop prices, especially vanilla.\nFiscal policy is constrained by erratic fiscal revenues, a bloated civil service wage bill, and an external debt that is far above the HIPC threshold. Membership in the franc zone, the main anchor of stability, has nevertheless helped contain pressures on domestic prices.\nThe Comoros has an inadequate transportation system, a young and rapidly increasing population, and few natural resources. The low educational level of the labour force contributes to a subsistence level of economic activity, high unemployment, and a heavy dependence on foreign grants and technical assistance. Agriculture contributes 40% to GDP and provides most of the exports.\nThe government is struggling to upgrade education and technical training, to privatise commercial and industrial enterprises, to improve health services, to diversify exports, to promote tourism, and to reduce the high population growth rate.\nThe Comoros is a member of the Organisation for the Harmonisation of Business Law in Africa (OHADA).\nDemographics.\nWith about 850,000 residents, the Comoros is one of the least populous countries in the world, but its population density is high, with an average of . In 2001, 34% of the population was considered urban, but the urban population has since grown; in recent years rural population growth has been negative, while overall population growth is still relatively high. In 1958 the population was 183,133.\nIn 2009, almost half the population of the Comoros was under the age of 15. Major urban centres include Moroni, Mitsamihuli, Foumbouni, Mutsamudu, Domoni, and Fomboni. There are between 200,000 and 350,000 Comorians in France.\nEthnic groups.\nThe islands of the Comoros are 97.1% ethnically Comorian, which is a mixture of Bantu, Malagasy, and Arab people. Minorities include Makua and Indian (mostly Ismaili). There are recent immigrants of Chinese origin in Grande Comore (especially Moroni). Although most French left after independence in 1975, a small Creole community, descended from settlers from France, Madagascar and R\u00e9union, lives in the Comoros.\nLanguages.\nThe most common languages in the Comoros are the Comorian languages, collectively known as \"Shikomori\". They are related to Swahili, and the four different variants (Shingazidja, Shimwali, Shindzwani and Shimaore) are spoken on each of the four islands. Arabic and Latin scripts are both used, Arabic being the more widely used, and an official orthography has recently been developed for the Latin script.\nArabic and French are also official languages, along with Comorian. Arabic is widely known as a second language, being the language of Quranic teaching. French is the administrative language and the language of most non-Quranic formal education.\nReligion.\nSunni Islam is the dominant religion, followed by as much as 99% of the population. Comoros is the only Muslim-majority country in Southern Africa and one of the three southernmost Muslim-majority territories, along with Mayotte and the Australian territory of Cocos Islands.\nA minority of the population of the Comoros are Christian, both Catholic and Protestant denominations are represented, and most Malagasy residents are also Christian. Immigrants from metropolitan France are mostly Catholic.\nHealth.\nThere are 15 physicians per 100,000 people. The fertility rate was 4.7 per adult woman in 2004. Life expectancy at birth is 67 for females and 62 for males.\nIn the 2024 Global Hunger Index (GHI), Comoros ranks 81st out of 127 countries with sufficient data, with a score of 18.8, which indicates a moderate level of hunger.\nEducation.\nAlmost all children attend Quran Religious schools, usually before, although increasingly in tandem with regular schooling. Children are taught about the Qur'an, and memorise it, and learn the Arabic script.: \nMost parents prefer their children to attend Koran Religious schools before moving on to the French- English based schooling system. Although the state sector is plagued by a lack of resources, and the teachers by unpaid salaries, there are numerous private and community schools of relatively good standard. The national curriculum, apart from a few years during the revolutionary period immediately post-independence, has been very much based on the French system, both because resources are French and most Comorans hope to go on to further education in France, Spain and Italy. There have recently been moves to Comorianise the syllabus and integrate the two systems, the formal and the Quran Religious Schools, into one, thus moving away from the secular educational system inherited from France.\nPre-colonisation education systems in Comoros focused on necessary skills such as agriculture, caring for livestock and completing household tasks. Religious education also taught Islam. The education system underwent a transformation during colonisation in the early 1900s which brought secular education based on the French system. This was mainly for children of the elite. After Comoros gained independence in 1975, the education system changed again. Funding for teachers' salaries was lost, and many went on strike. Thus, the public education system was not functioning between 1997 and 2001. Since gaining independence, the education system has also undergone a democratisation and options exist for those other than the elite. Enrollment has also grown.\nIn 2000, 44.2% of children aged 5 to 14 years were attending school. There is a general lack of facilities, equipment, qualified teachers, textbooks and other resources. Salaries for teachers are often so far in arrears that many refuse to work.\nPrior to 2000, students seeking a university education had to attend school outside of the country In Europe and Americas especially: France, United Kingdom.:\nHowever, in the early 2000s a university was created in the country. This served to help economic growth and to fight the \"flight\" of many educated people who were not returning to the islands to work.\nComorian has no native script, but both the Arabic and Latin alphabets are used. In 2004, about 57 percent of the population was literate in the Latin script while more than 90 percent were literate in the Arabic script.\nCulture.\nTraditionally, women on Ndzwani wear red and white patterned garments called \"shiromani\", while on Ngazidja and Mwali colourful shawls called \"leso\" are worn. Many women apply a paste of ground sandalwood and coral called \"msindzano\" to their faces. Traditional male clothing is a long white shirt known as a \"nkandu\", and a bonnet called a \"kofia\".\nMarriage.\nThere are two types of marriages in Comoros, the little marriage (known as \"Mna daho\" on Ngazidja) and the customary marriage (known as \"ada\" on Ngazidja, \"harusi\" on the other islands). The little marriage is a simple legal marriage. It is small, intimate, and inexpensive, and the bride's dowry is nominal. A man may undertake a number of \"Mna daho\" marriages in his lifetime, often at the same time, a woman fewer; but both men and women will usually only undertake one \"ada\", or grand marriage, and this must generally be within the village. The hallmarks of the grand marriage are dazzling gold jewelry, two weeks of celebration and an enormous bridal dowry. Although the expenses are shared between both families as well as with a wider social circle, an ada wedding on Ngazidja can cost up to \u20ac50,000. Many couples take a lifetime to save for their ada, and it is not uncommon for a marriage to be attended by a couple's adult children.\nThe \"ada\" marriage marks a man's transition in the Ngazidja age system from youth to elder. His status in the social hierarchy greatly increases, and he will henceforth be entitled to speak in public and participate in the political process, both in his village and more widely across the island. He will be entitled to display his status by wearing a \"mharuma\", a type of shawl, across his shoulders, and he can enter the mosque by the door reserved for elders, and sit at the front. A woman's status also changes, although less formally, as she becomes a \"mother\" and moves into her own house. The system is less formalised on the other islands, but the marriage is nevertheless a significant and costly event across the archipelago. The \"ada\" is often criticized because of its great expense, but at the same time it is a source of social cohesion and the main reason why migrants in France and elsewhere continue to send money home. Increasingly, marriages are also being taxed for the purposes of village development.\nKinship and social structure.\nComorian society has a bilateral descent system. Lineage membership and inheritance of immovable goods (land, housing) is matrilineal, passed in the maternal line, similar to many Bantu peoples who are also matrilineal, while other goods and patronymics are passed in the male line. However, there are differences between the islands, the matrilineal element being stronger on Ngazidja.\nMusic.\nTwarab music, imported from Zanzibar in the early 20th century, remains the most influential genre on the islands and is popular at \"ada\" marriages.\nMedia.\nThere are two daily national newspapers published in the Comoros, the government-owned \"Al-Watwan\", and the privately owned \"La Gazette des Comores\", both published in Moroni. There are a number of smaller newsletters published on an irregular basis as well as a variety of news websites. The government-owned ORTC (Office de Radio et T\u00e9l\u00e9vision des Comores) provides national radio and television service. There is a TV station run by the Anjouan regional government, and regional governments on the islands of Grande Comore and Anjouan each operate a radio station. There are also a few independent and small community radio stations that operate on the islands of Grande Comore and Moh\u00e9li, and these two islands have access to Mayotte Radio and French TV."}
{"id": "5404", "revid": "46993000", "url": "https://en.wikipedia.org/wiki?curid=5404", "title": "Critical philosophy", "text": "Critical philosophy () is a movement inaugurated by Immanuel Kant (1724\u20131804). It is dedicated to the self-examination of reason with the aim of exposing its inherent limitations, that is, to defining the possibilities of knowledge as a prerequisite to advancing to knowledge itself. According to Kant, only after such self-criticism does it become possible to develop metaphysics in a non-dogmatic way.\nThe three critical texts of the Kantian corpus are the \"Critique of Pure Reason\", \"Critique of Practical Reason\" and \"Critique of Judgement\", published between 1781 and 1790 and primarily concerned, respectively, with metaphysics, morality, and teleology.\nContemporaries of Kant such as Johann Georg Hamann and Johann Gottfried Herder rejected the notion of \"pure\" reason upon which this project depends. They claim that reason depends upon language, which always introduces historical contingencies."}
{"id": "5405", "revid": "44313162", "url": "https://en.wikipedia.org/wiki?curid=5405", "title": "China", "text": "China, officially the People's Republic of China (PRC), is a country in East Asia. With a population exceeding 1.4 billion, it is the second-most populous country after India, representing 17.4% of the world population. China spans the equivalent of five time zones and borders fourteen countries by land across an area of nearly , making it the third-largest country by total land area. The country is divided into 33 province-level divisions: 22 provinces, five autonomous regions, four municipalities, and two semi-autonomous special administrative regions. Beijing is the country's capital, while Shanghai is its most populous city by urban area and largest financial center.\nChina is considered one of the cradles of civilization: the first human inhabitants in the region arrived during the Paleolithic. By the late 2nd millennium\u00a0BCE, the earliest dynastic states had emerged in the Yellow River basin. The 8th\u20133rd centuries\u00a0BCE saw a breakdown in the authority of the Zhou dynasty, accompanied by the emergence of administrative and military techniques, literature, philosophy, and historiography. In 221\u00a0BCE, China was unified under an emperor, ushering in more than two millennia of imperial dynasties including the Qin, Han, Tang, Yuan, Ming, and Qing. With the invention of gunpowder and paper, the establishment of the Silk Road, and the building of the Great Wall, Chinese culture flourished and has heavily influenced both its neighbors and lands further afield. However, China began to cede parts of the country in the late 19th century to various European powers by a series of unequal treaties.\nAfter decades of Qing China on the decline, the 1911 Revolution overthrew the Qing dynasty and the monarchy and the Republic of China (ROC) was established the following year. The country under the nascent Beiyang government was unstable and ultimately fragmented during the Warlord Era, which was ended upon the Northern Expedition conducted by the Kuomintang (KMT) to reunify the country. The Chinese Civil War began in 1927, when KMT forces purged members of the rival Chinese Communist Party (CCP), who proceeded to engage in sporadic fighting against the KMT-led Nationalist government. Following the country's invasion by the Empire of Japan in 1937, the CCP and KMT formed the Second United Front to fight the Japanese. The Second Sino-Japanese War eventually ended in a Chinese victory; however, the CCP and the KMT resumed their civil war as soon as the war ended. In 1949, the resurgent Communists established control over most of the country, proclaiming the People's Republic of China and forcing the Nationalist government to retreat to the island of Taiwan. The country was split, with both sides claiming to be the sole legitimate government of China. Following the implementation of land reforms, further attempts by the PRC to realize communism failed: the Great Leap Forward was largely responsible for the Great Chinese Famine that ended with millions of Chinese people having died, and the subsequent Cultural Revolution was a period of social turmoil and persecution characterized by Maoist populism. Following the Sino-Soviet split, the Shanghai Communiqu\u00e9 in 1972 would precipitate the normalization of relations with the United States. Economic reforms that began in 1978 moved the country away from a socialist planned economy towards an increasingly capitalist market economy, spurring significant economic growth. A movement for increased democracy and liberalization stalled after the Tiananmen Square protests and massacre in 1989.\nChina is a unitary one-party socialist republic led by the CCP. It is one of the five permanent members of the UN Security Council; the UN representative for China was changed from the ROC to the PRC in 1971. It is a founding member of several multilateral and regional organizations such as the AIIB, the Silk Road Fund, the New Development Bank, and the RCEP. It is a member of the BRICS, the G20, APEC, the SCO, and the East Asia Summit. Making up around one-fifth of the world economy, the Chinese economy is the world's largest economy by PPP-adjusted GDP, the second-largest economy by nominal GDP, and the second-wealthiest country, albeit ranking poorly in measures of democracy, human rights and religious freedom. The country has been one of the fastest-growing major economies and is the world's largest manufacturer and exporter, as well as the second-largest importer. China is a nuclear-weapon state with the world's largest standing army by military personnel and the second-largest defense budget. It is a great power, and has been described as an emerging superpower. China is known for its cuisine and culture, and has 59 UNESCO World Heritage Sites, the second-highest number of any country.\nEtymology.\nThe word \"China\" has been used in English since the 16th century; however, it was not used by the Chinese themselves during this period. Its origin has been traced through Portuguese, Malay, and Persian back to the Sanskrit word , used in ancient India. \"China\" appears in Richard Eden's 1555 translation of the 1516 journal of the Portuguese explorer Duarte Barbosa. Barbosa's usage was derived from Persian (), which in turn derived from Sanskrit (). The origin of the Sanskrit word is a matter of debate. was first used in early Hindu scripture, including the \"Mahabharata\" (5th century\u00a0BCE) and the \"Laws of Manu\" (2nd century\u00a0BCE). In 1655, Martino Martini suggested that the word China is derived ultimately from the name of the Qin dynasty (221\u2013206 BCE). Although use in Indian sources precedes this dynasty, this derivation is still given in various sources. Alternative suggestions include the names for Yelang and the Jing or Chu state.\nThe official name of the modern state is the \"People's Republic of China\" (). The shorter form is \"China\" (), from ('central') and ('state'), a term which developed under the Western Zhou dynasty in reference to its royal demesne. It was used in official documents as an synonym for the state under the Qing. The name \"Zhongguo\" is also translated as in English. China is sometimes referred to as \"mainland China\" or \"the Mainland\" when distinguishing it from the Republic of China or the PRC's Special Administrative Regions.\nHistory.\nPrehistory.\nArchaeological evidence suggests that early hominids inhabited China 2.25 million years ago. The hominid fossils of Peking Man, a \"Homo erectus\" who used fire, have been dated to between 680,000 and 780,000 years ago. The fossilized teeth of \"Homo sapiens\" (dated to 125,000\u201380,000 years ago) have been discovered in Fuyan Cave. Chinese proto-writing existed in Jiahu around 6600 BCE, at Damaidi around 6000 BCE, Dadiwan from 5800 to 5400 BCE, and Banpo dating from the 5th millennium BCE. Some scholars have suggested that the Jiahu symbols (7th millennium BCE) constituted the earliest Chinese writing system.\nEarly dynastic rule.\nAccording to traditional Chinese historiography, the Xia dynasty was established during the late 3rd millennium\u00a0BCE, marking the beginning of the dynastic cycle that was understood to underpin China's entire political history. In the modern era, the Xia's historicity came under increasing scrutiny, in part due to the earliest known attestation of the Xia being written millennia after the date given for their collapse. In 1958, archaeologists discovered sites belonging to the Erlitou culture that existed during the early Bronze Age; they have since been characterized as the remains of the historical Xia, but this conception is often rejected. The Shang dynasty that traditionally succeeded the Xia is the earliest for which there are both contemporary written records and undisputed archaeological evidence. The Shang ruled much of the Yellow River valley until the 11th century\u00a0BCE, with the earliest hard evidence dated . The oracle bone script, attested from but generally assumed to be considerably older, represents the oldest known form of written Chinese, and is the direct ancestor of modern Chinese characters.\nThe Shang were overthrown by the Zhou, who ruled between the 11th and 5th centuries\u00a0BCE, though the centralized authority of Son of Heaven was slowly eroded by \"fengjian\" lords. Some principalities eventually emerged from the weakened Zhou and continually waged war with each other during the 300-year Spring and Autumn period. By the time of the Warring States period of the 5th\u20133rd centuries\u00a0BCE, there were seven major powerful states left.\nImperial China.\nQin and Han.\nThe Warring States period ended in 221\u00a0BCE after the state of Qin conquered the other six states, reunited China and established the dominant order of autocracy. King Zheng of Qin proclaimed himself the Emperor of the Qin dynasty, becoming the first emperor of a unified China. He enacted Qin's legalist reforms, notably the standardization of Chinese characters, measurements, road widths, and currency. His dynasty also conquered the Yue tribes in Guangxi, Guangdong, and Northern Vietnam. The Qin dynasty lasted only fifteen years, falling soon after the First Emperor's death.\nFollowing widespread revolts during which the imperial library was burned, the Han dynasty emerged to rule China between 206\u00a0BCE and 220\u00a0CE, creating a cultural identity among its populace still remembered in the ethnonym of the modern Han Chinese. The Han expanded the empire's territory considerably, with military campaigns reaching Central Asia, Mongolia, Korea, and Yunnan, and the recovery of Guangdong and northern Vietnam from Nanyue. Han involvement in Central Asia and Sogdia helped establish the land route of the Silk Road, replacing the earlier path over the Himalayas to India. Han China gradually became the largest economy of the ancient world. Despite the Han's initial decentralization and the official abandonment of the Qin philosophy of Legalism in favor of Confucianism, Qin's legalist institutions and policies continued to be employed by the Han government and its successors.\nThree Kingdoms, Jin, Northern and Southern dynasties.\nAfter the end of the Han dynasty, a period of strife known as Three Kingdoms followed, at the end of which Wei was swiftly overthrown by the Jin dynasty. The Jin fell to civil war upon the ascension of a developmentally disabled emperor; the Five Barbarians then rebelled and ruled northern China as the Sixteen States. The Xianbei unified them as the Northern Wei, whose Emperor Xiaowen reversed his predecessors' apartheid policies and enforced a drastic sinification on his subjects. In the south, the general Liu Yu secured the abdication of the Jin in favor of the Liu Song. The various successors of these states became known as the Northern and Southern dynasties, with the two areas finally reunited by the Sui in 581.\nSui, Tang and Song.\nThe Sui restored the Han to power through China, reformed its agriculture, economy and imperial examination system, constructed the Grand Canal, and patronized Buddhism. However, they fell quickly when their conscription for public works and a failed war in northern Korea provoked widespread unrest.\nUnder the succeeding Tang and Song dynasties, Chinese economy, technology, and culture entered a golden age. The Tang dynasty retained control of the Western Regions and the Silk Road, which brought traders to as far as Mesopotamia and the Horn of Africa, and made the capital Chang'an a cosmopolitan urban center. However, it was devastated and weakened by the An Lushan rebellion in the 8th century. In 907, the Tang disintegrated completely when the local military governors became ungovernable. The Song dynasty ended the separatist situation in 960, leading to a balance of power between the Song and the Liao dynasty. The Song was the first government in world history to issue paper money and the first Chinese polity to establish a permanent navy which was supported by the developed shipbuilding industry along with the sea trade.\nBetween the 10th and 11th century CE, the population of China doubled to around 100 million people, mostly because of the expansion of rice cultivation in central and southern China, and the production of abundant food surpluses. The Song dynasty also saw a revival of Confucianism, in response to the growth of Buddhism during the Tang, and a flourishing of philosophy and the arts, as landscape art and porcelain were brought to new levels of complexity. However, the military weakness of the Song army was observed by the Jin dynasty. In 1127, Emperor Emeritus Huizong, Emperor Qinzong of Song and the capital Bianjing were captured during the Jin\u2013Song wars. The remnants of the Song retreated to southern China and reestablished the Song at Jiankang.\nYuan.\nThe Mongol conquest of China began in 1205 with the campaigns against Western Xia by Genghis Khan, who also invaded Jin territories. In 1271, the Mongol leader Kublai Khan established the Yuan dynasty, which conquered the last remnant of the Song dynasty in 1279. Before the Mongol invasion, the population of Song China was 120 million citizens; this was reduced to 60 million by the time of the census in 1300. A peasant named Zhu Yuanzhang overthrew the Yuan in 1368 and founded the Ming dynasty as the Hongwu Emperor. Under the Ming dynasty, China enjoyed another golden age, developing one of the strongest navies in the world and a rich and prosperous economy amid a flourishing of art and culture. It was during this period that admiral Zheng He led the Ming treasure voyages throughout the Indian Ocean, reaching as far as East Africa.\nMing.\nIn the early Ming dynasty, China's capital was moved from Nanjing to Beijing. With the budding of capitalism, philosophers such as Wang Yangming critiqued and expanded Neo-Confucianism with concepts of individualism and equality of four occupations. The scholar-official stratum became a supporting force of industry and commerce in the tax boycott movements, which, together with the famines and defense against Japanese invasions of Korea (1592\u20131598) and Later Jin incursions led to an exhausted treasury. In 1644, Beijing was captured by a coalition of peasant rebel forces led by Li Zicheng. The Chongzhen Emperor committed suicide when the city fell. The Manchu Qing dynasty, then allied with Ming dynasty general Wu Sangui, overthrew Li's short-lived Shun dynasty and subsequently seized control of Beijing, which became the new capital of the Qing dynasty.\nQing.\nThe Qing dynasty, which lasted from 1644 until 1912, was the last imperial dynasty of China. The Ming-Qing transition (1618\u20131683) cost 25 million lives, but the Qing appeared to have restored China's imperial power and inaugurated another flowering of the arts. After the Southern Ming ended, the further conquest of the Dzungar Khanate added Mongolia, Tibet and Xinjiang to the empire. Meanwhile, China's population growth resumed and shortly began to accelerate. It is commonly agreed that pre-modern China's population experienced two growth spurts, one during the Northern Song period (960\u20131127), and other during the Qing period (around 1700\u20131830). By the High Qing era China was possibly the most commercialized country in the world, and imperial China experienced a second commercial revolution by the end of the 18th century. On the other hand, the centralized autocracy was strengthened in part to suppress anti-Qing sentiment with the policy of valuing agriculture and restraining commerce, like the \"Haijin\" during the early Qing period and ideological control as represented by the literary inquisition, causing some social and technological stagnation.\nFall of the Qing dynasty.\nIn the mid-19th century, the Opium Wars with Britain and France forced China to pay compensation, open treaty ports, allow extraterritoriality for foreign nationals, and cede Hong Kong to the British under the 1842 Treaty of Nanking, the first of what have been termed as the \"unequal treaties\". The First Sino-Japanese War (1894\u20131895) resulted in Qing China's loss of influence in the Korean Peninsula, as well as the cession of Taiwan to Japan. The Qing dynasty also began experiencing internal unrest in which tens of millions of people died, especially in the White Lotus Rebellion, the failed Taiping Rebellion that ravaged southern China in the 1850s and 1860s and the Dungan Revolt (1862\u20131877) in the northwest. The initial success of the Self-Strengthening Movement of the 1860s was frustrated by a series of military defeats in the 1880s and 1890s.\nIn the 19th century, the great Chinese diaspora began. Losses due to emigration were added to by conflicts and catastrophes such as the Northern Chinese Famine of 1876\u20131879, in which between 9 and 13 million people died. The Guangxu Emperor drafted a reform plan in 1898 to establish a modern constitutional monarchy, but these plans were thwarted by the Empress Dowager Cixi. The ill-fated anti-foreign Boxer Rebellion of 1899\u20131901 further weakened the dynasty. Although Cixi sponsored a program of reforms known as the late Qing reforms, the Xinhai Revolution of 1911\u20131912 ended the Qing dynasty and established the Republic of China. Puyi, the last Emperor, abdicated in 1912.\nEstablishment of the Republic and World War II.\nOn 1 January 1912, the Republic of China was established, and Sun Yat-sen of the Kuomintang (KMT) was proclaimed provisional president. In March 1912, the presidency was given to Yuan Shikai, a former Qing general who in 1915 proclaimed himself Emperor of China. In the face of popular condemnation and opposition from his own Beiyang Army, he was forced to abdicate and re-establish the republic in 1916. After Yuan Shikai's death in 1916, China was politically fragmented. Its Beijing-based government was internationally recognized but virtually powerless; regional warlords controlled most of its territory. During this period, China participated in World War I and saw a far-reaching popular uprising (the May Fourth Movement).\nIn the late 1920s, the Kuomintang under Chiang Kai-shek was able to reunify the country under its own control with a series of deft military and political maneuverings known collectively as the Northern Expedition. The Kuomintang moved the nation's capital to Nanjing and implemented \"political tutelage\", an intermediate stage of political development outlined in Sun Yat-sen's Three Principles of the People program for transforming China into a modern democratic state. The Kuomintang briefly allied with the Chinese Communist Party (CCP) during the Northern Expedition, though the alliance broke down in 1927 after Chiang violently suppressed the CCP and other leftists in Shanghai, marking the beginning of the Chinese Civil War. The CCP declared areas of the country as the Chinese Soviet Republic (Jiangxi Soviet) in November 1931 in Ruijin, Jiangxi. The Jiangxi Soviet was wiped out by the KMT armies in 1934, leading the CCP to initiate the Long March and relocate to Yan'an in Shaanxi. It would be the base of the communists before major combat in the Chinese Civil War ended in 1949.\nIn 1931, Japan invaded and occupied Manchuria. Japan invaded other parts of China in 1937, precipitating the Second Sino-Japanese War (1937\u20131945), a theater of World War II. The war forced an uneasy alliance between the Kuomintang and the CCP. Japanese forces committed numerous war atrocities against the civilian population; as many as 20 million Chinese civilians died. An estimated 40,000 to 300,000 Chinese were massacred in Nanjing alone during the Japanese occupation. China, along with the UK, the United States, and the Soviet Union, were recognized as the Allied \"Big Four\" in the Declaration by United Nations. Along with the other three great powers, China was one of the four major Allies of World War II, and was later considered one of the primary victors in the war. After the surrender of Japan in 1945, Taiwan, including the Penghu, was handed over to Chinese control; however, the validity of this handover is controversial.\nPeople's Republic.\nChina emerged victorious but war-ravaged and financially drained. The continued distrust between the Kuomintang and the Communists led to the resumption of civil war. Constitutional rule was established in 1947, but because of the ongoing unrest, many provisions of the ROC constitution were never implemented in mainland China. Afterwards, the CCP took control of most of mainland China, and the ROC government retreated offshore to Taiwan.\nOn 1 October 1949, CCP Chairman Mao Zedong formally proclaimed the People's Republic of China in Tiananmen Square, Beijing. In 1950, the PRC captured Hainan from the ROC and annexed Tibet. However, remaining Kuomintang forces continued to wage an insurgency in western China throughout the 1950s. The CCP consolidated its popularity among the peasants through the Land Reform Movement, which included the state-tolerated executions of between 1 and 2 million landlords by peasants and former tenants. Though the PRC initially allied closely with the Soviet Union, the relations between the two communist nations gradually deteriorated, leading China to develop an independent industrial system and its own nuclear weapons.\nThe Chinese population increased from 550 million in 1950 to 900 million in 1974. However, the Great Leap Forward, an idealistic massive industrialization project, resulted in an estimated 15 to 55 million deaths between 1959 and 1961, mostly from starvation. In 1964, China detonated its first atomic bomb. In 1966, Mao and his allies launched the Cultural Revolution, sparking a decade of political recrimination and social upheaval that lasted until Mao's death in 1976. In October 1971, the PRC replaced the ROC in the United Nations, and took its seat as a permanent member of the Security Council.\nReforms and contemporary history.\nAfter Mao's death, the Gang of Four was arrested by Hua Guofeng and held responsible for the Cultural Revolution. The Cultural Revolution was rebuked, with millions rehabilitated. Deng Xiaoping took power in 1978, and instituted large-scale political and economic reforms, together with the \"Eight Elders\", most senior and influential members of the party. The government loosened its control and the communes were gradually disbanded. Agricultural collectivization was dismantled and farmlands privatized. While foreign trade became a major focus, special economic zones (SEZs) were created. Inefficient state-owned enterprises (SOEs) were restructured and some closed. This marked China's transition away from planned economy. China adopted its current constitution on 4 December 1982.\nIn 1989, there were protests such those in Tiananmen Square, and then throughout the entire nation. Zhao Ziyang was put under house arrest for his sympathies to the protests and was replaced by Jiang Zemin. Jiang continued economic reforms, closing many SOEs and trimming down \"iron rice bowl\" (life-tenure positions). China's economy grew sevenfold during this time. British Hong Kong and Portuguese Macau returned to China in 1997 and 1999, respectively, as special administrative regions under the principle of one country, two systems. The country joined the World Trade Organization in 2001.At the 16th CCP National Congress in 2002, Hu Jintao succeeded Jiang as the general secretary. Under Hu, China maintained its high rate of economic growth, overtaking the United Kingdom, France, Germany and Japan to become the world's second-largest economy. However, the growth also severely impacted the country's resources and environment, and caused major social displacement. Xi Jinping succeeded Hu as paramount leader at the 18th CCP National Congress in 2012. Shortly after his ascension to power, Xi launched a vast anti-corruption crackdown, that prosecuted more than 2 million officials by 2022. During his tenure, Xi has consolidated power unseen since the initiation of economic and political reforms.\nGeography.\nChina's landscape is vast and diverse, ranging from the Gobi and Taklamakan Deserts in the arid north to the subtropical forests in the wetter south. The Himalaya, Karakoram, Pamir and Tian Shan mountain ranges separate China from much of South and Central Asia. The Yangtze and Yellow Rivers, the third- and sixth-longest in the world, respectively, run from the Tibetan Plateau to the densely populated eastern seaboard. China's coastline along the Pacific Ocean is long and is bounded by the Bohai, Yellow, East China and South China seas. China connects through the Kazakh border to the Eurasian Steppe.\nThe territory of China lies between latitudes 18\u00b0 and 54\u00b0 N, and longitudes 73\u00b0 and 135\u00b0 E. The geographical center of China is marked by the Center of the Country Monument at . China's landscapes vary significantly across its vast territory. In the east, along the shores of the Yellow Sea and the East China Sea, there are extensive and densely populated alluvial plains, while on the edges of the Inner Mongolian plateau in the north, broad grasslands predominate. Southern China is dominated by hills and low mountain ranges, while the central-east hosts the deltas of China's two major rivers, the Yellow River and the Yangtze River. Other major rivers include the Xi, Mekong, Brahmaputra and Amur. To the west sit major mountain ranges, most notably the Himalayas. High plateaus feature among the more arid landscapes of the north, such as the Taklamakan and the Gobi Desert. The world's highest point, Mount Everest (8,848\u00a0m), lies on the Sino-Nepalese border. The country's lowest point, and the world's third-lowest, is the dried lake bed of Ayding Lake (\u2212154\u00a0m) in the Turpan Depression.\nClimate.\nChina's climate is mainly dominated by dry seasons and wet monsoons, which lead to pronounced temperature differences between winter and summer. In the winter, northern winds coming from high-latitude areas are cold and dry; in summer, southern winds from coastal areas at lower latitudes are warm and moist.\nA major environmental issue in China is the continued expansion of its deserts, particularly the Gobi Desert. Although barrier tree lines planted since the 1970s have reduced the frequency of sandstorms, prolonged drought and poor agricultural practices have resulted in dust storms plaguing northern China each spring, which then spread to other parts of East Asia, including Japan and Korea. Water quality, erosion, and pollution control have become important issues in China's relations with other countries. Melting glaciers in the Himalayas could potentially lead to water shortages for hundreds of millions of people. According to academics, in order to limit climate change in China to electricity generation from coal in China without carbon capture must be phased out by 2045. With current policies, the GHG emissions of China will probably peak in 2025, and by 2030 they will return to 2022 levels. However, such pathway still leads to three-degree temperature rise.\nOfficial government statistics about Chinese agricultural productivity are considered unreliable, due to exaggeration of production at subsidiary government levels. Much of China has a climate very suitable for agriculture and the country has been the world's largest producer of rice, wheat, tomatoes, eggplant, grapes, watermelon, spinach, and many other crops. In 2021, 12 percent of global permanent meadows and pastures belonged to China, as well as 8% of global cropland.\nBiodiversity.\nChina is one of 17 megadiverse countries, lying in two of the world's major biogeographic realms: the Palearctic and the Indomalayan. By one measure, China has over 34,687 species of animals and vascular plants, making it the third-most biodiverse country in the world, after Brazil and Colombia. The country is a party to the Convention on Biological Diversity; its National Biodiversity Strategy and Action Plan was received by the convention in 2010.\nChina is home to at least 551 species of mammals (the third-highest in the world), 1,221 species of birds (eighth), 424 species of reptiles (seventh) and 333 species of amphibians (seventh). Wildlife in China shares habitat with, and bears acute pressure from, one of the world's largest population of humans. At least 840 animal species are threatened, vulnerable or in danger of local extinction, due mainly to human activity such as habitat destruction, pollution and poaching for food, fur and traditional Chinese medicine. Endangered wildlife is protected by law, and , the country has over 2,349 nature reserves, covering a total area of 149.95 million hectares, 15 percent of China's total land area. Most wild animals have been eliminated from the core agricultural regions of east and central China, but they have fared better in the mountainous south and west. The Baiji was confirmed extinct on 12 December 2006.\nChina has over 32,000 species of vascular plants, and is home to a variety of forest types. Cold coniferous forests predominate in the north of the country, supporting animal species such as moose and Asian black bear, along with over 120 bird species. The understory of moist conifer forests may contain thickets of bamboo. In higher montane stands of juniper and yew, the bamboo is replaced by rhododendrons. Subtropical forests, which are predominate in central and southern China, support a high density of plant species including numerous rare endemics. Tropical and seasonal rainforests, though confined to Yunnan and Hainan, contain a quarter of all the animal and plant species found in China. China has over 10,000 recorded species of fungi.\nEnvironment.\nIn the early 2000s, China has suffered from environmental deterioration and pollution due to its rapid pace of industrialization. Regulations such as the 1979 Environmental Protection Law are fairly stringent, though they are poorly enforced, frequently disregarded in favor of rapid economic development. China has the second-highest death toll because of air pollution, after India, with approximately 1 million deaths. Although China ranks as the highest CO emitting country, it only emits 8 tons of CO per capita, significantly lower than developed countries such as the United States (16.1), Australia (16.8) and South Korea (13.6). Greenhouse gas emissions by China are the world's largest. The country has significant water pollution problems; only 89.4% of China's national surface water was graded suitable for human consumption by the Ministry of Ecology and Environment in 2023.\nChina has prioritized clamping down on pollution, bringing a significant decrease in air pollution in the 2010s. In 2020, the Chinese government announced its aims for the country to reach its peak emissions levels before 2030, and achieve carbon neutrality by 2060 in line with the Paris Agreement, which, according to Climate Action Tracker, would lower the expected rise in global temperature by 0.2\u20130.3 degrees \u2013 \"the biggest single reduction ever estimated by the Climate Action Tracker\".\nChina is the world's leading investor in renewable energy and its commercialization, with $546\u00a0billion invested in 2022; it is a major manufacturer of renewable energy technologies and invests heavily in local-scale renewable energy projects. Long heavily relying on non-renewable energy sources such as coal, China's adaptation of renewable energy has increased significantly in recent years, with their share increasing from 26.3 percent in 2016 to 31.9 percent in 2022. In 2023, 60.5% of China's electricity came from coal (largest producer in the world), 13.2% from hydroelectric power (largest), 9.4% from wind (largest), 6.2% from solar energy (largest), 4.6% from nuclear energy (second-largest), 3.3% from natural gas (fifth-largest), and 2.2% from bioenergy (largest); in total, 31% of China's energy came from renewable energy sources. Despite its emphasis on renewables, China remains deeply connected to global oil markets and next to India, has been the largest importer of Russian crude oil in 2022.\nAccording to China's government, the forest coverage of the country grew from 10% of the overall territory in 1949 to 25% in 2024.\nPolitical geography.\nChina is the third-largest country in the world by land area after Russia, and the third- or fourth-largest country in the world by total area. China's total area is generally stated as being approximately . Specific area figures range from according to the \"Encyclop\u00e6dia Britannica\", to according to the \"UN Demographic Yearbook\", and \"The World Factbook\".China has the longest combined land border in the world, measuring and its coastline covers approximately from the mouth of the Yalu River (Amnok River) to the Gulf of Tonkin. China borders 14 nations and covers the bulk of East Asia, bordering Vietnam, Laos, and Myanmar in Southeast Asia; India, Bhutan, Nepal, Pakistan and Afghanistan in South Asia; Tajikistan, Kyrgyzstan and Kazakhstan in Central Asia; and Russia, Mongolia, and North Korea in Inner Asia and Northeast Asia. It is narrowly separated from Bangladesh and Thailand to the southwest and south, and has several maritime neighbors such as Japan, Philippines, Malaysia, and Indonesia.\nChina has resolved its land borders with 12 out of 14 neighboring countries, having pursued substantial compromises in most of them. China currently has a disputed land border with India and Bhutan. China is additionally involved in maritime disputes with multiple countries over territory in the East and South China Seas, such as the Senkaku Islands and the entirety of South China Sea Islands.\nGovernment and politics.\nThe People's Republic of China is a one-party state governed by the Chinese Communist Party (CCP). The CCP is officially guided by socialism with Chinese characteristics, which is Marxism adapted to Chinese circumstances. The Chinese constitution states that the PRC \"is a socialist state governed by a people's democratic dictatorship that is led by the working class and based on an alliance of workers and peasants\", that the state institutions \"shall practice the principle of democratic centralism\", and that \"the defining feature of socialism with Chinese characteristics is the leadership of the Chinese Communist Party.\"\nThe PRC officially terms itself as a democracy, using terms such as \"socialist consultative democracy\", and \"whole-process people's democracy\". However, the country is commonly described as an authoritarian one-party state and a dictatorship, with among the heaviest restrictions worldwide in many areas, most notably against freedom of the press, freedom of assembly, free formation of social organizations, freedom of religion and free access to the Internet. China has consistently been ranked amongst the lowest as an \"authoritarian regime\" by the Economist Intelligence Unit's Democracy Index, ranking at 148th out of 167 countries in 2023. Other sources suggest that terming China as \"authoritarian\" does not sufficiently account for the multiple consultation mechanisms that exist in Chinese government.\nChinese Communist Party.\nAccording to the CCP constitution, its highest body is the National Congress held every five years. The National Congress elects the Central Committee, who then elects the party's Politburo, Politburo Standing Committee and the general secretary (party leader), the top leadership of the country. The general secretary holds ultimate power and authority over party and state and serves as the informal paramount leader. The current general secretary is Xi Jinping, who took office on 15 November 2012. At the local level, the secretary of the CCP committee of a subdivision outranks the local government level; CCP committee secretary of a provincial division outranks the governor while the CCP committee secretary of a city outranks the mayor.\nGovernment.\nThe government in China is under the sole control of the CCP. The CCP controls appointments in government bodies, with most senior government officials being CCP members.\nThe National People's Congress (NPC), with nearly 3,000-members, is constitutionally the \"highest organ of state power\", though it has been also described as a \"rubber stamp\" body. The NPC meets annually, while the NPC Standing Committee, around 150 members elected from NPC delegates, meets every couple of months. Elections are indirect and not pluralistic, with nominations at all levels being controlled by the CCP. The NPC is dominated by the CCP, with another eight minor parties having nominal representation under the condition of upholding CCP leadership.\nThe president is elected by the NPC. The presidency is the ceremonial state representative, but not the constitutional head of state. The incumbent president is Xi Jinping, who is also the general secretary of the CCP and the chairman of the Central Military Commission, making him China's paramount leader and supreme commander of the Armed Forces. The premier is the head of government, with Li Qiang being the incumbent. The premier is officially nominated by the president and then elected by the NPC, and has generally been either the second- or third-ranking member of the Politburo Standing Committee (PSC). The premier presides over the State Council, China's cabinet, composed of four vice premiers, state councilors, and the heads of ministries and commissions. The Chinese People's Political Consultative Conference (CPPCC) is a political advisory body that is critical in China's \"united front\" system, which aims to gather non-CCP voices to support the CCP. Similar to the people's congresses, CPPCC's exist at various division, with the National Committee of the CPPCC being chaired by Wang Huning, fourth-ranking member of the PSC.\nThe governance of China is characterized by a high degree of political centralization but significant economic decentralization. Policy instruments or processes are often tested locally before being applied more widely, resulting in a policy that involves experimentation and feedback. Generally, central government leadership refrains from drafting specific policies, instead using the informal networks and site visits to affirm or suggest changes to the direction of local policy experiments or pilot programs. The typical approach is that central government leadership begins drafting formal policies, law, or regulations after policy has been developed at local levels.\nAdministrative divisions.\nThe PRC is constitutionally a unitary state divided into 23 provinces, five autonomous regions (each with a designated minority group), and four direct-administered municipalities\u2014collectively referred to as \"mainland China\"\u2014as well as the special administrative regions (SARs) of Hong Kong and Macau. The PRC regards the island of Taiwan as its Taiwan Province, Kinmen and Matsu as a part of Fujian Province and islands the ROC controls in the South China Sea as a part of Hainan Province and Guangdong Province, although all these territories are governed by the Republic of China (ROC). Geographically, all 31 provincial divisions of mainland China can be grouped into six regions: North China, East China, Southwestern China, Northwestern China, South Central China, and Northeast China.\nForeign relations.\nThe PRC has diplomatic relations with 179 United Nation members states and maintains embassies in 174. , China has one of the largest diplomatic networks of any country in the world. In 1971, the PRC replaced the Republic of China (ROC) as the sole representative of China in the United Nations and as one of the five permanent members of the United Nations Security Council. It is a member of intergovernmental organizations including the G20, the SCO, the BRICS, the East Asia Summit, and the APEC. China was also a former member and leader of the Non-Aligned Movement, and still considers itself an advocate for developing countries.\nThe PRC officially maintains the one-China principle, which holds the view that there is only one sovereign state in the name of China, represented by the PRC, and that Taiwan is part of that China. The unique status of Taiwan has led to countries recognizing the PRC to maintain unique \"one-China policies\" that differ from each other; some countries explicitly recognize the PRC's claim over Taiwan, while others, including the U.S. and Japan, only \"acknowledge\" the claim. Chinese officials have protested on numerous occasions when foreign countries have made diplomatic overtures to Taiwan, especially in the matter of armament sales. Most countries have switched recognition from the ROC to the PRC since the latter replaced the former in the UN in 1971.Much of current Chinese foreign policy is reportedly based on Premier Zhou Enlai's Five Principles of Peaceful Coexistence, and is also driven by the concept of \"harmony without uniformity\", which encourages diplomatic relations between states despite ideological differences. This policy may have led China to support or maintain close ties with states that are regarded as dangerous and repressive by Western nations, such as Sudan, North Korea and Iran. China's close relationship with Myanmar has involved support for its ruling governments as well as for its ethnic rebel groups, including the Arakan Army. China has a close political, economic and military relationship with Russia, and the two states often vote in unison in the UN Security Council. China's relationship with the United States is complex, and includes deep trade ties but significant political differences.\nSince the early 2000s, China has followed a policy of engaging with African nations for trade and bilateral co-operation. It maintains extensive and highly diversified trade links with the European Union, and became its largest trading partner for goods. China is increasing its influence in Central Asia and South Pacific. The country has strong trade ties with ASEAN countries and major South American economies, and is the largest trading partner of Brazil, Chile, Peru, Uruguay, Argentina, and several others.\nIn 2013, China initiated the Belt and Road Initiative (BRI), a large global infrastructure building initiative with funding on the order of $50\u2013100 billion per year. BRI could be one of the largest development plans in modern history. It expanded significantly over the next six years and, , included 138 countries and 30 international organizations. In addition to intensifying foreign policy relations, the focus is particularly on building efficient transport routes, especially the maritime Silk Road with its connections to East Africa and Europe. However many loans made under the program are unsustainable and China has faced a number of calls for debt relief from debtor nations.\nMilitary.\nThe People's Liberation Army (PLA) is considered one of the world's most powerful militaries and has rapidly modernized in the recent decades. It has also been accused of technology theft by some countries. Since 2024, it consists of four services: the Ground Force (PLAGF), the Navy (PLAN), the Air Force (PLAAF) and the Rocket Force (PLARF). It also has four independent arms: the Aerospace Force, the Cyberspace Force, the Information Support Force, and the Joint Logistics Support Force, the first three of which were split from the disbanded Strategic Support Force (PLASSF). Its nearly 2.2 million active duty personnel is the largest in the world. The PLA holds the world's third-largest stockpile of nuclear weapons, and the world's second-largest navy by tonnage. China's official military budget for 2023 totalled US$224 billion (1.55 trillion Yuan), the second-largest in the world, though SIPRI estimates that its real expenditure that year was US$296 billion, making up 12% of global military spending and accounting for 1.7% of the country's GDP. According to SIPRI, its military spending from 2012 to 2021 averaged US$215 billion per year or 1.7 per cent of GDP, behind only the United States at US$734 billion per year or 3.6 per cent of GDP. The PLA is commanded by the Central Military Commission (CMC) of the party and the state; though officially two separate organizations, the two CMCs have identical membership except during leadership transition periods and effectively function as one organization. The chairman of the CMC is the commander-in-chief of the PLA.\nSociopolitical issues and human rights.\nThe situation of human rights in China has attracted significant criticism from foreign governments, foreign press agencies, and non-governmental organizations, alleging widespread civil rights violations such as detention without trial, forced confessions, torture, restrictions of fundamental rights, and excessive use of the death penalty. Since its inception, Freedom House has ranked China as \"not free\" in its \"Freedom in the World\" survey, while Amnesty International has documented significant human rights abuses. The Chinese constitution states that the \"fundamental rights\" of citizens include freedom of speech, freedom of the press, the right to a fair trial, freedom of religion, universal suffrage, and property rights. However, in practice, these provisions do not afford significant protection against criminal prosecution by the state. China has limited protections regarding LGBT rights.\nAlthough some criticisms of government policies and the ruling CCP are tolerated, censorship of political speech and information are amongst the harshest in the world and routinely used to prevent collective action. China also has the most comprehensive and sophisticated Internet censorship regime in the world, with numerous websites being blocked. The government suppresses popular protests and demonstrations that it considers a potential threat to \"social stability\". China additionally uses a massive espionage network of cameras, facial recognition software, sensors, and surveillance of personal technology as a means of social control of persons living in the country.\nChina is regularly accused of large-scale repression and human rights abuses in Tibet and Xinjiang, where significant numbers of ethnic minorities reside, including violent police crackdowns and religious suppression. Since 2017, the Chinese government has been engaged in a harsh crackdown in Xinjiang, with around one million Uyghurs and other ethnic and religion minorities being detained in internment camps aimed at changing the political thinking of detainees, their identities, and their religious beliefs. According to Western reports, political indoctrination, torture, physical and psychological abuse, forced sterilization, sexual abuse, and forced labor are common in these facilities. According to a 2020 Foreign Policy report, China's treatment of Uyghurs meets the UN definition of genocide, while a separate UN Human Rights Office report said they could potentially meet the definitions for crimes against humanity. The Chinese authorities have also cracked down on dissent in Hong Kong, especially after the passage of a national security law in 2020.\nIn 2017 and 2020, the Pew Research Center ranked the severity of Chinese government restrictions on religion as being among the world's highest, despite ranking religious-related social hostilities in China as low in severity. The Global Slavery Index estimated that in 2016 more than 3.8 million people (0.25% of the population) were living in \"conditions of modern slavery\", including victims of human trafficking, forced labor, forced marriage, child labor, and state-imposed forced labor. The state-imposed re-education through labor (\"laojiao\") system was formally abolished in 2013, but it is not clear to what extent its practices have stopped. The much larger reform through labor (\"laogai\") system includes labor prison factories, detention centers, and re-education camps; the Laogai Research Foundation has estimated in June 2008 that there were nearly 1,422 of these facilities, though it cautioned that this number was likely an underestimate.\nPublic views of government.\nPolitical concerns in China include the growing gap between rich and poor and government corruption. Nonetheless, international surveys show the Chinese public have a high level of satisfaction with their government. These views are generally attributed to the material comforts and security available to large segments of the Chinese populace as well as the government's attentiveness and responsiveness. According to the World Values Survey (2022), 91% of Chinese respondents have significant confidence in their government. A Harvard University survey published in July 2020 found that citizen satisfaction with the government had increased since 2003, also rating China's government as more effective and capable than ever in the survey's history.\nEconomy.\nChina has the world's second-largest economy in terms of nominal GDP, and the world's largest in terms of purchasing power parity (PPP). , China accounts for around 18% of the global economy by nominal GDP. China is one of the world's fastest-growing major economies, with its economic growth having been almost consistently above 6 percent since the introduction of economic reforms in 1978. According to the World Bank, China's GDP grew from $150 billion in 1978 to $17.96 trillion by 2022. It ranks 64th by nominal GDP per capita, making it an upper-middle income country. Of the world's 500 largest companies, 135 are headquartered in China. As of at least 2024, China has the world's second-largest equity markets and futures markets, as well as the third-largest bond market.\nChina was one of the world's foremost economic powers throughout the arc of East Asian and global history. The country had one of the largest economies in the world for most of the past two millennia, during which it has seen cycles of prosperity and decline. Since economic reforms began in 1978, China has developed into a highly diversified economy and one of the most consequential players in international trade. Major sectors of competitive strength include manufacturing, retail, mining, steel, textiles, automobiles, energy generation, green energy, banking, electronics, telecommunications, real estate, e-commerce, and tourism. China has three out of the ten largest stock exchanges in the world\u2014Shanghai, Hong Kong and Shenzhen\u2014that together have a market capitalization of over $15.9 trillion, . China has three out of the world's ten most competitive financial centers according to the 2024 Global Financial Centres Index\u2014Shanghai, Hong Kong, and Shenzhen.\nModern-day China is often described as an example of state capitalism or party-state capitalism. The state dominates in strategic \"pillar\" sectors such as energy production and heavy industries, but private enterprise has expanded enormously, with around 30\u00a0million private businesses recorded in 2008. According to official statistics, privately owned companies constitute more than 60% of China's GDP.\nChina has been the world's largest manufacturing nation since 2010, after overtaking the U.S., which had been the largest for the previous hundred years. China has also been the second-largest in high-tech manufacturing country since 2012, according to US National Science Foundation. China is the second-largest retail market after the United States. China leads the world in e-commerce, accounting for over 37% of the global market share in 2021. China is the world's leader in electric vehicle consumption and production, manufacturing and buying half of all the plug-in electric cars (BEV and PHEV) in the world . China is also the leading producer of batteries for electric vehicles as well as several key raw materials for batteries.\nTourism.\nChina received 65.7\u00a0million international visitors in 2019, and in 2018 was the fourth-most-visited country in the world. It also experiences an enormous volume of domestic tourism; Chinese tourists made an estimated 6 billion travels within the country in 2019. China hosts the world's second-largest number of World Heritage Sites (56) after Italy, and is one of the most popular tourist destinations (first in the Asia-Pacific).\nWealth.\nChina accounted for 18.6% of the world's total wealth in 2022, second highest in the world after the U.S. China brought more people out of extreme poverty than any other country in history\u2014between 1978 and 2018, China reduced extreme poverty by 800 million. From 1990 to 2018, the proportion of the Chinese population living with an income of less than $1.90 per day (2011 PPP) decreased from 66.3% to 0.3%, the share living with an income of less than $3.20 per day from 90.0% to 2.9%, and the share living with an income of less than $5.50 per day decreased from 98.3% to 17.0%.\nFrom 1978 to 2018, the average standard of living multiplied by a factor of twenty-six. Wages in China have grown significantly in the last 40 years\u2014real (inflation-adjusted) wages grew seven-fold from 1978 to 2007. Per capita incomes have also risen significantly \u2013 when the PRC was founded in 1949, per capita income in China was one-fifth of the world average; per capita incomes now equal the world average itself. China's development is highly uneven; its major cities and coastal areas are far more prosperous than its rural and interior regions. It has a high level of economic inequality, which has increased quickly since the economic reforms. Income inequality decreased in the 2010s, and China's Gini coefficient was 0.357 in 2021.\nIn March 2024, China ranked second in the world, after the U.S., in total number of billionaires and total number of millionaires, with 473 Chinese billionaires and 6.2 million millionaires. In 2019, China overtook the U.S. as the home to the highest number of people who have a net personal wealth of at least $110,000, according to the global wealth report by Credit Suisse. China had 85 female billionaires , two-thirds of the global total. China has had the world's largest middle-class population since 2015; the middle-class grew to 500 million by 2024.\nChina in the global economy.\nChina has been a member of the WTO since 2001 and is the world's largest trading power. By 2016, China was the largest trading partner of 124 countries. China became the world's largest trading nation in 2013 by the sum of imports and exports, as well as the world's largest commodity importer, accounting for roughly 45% of maritime's dry-bulk market.\nChina's foreign exchange reserves reached US$3.246\u00a0trillion , making its reserves by far the world's largest. In 2022, China was amongst the world's largest recipient of inward foreign direct investment (FDI), attracting $180 billion, though most of these were speculated to be from Hong Kong. In 2021, China's foreign exchange remittances were $US53 billion making it the second-largest recipient of remittances in the world. China also invests abroad, with a total outward FDI of $147.9\u00a0billion in 2023, and a number of major takeovers of foreign firms by Chinese companies.\nEconomists have argued that the renminbi is undervalued, due to currency intervention from the Chinese government, giving China an unfair trade advantage. China has also been widely criticized for manufacturing large quantities of counterfeit goods. The U.S. government has also alleged that China does not respect intellectual property (IP) rights and steals IP through espionage operations. In 2020, Harvard University's Economic Complexity Index ranked complexity of China's exports 17th in the world, up from 24th in 2010.\nThe Chinese government has promoted the internationalization of the renminbi in order to wean itself off its dependence on the U.S. dollar as a result of perceived weaknesses of the international monetary system. The renminbi is a component of the IMF's special drawing rights and the world's fourth-most traded currency . However, partly due to capital controls that make the renminbi fall short of being a fully convertible currency, it remains far behind the Euro, the U.S. Dollar and the Japanese Yen in international trade volumes.\nScience and technology.\nHistorical.\nChina was a world leader in science and technology until the Ming dynasty. Ancient and medieval Chinese discoveries and inventions, such as papermaking, printing, the compass, and gunpowder (the Four Great Inventions), became widespread across East Asia, the Middle East and later Europe. Chinese mathematicians were the first to use negative numbers. By the 17th century, the Western World surpassed China in scientific and technological advancement. The causes of this early modern Great Divergence continue to be debated by scholars.\nAfter repeated military defeats by the European colonial powers and Imperial Japan in the 19th century, Chinese reformers began promoting modern science and technology as part of the Self-Strengthening Movement. After the Communists came to power in 1949, efforts were made to organize science and technology based on the model of the Soviet Union, in which scientific research was part of central planning. After Mao's death in 1976, science and technology were promoted as one of the Four Modernizations, and the Soviet-inspired academic system was gradually reformed.\nModern era.\nSince the end of the Cultural Revolution, China has made significant investments in scientific research and is quickly catching up with the U.S. in R&amp;D spending. China officially spent around 2.7% of its GDP on R&amp;D in 2024, totaling to around $496 billion. According to the World Intellectual Property Indicators, China received more applications than the U.S. did in 2018 and 2019 and ranked first globally in patents, utility models, trademarks, industrial designs, and creative goods exports in 2021. It was ranked 11th in the Global Innovation Index in 2024, a considerable improvement from its rank of 35th in 2013. Chinese supercomputers ranked among the fastest in the world. Its efforts to develop the most advanced semiconductors and jet engines have seen delays and setbacks.\nChina is developing its education system with an emphasis on science, technology, engineering, and mathematics (STEM). Its academic publication apparatus became the world's largest publisher of scientific papers in 2016. In 2022, China overtook the US in the Nature Index, which measures the share of published articles in leading scientific journals.\nSpace program.\nThe Chinese space program started in 1958 with some technology transfers from the Soviet Union. However, it did not launch the nation's first satellite until 1970 with the Dong Fang Hong I, which made China the fifth country to do so independently.\nIn 2003, China became the third country in the world to independently send humans into space with Yang Liwei's spaceflight aboard Shenzhou 5. As of 2023, eighteen Chinese nationals have journeyed into space, including two women. In 2011, China launched its first space station testbed, Tiangong-1. In 2013, a Chinese robotic rover \"Yutu\" successfully touched down on the lunar surface as part of the Chang'e 3 mission.\nIn 2019, China became the first country to land a probe\u2014Chang'e 4\u2014on the far side of the Moon. In 2020, Chang'e 5 successfully returned Moon samples to the Earth, making China the third country to do so independently. In 2021, China became the third country to land a spacecraft on Mars and the second one to deploy a rover (\"Zhurong\") on Mars. China completed its own modular space station, the Tiangong, in low Earth orbit on 3 November 2022. On 29 November 2022, China performed its first in-orbit crew handover aboard the \"Tiangong\".\nIn May 2023, China announced a plan to land humans on the Moon by 2030. To that end, China has been developing a lunar-capable super-heavy launcher, the Long March 10, a new crewed spacecraft, and a crewed lunar lander.\nChina sent Chang'e 6 on 3 May 2024, which conducted the first lunar sample return from Apollo Basin on the far side of the Moon. This is China's second lunar sample return mission, the first was achieved by Chang'e 5 from the lunar near side 4 years ago. It also carried a Chinese rover called \"Jinchan\" to conduct infrared spectroscopy of lunar surface and imaged Chang'e 6 lander on lunar surface. The lander-ascender-rover combination was separated with the orbiter and returner before landing on 1 June 2024, at 22:23 UTC. It landed on the Moon's surface on 1 June 2024. The ascender was launched back to lunar orbit on 3 June 2024, at 23:38 UTC, carrying samples collected by the lander, which later completed another robotic rendezvous, before docking in lunar orbit. The sample container was then transferred to the returner, which landed on Inner Mongolia in June 2024, completing China's far side extraterrestrial sample return mission.\nInfrastructure.\nAfter a decades-long infrastructural boom, China has produced numerous world-leading infrastructural projects: it has the largest high-speed rail network, the most supertall skyscrapers, the largest power plant (the Three Gorges Dam), the most extensive ultra-high-voltage transmission network and innovation infrastructure, and a global satellite navigation system with the largest number of satellites.\nTelecommunications.\nChina is the largest telecom market in the world and currently has the largest number of active cellphones of any country, with over 1.7 billion subscribers, . It has the largest number of internet and broadband users, with over 1.1 billion Internet users \u2014equivalent to around 78.6% of its population. By 2018, China had more than 1 billion 4G users, accounting for 40% of world's total. China is making rapid advances in 5G\u2014by late 2018, China had started large-scale and commercial 5G trials. , China had over 810 million 5G users and 3.38 million base stations installed.\nChina Mobile, China Unicom and China Telecom, are the three large providers of mobile and internet in China. China Telecom alone served more than 145 million broadband subscribers and 300 million mobile users; China Unicom had about 300 million subscribers; and China Mobile, the largest of them all, had 925 million users, . Combined, the three operators had over 3.4 million 4G base-stations in China. Several Chinese telecommunications companies, most notably Huawei and ZTE, have been accused of spying for the Chinese military.\nChina has developed its own satellite navigation system, dubbed BeiDou, which began offering commercial navigation services across Asia in 2012 as well as global services by the end of 2018. Beidou followed GPS and GLONASS as the third completed global navigation satellite.\nTransport.\nSince the late 1990s, China's national road network has been significantly expanded through the creation of a network of national highways and expressways. In 2022, China's highways had reached a total length of , making it the longest highway system in the world. China has the world's largest market for automobiles, having surpassed the United States in both auto sales and production. The country is the world's largest exporter of cars by number as of 2023. A side-effect of the rapid growth of China's road network has been a significant rise in traffic accidents. In urban areas, bicycles remain a common mode of transport, despite the increasing prevalence of automobiles \u2013 , there are approximately 200 million bicycles in China.\nChina's railways, which are operated by the state-owned China State Railway Group Company, are among the busiest in the world, handling a quarter of the world's rail traffic volume on only 6 percent of the world's tracks in 2006. , the country had of railways, the second-longest network in the world. The railways strain to meet enormous demand particularly during the Chinese New Year holiday, when the world's largest annual human migration takes place. China's high-speed rail (HSR) system started construction in the early 2000s. By the end of 2023, high speed rail in China had reached of dedicated lines alone, making it the longest HSR network in the world. Services on the Beijing\u2013Shanghai, Beijing\u2013Tianjin, and Chengdu\u2013Chongqing lines reach up to , making them the fastest conventional high speed railway services in the world. With an annual ridership of over 2.3 billion passengers in 2019, it is the world's busiest. The network includes the Beijing\u2013Guangzhou high-speed railway, the single longest HSR line in the world, and the Beijing\u2013Shanghai high-speed railway, which has three of longest railroad bridges in the world. The Shanghai maglev train, which reaches , is the fastest commercial train service in the world. Since 2000, the growth of rapid transit systems in Chinese cities has accelerated. , 55 Chinese cities have urban mass transit systems in operation. , China boasts the five longest metro systems in the world with the networks in Shanghai, Beijing, Guangzhou, Chengdu and Shenzhen being the largest.\nThe civil aviation industry in China is mostly state-dominated, with the Chinese government retaining a majority stake in the majority of Chinese airlines. The top three airlines in China, which collectively made up 71% of the market in 2018, are all state-owned. Air travel has expanded rapidly in the last decades, with the number of passengers increasing from 16.6 million in 1990 to 551.2 million in 2017. China had approximately 259 airports in 2024.\nChina has over 2,000 river and seaports, about 130 of which are open to foreign shipping. Of the fifty busiest container ports, 15 are located in China, of which the busiest is the Port of Shanghai, also the busiest port in the world. The country's inland waterways are the world's sixth-longest, and total .\nWater supply and sanitation.\nWater supply and sanitation infrastructure in China is facing challenges such as rapid urbanization, as well as water scarcity, contamination, and pollution. According to the Joint Monitoring Program for Water Supply and Sanitation, about 36% of the rural population in China still did not have access to improved sanitation in 2015. The ongoing South\u2013North Water Transfer Project intends to abate water shortage in the north.\nDemographics.\nThe 2020 Chinese census recorded the population as approximately 1,411,778,724. About 17.95% were 14 years old or younger, 63.35% were between 15 and 59 years old, and 18.7% were over 60 years old. Between 2010 and 2020, the average population growth rate was 0.53%.\nGiven concerns about population growth, China implemented a two-child limit during the 1970s, and, in 1979, began to advocate for an even stricter limit of one child per family. Beginning in the mid-1980s, however, given the unpopularity of the strict limits, China began to allow some major exemptions, particularly in rural areas, resulting in what was actually a \"1.5\"-child policy from the mid-1980s to 2015; ethnic minorities were also exempt from one-child limits. The next major loosening of the policy was enacted in December 2013, allowing families to have two children if one parent is an only child. In 2016, the one-child policy was replaced in favor of a two-child policy. A three-child policy was announced on 31 May 2021, due to population aging, and in July 2021, all family size limits as well as penalties for exceeding them were removed. In 2023, the total fertility rate was reported to be 1.09, ranking among the lowest in the world. In 2023, National Bureau of Statistics estimated that the population fell 850,000 from 2021 to 2022, the first decline since 1961.\nAccording to one group of scholars, one-child limits had little effect on population growth or total population size. However, these scholars have been challenged. The policy, along with traditional preference for boys, may have contributed to an imbalance in the sex ratio at birth. The 2020 census found that males accounted for 51.2% of the total population. However, China's sex ratio is more balanced than it was in 1953, when males accounted for 51.8% of the population.\nThe cultural preference for male children, combined with the one-child policy, led to an excess of female child orphans in China, and in the 1990s through around 2007, there was an active stream of adoptions of (mainly female) babies by American and other foreign parents. However, increased restrictions by the Chinese Government slowed foreign adoptions significantly in 2007 and again in 2015.\nUrbanization.\nChina has urbanized significantly in recent decades. The percent of the country's population living in urban areas increased from 20% in 1980 to over 67% in 2024. China has over 160 cities with a population of over one million, including the 17 megacities (cities with a population of over 10 million) of Chongqing, Shanghai, Beijing, Chengdu, Guangzhou, Shenzhen, Tianjin, Xi'an, Suzhou, Zhengzhou, Wuhan, Hangzhou, Linyi, Shijiazhuang, Dongguan, Qingdao and Changsha. The total permanent population of Chongqing, Shanghai, Beijing and Chengdu is above 20 million. Shanghai is China's most populous urban area while Chongqing is its largest city proper, the only city in China with a permanent population of over 30 million. The figures in the table below are from the 2020 census, and are only estimates of the urban populations within administrative city limits; a different ranking exists for total municipal populations. The large \"floating populations\" of migrant workers make conducting censuses in urban areas difficult; the figures below include only long-term residents.\nEthnic groups.\nChina legally recognizes 56 distinct ethnic groups, who comprise the \"Zhonghua minzu\". The largest of these nationalities are the Han Chinese, who constitute more than 91% of the total population. The Han Chinese \u2013 the world's largest single ethnic group \u2013 outnumber other ethnic groups in every place excluding Tibet, Xinjiang, Linxia, and autonomous prefectures like Xishuangbanna. Ethnic minorities account for less than 10% of the population of China, according to the 2020 census. Compared with the 2010 population census, the Han population increased by 60,378,693 persons, or 4.93%, while the population of the 55 national minorities combined increased by 11,675,179 persons, or 10.26%. The 2020 census recorded a total of 845,697 foreign nationals living in mainland China.\nLanguages.\nThere are as many as 292 living languages in China. The languages most commonly spoken belong to the Sinitic branch of the Sino-Tibetan language family, which contains Mandarin (spoken by 80% of the population), and other varieties of Chinese language: Jin, Wu, Min, Hakka, Yue, Xiang, Gan, Hui, Ping and unclassified Tuhua (Shaozhou Tuhua and Xiangnan Tuhua). Languages of the Tibeto-Burman branch, including Tibetan, Qiang, Naxi and Yi, are spoken across the Tibetan and Yunnan\u2013Guizhou Plateau. Other ethnic minority languages in southwestern China include Zhuang, Thai, Dong and Sui of the Tai-Kadai family, Miao and Yao of the Hmong\u2013Mien family, and Wa of the Austroasiatic family. Across northeastern and northwestern China, local ethnic groups speak Altaic languages including Manchu, Mongolian and several Turkic languages: Uyghur, Kazakh, Kyrgyz, Salar and Western Yugur. Korean is spoken natively along the border with North Korea. Sarikoli, the language of Tajiks in western Xinjiang, is an Indo-European language. Taiwanese indigenous peoples, including a small population on the mainland, speak Austronesian languages.\nStandard Chinese, a variety based on the Beijing dialect of Mandarin, is the national language of China, having de facto official status. It is used as a lingua franca between people of different linguistic backgrounds. In the autonomous regions of China, other languages may also serve as a lingua franca, such as Uyghur in Xinjiang, where governmental services in Uyghur are constitutionally guaranteed.\nReligion.\nFreedom of religion is guaranteed by China's constitution, although religious organizations that lack official approval can be subject to state persecution. The government of the country is officially atheist. Religious affairs and issues in the country are overseen by the National Religious Affairs Administration, under the United Front Work Department.\nOver the millennia, the Chinese civilization has been influenced by various religious movements. The \"three doctrines\" of Confucianism, Taoism, and Buddhism have historically shaped Chinese culture, enriching a theological and spiritual framework of traditional religion which harks back to the early Shang and Zhou dynasty. Chinese folk religion, which is framed by the three doctrines and by other traditions, consists in allegiance to the \"shen\", who can be deities of the surrounding nature or ancestral principles of human groups, concepts of civility, culture heroes, many of whom feature in Chinese mythology and history. Amongst the most popular cults of folk religion are those of the Yellow Emperor, embodiment of the God of Heaven and one of the two divine patriarchs of the Chinese people, of Mazu (goddess of the seas), Guandi (god of war and business), Caishen (god of prosperity and richness), Pangu and many others. In the early decades of the 21st century, the Chinese government has been engaged in a rehabilitation of folk cults\u2014formally recognizing them as \"folk beliefs\" as distinguished from doctrinal religions, and often reconstructing them into forms of \"highly curated\" civil religion\u2014as well as in a national and international promotion of Buddhism. China is home to many of the world's tallest religious statues, representing either deities of Chinese folk religion or enlightened beings of Buddhism; the tallest of all is the Spring Temple Buddha in Henan.\nStatistics on religious affiliation in China are difficult to gather due to complex and varying definitions of religion and the diffusive nature of Chinese religious traditions. Scholars note that in China there is no clear boundary between the three doctrines and local folk religious practices. Chinese religions or some of their currents are also definable as non-theistic and humanistic, since they do not hold that divine creativity is completely transcendent, but that it is inherent in the world and in particular in the human being. According to studies published in 2023, compiling demographic analyses conducted throughout the 2010s and the early 2020s, 70% of the Chinese population believed in or practiced Chinese folk religion\u2014among them, with an approach of non-exclusivity, 33.4% may be identified as Buddhists, 19.6% as Taoists, and 17.7% as adherents of other types of folk religion. Of the remaining population, 25.2% are fully non-believers or atheists, 2.5% are adherents of Christianity, and 1.6% are adherents of Islam. Chinese folk religion also comprises a variety of salvationist doctrinal organized movements which emerged since the Song dynasty. There are also ethnic minorities in China who maintain their own indigenous religions, while major religions characteristic of specific ethnic groups include Tibetan Buddhism among Tibetans, Mongols and Yugurs, and Islam among the Hui, Uyghur, Kazakh, and Kyrgyz peoples, and other ethnicities in the northern and northwestern regions of the country.\nEducation.\nCompulsory education in China comprises primary and junior secondary school, which together last for nine years from the age of 6 and 15. The Gaokao, China's national university entrance exam, is a prerequisite for entrance into most higher education institutions. Vocational education is available to students at the secondary and tertiary level. More than 10 million Chinese students graduated from vocational colleges every year. In 2023, about 91.8 percent of students continued their education at a three-year senior secondary school, while 60.2 percent of secondary school graduates were enrolled in higher education.\nChina has the largest education system in the world, with about 291 million students and 18.92 million full-time teachers in over 498,300 schools in 2023. Annual education investment went from less than US$50 billion in 2003 to more than US$817 billion in 2020. However, there remains an inequality in education spending. In 2010, the annual education expenditure per secondary school student in Beijing totalled \u00a520,023, while in Guizhou, one of the poorest provinces, it only totalled \u00a53,204. China's literacy rate has grown dramatically, from only 20% in 1949 and 65.5% in 1979, to 97% of the population over age 15 in 2020.\n, China has over 3,074 universities, with over 47.6 million students enrolled in mainland China, giving China the largest higher education system in the world. , China had the world's highest number of top universities. Currently, China trails only the United States and the United Kingdom in terms of representation on lists of the top 200 universities according to the 2023 \"Aggregate Ranking of Top Universities\", a composite ranking system of three world-most followed university rankings (ARWU+QS+THE). China is home to two of the highest-ranking universities (Tsinghua University and Peking University) in Asia and emerging economies, according to the Times Higher Education World University Rankings and the Academic Ranking of World Universities. These universities are members of the C9 League, an alliance of elite Chinese universities offering comprehensive and leading education.\nHealth.\nThe National Health Commission, together with its counterparts in the local commissions, oversees the health needs of the population. An emphasis on public health and preventive medicine has characterized Chinese health policy since the early 1950s. The Communist Party started the Patriotic Health Campaign, which was aimed at improving sanitation and hygiene, as well as treating and preventing several diseases. Diseases such as cholera, typhoid and scarlet fever, which were previously rife in China, were nearly eradicated by the campaign.\nAfter Deng Xiaoping began instituting economic reforms in 1978, the health of the Chinese public improved rapidly because of better nutrition, although many of the free public health services provided in the countryside disappeared. Healthcare in China became mostly privatized, and experienced a significant rise in quality. In 2009, the government began a three-year large-scale healthcare provision initiative worth US$124 billion. By 2011, the campaign resulted in 95% of China's population having basic health insurance coverage. By 2022, China had established itself as a key producer and exporter of pharmaceuticals, producing around 40 percent of active pharmaceutical ingredients in 2017.\n, the life expectancy at birth exceeds 78 years. , the infant mortality rate is 5 per thousand. Both have improved significantly since the 1950s. Rates of stunting, a condition caused by malnutrition, have declined from 33.1% in 1990 to 9.9% in 2010. Despite significant improvements in health and the construction of advanced medical facilities, China has several emerging public health problems, such as respiratory illnesses caused by widespread air pollution, hundreds of millions of cigarette smokers, and an increase in obesity among urban youths. In 2010, air pollution caused 1.2 million premature deaths in China. Chinese mental health services are inadequate. China's large population and densely populated cities have led to serious disease outbreaks, such as SARS in 2003, although this has since been largely contained. The COVID-19 pandemic was first identified in Wuhan in December 2019; pandemic led the government to enforce strict public health measures intended to completely eradicate the virus, a goal that was eventually abandoned in December 2022 after protests against the policy.\nCulture and society.\nSince ancient times, Chinese culture has been heavily influenced by Confucianism. Chinese culture, in turn, has heavily influenced East Asia and Southeast Asia. For much of the country's dynastic era, opportunities for social advancement could be provided by high performance in the prestigious imperial examinations, which have their origins in the Han dynasty. The literary emphasis of the exams affected the general perception of cultural refinement in China, such as the belief that calligraphy, poetry and painting were higher forms of art than dancing or drama. Chinese culture has long emphasized a sense of deep history and a largely inward-looking national perspective. Examinations and a culture of merit remain greatly valued in China today.\nToday, the Chinese government has accepted numerous elements of traditional Chinese culture as being integral to Chinese society. With the rise of Chinese nationalism and the end of the Cultural Revolution, various forms of traditional Chinese art, literature, music, film, fashion and architecture have seen a vigorous revival, and folk and variety art in particular have sparked interest nationally and even worldwide. Access to foreign media remains heavily restricted.\nArchitecture.\nChinese architecture has developed over millennia in China and has remained a vestigial source of perennial influence on the development of East Asian architecture, including in Japan, Korea, and Mongolia. and minor influences on the architecture of Southeast and South Asia including the countries of Malaysia, Singapore, Indonesia, Sri Lanka, Thailand, Laos, Cambodia, Vietnam and the Philippines.\nChinese architecture is characterized by bilateral symmetry, use of enclosed open spaces, feng shui (e.g. directional hierarchies), a horizontal emphasis, and an allusion to various cosmological, mythological or in general symbolic elements. Chinese architecture traditionally classifies structures according to type, ranging from \"pagodas\" to palaces.\nChinese architecture varies widely based on status or affiliation, such as whether the structures were constructed for emperors, commoners, or for religious purposes. Other variations in Chinese architecture are shown in vernacular styles associated with different geographic regions and different ethnic heritages, such as the stilt houses in the south, the Yaodong buildings in the northwest, the yurt buildings of nomadic people, and the Siheyuan buildings in the north.\nLiterature.\nChinese literature has its roots in the Zhou dynasty's literary tradition. The classical texts of China encompass a wide range of thoughts and subjects, such as the calendar, military, astrology, herbology, and geography, as well as many others. Among the most significant early works are the \"I Ching\" and the \"Shujing\", which are part of the Four Books and Five Classics. These texts were the cornerstone of the Confucian curriculum sponsored by the state throughout the dynastic periods. Inherited from the \"Classic of Poetry\", classical Chinese poetry developed to its floruit during the Tang dynasty. Li Bai and Du Fu opened the forking ways for the poetic circles through romanticism and realism respectively. Chinese historiography began with the \"Shiji\", the overall scope of the historiographical tradition in China is termed the Twenty-Four Histories, which set a vast stage for Chinese fictions along with Chinese mythology and folklore. Pushed by a burgeoning citizen class in the Ming dynasty, Chinese classical fiction rose to a boom of the historical, town and gods and demons fictions as represented by the Four Great Classical Novels which include \"Water Margin\", \"Romance of the Three Kingdoms\", \"Journey to the West\" and \"Dream of the Red Chamber\". Along with the wuxia fictions of Jin Yong and Liang Yusheng, it remains an enduring source of popular culture in the Chinese sphere of influence.\nIn the wake of the New Culture Movement after the end of the Qing dynasty, Chinese literature embarked on a new era with written vernacular Chinese for ordinary citizens. Hu Shih and Lu Xun were pioneers in modern literature. Various literary genres, such as misty poetry, scar literature, young adult fiction and the xungen literature, which is influenced by magic realism, emerged following the Cultural Revolution. Mo Yan, a xungen literature author, was awarded the Nobel Prize in Literature in 2012.\nMusic.\nChinese music covers a highly diverse range of music from traditional music to modern music. Chinese music dates back before the pre-imperial times. Traditional Chinese musical instruments were traditionally grouped into eight categories known as \"bayin\" (\u516b\u97f3). Traditional Chinese opera is a form of musical theatre in China originating thousands of years and has regional style forms such as Beijing and Cantonese opera. Chinese pop (C-Pop) includes mandopop and cantopop. Chinese hip hop and Hong Kong hip hop have become popular.\nFashion.\nHanfu is the historical clothing of the Han people in China. The qipao or cheongsam is a popular Chinese female dress. The hanfu movement has been popular in contemporary times and seeks to revitalize Hanfu clothing. China Fashion Week is the country's only national-level fashion festival.\nCinema.\nCinema was first introduced to China in 1896 and the first Chinese film, \"Dingjun Mountain,\" was released in 1905. China has the largest number of movie screens in the world since 2016; China became the largest cinema market in 2020. The top three highest-grossing films in China were \"The Battle at Lake Changjin\" (2021), \"Wolf Warrior 2\" (2017), and \"Hi, Mom\" (2021).\nCuisine.\nChinese cuisine is highly diverse, drawing on several millennia of culinary history and geographical variety, in which the most influential are known as the \"Eight Major Cuisines\", including Sichuan, Cantonese, Jiangsu, Shandong, Fujian, Hunan, Anhui, and Zhejiang cuisines. Chinese cuisine is known for its breadth of cooking methods and ingredients. China's staple food is rice in the northeast and south, and wheat-based breads and noodles in the north. Bean products such as tofu and soy milk remain a popular source of protein. Pork is now the most popular meat in China, accounting for about three-fourths of the country's total meat consumption. There is also the vegetarian Buddhist cuisine and the pork-free Chinese Islamic cuisine. Chinese cuisine, due to the area's proximity to the ocean and milder climate, has a wide variety of seafood and vegetables. Offshoots of Chinese food, such as Hong Kong cuisine and American Chinese cuisine, have emerged in the Chinese diaspora.\nSports.\nChina has one of the oldest sporting cultures. There is evidence that archery (\"sh\u00e8ji\u00e0n\") was practiced during the Western Zhou dynasty. Swordplay (\"ji\u00e0nsh\u00f9\") and \"cuju\", a sport loosely related to association football date back to China's early dynasties as well.\nPhysical fitness is widely emphasized in Chinese culture, with morning exercises such as \"qigong\" and tai chi widely practiced, and commercial gyms and private fitness clubs are gaining popularity. Basketball is the most popular spectator sport in China. The Chinese Basketball Association and the American National Basketball Association also have a huge national following amongst the Chinese populace, with native-born and NBA-bound Chinese players and well-known national household names such as Yao Ming and Yi Jianlian being held in high esteem. China's professional football league, known as Chinese Super League, is the largest football market in East Asia. Other popular sports include martial arts, table tennis, badminton, swimming and snooker. China is home to a huge number of cyclists, with an estimated 470 million bicycles . China has the world's largest esports market. Many more traditional sports, such as dragon boat racing, Mongolian-style wrestling and horse racing are also popular.\nChina has participated in the Olympic Games since 1932, although it has only participated as the PRC since 1952. China hosted the 2008 Summer Olympics in Beijing, where its athletes received 48 gold medals \u2013 the highest number of any participating nation that year. China also won the most medals at the 2012 Summer Paralympics, with 231 overall, including 95 gold. In 2011, Shenzhen hosted the 2011 Summer Universiade. China hosted the 2013 East Asian Games in Tianjin and the 2014 Summer Youth Olympics in Nanjing, the first country to host both regular and Youth Olympics. Beijing and its nearby city Zhangjiakou collaboratively hosted the 2022 Winter Olympics, making Beijing the first dual Olympic city by holding both the Summer Olympics and the Winter Olympics. China hosted the Asian Games in 1990 (Beijing), 2010 (Guangzhou), and 2023 (Hangzhou)."}
{"id": "5407", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=5407", "title": "California", "text": "California () is a state in the Western Region of the United States that lies on the Pacific Coast. It borders Oregon to the north, Nevada and Arizona to the east, and shares an international border with the Mexican state of Baja California to the south. With over 39million residents across an area of , it is the most populous U.S. state, the third-largest by area, and most populated subnational entity in North America.\nPrior to European colonization, California was one of the most culturally and linguistically diverse areas in pre-Columbian North America. European exploration in the 16th and 17th centuries led to the colonization by the Spanish Empire. The area became a part of Mexico in 1821, following its successful war for independence, but was ceded to the United States in 1848 after the Mexican\u2013American War. The California gold rush started in 1848 and led to social and demographic changes, including depopulation of Indigenous tribes.\nThe western portion of Alta California was then organized and admitted as the 31st state in 1850, as a free state, following the Compromise of 1850.\nThe Greater Los Angeles and San Francisco Bay areas are the nation's second- and fifth-most populous urban regions, with 19 million and 10 million residents respectively. Los Angeles is the state's most populous city and the nation's second-most; California's capital is Sacramento. Part of the Californias region of North America, the state's diverse geography ranges from the Pacific Coast and metropolitan areas in the west to the Sierra Nevada mountains in the east, and from the redwood and Douglas fir forests in the northwest to the Mojave Desert in the southeast. Two-thirds of the nation's earthquake risk lies in California. The Central Valley, a fertile agricultural area, dominates the state's center. The large size of the state results in climates that vary from moist temperate rainforest in the north to arid desert in the interior, as well as snowy alpine in the mountains. Droughts and wildfires are an ongoing issue, while simultaneously, atmospheric rivers are turning increasingly prevalent and leading to intense flooding events\u2014especially in the winter.\nCalifornia has the nation's largest economy with a gross state product of $4.132 trillion as of Q3 2024. It is the world's largest sub-national economy, and would by most estimations rank 5th globally by nominal GDP. The state also leads the nation in agricultural output, led by its production of dairy, almonds, and grapes. With the busiest port in the country (Los Angeles), California plays a pivotal role in the global supply chain, hauling in about 40% of goods imported to the US. Notable contributions to popular culture, ranging from entertainment, sports, music, and fashion, have their origins in California. California is the home of Hollywood, the oldest and one of the largest film industries in the world, profoundly influencing global entertainment. The San Francisco Bay and the Greater Los Angeles areas are seen as the centers of the global technology and U.S. film industries, respectively.\nEtymology.\nThe Spaniards gave the name to the peninsula of Baja California (in modern-day Mexico). As Spanish explorers and settlers moved north and inland, the region known as \"California\", or \"Las Californias\", grew. Eventually it included lands north of the peninsula, Alta California, part of which became the present-day U.S. state of California.\nA 2017 state legislative document states, \"Numerous theories exist as to the origin and meaning of the word 'California, and that all anyone knows is the name was added to a map by 1541 \"presumably by a Spanish navigator.\"\nThe name most likely derived from the mythical island of California in the fictional story of Queen Calafia, as recorded in a 1510 work \"The Adventures of Esplandi\u00e1n\" by Garci Rodr\u00edguez de Montalvo. Queen Calafia's kingdom was said to be a remote land rich in gold and pearls, inhabited by beautiful Black women who wore gold armor and lived like Amazons, as well as griffins and other strange beasts.\nAbbreviations of the state's name include CA, Cal., Calif., Califas, and US-CA.\nHistory.\nIndigenous tribes.\nCalifornia was one of the most culturally and linguistically diverse areas in pre-Columbian North America. Historians generally agree that there were at least 300,000 people living in California prior to European colonization. The Indigenous peoples of California included more than 70 distinct ethnic groups, inhabiting environments ranging from mountains and deserts to islands and redwood forests.\nLiving in these diverse geographic areas, the indigenous peoples developed complex forms of ecosystem management, including forest gardening to ensure the regular availability of food and medicinal plants. This was a form of sustainable agriculture. To mitigate destructive large wildfires from ravaging the natural environment, indigenous peoples developed a practice of controlled burning. This practice was recognized for its benefits by the California government in 2022.\nThese groups were also diverse in their political organization, with bands, tribes, villages, and, on the resource-rich coasts, large chiefdoms, such as the Chumash, Pomo and Salinan. Trade, intermarriage, craft specialists, and military alliances fostered social and economic relationships between many groups. Although nations would sometimes war, most armed conflicts were between groups of men for vengeance. Acquiring territory was not usually the purpose of these small-scale battles.\nMen and women generally had different roles in society. Women were often responsible for weaving, harvesting, processing, and preparing food, while men for hunting and other forms of physical labor. Most societies also had roles for people whom the Spanish referred to as \"joyas\", who they saw as \"men who dressed as women\". \"Joyas\" were responsible for death, burial, and mourning rituals, and they performed women's social roles. Indigenous societies had terms such as two-spirit to refer to them. The Chumash referred to them as \"'aqi.\" The early Spanish settlers detested and sought to eliminate them.\nSpanish period.\nThe first Europeans to explore the coast of California were the members of a Spanish maritime expedition led by Portuguese captain Juan Rodr\u00edguez Cabrillo in 1542. Cabrillo was commissioned by Antonio de Mendoza, the Viceroy of New Spain, to lead an expedition up the Pacific coast in search of trade opportunities; they entered San Diego Bay on September 28, 1542, and reached at least as far north as San Miguel Island. Privateer and explorer Francis Drake explored and claimed an undefined portion of the California coast in 1579, landing north of the future city of San Francisco. The first Asians to set foot on what would be the United States occurred in 1587, when Filipino sailors arrived in Spanish ships at Morro Bay. Coincidentally the descendants of the Muslim Caliph Hasan ibn Ali in formerly Islamic Manila and had converted, then mixed Christianity with Islam, upon Spanish conquest, transited through California (Named after a Caliph) on their way to Guerrero, Mexico where they played a future role in the wars of independence. Sebasti\u00e1n Vizca\u00edno explored and mapped the coast of California in 1602 for New Spain, putting ashore in Monterey. Despite the on-the-ground explorations of California in the 16th century, Rodr\u00edguez's idea of California as an island persisted. Such depictions appeared on many European maps well into the 18th century.\nThe Portol\u00e1 expedition of 1769\u201370 was a pivotal event in the Spanish colonization of California, resulting in the establishment of numerous missions, presidios, and pueblos. The military and civil contingent of the expedition was led by Gaspar de Portol\u00e1, who traveled over land from Sonora into California, while the religious component was headed by Jun\u00edpero Serra, who came by sea from Baja California. In 1769, Portol\u00e1 and Serra established Mission San Diego de Alcal\u00e1 and the Presidio of San Diego, the first religious and military settlements founded by the Spanish in California. By the end of the expedition in 1770, they would establish the Presidio of Monterey and Mission San Carlos Borromeo de Carmelo on Monterey Bay.\nAfter the Portol\u00e0 expedition, Spanish missionaries led by Father-President Serra set out to establish 21 Spanish missions of California along El Camino Real (\"The Royal Road\") and along the California coast, 16 sites of which having been chosen during the Portol\u00e1 expedition. Numerous major cities in California grew out of missions, including San Francisco (Mission San Francisco de As\u00eds), San Diego (Mission San Diego de Alcal\u00e1), Ventura (Mission San Buenaventura), and Santa Barbara (Mission Santa Barbara), among others.\nJuan Bautista de Anza led a similarly important expedition throughout California in 1775\u201376, which would extend deeper into the interior and north of California. The Anza expedition selected numerous sites for missions, presidios, and pueblos, which subsequently would be established by settlers. Gabriel Moraga, a member of the expedition, would also christen many of California's prominent rivers with their names in 1775\u20131776, such as the Sacramento River and the San Joaquin River. After the expedition, Gabriel's son, Jos\u00e9 Joaqu\u00edn Moraga, would found the pueblo of San Jose in 1777, making it the first civilian-established city in California.\nDuring this same period, sailors from the Russian Empire explored along the northern coast of California. In 1812, the Russian-American Company established a trading post and small fortification at Fort Ross on the North Coast. Fort Ross was primarily used to supply Russia's Alaskan colonies with food supplies. The settlement did not meet much success, failing to attract settlers or establish long term trade viability, and was abandoned by 1841.\nDuring the War of Mexican Independence, Alta California was largely unaffected and uninvolved in the revolution, though many Californios supported independence from Spain, which many believed had neglected California and limited its development. Spain's trade monopoly on California had limited local trade prospects. Following Mexican independence, California ports were freely able to trade with foreign merchants. Governor Pablo Vicente de Sol\u00e1 presided over the transition from Spanish colonial rule to independent Mexican rule.\nMexican period.\nIn 1821, the Mexican War of Independence gave the Mexican Empire (which included California) independence from Spain. For the next 25 years, Alta California remained a remote, sparsely populated, northwestern administrative district of the newly independent country of Mexico, which shortly after independence became a republic. The missions, which controlled most of the best land in the state, were secularized by 1834 and became the property of the Mexican government. The governor granted many square leagues of land to others with political influence. These huge \"ranchos\" or cattle ranches emerged as the dominant institutions of Mexican California. The ranchos developed under ownership by Californios (Hispanics native of California) who traded cowhides and tallow with Boston merchants. Beef did not become a commodity until the 1849 California Gold Rush.\nFrom the 1820s, trappers and settlers from the United States and Canada began to arrive in Northern California. These new arrivals used the Siskiyou Trail, California Trail, Oregon Trail and Old Spanish Trail to cross the rugged mountains and harsh deserts in and surrounding California. The early government of the newly independent Mexico was highly unstable, and in a reflection of this, from 1831 onwards, California also experienced a series of armed disputes, both internal and with the central Mexican government. During this tumultuous political period Juan Bautista Alvarado was able to secure the governorship during 1836\u20131842. The military action which first brought Alvarado to power had momentarily declared California to be an independent state, and had been aided by Anglo-American residents of California, including Isaac Graham. In 1840, one hundred of those residents who did not have passports were arrested, leading to the Graham Affair, which was resolved in part with the intercession of Royal Navy officials.\nOne of the largest ranchers in California was John Marsh. After failing to obtain justice against squatters on his land from the Mexican courts, he determined that California should become part of the United States. Marsh conducted a letter-writing campaign espousing the California climate, the soil, and other reasons to settle there, as well as the best route to follow, which became known as \"Marsh's route\". His letters were read, reread, passed around, and printed in newspapers throughout the country, and started the first wagon trains rolling to California. After ushering in the period of organized emigration to California, Marsh became involved in a military battle between the much-hated Mexican general, Manuel Micheltorena and the California governor he had replaced, Juan Bautista Alvarado. At the Battle of Providencia near Los Angeles, he convinced each side that they had no reason to be fighting each other. As a result of Marsh's actions, they abandoned the fight, Micheltorena was defeated, and California-born Pio Pico was returned to the governorship. This paved the way to California's ultimate acquisition by the United States.\nU.S. conquest and the California Republic.\nIn 1846, a group of American settlers in and around Sonoma rebelled against Mexican rule during the Bear Flag Revolt. Afterward, rebels raised the Bear Flag (featuring a bear, a star, a red stripe and the words \"California Republic\") at Sonoma. The Republic's only president was William B. Ide, who played a pivotal role during the Bear Flag Revolt. This revolt by American settlers served as a prelude to the later American military invasion of California and was closely coordinated with nearby American military commanders.\nThe California Republic was short-lived; the same year marked the outbreak of the Mexican\u2013American War (1846\u20131848).\nCommodore John D. Sloat of the United States Navy sailed into Monterey Bay in 1846 and began the U.S. military invasion of California, with Northern California capitulating in less than a month to the United States forces. In Southern California, Californios continued to resist American forces. Notable military engagements of the conquest include the Battle of San Pasqual and the Battle of Dominguez Rancho in Southern California, as well as the Battle of Ol\u00f3mpali and the Battle of Santa Clara in Northern California. After a series of defensive battles in the south, the Treaty of Cahuenga was signed by the Californios on January 13, 1847, securing a censure and establishing de facto American control in California.\nEarly American period.\nFollowing the Treaty of Guadalupe Hidalgo (February 2, 1848) that ended the war, the westernmost portion of the annexed Mexican territory of Alta California soon became the American state of California, and the remainder of the old territory was then subdivided into the new American Territories of Arizona, Nevada, Colorado and Utah. The even more lightly populated and arid lower region of old Baja California remained as a part of Mexico. In 1846, the total settler population of the western part of the old Alta California had been estimated to be no more than 8,000, plus about 100,000 Native Americans, down from about 300,000 before Hispanic settlement in 1769.\nIn 1848, only one week before the official American annexation of the area, gold was discovered in California, this being an event which was to forever alter both the state's demographics and its finances. Soon afterward, a massive influx of immigration into the area resulted, as prospectors and miners arrived by the thousands. The population burgeoned with United States citizens, Europeans, Middle Easterns, Chinese and other immigrants during the great California gold rush. By the time of California's application for statehood in 1850, the settler population of California had multiplied to 100,000. By 1854, more than 300,000 settlers had come. Between 1847 and 1870, the population of San Francisco increased from 500 to 150,000. \nThe seat of government for California under Spanish and later Mexican rule had been located in Monterey from 1777 until 1845. Pio Pico, the last Mexican governor of Alta California, had briefly moved the capital to Los Angeles in 1845. The United States consulate had also been located in Monterey, under consul Thomas O. Larkin.\nIn 1849, a state Constitutional Convention was first held in Monterey. Among the first tasks of the convention was a decision on a location for the new state capital. The first full legislative sessions were held in San Jose (1850\u20131851). Subsequent locations included Vallejo (1852\u20131853), and nearby Benicia (1853\u20131854); these locations eventually proved to be inadequate as well. The capital has been located in Sacramento since 1854 with only a short break in 1862 when legislative sessions were held in San Francisco due to flooding in Sacramento.\nOnce the state's Constitutional Convention had finalized its state constitution, it applied to the U.S. Congress for admission to statehood. On September 9, 1850, as part of the Compromise of 1850, California became a free state and September9 a state holiday.\nDuring the American Civil War (1861\u20131865), California sent gold shipments eastward to Washington in support of the Union. However, due to the existence of a large contingent of pro-South sympathizers within the state, the state was not able to muster any full military regiments to send eastwards to officially serve in the Union war effort. Still, several smaller military units within the Union army, such as the \"California 100 Company\", were unofficially associated with the state of California due to a majority of their members being from California.\nAt the time of California's admission into the Union, travel between California and the rest of the continental United States had been a time-consuming and dangerous feat. Nineteen years later, and seven years after it was greenlighted by President Lincoln, the first transcontinental railroad was completed in 1869. California was then reachable from the eastern States in a week's time.\nMuch of the state was extremely well suited to fruit cultivation and agriculture in general. Vast expanses of wheat, other cereal crops, vegetable crops, cotton, and nut and fruit trees were grown (including oranges in Southern California), and the foundation was laid for the state's prodigious agricultural production in the Central Valley and elsewhere.\nIn the nineteenth century, a large number of migrants from China traveled to the state as part of the Gold Rush or to seek work. Even though the Chinese proved indispensable in building the transcontinental railroad from California to Utah, perceived job competition with the Chinese led to anti-Chinese riots in the state, and eventually the US ended migration from China partially as a response to pressure from California with the 1882 Chinese Exclusion Act.\nCalifornia genocide.\nUnder earlier Spanish and Mexican rule, California's original native population had precipitously declined, above all, from Eurasian diseases to which the Indigenous people of California had not yet developed a natural immunity. Under its new American administration, California's first governor Peter Hardeman Burnett instituted policies that have been described as a state-sanctioned policy of elimination of California's indigenous people. Burnett announced in 1851 in his Second Annual Message to the Legislature: \"That a war of extermination will continue to be waged between the races until the Indian race becomes extinct must be expected. While we cannot anticipate the result with but painful regret, the inevitable destiny of the race is beyond the power and wisdom of man to avert.\"\nAs in other American states, indigenous peoples were forcibly removed from their lands by American settlers, like miners, ranchers, and farmers. Although California had entered the American union as a free state, the \"loitering or orphaned Indians\", were \"de facto\" enslaved by their new Anglo-American masters under the 1850 \"Act for the Government and Protection of Indians\". One of these \"de facto\" slave auctions was approved by the Los Angeles City Council and occurred for nearly twenty years. There were many massacres in which hundreds of indigenous people were killed by settlers for their land.\nBetween 1850 and 1860, the California state government paid around 1.5million dollars (some 250,000 of which was reimbursed by the federal government) to hire militias with the stated purpose of protecting settlers, however these militias perpetrated numerous massacres of indigenous people. Indigenous people were also forcibly moved to reservations and rancherias, which were often small and isolated and without enough natural resources or funding from the government to adequately sustain the populations living on them. As a result, settler colonialism was a calamity for indigenous people. Several scholars and Native American activists, including Benjamin Madley and Ed Castillo, have described the actions of the California government as a genocide, as well as the 40th governor of California Gavin Newsom. Benjamin Madley estimates that from 1846 to 1873, between 9,492 and 16,092 indigenous people were killed, including between 1,680 and 3,741 killed by the U.S. Army.\n1900\u2013present.\nIn the 20th century, thousands of Japanese people migrated to California. The state in 1913 passed the Alien Land Act, excluding Asian immigrants from owning land. During World War II, Japanese Americans in California were interned in concentration camps; in 2020, California apologized.\nMigration to California accelerated during the early 20th century with the completion of transcontinental highways like the Route 66. From 1900 to 1965, the population grew from fewer than one million to the greatest in the Union. In 1940, the Census Bureau reported California's population as 6% Hispanic, 2.4% Asian, and 90% non-Hispanic white.\nTo meet the population's needs, engineering feats like the California and Los Angeles Aqueducts; the Oroville and Shasta Dams; and the Bay and Golden Gate Bridges were built. The state government adopted the California Master Plan for Higher Education in 1960 to develop an efficient system of public education.\nMeanwhile, attracted to the mild Mediterranean climate, cheap land, and the state's variety of geography, filmmakers established the studio system in Hollywood in the 1920s. California manufactured 9% of US armaments produced during World War II, ranking third behind New York and Michigan. California easily ranked first in production of military ships at drydock facilities in San Diego, Los Angeles, and the San Francisco Bay Area. Due to the hiring opportunities California offered during the conflict, the population multiplied from the immigration it received due to the work in its war factories, military bases, and training facilities. After World War II, California's economy expanded due to strong aerospace and defense industries, whose size decreased following the end of the Cold War. Stanford University began encouraging faculty and graduates to stay instead of leaving the state, and develop a high-tech region, now known as Silicon Valley. As a result of this, California is a world center of the entertainment and music industries, of technology, engineering, and the aerospace industry, and as the US center of agricultural production. Just before the Dot Com Bust, California had the fifth-largest economy in the world.\nIn the mid and late twentieth century, race-related incidents occurred. Tensions between police and African Americans, combined with unemployment and poverty in inner cities, led to riots, such as the 1992 Rodney King riots. California was the hub of the Black Panther Party, known for arming African Americans to defend against racial injustice. Mexican, Filipino, and other migrant farm workers rallied in the state around Cesar Chavez for better pay in the 1960s and 70s.\nDuring the 20th century, two great disasters happened: the 1906 San Francisco earthquake and 1928 St. Francis Dam flood remain the deadliest in U.S. history.\nAlthough air pollution has been reduced, health problems associated with pollution continue. Brown haze known as \"smog\" has been substantially abated after federal and state restrictions on automobile exhaust. An energy crisis in 2001 led to rolling blackouts, soaring power rates, and the importation of electricity from neighboring states. Southern California Edison and Pacific Gas and Electric Company came under heavy criticism.\nHousing prices in urban areas continued to increase; a modest home which in the 1960s cost $25,000 would cost half a million dollars or more in urban areas by 2005. More people commuted longer hours to afford a home in more rural areas while earning larger salaries in the urban areas. Speculators bought houses, expecting to make a huge profit in months, then rolling it over by buying more properties. Mortgage companies were compliant, as people assumed prices would keep rising. The bubble burst in 2007\u20138 as prices began to crash. Hundreds of billions in property values vanished and foreclosures soared, as financial institutions and investors were badly hurt.\nIn the 21st century, droughts and frequent wildfires attributed to climate change have occurred. From 2011 to 2017, a persistent drought was the worst in its recorded history. The 2018 wildfire season was the state's deadliest and most destructive.\nOne of the first confirmed COVID-19 cases in the United States occurred in California on January 26, 2020. A state of emergency was declared in the state on March 4, 2020, and remained in effect until Governor Gavin Newsom ended it in February 2023. A mandatory statewide stay-at-home order was issued on March 19, 2020, which was ended in January 2021.\nCultural and language revitalization efforts among indigenous Californians have progressed among tribes as of 2022. Some land returns to indigenous stewardship have occurred. In 2022, the largest dam removal and river restoration project in US history was announced for the Klamath River, as a win for California tribes.\nGeography.\nCovering an area of , California is the third-largest state in the United States in area, after Alaska and Texas. California is one of the most geographically diverse states in the union and is often geographically bisected into two regions, Southern California, comprising the ten southernmost counties, and Northern California, comprising the 48 northernmost counties. It is bordered by Oregon to the north, Nevada to the east and northeast, Arizona to the southeast, the Pacific Ocean to the west and shares an international border with the Mexican state of Baja California to the south (with which it makes up part of The Californias region of North America, alongside Baja California Sur).\nIn the middle of the state lies the California Central Valley, bounded by the Sierra Nevada in the east, the coastal mountain ranges in the west, the Cascade Range to the north and by the Tehachapi Mountains in the south. The Central Valley is California's productive agricultural heartland.\nDivided in two by the Sacramento-San Joaquin River Delta, the northern portion, the Sacramento Valley serves as the watershed of the Sacramento River, while the southern portion, the San Joaquin Valley is the watershed for the San Joaquin River. Both valleys derive their names from the rivers that flow through them. With dredging, the Sacramento and the San Joaquin Rivers have remained deep enough for several inland cities to be seaports.\nThe Sacramento-San Joaquin River Delta is a critical water supply hub for the state. Water is diverted from the delta and through an extensive network of pumps and canals that traverse nearly the length of the state, to the Central Valley and the State Water Projects and other needs. Water from the Delta provides drinking water for nearly 23million people, almost two-thirds of the state's population as well as water for farmers on the west side of the San Joaquin Valley.\nSuisun Bay lies at the confluence of the Sacramento and San Joaquin Rivers. The water is drained by the Carquinez Strait, which flows into San Pablo Bay, a northern extension of San Francisco Bay, which then connects to the Pacific Ocean via the Golden Gate strait.\nThe Channel Islands are located off the Southern coast, while the Farallon Islands lie west of San Francisco.\nThe Sierra Nevada (Spanish for \"snowy range\") includes the highest peak in the contiguous 48 states, Mount Whitney, at . The range embraces Yosemite Valley, famous for its glacially carved domes, and Sequoia National Park, home to the giant sequoia trees, the largest living organisms on Earth, and the deep freshwater lake, Lake Tahoe, the largest lake in the state by volume.\nTo the east of the Sierra Nevada are Owens Valley and Mono Lake, an essential migratory bird habitat. In the western part of the state is Clear Lake, the largest freshwater lake by area entirely in California. Although Lake Tahoe is larger, it is divided by the California/Nevada border. The Sierra Nevada falls to Arctic temperatures in winter and has several dozen small glaciers, including Palisade Glacier, the southernmost glacier in the United States.\nThe Tulare Lake was the largest freshwater lake west of the Mississippi River. A remnant of Pleistocene-era Lake Corcoran, Tulare Lake dried up by the early 20th century after its tributary rivers were diverted for agricultural irrigation and municipal water uses.\nAbout 45 percent of the state's total surface area is covered by forests, and California's diversity of pine species is unmatched by any other state. California contains more forestland than any other state except Alaska. Many of the trees in the California White Mountains are the oldest in the world; an individual bristlecone pine is over 5,000 years old.\nIn the south is a large inland salt lake, the Salton Sea. The south-central desert is called the Mojave; to the northeast of the Mojave lies Death Valley, which contains the lowest and hottest place in North America, the Badwater Basin at . The horizontal distance from the bottom of Death Valley to the top of Mount Whitney is less than . Indeed, almost all of southeastern California is arid, hot desert, with routine extreme high temperatures during the summer. The southeastern border of California with Arizona is entirely formed by the Colorado River, from which the southern part of the state gets about half of its water.\nA majority of California's cities are located in either the San Francisco Bay Area or the Sacramento metropolitan area in Northern California; or the Los Angeles area, the Inland Empire, or the San Diego metropolitan area in Southern California. The Los Angeles Area, the Bay Area, and the San Diego metropolitan area are among several major metropolitan areas along the California coast.\nAs part of the Ring of Fire, California is subject to tsunamis, floods, droughts, Santa Ana winds, wildfires, and landslides on steep terrain; California also has several volcanoes. It has many earthquakes due to several faults running through the state, the largest being the San Andreas Fault. About 37,000 earthquakes are recorded each year; most are too small to be felt. Among Americans at risk of serious harm from a major earthquake, two-thirds of that population are residents of California.\nClimate.\nMost of the state has a Mediterranean climate. The cool California Current offshore often creates summer fog near the coast. Farther inland, there are colder winters and hotter summers. The maritime moderation results in the shoreline summertime temperatures of Los Angeles and San Francisco being the coolest of all major metropolitan areas of the United States and uniquely cool compared to areas on the same latitude in the interior and on the east coast of the North American continent. Even the San Diego shoreline bordering Mexico is cooler in summer than most areas in the contiguous United States. Just a few miles inland, summer temperature extremes are significantly higher, with downtown Los Angeles being several degrees warmer than at the coast. The same microclimate phenomenon is seen in the climate of the Bay Area, where areas sheltered from the ocean experience significantly hotter summers and colder winters in contrast with nearby areas closer to the ocean.\nNorthern parts of the state have more rain than the south. California's mountain ranges also influence the climate: some of the rainiest parts of the state are west-facing mountain slopes. Coastal northwestern California has a temperate climate, and the Central Valley has a Mediterranean climate but with greater temperature extremes than the coast. The high mountains, including the Sierra Nevada, have an alpine climate with snow in winter and mild to moderate heat in summer.\nCalifornia's mountains produce rain shadows on the eastern side, creating extensive deserts. The higher elevation deserts of eastern California have hot summers and cold winters, while the low deserts east of the Southern California mountains have hot summers and nearly frostless mild winters. Death Valley, a desert with large expanses below sea level, is considered the hottest location in the world; the highest temperature in the world, , was recorded there on July 10, 1913. The lowest temperature in California was on January 20, 1937, in Boca.\nThe table below lists average temperatures for January and August in a selection of places throughout the state; some highly populated and some not. This includes the relatively cool summers of the Humboldt Bay region around Eureka, the extreme heat of Death Valley, and the mountain climate of Mammoth in the Sierra Nevada.\nThe wide range of climates leads to a high demand for water. Over time, droughts have been increasing due to climate change and overextraction, becoming less seasonal and more year-round, further straining California's electricity supply and water security and having an impact on California business, industry, and agriculture.\nIn 2022, a new state program was created in collaboration with indigenous peoples of California to revive the practice of controlled burns as a way of clearing excessive forest debris and making landscapes more resilient to wildfires. Native American use of fire in ecosystem management was outlawed in 1911, yet has now been recognized.\nEcology.\nCalifornia is one of the ecologically richest and most diverse parts of the world, and includes some of the most endangered ecological communities. California is part of the Nearctic realm and spans a number of terrestrial ecoregions.\nCalifornia's large number of endemic species includes relict species, which have died out elsewhere, such as the Catalina ironwood (\"Lyonothamnus floribundus\"). Many other endemics originated through differentiation or adaptive radiation, whereby multiple species develop from a common ancestor to take advantage of diverse ecological conditions such as the California lilac (\"Ceanothus\"). Many California endemics have become endangered, as urbanization, logging, overgrazing, and the introduction of exotic species have encroached on their habitat.\nFlora and fauna.\nCalifornia boasts several superlatives in its collection of flora: the largest trees, the tallest trees, and the oldest trees. California's native grasses are perennial plants, and there are close to hundred succulent species native to the state. After European contact, these were generally replaced by invasive species of European annual grasses; and, in modern times, California's hills turn a characteristic golden-brown in summer.\nBecause California has the greatest diversity of climate and terrain, the state has six life zones which are the lower Sonoran Desert; upper Sonoran (foothill regions and some coastal lands), transition (coastal areas and moist northeastern counties); and the Canadian, Hudsonian, and Arctic Zones, comprising the state's highest elevations.\nPlant life in the dry climate of the lower Sonoran zone contains a diversity of native cactus, mesquite, and paloverde. The Joshua tree is found in the Mojave Desert. Flowering plants include the dwarf desert poppy and a variety of asters. Fremont cottonwood and valley oak thrive in the Central Valley. The upper Sonoran zone includes the chaparral belt, characterized by forests of small shrubs, stunted trees, and herbaceous plants. \"Nemophila\", mint, \"Phacelia\", \"Viola\", and the California poppy (\"Eschscholzia californica\", the state flower) also flourish in this zone, along with the lupine, more species of which occur here than anywhere else in the world.\nThe transition zone includes most of California's forests with the redwood (\"Sequoia sempervirens\") and the \"big tree\" or giant sequoia (\"Sequoiadendron giganteum\"), among the oldest living things on earth (some are said to have lived at least 4,000 years). Tanbark oak, California laurel, sugar pine, madrona, broad-leaved maple, and Douglas-fir also grow here. Forest floors are covered with swordfern, alumnroot, barrenwort, and trillium, and there are thickets of huckleberry, azalea, elder, and wild currant. Characteristic wild flowers include varieties of mariposa, tulip, and tiger and leopard lilies.\nThe high elevations of the Canadian zone allow the Jeffrey pine, red fir, and lodgepole pine to thrive. Brushy areas are abundant with dwarf manzanita and ceanothus; the unique Sierra puffball is also found here. Right below the timberline, in the Hudsonian zone, the whitebark, foxtail, and silver pines grow. At about , begins the Arctic zone, a treeless region whose flora include a number of wildflowers, including Sierra primrose, yellow columbine, alpine buttercup, and alpine shooting star.\nPalm trees are a well-known feature of California, particularly in Southern California and Los Angeles; many species have been imported, though the \"Washington filifera\" (commonly known as the \"California fan palm\") is native to the state, mainly growing in the Colorado Desert oases. Other common plants that have been introduced to the state include the eucalyptus, acacia, pepper tree, geranium, and Scotch broom. The species that are federally classified as endangered are the Contra Costa wallflower, Antioch Dunes evening primrose, Solano grass, San Clemente Island larkspur, salt marsh bird's beak, McDonald's rock-cress, and Santa Barbara Island liveforever. , 85 plant species were listed as threatened or endangered.\nIn the deserts of the lower Sonoran zone, the mammals include the jackrabbit, kangaroo rat, squirrel, and opossum. Common birds include the owl, roadrunner, cactus wren, and various species of hawk. The area's reptilian life include the sidewinder viper, desert tortoise, and horned toad. The upper Sonoran zone boasts mammals such as the antelope, brown-footed woodrat, and ring-tailed cat. Birds unique to this zone are the California thrasher, bushtit, and California condor.\nIn the transition zone, there are Colombian black-tailed deer, black bears, gray foxes, cougars, bobcats, and Roosevelt elk. Reptiles such as the garter snakes and rattlesnakes inhabit the zone. In addition, amphibians such as the water puppy and redwood salamander are common too. Birds such as the kingfisher, chickadee, towhee, and hummingbird thrive here as well.\nThe Canadian zone mammals include the mountain weasel, snowshoe hare, and several species of chipmunks. Conspicuous birds include the blue-fronted jay, mountain chickadee, hermit thrush, American dipper, and Townsend's solitaire. As one ascends into the Hudsonian zone, birds become scarcer. While the gray-crowned rosy finch is the only bird native to the high Arctic region, other bird species such as Anna's hummingbird and Clark's nutcracker. Principal mammals found in this region include the Sierra coney, white-tailed jackrabbit, and the bighorn sheep. , the bighorn sheep was listed as endangered by the U.S. Fish and Wildlife Service. The fauna found throughout several zones are the mule deer, coyote, mountain lion, northern flicker, and several species of hawk and sparrow.\nAquatic life in California thrives, from the state's mountain lakes and streams to the rocky Pacific coastline. Numerous trout species are found, among them rainbow, golden, and cutthroat. Migratory species of salmon are common as well. Deep-sea life forms include sea bass, yellowfin tuna, barracuda, and several types of whale. Native to the cliffs of northern California are seals, sea lions, and many types of shorebirds, including migratory species.\n, 118 California animals were on the federal endangered list; 181 plants were listed as endangered or threatened. Endangered animals include the San Joaquin kitfox, Point Arena mountain beaver, Pacific pocket mouse, salt marsh harvest mouse, Morro Bay kangaroo rat (and five other species of kangaroo rat), Amargosa vole, California least tern, California condor, loggerhead shrike, San Clemente sage sparrow, San Francisco garter snake, five species of salamander, three species of chub, and two species of pupfish. Eleven butterflies are also endangered and two that are threatened are on the federal list. Among threatened animals are the coastal California gnatcatcher, Paiute cutthroat trout, southern sea otter, and northern spotted owl. California has a total of of National Wildlife Refuges. , 123 California animals were listed as either endangered or threatened on the federal list. Also, , 178 species of California plants were listed either as endangered or threatened on this federal list.\nRivers.\nThe most prominent river system within California is formed by the Sacramento River and San Joaquin River, which are fed mostly by snowmelt from the west slope of the Sierra Nevada, and respectively drain the north and south halves of the Central Valley. The two rivers join in the Sacramento\u2013San Joaquin River Delta, flowing into the Pacific Ocean through San Francisco Bay. Many major tributaries feed into the Sacramento\u2013San Joaquin system, including the Pit River, Feather River and Tuolumne River.\nThe Klamath and Trinity Rivers drain a large area in far northwestern California. The Eel River and Salinas River each drain portions of the California coast, north and south of San Francisco Bay, respectively. The Mojave River is the primary watercourse in the Mojave Desert, and the Santa Ana River drains much of the Transverse Ranges as it bisects Southern California. The Colorado River forms the state's southeast border with Arizona.\nMost of California's major rivers are dammed as part of two massive water projects: the Central Valley Project, providing water for agriculture in the Central Valley, and the California State Water Project diverting water from Northern to Southern California. The state's coasts, rivers, and other bodies of water are regulated by the California Coastal Commission.\nRegions.\nCalifornia is traditionally separated into Northern California and Southern California, divided by a straight border which runs across the state, separating the northern 48 counties from the southern 10 counties. Despite the persistence of the northern-southern divide, California is more precisely divided into many regions, multiple of which stretch across the northern-southern divide.\nCities and towns.\nThe state has 483 incorporated cities and towns, of which 461 are cities and 22 are towns. Under California law, the terms \"city\" and \"town\" are explicitly interchangeable; the name of an incorporated municipality in the state can either be \"City of (Name)\" or \"Town of (Name)\".\nSacramento became California's first incorporated city on February 27, 1850. San Jose, San Diego, and Benicia tied for California's second incorporated city, each receiving incorporation on March 27, 1850. Mountain House became the state's most recent and 483rd incorporated municipality on July 1, 2024.\nThe majority of these cities and towns are within one of five metropolitan areas: the Los Angeles Metropolitan Area, the San Francisco Bay Area, the Riverside-San Bernardino Area, the San Diego metropolitan area, or the Sacramento metropolitan area.\nDemographics.\nPopulation.\nPresently, close to one out of every nine United States residents live in California. The United States Census Bureau reported that the population of California was 39.54 million on April 1, 2020, a 6.13% increase since the 2010 census. During that decade, the state's population grew more slowly than the rest of the nation, resulting in the loss of one seat on the US House of Representatives, the first loss in its entire history. The estimated state population in 2023 was 38.94 million. For well over a century (1900\u20132020), California experienced steady population growth. Even while the rate of growth began to slow by the 1990s, some growth continued into the first two decades of the 21st century; California added an average of around 400,000 people per year to its population during the period 1940\u20132020. Then in 2020, the state began to experience population declines continuing every year, attributable mostly to moves out of state but also due to declining birth rates, COVID-19 pandemic deaths, and less internal migration from other states to California. According to the U.S. Census Bureau, between 2021 and 2022, 818,000 California residents moved out of state with emigrants listing high cost of living, high taxes, and a difficult business environment as the motivation. The net loss of population in California between July 2020 and July 2023 was 433,000.\nThe Greater Los Angeles Area is the second-largest metropolitan area in the United States (U.S.), while Los Angeles is the second-largest city in the U.S. Los Angeles County has held the title of most populous U.S. county for decades, and it alone is more populous than 42 U.S. states. San Francisco is the most densely-populated city in California and one of the most densely populated cities in the U.S.. Four of the top 20 most populous cities in the U.S. are in California: Los Angeles (2nd), San Diego (8th), San Jose (13th), and San Francisco (17th). The center of population of California is located four miles west-southwest of the city of Shafter, Kern County.\nAs of 2020, California ranked fourth among states by life expectancy, with a life expectancy of 79.0 years.\nStarting in the year 2010, for the first time since the California Gold Rush, California-born residents made up the majority of the state's population. Along with the rest of the United States, California's immigration pattern has also shifted over the course of the late 2000s to early 2010s. Immigration from Latin American countries has dropped significantly with most immigrants now coming from Asia. In total for 2011, there were 277,304 immigrants. Fifty-seven percent came from Asian countries versus 22% from Latin American countries. Net immigration from Mexico, previously the most common country of origin for new immigrants, has dropped to zero / less than zero since more Mexican nationals are departing for their home country than immigrating.\nThe state's population of undocumented immigrants has been shrinking in recent years, due to increased enforcement and decreased job opportunities for lower-skilled workers. The number of migrants arrested attempting to cross the Mexican border in the Southwest decreased from a high of 1.1million in 2005 to 367,000 in 2011. Despite these recent trends, illegal aliens constituted an estimated 7.3 percent of the state's population, the third highest percentage of any state in the country, totaling nearly 2.6million. In particular, illegal immigrants tended to be concentrated in Los Angeles, Monterey, San Benito, Imperial, and Napa Counties\u2014the latter four of which have significant agricultural industries that depend on manual labor. More than half of illegal immigrants originate from Mexico. The state of California and some California cities, including Los Angeles, Oakland, and San Francisco, have adopted sanctuary policies.\nAccording to HUD's 2022 Annual Homeless Assessment Report, there were an estimated 171,521 homeless people in California.\nRace and ethnicity.\nAccording to the United States Census Bureau in 2022 the population self-identified as (alone or in combination): 56.5% White (including Hispanic Whites), 33.7% non-Hispanic white, 18.1% Asian, 7.3% Black or African American, 3.2% Native American and Alaska Native, 0.9% Native Hawaiian or Pacific Islander, and 34.3% some other race. These numbers add up to more than 100% because respondents can select multiple racial identities. 19% of Californians identified as two or more races in 2022, although excluding respondents who selected \"some other race\", only 5% identified as two or more races.\nBy ethnicity, in 2018 the population was 60.7% non-Hispanic (of any race) and 39.3% Hispanic or Latino (of any race). Hispanics are the largest single ethnic group in California. Non-Hispanic whites constituted 36.8% of the state's population. \"Californios\" are the Hispanic residents native to California, who make up the Spanish-speaking community that has existed in California since 1542, of varying Mexican American/Chicano, Criollo Spaniard, and Mestizo origin. However, they make up only a small part of California's Hispanic population today, estimated at 500,000. California has the largest Mexican, Salvadoran, and Guatemalan population, together making up over 90% of the state's Latino population.\nAccording to 2022 estimates from the American Community Survey, 32.4% of the population had Mexican ancestry, 6.6% had German ancestry, 6.1% had English ancestry, 5.6% had Irish ancestry, 4.9% had Chinese ancestry, 4.3% had Filipino ancestry, 4% had Central American ancestry (Mostly Salvadoran and Guatemalan), 3.4% had Italian ancestry, 2.8% listed themselves as American, and 2.5% had Indian ancestry.\n, 75.1% of California's population younger than age 1 were minorities, meaning they had at least one parent who was not non-Hispanic white (white Hispanics are counted as minorities).\nIn terms of total numbers, California has the largest population of White Americans in the United States, an estimated 22,200,000 residents including people identifying as white in combination with any other race. The state has the 5th largest population of African Americans in the United States, an estimated 2,250,000 residents. California's Asian American population is estimated at 7.1million, constituting a third of the nation's total. California's Native American population of 504,000 is the most of any state, with 103,030 identifying as Non-Hispanic and belonging mostly to the Indigenous peoples of California. Most of the state's Native American population identifies as Hispanic and belongs to Indigenous Mexican or Central American ethnic groups, including 185,200 identifying as Mexican American Indian and 67,904 identifying as Central American Indian.\nAccording to estimates from 2011, California has the largest minority population in the United States by numbers, making up 60% of the state population. Over the past 25 years, the population of non-Hispanic whites has declined, while Hispanic and Asian populations have grown. Between 1970 and 2011, non-Hispanic whites declined from 80% of the state's population to 40%, while Hispanics grew from 32% in 2000 to 38% in 2011. It is currently projected that Hispanics will rise to 49% of the population by 2060, primarily due to domestic births rather than immigration. With the decline of immigration from Latin America, Asian Americans now constitute the fastest growing racial/ethnic group in California; this growth is primarily driven by immigration from China, India, and the Philippines, respectively.\nMost of California's immigrant population are born in Mexico (3.9 million), the Philippines (825,200), China (768,400), India (556,500), and Vietnam (502,600).\nCalifornia has the largest multiracial population in the United States. Mexican is the most common ancestry in California, followed by English, German, and Irish.\nLanguages.\nEnglish serves as California's de jure and de facto official language. According to the 2021 American Community Survey conducted by the United States Census Bureau, 56.08% (20,763,638) of California residents age5 and older spoke only English at home, while 43.92% spoke another language at home. 60.35% of people who speak a language other than English at home are able to speak English \"well\" or \"very well\", with this figure varying significantly across the different linguistic groups. Like most U.S. states (32 out of 50), California law enshrines English as its official language, and has done so since the passage of Proposition 63 by California voters in 1986. Various government agencies do, and are often required to, furnish documents in the various languages needed to reach their intended audiences.\nSpanish is the most commonly spoken language in California, behind English, spoken by 28.18% (10,434,308) of the population (in 2021). The Spanish language has been spoken in California since 1542 and is deeply intertwined with California's cultural landscape and history. Spanish was the official administrative language of California through the Spanish and Mexican eras, until 1848. Following the U.S. Conquest of California and the Treaty of Guadalupe-Hidalgo, the U.S. Government guaranteed the rights of Spanish-speaking Californians. The first Constitution of California was written in both languages at the Monterey Constitutional Convention of 1849 and protected the rights of Spanish speakers to use their language in government proceedings and mandating that all government documents be published in both English and Spanish.\nDespite the initial recognition of Spanish by early American governments in California, the revised 1879 constitution stripped the rights of Spanish speakers and the official status of Spanish. The growth of the English-only movement by the mid-20th century led to the passage of 1986 California Proposition 63, which enshrined English as the only official language in California and ended Spanish language instruction in schools. 2016 California Proposition 58 reversed the prohibition on bilingual education, though there are still many barriers to the proliferation of Spanish bilingual education, including a shortage of teachers and lack of funding. The government of California has since made efforts to promote Spanish language access and bilingual education, as have private educational institutions in California. Many businesses in California promote the usage of Spanish by their employees, to better serve both California's Hispanic population and the larger Spanish-speaking world.\nCalifornia has historically been one of the most linguistically diverse areas in the world, with more than 70 indigenous languages derived from 64 root languages in six language families. A survey conducted between 2007 and 2009 identified 23 different indigenous languages among California farmworkers. All of California's indigenous languages are endangered, although there are now efforts toward language revitalization. California has the highest concentration nationwide of Chinese, Vietnamese and Punjabi speakers.\nAs a result of the state's increasing diversity and migration from other areas across the country and around the globe, linguists began noticing a noteworthy set of emerging characteristics of spoken American English in California since the late 20th century. This variety, known as California English, has a vowel shift and several other phonological processes that are different from varieties of American English used in other regions of the United States.\nReligion.\nThe largest religious denominations by number of adherents as a percentage of California's population in 2014 were the Catholic Church with 28 percent, Evangelical Protestants with 20 percent, and Mainline Protestants with 10 percent. Together, all kinds of Protestants accounted for 32 percent. Those unaffiliated with any religion represented 27 percent of the population. The breakdown of other religions is 1% Muslim, 2% Hindu and 2% Buddhist. This is a change from 2008, when the population identified their religion with the Catholic Church with 31 percent; Evangelical Protestants with 18 percent; and Mainline Protestants with 14 percent. In 2008, those unaffiliated with any religion represented 21 percent of the population. The breakdown of other religions in 2008 was 0.5% Muslim, 1% Hindu and 2% Buddhist. The \"American Jewish Year Book\" placed the total Jewish population of California at about 1,194,190 in 2006. According to the Association of Religion Data Archives (ARDA) the largest denominations by adherents in 2010 were the Catholic Church with 10,233,334; The Church of Jesus Christ of Latter-day Saints with 763,818; and the Southern Baptist Convention with 489,953.\nCalifornia has a large Catholic population due to the large numbers of Mexicans and Central Americans living within its borders. California has twelve dioceses and two archdioceses, the Archdiocese of Los Angeles and the Archdiocese of San Francisco, the former being the largest archdiocese in the United States.\nA Pew Research Center survey revealed that California is somewhat less religious than the rest of the states: 62 percent of Californians say they are \"absolutely certain\" of their belief in God, while in the nation 71 percent say so. The survey also revealed 48 percent of Californians say religion is \"very important\", compared to 56 percent nationally.\nCulture.\nThe culture of California is a Western culture and has its modern roots in the culture of the United States, but also, historically, many Hispanic Californio and Mexican influences. As a border and coastal state, California culture has been greatly influenced by several large immigrant populations, especially those from Latin America and Asia.\nCalifornia has long been a subject of interest in the public mind and has often been promoted by its boosters as a kind of paradise. In the early 20th century, fueled by the efforts of state, the building projects during the Great Depression and local boosters, many Americans saw the Golden State as an ideal resort destination, sunny and dry all year round with easy access to the ocean and mountains. In the 1960s, popular music groups such as the Beach Boys promoted the image of Californians as laid-back, tanned beach-goers.\nMedia and entertainment.\nHollywood and the rest of the Los Angeles area is a major global center for entertainment, with the U.S. film industry's \"Big Five\" major film studios (Columbia, Disney, Paramount, Universal, and Warner Bros.) as well as many minor film studios being based in or around the area. Many animation studios are also headquartered in the state.\nThe four major American television commercial broadcast networks (ABC, CBS, NBC, and Fox) as well as other networks all have production facilities and offices in the state. All the four major commercial broadcast networks, plus the two major Spanish-language networks (Telemundo and Univision) each have at least three owned-and-operated TV stations in California, including at least one in Los Angeles and at least one in San Francisco.\nOne of the oldest radio stations in the United States still in existence, KCBS (AM) in the San Francisco Bay Area, was founded in 1909. Universal Music Group, one of the \"Big Four\" record labels, is based in Santa Monica, while Warner Records is based in Los Angeles. Many independent record labels, such as Mind of a Genius Records, are also headquartered in the state. California is also the birthplace of several international music genres, including the Bakersfield sound, Bay Area thrash metal, alternative rock, g-funk, nu metal, glam metal, thrash metal, psychedelic rock, stoner rock, punk rock, hardcore punk, metalcore, pop punk, surf music, third wave ska, west coast hip hop, west coast jazz, jazz rap, and many other genres. Other genres such as pop rock, indie rock, hard rock, hip hop, pop, rock, rockabilly, country, heavy metal, grunge, new wave and disco were popularized in the state. In addition, many British bands, such as Led Zeppelin, Deep Purple, Black Sabbath, and the Rolling Stones settled in the state after becoming internationally famous.\nAs the home of Silicon Valley, the Bay Area is the headquarters of several prominent internet media, social media, and other technology companies. Three of the \"Big Five\" technology companies (Apple, Meta, and Google) are based in the area as well as other services such as Netflix, Pandora Radio, Twitter, Yahoo!, and YouTube. Other prominent companies that are headquartered here include HP inc. and Intel. Microsoft and Amazon also have offices in the area.\nCalifornia, particularly Southern California, is considered the birthplace of modern car culture.\nSeveral fast food, fast casual, and casual dining chains were also founded California, including some that have since expanded internationally like California Pizza Kitchen, Denny's, IHOP, McDonald's, Panda Express, and Taco Bell.\nSports.\nCalifornia has 18 major professional sports league franchises, far more than any other state. The San Francisco Bay Area has five major league teams, while the Greater Los Angeles Area is home to ten. San Diego has two major league teams and Sacramento has one. The NFL Super Bowl has been hosted in California 12 times at five different stadiums: Los Angeles Memorial Coliseum, the Rose Bowl, Stanford Stadium, Levi's Stadium, and San Diego Stadium. A thirteenth, Super Bowl LVI, was held at SoFi Stadium in Inglewood on February 13, 2022.\nCalifornia has long had many respected collegiate sports programs. California is home to the oldest college bowl game, the annual Rose Bowl, among others.\nThe NFL has three teams in the state: the Los Angeles Rams, Los Angeles Chargers, and San Francisco 49ers.\nMLB has four teams in the state: the San Francisco Giants, Los Angeles Dodgers, Los Angeles Angels, and San Diego Padres.\nThe NBA has four teams in the state: the Golden State Warriors, Los Angeles Clippers, Los Angeles Lakers, and Sacramento Kings. Additionally, the WNBA also has one team in the state: the Los Angeles Sparks.\nThe NHL has three teams in the state: the Anaheim Ducks, Los Angeles Kings, and San Jose Sharks.\nMLS has four teams in the state: the Los Angeles Galaxy, San Jose Earthquakes, Los Angeles FC, and San Diego FC.\nMLR has one team in the state: the San Diego Legion.\nCalifornia is the only U.S. state to have hosted both the Summer and Winter Olympics. The 1932 and 1984 summer games were held in Los Angeles. Squaw Valley Ski Resort (now Palisades Tahoe) in the Lake Tahoe region hosted the 1960 Winter Olympics. Los Angeles will host the 2028 Summer Olympics, marking the fourth time that California will have hosted the Olympic Games. Multiple games during the 1994 FIFA World Cup took place in California, with the Rose Bowl hosting eight matches (including the final), while Stanford Stadium hosted six matches.\nIn addition to the Olympic games, California also hosts the California State Games.\nMany sports, such as surfing, snowboarding, and skateboarding, were invented in California, while others like volleyball, beach soccer, and skiing were popularized in the state.\nOther sports that are big in the state include golf, rodeo, tennis, mountain climbing, marathon running, horse racing, bowling, mixed martial arts, boxing, and motorsports, especially NASCAR and Formula One.\nEducation.\nCalifornia has the most school students in the country, with over 6.2 million in the 2005\u201306 school year, giving California more students in school than 36 states have in total population and one of the highest projected enrollments in the country.\nPublic secondary education consists of high schools that teach elective courses in trades, languages, and liberal arts with tracks for gifted, college-bound and industrial arts students. California's public educational system is supported by a unique constitutional amendment that requires a minimum annual funding level for grades K\u201312 and community colleges that grows with the economy and student enrollment figures.\nIn 2016, California's K\u201312 public school per-pupil spending was ranked 22nd in the nation ($11,500 per student vs. $11,800 for the U.S. average).\nFor 2012, California's K\u201312 public schools ranked 48th in the number of employees per student, at 0.102 (the U.S. average was 0.137), while paying the 7th most per employee, $49,000 (the U.S. average was $39,000).\nHigher education.\nCalifornia public postsecondary education is organized into three separate systems:\nCalifornia is also home to notable private universities such as Stanford University, the California Institute of Technology (Caltech), the University of Southern California, the Claremont Colleges, Santa Clara University, Loyola Marymount University, the University of San Diego, the University of San Francisco, Chapman University, Pepperdine University, Occidental College, and University of the Pacific, among numerous other private colleges and universities, including many religious and special-purpose institutions. California has a particularly high density of arts colleges, including the California College of the Arts, California Institute of the Arts, San Francisco Art Institute, Art Center College of Design, and Academy of Art University, among others.\nEconomy.\nCalifornia's economy ranks among the largest in the world. , the gross state product (GSP) is $4.0trillion ($102,500 per capita), the largest in the United States. California is responsible for one seventh of the nation's gross domestic product (GDP). , California's nominal GDP is larger than all but four countries. In terms of purchasing power parity (PPP), it is larger than all but eight countries. California's economy is larger than Africa and Australia and is almost as large as South America. The state recorded total, non-farm employment of 16,677,800 among 966,224 employer establishments.\nAs the largest and second-largest U.S. ports respectively, the Port of Los Angeles and the Port of Long Beach in Southern California collectively play a pivotal role in the global supply chain, together hauling in about 40% of all imports to the United States by TEU volume. The Port of Oakland and Port of Hueneme are the 10th and 26th largest seaports in the U.S., respectively, by number of TEUs handled.\nThe five largest sectors of employment in California are trade, transportation, and utilities; government; professional and business services; education and health services; and leisure and hospitality. In output, the five largest sectors are financial services, followed by trade, transportation, and utilities; education and health services; government; and manufacturing. California has an unemployment rate of 3.9% .\nCalifornia's economy is dependent on trade and international related commerce accounts for about one-quarter of the state's economy, and representing 7% of their GDP; California's biggest trade partner is Mexico. In 2008, California exported $144billion worth of goods, up from $134billion in 2007 and $127billion in 2006. Vehicles, computers and electronic products are California's top exports, accounting for 42 percent of all the state's exports in 2008; over 50 car companies operate in California, such as Tesla and Mazda.\nAgriculture.\nAgriculture is an important sector in California's economy. According to the USDA in 2011, the three largest California agricultural products by value were milk and cream, shelled almonds, and grapes. Farming-related sales more than quadrupled over the past three decades, from $7.3billion in 1974 to nearly $31billion in 2004. This increase has occurred despite a 15 percent decline in acreage devoted to farming during the period, and water supply suffering from chronic instability. Factors contributing to the growth in sales-per-acre include more intensive use of active farmlands and technological improvements in crop production. In 2008, California's 81,500 farms and ranches generated $36.2billion products revenue. In 2011, that number grew to $43.5billion products revenue. The agriculture sector accounts for two percent of the state's GDP and employs around three percent of its total workforce.\nIncome.\nPer capita GDP in 2021 was $85,546, ranking fourth in the nation. Per capita income varies widely by geographic region and profession. The Central Valley is the most impoverished, with migrant farm workers making less than minimum wage. According to a 2005 report by the Congressional Research Service, the San Joaquin Valley was characterized as one of the most economically depressed regions in the United States, on par with the region of Appalachia.\nUsing the supplemental poverty measure, California has a poverty rate of 23.5%, the highest of any state in the country. However, using the official measure the poverty rate was only 13.3% as of 2017. Many coastal cities include some of the wealthiest per-capita areas in the United States. The high-technology sectors in Northern California, specifically Silicon Valley, in Santa Clara and San Mateo counties, have emerged from the economic downturn caused by the dot-com bust.\nIn 2019, there were 1,042,027 millionaire households in the state, more than any other state in the nation. In 2010, California residents were ranked first among the states with the best average credit score of 754.\nState finances.\nState spending increased from $56billion in 1998 to $127billion in 2011. California has the third highest per capita spending on welfare among the states, as well as the highest spending on welfare at $6.67billion. In January 2011, California's total debt was at least $265billion. On June 27, 2013, Governor Jerry Brown signed a balanced budget (no deficit) for the state, its first in decades; however, the state's debt remains at $132billion.\nWith the passage of Proposition 30 in 2012 and Proposition 55 in 2016, California now levies a 13.3% maximum marginal income tax rate with ten tax brackets, ranging from 1% at the bottom tax bracket of $0 annual individual income to 13.3% for annual individual income over $1,000,000 (though the top brackets are only temporary until Proposition 55 expires at the end of 2030). While Proposition 30 also enacted a minimum state sales tax of 7.5%, this sales tax increase was not extended by Proposition 55 and reverted to a previous minimum state sales tax rate of 7.25% in 2017. Local governments can and do levy additional sales taxes in addition to this minimum rate.\nAll real property is taxable annually. Property tax increases are capped at 2% annually or the rate of inflation (whichever is lower), per Proposition 13.\nEnergy.\nBecause it is the most populous state in the United States, California is one of the country's largest users of energy. The state has extensive hydro-electric energy generation facilities, however, moving water is the single largest energy use in the state. Also, due to high energy rates, conservation mandates, mild weather in the largest population centers and strong environmental movement, its \"per capita\" energy use is one of the smallest of any state in the United States. Due to the high electricity demand, California imports more electricity than any other state, primarily hydroelectric power from states in the Pacific Northwest (via Path 15 and Path 66) and coal- and natural gas-fired production from the desert Southwest via Path 46.\nThe state's crude oil and natural gas deposits are located in the Central Valley and along the coast, including the large Midway-Sunset Oil Field. Natural gas-fired power plants typically account for more than one-half of state electricity generation.\nAs a result of the state's strong environmental movement, California has some of the most aggressive renewable energy goals in the United States. The Clean Energy, Jobs and Affordability Act of 2022 commits the state to running its operations on clean, renewable energy resources by 2035, and SB 1203 also requires the state to achieve net-zero operations for all agencies. Currently, several solar power plants such as the Solar Energy Generating Systems facility are located in the Mojave Desert. California's wind farms include Altamont Pass, San Gorgonio Pass, and Tehachapi Pass. The Tehachapi area is also where the Tehachapi Energy Storage Project is located. Several dams across the state provide hydro-electric power. It would be possible to convert the total supply to 100% renewable energy, including heating, cooling and mobility, by 2050.\nCalifornia has one major nuclear power plant (Diablo Canyon) in operation. The San Onofre nuclear plant was shut down in 2013. More than 1,700tons of radioactive waste are stored at San Onofre, and sit on the coast where there is a record of past tsunamis. Voters banned the approval of new nuclear power plants since the late 1970s because of concerns over radioactive waste disposal. Several cities such as Oakland, Berkeley and Davis have declared themselves as nuclear-free zones.\nTransportation.\nHighways.\nCalifornia's vast terrain is connected by an extensive system of controlled-access highways ('freeways'), limited-access roads ('expressways'), and highways. California is known for its car culture, giving California's cities a reputation for severe traffic congestion. Construction and maintenance of state roads and statewide transportation planning are primarily the responsibility of the California Department of Transportation, nicknamed \"Caltrans\". The rapidly growing population of the state is straining all of its transportation networks, and California has some of the worst roads in the United States. The Reason Foundation's 19th Annual Report on the Performance of State Highway Systems ranked California's highways the third-worst of any state, with Alaska second, and Rhode Island first.\nThe state has been a pioneer in road construction. One of the state's more visible landmarks, the Golden Gate Bridge, was the longest suspension bridge main span in the world at between 1937 (when it opened) and 1964. With its orange paint and panoramic views of the bay, this highway bridge is a popular tourist attraction and also accommodates pedestrians and bicyclists. The San Francisco\u2013Oakland Bay Bridge (often abbreviated the \"Bay Bridge\"), completed in 1936, transports about 280,000 vehicles per day on two-decks. Its two sections meet at Yerba Buena Island through the world's largest diameter transportation bore tunnel, at wide by high. The Arroyo Seco Parkway, connecting Los Angeles and Pasadena, opened in 1940 as the first freeway in the Western United States. It was later extended south to the Four Level Interchange in downtown Los Angeles, regarded as the first stack interchange ever built.\nThe California Highway Patrol is the largest statewide police agency in the United States in employment with more than 10,000 employees. They are responsible for providing any police-sanctioned service to anyone on California's state-maintained highways and on state property.\nBy the end of 2021, 30,610,058 people in California held a California Department of Motor Vehicles-issued driver's licenses or state identification card, and there were 36,229,205 registered vehicles, including 25,643,076 automobiles, 853,368 motorcycles, 8,981,787 trucks and trailers, and 121,716 miscellaneous vehicles (including historical vehicles and farm equipment).\nAir travel.\nLos Angeles International Airport (LAX), the 4th busiest airport in the world in 2018, and San Francisco International Airport (SFO), the 25th busiest airport in the world in 2018, are major hubs for trans-Pacific and transcontinental traffic. There are about a dozen important commercial airports and many more general aviation airports throughout the state.\nRailroads.\nInter-city rail travel is provided by Amtrak California; the three routes, the \"Capitol Corridor\", \"Pacific Surfliner\", and \"San Joaquin\", are funded by Caltrans. These services are the busiest intercity rail lines in the United States outside the Northeast Corridor and ridership is continuing to set records. The routes are becoming increasingly popular over flying, especially on the LAX-SFO route. Integrated subway and light rail networks are found in Los Angeles (Los Angeles Metro Rail) and San Francisco (Muni Metro). Light rail systems are also found in San Jose (VTA light rail), San Diego (San Diego Trolley), Sacramento (SacRT light rail), and Northern San Diego County (Sprinter). Furthermore, commuter rail networks serve the San Francisco Bay Area (Altamont Corridor Express, Bay Area Rapid Transit, Caltrain, Sonoma\u2013Marin Area Rail Transit), Greater Los Angeles (Metrolink), and San Diego County (Coaster).\nThe California High-Speed Rail Authority was authorized in 1996 by the state legislature to plan a California High-Speed Rail system to put before the voters. The plan they devised, 2008 California Proposition 1A, connecting all the major population centers in the state, was approved by the voters at the November 2008 general election. The first phase of construction was begun in 2015, and the first segment long, is planned to be put into operation by the end of 2030. Planning and work on the rest of the system is continuing, with funding for completing it is an ongoing issue. California's 2023 integrated passenger rail master plan includes a high speed rail system.\nBusses.\nNearly all counties operate bus lines, and many cities operate their own city bus lines as well. Intercity bus travel is provided by Greyhound, Megabus, and Amtrak Thruway.\nWater.\nCalifornia's interconnected water system is the world's largest, managing over of water per year, centered on six main systems of aqueducts and infrastructure projects. Water use and conservation in California is a politically divisive issue, as the state experiences periodic droughts and has to balance the demands of its large agricultural and urban sectors, especially in the arid southern portion of the state. The state's widespread redistribution of water also invites the frequent scorn of environmentalists.\nThe California Water Wars, a conflict between Los Angeles and the Owens Valley over water rights, is one of the most well-known examples of the struggle to secure adequate water supplies. Former California Governor Arnold Schwarzenegger said: We've been in crisis for quite some time because we're now 38million people and not anymore 18million people like we were in the late 60s. So it developed into a battle between environmentalists and farmers and between the south and the north and between rural and urban. And everyone has been fighting for the last four decades about water.\nGovernment and politics.\nState government.\nThe capital city of California is Sacramento.\nThe state is organized into three branches of government\u2014the executive branch consisting of the governor and the other independently elected constitutional officers; the legislative branch consisting of the Assembly and Senate; and the judicial branch consisting of the Supreme Court of California and lower courts. The state also allows ballot propositions: direct participation of the electorate by initiative, referendum, recall, and ratification. Before the passage of Proposition 14 in 2010, California allowed each political party to choose whether to have a closed primary or a primary where only party members and independents vote. After June 8, 2010, when Proposition 14 was approved, excepting only the United States president and county central committee offices, all candidates in the primary elections are listed on the ballot with their preferred party affiliation, but they are not the official nominee of that party. At the primary election, the two candidates with the top votes will advance to the general election regardless of party affiliation. This is known as a nonpartisan blanket primary. If at a special primary election, one candidate receives more than 50% of all the votes cast, they are elected to fill the vacancy and no special general election will be held.\nExecutive branch.\nThe California executive branch consists of the governor and seven other elected constitutional officers: lieutenant governor, attorney general, secretary of state, state controller, state treasurer, insurance commissioner, and state superintendent of public instruction. They serve four-year terms and may be re-elected only once.\nThe many California state agencies that are under the governor's cabinet are grouped together to form cabinet-level entities that are referred to by government officials as \"superagencies\". Those departments that are directly under the other independently elected officers work separately from these superagencies.\nLegislative branch.\nThe California State Legislature consists of a 40-member Senate and 80-member Assembly. Senators serve four-year terms and Assembly members two. Members of the Assembly are subject to term limits of six terms, and members of the Senate are subject to term limits of three terms.\nJudicial branch.\nCalifornia's legal system is explicitly based upon English common law but carries many features from Spanish civil law, such as community property. California's prison population grew from 25,000 in 1980 to over 170,000 in 2007. Capital punishment is a legal form of punishment and the state has the largest \"Death Row\" population in the country (though Oklahoma and Texas are far more active in carrying out executions). California has performed 13 executions since 1976, with the last being in 2006.\nCalifornia's judiciary system is the largest in the United States with a total of 1,600 judges (the federal system has only about 840). At the apex is the seven-member Supreme Court of California, while the California Courts of Appeal serve as the primary appellate courts and the California Superior Courts serve as the primary trial courts. Justices of the Supreme Court and Courts of Appeal are appointed by the governor, but are subject to retention by the electorate every 12 years.\nThe administration of the state's court system is controlled by the Judicial Council, composed of the chief justice of the California Supreme Court, 14 judicial officers, four representatives from the State Bar of California, and one member from each house of the state legislature.\nIn fiscal year 2020\u20132021, the state judiciary's 2,000 judicial officers and 18,000 judicial branch employees processed approximately 4.4 million cases.\nLocal government.\nCalifornia has an extensive system of local government that manages public functions throughout the state. Like most states, California is divided into counties, of which there are 58 (including San Francisco) covering the entire state. Most urbanized areas are incorporated as cities. School districts, which are independent of cities and counties, handle public education. Many other functions, such as fire protection and water supply, especially in unincorporated areas, are handled by special districts.\nCounties.\nCalifornia is divided into 58 counties. Per Article 11, Section 1, of the Constitution of California, they are the legal subdivisions of the state. The county government provides countywide services such as law enforcement, jails, elections and voter registration, vital records, property assessment and records, tax collection, public health, health care, social services, libraries, flood control, fire protection, animal control, agricultural regulations, building inspections, ambulance services, and education departments in charge of maintaining statewide standards. In addition, the county serves as the local government for all unincorporated areas. Each county is governed by an elected board of supervisors.\nCity and town governments.\nIncorporated cities and towns in California are either charter or general-law municipalities. General-law municipalities owe their existence to state law and are consequently governed by it; charter municipalities are governed by their own city or town charters. Municipalities incorporated in the 19th century tend to be charter municipalities. All ten of the state's most populous cities are charter cities. Most small cities have a council\u2013manager form of government, where the elected city council appoints a city manager to supervise the operations of the city. Some larger cities have a directly elected mayor who oversees the city government. In many council-manager cities, the city council selects one of its members as a mayor, sometimes rotating through the council membership\u2014but this type of mayoral position is primarily ceremonial. The Government of San Francisco is the only consolidated city-county in California, where both the city and county governments have been merged into one unified jurisdiction.\nSchool districts and special districts.\nAbout 1,102 school districts, independent of cities and counties, handle California's public education. California school districts may be organized as elementary districts, high school districts, unified school districts combining elementary and high school grades, or community college districts.\nThere are about 3,400 special districts in California. A special district, defined by California Government Code \u00a7 16271(d) as \"any agency of the state for the local performance of governmental or proprietary functions within limited boundaries\", provides a limited range of services within a defined geographic area. The geographic area of a special district can spread across multiple cities or counties, or could consist of only a portion of one. Most of California's special districts are \"single-purpose districts\", and provide one service.\nFederal representation.\nThe state of California sends 52 members to the House of Representatives, the nation's largest congressional state delegation. Consequently, California also has the largest number of electoral votes in national presidential elections, with 54. The former speaker of the House of Representatives is the representative of California's 20th district, Kevin McCarthy.\nCalifornia is represented in the United States Senate by Alex Padilla and Adam Schiff.\nArmed forces.\nIn California, , the U.S. Department of Defense had a total of 117,806 active duty servicemembers of which 88,370 were Sailors or Marines, 18,339 were Airmen, and 11,097 were Soldiers, with 61,365 Department of Defense civilian employees. Additionally, there were a total of 57,792 Reservists and Guardsman in California.\nIn 2010, Los Angeles County was the largest origin of military recruits in the United States by county, with 1,437 individuals enlisting in the military. However, , Californians were relatively under-represented in the military as a proportion to its population.\nIn 2000, California, had 2,569,340 veterans of United States military service. , there were 1,942,775 veterans living in California, of which 1,457,875 served during a period of armed conflict, and just over four thousand served before World WarII (the largest population of this group of any state).\nCalifornia's military forces consist of the Army and Air National Guard, the naval and state military reserve (militia), and the California Cadet Corps.\nPolitics.\nCalifornia has an idiosyncratic political culture compared to the rest of the country, and is sometimes regarded as a trendsetter. In socio-cultural mores and national politics, Californians are perceived as more liberal than other Americans, especially those who live in the inland states. In the 2016 United States presidential election, California had the third highest percentage of Democratic votes behind the District of Columbia and Hawaii. In the 2020 United States presidential election, it had the 6th highest behind the District of Columbia, Vermont, Massachusetts, Maryland, and Hawaii. According to the Cook Political Report, California contains five of the 15 most Democratic congressional districts in the United States.\nCalifornia was the second state to recall their state governor, the second state to legalize abortion, and the only state to ban marriage for gay couples twice by vote (including Proposition8 in 2008). Voters also passed Proposition 71 in 2004 to fund stem cell research, making California the second state to legalize stem cell research, and Proposition 14 in 2010 to completely change the state's primary election process. California has also experienced disputes over water rights; and a tax revolt, culminating with the passage of Proposition 13 in 1978, limiting state property taxes. California voters have rejected affirmative action on multiple occasions, most recently in November 2020.\nThe state's trend towards the Democratic Party and away from the Republican Party can be seen in state elections. From 1899 to 1939, California had exclusively Republican governors. Since 1990, California has generally elected Democratic candidates to federal, state and local offices, including current Governor Gavin Newsom; however, the state has elected Republican Governors, though many of its Republican Governors, such as Arnold Schwarzenegger, tend to be considered moderate Republicans and more centrist than the national party.\nSeveral political movements have advocated for California independence. The California National Party and the California Freedom Coalition both advocate for California independence along the lines of progressivism and civic nationalism. The Yes California movement attempted to organize an independence referendum via ballot initiative for 2019, which was then postponed.\nThe Democrats also hold a supermajority in both houses of the state legislature. There are 62 Democrats and 18 Republicans in the Assembly; and 32 Democrats and eight Republicans in the Senate.\nFrom 1952 through 1988, California was a Republican-leaning state, as the party carried the state's electoral votes in nine of ten elections, with 1964 as the sole exception. Southern California Republicans Richard Nixon and Ronald Reagan were both elected twice as the 37th and 40th U.S. Presidents, respectively. However, Democrats have won all of California's electoral votes for the last eight elections, starting in 1992.\nIn the United States House, the Democrats held a 34\u201319 edge in the California delegation of the 110th United States Congress in 2007. As the result of gerrymandering, the districts in California were usually dominated by one or the other party, and few districts were considered competitive. In 2008, Californians passed Proposition 20 to empower a 14-member independent citizen commission to redraw districts for both local politicians and Congress. After the 2012 elections, when the new system took effect, Democrats gained four seats and held a 38\u201315 majority in the delegation. Following the 2018 midterm House elections, Democrats won 46 out of 53 congressional house seats in California, leaving Republicans with seven.\nIn general, Democratic strength is centered in the populous coastal regions of the Los Angeles metropolitan area and the San Francisco Bay Area. Republican strength is still greatest in eastern parts of the state. Orange County had remained largely Republican until the 2016 and 2018 elections, in which a majority of the county's votes were cast for Democratic candidates. One study ranked Berkeley, Oakland, Inglewood and San Francisco in the top 20 most liberal American cities; and Bakersfield, Orange, Escondido, Garden Grove, and Simi Valley in the top 20 most conservative cities.\nIn October 2022, out of the 26,876,800 people eligible to vote, 21,940,274 people were registered to vote. Of the people registered, the three largest registered groups were Democrats (10,283,258), Republicans (5,232,094), and No Party Preference (4,943,696).\nCalifornia retains the death penalty, though it has not been used since 2006.\nTwinned regions.\nCalifornia has region twinning arrangements with:"}
{"id": "5408", "revid": "6941696", "url": "https://en.wikipedia.org/wiki?curid=5408", "title": "Columbia River", "text": "The Columbia River (Upper Chinook: ' or '; Sahaptin: \"Nch\u2019i-W\u00e0na\" or \"Nchi wana\"; Sinixt dialect\" \") is the largest river in the Pacific Northwest region of North America. The river forms in the Rocky Mountains of British Columbia, Canada. It flows northwest and then south into the U.S. state of Washington, then turns west to form most of the border between Washington and the state of Oregon before emptying into the Pacific Ocean. The river is long, and its largest tributary is the Snake River. Its drainage basin is roughly the size of France and extends into seven states of the United States and one Canadian province. The fourth-largest river in the United States by flow, the Columbia has the greatest flow of any river into the eastern Pacific.\nThe Columbia and its tributaries have been central to the region's culture and economy for thousands of years. They have been used for transportation since ancient times, linking the region's many cultural groups. The river system hosts many species of anadromous fish, which migrate between freshwater habitats and the saline waters of the Pacific Ocean. These fish\u2014especially the salmon species\u2014provided the core subsistence for native peoples.\nThe first documented European discovery of the Columbia River occurred when Bruno de Heceta sighted the river's mouth in 1775. On May 11, 1792, a private American ship, Columbia Rediviva, under Captain Robert Gray from Boston became the first non-indigenous vessel to enter the river. Later in 1792, William Robert Broughton of the British Royal Navy commanding HMS \"Chatham\" as part of the Vancouver Expedition, navigated past the Oregon Coast Range and upriver to what is now Vancouver, Washington. In the following decades, fur-trading companies used the Columbia as a key transportation route. Overland explorers entered the Willamette Valley through the scenic, but treacherous Columbia River Gorge, and pioneers began to settle the valley in increasing numbers. Steamships along the river linked communities and facilitated trade; the arrival of railroads in the late 19th century, many running along the river, supplemented these links.\nSince the late 19th century, public and private sectors have extensively developed the river. To aid ship and barge navigation, locks have been built along the lower Columbia and its tributaries, and dredging has opened, maintained, and enlarged shipping channels. Since the early 20th century, dams have been built across the river for power generation, navigation, irrigation, and flood control. The 14 hydroelectric dams on the Columbia's main stem and many more on its tributaries produce more than 44\u00a0percent of total U.S. hydroelectric generation. Production of nuclear power has taken place at two sites along the river. Plutonium for nuclear weapons was produced for decades at the Hanford Site, which is now the most contaminated nuclear site in the United States. These developments have greatly altered river environments in the watershed, mainly through industrial pollution and barriers to fish migration.\nCourse.\nThe Columbia begins its journey in the southern Rocky Mountain Trench in British Columbia (BC). Columbia Lake above sea level and the adjoining Columbia Wetlands form the river's headwaters. The trench is a broad, deep, and long glacial valley between the Canadian Rockies and the Columbia Mountains in BC. For its first , the Columbia flows northwest along the trench through Windermere Lake and the town of Invermere, a region known in BC as the Columbia Valley, then northwest to Golden and into Kinbasket Lake. Rounding the northern end of the Selkirk Mountains, the river turns sharply south through a region known as the Big Bend Country, passing through Revelstoke Lake and the Arrow Lakes. Revelstoke, the Big Bend, and the Columbia Valley combined are referred to in BC parlance as the Columbia Country. Below the Arrow Lakes, the Columbia passes the cities of Castlegar, located at the Columbia's confluence with the Kootenay River, and Trail, two major population centers of the West Kootenay region. The Pend Oreille River joins the Columbia about north of the United States\u2013Canada border.\nThe Columbia enters eastern Washington flowing south and turning to the west at the Spokane River confluence. It marks the southern and eastern borders of the Colville Indian Reservation and the western border of the Spokane Indian Reservation. The river turns south after the Okanogan River confluence, then southeasterly near the confluence with the Wenatchee River in central Washington. This C-shaped segment of the river is also known as the \"Big Bend\". During the Missoula Floods 1015,000\u00a0years ago, much of the floodwater took a more direct route south, forming the ancient river bed known as the Grand Coulee. After the floods, the river found its present course, and the Grand Coulee was left dry. The construction of the Grand Coulee Dam in the mid-20th\u00a0century impounded the river, forming Lake Roosevelt, from which water was pumped into the dry coulee, forming the reservoir of Banks Lake.\nThe river flows past The Gorge Amphitheatre, a prominent concert venue in the Northwest, then through Priest Rapids Dam, and then through the Hanford Nuclear Reservation. Entirely within the reservation is Hanford Reach, the only U.S. stretch of the river that is completely free-flowing, unimpeded by dams, and not a tidal estuary. The Snake River and Yakima River join the Columbia in the Tri-Cities population center. The Columbia makes a sharp bend to the west at the Washington\u2013Oregon border. The river defines that border for the final of its journey.\nThe Deschutes River joins the Columbia near The Dalles. Between The Dalles and Portland, the river cuts through the Cascade Range, forming the dramatic Columbia River Gorge. Via the gorge, the Columbia crosses the Cascades at a lower elevation than any other river. The gorge is known for its strong and steady winds, scenic beauty, and its role as an important transportation link. The river continues west, bending sharply to the north-northwest near Portland and Vancouver, Washington, at the Willamette River confluence. Here the river slows considerably, dropping sediment that might otherwise form a river delta at the Columbia's mouth. Near Longview, Washington and the Cowlitz River confluence, the river turns west again. The Columbia empties into the Pacific Ocean just west of Astoria, Oregon, over the Columbia Bar, a shifting sandbar that makes the river's mouth one of the most hazardous stretches of water to navigate in the world. Because of the danger and the many shipwrecks near the mouth, it acquired a reputation as the \"Graveyard of Ships\".\nThe Columbia drains an area of about . Its drainage basin covers nearly all of Idaho, large portions of British Columbia, Oregon, and Washington, and ultimately all of Montana west of the Continental Divide, and small portions of Wyoming, Utah, and Nevada; the total area is similar to the size of France. Roughly of the river's length and 85\u00a0percent of its drainage basin are in the US. The Columbia is the twelfth-longest river and has the sixth-largest drainage basin in the United States. In Canada, where the Columbia flows for and drains , the river ranks 23rd in length, and the Canadian part of its basin ranks 13th in size among Canadian basins. \nThe Columbia shares its name with nearby places, such as British Columbia, as well as with landforms and bodies of water.\nDischarge.\nWith an average flow at the mouth of about , the Columbia is the largest river by discharge flowing into the Pacific from the Americas and is the fourth-largest by volume in the U.S. The average flow where the river crosses the international border between Canada and the United States is from a drainage basin of . This amounts to about 15 percent of the entire Columbia watershed. The Columbia's highest recorded flow, measured at The Dalles, was in June 1894, before the river was dammed. The lowest flow recorded at The Dalles was on April 16, 1968, and was caused by the initial closure of the John Day Dam, upstream. The Dalles is about from the mouth; the river at this point drains about or about 91 percent of the total watershed. Flow rates on the Columbia are affected by many large upstream reservoirs, many diversions for irrigation, and, on the lower stretches, reverse flow from the tides of the Pacific Ocean. The National Ocean Service observes water levels at six tide gauges and issues tide forecasts for twenty-two additional locations along the river between the entrance at the North Jetty and the base of Bonneville Dam, its head of tide.\nGeology.\nWhen the rifting of Pangaea, due to the process of plate tectonics, pushed North America away from Europe and Africa and into the Panthalassic Ocean (ancestor to the modern Pacific Ocean), the Pacific Northwest was not part of the continent. As the North American continent moved westward, the Farallon Plate subducted under its western margin. As the plate subducted, it carried along island arcs which were accreted to the North American continent, resulting in the creation of the Pacific Northwest between 150 and 90\u00a0million years ago. The general outline of the Columbia Basin was not complete until between 60 and 40\u00a0million years ago, but it lay under a large inland sea later subject to uplift. Between 50 and 20\u00a0million years ago, from the Eocene through the Miocene eras, tremendous volcanic eruptions frequently modified much of the landscape traversed by the Columbia. The lower reaches of the ancestral river passed through a valley near where Mount Hood later arose. Carrying sediments from erosion and erupting volcanoes, it built a thick delta that underlies the foothills on the east side of the Coast Range near Vernonia in northwestern Oregon. Between 17\u00a0million and 6\u00a0million years ago, huge outpourings of flood basalt lava covered the Columbia River Plateau and forced the lower Columbia into its present course. The modern Cascade Range began to uplift 5 to 4\u00a0million years ago. Cutting through the uplifting mountains, the Columbia River significantly deepened the Columbia River Gorge.\nThe river and its drainage basin experienced some of the world's greatest known catastrophic floods toward the end of the last ice age. The periodic rupturing of ice dams at Glacial Lake Missoula resulted in the Missoula Floods, with discharges exceeding the combined flow of all the other rivers in the world, dozens of times over thousands of years. The exact number of floods is unknown, but geologists have documented at least 40; evidence suggests that they occurred between about 19,000 and 13,000\u00a0years ago.\nThe floodwaters rushed across eastern Washington, creating the channeled scablands, which are a complex network of dry canyon-like channels, or coulees that are often braided and sharply gouged into the basalt rock underlying the region's deep topsoil. Numerous flat-topped buttes with rich soil stand high above the chaotic scablands. Constrictions at several places caused the floodwaters to pool into large temporary lakes, such as Lake Lewis, in which sediments were deposited. Water depths have been estimated at at Wallula Gap and over modern Portland, Oregon. Sediments were also deposited when the floodwaters slowed in the broad flats of the Quincy, Othello, and Pasco Basins. The floods' periodic inundation of the lower Columbia River Plateau deposited rich sediments; 21st-century farmers in the Willamette Valley \"plow fields of fertile Montana soil and clays from Washington's Palouse\".\nOver the last several thousand years a series of large landslides have occurred on the north side of the Columbia River Gorge, sending massive amounts of debris south from Table Mountain and Greenleaf Peak into the gorge near the present site of Bonneville Dam. The most recent and significant is known as the Bonneville Slide, which formed a massive earthen dam, filling of the river's length. Various studies have placed the date of the Bonneville Slide anywhere between 1060 and 1760 AD; the idea that the landslide debris present today was formed by more than one slide is relatively recent and may explain the large range of estimates. It has been suggested that if the later dates are accurate there may be a link with the 1700 Cascadia earthquake. The pile of debris resulting from the Bonneville Slide blocked the river until rising water finally washed away the sediment. It is not known how long it took the river to break through the barrier; estimates range from several months to several years. Much of the landslide's debris remained, forcing the river about south of its previous channel and forming the Cascade Rapids. In 1938, the construction of Bonneville Dam inundated the rapids as well as the remaining trees that could be used to refine the estimated date of the landslide.\nIn 1980, the eruption of Mount St.\u00a0Helens deposited large amounts of sediment in the lower Columbia, temporarily reducing the depth of the shipping channel by .\nIndigenous peoples.\nHumans have inhabited the Columbia's watershed for more than 15,000\u00a0years, with a transition to a sedentary lifestyle based mainly on salmon starting about 3,500\u00a0years ago. In 1962, archaeologists found evidence of human activity dating back 11,230\u00a0years at the Marmes Rockshelter, near the confluence of the Palouse and Snake rivers in eastern Washington. In 1996 the skeletal remains of a 9,000-year-old prehistoric man (dubbed Kennewick Man) were found near Kennewick, Washington. The discovery rekindled debate in the scientific community over the origins of human habitation in North America and sparked a protracted controversy over whether the scientific or Native American community was entitled to possess and/or study the remains.\nMany different Native Americans and First Nations peoples have a historical and continuing presence on the Columbia. South of the Canada\u2013US\u00a0border, the Colville, Spokane, Coeur d'Alene, Yakama, Nez Perce, Cayuse, Palus, Umatilla, Cowlitz, and the Confederated Tribes of Warm Springs live along the US stretch. Along the upper Snake River and Salmon River, the Shoshone Bannock tribes are present. The Sinixt or Lakes people lived on the lower stretch of the Canadian portion, while above that the Shuswap people (Secwepemc in their own language) reckon the whole of the upper Columbia east to the Rockies as part of their territory. The Canadian portion of the Columbia Basin outlines the traditional homelands of the Canadian Kootenay\u2013Ktunaxa.\nThe Chinook tribe, which is not federally recognized, who live near the lower Columbia River, call it ' or ' in the Upper Chinook (Kiksht) language, and it is \"Nch\u2019i-W\u00e0na\" or \"Nchi wana\" to the Sahaptin (Ichishk\u00edin S\u0268\u0301nwit)-speaking peoples of its middle course in present-day Washington. The river is known as \"\" by the Sinixt people, who live in the area of the Arrow Lakes in the river's upper reaches in Canada. All three terms essentially mean \"the big river\".\nOral histories describe the formation and destruction of the Bridge of the Gods, a land bridge that connected the Oregon and Washington sides of the river in the Columbia River Gorge. The bridge, which aligns with geological records of the Bonneville Slide, was described in some stories as the result of a battle between gods, represented by Mount Adams and Mount Hood, in their competition for the affection of a goddess, represented by Mount St.\u00a0Helens. Native American stories about the bridge differ in their details but agree in general that the bridge permitted increased interaction between tribes on the north and south sides of the river.\nHorses, originally acquired from Spanish New Mexico, spread widely via native trade networks, reaching the Shoshone of the Snake River Plain by 1700. The Nez Perce, Cayuse, and Flathead people acquired their first horses around 1730. Along with horses came aspects of the emerging plains culture, such as equestrian and horse training skills, greatly increased mobility, hunting efficiency, trade over long distances, intensified warfare, the linking of wealth and prestige to horses and war, and the rise of large and powerful tribal confederacies. The Nez Perce and Cayuse kept large herds and made annual long-distance trips to the Great Plains for bison hunting, adopted the plains culture to a significant degree, and became the main conduit through which horses and the plains culture diffused into the Columbia River region. Other peoples acquired horses and aspects of the plains culture unevenly. The Yakama, Umatilla, Palus, Spokane, and Coeur d'Alene maintained sizable herds of horses and adopted some of the plains cultural characteristics, but fishing and fish-related economies remained important. Less affected groups included the Molala, Klickitat, Wenatchi, Okanagan, and Sinkiuse-Columbia peoples, who owned small numbers of horses and adopted few plains culture features. Some groups remained essentially unaffected, such as the Sanpoil and Nespelem people, whose culture remained centered on fishing.\nNatives of the region encountered foreigners at several times and places during the 18th\u00a0and 19th\u00a0centuries. European and American vessels explored the coastal area around the mouth of the river in the late 18th\u00a0century, trading with local natives. The contact would prove devastating to the indigenous Chinookan speaking peoples; a large portion of their population was wiped out by a smallpox epidemic. Canadian explorer Alexander Mackenzie crossed what is now interior British Columbia in 1793. From 1805 to 1806, the Lewis and Clark Expedition entered the Oregon Country along the Clearwater and Snake rivers, and encountered numerous small settlements of natives. Their records recount tales of hospitable traders who were not above stealing small items from the visitors. They also noted brass teakettles, a British musket, and other artifacts that had been obtained in trade with coastal tribes. From the earliest contact with westerners, the natives of the mid- and lower Columbia were not tribal, but instead congregated in social units no larger than a village, and more often at a family level; these units would shift with the season as people moved about, following the salmon catch up and down the river's tributaries.\nSparked by the 1847 Whitman Massacre, a number of violent battles were fought between American settlers and the region's natives. The subsequent wars over Northwest territory, especially the Yakima War, decimated the native population and removed much land from native control. As years progressed, the right of natives to fish along the Columbia became the central issue of contention with the states, commercial fishers, and private property owners. The US Supreme Court upheld fishing rights in landmark cases in 1905 and 1918, as well as the 1974 case \"United States v. Washington\", commonly called the Boldt Decision.\nFish were central to the culture of the region's natives, both as sustenance and as part of their religious beliefs. Natives drew fish from the Columbia at several major sites, which also served as trading posts. Celilo Falls, located east of the modern city of The Dalles, was a vital hub for trade and the interaction of different cultural groups, being used for fishing and trading for 11,000\u00a0years. Prior to contact with westerners, villages along this stretch may have at times had a population as great as 10,000. The site drew traders from as far away as the Great Plains.\nThe Cascades Rapids of the Columbia River Gorge, and Kettle Falls and Priest Rapids in eastern Washington, were also major fishing and trading sites.\nIn prehistoric times the Columbia's salmon and steelhead runs numbered an estimated annual average of 10 to 16\u00a0million fish. In comparison, the largest run since 1938 was in 1986, with 3.2\u00a0million fish entering the Columbia. The annual catch by natives has been estimated at . The most important and productive native fishing site was located at Celilo Falls, which was perhaps the most productive inland fishing site in North America. The falls were located at the border between Chinookan- and Sahaptian-speaking peoples and served as the center of an extensive trading network across the Pacific Plateau. Celilo was the oldest continuously inhabited community on the North American continent.\nSalmon canneries established by white settlers beginning in 1866 had a strong negative impact on the salmon population, and in 1908 US president Theodore Roosevelt observed that the salmon runs were but a fraction of what they had been 25\u00a0years prior.\nAs river development continued in the 20th\u00a0century, each of these major fishing sites was flooded by a dam, beginning with Cascades Rapids in 1938. The development was accompanied by extensive negotiations between natives and US government agencies. The Confederated Tribes of Warm Springs, a coalition of various tribes, adopted a constitution and incorporated after the 1938 completion of the Bonneville Dam flooded Cascades Rapids; Still, in the 1930s, there were natives who lived along the river and fished year round, moving along with the fish's migration patterns throughout the seasons. The Yakama were slower to do so, organizing a formal government in 1944. In the 21st\u00a0century, the Yakama, Nez Perce, Umatilla, and Warm Springs tribes all have treaty fishing rights along the Columbia and its tributaries.\nIn 1957 Celilo Falls was submerged by the construction of The Dalles Dam, and the native fishing community was displaced. The affected tribes received a $26.8\u00a0million settlement for the loss of Celilo and other fishing sites submerged by The Dalles Dam. The Confederated Tribes of Warm Springs used part of its $4\u00a0million settlement to establish the Kah-Nee-Ta resort south of Mount Hood.\nIn 1977, 75 indigenous fishermen of the Yakama Tribe were arrested in a federal sting operation which claimed that fishermen were poaching up to 40,000 fish in the Columbia River. Fishermen placed on trial received sentences ranging from six months to five years. The federal government pinned Yakama Tribe member David Sohappy ringleader of the operation. After the trial ended, it was determined that the fish were not poached, but driven away because of harmful chemicals present in the power plant. These harmful chemicals mainly consisted of aluminum. This event is commonly known today as the \"Salmon Scam\".\nShortly after the Salmon Scam, many Columbia River-based indigenous tribes received federally recognized status. The Siletz Tribe was the first to restore its federal recognition in 1977, followed by the Cow Creek Band of the Umpqua Tribe in 1982, the Grand Ronde Tribe in 1983, the Lower Umpqua Tribe, Siuslaw Tribe, and Coos Tribe in 1984, the Klamath Tribe in 1986, and the Coquille Tribe in 1989. While all the aforementioned tribes received federally recognized status, the Chinook Indian Nation had their federal recognition revoked in 2002 by the Bush administration, and are fighting to have it restored.\nIn 2023, members of the Yakama Nation expressed their dismay for the construction of a Goldendale-based pumped hydroelectric energy storage project. Jeremy Takala of the Yakama Nation embodies Yakama belief on the importance of Columbia River crops to food and medicine, stating \"the [Goldendale] project being proposed here, it will definitely impact our life\". \u00a0The Goldendale-pumped hydro storage unit could allow for reused water use in reservoirs, which would be placed on mountainous terrain overlooking the Columbia River. The mountainous terrain where the unit would be placed in is Juniper Point, referred to by the Yakama as Pushpum. Pushpum has rock formations, as well as food and medicine capabilities that are essential to the Yakama. Members of the Yakama tribe wish for consent on the Goldendale project, as opposed to consultation.\nNew waves of explorers.\nSome historians believe that Japanese or Chinese vessels blown off course reached the Northwest Coast long before Europeans\u2014possibly as early as 219\u00a0BCE. Historian Derek Hayes claims that \"It is a near certainty that Japanese or Chinese people arrived on the northwest coast long before any European.\" It is unknown whether they landed near the Columbia. Evidence exists that Spanish castaways reached the shore in 1679 and traded with the Clatsop; if these were the first Europeans to see the Columbia, they failed to send word home to Spain.\nIn the 18th\u00a0century, there was strong interest in discovering a Northwest Passage that would permit navigation between the Atlantic (or inland North America) and the Pacific Ocean. Many ships in the area, especially those under Spanish and British command, searched the northwest coast for a large river that might connect to Hudson Bay or the Missouri River. The first documented European discovery of the Columbia River was that of Bruno de Heceta, who in 1775 sighted the river's mouth. On the advice of his officers, he did not explore it, as he was short-staffed and the current was strong. He considered it a bay, and called it \"Ensenada de Asunci\u00f3n\" (\"Assumption Cove\"). Later Spanish maps, based on his sighting, showed a river, labeled \"R\u00edo de San Roque\" (\"The Saint Roch River\"), or an entrance, called \"Entrada de Hezeta\", named for Bruno de Hezeta, who sailed the region. Following Hezeta's reports, British maritime fur trader Captain John Meares searched for the river in 1788 but concluded that it did not exist. He named Cape Disappointment for the non-existent river, not realizing the cape marks the northern edge of the river's mouth.\nWhat happened next would form the basis for decades of both cooperation and dispute between British and American exploration of, and ownership claim to, the region. Royal Navy commander George Vancouver sailed past the mouth in April 1792 and observed a change in the water's color, but he accepted Meares' report and continued on his journey northward. Later that month, Vancouver encountered the American captain Robert Gray at the Strait of Juan de Fuca. Gray reported that he had seen the entrance to the Columbia and had spent nine days trying but failing to enter.\nOn May 12, 1792, Gray returned south and crossed the Columbia Bar, becoming the first known explorer of European descent to enter the river. Gray's fur trading mission had been financed by Boston merchants, who outfitted him with a private vessel named \"Columbia Rediviva\"; he named the river after the ship on May 18. Gray spent nine days trading near the mouth of the Columbia, then left without having gone beyond upstream. The farthest point reached was Grays Bay at the mouth of Grays River. Gray's discovery of the Columbia River was later used by the United States to support its claim to the Oregon Country, which was also claimed by Russia, Great Britain, Spain and other nations.\nIn October 1792, Vancouver sent Lieutenant William Robert Broughton, his second-in-command, up the river. Broughton got as far as the Sandy River at the western end of the Columbia River Gorge, about upstream, sighting and naming Mount Hood. Broughton formally claimed the river, its drainage basin, and the nearby coast for Britain. In contrast, Gray had not made any formal claims on behalf of the United States.\nBecause the Columbia was at the same latitude as the headwaters of the Missouri River, there was some speculation that Gray and Vancouver had discovered the long-sought Northwest Passage. A 1798 British map showed a dotted line connecting the Columbia with the Missouri. When the American explorers Meriwether Lewis and William Clark charted the vast, unmapped lands of the American West in their overland expedition (1803\u20131805), they found no passage between the rivers. After crossing the Rocky Mountains, Lewis and Clark built dugout canoes and paddled down the Snake River, reaching the Columbia near the present-day Tri-Cities, Washington. They explored a few miles upriver, as far as Bateman Island, before heading down the Columbia, concluding their journey at the river's mouth and establishing Fort Clatsop, a short-lived establishment that was occupied for less than three months.\nCanadian explorer David Thompson, of the North West Company, spent the winter of 180708 at Kootanae House near the source of the Columbia at present-day Invermere, BC. Over the next few years he explored much of the river and its northern tributaries. In 1811 he traveled down the Columbia to the Pacific Ocean, arriving at the mouth just after John Jacob Astor's Pacific Fur Company had founded Astoria. On his return to the north, Thompson explored the one remaining part of the river he had not yet seen, becoming the first Euro-descended person to travel the entire length of the river.\nIn 1825, the Hudson's Bay Company (HBC) established Fort Vancouver on the bank of the Columbia, in what is now Vancouver, Washington, as the headquarters of the company's Columbia District, which encompassed everything west of the Rocky Mountains, north of California, and south of Russian-claimed Alaska. Chief Factor John McLoughlin, a physician who had been in the fur trade since 1804, was appointed superintendent of the Columbia District. The HBC reoriented its Columbia District operations toward the Pacific Ocean via the Columbia, which became the region's main trunk route. In the early 1840s Americans began to colonize the Oregon country in large numbers via the Oregon Trail, despite the HBC's efforts to discourage American settlement in the region. For many the final leg of the journey involved travel down the lower Columbia River to Fort Vancouver. This part of the Oregon Trail, the treacherous stretch from The Dalles to below the Cascades, could not be traversed by horses or wagons (only watercraft, at great risk). This prompted the 1846 construction of the Barlow Road.\nIn the Treaty of 1818 the United States and Britain agreed that both nations were to enjoy equal rights in Oregon Country for 10\u00a0years. By 1828, when the so-called \"joint occupation\" was renewed indefinitely, it seemed probable that the lower Columbia River would in time become the border between the two nations. For years the Hudson's Bay Company successfully maintained control of the Columbia River and American attempts to gain a foothold were fended off. In the 1830s, American religious missions were established at several locations in the lower Columbia River region. In the 1840s a mass migration of American settlers undermined British control. The Hudson's Bay Company tried to maintain dominance by shifting from the fur trade, which was in decline, to exporting other goods such as salmon and lumber. Colonization schemes were attempted, but failed to match the scale of American settlement. Americans generally settled south of the Columbia, mainly in the Willamette Valley. The Hudson's Bay Company tried to establish settlements north of the river, but nearly all the British colonists moved south to the Willamette Valley. The hope that the British colonists might dilute the American presence in the valley failed in the face of the overwhelming number of American settlers. These developments rekindled the issue of \"joint occupation\" and the boundary dispute. While some British interests, especially the Hudson's Bay Company, fought for a boundary along the Columbia River, the Oregon Treaty of 1846 set the boundary at the 49th\u00a0parallel. As part of the treaty, the British retained all areas north of the line while the United States acquired the south. The Columbia River became much of the border between the U.S. territories of Oregon and Washington. Oregon became a U.S. state in 1859, while Washington later entered into the Union in 1889.\nBy the turn of the 20th\u00a0century, the difficulty of navigating the Columbia was seen as an impediment to the economic development of the Inland Empire region east of the Cascades. The dredging and dam building that followed would permanently alter the river, disrupting its natural flow but also providing electricity, irrigation, navigability and other benefits to the region.\nNavigation.\nAmerican captain Robert Gray and British captain George Vancouver, who explored the river in 1792, proved that it was possible to cross the Columbia Bar. Many of the challenges associated with that feat remain today; even with modern engineering alterations to the mouth of the river, the strong currents and shifting sandbar make it dangerous to pass between the river and the Pacific Ocean.\nThe use of steamboats along the river, beginning with the British \"Beaver\" in 1836 and followed by American vessels in 1850, contributed to the rapid settlement and economic development of the region. Steamboats operated in several distinct stretches of the river: on its lower reaches, from the Pacific Ocean to Cascades Rapids; from the Cascades to the Dalles-Celilo Falls; from Celilo to Priests Rapids; on the Wenatchee Reach of eastern Washington; on British Columbia's Arrow Lakes; and on tributaries like the Willamette, the Snake and Kootenay Lake. The boats, initially powered by burning wood, carried passengers and freight throughout the region for many years. Early railroads served to connect steamboat lines interrupted by waterfalls on the river's lower reaches. In the 1880s, railroads maintained by companies such as the Oregon Railroad and Navigation Company began to supplement steamboat operations as the major transportation links along the river.\nOpening the passage to Lewiston.\nAs early as 1881, industrialists proposed altering the natural channel of the Columbia to improve navigation. Changes to the river over the years have included the construction of jetties at the river's mouth, dredging, and the construction of canals and navigation locks. Today, ocean freighters can travel upriver as far as Portland and Vancouver, and barges can reach as far inland as Lewiston, Idaho.\nThe shifting Columbia Bar makes passage between the river and the Pacific Ocean difficult and dangerous, and numerous rapids along the river hinder navigation. \"Pacific Graveyard,\" a 1964 book by James A. Gibbs, describes the many shipwrecks near the mouth of the Columbia. Jetties, first constructed in 1886, extend the river's channel into the ocean. Strong currents and the shifting sandbar remain a threat to ships entering the river and necessitate continuous maintenance of the jetties.\nIn 1891, the Columbia was dredged to enhance shipping. The channel between the ocean and Portland and Vancouver was deepened from to . \"The Columbian\" called for the channel to be deepened to as early as 1905, but that depth was not attained until 1976.\nCascade Locks and Canal were first constructed in 1896 around the Cascades Rapids, enabling boats to travel safely through the Columbia River Gorge. The Celilo Canal, bypassing Celilo Falls, opened to river traffic in 1915. In the mid-20th\u00a0century, the construction of dams along the length of the river submerged the rapids beneath a series of reservoirs. An extensive system of locks allowed ships and barges to pass easily between reservoirs. A navigation channel reaching Lewiston, Idaho, along the Columbia and Snake rivers, was completed in 1975. Among the main commodities are wheat and other grains, mainly for export. As of 2016, the Columbia ranked third, behind the Mississippi and Paran\u00e1 rivers, among the world's largest export corridors for grain.\nThe 1980 eruption of Mount St. Helens caused mudslides in the area, which reduced the Columbia's depth by for a stretch, disrupting Portland's economy.\nDeeper shipping channel.\nEfforts to maintain and improve the navigation channel have continued to the present day. In 1990 a new round of studies examined the possibility of further dredging on the lower Columbia. The plans were controversial from the start because of economic and environmental concerns.\nIn 1999, Congress authorized deepening the channel between Portland and Astoria from , which will make it possible for large container and grain ships to reach Portland and Vancouver. The project has met opposition because of concerns about stirring up toxic sediment on the riverbed. Portland-based Northwest Environmental Advocates brought a lawsuit against the Army Corps of Engineers, but it was rejected by the Ninth U.S.\u00a0Circuit Court of Appeals in August 2006. The project includes measures to mitigate environmental damage; for instance, the US Army Corps of Engineers must restore 12 times the area of wetland damaged by the project. In early 2006, the Corps spilled of hydraulic oil into the Columbia, drawing further criticism from environmental organizations.\nWork on the project began in 2005 and concluded in 2010. The project's cost is estimated at $150\u00a0million. The federal government is paying 65\u00a0percent, Oregon and Washington are paying $27\u00a0million each, and six local ports are also contributing to the cost.\nDams.\nIn 1902, the United States Bureau of Reclamation was established to aid in the economic development of arid western states. One of its major undertakings was building Grand Coulee Dam to provide irrigation for the of the Columbia Basin Project in central Washington. With the onset of World War II, the focus of dam construction shifted to production of hydroelectricity. Irrigation efforts resumed after the war.\nRiver development occurred within the structure of the 1909 International Boundary Waters Treaty between the United States and Canada. The United States Congress passed the Rivers and Harbors Act of 1925, which directed the U.S. Army Corps of Engineers and the Federal Power Commission to explore the development of the nation's rivers. This prompted agencies to conduct the first formal financial analysis of hydroelectric development; the reports produced by various agencies were presented in House Document\u00a0308. Those reports, and subsequent related reports, are referred to as 308 Reports.\nThe 308 Reports generated 176 publications across the United States. Of those 176 documents, thirteen of them were generated in the Pacific Northwest. In 1932, one of the thirteen reports was released on the Columbia River, titled \"The Columbia River and Minor Tributaries.\" The report was backed by many engineers and state politicians who believed that the creation of Dams along the Columbia River would be a strong candidate for generation of hydroelectric power. The report led to congressional action, where dams at Bonneville and Grand Coulee were authorized in 1933.\nThe report itself emphasized the economic values of dam creation. Additionally, the reports emphasized the importance of dams for river navigation. Furthermore, the reports emphasized the importance of hydropower, storage of water for irrigation techniques, and flood control. To ensure that the dams did not affect biodiversity, many engineers and state politicians regarded the importance of salmon within the region. Thus, dams were to be built at a low height that would permit salmon to pass over.\nIn the late 1920s, political forces in the Northwestern United States generally favored the private development of hydroelectric dams along the Columbia. But the overwhelming victories of gubernatorial candidate George\u00a0W. Joseph in the 1930 Republican primary, and later his law partner Julius Meier, were understood to demonstrate strong public support for public ownership of dams. In 1933, President Franklin\u00a0D. Roosevelt signed a bill that enabled the construction of the Bonneville and Grand Coulee dams as public works projects. The legislation was attributed to the efforts of Oregon Senator Charles McNary, Washington Senator Clarence Dill, and Oregon Congressman Charles Martin, among others.\nIn 1948, floods swept through the Columbia watershed, destroying Vanport, then the second largest city in Oregon, and impacting cities as far north as Trail, BC. The flooding prompted the U.S. Congress to pass the Flood Control Act of 1950, authorizing the federal development of additional dams and other flood control mechanisms. By that time local communities had become wary of federal hydroelectric projects, and sought local control of new developments; a public utility district in Grant County, Washington, ultimately began construction of the dam at Priest Rapids.\nIn the 1960s, the United States and Canada signed the Columbia River Treaty, which focused on flood control and the maximization of downstream power generation. Canada agreed to build dams and provide reservoir storage, and the United States agreed to deliver to Canada one-half of the increase in United States downstream power benefits as estimated five years in advance. Canada's obligation was met by building three dams (two on the Columbia, and one on the Duncan River), the last of which was completed in 1973.\nToday the main stem of the Columbia River has fourteen dams, of which three are in Canada and eleven in the United States. Four mainstem dams and four lower Snake River dams contain navigation locks to allow ship and barge passage from the ocean as far as Lewiston, Idaho. The river system as a whole has more than 400\u00a0dams for hydroelectricity and irrigation. The dams address a variety of demands, including flood control, navigation, stream flow regulation, storage, and delivery of stored waters, reclamation of public lands and Indian reservations and territories, and the generation of hydroelectric power.\nThe larger U.S. dams are owned and operated by the federal government (some by the Army Corps of Engineers and some by the Bureau of Reclamation), while the smaller dams are operated by public utility districts and private power companies. The federally operated system is known as the Federal Columbia River Power System, which includes 31\u00a0dams on the Columbia and its tributaries. The system has altered the seasonal flow of the river to meet higher electricity demands during the winter. At the beginning of the 20th\u00a0century, roughly 75\u00a0percent of the Columbia's flow occurred in the summer, between April and September. By 1980, the summer proportion had been lowered to about 50\u00a0percent, essentially eliminating the seasonal pattern.\nUpon its creation in 1942, the Grand Coulee Dam required land inundation for construction. This flooding included parts of the Colville Reservation and Spokane Reservation. Reservation flooding has resulted in the displacement of 2,250 indigenous peoples located on said reservations.\nThe installation of dams dramatically altered the landscape and ecosystem of the river. At one time, the Columbia was one of the top salmon-producing river systems in the world. Previously active fishing sites, such as Celilo Falls in the eastern Columbia River Gorge, have exhibited a sharp decline in fishing along the Columbia in the last century, and salmon populations have been dramatically reduced. Fish ladders have been installed at some dam sites to help the fish journey to spawning waters. Fish ladders have been seen as highly effective when configurations to fishway exits are properly configured, as reconstructing the Bradford Island fish ladder allowed Sockeye salmon to stray away from spillway zones, reducing fallback (the rate at which fish are found moving away from a dam post-migration) and mortality birthrates. However, improper construction of fish ladders can result in salmon populations exerting significantly more energy when breeding, which results in higher levels of fallback. Chief Joseph Dam has no fish ladders and completely blocks fish migration to the upper half of the Columbia River system.\nIn 2019, both the Yakama and Lummi Northwest Nations proposed to remove the Bonneville, John Day, and The Dalles dams due to their belief removal would strengthen salmon population. The former Chairman of the Lummi Nation, Jay Julius, stated in 2019 that the fate of salmon without dam removal for the Lummi Nation is a \"horrifying reality\". JoDe Goudy, a former Chairman of the Yakama Nation, coincides with beliefs of the Lummi Nation, stating \"The Columbia River Dams were built on this false legal foundation, and decimated the Yakama Nation\u2019s fisheries, traditional foods, and cultural sites.\". Both Nations have worked with nonprofit advocacy organizations to further removal agendas.\nThroughout 2021, the nonprofit organization Earthjustice represented 10 conservation and fishing organizations in negotiations with President Joe Biden over removal of dam operations on the Snake River. These negotiations culminated in a lawsuit, with Earthjustice requesting to completely halt operations due to their belief of Snake River Dam effects on salmon and steelhead fish runs. On December 15, 2021, the two parties agreed to settle the dispute and focus on a plan to mitigate fish extinction.\nProposed in December 2023, President Joe Biden agreed to a $1 billion mandate, which will attempt to reintroduce Columbia River salmon blockaded by dams. The mandate asked for the Bonneville Power Administration to supply US$300 million over a ten-year span starting in 2024, which includes habitat restoration and upgrades in fish hatcheries. Furthermore, the Biden administration has publicly stated it will examine the possibility of doubling fish and wildlife spending to meet tribal needs. As a benchmark for financial utilization, US$1 billion in backlogged projects have been identified in the Columbia River Basin. The mandate did not call for the removal of four dams on the Snake River.\nIrrigation.\nThe Bureau of Reclamation's Columbia Basin Project focused on the generally dry region of central Washington known as the Columbia Basin, which features rich loess soil. Several groups developed competing proposals, and in 1933, President Franklin\u00a0D. Roosevelt authorized the Columbia Basin Project. The Grand Coulee Dam was the project's central component; upon completion, it pumped water up from the Columbia to fill the formerly dry Grand Coulee, forming Banks Lake. By 1935, the intended height of the dam was increased from a range between to , a height that would extend the lake impounded by the dam to the Canada\u2013United States border; the project had grown from a local New Deal relief measure to a major national project.\nThe project's initial purpose was irrigation, but the onset of World War\u00a0II created a high electricity demand, mainly for aluminum production and for the development of nuclear weapons at the Hanford Site. Irrigation began in 1951. The project provides water to more than of fertile but arid land in central Washington, transforming the region into a major agricultural center. Important crops include orchard fruit, potatoes, alfalfa, mint, beans, beets, and wine grapes.\nSince 1750, the Columbia has experienced six multi-year droughts. The longest, lasting 12\u00a0years in the mid\u201119th century, reduced the river's flow to 20\u00a0percent below average. Scientists have expressed concern that a similar drought would have grave consequences in a region so dependent on the Columbia. In 1992\u20131993, a lesser drought affected farmers, hydroelectric power producers, shippers, and wildlife managers.\nMany farmers in central Washington build dams on their property for irrigation and to control frost on their crops. The Washington Department of Ecology, using new techniques involving aerial photographs, estimated there may be as many as a hundred such dams in the area, most of which are illegal. Six such dams have failed in recent years, causing hundreds of thousands of dollars of damage to crops and public roads. Fourteen farms in the area have gone through the permitting process to build such dams legally.\nHydroelectricity.\nThe Columbia's heavy flow and large elevation drop over a short distance, , give it tremendous capacity for hydroelectricity generation. In comparison, the Mississippi drops less than . The Columbia alone possesses one-third of the United States's hydroelectric potential. In 2012, the river and its tributaries accounted for 29 GW of hydroelectric generating capacity, contributing 44\u00a0percent of the total hydroelectric generation in the nation.\nThe largest of the 150 hydroelectric projects, the Grand Coulee Dam and Chief Joseph Dam are also the largest in the United States. As of 2017, Grand Coulee is the fifth largest hydroelectric plant in the world.\nInexpensive hydropower supported the location of a large aluminum industry in the region because its reduction from bauxite requires large amounts of electricity. Until 2000, the Northwestern United States produced up to 17\u00a0percent of the world's aluminum and 40\u00a0percent of the aluminum produced in the United States. The commoditization of power in the early 21st century, coupled with a drought that reduced the generation capacity of the river, damaged the industry and by 2001, Columbia River aluminum producers had idled 80\u00a0percent of its production capacity. By 2003, the entire United States produced only 15\u00a0percent of the world's aluminum and many smelters along the Columbia had gone dormant or out of business.\nPower remains relatively inexpensive along the Columbia, and since the mid-2000 several global enterprises have moved server farm operations into the area to avail themselves of cheap power. Downriver of Grand Coulee, each dam's reservoir is closely regulated by the Bonneville Power Administration (BPA), the U.S. Army Corps of Engineers, and various Washington public utility districts to ensure flow, flood control, and power generation objectives are met. Increasingly, hydro-power operations are required to meet standards under the U.S. Endangered Species Act and other agreements to manage operations to minimize impacts on salmon and other fish, and some conservation and fishing groups support removing four dams on the lower Snake River, the largest tributary of the Columbia.\nIn 1941, the BPA hired Oklahoma folksinger Woody Guthrie to write songs for a documentary film promoting the benefits of hydropower. In the month he spent traveling the region Guthrie wrote 26 songs, which have become an important part of the cultural history of the region.\nEcology and environment.\nFish migration.\nThe Columbia supports several species of anadromous fish that migrate between the Pacific Ocean and freshwater tributaries of the river. Sockeye salmon, Coho and Chinook (\"king\") salmon, and steelhead, all of the genus \"Oncorhynchus\", are ocean fish that migrate up the rivers at the end of their life cycles to spawn. White sturgeon, which take 15 to 25\u00a0years to mature, typically migrate between the ocean and the upstream habitat several times during their lives.\nSalmon populations declined dramatically after the establishment of canneries in 1867. In 1879 it was reported that 545,450 salmon, with an average weight of were caught (in a recent season) and mainly canned for export to England. A can weighing could be sold for 8d or 9d. By 1908, there was widespread concern about the decline of salmon and sturgeon. In that year, the people of Oregon passed two laws under their newly instituted program of citizens' initiatives limiting fishing on the Columbia and other rivers. Then in 1948, another initiative banned the use of seine nets (devices already used by Native Americans, and refined by later settlers) altogether.\nDams interrupt the migration of anadromous fish. Salmon and steelhead return to the streams in which they were born to spawn; where dams prevent their return, entire populations of salmon die. Some of the Columbia and Snake River dams employ fish ladders, which are effective to varying degrees at allowing these fish to travel upstream. Another problem exists for the juvenile salmon headed downstream to the ocean. Previously, this journey would have taken two to three weeks. With river currents slowed by the dams, and the Columbia converted from a wild river to a series of slackwater pools, the journey can take several months, which increases the mortality rate. In some cases, the Army Corps of Engineers transports juvenile fish downstream by truck or river barge. The Chief Joseph Dam and several dams on the Columbia's tributaries entirely block migration, and there are no migrating fish on the river above these dams. Sturgeons have different migration habits and can survive without ever visiting the ocean. In many upstream areas cut off from the ocean by dams, sturgeon simply live upstream of the dam.\nNot all fish have suffered from the modifications to the river; the northern pikeminnow (formerly known as the \"squawfish\") thrives in the warmer, slower water created by the dams. Research in the mid-1980s found that juvenile salmon were suffering substantially from the predatory pikeminnow, and in 1990, in the interest of protecting salmon, a \"bounty\" program was established to reward anglers for catching pikeminnow.\nIn 1994, the salmon catch was smaller than usual in the rivers of Oregon, Washington, and British Columbia, causing concern among commercial fishermen, government agencies, and tribal leaders. US government intervention, to which the states of Alaska, Idaho, and Oregon objected, included an 11-day closure of an Alaska fishery. In April 1994 the Pacific Fisheries Management Council unanimously approved the strictest regulations in 18\u00a0years, banning all commercial salmon fishing for that year from Cape Falcon north to the Canada\u2013US border. In the winter of 1994, the return of coho salmon far exceeded expectations, which was attributed in part to the fishing ban.\nAlso in 1994, United States Secretary of the Interior Bruce Babbitt proposed the removal of several Pacific Northwest dams because of their impact on salmon spawning. The Northwest Power Planning Council approved a plan that provided more water for fish and less for electricity, irrigation, and transportation. Environmental advocates have called for the removal of certain dams in the Columbia system in the years since. Of the 227\u00a0major dams in the Columbia River drainage basin, the four Washington dams on the lower Snake River are often identified for removal, for example in an ongoing lawsuit concerning a Bush administration plan for salmon recovery. These dams and reservoirs limit the recovery of upriver salmon runs to Idaho's Salmon and Clearwater rivers. Historically, the Snake produced over 1.5\u00a0million spring and summer Chinook salmon, a number that has dwindled to several thousand in recent years. Idaho Power Company's Hells Canyon dams have no fish ladders (and do not pass juvenile salmon downstream), and thus allow no steelhead or salmon to migrate above Hells Canyon. In 2007, the destruction of the Marmot Dam on the Sandy River was the first dam removal in the system. Other Columbia Basin dams that have been removed include Condit Dam on Washington's White Salmon River, and the Milltown Dam on the Clark Fork in Montana.\nPollution.\nIn southeastern Washington, a stretch of the river passes through the Hanford Site, established in 1943 as part of the Manhattan Project. The site served as a plutonium production complex, with nine nuclear reactors and related facilities along the banks of the river. From 1944 to 1971, pump systems drew cooling water from the river and, after treating this water for use by the reactors, returned it to the river. Before being released back into the river, the used water was held in large tanks known as retention basins for up to six hours. Longer-lived isotopes were not affected by this retention, and several terabecquerels entered the river every day. By 1957, the eight plutonium production reactors at Hanford dumped a daily average of 50,000\u00a0curies of radioactive material into the Columbia. These releases were kept secret by the federal government until the release of declassified documents in the late 1980s. Radiation was measured downstream as far west as the Washington and Oregon coasts.\nThe nuclear reactors were decommissioned at the end of the Cold War, and the Hanford site is the focus of one of the world's largest environmental cleanup, managed by the Department of Energy under the oversight of the Washington Department of Ecology and the Environmental Protection Agency. Nearby aquifers contain an estimated 270\u00a0billion\u00a0US\u00a0gallons (1\u00a0billion\u00a0m3) of groundwater contaminated by high-level nuclear waste that has leaked out of Hanford's underground storage tanks. , 1\u00a0million\u00a0US\u00a0gallons (3,785\u00a0m3) of highly radioactive waste is traveling through groundwater toward the Columbia River. This waste is expected to reach the river in 12 to 50\u00a0years if cleanup does not proceed on schedule.\nIn addition to concerns about nuclear waste, numerous other pollutants are found in the river. These include chemical pesticides, bacteria, arsenic, dioxins, and polychlorinated biphenyls (PCB).\nStudies have also found significant levels of toxins in fish and the waters they inhabit within the basin. Accumulation of toxins in fish threatens the survival of fish species, and human consumption of these fish can lead to health problems. Water quality is also an important factor in the survival of other wildlife and plants that grow in the Columbia River drainage basin. The states, indigenous tribes, and federal government are all engaged in efforts to restore and improve the water, land, and air quality of the Columbia River drainage basin and have committed to work together to accomplish critical ecosystem restoration efforts. Several cleanup efforts are underway, including Superfund projects at Portland Harbor, Hanford, and Lake Roosevelt.\nIn early 2022, thousands of protestors demonstrated a demand for heightened cleanup efforts within the Hanford Nuclear Site. In June 2022, nearly 200 protestors attended the Hanford Journey Event, an educational tour which allowed those concerned about the Hanford Nuclear Site to learn about proposed cleanup efforts.The Hanford Journey Event was co-sponsored by the Yakama Nation. Davis Washines, a representative from the Yakama Nation Department of Natural Resources, alluded to \"[the Hanford cleanup] has a lot of meaning to us, to our people. And not just for us, our personal safety, but to this ground, to the water, because they were here before we were\". Congress responded in 2022 by increasing the budget for the Hanford clean-up efforts, after thousands of comments were sent to federal authorities to prevent high-level waste storage at Hanford.\nStarting between April and June 2025, the Hanford Nuclear Site is expected to melt radioactive wastes combined with glass flakes at a rate of 21 metric tons per day. \u00a0The melters being utilized have a lifespan of five years, which requires their replacement in 2030. As of 2023, the Hanford site cleanup project is sixteen years behind schedule. One of the harmful chemicals located in the Hanford site, strontium-90, reached over 2,000 times the standard concentration for drinking water in August 2022.\nTimber industry activity further contaminates river water, for example in the increased sediment runoff that results from clearcuts. The Northwest Forest Plan, a piece of federal legislation from 1994, mandated that timber companies consider the environmental impacts of their practices on rivers like the Columbia.\nOn July 1, 2003, Christopher Swain became the first person to swim the Columbia River's entire length, to raise public awareness about the river's environmental health.\nThroughout 2019, a series of wildfires were ongoing in Oregon on indigenous land belonging to the Umpqua Tribe. The Umpqua Tribe lost possession to their former territory in 1853, which is now known as Elliott State Forest. Elliott State Forest has been the subject of many deforestation initiatives over past years. In December 2018, the Umpqua tribe settled in Oregon after purchasing land from the Bureau of Land Management. Michael Rondeau, a descendent of the Umpqua tribe, expressed his contradictory emotions on the acquisition, stating that he felt \"sadness that my grandparents and great aunts and uncles and beyond that did not have a day of recognition\". On July 14, 2019, in the Milepost 97 wildfires, 25% of the Umpqua's forest territory burned down.\nOn March 7, 2022, Columbia Riverkeeper, a climate advocacy group, sued Weyerhaeuser, a timber and forest products company, for possible contamination of the Columbia River. Columbia Riverkeeper accused Weyerhaeuser of releasing harmful levels of runoff through their Longview Mill into the Columbia River. Furthermore, Columbia Riverkeeper stated that runoff could result in harmful bacteria growth. As of May 6, 2022, Weyerhaeuser reached a settlement with Columbia Riverkeeper. Weyerhaeuser was determined to contribute US$600,000 for river restoration and proposed potential fines of up to US$5,000 for each subsequent act of pollution between 2023 and 2025.\nNutrient cycle.\nBoth natural and anthropogenic processes are involved in the cycling of nutrients in the Columbia River basin. Natural processes in the system include estuarine mixing of fresh and ocean waters, and climate variability patterns such as the Pacific Decadal Oscillation and the El Nino Southern Oscillation (both climatic cycles that affect the amount of regional snowpack and river discharge). Natural sources of nutrients in the Columbia River include weathering, leaf litter, salmon carcasses, runoff from its tributaries, and ocean estuary exchange. Major anthropogenic impacts on nutrients in the basin are due to fertilizers from agriculture, sewage systems, logging, and the construction of dams.\nNutrient dynamics vary in the river basin from the headwaters to the main river and dams, to finally reaching the Columbia River estuary and ocean. Upstream in the headwaters, salmon runs are the main source of nutrients. Dams along the river impact nutrient cycling by increasing residence time of nutrients, and reducing the transport of silicate to the estuary, which directly impacts diatoms, a type of phytoplankton. The dams are also a barrier to salmon migration and can increase the amount of methane locally produced. The Columbia River estuary exports high rates of nutrients into the Pacific, except for nitrogen, which is delivered into the estuary by ocean upwelling sources.\nWatershed.\nMost of the Columbia's drainage basin (which, at , is about the size of France) lies roughly between the Rocky Mountains on the east and the Cascade Mountains on the west. In the United States and Canada the term watershed is often used to mean drainage basin. The term \"Columbia Basin\" is used to refer not only to the entire drainage basin but also to subsets of the river's watershed, such as the relatively flat and unforested area in eastern Washington bounded by the Cascades, the Rocky Mountains, and the Blue Mountains. Within the watershed are diverse landforms including mountains, arid plateaus, river valleys, rolling uplands, and deep gorges. Grand Teton National Park lies in the watershed, as well as parts of Yellowstone National Park, Glacier National Park, Mount Rainier National Park, and North Cascades National Park. Canadian National Parks in the watershed include Kootenay National Park, Yoho National Park, Glacier National Park, and Mount Revelstoke National Park. Hells Canyon, the deepest gorge in North America, and the Columbia Gorge are in the watershed. Vegetation varies widely, ranging from western hemlock and western redcedar in the moist regions to sagebrush in the arid regions. The watershed provides habitat for 609\u00a0known fish and wildlife species, including the bull trout, bald eagle, gray wolf, grizzly bear, and Canada lynx.\nThe World Wide Fund for Nature (WWF) divides the waters of the Columbia and its tributaries into three freshwater ecoregions: Columbia Glaciated, Columbia Unglaciated, and Upper Snake. The Columbia Glaciated ecoregion, about a third of the total watershed, lies in the north and was covered with ice sheets during the Pleistocene. The ecoregion includes the mainstem Columbia north of the Snake River and tributaries such as the Yakima, Okanagan, Pend Oreille, Clark Fork, and Kootenay rivers. The effects of glaciation include a number of large lakes and a relatively low diversity of freshwater fish. The Upper Snake ecoregion is defined as the Snake River watershed above Shoshone Falls, which totally blocks fish migration. This region has 14\u00a0species of fish, many of which are endemic. The Columbia Unglaciated ecoregion makes up the rest of the watershed. It includes the mainstem Columbia below the Snake River and tributaries such as the Salmon, John Day, Deschutes, and lower Snake Rivers. Of the three ecoregions it is the richest in terms of freshwater species diversity. There are 35\u00a0species of fish, of which four are endemic. There are also high levels of mollusk endemism.\nIn 2016, over eight million people lived within the Columbia's drainage basin. Of this total about 3.5\u00a0million people lived in Oregon, 2.1\u00a0million in Washington, 1.7\u00a0million in Idaho, half a million in British Columbia, and 0.4\u00a0million in Montana. Population in the watershed has been rising for many decades and is projected to rise to about 10\u00a0million by 2030. The highest population densities are found west of the Cascade Mountains along the I-5 corridor, especially in the Portland-Vancouver urban area. High densities are also found around Spokane, Washington, and Boise, Idaho. Although much of the watershed is rural and sparsely populated, areas with recreational and scenic values are growing rapidly. The central Oregon county of Deschutes is the fastest-growing in the state. Populations have also been growing just east of the Cascades in central Washington around the city of Yakima and the Tri-Cities area. Projections for the coming decades assume growth throughout the watershed. The Canadian part of the Okanagan subbasin is also growing rapidly.\nClimate varies greatly within the watershed. Elevation ranges from sea level at the river mouth to more than in the mountains, and temperatures vary with elevation. The highest peak is Mount Rainier, at . High elevations have cold winters and short cool summers; interior regions are subject to great temperature variability and severe droughts. Over some of the watershed, especially west of the Cascade Mountains, precipitation maximums occur in winter, when Pacific storms come ashore. Atmospheric conditions block the flow of moisture in summer, which is generally dry except for occasional thunderstorms in the interior. In some of the eastern parts of the watershed, especially shrub-steppe regions with Continental climate patterns, precipitation maximums occur in early summer. Annual precipitation varies from more than a year in the Cascades to less than in the interior. Much of the watershed gets less than a year.\nSeveral major North American drainage basins and many minor ones border the Columbia River's drainage basin. To the east, in northern Wyoming and Montana, the Continental Divide separates the Columbia watershed from the Mississippi-Missouri watershed, which empties into the Gulf of Mexico. To the northeast, mostly along the southern border between British Columbia and Alberta, the Continental Divide separates the Columbia watershed from the Nelson-Lake Winnipeg-Saskatchewan watershed, which empties into Hudson Bay. The Mississippi and Nelson watersheds are separated by the Laurentian Divide, which meets the Continental Divide at Triple Divide Peak near the headwaters of the Columbia's Flathead River tributary. This point marks the meeting of three of North America's main drainage patterns, to the Pacific Ocean, to Hudson Bay, and to the Atlantic Ocean via the Gulf of Mexico.\nFurther north along the Continental Divide, a short portion of the combined Continental and Laurentian divides separate the Columbia watershed from the Mackenzie-Slave-Athabasca watershed, which empties into the Arctic Ocean. The Nelson and Mackenzie watersheds are separated by a divide between streams flowing to the Arctic Ocean and those of the Hudson Bay watershed. This divide meets the Continental Divide at Snow Dome (also known as Dome), near the northernmost bend of the Columbia River.\nTo the southeast, in western Wyoming, another divide separates the Columbia watershed from the Colorado\u2013Green watershed, which empties into the Gulf of California. The Columbia, Colorado, and Mississippi watersheds meet at Three Waters Mountain in the Wind River Range of . To the south, in Oregon, Nevada, Utah, Idaho, and Wyoming, the Columbia watershed is divided from the Great Basin, whose several watersheds are endorheic, not emptying into any ocean but rather drying up or sinking into sumps. Great Basin watersheds that share a border with the Columbia watershed include Harney Basin, Humboldt River, and Great Salt Lake. The associated triple divide points are Commissary Ridge North, Wyoming, and Sproats Meadow Northwest, Oregon. To the north, mostly in British Columbia, the Columbia watershed borders the Fraser River watershed. To the west and southwest the Columbia watershed borders a number of smaller watersheds that drain to the Pacific Ocean, such as the Klamath River in Oregon and California and the Puget Sound Basin in Washington.\nMajor tributaries.\nThe Columbia receives more than 60\u00a0significant tributaries. The four largest that empty directly into the Columbia (measured either by discharge or by size of watershed) are the Snake River (mostly in Idaho), the Willamette River (in northwest Oregon), the Kootenay River (mostly in British Columbia), and the Pend Oreille River (mostly in northern Washington and Idaho, also known as the lower part of the Clark Fork). Each of these four averages more than and drains an area of more than .\nThe Snake is by far the largest tributary. Its watershed of is larger than the state of Idaho. Its discharge is roughly a third of the Columbia's at the rivers' confluence but compared to the Columbia upstream of the confluence the Snake is longer (113%) and has a larger drainage basin (104%).\nThe Pend Oreille River system (including its main tributaries, the Clark Fork and Flathead rivers) is also similar in size to the Columbia at their confluence. Compared to the Columbia River above the two rivers' confluence, the Pend Oreille-Clark-Flathead is nearly as long (about 86%), its basin about three-fourths as large (76%), and its discharge over a third (37%)."}
