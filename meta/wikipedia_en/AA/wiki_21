{"id": "2756", "revid": "22388063", "url": "https://en.wikipedia.org/wiki?curid=2756", "title": "Asexual reproduction", "text": "Asexual reproduction is a type of reproduction that does not involve the fusion of gametes or change in the number of chromosomes. The offspring that arise by asexual reproduction from either unicellular or multicellular organisms inherit the full set of genes of their single parent and thus the newly created individual is genetically and physically similar to the parent or an exact clone of the parent. Asexual reproduction is the primary form of reproduction for single-celled organisms such as archaea and bacteria. Many eukaryotic organisms including plants, animals, and fungi can also reproduce asexually. In vertebrates, the most common form of asexual reproduction is parthenogenesis, which is typically used as an alternative to sexual reproduction in times when reproductive opportunities are limited. Some monitor lizards, including Komodo dragons, can reproduce asexually.\nWhile all prokaryotes reproduce without the formation and fusion of gametes, mechanisms for lateral gene transfer such as conjugation, transformation and transduction can be likened to sexual reproduction in the sense of genetic recombination in meiosis.\nTypes of asexual reproduction.\nFission.\nProkaryotes (Archaea and Bacteria) reproduce asexually through binary fission, in which the parent organism divides in two to produce two genetically identical daughter organisms. Eukaryotes (such as protists and unicellular fungi) may reproduce in a functionally similar manner by mitosis; most of these are also capable of sexual reproduction.\nMultiple fission at the cellular level occurs in many protists, e.g. sporozoans and algae. The nucleus of the parent cell divides several times by mitosis, producing several nuclei. The cytoplasm then separates, creating multiple daughter cells.\nIn apicomplexans, multiple fission, or schizogony appears either as merogony, sporogony or gametogony. Merogony results in merozoites, which are multiple daughter cells, that originate within the same cell membrane, sporogony results in sporozoites, and gametogony results in microgametes.\nBudding.\nSome cells divide by budding (for example baker's yeast), resulting in a \"mother\" and a \"daughter\" cell that is initially smaller than the parent. Budding is also known on a multicellular level; an animal example is the hydra, which reproduces by budding. The buds grow into fully matured individuals which eventually break away from the parent organism.\nInternal budding is a process of asexual reproduction, favoured by parasites such as \"Toxoplasma gondii\". It involves an unusual process in which two (\"endodyogeny\") or more (\"endopolygeny\") daughter cells are produced inside a mother cell, which is then consumed by the offspring prior to their separation.\nAlso, budding (external or internal) occurs in some worms like \"Taenia\" or \"Echinococcus\"; these worms produce cysts and then produce (invaginated or evaginated) protoscolex with budding.\nVegetative propagation.\nVegetative propagation is a type of asexual reproduction found in plants where new individuals are formed without the production of seeds or spores and thus without syngamy or meiosis. Examples of vegetative reproduction include the formation of miniaturized plants called plantlets on specialized leaves, for example in kalanchoe (\"Bryophyllum daigremontianum\") and many produce new plants from rhizomes or stolon (for example in strawberry). Some plants reproduce by forming bulbs or tubers, for example tulip bulbs and \"Dahlia\" tubers. In these examples, all the individuals are clones, and the clonal population may cover a large area.\nSpore formation.\nMany multicellular organisms produce spores during their biological life cycle in a process called \"sporogenesis\". Exceptions are animals and some protists, which undergo \"meiosis\" immediately followed by fertilization. Plants and many algae on the other hand undergo \"sporic meiosis\" where meiosis leads to the formation of haploid spores rather than gametes. These spores grow into multicellular individuals called gametophytes, without a fertilization event. These haploid individuals produce gametes through mitosis. Meiosis and gamete formation therefore occur in separate multicellular generations or \"phases\" of the life cycle, referred to as alternation of generations. Since sexual reproduction is often more narrowly defined as the fusion of gametes (fertilization), spore formation in plant sporophytes and algae might be considered a form of asexual reproduction (agamogenesis) despite being the result of meiosis and undergoing a reduction in ploidy. However, both events (spore formation and fertilization) are necessary to complete sexual reproduction in the plant life cycle.\nFungi and some algae can also utilize true asexual spore formation, which involves mitosis giving rise to reproductive cells called mitospores that develop into a new organism after dispersal. This method of reproduction is found for example in conidial fungi and the red algae \"Polysiphonia\", and involves sporogenesis without meiosis. Thus the chromosome number of the spore cell is the same as that of the parent producing the spores. However, mitotic sporogenesis is an exception and most spores, such as those of plants and many algae, are produced by meiosis.\nFragmentation.\nFragmentation is a form of asexual reproduction where a new organism grows from a fragment of the parent. Each fragment develops into a mature, fully grown individual. Fragmentation is seen in many organisms. Animals that reproduce asexually include planarians, many annelid worms including polychaetes and some oligochaetes, turbellarians and sea stars. Many fungi and plants reproduce asexually. Some plants have specialized structures for reproduction via fragmentation, such as \"gemmae\" in mosses and liverworts. Most lichens, which are a symbiotic union of a fungus and photosynthetic algae or cyanobacteria, reproduce through fragmentation to ensure that new individuals contain both symbionts. These fragments can take the form of \"soredia\", dust-like particles consisting of fungal hyphae wrapped around photobiont cells.\nClonal Fragmentation in multicellular or colonial organisms is a form of asexual reproduction or cloning where an organism is split into fragments. Each of these fragments develop into mature, fully grown individuals that are clones of the original organism. In echinoderms, this method of reproduction is usually known as \"fissiparity\". Due to many environmental and epigenetic differences, clones originating from the same ancestor might actually be genetically and epigenetically different.\nAgamogenesis.\nAgamogenesis is any form of reproduction that does not involve a male gamete. Examples are parthenogenesis and apomixis.\nParthenogenesis.\nParthenogenesis is a form of agamogenesis in which an unfertilized egg develops into a new individual. It has been documented in over 2,000 species. Parthenogenesis occurs in the wild in many invertebrates (e.g. water fleas, rotifers, aphids, stick insects, some ants, bees and parasitic wasps) and vertebrates (mostly reptiles, amphibians, and fish). It has also been documented in domestic birds and in genetically altered lab mice. Plants can engage in parthenogenesis as well through a process called apomixis. However this process is considered by many to not be an independent reproduction method, but instead a breakdown of the mechanisms behind sexual reproduction. Parthenogenetic organisms can be split into two main categories: facultative and obligate.\nFacultative parthenogenesis.\nIn facultative parthenogenesis, females can reproduce both sexually and asexually. Because of the many advantages of sexual reproduction, most facultative parthenotes only reproduce asexually when forced to. This typically occurs in instances when finding a mate becomes difficult. For example, female zebra sharks will reproduce asexually if they are unable to find a mate in their ocean habitats.\nParthenogenesis was previously believed to rarely occur in vertebrates, and only be possible in very small animals. However, it has been discovered in many more species in recent years. Today, the largest species that has been documented reproducing parthenogenically is the Komodo dragon at 10 feet long and over 300 pounds.\nHeterogony is a form of facultative parthenogenesis where females alternate between sexual and asexual reproduction at regular intervals (see Alternation between sexual and asexual reproduction). Aphids are one group of organism that engages in this type of reproduction. They use asexual reproduction to reproduce quickly and create winged offspring that can colonize new plants and reproduce sexually in the fall to lay eggs for the next season. However, some aphid species are obligate parthenotes.\nObligate parthenogenesis.\nIn obligate parthenogenesis, females only reproduce asexually. One example of this is the desert grassland whiptail lizard, a hybrid of two other species. Typically hybrids are infertile but through parthenogenesis this species has been able to develop stable populations.\nGynogenesis is a form of obligate parthenogenesis where a sperm cell is used to initiate reproduction. However, the sperm's genes never get incorporated into the egg cell. The best known example of this is the Amazon molly. Because they are obligate parthenotes, there are no males in their species so they depend on males from a closely related species (the Sailfin molly) for sperm.\nApomixis and nucellar embryony.\nApomixis in plants is the formation of a new sporophyte without fertilization. It is important in ferns and in flowering plants, but is very rare in other seed plants. In flowering plants, the term \"apomixis\" is now most often used for agamospermy, the formation of seeds without fertilization, but was once used to include vegetative reproduction. An example of an apomictic plant would be the triploid European dandelion. Apomixis mainly occurs in two forms: In gametophytic apomixis, the embryo arises from an unfertilized egg within a diploid embryo sac that was formed without completing meiosis. In nucellar embryony, the embryo is formed from the diploid nucellus tissue surrounding the embryo sac. Nucellar embryony occurs in some citrus seeds. Male apomixis can occur in rare cases, such as in the Saharan Cypress \"Cupressus dupreziana\", where the genetic material of the embryo is derived entirely from pollen.\nAndrogenesis.\nAndrogenesis occurs when a zygote is produced with only paternal nuclear genes. During standard sexual reproduction, one female and one male parent each produce haploid gametes (such as a sperm or egg cell, each containing only a single set of chromosomes), which recombine to create offspring with genetic material from both parents. However, in androgenesis, there is no recombination of maternal and paternal chromosomes, and only the paternal chromosomes are passed down to the offspring (the inverse of this is gynogenesis, where only the maternal chromosomes are inherited, which is more common than androgenesis). The offspring produced in androgenesis will still have maternally inherited mitochondria, as is the case with most sexually reproducing species.\nAndrogenesis occurs in nature in many invertebrates (for example, clams, stick insects, some ants, bees, flies and parasitic wasps) and vertebrates (mainly amphibians and fish). The androgenesis has also been seen in genetically modified laboratory mice.\nOne of two things can occur to produce offspring with exclusively paternal genetic material: the maternal nuclear genome can be eliminated from the zygote, or the female can produce an egg with no nucleus, resulting in an embryo developing with only the genome of the male gamete.\nMale apomixis.\nOther type of androgenesis is the male apomixis or paternal apomixis is a reproductive process in which a plant develops from a sperm cell (male gamete) without the participation of a female cell (ovum). In this process, the zygote is formed solely with genetic material from the father, resulting in offspring genetically identical to the male organism. This has been noted in many plants like \"Nicotiana\", \"Capsicum frutescens\", \"Cicer arietinum\", \"Poa arachnifera\", \"Solanum verrucosum\", \"Phaeophyceae\", \"Pripsacum dactyloides\", \"Zea mays\", and occurs as the regular reproductive method in \"Cupressus dupreziana\". This contrasts with the more common apomixis, where development occurs without fertilization, but with genetic material only from the mother.\nThere are also clonal species that reproduce through vegetative reproduction like \"Lomatia tasmanica\" and \"Pando\", where the genetic material is exclusively male.\nOther species where androgenesis has been observed naturally are the stick insects \"Bacillus rossius and Bassillus Grandii\", the little fire ant \"Wasmannia auropunctata\", \"Vollenhovia emeryi\", \"Paratrechina longicornis\", occasionally in \"Apis mellifera\", the \"Hypseleotris\" carp gudgeons, the parasitoid \"Venturia canescens\", and occasionally in fruit flies \"Drosophila melanogaster\" carrying a specific mutant allele. It has also been induced in many crops and fish via irradiation of an egg cell to destroy the maternal nuclear genome.\nObligate androgenesis.\nObligate androgenesis is the process in which males are capable of producing both eggs and sperm, however, the eggs have no genetic contribution and the offspring come only from the sperm, which allows these individuals to self-fertilize and produce clonal offspring without the need for females. They are also capable of interbreeding with sexual and other androgenetic lineages in a phenomenon known as \"egg parasitism.\" This method of reproduction has been found in several species of the clam genus \"Corbicula\", many plants like, \"Cupressus dupreziana\", \"Lomatia tasmanica\", \"Pando\" and recently in the fish \"Squalius alburnoides\". \nOther species where androgenesis has been observed naturally are the stick insects \"Bacillus rossius and Bassillus Grandii\", the little fire ant \"Wasmannia auropunctata\", \"Vollenhovia emeryi\", \"Paratrechina longicornis\", occasionally in \"Apis mellifera\", the \"Hypseleotris\" carp gudgeons, the parasitoid \"Venturia canescens\", and occasionally in fruit flies \"Drosophila melanogaster\" carrying a specific mutant allele. It has also been induced in many crops and fish via irradiation of an egg cell to destroy the maternal nuclear genome.\nAlternation between sexual and asexual reproduction.\nSome species can alternate between sexual and asexual strategies, an ability known as \"heterogamy\", depending on many conditions. Alternation is observed in several rotifer species (cyclical parthenogenesis e.g. in Brachionus species) and a few types of insects.\nOne example of this is aphids which can engage in heterogony. In this system, females are born pregnant and produce only female offspring. This cycle allows them to reproduce very quickly. However, most species reproduce sexually once a year. This switch is triggered by environmental changes in the fall and causes females to develop eggs instead of embryos. This dynamic reproductive cycle allows them to produce specialized offspring with polyphenism, a type of polymorphism where different phenotypes have evolved to carry out specific tasks.\nThe cape bee \"Apis mellifera\" subsp. \"capensis\" can reproduce asexually through a process called thelytoky. The freshwater crustacean \"Daphnia\" reproduces by parthenogenesis in the spring to rapidly populate ponds, then switches to sexual reproduction as the intensity of competition and predation increases. Monogonont rotifers of the genus \"Brachionus\" reproduce via cyclical parthenogenesis: at low population densities females produce asexually and at higher densities a chemical cue accumulates and induces the transition to sexual reproduction. Many protists and fungi alternate between sexual and asexual reproduction. A few species of amphibians, reptiles, and birds have a similar ability.\nThe slime mold \"Dictyostelium\" undergoes binary fission (mitosis) as single-celled amoebae under favorable conditions. However, when conditions turn unfavorable, the cells aggregate and follow one of two different developmental pathways, depending on conditions. In the social pathway, they form a multi-cellular slug which then forms a fruiting body with asexually generated spores. In the sexual pathway, two cells fuse to form a giant cell that develops into a large cyst. When this macrocyst germinates, it releases hundreds of amoebic cells that are the product of meiotic recombination between the original two cells.\nThe hyphae of the common mold (\"Rhizopus\") are capable of producing both mitotic as well as meiotic spores. Many algae similarly switch between sexual and asexual reproduction. A number of plants use both sexual and asexual means to produce new plants, some species alter their primary modes of reproduction from sexual to asexual under varying environmental conditions.\nInheritance in asexual species.\nIn the rotifer \"Brachionus calyciflorus\" asexual reproduction (obligate parthenogenesis) can be inherited by a recessive allele, which leads to loss of sexual reproduction in homozygous offspring.\nInheritance of asexual reproduction by a single recessive locus has also been found in the parasitoid wasp \"Lysiphlebus fabarum\".\nExamples in animals.\nAsexual reproduction is found in nearly half of the animal phyla. Parthenogenesis occurs in the hammerhead shark and the blacktip shark. In both cases, the sharks had reached sexual maturity in captivity in the absence of males, and in both cases the offspring were shown to be genetically identical to the mothers. The New Mexico whiptail is another example.\nSome reptiles use the ZW sex-determination system, which produces either males (with ZZ sex chromosomes) or females (with ZW or WW sex chromosomes). Until 2010, it was thought that the ZW chromosome system used by reptiles was incapable of producing viable WW offspring, but a (ZW) female boa constrictor was discovered to have produced viable female offspring with WW chromosomes. The female boa could have chosen any number of male partners (and had successfully in the past) but on this occasion she reproduced asexually, creating 22 female babies with WW sex-chromosomes.\nPolyembryony is a widespread form of asexual reproduction in animals, whereby the fertilized egg or a later stage of embryonic development splits to form genetically identical clones. Within animals, this phenomenon has been best studied in the parasitic Hymenoptera. In the nine-banded armadillos, this process is obligatory and usually gives rise to genetically identical quadruplets. In other mammals, monozygotic twinning has no apparent genetic basis, though its occurrence is common. There are at least 10 million identical human twins and triplets in the world today.\nBdelloid rotifers reproduce exclusively asexually, and all individuals in the class Bdelloidea are females. Asexuality evolved in these animals millions of years ago and has persisted since. There is evidence to suggest that asexual reproduction has allowed the animals to evolve new proteins through the Meselson effect that have allowed them to survive better in periods of dehydration. Bdelloid rotifers are extraordinarily resistant to damage from ionizing radiation due to the same DNA-preserving adaptations used to survive dormancy. These adaptations include an extremely efficient mechanism for repairing DNA double-strand breaks. This repair mechanism was studied in two Bdelloidea species, \"Adineta vaga\", and \"Philodina roseola\". and appears to involve mitotic recombination between homologous DNA regions within each species.\nMolecular evidence strongly suggests that several species of the stick insect genus \"Timema\" have used only asexual (parthenogenetic) reproduction for millions of years, the longest period known for any insect. Similar findings suggest that the mite species \"Oppiella nova\" may have reproduced entirely asexually for millions of years.\nIn the grass thrips genus \"Aptinothrips\" there have been several transitions to asexuality, likely due to different causes.\nAdaptive significance of asexual reproduction.\nA complete lack of sexual reproduction is relatively rare among multicellular organisms, particularly animals. It is not entirely understood why the ability to reproduce sexually is so common among them. Current hypotheses suggest that asexual reproduction may have short term benefits when rapid population growth is important or in stable environments, while sexual reproduction offers a net advantage by allowing more rapid generation of genetic diversity, allowing adaptation to changing environments. Developmental constraints may underlie why few animals have relinquished sexual reproduction completely in their life-cycles. Almost all asexual modes of reproduction maintain meiosis either in a modified form or as an alternative pathway. Facultatively apomictic plants increase frequencies of sexuality relative to apomixis after abiotic stress. Another constraint on switching from sexual to asexual reproduction would be the concomitant loss of meiosis and the protective recombinational repair of DNA damage afforded as one function of meiosis."}
{"id": "2758", "revid": "25511559", "url": "https://en.wikipedia.org/wiki?curid=2758", "title": "Aelbert Cuyp", "text": "Aelbert Jacobszoon Cuyp or Cuijp (; 20 October 1620 \u2013 15 November 1691) was one of the leading Dutch Golden Age painters, producing mainly landscapes. The most famous of a family of painters, the pupil of his father, Jacob Gerritszoon Cuyp (1594\u20131651/52), he is especially known for his large views of Dutch riverside scenes in a golden early morning or late afternoon light. He was born and died in Dordrecht.\nBiography.\nKnown as the Dutch equivalent of Claude Lorrain, he inherited a considerable fortune. His family were all artists, with his uncle Benjamin and grandfather Gerrit being stained glass cartoon designers. Jacob Gerritszoon Cuyp, his father, was a portraitist. Cuyp's father was his first teacher and they collaborated on many paintings throughout his lifetime.\nLittle is known about Aelbert Cuyp's life. Even Arnold Houbraken, a noted historian of Dutch Golden Age paintings and the sole authority on Cuyp for the hundred years following his death, paints a very thin biographical picture.\nHis period of activity as a painter is traditionally limited to the two decades between 1639 and 1660, fitting within the generally accepted limits of the Dutch Golden Age's most significant period, 1640\u20131665. He is known to have been married to Cornelia Bosman in 1658, a date coinciding so directly with the end of his productivity as a painter that it has been accepted that his marriage played a role in the end of his artistic career.\nThe year after his marriage, Cuyp became the deacon of the reformed church. Houbraken recalled that Cuyp was a devout Calvinist and the fact that when he died, there were no paintings of other artists found in his home.\nStyle.\nThe development of Cuyp, who was trained as a landscape painter, may be roughly sketched in three phases based on the painters who most influenced him during that time and the subsequent artistic characteristics that are apparent in his paintings. Generally, Cuyp learned tone from the exceptionally prolific Jan van Goyen, light from Jan Both and form from his father, Jacob Gerritsz Cuyp.\nCuyp's \"van Goyen phase\" can be placed approximately in the early 1640s. Cuyp probably first encountered a painting by van Goyen in 1640 when van Goyen was, as Stephen Reiss points, out \"at the height of [his] powers\". This is noticeable in the comparison between two of Cuyp's landscape paintings inscribed 1639 where no properly formed style is apparent and the landscape backgrounds he painted two years later for two of his father's group portraits that are distinctly van Goyenesque. Cuyp took from van Goyen the straw yellow and light brown tones that are so apparent in his \"Dunes\" (1629) and the broken brush technique also very noticeable in that same work. This technique, a precursor to impressionism, is noted for the short brush strokes where the colors are not necessarily blended smoothly. In Cuyp's \"River Scene, Two Men Conversing\" (1641) both of these van Goyen-influenced stylistic elements are noticeable.\nThe next phase in the development of Cuyp's increasingly amalgamated style is due to the influence of Jan Both. In the mid-1640s Both, a native and resident of Utrecht, had just returned to his hometown from a trip to Rome. It is around this same time that Cuyp's style changed fundamentally. In Rome, Both had developed a new style of composition due, at least in part, to his interaction with Claude Lorrain. This new style was focused on changing the direction of light in the painting. Instead of the light being placed at right angles in relation to the line of vision, Both started moving it to a diagonal position from the back of the picture.\nIn this new form of lighting, the artist (and viewer of the painting) faced the sun more or less contre-jour. Both, and subsequently Cuyp, used the advantages of this new lighting style to alter the sense of depth and luminosity possible in a painting. To make notice of these new capabilities, much use was made of elongated shadows. Cuyp was one of the first Dutch painters to appreciate this new leap forward in style and while his own Both-inspired phase was quite short (limited to the mid-1640s) he did, more than any other contemporary Dutch artist, maximize the full chromatic scale for sunsets and sunrises.Cuyp's third stylistic phase (which occurred throughout his career) is based on the influence of his father. While it is assumed that the younger Cuyp did work with his father initially to develop rudimentary talents, Aelbert became more focused on landscape paintings while Jacob was a portrait painter by profession. As has been mentioned and as will be explained in depth below, there are pieces where Aelbert provided the landscape background for his father's portraits. What is meant by stating that Aelbert learned from his father is that his eventual transition from a specifically landscape painter to the involvement of foreground figures is attributed to his interaction with his father Jacob. The evidence for Aelbert's evolution to foreground figure painter is in the production of some paintings from 1645 to 1650 featuring foreground animals that do not fit with Jacob's style. Adding to the confusion regarding Aelbert's stylistic development and the problem of attribution is of course the fact that Jacob's style was not stagnant either. Their converging styles make it difficult to exactly understand the influences each had on the other, although it is clear enough to say that Aelbert started representing large scale forms (something he had not done previously) and placing animals as the focus of his paintings (something that was specific to him).\nPaintings.\nSunlight in his paintings rakes across the panel, accentuating small bits of detail in the golden light. In large, atmospheric panoramas of the countryside, the highlights on a blade of meadow grass, the mane of a tranquil horse, the horn of a dairy cow reclining by a stream, or the tip of a peasant's hat are all caught in a bath of yellow ocher light. The richly varnished medium refracts the rays of light like a jewel as it dissolves into numerous glazed layers. Cuyp's landscapes were based on reality and on his own invention of what an enchanting landscape should be.Cuyp's drawings reveal him to be a draftsman of superior quality. Light-drenched washes of golden brown ink depict a distant view of the city of Dordrecht or Utrecht.\nA Cuyp drawing may look like he intended it to be a finished work of art, but it was most likely taken back to the studio and used as a reference for his paintings. Often the same section of a sketch can be found in several different pictures.\nCuyp signed many of his works but rarely dated them, so that a chronology of his career has not been satisfactorily reassembled. A phenomenal number of paintings are ascribed to him, some of which are likely to be by other masters of the golden landscape, such as Abraham Calraet (1642\u20131722), whose initials \"A.C.\" may be mistaken for Cuyp's.\nHowever, not everyone appreciates his work and \"River Landscape\" (1660), despite being widely regarded as amongst his best work, has been described as having \"chocolate box blandness\".\nAt the Madrid's Thyssen-Bornemisza Museum most likely, the sole Cuyp's painting in Spanish public collections can be seen, a \"Landscape with a sunset\" ca. 1655 with animals.\nMisattribution of paintings.\nIn addition to the scarcely documented and confirmed biography of Cuyp's life, and even more so than his amalgamated style from his three main influences, there are yet other factors that have led to the misattribution and confusion over Aelbert Cuyp's works for hundreds of years. His highly influenced style which incorporated Italianate lighting from Jan Both, broken brush technique and atonality from Jan van Goyen, and his ever-developing style from his father Jacob Gerritsz Cuyp was studied acutely by his most prominent follower, Abraham van Calraet. Calraet mimicked Cuyp's style, incorporating the same aspects, and produced similar landscapes to that of the latter. This made it quite difficult to tell whose paintings were whose. Adding to the confusion is the similar initials between the two and the inconsistent signing of paintings which were produced by Cuyp's studio.\nAlthough Aelbert Cuyp signed many of his paintings with a script \"A. Cuyp\" insignia, many paintings were left unsigned (not to mention undated) after being painted, and so a similar signature was added later on, presumably by collectors who inherited or discovered the works. Furthermore, many possible Cuyp paintings were not signed but rather initialed \"A. C.\" referring to his name. However, Abraham van Calraet could also have used the same initials to denote a painting. Although this is unlikely (as Calraet would likely have signed his paintings \"A. v.C.\"), this brings up the question of how paintings were signed to show ownership. Most original Cuyp paintings were signed by him, and in the script manner in which his name was inscribed. This would denote that the painting was done almost entirely by him. Conversely, paintings which came out of his workshop that were not necessarily physically worked on by Cuyp but merely overseen by him technically, were marked with A.C. to show that it was his instruction which saw the paintings' completion. Cuyp's pupils and assistants often worked on paintings in his studio, and so most of the work of a painting could be done without Cuyp ever touching the canvas, but merely approving its finality. Hence, the initialed inscription rather than a signature.\nCommon among the mislabeled works are all of the reasons identified for misattributing Cuyp's works: the lack of biography and chronology of his works made it difficult to discern when paintings were created (making it difficult to pinpoint an artist); contentious signatures added to historians' confusion as to who actually painted the works; and the collaborations and influences by different painters makes it hard to justify that a painting is genuinely that of Aelbert Cuyp; and finally, accurate identification is made extremely difficult by the fact that this same style was copied (rather accurately) by his predecessor. As it turns out, even the historians and expert researchers have been fooled and forced to reassess their conclusions over \"Cuyp's\" paintings over the years.\nLater life.\nAfter he married Cornelia Boschman in 1658, the number of works produced by him declined almost to nothing. This may have been because his wife was a very religious woman and a not very big patron of the arts. It could also be that he became more active in the church under his wife's guidance. He was also active as deacon and elder of the Reformed Church.\nLegacy.\nThough long lacking a modern biography, and with the chronology of his works rather unclear, his style emerged from various influences and makes his works distinctive, although his collaborations with his father and works by his imitators often make attributions uncertain. His follower Abraham van Calraet represents a particular problem, and the signatures on paintings are not to be relied on. The Rijksmuseum has reattributed many works to other painters; Abraham van Calraet does not even appear in a Museum catalogue until 1926, and even then he was not given his own entry."}
{"id": "2760", "revid": "13225464", "url": "https://en.wikipedia.org/wiki?curid=2760", "title": "Arabic (disambiguation)", "text": ""}
{"id": "2761", "revid": "41494412", "url": "https://en.wikipedia.org/wiki?curid=2761", "title": "Alkene", "text": "In organic chemistry, an alkene, or olefin, is a hydrocarbon containing a carbon\u2013carbon double bond. The double bond may be internal or in the terminal position. Terminal alkenes are also known as \u03b1-olefins.\nThe International Union of Pure and Applied Chemistry (IUPAC) recommends using the name \"alkene\" only for acyclic hydrocarbons with just one double bond; alkadiene, alkatriene, etc., or polyene for acyclic hydrocarbons with two or more double bonds; cycloalkene, cycloalkadiene, etc. for cyclic ones; and \"olefin\" for the general class \u2013 cyclic or acyclic, with one or more double bonds.\nAcyclic alkenes, with only one double bond and no other functional groups (also known as mono-enes) form a homologous series of hydrocarbons with the general formula with \"n\" being a &gt;1 natural number (which is two hydrogens less than the corresponding alkane). When \"n\" is four or more, isomers are possible, distinguished by the position and conformation of the double bond.\nAlkenes are generally colorless non-polar compounds, somewhat similar to alkanes but more reactive. The first few members of the series are gases or liquids at room temperature. The simplest alkene, ethylene () (or \"ethene\" in the IUPAC nomenclature) is the organic compound produced on the largest scale industrially.\nAromatic compounds are often drawn as cyclic alkenes, however their structure and properties are sufficiently distinct that they are not classified as alkenes or olefins. Hydrocarbons with two overlapping double bonds () are called allenes\u2014the simplest such compound is itself called \"allene\"\u2014and those with three or more overlapping bonds (, , etc.) are called cumulenes.\nStructural isomerism.\nAlkenes having four or more carbon atoms can form diverse structural isomers. Most alkenes are also isomers of cycloalkanes. Acyclic alkene structural isomers with only one double bond follow:\nMany of these molecules exhibit \"cis\"\u2013\"trans\" isomerism. There may also be chiral carbon atoms particularly within the larger molecules (from ). The number of potential isomers increases rapidly with additional carbon atoms.\nStructure and bonding.\nBonding.\nA carbon\u2013carbon double bond consists of a sigma bond and a pi bond. This double bond is stronger than a single covalent bond (611\u00a0kJ/mol for C=C vs. 347\u00a0kJ/mol for C\u2013C), but not twice as strong. Double bonds are shorter than single bonds with an average bond length of 1.33 \u00c5 (133 pm) vs 1.53 \u00c5 for a typical C-C single bond.\nEach carbon atom of the double bond uses its three sp2 hybrid orbitals to form sigma bonds to three atoms (the other carbon atom and two hydrogen atoms). The unhybridized 2p atomic orbitals, which lie perpendicular to the plane created by the axes of the three sp2 hybrid orbitals, combine to form the pi bond. This bond lies outside the main C\u2013C axis, with half of the bond on one side of the molecule and a half on the other. With a strength of 65 kcal/mol, the pi bond is significantly weaker than the sigma bond.\nRotation about the carbon\u2013carbon double bond is restricted because it incurs an energetic cost to break the alignment of the p orbitals on the two carbon atoms. Consequently \"cis\" or \"trans\" isomers interconvert so slowly that they can be freely handled at ambient conditions without isomerization. More complex alkenes may be named with the \"E\"\u2013\"Z\" notation for molecules with three or four different substituents (side groups). For example, of the isomers of butene, the two methyl groups of (\"Z\")-but-2-ene (a.k.a. \"cis\"-2-butene) appear on the same side of the double bond, and in (\"E\")-but-2-ene (a.k.a. \"trans\"-2-butene) the methyl groups appear on opposite sides. These two isomers of butene have distinct properties.\nShape.\nAs predicted by the VSEPR model of electron pair repulsion, the molecular geometry of alkenes includes bond angles about each carbon atom in a double bond of about 120\u00b0. The angle may vary because of steric strain introduced by nonbonded interactions between functional groups attached to the carbon atoms of the double bond. For example, the C\u2013C\u2013C bond angle in propylene is 123.9\u00b0.\nFor bridged alkenes, Bredt's rule states that a double bond cannot occur at the bridgehead of a bridged ring system unless the rings are large enough. Following Fawcett and defining \"S\" as the total number of non-bridgehead atoms in the rings, bicyclic systems require \"S\"\u00a0\u2265\u00a07 for stability and tricyclic systems require \"S\"\u00a0\u2265\u00a011.\nIsomerism.\nIn organic chemistry,the prefixes cis- and trans- are used to describe the positions of functional groups attached to carbon atoms joined by a double bond. In Latin, \"cis\" and \"trans\" mean \"on this side of\" and \"on the other side of\" respectively. Therefore, if the functional groups are both on the same side of the carbon chain, the bond is said to have cis- configuration, otherwise (i.e. the functional groups are on the opposite side of the carbon chain), the bond is said to have trans- configuration.\nFor there to be cis- and trans- configurations, there must be a carbon chain, or at least one functional group attached to each carbon is the same for both. E- and Z- configuration can be used instead in a more general case where all four functional groups attached to carbon atoms in a double bond are different. E- and Z- are abbreviations of German words \"zusammen\" (together) and \"entgegen\" (opposite). In E- and Z-isomerism, each functional group is assigned a priority based on the Cahn\u2013Ingold\u2013Prelog priority rules. If the two groups with higher priority are on the same side of the double bond, the bond is assigned Z- configuration, otherwise (i.e. the two groups with higher priority are on the opposite side of the double bond), the bond is assigned E- configuration. Cis- and trans- configurations do not have a fixed relationship with E- and Z-configurations.\nPhysical properties.\nMany of the physical properties of alkenes and alkanes are similar: they are colorless, nonpolar, and combustible. The physical state depends on molecular mass: like the corresponding saturated hydrocarbons, the simplest alkenes (ethylene, propylene, and butene) are gases at room temperature. Linear alkenes of approximately five to sixteen carbon atoms are liquids, and higher alkenes are waxy solids. The melting point of the solids also increases with increase in molecular mass.\nAlkenes generally have stronger smells than their corresponding alkanes. Ethylene has a sweet and musty odor. Strained alkenes, in particular, like norbornene and \"trans\"-cyclooctene are known to have strong, unpleasant odors, a fact consistent with the stronger \u03c0 complexes they form with metal ions including copper.\nBoiling and melting points.\nBelow is a list of the boiling and melting points of various alkenes with the corresponding alkane and alkyne analogues. \nInfrared spectroscopy.\nIn the IR spectrum, the stretching/compression of C=C bond gives a peak at 1670\u20131600\u00a0cm\u22121. The band is weak in symmetrical alkenes. The bending of C=C bond absorbs between 1000 and 650\u00a0cm\u22121 wavelength\nNMR spectroscopy.\nIn 1H NMR spectroscopy, the hydrogen bonded to the carbon adjacent to double bonds will give a \u03b4H of 4.5\u20136.5\u00a0ppm. The double bond will also deshield the hydrogen attached to the carbons adjacent to sp2 carbons, and this generates \u03b4H=1.6\u20132.\u00a0ppm peaks. Cis/trans isomers are distinguishable due to different J-coupling effect. Cis vicinal hydrogens will have coupling constants in the range of 6\u201314\u00a0Hz, whereas the trans will have coupling constants of 11\u201318\u00a0Hz.\nIn their 13C NMR spectra of alkenes, double bonds also deshield the carbons, making them have low field shift. C=C double bonds usually have chemical shift of about 100\u2013170\u00a0ppm.\nCombustion.\nLike most other hydrocarbons, alkenes combust to give carbon dioxide and water.\nThe combustion of alkenes release less energy than burning same molarity of saturated ones with same number of carbons. \nThis trend can be clearly seen in the list of standard enthalpy of combustion of hydrocarbons.\nReactions.\nAlkenes are relatively stable compounds, but are more reactive than alkanes. Most reactions of alkenes involve additions to this pi bond, forming new single bonds. Alkenes serve as a feedstock for the petrochemical industry because they can participate in a wide variety of reactions, prominently polymerization and alkylation. Except for ethylene, alkenes have two sites of reactivity: the carbon\u2013carbon pi-bond and the presence of allylic CH centers. The former dominates but the allylic sites are important too.\nAddition to the unsaturated bonds.\nHydrogenation involves the addition of H2 ,resulting in an alkane. The equation of hydrogenation of ethylene to form ethane is:\nHydrogenation reactions usually require catalysts to increase their reaction rate. The total number of hydrogens that can be added to an unsaturated hydrocarbon depends on its degree of unsaturation.\nSimilarly, halogenation involves the addition of a halogen molecule, such as Br2, resulting in a dihaloalkane. The equation of bromination of ethylene to form ethane is:\nUnlike hydrogenation, these halogenation reactions do not require catalysts. The reaction occurs in two steps, with a halonium ion as an intermediate.\nBromine test is used to test the saturation of hydrocarbons. The bromine test can also be used as an indication of the degree of unsaturation for unsaturated hydrocarbons. Bromine number is defined as gram of bromine able to react with 100g of product. Similar as hydrogenation, the halogenation of bromine is also depend on the number of \u03c0 bond. A higher bromine number indicates higher degree of unsaturation.\nThe \u03c0 bonds of alkenes hydrocarbons are also susceptible to hydration. The reaction usually involves strong acid as catalyst. The first step in hydration often involves formation of a carbocation. The net result of the reaction will be an alcohol. The reaction equation for hydration of ethylene is:\nHydrohalogenation involves addition of H\u2212X to unsaturated hydrocarbons. This reaction results in new C\u2212H and C\u2212X \u03c3 bonds. The formation of the intermediate carbocation is selective and follows Markovnikov's rule. The hydrohalogenation of alkene will result in haloalkane. The reaction equation of HBr addition to ethylene is:\nCycloaddition.\nAlkenes add to dienes to give cyclohexenes. This conversion is an example of a Diels-Alder reaction. Such reaction proceed with retention of stereochemistry. The rates are sensitive to electron-withdrawing or electron-donating substituents. When irradiated by UV-light, alkenes dimerize to give cyclobutanes. Another example is the Schenck ene reaction, in which singlet oxygen reacts with an allylic structure to give a transposed allyl peroxide:\nOxidation.\nAlkenes react with percarboxylic acids and even hydrogen peroxide to yield epoxides:\nFor ethylene, the epoxidation is conducted on a very large scale industrially using oxygen in the presence of silver-based catalysts:\nAlkenes react with ozone, leading to the scission of the double bond. The process is called ozonolysis. Often the reaction procedure includes a mild reductant, such as dimethylsulfide ():\nWhen treated with a hot concentrated, acidified solution of , alkenes are cleaved to form ketones and/or carboxylic acids. The stoichiometry of the reaction is sensitive to conditions. This reaction and the ozonolysis can be used to determine the position of a double bond in an unknown alkene.\nThe oxidation can be stopped at the vicinal diol rather than full cleavage of the alkene by using osmium tetroxide or other oxidants:\nThis reaction is called dihydroxylation.\nIn the presence of an appropriate photosensitiser, such as methylene blue and light, alkenes can undergo reaction with reactive oxygen species generated by the photosensitiser, such as hydroxyl radicals, singlet oxygen or superoxide ion. Reactions of the excited sensitizer can involve electron or hydrogen transfer, usually with a reducing substrate (Type I reaction) or interaction with oxygen (Type II reaction). These various alternative processes and reactions can be controlled by choice of specific reaction conditions, leading to a wide range of products. A common example is the [4+2]-cycloaddition of singlet oxygen with a diene such as cyclopentadiene to yield an endoperoxide:\nPolymerization.\nTerminal alkenes are precursors to polymers via processes termed polymerization. Some polymerizations are of great economic significance, as they generate the plastics polyethylene and polypropylene. Polymers from alkene are usually referred to as \"polyolefins\" although they contain no olefins. Polymerization can proceed via diverse mechanisms. Conjugated dienes such as buta-1,3-diene and isoprene (2-methylbuta-1,3-diene) also produce polymers, one example being natural rubber.\nAllylic substitution.\nThe presence of a C=C \u03c0 bond in unsaturated hydrocarbons weakens the dissociation energy of the allylic C\u2212H bonds. Thus, these groupings are susceptible to free radical substitution at these C-H sites as well as addition reactions at the C=C site. In the presence of radical initiators, allylic C-H bonds can be halogenated. The presence of two C=C bonds flanking one methylene, i.e., doubly allylic, results in particularly weak HC-H bonds. The high reactivity of these situations is the basis for certain free radical reactions, manifested in the chemistry of drying oils.\nMetathesis.\nAlkenes undergo olefin metathesis, which cleaves and interchanges the substituents of the alkene. A related reaction is ethenolysis:\nMetal complexation.\nIn transition metal alkene complexes, alkenes serve as ligands for metals. In this case, the \u03c0 electron density is donated to the metal d orbitals. The stronger the donation is, the stronger the back bonding from the metal d orbital to \u03c0* anti-bonding orbital of the alkene. This effect lowers the bond order of the alkene and increases the C-C bond length. One example is the complex . These complexes are related to the mechanisms of metal-catalyzed reactions of unsaturated hydrocarbons.\nSynthesis.\nIndustrial methods.\nAlkenes are produced by hydrocarbon cracking. Raw materials are mostly natural-gas condensate components (principally ethane and propane) in the US and Mideast and naphtha in Europe and Asia. Alkanes are broken apart at high temperatures, often in the presence of a zeolite catalyst, to produce a mixture of primarily aliphatic alkenes and lower molecular weight alkanes. The mixture is feedstock and temperature dependent, and separated by fractional distillation. This is mainly used for the manufacture of small alkenes (up to six carbons).\nRelated to this is catalytic dehydrogenation, where an alkane loses hydrogen at high temperatures to produce a corresponding alkene. This is the reverse of the catalytic hydrogenation of alkenes.\nThis process is also known as reforming. Both processes are endothermic and are driven towards the alkene at high temperatures by entropy.\nCatalytic synthesis of higher \u03b1-alkenes (of the type RCH=CH2) can also be achieved by a reaction of ethylene with the organometallic compound triethylaluminium in the presence of nickel, cobalt, or platinum.\nElimination reactions.\nOne of the principal methods for alkene synthesis in the laboratory is the elimination reaction of alkyl halides, alcohols, and similar compounds. Most common is the \u03b2-elimination via the E2 or E1 mechanism. A commercially significant example is the production of vinyl chloride.\nThe E2 mechanism provides a more reliable \u03b2-elimination method than E1 for most alkene syntheses. Most E2 eliminations start with an alkyl halide or alkyl sulfonate ester (such as a tosylate or triflate). When an alkyl halide is used, the reaction is called a dehydrohalogenation. For unsymmetrical products, the more substituted alkenes (those with fewer hydrogens attached to the C=C) tend to predominate (see Zaitsev's rule). Two common methods of elimination reactions are dehydrohalogenation of alkyl halides and dehydration of alcohols. A typical example is shown below; note that if possible, the H is \"anti\" to the leaving group, even though this leads to the less stable \"Z\"-isomer.\nAlkenes can be synthesized from alcohols via dehydration, in which case water is lost via the E1 mechanism. For example, the dehydration of ethanol produces ethylene:\nAn alcohol may also be converted to a better leaving group (e.g., xanthate), so as to allow a milder \"syn\"-elimination such as the Chugaev elimination and the Grieco elimination. Related reactions include eliminations by \u03b2-haloethers (the Boord olefin synthesis) and esters (ester pyrolysis). A thioketone and a phosphite ester combined (the Corey-Winter olefination) or diphosphorus tetraiodide will deoxygenate glycols to alkenes. \nAlkenes can be prepared indirectly from alkyl amines. The amine or ammonia is not a suitable leaving group, so the amine is first either alkylated (as in the Hofmann elimination) or oxidized to an amine oxide (the Cope reaction) to render a smooth elimination possible. The Cope reaction is a \"syn\"-elimination that occurs at or below 150\u00a0\u00b0C, for example:\nThe Hofmann elimination is unusual in that the \"less\" substituted (non-Zaitsev) alkene is usually the major product.\nAlkenes are generated from \u03b1-halosulfones in the Ramberg\u2013B\u00e4cklund reaction, via a three-membered ring sulfone intermediate.\nSynthesis from carbonyl compounds.\nAnother important class of methods for alkene synthesis involves construction of a new carbon\u2013carbon double bond by coupling or condensation of a carbonyl compound (such as an aldehyde or ketone) to a carbanion or its equivalent. Pre-eminent is the aldol condensation. Knoevenagel condensations are a related class of reactions that convert carbonyls into alkenes.Well-known methods are called \"olefinations\". The Wittig reaction is illustrative, but other related methods are known, including the Horner\u2013Wadsworth\u2013Emmons reaction.\nThe Wittig reaction involves reaction of an aldehyde or ketone with a Wittig reagent (or phosphorane) of the type Ph3P=CHR to produce an alkene and Ph3P=O. The Wittig reagent is itself prepared easily from triphenylphosphine and an alkyl halide.\nRelated to the Wittig reaction is the Peterson olefination, which uses silicon-based reagents in place of the phosphorane. This reaction allows for the selection of \"E\"- or \"Z\"-products. If an \"E\"-product is desired, another alternative is the Julia olefination, which uses the carbanion generated from a phenyl sulfone. The Takai olefination based on an organochromium intermediate also delivers E-products. A titanium compound, Tebbe's reagent, is useful for the synthesis of methylene compounds; in this case, even esters and amides react.\nA pair of ketones or aldehydes can be deoxygenated to generate an alkene. Symmetrical alkenes can be prepared from a single aldehyde or ketone coupling with itself, using titanium metal reduction (the McMurry reaction). If different ketones are to be coupled, a more complicated method is required, such as the Barton\u2013Kellogg reaction.\nA single ketone can also be converted to the corresponding alkene via its tosylhydrazone, using sodium methoxide (the Bamford\u2013Stevens reaction) or an alkyllithium (the Shapiro reaction).\nSynthesis from alkenes.\nThe formation of longer alkenes via the step-wise polymerisation of smaller ones is appealing, as ethylene (the smallest alkene) is both inexpensive and readily available, with hundreds of millions of tonnes produced annually. The Ziegler\u2013Natta process allows for the formation of very long chains, for instance those used for polyethylene. Where shorter chains are wanted, as they for the production of surfactants, then processes incorporating a olefin metathesis step, such as the Shell higher olefin process are important. \nOlefin metathesis is also used commercially for the interconversion of ethylene and 2-butene to propylene. Rhenium- and molybdenum-containing heterogeneous catalysis are used in this process:\nTransition metal catalyzed hydrovinylation is another important alkene synthesis process starting from alkene itself. It involves the addition of a hydrogen and a vinyl group (or an alkenyl group) across a double bond.\nFrom alkynes.\nReduction of alkynes is a useful method for the stereoselective synthesis of disubstituted alkenes. If the \"cis\"-alkene is desired, hydrogenation in the presence of Lindlar's catalyst (a heterogeneous catalyst that consists of palladium deposited on calcium carbonate and treated with various forms of lead) is commonly used, though hydroboration followed by hydrolysis provides an alternative approach. Reduction of the alkyne by sodium metal in liquid ammonia gives the \"trans\"-alkene.\nFor the preparation multisubstituted alkenes, carbometalation of alkynes can give rise to a large variety of alkene derivatives.\nRearrangements and related reactions.\nAlkenes can be synthesized from other alkenes via rearrangement reactions. Besides olefin metathesis (described above), many pericyclic reactions can be used such as the ene reaction and the Cope rearrangement.\nIn the Diels\u2013Alder reaction, a cyclohexene derivative is prepared from a diene and a reactive or electron-deficient alkene.\nApplication.\nUnsaturated hydrocarbons are widely used to produce plastics, medicines, and other useful materials. \nNatural occurrence.\nAlkenes are prevalent in nature.\nPlants are the main natural source of alkenes in the form of terpenes. Many of the most vivid natural pigments are terpenes; e.g. lycopene (red in tomatoes), carotene (orange in carrots), and xanthophylls (yellow in egg yolk). The simplest of all alkenes, ethylene is a signaling molecule that influences the ripening of plants.\nIUPAC Nomenclature.\nAlthough the nomenclature is not followed widely, according to IUPAC, an alkene is an acyclic hydrocarbon with just one double bond between carbon atoms. Olefins comprise a larger collection of cyclic and acyclic alkenes as well as dienes and polyenes.\nTo form the root of the IUPAC names for straight-chain alkenes, change the \"-an-\" infix of the parent to \"-en-\". For example, CH3-CH3 is the alkane \"ethANe\". The name of CH2=CH2 is therefore \"ethENe\".\nFor straight-chain alkenes with 4 or more carbon atoms, that name does not completely identify the compound. For those cases, and for branched acyclic alkenes, the following rules apply:\nThe position of the double bond is often inserted before the name of the chain (e.g. \"2-pentene\"), rather than before the suffix (\"pent-2-ene\").\nThe positions need not be indicated if they are unique. Note that the double bond may imply a different chain numbering than that used for the corresponding alkane: C\u2013\u2013 is \"2,2-dimethyl pentane\", whereas C\u2013= is \"3,3-dimethyl 1-pentene\".\nMore complex rules apply for polyenes and cycloalkenes.\n\"Cis\"\u2013\"trans\" isomerism.\nIf the double bond of an acyclic mono-ene is not the first bond of the chain, the name as constructed above still does not completely identify the compound, because of \"cis\"\u2013\"trans\" isomerism. Then one must specify whether the two single C\u2013C bonds adjacent to the double bond are on the same side of its plane, or on opposite sides. For monoalkenes, the configuration is often indicated by the prefixes \"cis\"- (from Latin \"on this side of\") or \"trans\"- (\"across\", \"on the other side of\") before the name, respectively; as in \"cis\"-2-pentene or \"trans\"-2-butene.\nMore generally, \"cis\"\u2013\"trans\" isomerism will exist if each of the two carbons of in the double bond has two different atoms or groups attached to it. Accounting for these cases, the IUPAC recommends the more general E\u2013Z notation, instead of the \"cis\" and \"trans\" prefixes. This notation considers the group with highest CIP priority in each of the two carbons. If these two groups are on opposite sides of the double bond's plane, the configuration is labeled \"E\" (from the German \"entgegen\" meaning \"opposite\"); if they are on the same side, it is labeled \"Z\" (from German \"zusammen\", \"together\"). This labeling may be taught with mnemonic \"\"Z\" means 'on ze zame zide'\".\nGroups containing C=C double bonds.\nIUPAC recognizes two names for hydrocarbon groups containing carbon\u2013carbon double bonds, the vinyl group and the allyl group."}
{"id": "2762", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=2762", "title": "Allenes", "text": "In organic chemistry, allenes are organic compounds in which one carbon atom has double bonds with each of its two adjacent carbon atoms (, where R is H or some organyl group). Allenes are classified as cumulated dienes. The parent compound of this class is propadiene (), which is itself also called \"allene\". A group of the structure is called allenyl, while a substituent attached to an allene is referred to as an allenic substituent (R is H or some alkyl group). In analogy to allylic and propargylic, a substituent attached to a saturated carbon \u03b1 (i.e., directly adjacent) to an allene is referred to as an allenylic substituent. While allenes have two consecutive ('cumulated') double bonds, compounds with three or more cumulated double bonds are called cumulenes.\nHistory.\nFor many years, allenes were viewed as curiosities but thought to be synthetically useless and difficult to prepare and to work with. Reportedly, the first synthesis of an allene, glutinic acid, was performed in an attempt to prove the non-existence of this class of compounds. The situation began to change in the 1950s, and more than 300 papers on allenes have been published in 2012 alone. These compounds are not just interesting intermediates but synthetically valuable targets themselves; for example, over 150 natural products are known with an allene or cumulene fragment.\nStructure and properties.\nGeometry.\nThe central carbon atom of allenes forms two sigma bonds and two pi bonds. The central carbon atom is sp-hybridized, and the two terminal carbon atoms are sp2-hybridized. The bond angle formed by the three carbon atoms is 180\u00b0, indicating linear geometry for the central carbon atom. The two terminal carbon atoms are planar, and these planes are twisted 90\u00b0 from each other. The structure can also be viewed as an \"extended tetrahedral\" with a similar shape to methane, an analogy that is continued into the stereochemical analysis of certain derivative molecules.\nSymmetry.\nThe symmetry and isomerism of allenes has long fascinated organic chemists. For allenes with four identical substituents, there exist two twofold axes of rotation through the central carbon atom, inclined at 45\u00b0 to the CH2 planes at either end of the molecule. The molecule can thus be thought of as a two-bladed propeller. A third twofold axis of rotation passes through the C=C=C bonds, and there is a mirror plane passing through both CH2 planes. Thus this class of molecules belong to the D2d point group. Because of the symmetry, an unsubstituted allene has no net dipole moment, that is, it is a non-polar molecule.\nAn allene with two different substituents on each of the two carbon atoms will be chiral because there will no longer be any mirror planes. The chirality of these types of allenes was first predicted in 1875 by Jacobus Henricus van 't Hoff, but not proven experimentally until 1935. Where A has a greater priority than B according to the Cahn\u2013Ingold\u2013Prelog priority rules, the configuration of the axial chirality can be determined by considering the substituents on the front atom followed by the back atom when viewed along the allene axis. For the back atom, only the group of higher priority need be considered.\nChiral allenes have been recently used as building blocks in the construction of organic materials with exceptional chiroptical properties. There are a few examples of drug molecule having an allene system in their structure. \u00a0Mycomycin, an antibiotic with tuberculostatic properties, is a typical example. This drug exhibits enantiomerism due to the presence of a suitably substituted allene system.\nAlthough the semi-localized textbook \u03c3-\u03c0 separation model describes the bonding of allene using a pair of localized orthogonal \u03c0 orbitals, the full molecular orbital description of the bonding is more subtle. The symmetry-correct doubly-degenerate HOMOs of allene (adapted to the D2d point group) can either be represented by a pair of orthogonal MOs \"or\" as twisted helical linear combinations of these orthogonal MOs. The symmetry of the system and the degeneracy of these orbitals imply that both descriptions are correct (in the same way that there are infinitely many ways to depict the doubly-degenerate HOMOs and LUMOs of benzene that correspond to different choices of eigenfunctions in a two-dimensional eigenspace). However, this degeneracy is lifted in substituted allenes, and the helical picture becomes the only symmetry-correct description for the HOMO and HOMO\u20131 of the C2-symmetric 1,3-dimethylallene. This qualitative MO description extends to higher odd-carbon cumulenes (e.g., 1,2,3,4-pentatetraene).\nChemical and spectral properties.\nAllenes differ considerably from other alkenes in terms of their chemical properties. Compared to isolated and conjugated dienes, they are considerably less stable: comparing the isomeric pentadienes, the allenic 1,2-pentadiene has a heat of formation of 33.6 kcal/mol, compared to 18.1 kcal/mol for (\"E\")-1,3-pentadiene and 25.4 kcal/mol for the isolated 1,4-pentadiene.\nThe C\u2013H bonds of allenes are considerably weaker and more acidic compared to typical vinylic C\u2013H bonds: the bond dissociation energy is 87.7 kcal/mol (compared to 111 kcal/mol in ethylene), while the gas-phase acidity is 381 kcal/mol (compared to 409 kcal/mol for ethylene), making it slightly more acidic than the propargylic C\u2013H bond of propyne (382 kcal/mol). \nThe 13C NMR spectrum of allenes is characterized by the signal of the sp-hybridized carbon atom, resonating at a characteristic 200-220 ppm. In contrast, the sp2-hybridized carbon atoms resonate around 80 ppm in a region typical for alkyne and nitrile carbon atoms, while the protons of a CH2 group of a terminal allene resonate at around 4.5 ppm \u2014 somewhat upfield of a typical vinylic proton. \nAllenes possess a rich cycloaddition chemistry, including both [4+2] and [2+2] modes of addition, as well as undergoing formal cycloaddition processes catalyzed by transition metals. Allenes also serve as substrates for transition metal catalyzed hydrofunctionalization reactions.\nSynthesis.\nAlthough allenes often require specialized syntheses, the parent allene, propadiene is produced industrially on a large scale as an equilibrium mixture with propyne:\nThis mixture, known as MAPP gas, is commercially available. At 298 K, the \u0394\"G\u00b0\" of this reaction is \u20131.9 kcal/mol, corresponding to \"K\"eq = 24.7.\nThe first allene to be synthesized was penta-2,3-dienedioic acid, which was prepared by Burton and Pechmann in 1887. However, the structure was only correctly identified in 1954.\nLaboratory methods for the formation of allenes include:\nThe chemistry of allenes has been reviewed in a number of books and journal articles. Some key approaches towards allenes are outlined in the following scheme:\nOne of the older methods is the Skatteb\u00f8l rearrangement (also called the Doering\u2013Moore\u2013Skatteb\u00f8l or Doering\u2013LaFlamme rearrangement), in which a gem-dihalocyclopropane 3 is treated with an organolithium compound (or dissolving metal) and the presumed intermediate rearranges into an allene either directly or via carbene-like species. Notably, even strained allenes can be generated by this procedure. Modifications involving leaving groups of different nature are also known. Arguably, the most convenient modern method of allene synthesis is by sigmatropic rearrangement of propargylic substrates. Johnson\u2013Claisen and Ireland\u2013Claisen rearrangements of ketene acetals 4 have been used a number of times to prepare allenic esters and acids. Reactions of vinyl ethers 5 (the Saucy\u2013Marbet rearrangement) give allene aldehydes, while propargylic sulfenates 6 give allene sulfoxides. Allenes can also be prepared by nucleophilic substitution in 9 and 10 (nucleophile Nu\u2212 can be a hydride anion), 1,2-elimination from 8, proton transfer in 7, and other, less general, methods.\nUse and occurrence.\nAllene itself is the most commonly used member of this family; it exists in equilibrium with propyne as a component of MAPP gas.\nResearch.\nThe reactivity of substituted allenes has been well explored. \nThe two \u03c0-bonds are located at the 90\u00b0 angle to each other, and thus require a reagent to approach from somewhat different directions. With an appropriate substitution pattern, allenes exhibit axial chirality as predicted by van\u2019t Hoff as early as 1875. Protonation of allenes gives cations 11 that undergo further transformations. Reactions with soft electrophiles (e.g. Br+) deliver positively charged onium ions 13. Transition-metal-catalysed reactions proceed via allylic intermediates 15 and have attracted significant interest in recent years. Numerous cycloadditions are also known, including [4+2]-, (2+1)-, and [2+2]-variants, which deliver, e.g., 12, 14, and 16, respectively.\nAnother area of allene chemistry involves cyclic allenes, where the allene double bonds exist in a ring. This approach was applied to the total synthesis of lissodendoric acid A. Cyclic allenes also participate in metal-mediated processes. and the generation of DNA-encoded libraries using cyclic allene intermediates.\nOccurrence.\nNumerous natural products contain the allene functional group. Noteworthy are the pigments fucoxanthin and peridinin. Little is known about the biosynthesis, although it is conjectured that they are often generated from alkyne precursors.\nAllenes serve as ligands in organometallic chemistry. A typical complex is Pt(\u03b72-allene)(PPh3)2. Ni(0) reagents catalyze the cyclooligomerization of allene. Using a suitable catalyst (e.g. Wilkinson's catalyst), it is possible to reduce just one of the double bonds of an allene.\nDelta convention.\nMany rings or ring systems are known by semisystematic names that assume a maximum number of noncumulative bonds. To unambiguously specify derivatives that include cumulated bonds (and hence fewer hydrogen atoms than would be expected from the skeleton), a lowercase delta may be used with a subscript indicating the number of cumulated double bonds from that atom, e.g. 8\u03b42-benzocyclononene. This may be combined with the \u03bb-convention for specifying nonstandard valency states, e.g. 2\u03bb4\u03b42,5\u03bb4\u03b42-thieno[3,4-c]thiophene."}
{"id": "2763", "revid": "2821695", "url": "https://en.wikipedia.org/wiki?curid=2763", "title": "Alkyne", "text": "formula_1\nAcetylene\nformula_2\n1-Butyne\nIn organic chemistry, an alkyne is an unsaturated hydrocarbon containing at least one carbon\u2014carbon triple bond. The simplest acyclic alkynes with only one triple bond and no other functional groups form a homologous series with the general chemical formula . Alkynes are traditionally known as acetylenes, although the name \"acetylene\" also refers specifically to , known formally as ethyne using IUPAC nomenclature. Like other hydrocarbons, alkynes are generally hydrophobic.\nStructure and bonding.\nIn acetylene, the H\u2013C\u2261C bond angles are 180\u00b0. By virtue of this bond angle, alkynes are rod-like. Correspondingly, cyclic alkynes are rare. Benzyne cannot be isolated. The C\u2261C bond distance of 118 picometers (for C2H2) is much shorter than the C=C distance in alkenes (132\u00a0pm, for C2H4) or the C\u2013C bond in alkanes (153\u00a0pm).\nThe triple bond is very strong with a bond strength of 839 kJ/mol. The sigma bond contributes 369\u00a0kJ/mol, the first pi bond contributes 268\u00a0kJ/mol. and the second pi bond 202\u00a0kJ/mol. Bonding is usually discussed in the context of molecular orbital theory, which recognizes the triple bond as arising from overlap of s and p orbitals. In the language of valence bond theory, the carbon atoms in an alkyne bond are sp hybridized: they each have two unhybridized p orbitals and two sp hybrid orbitals. Overlap of an sp orbital from each atom forms one sp\u2013sp sigma bond. Each p orbital on one atom overlaps one on the other atom, forming two pi bonds, giving a total of three bonds. The remaining sp orbital on each atom can form a sigma bond to another atom, for example to hydrogen atoms in the parent acetylene. The two sp orbitals project on opposite sides of the carbon atom.\nTerminal and internal alkynes.\nInternal alkynes feature carbon substituents on each acetylenic carbon. Symmetrical examples include diphenylacetylene and 3-hexyne. They may also be asymmetrical, such as in 2-pentyne.\nTerminal alkynes have the formula , where at least one end of the alkyne is a hydrogen atom. An example is methylacetylene (propyne using IUPAC nomenclature). They are often prepared by alkylation of monosodium acetylide. Terminal alkynes, like acetylene itself, are mildly acidic, with p\"K\"a values of around 25. They are far more acidic than alkenes and alkanes, which have p\"K\"a values of around 40 and 50, respectively. The acidic hydrogen on terminal alkynes can be replaced by a variety of groups resulting in halo-, silyl-, and alkoxoalkynes. The carbanions generated by deprotonation of terminal alkynes are called acetylides. Internal alkynes are also considerably more acidic than alkenes and alkanes, though not nearly as acidic as terminal alkynes. The C\u2013H bonds at the \u03b1 position of alkynes (propargylic C\u2013H bonds) can also be deprotonated using strong bases, with an estimated p\"K\"a of 35. This acidity can be used to isomerize internal alkynes to terminal alkynes using the alkyne zipper reaction.\nNaming alkynes.\nIn systematic chemical nomenclature, alkynes are named with the Greek prefix system without any additional letters. Examples include ethyne or octyne. In parent chains with four or more carbons, it is necessary to say where the triple bond is located. For octyne, one can either write 3-octyne or oct-3-yne when the bond starts at the third carbon. The lowest number possible is given to the triple bond. When no superior functional groups are present, the parent chain must include the triple bond even if it is not the longest possible carbon chain in the molecule. Ethyne is commonly called by its trivial name acetylene.\nIn chemistry, the suffix -yne is used to denote the presence of a triple bond. In organic chemistry, the suffix often follows IUPAC nomenclature. However, inorganic compounds featuring unsaturation in the form of triple bonds may be denoted by substitutive nomenclature with the same methods used with alkynes (i.e. the name of the corresponding saturated compound is modified by replacing the \"-ane\" ending with \"-yne\"). \"-diyne\" is used when there are two triple bonds, and so on. The position of unsaturation is indicated by a numerical locant immediately preceding the \"-yne\" suffix, or 'locants' in the case of multiple triple bonds. Locants are chosen so that the numbers are low as possible. \"-yne\" is also used as a suffix to name substituent groups that are triply bound to the parent compound.\nSometimes a number between hyphens is inserted before it to state which atoms the triple bond is between. This suffix arose as a collapsed form of the end of the word \"acetylene\". The final \"-e\" disappears if it is followed by another suffix that starts with a vowel.\nStructural isomerism.\nAlkynes having four or more carbon atoms can form different structural isomers by having the triple bond in different positions or having some of the carbon atoms be substituents rather than part of the parent chain. Other non-alkyne structural isomers are also possible.\nSynthesis.\nFrom calcium carbide.\nClassically, acetylene was prepared by hydrolysis (protonation) of calcium carbide (Ca2+[:C\u2261C:]2\u2013):\nwhich was in turn synthesized by combining quicklime and coke in an electric arc furnace at 2200 \u00b0C:\nThis was an industrially important process which provided access to hydrocarbons from coal resources for countries like Germany and China. However, the energy-intensive nature of this process is a major disadvantage and its share of the world's production of acetylene has steadily decreased relative to hydrocarbon cracking.\nCracking.\nCommercially, the dominant alkyne is acetylene itself, which is used as a fuel and a precursor to other compounds, e.g., acrylates. Hundreds of millions of kilograms are produced annually by partial oxidation of natural gas:\nPropyne, also industrially useful, is also prepared by thermal cracking of hydrocarbons. \nAlkylation and arylation of terminal alkynes.\nTerminal alkynes (RC\u2261CH, including acetylene itself) can be deprotonated by bases like NaNH2, BuLi, or EtMgBr to give acetylide anions (RC\u2261C:\u2013M+, M = Na, Li, MgBr) which can be alkylated by addition to carbonyl groups (Favorskii reaction), ring opening of epoxides, or SN2-type substitution of unhindered primary alkyl halides. \nIn the presence of transition metal catalysts, classically a combination of Pd(PPh3)2Cl2 and CuI, terminal acetylenes (RC\u2261CH) can react with aryl iodides and bromides (ArI or ArBr) in the presence of a secondary or tertiary amine like Et3N to give arylacetylenes (RC\u2261CAr) in the Sonogashira reaction.\nThe availability of these reliable reactions makes terminal alkynes useful building blocks for preparing internal alkynes.\nDehydrohalogenation and related reactions.\nAlkynes are prepared from 1,1- and 1,2-dihaloalkanes by double dehydrohalogenation. The reaction provides a means to generate alkynes from alkenes, which are first halogenated and then dehydrohalogenated. For example, phenylacetylene can be generated from styrene by bromination followed by treatment of the resulting of 1,2-dibromo-1-phenylethane with sodium amide in ammonia:\nVia the Fritsch\u2013Buttenberg\u2013Wiechell rearrangement, alkynes are prepared from vinyl bromides. Alkynes can be prepared from aldehydes using the Corey\u2013Fuchs reaction and from aldehydes or ketones by the Seyferth\u2013Gilbert homologation. \nVinyl halides are susceptible to dehydrohalogenation.\nReactions, including applications.\nFeaturing a reactive functional group, alkynes participate in many organic reactions. Such use was pioneered by Ralph Raphael, who in 1955 wrote the first book describing their versatility as intermediates in synthesis. In spite of their kinetic stability (persistence) due to their strong triple bonds, alkynes are a thermodynamically unstable functional group, as can be gleaned from the highly positive heats of formation of small alkynes. For example, acetylene has a heat of formation of +227.4 kJ/mol (+54.2 kcal/mol), indicating a much higher energy content compared to its constituent elements. The highly exothermic combustion of acetylene is exploited industrially in oxyacetylene torches used in welding. Other reactions involving alkynes are often highly thermodynamically favorable (exothermic/exergonic) for the same reason.\nHydrogenation.\nBeing more unsaturated than alkenes, alkynes characteristically undergo reactions that show that they are \"doubly unsaturated\". Alkynes are capable of adding two equivalents of , whereas an alkene adds only one equivalent. Depending on catalysts and conditions, alkynes add one or two equivalents of hydrogen. Partial hydrogenation, stopping after the addition of only one equivalent to give the alkene, is usually more desirable since alkanes are less useful:\nThe largest scale application of this technology is the conversion of acetylene to ethylene in refineries (the steam cracking of alkanes yields a few percent acetylene, which is selectively hydrogenated in the presence of a palladium/silver catalyst). For more complex alkynes, the Lindlar catalyst is widely recommended to avoid formation of the alkane, for example in the conversion of phenylacetylene to styrene. Similarly, halogenation of alkynes gives the alkene dihalides or alkyl tetrahalides:\nThe addition of one equivalent of to internal alkynes gives cis-alkenes.\nAddition of halogens and related reagents.\nAlkynes characteristically are capable of adding two equivalents of halogens and hydrogen halides. \nThe addition of nonpolar bonds across is general for silanes, boranes, and related hydrides. The hydroboration of alkynes gives vinylic boranes which oxidize to the corresponding aldehyde or ketone. In the thiol-yne reaction the substrate is a thiol.\nAddition of hydrogen halides has long been of interest. In the presence of mercuric chloride as a catalyst, acetylene and hydrogen chloride react to give vinyl chloride. While this method has been abandoned in the West, it remains the main production method in China.\nHydration.\nThe hydration reaction of acetylene gives acetaldehyde. The reaction proceeds by formation of vinyl alcohol, which tautomerizes to form the aldehyde. This reaction was once a major industrial process but it has been displaced by the Wacker process. This reaction occurs in nature, the catalyst being acetylene hydratase.\nHydration of phenylacetylene gives acetophenone:\n catalyzes hydration of 1,8-nonadiyne to 2,8-nonanedione:\nIsomerization to allenes.\nAlkynes can be isomerized by strong base or transition metals to allenes. Due to their comparable thermodynamic stabilities, the equilibrium constant of alkyne/allene isomerization is generally within several orders of magnitude of unity. For example propyne can be isomerized to give an equilibrium mixture with propadiene:\nCycloadditions and oxidation.\nAlkynes undergo diverse cycloaddition reactions. The Diels\u2013Alder reaction with 1,3-dienes gives 1,4-cyclohexadienes. This general reaction has been extensively developed. Electrophilic alkynes are especially effective dienophiles. The \"cycloadduct\" derived from the addition of alkynes to 2-pyrone eliminates carbon dioxide to give the aromatic compound. Other specialized cycloadditions include multicomponent reactions such as alkyne trimerisation to give aromatic compounds and the [2+2+1]-cycloaddition of an alkyne, alkene and carbon monoxide in the Pauson\u2013Khand reaction. Non-carbon reagents also undergo cyclization, e.g. azide alkyne Huisgen cycloaddition to give triazoles. Cycloaddition processes involving alkynes are often catalyzed by metals, e.g. enyne metathesis and alkyne metathesis, which allows the scrambling of carbyne (RC) centers:\nOxidative cleavage of alkynes proceeds via cycloaddition to metal oxides. Most famously, potassium permanganate converts alkynes to a pair of carboxylic acids.\nReactions specific for terminal alkynes.\nTerminal alkynes are readily converted to many derivatives, e.g. by coupling reactions and condensations. Via the condensation with formaldehyde and acetylene is produced butynediol: \nIn the Sonogashira reaction, terminal alkynes are coupled with aryl or vinyl halides:\nThis reactivity exploits the fact that terminal alkynes are weak acids, whose typical p\"K\"a values around 25 place them between that of ammonia (35) and ethanol (16):\nwhere MX = NaNH2, LiBu, or RMgX.\nThe reactions of alkynes with certain metal cations, e.g. and also gives acetylides. Thus, few drops of diamminesilver(I) hydroxide () reacts with terminal alkynes signaled by formation of a white precipitate of the silver acetylide. This reactivity is the basis of alkyne coupling reactions, including the Cadiot\u2013Chodkiewicz coupling, Glaser coupling, and the Eglinton coupling shown below: \nIn the Favorskii reaction and in alkynylations in general, terminal alkynes add to carbonyl compounds to give the hydroxyalkyne.\nMetal complexes.\nAlkynes form complexes with transition metals. Such complexes occur also in metal catalyzed reactions of alkynes such as alkyne trimerization. Terminal alkynes, including acetylene itself, react with water to give aldehydes. The transformation typically requires metal catalysts to give this anti-Markovnikov addition result.\nAlkynes in nature and medicine.\nAccording to Ferdinand Bohlmann, the first naturally occurring acetylenic compound, dehydromatricaria ester, was isolated from an \"Artemisia\" species in 1826. In the nearly two centuries that have followed, well over a thousand naturally occurring acetylenes have been discovered and reported. Polyynes, a subset of this class of natural products, have been isolated from a wide variety of plant species, cultures of higher fungi, bacteria, marine sponges, and corals. Some acids like tariric acid contain an alkyne group. Diynes and triynes, species with the linkage RC\u2261C\u2013C\u2261CR\u2032 and RC\u2261C\u2013C\u2261C\u2013C\u2261CR\u2032 respectively, occur in certain plants (\"Ichthyothere\", \"Chrysanthemum\", \"Cicuta\", \"Oenanthe\" and other members of the Asteraceae and Apiaceae families). Some examples are cicutoxin, oenanthotoxin, and falcarinol. These compounds are highly bioactive, e.g. as nematocides. 1-Phenylhepta-1,3,5-triyne is illustrative of a naturally occurring triyne. Biosynthetically, the enediyne natural products are also derived from a polyyne precursor.\nAlkynes occur in some pharmaceuticals, including the contraceptive noretynodrel. A carbon\u2013carbon triple bond is also present in marketed drugs such as the antiretroviral efavirenz and the antifungal terbinafine. Molecules called ene-diynes feature a ring containing an alkene (\"ene\") between two alkyne groups (\"diyne\"). These compounds, e.g. calicheamicin, are some of the most aggressive antitumor drugs known, so much so that the ene-diyne subunit is sometimes referred to as a \"warhead\". Ene-diynes undergo rearrangement via the Bergman cyclization, generating highly reactive radical intermediates that attack DNA within the tumor."}
{"id": "2764", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=2764", "title": "AbiWord", "text": "AbiWord () is a free and open-source word processor. It is written in C++ and since version 3 it is based on GTK+ 3. The name \"AbiWord\" is derived from the root of the Spanish word \"abierto\", meaning \"open\".\nAbiWord was originally started by SourceGear Corporation as the first part of a proposed AbiSuite but was adopted by open source developers after SourceGear changed its business focus and ceased development. It now runs on Linux, ReactOS, Solaris, AmigaOS 4.0 (through its Cygwin X11 engine), MeeGo (on the Nokia N9 smartphone), Maemo (on the Nokia N810), QNX and other operating systems. Development of a version for Microsoft Windows has temporarily ended due to lack of maintainers (the latest released versions are 2.8.6 and 2.9.4 beta).\nThe macOS port has remained on version 2.4 since 2005, although the current version does run non-natively on macOS through XQuartz.\nAbiWord is part of the AbiSource project which develops a number of office-related technologies.\nFeatures.\nAbiWord supports both basic word processing features such as lists, indents and character formats, and more sophisticated features including tables, styles, page headers and footers, footnotes, templates, multiple views, page columns, spell checking, and grammar checking. The Presentation view of AbiWord, which permits easy display of presentations created in AbiWord on \"screen-sized\" pages, is another feature not often found in word processors.\nInterface.\nAbiWord generally works similarly to classic versions (pre-Office 2007) of Microsoft Word, as direct ease of migration was a high priority early goal. While many interface similarities remain, cloning the Word interface is no longer a top priority. The interface is intended to follow user interface guidelines for each respective platform.\nCollaboration.\nAbiWord allows users to share and collaborate on documents in a similar manner to Google Docs, using a system known as GOCollab. Users can collaborate using a varitety of different protocols including TCP and XMPP, and formerly over AbiCollab.net, a web based service that facilitated collaboration between users.\nFile formats.\nAbiWord comes with several import and export filters providing partial support for such formats as HTML, Microsoft Word (.doc), Office Open XML (.docx), OpenDocument Text (.odt), Rich Text Format (.rtf), and text documents (.txt). LaTeX is supported for export only. Plug-in filters are available to deal with many other formats, notably WordPerfect documents. The native file format, .abw, uses XML, so as to mitigate vendor lock-in concerns with respect to interoperability and digital archiving.\nGrammar checking.\nThe AbiWord project includes a US English-only grammar checking plugin using Link Grammar. AbiWord had grammar checking before any other open source word processor, although a grammar checker was later added to OpenOffice.org. Link Grammar is both a theory of syntax and an open source parser which is now developed by the AbiWord project."}
{"id": "2765", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2765", "title": "ATC classification", "text": ""}
{"id": "2766", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=2766", "title": "Ames test", "text": "The Ames test is a widely employed method that uses bacteria to test whether a given chemical can cause mutations in the DNA of the test organism. More formally, it is a biological assay to assess the mutagenic potential of chemical compounds. A positive test indicates that the chemical is mutagenic and therefore may act as a carcinogen, because cancer is often linked to mutation. The test serves as a quick and convenient assay to estimate the carcinogenic potential of a compound because standard carcinogen assays on mice and rats are time-consuming (taking two to three years to complete) and expensive. However, false-positives and false-negatives are known.\nThe procedure was described in a series of papers in the early 1970s by Bruce Ames and his group at the University of California, Berkeley.\nGeneral procedure.\nThe Ames test uses several strains of the bacterium \"Salmonella typhimurium\" that carry mutations in genes involved in histidine synthesis. These strains are auxotrophic mutants, i.e. they require histidine for growth, but cannot produce it. The method tests the capability of the tested substance in creating mutations that result in a return to a \"prototrophic\" state, so that the cells can grow on a histidine-free medium.\nThe tester strains are specially constructed to detect either frameshift (e.g. strains TA-1537 and TA-1538) or point (e.g. strain TA-1531) mutations in the genes required to synthesize histidine, so that mutagens acting via different mechanisms may be identified. Some compounds are quite specific, causing reversions in just one or two strains. The tester strains also carry mutations in the genes responsible for lipopolysaccharide synthesis, making the cell wall of the bacteria more permeable, and in the excision repair system to make the test more sensitive.\nLarger organisms like mammals have metabolic processes that could potentially turn a chemical considered not mutagenic into one that is or one that is considered mutagenic into one that is not. Therefore, to more effectively test a chemical compound's mutagenicity in relation to larger organisms, rat liver enzymes can be added in an attempt to replicate the metabolic processes' effect on the compound being tested in the Ames Test. Rat liver extract is optionally added to simulate the effect of metabolism, as some compounds, like benzo[\"a\"]pyrene, are not mutagenic themselves but their metabolic products are.\nThe bacteria are spread on an agar plate with a small amount of histidine. This small amount of histidine in the growth medium allows the bacteria to grow for an initial time and have the opportunity to mutate.\nWhen the histidine is depleted only bacteria that have mutated to gain the ability to produce its own histidine will survive. The plate is incubated for 48 hours. The mutagenicity of a substance is proportional to the number of colonies observed.\nAmes test and carcinogens.\nMutagens identified via Ames test are also possible carcinogens, and early studies by Ames showed that 90% of known carcinogens may be identified via this test. Later studies however showed identification of 50\u201370% of known carcinogens. The test was used to identify a number of compounds previously used in commercial products as potential carcinogens. Examples include tris(2,3-dibromopropyl)phosphate, which was used as a flame retardant in plastic and textiles such as children's sleepwear, and furylfuramide which was used as an antibacterial additive in food in Japan in the 1960s and 1970s. Furylfuramide in fact had previously passed animal tests, but more vigorous tests after its identification in the Ames test showed it to be carcinogenic. Their positive tests resulted in those chemicals being withdrawn from use in consumer products.\nOne interesting result from the Ames test is that the dose response curve using varying concentrations of the chemical is almost always linear, indicating that there is no threshold concentration for mutagenesis. It therefore suggests that, as with radiation, there may be no safe threshold for chemical mutagens or carcinogens. However, some have proposed that organisms could tolerate low levels of mutagens due to protective mechanisms such as DNA repair, and thus a threshold may exist for certain chemical mutagens. Bruce Ames himself argued against linear dose-response extrapolation from the high dose used in carcinogenesis tests in animal systems to the lower dose of chemicals normally encountered in human exposure, as the results may be false positives due to mitogenic response caused by the artificially high dose of chemicals used in such tests. He also cautioned against the \"hysteria over tiny traces of chemicals that may or may not cause cancer\", that \"completely drives out the major risks you should be aware of\".\nThe Ames test is often used as one of the initial screens for potential drugs to weed out possible carcinogens, and it is one of the eight tests required under the Pesticide Act (USA) and one of the six tests required under the Toxic Substances Control Act (USA).\nLimitations.\n\"Salmonella typhimurium\" is a prokaryote, therefore it is not a perfect model for humans. Rat liver S9 fraction is used to mimic the mammalian metabolic conditions so that the mutagenic potential of metabolites formed by a parent molecule in the hepatic system can be assessed; however, there are differences in metabolism between humans and rats that can affect the mutagenicity of the chemicals being tested. The test may therefore be improved by the use of human liver S9 fraction; its use was previously limited by its availability, but it is now available commercially and therefore may be more feasible. An adapted \"in vitro\" model has been made for eukaryotic cells, for example yeast.\nMutagens identified in the Ames test need not necessarily be carcinogenic, and further tests are required for any potential carcinogen identified in the test. Drugs that contain the nitrate moiety sometimes come back positive for Ames when they are indeed safe. The nitrate compounds may generate nitric oxide, an important signal molecule that can give a false positive. Nitroglycerin is an example that gives a positive Ames yet is still used in treatment today. Nitrates in food however may be reduced by bacterial action to nitrites which are known to generate carcinogens by reacting with amines and amides. Long toxicology and outcome studies are needed with such compounds to disprove a positive Ames test.\nFluctuation method.\nThe Ames test was initially developed using agar plates (the plate incorporation technique), as described above. Since that time, an alternative to performing the Ames test has been developed, which is known as the \"fluctuation method\". This technique is the same in concept as the agar-based method, with bacteria being added to a reaction mixture with a small amount of histidine, which allows the bacteria to grow and mutate, returning to synthesize their own histidine. By including a pH indicator, the frequency of mutation is counted in microplates as the number of wells which have changed color (caused by a drop in pH due to metabolic processes of reproducing bacteria). As with the traditional Ames test, the sample is compared to the natural background rate of reverse mutation in order to establish the genotoxicity of a substance. The fluctuation method is performed entirely in liquid culture and is scored by counting the number of wells that turn yellow from purple in 96-well or 384-well microplates.\nIn the 96-well plate method the frequency of mutation is counted as the number of wells out of 96 which have changed color. The plates are incubated for up to five days, with mutated (yellow) colonies being counted each day and compared to the background rate of reverse mutation using established tables of significance to determine the significant differences between the background rate of mutation and that for the tested samples.\nIn the more scaled-down 384-well plate microfluctuation method the frequency of mutation is counted as the number of wells out of 48 which have changed color after 2 days of incubation. A test sample is assayed across 6 dose levels with concurrent zero-dose (background) and positive controls which all fit into one 384-well plate. The assay is performed in triplicates to provide statistical robustness. It uses the recommended OECD Guideline 471 tester strains (histidine auxotrophs and tryptophan auxotrophs).\nThe fluctuation method is comparable to the traditional pour plate method in terms of sensitivity and accuracy, however, it does have a number of advantages: it needs less test sample, it has a simple colorimetric endpoint, counting the number of positive wells out of possible 96 or 48 wells is much less time-consuming than counting individual colonies on an agar plate. Several commercial kits are available. Most kits have consumable components in a ready-to-use state, including lyophilized bacteria, and tests can be performed using multichannel pipettes. The fluctuation method also allows for testing higher volumes of aqueous samples (up to 75% v/v), increasing the sensitivity and extending its application to low-level environmental mutagens."}
{"id": "2767", "revid": "1934512", "url": "https://en.wikipedia.org/wiki?curid=2767", "title": "ACE inhibitor", "text": "Angiotensin-converting-enzyme inhibitors (ACE inhibitors) are a class of medication used primarily for the treatment of high blood pressure and heart failure. This class of medicine works by causing relaxation of blood vessels as well as a decrease in blood volume, which leads to lower blood pressure and decreased oxygen demand from the heart.\nACE inhibitors inhibit the activity of angiotensin-converting enzyme, an important component of the renin\u2013angiotensin system which converts angiotensin I to angiotensin II, and hydrolyses bradykinin. Therefore, ACE inhibitors decrease the formation of angiotensin II, a vasoconstrictor, and increase the level of bradykinin, a peptide vasodilator. This combination is synergistic in lowering blood pressure.\nAs a result of inhibiting the ACE enzyme in the bradykinin system, the ACE inhibitor drugs allow for increased levels of bradykinin which would normally be degraded. Bradykinin produces prostaglandin. This mechanism can explain the two most common side effects seen with ACE Inhibitors: angioedema and cough.\nFrequently prescribed ACE inhibitors include benazepril, zofenopril, perindopril, trandolapril, captopril, enalapril, lisinopril, and ramipril.\nMedical use.\nACE inhibitors were initially approved for the treatment of hypertension and can be used alone or in combination with other anti-hypertensive medications. Later, they were found useful for other cardiovascular and kidney diseases including:\nIn treating high blood pressure, ACE inhibitors are often the first drug choice, particularly when diabetes is present, but age can lead to different choices and it is common to need more than one drug to obtain the desired improvement. There are fixed-dose combination drugs, such as ACE inhibitor and thiazide combinations. ACE inhibitors have also been used in chronic kidney failure and kidney involvement in systemic sclerosis (hardening of tissues, as scleroderma renal crisis). In those with stable coronary artery disease, but no heart failure, benefits are similar to other usual treatments.\nIn 2012, a meta-analysis published in the BMJ described the protective role of ACE inhibitors in reducing the risk of pneumonia when compared to angiotensin II receptor blocker (ARBs). The authors found a decreased risk in patients with previous stroke (54% risk reduction), with heart failure (37% risk reduction), and of Asian descent (43% risk reduction vs 54% risk reduction in non-Asian population). However, no reduced pneumonia-related mortality was observed.\nOther.\nACE inhibitors may also be used to help decrease excessive water consumption in people with schizophrenia resulting in psychogenic polydipsia. A double-blind, placebo-controlled trial showed that when used for this purpose, enalapril led to decreased consumption (determined by urine output and osmolality) in 60% of people; the same effect has been demonstrated in other ACE inhibitors.\nAdditionally ACE-I are commonly used after renal transplant to manage post-transplant erythrocytosis, a condition characterised by a persistently high hematocrit greater than 51% which often develops 8\u201324 months after successful transplantation, as ACE-I have been shown to decrease erythropoietin production.\nAdverse effects.\nCommon side effects include: low blood pressure, cough, hyperkalemia, headache, dizziness, fatigue, nausea, and kidney impairment.\nThe main adverse effects of ACE inhibition can be understood from their pharmacological action. The other reported adverse effects are liver problems and effects on the fetus. Kidney problems may occur with all ACE inhibitors that directly follows from their mechanism of action. Patients starting on an ACE inhibitor usually have a modest reduction in glomerular filtration rate (GFR). However, the decrease may be significant in conditions of \"pre-existing\" decreased renal perfusions, such as renal artery stenosis, heart failure, polycystic kidney disease, or volume depletion. In these patients, the maintenance of GFR depends on angiotensin-II-dependent efferent vasomotor tone. Therefore, renal function should be closely monitored over the first few days after initiation of treatment with ACE inhibitor in patients with decreased renal perfusion. A moderate reduction in renal function, no greater than 30% rise in serum creatinine, that is stabilized after a week of treatment is deemed acceptable as part of the therapeutic effect, providing the residual renal function is sufficient.\nReduced GFR is especially a problem if the patient is concomitantly taking an NSAID and a diuretic. When the three drugs are taken together, the risk of developing renal failure is significantly increased.\nHigh blood potassium is another possible complication of treatment with an ACE inhibitor due to its effect on aldosterone. Suppression of angiotensin II leads to a decrease in aldosterone levels. Since aldosterone is responsible for increasing the excretion of potassium, ACE inhibitors can cause retention of potassium. Some people, however, can continue to lose potassium while on an ACE inhibitor. Hyperkalemia may decrease the velocity of impulse conduction in the nerves and muscles, including cardiac tissues. This leads to cardiac dysfunction and neuromuscular consequences, such as muscle weakness, paresthesia, nausea, diarrhea, and others. Close monitoring of potassium levels is required in patients receiving treatment with ACE inhibitors who are at risk of hyperkalemia.\nAnother possible adverse effect specific for ACE inhibitors, but not for other RAAS blockers, is an increase in bradykinin level.\nA persistent dry cough is a relatively common adverse effect believed to be associated with the increases in bradykinin levels produced by ACE inhibitors, although the role of bradykinin in producing these symptoms has been disputed. Many cases of cough in people on ACE inhibitors may not be from the medication itself, however. People who experience this cough are often switched to angiotensin II receptor antagonists.\nSome (0.7%) develop angioedema due to increased bradykinin levels. A genetic predisposition may exist.\nA severe rare allergic reaction can affect the bowel wall and secondarily cause abdominal pain.\nBlood.\nHematologic effects, such as neutropenia, agranulocytosis and other blood dyscrasias, have occurred during therapy with ACE inhibitors, especially in people with additional risk factors.\nPregnancy.\nIn pregnant women, ACE inhibitors taken during all the trimesters have been reported to cause congenital malformations, stillbirths, and neonatal deaths. Commonly reported fetal abnormalities include hypotension, renal dysplasia, anuria/oliguria, oligohydramnios, intrauterine growth retardation, pulmonary hypoplasia, patent ductus arteriosus, and incomplete ossification of the skull. Overall, about half of newborns exposed to ACE inhibitors are adversely affected, leading to birth defects.\nACE inhibitors are ADEC pregnancy category D and should be avoided in women who are likely to become pregnant. In the U.S., ACE inhibitors must be labeled with a boxed warning concerning the risk of birth defects when taken during the second and third trimester. Their use in the first trimester is also associated with a risk of major congenital malformations, particularly affecting the cardiovascular and central nervous systems.\nOverdose.\nSymptoms and Treatment: There are few reports of ACE inhibitor overdose in the literature. The most likely manifestations are hypotension, which may be severe, hyperkalemia, hyponatremia and renal impairment with metabolic acidosis. Treatment should be mainly symptomatic and supportive, with volume expansion using normal saline to correct hypotension and improve renal function, and gastric lavage followed by activated charcoal and a cathartic to prevent further absorption of the drug. Captopril, enalapril, lisinopril and perindopril are known to be removable by hemodialysis.\nContraindications and precautions.\nThe ACE inhibitors are contraindicated in people with:\nACE inhibitors should be used with caution in people with:\nA combination of ACE inhibitor with other drugs may increase effects of these drugs, but also the risk of adverse effects. The commonly reported adverse effects of drug combination with ACE inhibitor are acute renal failure, hypotension, and hyperkalemia. The drugs interacting with ACE inhibitor should be prescribed with caution. Special attention should be given to combinations of ACE inhibitor with other RAAS blockers, diuretics (especially potassium-sparing diuretics), NSAIDs, anticoagulants, cyclosporine, DPP-4 inhibitors, and potassium supplements.\nPotassium supplementation should be used with caution and under medical supervision owing to the hyperkalemic effect of ACE inhibitors.\nConcomitant use with cyclooxygenase inhibitors tends to decrease ACE inhibitor's hypotensive effect.\nMechanism of action.\nACE inhibitors reduce the activity of the renin\u2013angiotensin\u2013aldosterone system (RAAS) as the primary etiologic (causal) event in the development of hypertension in people with diabetes mellitus, as part of the insulin-resistance syndrome or as a manifestation of renal disease.\nRenin\u2013angiotensin\u2013aldosterone system.\nThe renin\u2013angiotensin\u2013aldosterone system is a major blood pressure regulating mechanism. Markers of electrolyte and water imbalance in the body such as hypotension, low distal tubule sodium concentration, decreased blood volume and high sympathetic tone trigger the release of the enzyme renin from the cells of juxtaglomerular apparatus in the kidney.\nRenin activates a circulating liver derived prohormone angiotensinogen by proteolytic cleavage of all but its first ten amino acid residues known as angiotensin I. ACE (angiotensin converting enzyme) then removes a further two residues, converting angiotensin I into angiotensin II. ACE is found in the pulmonary circulation and in the endothelium of many blood vessels. The system increases blood pressure by increasing the amount of salt and water the body retains, although angiotensin II is also a potent vasoconstrictor.\nEffects.\nACE inhibitors block the conversion of angiotensin I (ATI) to angiotensin II (ATII). They thereby lower arteriolar resistance and increase venous capacity; decrease cardiac output, cardiac index, stroke work, and volume; lower resistance in blood vessels in the kidneys; and lead to increased natriuresis (excretion of sodium in the urine). Renin increases in concentration in the blood as a result of negative feedback of conversion of ATI to ATII. ATI increases for the same reason; ATII and aldosterone decrease. Bradykinin increases because of less inactivation by ACE.\nUnder normal conditions, angiotensin II has these effects:\nDuring the course of ACE inhibitor use, the production of ATII is decreased, which prevents aldosterone release from the adrenal cortex. This allows the kidney to excrete sodium ions along with obligate water, and retain potassium ions. This decreases blood volume, leading to decreased blood pressure.\nEpidemiological and clinical studies have shown ACE inhibitors reduce the progress of diabetic nephropathy independently from their blood pressure-lowering effect. This action of ACE inhibitors is used in the prevention of diabetic renal failure.\nACE inhibitors have been shown to be effective for indications other than hypertension even in patients with normal blood pressure. The use of a maximum dose of ACE inhibitors in such patients (including for prevention of diabetic nephropathy, congestive heart failure, and prophylaxis of cardiovascular events) is justified, because it improves clinical outcomes independently of the blood pressure-lowering effect of ACE inhibitors. Such therapy, of course, requires careful and gradual titration of the dose to prevent the effects of rapidly decreasing blood pressure (dizziness, fainting, etc.).\nACE inhibitors have also been shown to cause a central enhancement of parasympathetic nervous system activity in healthy volunteers and patients with heart failure. This action may reduce the prevalence of malignant cardiac arrhythmias, and the reduction in sudden death reported in large clinical trials.\nACE Inhibitors also reduce plasma norepinephrine levels, and its resulting vasoconstriction effects, in heart failure patients, thus breaking the vicious circles of sympathetic and renin angiotensin system activation, which sustains the downward spiral in cardiac function in congestive heart failure\nThe ACE inhibitor enalapril has also been shown to reduce cardiac cachexia in patients with chronic heart failure. Cachexia is a poor prognostic sign in patients with chronic heart failure.\nACE inhibitors are under early investigation for the treatment of frailty and muscle wasting (sarcopenia) in elderly patients without heart failure.\nExamples.\nCurrently, there are 10 ACE inhibitors approved for use in the United States by the FDA: captopril (1981), enalapril (1985), lisinopril (1987), benazepril (1991), fosinopril (1991), quinapril (1991), ramipril (1991), perindopril (1993), moexipril (1995) and trandolapril (1996).\nACE inhibitors are easily identifiable by their common suffix, '-pril'. ACE inhibitors can be divided into three groups based on their molecular structure of the enzyme binding sites (sulfhydryl, phosphinyl, carboxyl) to the active center of ACE:\nSulfhydryl-containing agents.\nThese agents appear to show antioxidative properties but may be involved in adverse events such as skin eruptions.\nDicarboxylate-containing agents.\nThis is the largest group, including:\nComparative information.\nAll ACE inhibitors have similar antihypertensive efficacy when equivalent doses are administered. The main differences lie with captopril, the first ACE inhibitor. Captopril has a shorter duration of action and an increased incidence of adverse effects. It is also the only ACE inhibitor capable of passing through the blood\u2013brain barrier, although the significance of this characteristic has not been shown to have any positive clinical effects.\nIn a large clinical study, one of the agents in the ACE inhibitor class, ramipril (Altace), demonstrated an ability to reduce the mortality rates of patients with a myocardial infarction and to slow the subsequent development of heart failure. This finding was made after it was discovered that regular use of ramipril reduced mortality rates even in test subjects who did not have hypertension.\nSome believe ramipril's additional benefits may be shared by some or all drugs in the ACE-inhibitor class. However, ramipril currently remains the only ACE inhibitor for which such effects are actually evidence-based.\nA meta-analysis confirmed that ACE inhibitors are effective and certainly the first-line choice in hypertension treatment. This meta-analysis was based on 20 trials and a cohort of 158,998 patients, of whom 91% were hypertensive. ACE inhibitors were used as the active treatment in seven trials (n=76,615) and angiotensin receptor blocker (ARB) in 13 trials (n=82,383).\nACE inhibitors were associated with a statistically significant 10% mortality reduction: (HR 0.90; 95% CI, 0.84\u20130.97; P=0.004). In contrast, no significant mortality reduction was observed with ARB treatment (HR 0.99; 95% CI, 0.94\u20131.04; P=0.683). Analysis of mortality reduction by different ACE inhibitors showed that perindopril-based regimens are associated with a statistically significant 13% all-cause mortality reduction.\nTaking into account the broad spectrum of the hypertensive population, one might expect that an effective treatment with ACE inhibitors, in particular with perindopril, would result in an important gain of lives saved.\nEquivalent doses in hypertension.\nThe ACE inhibitors have different strengths with different starting dosages. Dosage should be adjusted according to the clinical response.\nCombination with angiotensin II receptor antagonists.\nACE inhibitors possess many common characteristics with another class of cardiovascular drugs, angiotensin II receptor antagonists, which are often used when patients are intolerant of the adverse effects produced by ACE inhibitors. ACE inhibitors do not completely prevent the formation of angiotensin II, as blockage is dose-dependent, so angiotensin II receptor antagonists may be useful because they act to prevent the action of angiotensin II at the AT1 receptor, leaving AT2 receptor unblocked; the latter may have consequences needing further study.\nThe combination therapy of angiotensin II receptor antagonists with ACE inhibitors may be superior to either agent alone. This combination may increase levels of bradykinin while blocking the generation of angiotensin II and its activity at the AT1 receptor. This 'dual blockade' may be more effective than using an ACE inhibitor alone, because angiotensin II can be generated via non-ACE-dependent pathways. Preliminary studies suggest this combination of pharmacologic agents may be advantageous in the treatment of essential hypertension, chronic heart failure, and nephropathy. However, the more recent ONTARGET study showed no benefit of combining the agents and more adverse events. While statistically significant results have been obtained for its role in treating hypertension, clinical significance may be lacking. There are warnings about the combination of ACE inhibitors with ARBs.\nPatients with heart failure may benefit from the combination in terms of reducing morbidity and ventricular remodeling.\nThe most compelling evidence for the treatment of nephropathy has been found: This combination therapy partially reversed the proteinuria and also exhibited a renoprotective effect in patients with diabetic nephropathy, and pediatric IgA nephropathy.\nHistory.\nLeonard T. Skeggs and his colleagues (including Norman Shumway) discovered ACE in plasma in 1956. It was also noted that those who worked in banana plantations in South-western Brazil collapsed after being bitten by a pit viper, leading to a search for a blood pressure lowering component in its venom. Brazilian scientist S\u00e9rgio Henrique Ferreira reported a bradykinin-potentiating factor (BPF) present in the venom of \"Bothrops jararaca\", a South American pit viper, in 1965. Ferreira then went to John Vane's laboratory as a postdoctoral fellow with his already-isolated BPF. The conversion of the inactive angiotensin I to the potent angiotensin II was thought to take place in the plasma. However, in 1967, Kevin K. F. Ng and John R. Vane showed plasma ACE is too slow to account for the conversion of angiotensin I to angiotensin II \"in vivo\". Subsequent investigation showed rapid conversion occurs during its passage through the pulmonary circulation.\nBradykinin is rapidly inactivated in the circulating blood, and it disappears completely in a single pass through the pulmonary circulation. Angiotensin I also disappears in the pulmonary circulation because of its conversion to angiotensin II. Furthermore, angiotensin II passes through the lungs without any loss. The inactivation of bradykinin and the conversion of angiotensin I to angiotensin II in the lungs was thought to be caused by the same enzyme. In 1970, Ng and Vane, using BPF provided by Ferreira, showed the conversion is inhibited during its passage through the pulmonary circulation.\nBPFs are members of a family of peptides whose potentiating action is linked to inhibition of bradykinin by ACE. Molecular analysis of BPF yielded a nonapeptide BPF teprotide (SQ 20,881), which showed the greatest ACE inhibition potency and hypotensive effect \"in vivo\". Teprotide had limited clinical value as a result of its peptide nature and lack of activity when given orally. In the early 1970s, knowledge of the structure-activity relationship required for inhibition of ACE was growing. David Cushman, Miguel Ondetti and colleagues used peptide analogues to study the structure of ACE, using carboxypeptidase A as a model. Their discoveries led to the development of captopril, the first orally-active ACE inhibitor, in 1975.\nCaptopril was approved by the United States Food and Drug Administration in 1981. The first nonsulfhydryl-containing ACE inhibitor, enalapril, was approved four years later. At least 8 other ACE inhibitors have since been marketed.\nIn 1991, Japanese scientists created the first milk-based ACE inhibitor, in the form of a fermented milk drink, using specific cultures to liberate the tripeptide isoleucine-proline-proline (IPP) from the dairy protein. Valine-proline-proline (VPP) is also liberated in this process\u2014another milk tripeptide with a very similar chemical structure to IPP. Together, these peptides are now often referred to as lactotripeptides. In 1996, the first human study confirmed the blood pressure-lowering effect of IPP in fermented milk. Although twice the amount of VPP is needed to achieve the same ACE-inhibiting activity as the originally discovered IPP, VPP also is assumed to add to the total blood pressure lowering effect.\nSince the first lactotripeptides discovery, more than 20 human clinical trials have been conducted in many different countries."}
{"id": "2768", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2768", "title": "Antiarrhythmic medication", "text": ""}
{"id": "2769", "revid": "21112944", "url": "https://en.wikipedia.org/wiki?curid=2769", "title": "Antianginal", "text": "An antianginal is a drug used in the treatment of \"angina pectoris\", a symptom of ischaemic heart disease.\nMyocardial ischemia arises from the dysfunction of coronary macrovascular or microvascular components, leading to a compromised supply of oxygen and nutrients to the myocardium. The underlying pathophysiological mechanisms encompass a range of factors, including atherosclerosis in epicardial coronary arteries, vasospasm in large or small vessels, and microvascular dysfunction\u2014whose clinical significance is increasingly acknowledged. The diverse clinical presentations of myocardial ischemia collectively fall under the term chronic coronary syndromes.\nAddressing these conditions involves a multifaceted approach, where the most common antianginal medications alleviate symptoms by inducing coronary vasodilation and modifying the determinants of myocardial oxygen consumption, such as heart rate, myocardial wall stress, and ventricular contractility. Additionally, these medications can alter cardiac substrate metabolism to alleviate ischemia by enhancing the efficiency of myocardial oxygen utilization. While there is consensus on the prognostic importance of lifestyle interventions and preventive measures like aspirin and statin therapy, determining the optimal antianginal treatment for chronic coronary syndrome patients remains less defined.\nThe majority of individuals experiencing stable angina can effectively address their condition through lifestyle modifications, particularly by embracing smoking cessation and incorporating regular exercise into their routine. Alongside these lifestyle changes, the use of antianginal drugs is a common approach. However, findings from randomized controlled trials reveal that the efficacy of various antianginal drugs is comparable, with none demonstrating a significant reduction in mortality or the risk of myocardial infarction (MI). Despite this, prevailing guidelines lean towards recommending beta-blockers and calcium channel blockers as the preferred first-line treatment.\nThe European Society of Cardiology (ESC) guidelines for managing stable coronary artery disease provide well-defined classes of recommendation with corresponding levels of evidence. In a parallel vein, the National Institute for Health and Care Excellence (NICE) guidelines for stable angina management consider cost-effectiveness in their recommendations, designating terms such as first-line and second-line therapy. Notably, both sets of guidelines advocate for the use of low-dose aspirin and statins as disease-modifying agents.\nThis article aims to critically examine and evaluate the pharmacological recommendations outlined in these guidelines for the management of patients with stable angina. By delving into the nuances of these recommendations, we seek to provide a comprehensive understanding of the rationale behind the suggested pharmacological interventions for stable angina, shedding light on their respective strengths and considerations in clinical practice.\nPolitical Considerations.\nThe 2019 guidelines from the European Society of Cardiology (ESC) advocate for a personalized approach in which antianginal medications are tailored to an individual patient's comorbidities and hemodynamic profile. It's noteworthy that, although antianginal medications do not improve survival, their effectiveness in symptom reduction significantly depends on the underlying mechanism of angina.\nKey considerations in antianginal therapies involve enhancing coronary vascular oxygen supply to the ischemic myocardium, reducing heart rate, myocardial work, and oxygen consumption, as well as optimizing the energetic efficiency of cardiomyocytes. Despite current guidelines recommending \u03b2-blockers and calcium-channel blockers as first-line therapy, there is a lack of evidence demonstrating their superiority over second-line therapies.\nIn this comprehensive review, it is crucial to emphasize that, thus far, neither drugs nor interventions that reduce ischemia have been shown to prolong survival in patients with chronic coronary syndromes.\nExamples.\nDrugs used are nitrates, beta blockers, or calcium channel blockers.\nNitrates.\nNitrates cause vasodilation of the venous capacitance vessels by stimulating the endothelium-derived relaxing factor (EDRF). Used to relieve both exertional and vasospastic angina by allowing venous pooling, reducing the pressure in the ventricles and so reducing wall tension and oxygen requirements in, the heart. Short-acting nitrates are used to abort angina attacks that have occurred, while longer-acting nitrates are used in the prophylactic management of the condition.\nAgents include glyceryl trinitrate (GTN), pentaerythritol tetranitrate, isosorbide dinitrate and isosorbide mononitrate.\nBeta blockers.\nBeta blockers are used in the prophylaxis of exertional angina by reducing the myocardial oxygen demand below the level that would provoke an angina attack.\nThey are contraindicated in variant angina and can precipitate heart failure. They are also contraindicated in severe asthmatics due to bronchoconstriction, and should be used cautiously in diabetics as they can mask symptoms of hypoglycemia.\nAgents include either cardioselectives such as acebutolol or metoprolol, or non-cardioselectives such as oxprenolol or sotalol.\nCalcium channel blockers.\nCalcium ion (Ca++) antagonists (Calcium channel blockers) are used in the treatment of chronic stable angina, and most effectively in the treatment of variant angina (directly preventing coronary artery vasospasm). They are not used in the treatment of unstable angina .\nIn vitro, they dilate the coronary and peripheral arteries and have negative inotropic and chronotropic effects - decreasing afterload, improving myocardial efficiency, reducing heart rate and improving coronary blood flow.\n\"In vivo\", the vasodilation and hypotension trigger the baroreceptor reflex. Therefore, the net effect is the interplay of direct and reflex actions.\nExamples include Class I agents (\"e.g.\", verapamil), Class II agents (\"e.g.\", amlodipine, nifedipine), or the Class III agent diltiazem.\nNifedipine is more a potent vasodilator and more effective in angina. It is in the class of dihydropyridines and does not affect refractory period on SA node conduction."}
{"id": "2770", "revid": "8524693", "url": "https://en.wikipedia.org/wiki?curid=2770", "title": "Anatomical Therapeutic Chemical Classification System", "text": "The Anatomical Therapeutic Chemical (ATC) Classification System is a drug classification system that classifies the active ingredients of drugs according to the organ or system on which they act and their therapeutic, pharmacological and chemical properties. Its purpose is an aid to monitor drug use and for research to improve quality medication use. It does not imply drug recommendation or efficacy. It is controlled by the World Health Organization Collaborating Centre for Drug Statistics Methodology (WHOCC), and was first published in 1976.\nCoding system.\nThis pharmaceutical coding system divides drugs into different groups according to the organ or system on which they act, their therapeutic intent or nature, and the drug's chemical characteristics. Different brands share the same code if they have the same active substance and indications. Each bottom-level ATC code stands for a pharmaceutically used substance, or a combination of substances, in a single indication (or use). This means that one drug can have more than one code, for example acetylsalicylic acid (aspirin) has as a drug for local oral treatment, as a platelet inhibitor, and as an analgesic and antipyretic; as well as one code can represent more than one active ingredient, for example is the combination of perindopril with amlodipine, two active ingredients that have their own codes ( and respectively) when prescribed alone.\nThe ATC classification system is a strict hierarchy, meaning that each code necessarily has one and only one parent code, except for the 14 codes at the topmost level which have no parents. The codes are semantic identifiers, meaning they depict information by themselves beyond serving as identifiers (namely, the codes depict themselves the complete lineage of parenthood). As of 7 May 2020, there are 6,331 codes in ATC; the table below gives the count per level.\nHistory.\nThe ATC system is based on the earlier Anatomical Classification System, which is intended as a tool for the pharmaceutical industry to classify pharmaceutical products (as opposed to their active ingredients). This system, confusingly also called ATC, was initiated in 1971 by the European Pharmaceutical Market Research Association (EphMRA) and is being maintained by the EphMRA and Intellus. Its codes are organised into four levels. The WHO's system, having five levels, is an extension and modification of the EphMRA's. It was first published in 1976.\nClassification.\nIn this system, drugs are classified into groups at five different levels:\nFirst level.\nThe first level of the code indicates the anatomical main group and consists of one letter. There are 14 main groups:\n\"Example\": C Cardiovascular system\nSecond level.\nThe second level of the code indicates the therapeutic subgroup and consists of two digits.\n\"Example\": C03 Diuretics\nThird level.\nThe third level of the code indicates the therapeutic/pharmacological subgroup and consists of one letter.\n\"Example\": C03C High-ceiling diuretics\nFourth level.\nThe fourth level of the code indicates the chemical/therapeutic/pharmacological subgroup and consists of one letter.\n\"Example\": C03CA Sulfonamides\nFifth level.\nThe fifth level of the code indicates the chemical substance and consists of two digits.\n\"Example\": C03CA01 furosemide\nOther ATC classification systems.\nATCvet.\nThe \"Anatomical Therapeutic Chemical Classification System for veterinary medicinal products\" (ATCvet) is used to classify veterinary drugs. ATCvet codes can be created by placing the letter Q in front of the ATC code of most human medications. For example, furosemide for veterinary use has the code QC03CA01.\nSome codes are used exclusively for veterinary drugs, such as \"QI Immunologicals\", \"QJ51 Antibacterials for intramammary use\" or \"QN05AX90 amperozide\".\nHerbal ATC (HATC).\nThe Herbal ATC system (HATC) is an ATC classification of herbal substances; it differs from the regular ATC system by using 4 digits instead of 2 at the 5th level group.\nThe herbal classification is not adopted by WHO. The Uppsala Monitoring Centre is responsible for the Herbal ATC classification, and it is part of the WHODrug Global portfolio available by subscription.\nDefined daily dose.\nThe ATC system also includes defined daily doses (DDDs) for many drugs. This is a measurement of drug consumption based on the usual daily dose for a given drug. According to the definition, \"[t]he DDD is the assumed average maintenance dose per day for a drug used for its main indication in adults.\"\nAdaptations and updates.\nNational issues of the ATC classification, such as the German \"Anatomisch-therapeutisch-chemische Klassifikation mit Tagesdosen\", may include additional codes and DDDs not present in the WHO version.\nATC follows guidelines in creating new codes for newly approved drugs. An application is submitted to WHO for ATC classification and DDD assignment. A preliminary or temporary code is assigned and published on the website and in the \"WHO Drug Information\" for comment or objection. New ATC/DDD codes are discussed at the semi-annual Working Group meeting. If accepted it becomes a final decision and published semi-annually on the website and \"WHO Drug Information\" and implemented in the annual print/on-line ACT/DDD Index on January 1. \nChanges to existing ATC/DDD follow a similar process to become temporary codes and if accepted become a final decision as ATC/DDD alterations. ATC and DDD alterations are only valid and implemented in the coming annual updates; the original codes must continue until the end of the year. An updated version of the complete on-line/print ATC index with DDDs is published annually on January 1."}
{"id": "2771", "revid": "44690822", "url": "https://en.wikipedia.org/wiki?curid=2771", "title": "Air conditioner", "text": ""}
{"id": "2774", "revid": "6917124", "url": "https://en.wikipedia.org/wiki?curid=2774", "title": "Alfred C. Kinsey", "text": ""}
{"id": "2775", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2775", "title": "Automobile racing", "text": ""}
{"id": "2776", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2776", "title": "Anti-Semite", "text": ""}
{"id": "2778", "revid": "48498697", "url": "https://en.wikipedia.org/wiki?curid=2778", "title": "Parallel ATA", "text": "Parallel ATA (PATA), originally , also known as Integrated Drive Electronics (IDE), is a standard interface designed for IBM PC-compatible computers. It was first developed by Western Digital and Compaq in 1986 for compatible hard drives and CD or DVD drives. The connection is used for storage devices such as hard disk drives, floppy disk drives, optical disc drives, and tape drives in computers.\nThe standard is maintained by the X3/INCITS committee. It uses the underlying (ATA) and Packet Interface (ATAPI) standards.\nThe Parallel ATA standard is the result of a long history of incremental technical development, which began with the original AT Attachment interface, developed for use in early PC AT equipment. The ATA interface itself evolved in several stages from Western Digital's original Integrated Drive Electronics (IDE) interface. As a result, many near-synonyms for ATA/ATAPI and its previous incarnations are still in common informal use, in particular Extended IDE (EIDE) and Ultra ATA (UATA). After the introduction of SATA in 2003, the original ATA was renamed to Parallel ATA, or PATA for short.\nParallel ATA cables have a maximum allowable length of . Because of this limit, the technology normally appears as an internal computer storage interface. For many years, ATA provided the most common and the least expensive interface for this application. It has largely been replaced by SATA in newer systems.\nHistory and terminology.\nThe standard was originally conceived as the \"AT Bus Attachment\", officially called \"AT Attachment\" and abbreviated \"ATA\" because its primary feature was a direct connection to the 16-bit ISA bus introduced with the IBM PC/AT. The original ATA specifications published by the standards committees use the name \"AT Attachment\". The \"AT\" in the IBM PC/AT referred to \"Advanced Technology\" so ATA has also been referred to as \"Advanced Technology Attachment\". When a newer Serial ATA (SATA) was introduced in 2003, the original ATA was renamed to Parallel ATA, or PATA for short.\nPhysical ATA interfaces became a standard component in all PCs, initially on host bus adapters, sometimes on a sound card but ultimately as two physical interfaces embedded in a Southbridge chip on a motherboard. Called the \"primary\" and \"secondary\" ATA interfaces, they were assigned to base addresses 0x1F0 and 0x170 on ISA bus systems. They were replaced by SATA interfaces.\nIDE and ATA-1.\nThe first version of what is now called the ATA/ATAPI interface was developed by Western Digital under the name \"Integrated Drive Electronics\" (IDE). Together with Compaq (the initial customer), they worked with various disk drive manufacturers to develop and ship early products with the goal of remaining software compatible with the existing IBM PC hard drive interface. The first such drives appeared internally in Compaq PCs in 1986 and were first separately offered by Conner Peripherals as the CP342 in June 1987.\nThe term \"Integrated Drive Electronics\" refers to the drive controller being integrated into the drive, as opposed to a separate controller situated at the other side of the connection cable to the drive. On an IBM PC compatible, CP/M machine, or similar, this was typically a card installed on a motherboard. The interface cards used to connect a parallel ATA drive to, for example, an ISA Slot, are not drive controllers: they are merely bridges between the host bus and the ATA interface. Since the original ATA interface is essentially just a 16-bit ISA bus, the bridge was especially simple in case of an ATA connector being located on an ISA interface card. The integrated controller presented the drive to the host computer as an array of 512-byte blocks with a relatively simple command interface. This relieved the mainboard and interface cards in the host computer of the chores of stepping the disk head arm, moving the head arm in and out, and so on, as had to be done with earlier ST-506 and ESDI hard drives. All of these low-level details of the mechanical operation of the drive were now handled by the controller on the drive itself. This also eliminated the need to design a single controller that could handle many different types of drives, since the controller could be unique for the drive. The host need only to ask for a particular sector, or block, to be read or written, and either accept the data from the drive or send the data to it.\nThe interface used by these drives was standardized in 1994 as ANSI standard X3.221-1994, \"AT Attachment Interface for Disk Drives\". After later versions of the standard were developed, this became known as \"ATA-1\".\nA short-lived, seldom-used implementation of ATA was created for the IBM XT and similar machines that used the 8-bit version of the ISA bus. It has been referred to as \"XT-IDE\", \"XTA\" or \"XT Attachment\".\nEIDE and ATA-2.\nIn 1994, about the same time that the ATA-1 standard was adopted, Western Digital introduced drives under a newer name, Enhanced IDE (EIDE). These included most of the features of the forthcoming ATA-2 specification and several additional enhancements. Other manufacturers introduced their own variations of ATA-1 such as \"Fast ATA\" and \"Fast ATA-2\".\nThe new version of the ANSI standard, \"AT Attachment Interface with Extensions ATA-2\" (X3.279-1996), was approved in 1996. It included most of the features of the manufacturer-specific variants.\nATA-2 also was the first to note that devices other than hard drives could be attached to the interface:\nATAPI.\nATA was originally designed for, and worked only with, hard disk drives and devices that could emulate them. The introduction of ATAPI (ATA Packet Interface) by a group called the Small Form Factor committee (SFF) allowed ATA to be used for a variety of other devices that require functions beyond those necessary for hard disk drives. For example, any removable media device needs a \"media eject\" command, and a way for the host to determine whether the media is present, and these were not provided in the ATA protocol.\nATAPI is a protocol allowing the ATA interface to carry SCSI commands and responses; therefore, all ATAPI devices are actually \"speaking SCSI\" other than at the electrical interface. The SCSI commands and responses are embedded in \"packets\" (hence \"ATA Packet Interface\") for transmission on the ATA cable. This allows any device class for which a SCSI command set has been defined to be interfaced via ATA/ATAPI.\nATAPI devices are also \"speaking ATA\", as the ATA physical interface and protocol are still being used to send the packets. On the other hand, ATA hard drives and solid state drives do not use ATAPI.\nATAPI devices include CD-ROM and DVD-ROM drives, tape drives, and large-capacity floppy drives such as the Zip drive and SuperDisk drive. Some early ATAPI devices were simply SCSI devices with an ATA/ATAPI to SCSI protocol converter added on.\nThe SCSI commands and responses used by each class of ATAPI device (CD-ROM, tape, etc.) are described in other documents or specifications specific to those device classes and are not within ATA/ATAPI or the T13 committee's purview. One commonly used set is defined in the MMC SCSI command set.\nATAPI was adopted as part of ATA in INCITS 317-1998, \"AT Attachment with Packet Interface Extension (ATA/ATAPI-4)\".\nUDMA and ATA-4.\nThe ATA/ATAPI-4 standard also introduced several \"Ultra DMA\" transfer modes. These initially supported speeds from 16 to 33\u00a0MB/s. In later versions, faster Ultra DMA modes were added, requiring new 80-wire cables to reduce crosstalk. The latest versions of Parallel ATA support up to 133\u00a0MB/s.\nUltra ATA.\nUltra ATA, abbreviated UATA, is a designation that has been primarily used by Western Digital for different speed enhancements to the ATA/ATAPI standards. For example, in 2000 Western Digital published a document describing \"Ultra ATA/100\", which brought performance improvements for the then-current ATA/ATAPI-5 standard by improving maximum speed of the Parallel ATA interface from 66 to 100\u00a0MB/s. Most of Western Digital's changes, along with others, were included in the ATA/ATAPI-6 standard (2002).\nx86 BIOS size limitations.\nInitially, the size of an ATA drive was stored in the system x86 BIOS using a type number (1 through 45) that predefined the C/H/S parameters and also often the landing zone, in which the drive heads are parked while not in use. Later, a \"user definable\" format called C/H/S or cylinders, heads, sectors was made available. These numbers were important for the earlier ST-506 interface, but were generally meaningless for ATA\u2014the CHS parameters for later ATA large drives often specified impossibly high numbers of heads or sectors that did not actually define the internal physical layout of the drive at all. From the start, and up to ATA-2, every user had to specify explicitly how large every attached drive was. From ATA-2 on, an \"identify drive\" command was implemented that can be sent and which will return all drive parameters.\nOwing to a lack of foresight by motherboard manufacturers, the system BIOS was often hobbled by artificial C/H/S size limitations due to the manufacturer assuming certain values would never exceed a particular numerical maximum.\nThe first of these BIOS limits occurred when ATA drives reached sizes in excess of 504 MiB, because some motherboard BIOSes would not allow C/H/S values above 1024 cylinders, 16 heads, and 63 sectors. Multiplied by 512 bytes per sector, this totals bytes which, divided by bytes per MiB, equals 504 MiB (528 MB).\nThe second of these BIOS limitations occurred at 1024 cylinders, 256 heads, and 63 sectors, and a problem in MS-DOS limited the number of heads to 255. This totals to bytes (8032.5 MiB), commonly referred to as the 8.4 gigabyte barrier. This is again a limit imposed by x86 BIOSes, and not a limit imposed by the ATA interface.\nIt was eventually determined that these size limitations could be overridden with a small program loaded at startup from a hard drive's boot sector. Some hard drive manufacturers, such as Western Digital, started including these override utilities with large hard drives to help overcome these problems. However, if the computer was booted in some other manner without loading the special utility, the invalid BIOS settings would be used and the drive could either be inaccessible or appear to the operating system to be damaged.\nLater, an extension to the x86 BIOS disk services called the \"Enhanced Disk Drive\" (EDD) was made available, which makes it possible to address drives as large as 264 sectors.\nInterface size limitations.\nThe first drive interface used 22-bit addressing mode which resulted in a maximum drive capacity of two gigabytes. Later, the first formalized ATA specification used a 28-bit addressing mode through LBA28, allowing for the addressing of 228 () sectors (blocks) of 512 bytes each, resulting in a maximum capacity of 128\u00a0GiB (137\u00a0GB).\nATA-6 introduced 48-bit addressing, increasing the limit to 128 PiB (144 PB). As a consequence, any ATA drive of capacity larger than about 137\u00a0GB must be an ATA-6 or later drive. Connecting such a drive to a host with an ATA-5 or earlier interface will limit the usable capacity to the maximum of the interface.\nSome operating systems, including Windows XP pre-SP1, and Windows 2000 pre-SP3, disable LBA48 by default, requiring the user to take extra steps to use the entire capacity of an ATA drive larger than about 137 gigabytes.\nOlder operating systems, such as Windows 98, do not support 48-bit LBA at all. However, members of the third-party group MSFN have modified the Windows 98 disk drivers to add unofficial support for 48-bit LBA to Windows 95 OSR2, Windows 98, Windows 98 SE and Windows ME.\nSome 16-bit and 32-bit operating systems supporting LBA48 may still not support disks larger than 2 TiB due to using 32-bit arithmetic only; a limitation also applying to many boot sectors.\nPrimacy and obsolescence.\nParallel ATA (then simply called ATA or IDE) became the primary storage device interface for PCs soon after its introduction. In some systems, a third and fourth motherboard interface was provided, allowing up to eight ATA devices to be attached to the motherboard. Often, these additional connectors were implemented by inexpensive RAID controllers.\nSoon after the introduction of Serial ATA (SATA) in 2003, use of Parallel ATA declined. Some PCs and laptops of the era have a SATA hard disk and an optical drive connected to PATA.\nAs of 2007, some PC chipsets, for example the Intel ICH10, had removed support for PATA. Motherboard vendors still wishing to offer Parallel ATA with those chipsets must include an additional interface chip. In more recent computers, the Parallel ATA interface is rarely used even if present, as four or more Serial ATA connectors are usually provided on the motherboard and SATA devices of all types are common.\nWith Western Digital's withdrawal from the PATA market, hard disk drives with the PATA interface were no longer in production after December 2013 for other than specialty applications.\nInterface.\nParallel ATA cables transfer data 16 bits at a time. The traditional cable uses 40-pin female insulation displacement connectors (IDC) attached to a 40- or 80-conductor ribbon cable. Each cable has two or three connectors, one of which plugs into a host adapter interfacing with the rest of the computer system. The remaining connector(s) plug into storage devices, most commonly hard disk drives or optical drives. Each connector has 39 physical pins arranged into two rows (2.54\u00a0mm, -inch pitch), with a gap or key at pin 20. Earlier connectors may not have that gap, with all 40 pins available. Thus, later cables with the gap filled in are incompatible with earlier connectors, although earlier cables are compatible with later connectors.\nRound parallel ATA cables (as opposed to ribbon cables) were eventually made available for 'case modders' for cosmetic reasons, as well as claims of improved computer cooling and were easier to handle; however, only ribbon cables are supported by the ATA specifications.\n44-pin variant.\nA 44-pin variant PATA connector is used for 2.5 inch drives inside laptops. The pins are closer together (2.0\u00a0mm pitch) and the connector is physically smaller than the 40-pin connector. The extra pins carry power.\n80-conductor variant.\nATA's cables have had 40 conductors for most of its history (44 conductors for the smaller form-factor version used for 2.5\" drives\u2014the extra four for power), but an 80-conductor version appeared with the introduction of the \"UDMA/66\" mode. All of the additional conductors in the new cable are grounds, interleaved with the signal conductors to reduce the effects of capacitive coupling between neighboring signal conductors, reducing crosstalk. Capacitive coupling is more of a problem at higher transfer rates, and this change was necessary to enable the 66 megabytes per second (MB/s) transfer rate of \"UDMA4\" to work reliably. The faster \"UDMA5\" and \"UDMA6\" modes also require 80-conductor cables.\nThough the number of conductors doubled, the number of connector pins and the pinout remain the same as 40-conductor cables, and the external appearance of the connectors is identical. Internally, the connectors are different; the connectors for the 80-conductor cable connect a larger number of ground conductors to the ground pins, while the connectors for the 40-conductor cable connect ground conductors to ground pins one-to-one. 80-conductor cables usually come with three differently colored connectors (blue, black, and gray for controller, master drive, and slave drive respectively) as opposed to uniformly colored 40-conductor cable's connectors (commonly all gray). The gray connector on 80-conductor cables has pin 28 CSEL not connected, making it the slave position for drives configured cable select.\nMultiple devices on a cable.\nIf two devices are attached to a single cable, one must be designated as \"Device 0\" (in the past, commonly designated \"master\") and the other as \"Device 1\" (in the past, commonly designated as \"slave\"). This distinction is necessary to allow both drives to share the cable without conflict. The \"Device 0\" drive is the drive that usually appears \"first\" to the computer's BIOS and/or operating system. In most personal computers the drives are often designated as \"C:\" for the \"Device 0\" and \"D:\" for the \"Device 1\" referring to one active primary partitions on each.\nThe mode that a device must use is often set by a jumper setting on the device itself, which must be manually set to \"Device 0\" (\"Master\") or \"Device 1\" (\"Slave\"). If there is a single device on a cable, it should be configured as \"Device 0\". However, some certain era drives have a special setting called \"Single\" for this configuration (Western Digital, in particular). Also, depending on the hardware and software available, a \"Single\" drive on a cable will often work reliably even though configured as the \"Device 1\" drive (most often seen where an optical drive is the only device on the secondary ATA interface).\nThe words \"primary\" and \"secondary\" typically refers to the two IDE cables, which can have two drives each (primary master, primary slave, secondary master, secondary slave).\nThere are many debates about how much a slow device can impact the performance of a faster device on the same cable. On early ATA host adapters, both devices' data transfers can be constrained to the speed of the slower device, if two devices of different speed capabilities are on the same cable. For all modern ATA host adapters, this is not true, as modern ATA host adapters support \"independent device timing\". This allows each device on the cable to transfer data at its own best speed. Even with earlier adapters without independent timing, this effect applies only to the data transfer phase of a read or write operation. This is caused by the omission of both overlapped and queued feature sets from most parallel ATA products. Only one device on a cable can perform a read or write operation at one time; therefore, a fast device on the same cable as a slow device under heavy use will find it has to wait for the slow device to complete its task first. However, most modern devices will report write operations as complete once the data is stored in their onboard cache memory, before the data is written to the (slow) magnetic storage. This allows commands to be sent to the other device on the cable, reducing the impact of the \"one operation at a time\" limit. The impact of this on a system's performance depends on the application. For example, when copying data from an optical drive to a hard drive (such as during software installation), this effect probably will not matter. Such jobs are necessarily limited by the speed of the optical drive no matter where it is. But if the hard drive in question is also expected to provide good throughput for other tasks at the same time, it probably should not be on the same cable as the optical drive.\nCable select.\nA drive mode called \"cable select\" was described as optional in ATA-1 and has come into fairly widespread use with ATA-5 and later. A drive set to \"cable select\" automatically configures itself as \"Device 0\" or \"Device 1\", according to its position on the cable. Cable select is controlled by pin 28. The host adapter grounds this pin; if a device sees that the pin is grounded, it becomes the \"Device 0\" (master) device; if it sees that pin 28 is open, the device becomes the \"Device 1\" (slave) device.\nThis setting is usually chosen by a jumper setting on the drive called \"cable select\", usually marked \"CS\", which is separate from the \"Device 0/1\" setting.\nIf two drives are configured as \"Device 0\" and \"Device 1\" manually, this configuration does not need to correspond to their position on the cable. Pin 28 is only used to let the drives know their position on the cable; it is not used by the host when communicating with the drives. In other words, the manual master/slave setting using jumpers on the drives takes precedence and allows them to be freely placed on either connector of the ribbon cable.\nWith the 40-conductor cable, it was very common to implement cable select by simply cutting the pin 28 wire between the two device connectors; putting the slave \"Device 1\" device at the end of the cable, and the master \"Device 0\" on the middle connector. This arrangement eventually was standardized in later versions. However, it had one drawback: if there is just one master device on a 2-drive cable, using the middle connector, this results in an unused stub of cable, which is undesirable for physical convenience and electrical reasons. The stub causes signal reflections, particularly at higher transfer rates.\nStarting with the 80-conductor cable defined for use in ATAPI5/UDMA4, the master \"Device 0\" device goes at the far-from-the-host end of the cable on the black connector, the slave \"Device 1\" goes on the grey middle connector, and the blue connector goes to the host (e.g. motherboard IDE connector, or IDE card). So, if there is only one (\"Device 0\") device on a two-drive cable, using the black connector, there is no cable stub to cause reflections (the unused connector is now in the middle of the ribbon). Also, cable select is now implemented in the grey middle device connector, usually simply by omitting the pin 28 contact from the connector body.\nSerialized, overlapped, and queued operations.\nThe parallel ATA protocols up through ATA-3 require that once a command has been given on an ATA interface, it must complete before any subsequent command may be given. Operations on the devices must be serializedwith only one operation in progress at a timewith respect to the ATA host interface. A useful mental model is that the host ATA interface is busy with the first request for its entire duration, and therefore can not be told about another request until the first one is complete. The function of serializing requests to the interface is usually performed by a device driver in the host operating system.\nThe ATA-4 and subsequent versions of the specification have included an \"overlapped feature set\" and a \"queued feature set\" as optional features, both being given the name \"Tagged Command Queuing\" (TCQ), a reference to a set of features from SCSI which the ATA version attempts to emulate. However, support for these is extremely rare in actual parallel ATA products and device drivers because these feature sets were implemented in such a way as to maintain software compatibility with its heritage as originally an extension of the ISA bus. This implementation resulted in excessive CPU utilization which largely negated the advantages of command queuing. By contrast, overlapped and queued operations have been common in other storage buses; in particular, SCSI's version of tagged command queuing had no need to be compatible with APIs designed for ISA, allowing it to attain high performance with low overhead on buses which supported first party DMA like PCI. This has long been seen as a major advantage of SCSI.\nThe Serial ATA standard has supported native command queueing (NCQ) since its first release, but it is an optional feature for both host adapters and target devices. Many obsolete PC motherboards do not support NCQ, but modern SATA hard disk drives and SATA solid-state drives usually support NCQ, which is not the case for removable (CD/DVD) drives because the ATAPI command set used to control them prohibits queued operations.\nHDD passwords and security.\nATA devices may support an optional security feature which is defined in an ATA specification, and thus not specific to any brand or device. The security feature can be enabled and disabled by sending special ATA commands to the drive. If a device is locked, it will refuse all access until it is unlocked. A device can have two passwords: A User Password and a Master Password; either or both may be set. There is a Master Password identifier feature which, if supported and used, can identify the current Master Password (without disclosing it). The master password, if set, can used by the administrator to reset user password, if the end user forgot the user password. On some laptops and some business computers, their BIOS can control the ATA passwords.\nA device can be locked in two modes: High security mode or Maximum security mode. Bit 8 in word 128 of the IDENTIFY response shows which mode the disk is in: 0 = High, 1 = Maximum. In High security mode, the device can be unlocked with either the User or Master password, using the \"SECURITY UNLOCK DEVICE\" ATA command. There is an attempt limit, normally set to 5, after which the disk must be power cycled or hard-reset before unlocking can be attempted again. Also in High security mode, the SECURITY ERASE UNIT command can be used with either the User or Master password. In Maximum security mode, the device can be unlocked only with the User password. If the User password is not available, the only remaining way to get at least the bare hardware back to a usable state is to issue the SECURITY ERASE PREPARE command, immediately followed by SECURITY ERASE UNIT. In Maximum security mode, the SECURITY ERASE UNIT command requires the Master password and will completely erase all data on the disk. Word 89 in the IDENTIFY response indicates how long the operation will take. While the ATA lock is intended to be impossible to defeat without a valid password, there are purported workarounds to unlock a device.\nFor NVMe drives, the security features, including lock passwords, were defined in the OPAL standard.\nFor sanitizing entire disks, the built-in Secure Erase command is effective when implemented correctly. There have been a few reported instances of failures to erase some or all data. On some laptops and some business computers, their BIOS can utilize Secure Erase to erase all data of the disk.\nExternal parallel ATA devices.\nDue to a short cable length specification and shielding issues it is extremely uncommon to find external PATA devices that directly use PATA for connection to a computer. A device connected externally needs additional cable length to form a U-shaped bend so that the external device may be placed alongside, or on top of the computer case, and the standard cable length is too short to permit this. For ease of reach from motherboard to device, the connectors tend to be positioned towards the front edge of motherboards, for connection to devices protruding from the front of the computer case. This front-edge position makes extension out the back to an external device even more difficult. Ribbon cables are poorly shielded, and the standard relies upon the cabling to be installed inside a shielded computer case to meet RF emissions limits.\nExternal hard disk drives or optical disk drives that have an internal PATA interface, use some other interface technology to bridge the distance between the external device and the computer. USB is the most common external interface, followed by Firewire. A bridge chip inside the external devices converts from the USB interface to PATA, and typically only supports a single external device without cable select or master/slave.\nSpecifications.\nThe following table shows the names of the versions of the ATA standards and the transfer modes and rates supported by each. Note that the transfer rate for each mode (for example, 66.7\u00a0MB/s for UDMA4, commonly called \"Ultra-DMA 66\", defined by ATA-5) gives its maximum theoretical transfer rate on the cable. This is simply two bytes multiplied by the effective clock rate, and presumes that every clock cycle is used to transfer end-user data. In practice, of course, protocol overhead reduces this value.\nCongestion on the host bus to which the ATA adapter is attached may also limit the maximum burst transfer rate. For example, the maximum data transfer rate for conventional PCI bus is 133\u00a0MB/s, and this is shared among all active devices on the bus.\nIn addition, no ATA hard drives existed in 2005 that were capable of measured sustained transfer rates of above 80\u00a0MB/s. Furthermore, sustained transfer rate tests do not give realistic throughput expectations for most workloads: They use I/O loads specifically designed to encounter almost no delays from seek time or rotational latency. Hard drive performance under most workloads is limited first and second by those two factors; the transfer rate on the bus is a distant third in importance. Therefore, transfer speed limits above 66\u00a0MB/s really affect performance only when the hard drive can satisfy all I/O requests by reading from its internal cache\u2014a very unusual situation, especially considering that such data is usually already buffered by the operating system.\n, mechanical hard disk drives can transfer data at up to 524\u00a0MB/s, which is far beyond the capabilities of the PATA/133 specification. High-performance solid state drives can transfer data at up to 7000\u20137500\u00a0MB/s.\nOnly the Ultra DMA modes use CRC to detect errors in data transfer between the controller and drive. This is a 16-bit CRC, and it is used for data blocks only. Transmission of command and status blocks do not use the fast signaling methods that would necessitate CRC. For comparison, in Serial ATA, 32-bit CRC is used for both commands and data.\nRelated standards, features, and proposals.\nATAPI Removable Media Device (ARMD).\nATAPI devices with removable media, other than CD and DVD drives, are classified as ARMD (ATAPI Removable Media Device) and can appear as either a super-floppy (non-partitioned media) or a hard drive (partitioned media) to the operating system. These can be supported as bootable devices by a BIOS complying with the ATAPI Removable Media Device BIOS Specification, originally developed by Compaq Computer Corporation and Phoenix Technologies. It specifies provisions in the BIOS of a personal computer to allow the computer to be bootstrapped from devices such as Zip drives, Jaz drives, SuperDisk (LS-120) drives, and similar devices.\nThese devices have removable media like floppy disk drives, but capacities more commensurate with hard drives, and programming requirements unlike either. Due to limitations in the floppy controller interface most of these devices were ATAPI devices, connected to one of the host computer's ATA interfaces, similarly to a hard drive or CD-ROM device. However, existing BIOS standards did not support these devices. An ARMD-compliant BIOS allows these devices to be booted from and used under the operating system without requiring device-specific code in the OS.\nA BIOS implementing ARMD allows the user to include ARMD devices in the boot search order. Usually an ARMD device is configured earlier in the boot order than the hard drive. Similarly to a floppy drive, if bootable media is present in the ARMD drive, the BIOS will boot from it; if not, the BIOS will continue in the search order, usually with the hard drive last.\nThere are two variants of ARMD, ARMD-FDD and ARMD-HDD. Originally ARMD caused the devices to appear as a sort of very large floppy drive, either the primary floppy drive device 00h or the secondary device 01h. Some operating systems required code changes to support floppy disks with capacities far larger than any standard floppy disk drive. Also, standard-floppy disk drive emulation proved to be unsuitable for certain high-capacity floppy disk drives such as Iomega Zip drives. Later the ARMD-HDD, ARMD-\"Hard disk device\", variant was developed to address these issues. Under ARMD-HDD, an ARMD device appears to the BIOS and the operating system as a hard drive.\nATA over Ethernet.\nIn August 2004, Sam Hopkins and Brantley Coile of Coraid specified a lightweight ATA over Ethernet protocol to carry ATA commands over Ethernet instead of directly connecting them to a PATA host adapter. This permitted the established block protocol to be reused in storage area network (SAN) applications.\nCompact Flash.\nCompact Flash in its \"IDE mode\" is essentially a miniaturized ATA interface, intended for use on devices that use flash memory storage. No interfacing chips or circuitry are required, other than to directly adapt the smaller CF socket onto the larger ATA connector. (Although most CF cards only support IDE mode up to PIO4, making them much slower in IDE mode than their CF capable speed)\nThe ATA connector specification does not include pins for supplying power to a CF device, so power is inserted into the connector from a separate source. The exception to this is when the CF device is connected to a 44-pin ATA bus designed for 2.5-inch hard disk drives, commonly found in notebook computers, as this bus implementation must provide power to a standard hard disk drive.\nCF devices can be designated as devices 0 or 1 on an ATA interface, though since most CF devices offer only a single socket, it is not necessary to offer this selection to end users. Although CF can be hot-pluggable with additional design methods, by default when wired directly to an ATA interface, it is not intended to be hot-pluggable."}
{"id": "2779", "revid": "96647", "url": "https://en.wikipedia.org/wiki?curid=2779", "title": "Atari 2600", "text": "The Atari 2600 is a home video game console developed and produced by Atari, Inc. Released in September 1977 as the Atari Video Computer System (Atari VCS), it popularized microprocessor-based hardware and games stored on swappable ROM cartridges, a format first used with the Fairchild Channel F in 1976. The VCS was bundled with two joystick controllers, a conjoined pair of paddle controllers, and a game cartridgeinitially \"Combat\" and later \"Pac-Man\". Sears sold the system as the Tele-Games Video Arcade. Atari rebranded the VCS as the Atari 2600 in November 1982, alongside the release of the Atari 5200.\nAtari was successful at creating arcade video games, but their development cost and limited lifespan drove CEO Nolan Bushnell to seek a programmable home system. The first inexpensive microprocessors from MOS Technology in late 1975 made this feasible. The console was prototyped under the codename Stella by Atari subsidiary Cyan Engineering. Lacking funding to complete the project, Bushnell sold Atari to Warner Communications in 1976.\nThe Atari VCS launched in 1977 with nine games on 2 KB cartridges. Atari ported many of their arcade games to the system, and the VCS versions of \"Breakout\" and \"Night Driver\" are in color while the arcade originals have monochrome graphics. The system's first killer application was the home conversion of Taito's \"Space Invaders\" in 1980. \"Adventure\", also released in 1980, was one of the first action-adventure video games and contains the first widely recognized Easter egg. Beginning with the VCS version of \"Asteroids\" in 1980, many games used bank switching to allow 8 KB or larger cartridges. By the time of the system's peak in 1982-3, games were released with significantly more advanced visuals and gameplay than the system was designed for, such as Activision's \"Pitfall!\". The popularity of the VCS led to the founding of Activision and other third-party game developers and competition from the Intellivision and, later, ColecoVision consoles. \nBy 1982, the Atari 2600 was the dominant game system in North America. Poor decisions by Atari management damaged both the system and company's reputation, most notably the release of two highly anticipated games for the 2600: a port of the arcade game \"Pac-Man\" and \"E.T. the Extra-Terrestrial\". \"Pac-Man\" became the 2600's bestselling game, but was panned for being inferior to the arcade version. \"E.T.\" was rushed to market for the holiday shopping season and was similarly disparaged. Both games, and a glut of third-party shovelware, were factors in ending Atari's relevance in the console market, contributing to the video game crash of 1983.\nWarner sold the assets of Atari's consumer electronics division to former Commodore CEO Jack Tramiel in 1984. In 1986, the new Atari Corporation under Tramiel released a revised, low-cost 2600 model, and the backward-compatible Atari 7800, but it was Nintendo that led the recovery of the industry with its 1985 launch of the Nintendo Entertainment System. Production of the Atari 2600 ended in 1992, with an estimated 30 million units sold across its lifetime.\nHistory.\nAtari, Inc. was founded by Nolan Bushnell and Ted Dabney in 1972. Its first major product was \"Pong\", released in 1972, the first successful coin-operated video game. While Atari continued to develop new arcade games in following years, \"Pong\" gave rise to a number of competitors to the growing arcade game market. The competition along with other missteps by Atari led to financial problems in 1974, though recovering by the end of the year. By 1975, Atari had released a \"Pong\" home console, competing against Magnavox, the only other major producer of home consoles at the time. Atari engineers recognized, however, the limitation of custom logic integrated onto the circuit board, permanently confining the whole console to only one game. The increasing competition increased the risk, as Atari had found with past arcade games and again with dedicated home consoles. Both platforms are built from integrating discrete electro-mechanical components into circuits, rather than programmed as on a mainframe computer. Therefore, development of a console had cost at least plus time to complete, but the final product only had about a three-month shelf life until becoming outdated by competition.\nBy 1974, Atari had acquired Cyan Engineering, a Grass Valley electronics company founded by Steve Mayer and Larry Emmons, both former colleagues of Bushnell and Dabney from Ampex, who helped to develop new ideas for Atari's arcade games. Even prior to the release of the home version of \"Pong\", Cyan's engineers, led by Mayer and Ron Milner, had envisioned a home console powered by new programmable microprocessors capable of playing Atari's current arcade offerings. The programmable microprocessors would make a console's design significantly simpler and more powerful than any dedicated single-game unit. However, the cost of such chips was far outside the range that their market would tolerate. Atari had opened negotiations to use Motorola's new 6800 in future systems.\nMOS Technology 6502/6507.\nIn September 1975, MOS Technology debuted the 6502 microprocessor for at the Wescon trade show in San Francisco. Mayer and Milner attended, and met with the leader of the team that created the chip, Chuck Peddle. They proposed using the 6502 in a game console, and offered to discuss it further at Cyan's facilities after the show.\nOver two days, MOS and Cyan engineers sketched out a 6502-based console design by Meyer and Milner's specifications. Financial models showed that even at , the 6502 would be too expensive, and Peddle offered them a planned 6507 microprocessor, a cost-reduced version of the 6502, and MOS's RIOT chip for input/output. Cyan and MOS negotiated the 6507 and RIOT chips at a pair. MOS also introduced Cyan to Microcomputer Associates, who had separately developed debugging software and hardware for MOS, and had developed the JOLT Computer for testing the 6502, which Peddle suggested would be useful for Atari and Cyan to use while developing their system. Milner was able to demonstrate a proof-of-concept for a programmable console by implementing \"Tank\", an arcade game by Atari's subsidiary Kee Games, on the JOLT.\nAs part of the deal, Atari wanted a second source of the chipset. Peddle and Paivinen suggested Synertek whose co-founder, Bob Schreiner, was a friend of Peddle. In October 1975, Atari informed the market that it was moving forward with MOS. The Motorola sales team had already told its management that the Atari deal was finalized, and Motorola management was livid. They announced a lawsuit against MOS the next week.\nBuilding the system.\nBy December 1975, Atari hired Joe Decuir, a recent graduate from University of California, Berkeley who had been doing his own testing on the 6502. Decuir began debugging the first prototype designed by Mayer and Milner, which gained the codename \"Stella\" after the brand of Decuir's bicycle. This prototype included a breadboard-level design of the graphics interface to build upon. A second prototype was completed by March 1976 with the help of Jay Miner, who created a chip called the Television Interface Adaptor (TIA) to send graphics and audio to a television. The second prototype included a TIA, a 6507, and a ROM cartridge slot and adapter.\nAs the TIA's design was refined, Al Alcorn brought in Atari's game developers to provide input on features. There are significant limitations in the 6507, the TIA, and other components, so the programmers creatively optimized their games to maximize the console. The console lacks a framebuffer and requires games to instruct the system to generate graphics in synchronization with the electron gun in the cathode-ray tube (CRT) as it scans across rows on the screen. The programmers found ways to \"race the beam\" to perform other functions while the electron gun scans outside of the visible screen.\nAlongside the electronics development, Bushnell brought in Gene Landrum, a consultant who had just prior consulted for Fairchild Camera and Instrument for its upcoming Channel F, to determine the consumer requirements for the console. In his final report, Landrum suggested a living room aesthetic, with a wood grain finish, and the cartridges must be \"idiot proof, child proof and effective in resisting potential static [electricity] problems in a living room environment\". Landrum recommended it include four to five dedicated games in addition to the cartridges, but this was dropped in the final designs. The cartridge design was done by James Asher and Douglas Hardy. Hardy had been an engineer for Fairchild and helped in the initial design of the Channel F cartridges, but he quit to join Atari in 1976. The interior of the cartridge that Asher and Hardy designed was sufficiently different to avoid patent conflicts, but the exterior components were directly influenced by the Channel F to help work around the static electricity concerns.\nAtari was still recovering from its 1974 financial woes and needed additional capital to fully enter the home console market, though Bushnell was wary of being beholden to outside financial sources.&gt; Atari obtained smaller investments through 1975, but not at the scale it needed, and began considering a sale to a larger firm by early 1976. Atari was introduced to Warner Communications, which saw the potential for the growing video game industry to help offset declining profits from its film and music divisions. Negotiations took place during 1976, during which Atari cleared itself of liabilities, including settling a patent infringement lawsuit with Magnavox over Ralph H. Baer's patents that were the basis for the Magnavox Odyssey. In mid-1976, Fairchild announced the Channel F, planned for release later that year, beating Atari to the market.\nBy October 1976, Warner and Atari agreed to the purchase of Atari for . Warner provided an estimated which was enough to fast-track Stella. By 1977, development had advanced enough to brand it the \"Atari Video Computer System\" (VCS) and start developing games.\nLaunch and success.\nThe unit was showcased on June 4, 1977, at the Summer Consumer Electronics Show with plans for retail release in October. The announcement was purportedly delayed to wait out the terms of the Magnavox patent lawsuit settlement, which would have given Magnavox all technical information on any of Atari's products announced between June 1, 1976, and June 1, 1977. However, Atari encountered production problems during its first batch, and its testing was complicated by the use of cartridges.\nThe Atari VCS was launched in September 1977 at , with two joysticks and a \"Combat\" cartridge; eight additional games were sold separately. Most of the launch games were based on arcade games developed by Atari or its subsidiary Kee Games: for example, \"Combat\" was based on Kee's \"Tank\" (1974) and Atari's \"Jet Fighter\" (1975). Atari sold between 350,000 and 400,000 Atari VCS units during 1977, attributed to the delay in shipping the units and consumers' unfamiliarity with a swappable-cartridge console that is not dedicated to only one game.\nIn 1978, Atari sold only 550,000 of the 800,000 systems manufactured. This required further financial support from Warner to cover losses. Bushnell pushed the Warner Board of Directors to start working on \"Stella 2\", as he grew concerned that rising competition and aging tech specs of the VCS would render the console obsolete. However, the board stayed committed to the VCS and ignored Bushnell's advice, leading to his departure from Atari in 1979. Atari sold about 600,000 VCS systems in 1979, bringing the installed base to a little over 1.3 million.\nAtari obtained a license from Taito to develop a VCS conversion of its 1978 arcade hit \"Space Invaders\". This is the first officially licensed arcade conversion for a home console. Atari sold 1.25 million \"Space Invaders\" cartridges and over 1 million VCS systems in 1980, nearly doubling the install base to over 2 million, and then an estimated 3.1 million VCS systems in 1981. By 1982, 10 million consoles had been sold in the United States, while its best-selling game was \"Pac-Man\" at over copies sold by 1990. \"Pac-Man\" propelled worldwide Atari VCS sales to units during 1982, according to a November 1983 article in \"InfoWorld\" magazine. An August 1984 \"InfoWorld\" magazine article says more than Atari 2600 machines were sold by 1982. A March 1983 article in \"IEEE Spectrum\" magazine has about 3 million VCS sales in 1981, about 5.5 million in 1982, as well as a total of over 12 million VCS systems and an estimated 120 million cartridges sold.\nIn Europe, the Atari VCS sold 125,000 units in the United Kingdom during 1980, and 450,000 in West Germany by 1984. In France, where the VCS released in 1982, the system sold 600,000 units by 1989. The console was distributed by Epoch Co. in Japan in 1979 under the name \"Cassette TV Game\", but did not sell as well as Epoch's own Cassette Vision system in 1981.\nIn 1982, Atari launched its second programmable console, the Atari 5200. To standardize naming, the VCS was renamed to the \"Atari 2600 Video Computer System\", or \"Atari 2600\", derived from the manufacture part number CX2600. By 1982, the 2600 cost Atari about to make and was sold for an average of . The company spent .50 to to manufacture each cartridge, plus to for advertising, wholesaling for .\nThird-party development.\nActivision, formed by Crane, Whitehead, and Miller in 1979, started developing third-party VCS games using their knowledge of VCS design and programming tricks and began releasing games in 1980. \"Kaboom!\" (1981) and \"Pitfall!\" (1982) are among the most successful with at least one and four million copies sold, respectively. In 1980, Atari attempted to block the sale of the Activision cartridges, accusing the four of intellectual property infringement. The two companies settled out of court, with Activision agreeing to pay Atari a licensing fee for their games. This made Activision the first third-party video game developer and established the licensing model that continues to be used by console manufacturers for game development.\nActivision's success led to the establishment of other third-party VCS game developers following Activision's model in the early 1980s, including U.S. Games, Telesys, Games by Apollo, Data Age, Zimag, Mystique, and CommaVid. The founding of Imagic included ex-Atari programmers. Mattel and Coleco, each already producing its own more advanced console, created simplified versions of their existing games for the 2600. Mattel used the M Network brand name for its cartridges. Third-party games accounted for half of VCS game sales by 1982.\nDecline and redesign.\nIn addition to third-party game development, Atari also received the first major threat to its hardware dominance from the ColecoVision. Coleco had a license from Nintendo to develop a version of the arcade game \"Donkey Kong\" (1981), which was bundled with every ColecoVision console. Coleco gained about 17% of the hardware market in 1982 compared to Atari's 58%. With third parties competing for market share, Atari worked to maintain dominance in the market by acquiring licenses for popular arcade games and other properties to make games from. \"Pac-Man\" has numerous technical and aesthetic flaws, but nevertheless more than 7 million copies were sold. Heading into the 1982 holiday shopping season, Atari had placed high sales expectations on \"E.T. the Extra-Terrestrial\", a game programmed in about six weeks. Atari produced an estimated four million cartridges, but the game was poorly reviewed, and only about 1.5 million units were sold.\nWarner Communications issued revised earnings guidance in December 1982 to its shareholders, having expected a 50% year-to-year growth but now only expecting 10\u201315% due to declining sales at Atari. Coupled with the oversaturated home game market, Atari's weakened position led investors to start pulling funds out of video games, beginning a cascade of disastrous effects known as the video game crash of 1983. Many of the third-party developers formed prior to 1983 were closed, and Mattel and Coleco left the video game market by 1985.\nIn September 1983, Atari sent 14 truckloads of unsold Atari 2600 cartridges and other equipment to a landfill in the New Mexico desert, later labeled the Atari video game burial. Long considered an urban legend that claimed the burial contained millions of unsold cartridges, the site was excavated in 2014, confirming reports from former Atari executives that only about 700,000 cartridges had actually been buried. Atari reported a loss for 1983 as a whole, and continued to lose money into 1984, with a loss reported in the second quarter. By mid-1984, software development for the 2600 had essentially stopped except that of Atari and Activision.\nWarner, wary of supporting its failing Atari division, started looking for buyers in 1984. Warner sold most of the assets of Atari's counsumer electronics and home computer divisions to Jack Tramiel, the founder of Commodore International, in July 1984 in a deal valued at , though Warner retained Atari's arcade business. Tramiel was a proponent of personal computers, and halted all new 2600 game development soon after the sale.\nThe North American video game market did not recover until about 1986, after Nintendo's 1985 launch of the Nintendo Entertainment System in North America. Atari Corporation released a redesigned model of the 2600 in 1986, supported by an ad campaign touting a price of \"under 50 bucks\". With a large library of cartridges and a low price point, the 2600 continued to sell into the late 1980s. Atari released the last batch of games in 1989\u201390 including \"Secret Quest\" and \"Fatal Run\". By 1986, over Atari VCS units had been sold worldwide. The final Atari-licensed release is the PAL-only version of the arcade game \"KLAX\" in 1990.\nAfter more than 14 years on the market, 2600 production ended in 1992, along with the Atari 7800 and Atari 8-bit computers. In Europe, last stocks of the 2600 were sold until Summer/Fall of 1995.\nHardware.\nConsole.\nThe Atari 2600's CPU is the MOS Technology 6507, a version of the 6502, running at 1.19 MHz in the 2600. Though their internal silicon was identical, the 6507 was cheaper than the 6502 because its package included fewer memory-address pins\u201413 instead of 16. The designers of the Atari 2600 selected an inexpensive cartridge interface that has one fewer address pins than the 13 allowed by the 6507, further reducing the already limited addressable memory from 8 KB (213 = 8,192) to 4 KB (212 = 4,096). This was believed to be sufficient as \"Combat\" was only 2 KB. Later games circumvented this limitation with bank switching.\nThe console has 128 bytes of RAM for scratch space, the call stack, and the state of the game environment.\nThe top bezel of the console originally had six switches: power, TV type selection (color or black-and-white), game selection, left and right player difficulty, and game reset. The difficulty switches were moved to the back of the bezel in later versions of the console. The back bezel also included the controller ports, TV output, and power input.\nGraphics.\nThe Atari 2600 was designed to be compatible with the cathode-ray tube television sets produced in the late 1970s and early 1980s, which commonly lack auxiliary video inputs to receive audio and video from another device. Therefore, to connect to a TV, the console generates a radio frequency signal compatible with the regional television standards (NTSC, PAL, or SECAM), using a special switch box to act as the television's antenna.\nAtari developed the Television Interface Adaptor (TIA) chip in the VCS to handle the graphics and conversion to a television signal. It provides a single-color, 20-bit background register that covers the left half of the screen (each bit represents 4 adjacent pixels) and is either repeated or reflected on the right side. There are 5 single-color sprites: two 8-pixel wide \"players\"; two 1 bit \"missiles\", which share the same colors as the players; and a 1-pixel \"ball\", which shares the background color. The 1-bit sprites all can be controlled to stretch to 1, 2, 4, or 8 pixels.\nThe system was designed without a frame buffer to avoid the cost of the associated RAM. The background and sprites apply to a single scan line, and as the display is output to the television, the program can change colors, sprite positions, and background settings. The careful timing required to sync the code to the screen on the part of the programmer was labeled \"racing the beam\"; the actual game logic runs when the television beam is outside of the visible area of the screen. Early games for the system use the same visuals for pairs of scan lines, giving a lower vertical resolution, to allow more time for the next row of graphics to be prepared. Later games, such as \"Pitfall!\", change the visuals for each scan line or extend the black areas around the screen to extend the game code's processing time.\nRegional releases of the Atari 2600 use modified TIA chips for each region's television formats, which require games to be developed and published separately for each region. All modes are 160 pixels wide. NTSC mode provides 192 visible lines per screen, drawn at 60\u00a0Hz, with 16 colors, each at 8 levels of brightness. PAL mode provides more vertical scanlines, with 228 visible lines per screen, but drawn at 50\u00a0Hz and only 13 colors. SECAM mode, also a 50\u00a0Hz format, is limited to 8 colors, each with only a single brightness level.\nControllers.\nThe first VCS bundle has two types of controllers: a joystick (part number CX10) and pair of rotary paddle controllers (CX30). Driving controllers, which are similar to paddle controllers but can be continuously rotated, shipped with the \"Indy 500\" launch game. After less than a year, the CX10 joystick was replaced with the CX40 model designed by James C. Asher. Because the Atari joystick port and CX40 joystick became industry standards, 2600 joysticks and some other peripherals work with later systems, including the MSX, Commodore 64, Amiga, Atari 8-bit computers, and Atari ST. The CX40 joystick can be used with the Master System and Sega Genesis, but does not provide all the buttons of a native controller. Third-party controllers include Wico's Command Control joystick. Later, the CX42 Remote Control Joysticks, similar in appearance but using wireless technology, were released, together with a receiver whose wires could be inserted in the controller jacks.\nAtari introduced the CX50 Keyboard Controller in June 1978 along with two games that require it: \"Codebreaker\" and \"Hunt &amp; Score\". The similar, but simpler, CX23 Kid's Controller was released later for a series of games aimed at a younger audience. The CX22 Trak-Ball controller was announced in January 1983 and is compatible with the Atari 8-bit computers.\nThere were two attempts to turn the Atari 2600 into a keyboard-equipped home computer: Atari's never-released CX3000 \"Graduate\" keyboard, and the CompuMate keyboard by Spectravideo which was released in 1983.\nConsole models.\nMinor revisions.\nThe initial production of the VCS was made in Sunnyvale during 1977, using thick polystyrene plastic for the casing as to give the impression of weight from what was mostly an empty shell inside. The initial Sunnyvale batch had also included potential mounts for an internal speaker system on the casing, though the speakers were found to be too expensive to include; instead sound was routed through the TIA to the connected television. All six console switches were mounted on the front panel. Production of the unit was moved to Taiwan in 1978, where a less thick internal metal shielding was used and thinner plastic was used for the casing, reducing the system's weight. These two versions are commonly referred to as \"Heavy Sixers\" and \"Light Sixers\" respectively, referencing the six front switches.\nIn 1980, the difficulty switches were moved to the back of the console, leaving four switches on the front and replacing the previous all lowercase font for the switch labels to fully capitalized wording. Otherwise, these four-switch consoles look nearly identical to the earlier six-switch models. In 1982, to coincide with the release of the Atari 5200, Atari rebranded the console as the \"Atari 2600\", a name first used on a version of the four-switch model without woodgrain, giving it an all-black appearance. This all-black model is commonly referred to by fans as the \"Vader\" model, due to its resemblance to the \"Star Wars\" character of the same name.\nSears Video Arcade.\nAtari continued its OEM relationship with Sears under the latter's Tele-Games brand, which started in 1975 with the original \"Pong\". This is unrelated to the company Telegames, which later produced 2600 cartridges. Sears released several models of the VCS as the Sears Video Arcade series starting in 1977. The final Sears-specific model was the Video Arcade II, released during the fall of 1982.\nSears released versions of Atari's games with Tele-Games branding, usually with different titles. Three games were produced by Atari for Sears as exclusive releases: \"Steeplechase\", \"Stellar Track\", and \"Submarine Commander\".\nAtari 2800.\nThe Atari 2800 is the Japanese version of the 2600 released in October 1983. It is the first Japan-specific release of a 2600, though companies like Epoch had distributed the 2600 in Japan previously. The 2800 was released a short time after Nintendo's Family Computer (which became the dominant console in Japan), and it did not gain a significant share of the market. Sears previously released the 2800 in the US during late 1982 as the Sears Video Arcade II, which came packaged with two controllers and \"Space Invaders\". Around 30 specially branded games were released for the 2800.\nDesigned by engineer Joe Tilly, the 2800 has four controller ports instead of the two of the 2600. The controllers are an all-in one design using a combination of an 8-direction digital joystick and a 270-degree paddle, designed by John Amber. The 2800's case design departed from the 2600, using a wedge shape with non-protruding switches. The case style is the basis for the Atari 7800, which was redesigned for the 7800 by Barney Huang.\n1986 model.\nThe cost-reduced 1986 model, sometimes referred to as the \"2600 Jr.\", has a smaller form factor with an Atari 7800-like appearance. It was advertised as a budget gaming system (under ) with the ability to run a large collection of games. Released after the video game crash of 1983, and after the North American launch of the Nintendo Entertainment System, the 2600 was supported with new games and television commercials promoting \"The fun is back!\". Atari released several minor stylistic variations: the \"large rainbow\" (shown), \"short rainbow\", and an all-black version sold only in Ireland. Later European versions include a joypad.\nUnreleased prototypes.\nThe Atari 2700 was a version of the 2600 with wireless controllers.\nThe CX2000, with integrated joystick controllers, was a redesign based on human factor analysis by Henry Dreyfuss Associates.\nThe circa-1982 Atari 3200 was a backwards compatible 2600 successor with \"more memory, higher resolution graphics and improved sound\".\nRelated hardware and recreations.\nThe Atari 7800, announced in 1984 and released in 1986, is the official successor to the Atari 2600 and is backward compatible with 2600 cartridges.\nMultiple retro-style consoles and microconsoles have been released since the lifespan of the original Atari 2600:\nGames.\nIn 1977, nine games were released on cartridge to accompany the launch of the console: \"Air-Sea Battle\", \"Basic Math\", \"Blackjack\", \"Combat\", \"Indy 500\", \"Star Ship\", \"Street Racer\", \"Surround\", and \"Video Olympics\". \"Indy 500\" shipped with special \"driving controllers\", which are like paddles but rotate freely. \"Street Racer\" and \"Video Olympics\" use the standard paddle controllers. Atari, Inc. was the only developer for the first few years, releasing dozens of games.\nAtari determined that box art featuring only descriptions of the game and screenshots would not be sufficient to sell games in retail stores, since most games were based on abstract principles and screenshots give little information. Atari outsourced box art to Cliff Spohn, who created visually interesting artwork with implications of dynamic movement intended to engage the player's imagination while staying true to the gameplay. Spohn's style became a standard for Atari when bringing in assistant artists, including Susan Jaekel, Rick Guidice, John Enright, and Steve Hendricks. Spohn and Hendricks were the largest contributors to the covers in the Atari 2600 library. Ralph McQuarrie, a concept artist on the \"Star Wars\" series, was commissioned for one cover, the arcade conversion of \"Vanguard\". These artists generally conferred with the programmer to learn about the game before drawing the art.\nAn Atari VCS port of the \"Breakout\" arcade game appeared in 1978. The original is in black and white with a colored overlay, and the home version is in color. In 1980, Atari released \"Adventure\", the first action-adventure game, and the first home game with a hidden Easter egg.\nRick Maurer's port of Taito's \"Space Invaders\", released in 1980, is the first VCS game to have more than one million copies sold\u2014eventually doubling that within a year and totaling more than cartridges by 1983. It became the killer app to drive console sales. Versions of Atari's own \"Asteroids\" and \"Missile Command\" arcade games, released in 1981, were also major hits.\nEach early VCS game is in a 2K ROM. Later games, like \"Space Invaders\", have 4K. The VCS port of \"Asteroids\" (1981) is the first game for the system to use 8K via a bank switching technique between two 4K segments. Some later releases, including Atari's ports of \"Dig Dug\" and \"Crystal Castles\", are 16K cartridges. One of the final games, \"Fatal Run\" (1990), doubled this to 32K.\nMany early VCS titles were able to display in both monochrome (black and white) and full color through the use of the \"TV type\" switch on the console. This allowed the VCS games to function on both monochrome and color televisions. However, beginning around the rebranding from \"VCS\" to \"2600\", support for black and white display modes diminished greatly, with most releases during this period only displaying in color and the TV type switch serving no function. Other later titles, such as \"Secret Quest\", began using the TV type switch for gameplay functions, such as pausing.\nTwo Atari-published games, both from the system's peak in 1982, \"E.T. the Extra-Terrestrial\" and \"Pac-Man\", are cited as factors in the video game crash of 1983.\nA company named American Multiple Industries produced a number of pornographic games for the 2600 under the \"Mystique Presents Swedish Erotica\" label. The most notorious, \"Custer's Revenge\", was protested by women's and Native American groups because it depicted General George Armstrong Custer raping a bound Native American woman. Atari sued American Multiple Industries in court over the release of the game.\nLegacy.\nThe 2600 was so successful in the late 1970s and early 1980s that \"Atari\" was a synonym for the console in mainstream media and for video games in general. Jay Miner directed the creation of the successors to the 2600's TIA chip\u2014CTIA and ANTIC\u2014which are central to the Atari 8-bit computers released in 1979 and later the Atari 5200 console.\nThe Atari 2600 was inducted into the National Toy Hall of Fame at The Strong in Rochester, New York, in 2007. In 2009, the Atari 2600 was named the number two console of all time by IGN, which cited its remarkable role behind both the first video game boom and the video game crash of 1983, and called it \"the console that our entire industry is built upon\".\nIn November 2021, the current incarnation of Atari announced three 2600 games to be published under \"Atari XP\" label: \"Yars' Return\", \"Aquaventure\", and \"Saboteur\". These were previously included in Atari Flashback consoles.\nA model of the Atari 2600 was released by Lego in 2022. Included are the three games \"Asteroid\", \"Centipede\", and \"Adventure\". Included is a minifigure with a bedroom designed from the 1980s."}
{"id": "2780", "revid": "36449898", "url": "https://en.wikipedia.org/wiki?curid=2780", "title": "Atari 5200", "text": "The Atari 5200 SuperSystem or simply Atari 5200 is a home video game console introduced in 1982 by Atari, Inc. as a higher-end complement for the popular Atari Video Computer System. The VCS was renamed to Atari 2600 at the time of the 5200's launch. Created to compete with Mattel's Intellivision, the 5200 wound up a direct competitor of ColecoVision shortly after its release. While the Coleco system shipped with the first home version of Nintendo's \"Donkey Kong\", the 5200 included the 1978 arcade game \"Super Breakout\", which had already appeared on previous Atari home platforms.\nThe system architecture is almost identical to that of the Atari 8-bit computers, although software is not directly compatible between them. The 5200's controllers have an analog joystick and a numeric keypad along with start, pause, and reset buttons. The 360-degree non-centering joystick was touted as offering more control than the eight-way Atari CX40 joystick of the 2600, but was a focal point for criticism.\nOn May 21, 1984, during a press conference at which the Atari 7800 was introduced, company executives revealed that the 5200 had been discontinued after less than two years on the market. Total sales of the system were reportedly in excess of 1 million units, far short of its predecessor's sales of over 30 million.\nHardware.\nFollowing the release of the Video Computer System in 1977, Atari began developing hardware for a next generation game console. Instead, it was used as the basis for the Atari 400 and 800 home computers.\nAtari later decided to re-enter the console market using the same technology. Prototypes were called the \"Atari Video System X \u2013 Advanced Video Computer System\". Actual working \"Atari Video System X\" machines, whose hardware is 100% identical to the Atari 5200 do exist, but are extremely rare.\nThe initial 1982 release of the system had four controller ports, compared to two in most other consoles. The controllers have an analog joystick, numeric keypad, two fire buttons on each side of the controller, and game function keys for Start, Pause, and Reset. The 5200 also featured the innovation of the first automatic TV switchbox, allowing it to automatically switch from regular TV viewing to the game system signal when the system was activated. Previous RF adapters required the user to slide a switch on the adapter by hand. The RF box was also where the power supply connected in a unique dual power/television signal setup similar to the RCA Studio II's. A single cable coming out of the 5200 plugged into the switch box and carried both electricity and the television signal.\nThe 1983 revision of the Atari 5200 has two controller ports instead of four, and a change back to the more conventional separate power supply and standard non-autoswitching RF switch. It also has changes in the cartridge port address lines to allow for the Atari 2600 adapter released that year. While the adapter was only made to work on the two-port version, modifications can be made to the four-port to make it line-compatible. In fact, towards the end of the four-port model's production run, there were a limited number of consoles produced which included these modifications. These consoles can be identified by an asterisk in their serial numbers.\nAt one point following the 5200's release, Atari planned a smaller, cost-reduced version of the Atari 5200, which removed the controller storage bin. Code-named the \"Atari 5100\" (a.k.a. \"Atari 5200 Jr.\"), only a few fully working prototype 5100s were made before the project was canceled.\nControllers.\nThe controller prototypes used in the electrical development lab employed a yoke-and-gimbal mechanism that came from an RC airplane controller kit. The design of the analog joystick, which used a weak rubber boot rather than springs to provide centering, proved to be ungainly and unreliable. They quickly became the Achilles' heel of the system due to the combination of an overly complex mechanical design and a very low-cost internal flex circuit system. Another major flaw of the controllers was that the design did not translate into a linear acceleration from the center through the arc of the stick travel. The controllers did, however, include a pause button, a unique feature at the time. Various third-party replacement joysticks were also released, including those made by Wico.\nAtari Inc. released the Pro-Line Trak-Ball controller, which is used for games such as \"Centipede\" and \"Missile Command\". A paddle controller and an updated self-centering version of the original controller were also in development, but never made it to market.\nGames were shipped with plastic card overlays that snapped in over the keypad. The card would indicate which game functions, such as changing the view or vehicle speed, were assigned to each key.\nThe primary controller was ranked the 10th worst video game controller by IGN editor Craig Harris. An editor for \"Next Generation\" said that their non-centering joysticks \"rendered many games nearly unplayable\".\nDifferences from Atari 8-bit computers.\nDavid H. Ahl in 1983 described the Atari 5200 as \"a 400 computer in disguise\". Its internal design is similar to that of Atari 8-bit computers using the ANTIC, POKEY, and GTIA coprocessors. Software designed for one does not run on the other, but source code can be mechanically converted unless it uses computer-specific features. \"Antic\" magazine reported in 1984 that \"the similarities grossly outweigh the differences, so that a 5200 program can be developed and almost entirely debugged [on an Atari 8-bit computer] before testing on a 5200\". John J. Anderson of \"Creative Computing\" alluded to the incompatibility being intentional, caused by Atari's console division removing 8-bit compatibility to not lose control to the rival computer division.\nBesides the 5200's lack of a keyboard, the differences are:\nIn 1987, Atari Corporation released the XE Game System console, which is a repackaged 65XE (from 1985) with a detachable keyboard that can run home computer titles directly, unlike the 5200. Anderson wrote in 1984 that Atari could have released a console compatible with computer software in 1981.\nReception.\nThe Atari 5200 did not fare well commercially compared to its predecessor, the Atari 2600. While it touted superior graphics to the 2600 and Mattel's Intellivision, the system was initially incompatible with the 2600's expansive library of games, and some market analysts have speculated that this hurt its sales, especially since an Atari 2600 cartridge adapter had been released for the Intellivision II. (A revised two-port model was released in 1983, along with a game adapter that allowed gamers to play all 2600 games.) This lack of new games was due in part to a lack of funding, with Atari continuing to develop most of its games for the saturated 2600 market.\nMany of the 5200's games appeared simply as updated versions of 2600 titles, which failed to excite consumers. Its pack-in game, \"Super Breakout\", was criticized for not doing enough to demonstrate the system's capabilities. This gave the ColecoVision a significant advantage as its pack-in, \"Donkey Kong\", delivered a more authentic arcade experience than any previous game cartridge. In its list of the top 25 game consoles of all time, IGN claimed that the main reason for the 5200's market failure was the technological superiority of its competitor, while other sources maintain that the two consoles are roughly equivalent in power.\nThe 5200 received much criticism for the \"sloppy\" design of its non-centering analog controllers. Anderson described the controllers as \"absolutely atrocious\".\nDavid H. Ahl of \"Creative Computing Video &amp; Arcade Games\" said in 1983 that the \"Atari 5200 is, dare I say it, Atari's answer to Intellivision, Colecovision, and the Astrocade\", describing the console as a \"true mass market\" version of the Atari 8-bit computers despite the software incompatibility. He criticized the joystick's imprecise control but said that \"it is at least as good as many other controllers\", and wondered why \"Super Breakout\" was the pack-in game when it did not use the 5200's improved graphics.\nPopular culture.\nCritical to the plot of the 1984 film \"Cloak &amp; Dagger\" is an Atari 5200 game cartridge called \"Cloak &amp; Dagger\". The arcade version appears in the movie. In actuality the Atari 5200 version was started but never completed. The game was under development with the title \"Agent X\" when the movie producers and Atari learned of each other's projects and decided to cooperate. This collaboration was part of a larger phenomenon, of films featuring video games as critical plot elements (as with \"Tron\" and \"The Last Starfighter\") and of video game tie-ins to the same films (as with the \"Tron\" games for the Intellivision and other platforms)."}
{"id": "2781", "revid": "31196195", "url": "https://en.wikipedia.org/wiki?curid=2781", "title": "Atari 7800", "text": "The Atari 7800 ProSystem, or simply the Atari 7800, is a home video game console officially released by Atari Corporation in 1986 as the successor to both the Atari 2600 and Atari 5200. It can run almost all Atari 2600 cartridges, making it one of the first consoles with backward compatibility. It shipped with a different joystick than the 2600-standard CX40 and included \"Pole Position II\" as the pack-in game. The European model has a gamepad instead of a joystick. Most of the early releases for the system are ports of 1981\u20131983 arcade video games. The final wave of 7800 cartridges are closer in style to what was available on other late 1980s consoles, such as \"Scrapyard Dog\" and \"Midnight Mutants\".\nDesigned by General Computer Corporation, the 7800 has graphics hardware similar to early 1980s arcade video games and is a significant improvement over Atari's previous consoles. It uses same Television Interface Adaptor chip that launched with the 2600 in 1977 to generate two-channel audio. In an effort to prevent the flood of poor quality games that contributed to the video game crash of 1983, cartridges had to be digitally signed by Atari.\nThe Atari 7800 was first announced by Atari, Inc. on May 21, 1984, but a general release was shelved until May 1986 due to the sale of the company. Atari Corporation dropped support for the 7800, along with the 2600 and the Atari 8-bit computers, on January 1, 1992.\nHistory.\nThe Atari 7800 ProSystem was the first console from Atari, Inc. designed by an outside company: General Computer Corporation. It was developed in 1983\u201384 with an intended mass market rollout in June 1984, but was canceled after the sale of the company to Tramel Technology Ltd on July 2, 1984. The project was originally called the Atari 3600.\nWith a background in creating arcade games such as \"Food Fight\", GCC designed the new system with a graphics architecture similar to arcade machines of the time. The CPU is a slightly customized 6502 processor, the Atari SALLY, running at 1.79\u00a0MHz. By some measures the 7800 is more powerful, and by others less, than the 1983 Nintendo Entertainment System. It uses the 2600's Television Interface Adaptor chip, with the same restrictions, for generating two-channels of audio.\nLaunch.\nThe 7800 was announced on May 21, 1984. Thirteen games were announced for the system's launch: \"Ms. Pac-Man\", \"Pole Position II\", \"Centipede\", \"Joust\", \"Dig Dug\", \"Nile Flyer\" (eventually released as \"Desert Falcon\"), \"\", \"Galaga\", \"Food Fight\", \"Ballblazer\", \"Rescue on Fractalus!\" (later canceled), \"Track &amp; Field\", and \"Xevious\". It was a significant technological leap over the Atari 2600 and Atari 5200.\nOn July 2, 1984, Warner Communications sold Atari's Consumer Division to Jack Tramiel. All projects were halted during an initial evaluation period. GCC had not been paid for their development of the 7800, and Warner and Tramiel fought over who was accountable. In May 1985, Tramiel relented and paid GCC. This led to additional negotiations regarding the launch titles GCC had developed, then an effort to find someone to lead their new video game division, which was completed in November 1985. The original production run of the Atari 7800 languished in warehouses until it was introduced in January 1986.\nThe console was released nationwide in May 1986 for $79.95. It launched with titles intended for the 7800's debut in 1984 and was aided by a marketing campaign with a budget in the \"low millions\" according to Atari Corporation officials. This was substantially less than the $9 million spent by Sega and the $16 million spent by Nintendo. The keyboard and high score cartridge planned by Warner were cancelled. The 7800 addressed many of the most common complaints with the preceding 5200, including a smaller size, built-in backward compatibility, and an improved controller design.\nIn February 1987, \"Computer Entertainer\" reported that 100,000 Atari 7800 consoles had been sold in the United States, including those which had been warehoused since 1984. This was less than the Master System's 125,000 and the NES's 1.1 million. A complaint from owners in 1986 was the slow release of games. \"Galaga\" in August was followed by \"Xevious\" in November. By the end of 1986, the 7800 had 10 games, compared to Sega's 20 and Nintendo's 36. Atari would sell over 1 million 7800 consoles by June 1988.\nDiscontinuation.\nOn January 1, 1992, Atari Corporation announced the end of production and support for the 7800, 2600, and the 8-bit computer family including the Atari XEGS. At least one game, an unreleased port of \"Toki\", was worked on past this date. By the time of the discontinuation, the Nintendo Entertainment System controlled 80% of the North American market while Atari had 12%. In Europe, last stocks of the 7800 were sold until summer/fall of 1995.\n\"Retro Gamer\" magazine issue 132 reported that according to Atari UK Marketing Manager Darryl Still, \"it was very well stocked by European retail; although it never got the consumer traction that the 2600 did, I remember we used to sell a lot of units through mail order catalogues and in the less affluent areas\".\nTechnical specifications.\nGraphics.\nGraphics are generated by the custom MARIA chip, which uses an approach common in contemporary arcade system boards and is different from other second and third generation consoles. Instead of a limited number of hardware sprites, MARIA treats everything as a sprite described in a series of display lists. Each display list contains pointers to graphics data and color and positioning information.\nMARIA supports a palette of 256 colors and graphics modes which are either 160 pixels wide or 320 pixels wide. While the 320 pixel modes theoretically enable the 7800 to create games at higher resolution than the 256 pixel wide graphics found in the Nintendo Entertainment System and Master System, the processing demands of MARIA result in most games using the 160 pixel mode.\nEach sprite can have from 1 to 12 colors, with 3 colors plus transparency being the most common. In this format, the sprite references one of 8 palettes, where each palette holds 3 colors. The background (visible when not covered by other objects) can also be assigned a color. In total, 25 colors can appear on a scan line.\nThe graphics resolution, color palettes, and background color can be adjusted between scan lines. This can be used to render high resolution text in one area of the screen, while displaying more colorful graphics at lower resolution in the gameplay area.\nSound.\nThe 7800 uses the TIA chip for two channel audio, the same chip used in the 1977 Atari VCS, and the sound is of the same quality as that system. To compensate, GCC's engineers allowed games to include a POKEY audio chip in the cartridge. Only \"Ballblazer\" and \"Commando\" do this.\nGCC planned to make a low-cost, high performance sound chip, GUMBY, which could also be placed in 7800 cartridges to enhance its sound capabilities further. This project was cancelled when Atari was sold to Jack Tramiel.\nDigitally signed cartridges.\nFollowing the large number of low quality, third party games for the Atari 2600, Atari required that cartridges for the 7800 be digitally signed. When a cartridge is inserted into the system, the BIOS generates a signature of the cartridge ROM and compares it to the one stored on the cartridge. If they match, the console operates in 7800 mode, granting the game access to MARIA and other features, otherwise the console operates as a 2600. This digital signature code is not present in PAL 7800s, which use various heuristics to detect 2600 cartridges, due to export restrictions.\nBackward compatibility.\nThe 7800's compatibility with the Atari 2600 is made possible by including many of the same chips used in the 2600. When playing an Atari 2600 game, the 7800 uses a Television Interface Adaptor chip to generate graphics and sound. The processor is slowed to 1.19\u00a0MHz, to mirror the performance of the 2600's 6507 chip. RAM is limited to 128 bytes and cartridge data is accessed in 4K blocks.\nWhen in 7800 mode (signified by the appearance of the full-screen Atari logo), the graphics are generated entirely by the MARIA graphics processing unit. All system RAM is available and cartridge data is accessed in larger 48K blocks. The system's SALLY 6502 runs at its normal 1.79\u00a0MHz. The 2600 chips are used to generate sound and to provide the interfaces to the controllers and console switches.\nPeripherals.\nThe Atari 7800 came bundled with the Atari Pro-Line Joystick, a two-button controller with a joystick for movement. The Pro-Line was developed for the 2600 and advertised in 1983, but delayed until Atari proceeded with the 7800. The right fire button only works as a separate fire button for certain 7800 games; otherwise, it duplicates the left fire button, allowing either button to be used for 2600 games. While physically compatible, the 7800's controllers do not work with the Sega Master System, and Sega's controllers are unable to use the 7800's two-button mode.\nIn response to criticism over ergonomic issues with the Pro-Line controllers, Atari later released a joypad controller with the European 7800. Similar in style to controllers found on Nintendo and Sega systems, it was not available in the United States.\nThe Atari XG-1 light gun, bundled with the Atari XEGS and also sold separately, is compatible with the 7800. Atari released five 7800 light gun games: \"Alien Brigade\", \"Barnyard Blaster\", \"Crossbow\", \"Meltdown\", and \"Sentinel\".\nCancelled peripherals.\nAfter the acquisition of the Atari Consumer Division by Jack Tramiel in 1984, several expansion options for the system were cancelled:\nGames.\nWhile the system can play the over 400 games for the Atari 2600, there were only 59 official releases for the 7800. The lineup emphasized high-quality versions of games from the golden age of arcade video games. \"Pole Position II\", \"Dig Dug\", and \"Galaga\", by the time of the 1986 launch, were three, four, and five years old, respectively. A raster graphics version of 1979's \"Asteroids\" was released in 1987. In 1988, Atari published a conversion of Nintendo's \"Donkey Kong\", seven years after the original arcade game and five years after the Atari 8-bit computer cartridge. Atari also marketed a line of games called \"Super Games\" which were arcade and computer games previously not playable on a home console such as \"\" and \"Impossible Mission\".\nEleven games were developed and sold by three third-party companies under their own labels (Absolute Entertainment, Activision, and Froggo) with the rest published by Atari Corporation. Most of the games from Atari were developed by outside companies under contract.\nSome NES games were developed by companies who had licensed their title from a different arcade manufacturer. While the creator of the NES version would be restricted from making a competitive version of an NES game, the original arcade copyright holder was not precluded from licensing out rights for a home version of an arcade game to multiple systems. Through this loophole, Atari 7800 conversions of \"Mario Bros.\", \"Double Dragon\", \"Commando\", \"Rampage\", \"Xenophobe\", \"Ikari Warriors\", and \"Kung-Fu Master\" were licensed and developed.\nA final batch of games was released by Atari in 1990: \"Alien Brigade\", \"Basketbrawl\", \"Fatal Run\", \"Meltdown\", \"Midnight Mutants\", \"MotorPsycho\", \"Ninja Golf\", \"Planet Smashers\", and \"Scrapyard Dog\". \"Scrapyard Dog\" was later released for the Atari Lynx.\nLegacy.\nAtari Flashback.\nIn 2004, the Infogrames-owned version of Atari released the Atari Flashback console. It resembles a miniature Atari 7800 and has five 7800 and fifteen 2600 games built-in. Built using the NES-On-A-Chip hardware instead of recreating the Atari 7800 hardware, it was criticized for failing to properly replicate the actual gaming experience. A subsequent 7800 project was cancelled after prototypes were made.\nGame development.\nThe digital signature long prevented aftermarket games from being developed. The signing software was eventually found and released at Classic Gaming Expo in 2001.\nSeveral new Atari 7800 games such as \"Beef Drop\", \"B*nQ\", \"Combat 1990\", \"CrazyBrix\", \"Failsafe\", and \"Santa Simon\" have been released..\nSource code.\nIn July 2009, the source code to 13 games, the operating system, and Atari ST-hosted development tools, were released. Commented assembly language source code was made available for \"Centipede\", \"Commando\", \"Crossbow\", \"Desert Falcon\", \"Dig Dug\", \"Food Fight\", \"Galaga\", \"Hat Trick\", \"Joust\", \"Ms. Pac-Man\", \"Super Stunt Cycle\", \"\", and \"Xevious\".\nAtari 7800+.\nAtari Inc. and Plaion are developing a microconsole, the Atari 7800+, for release by the end of 2024. The console, built as a small scale replica of the original 7800, includes support for physical cartridges of both the Atari 2600 and Atari 7800. It is effectively a variant of the Atari 2600+ introduced in 2023. "}
{"id": "2782", "revid": "9798395", "url": "https://en.wikipedia.org/wiki?curid=2782", "title": "Atari Jaguar", "text": "The Jaguar is a home video game console developed by Atari Corporation and released in North America in November 1993. It is in the fifth generation of video game consoles, and it competed with fourth generation consoles, including the 16-bit Genesis, the 16-bit Super NES, and the 32-bit 3DO Interactive Multiplayer. Jaguar has a Motorola 68000 CPU and two custom 32-bit coprocessors named Tom and Jerry. Atari marketed it as the world's first 64-bit game system, emphasizing its blitter's 64-bit bus; however, none of its three processors have a 64-bit instruction set, as do later 64-bit consoles such as PlayStation 2 or Nintendo 64. The Jaguar launched with \"Cybermorph\" as the pack-in game, which received mixed reviews. The system's library ultimately comprises only 50 licensed games.\nDevelopment started in the early 1990s by Flare Technology, which focused on the system after cancellation of the Panther console. The Jaguar was an important system for Atari after discontinuing Atari ST computers in favor of video games. However, game development was complicated by the multi-chip architecture, hardware bugs, and poor programming tools. Underwhelming sales further eroded third-party support.\nAtari attempted to extend the system's lifespan with the Jaguar CD add-on, with an additional 13 games, and emphasizing the Jaguar's price of over less than its competitors. Jaguar could not compete against the Saturn and PlayStation, both released in 1995. Atari had internally abandoned the system by the end of that year, liquidating its inventory by 1996. The commercial failure of the Jaguar prompted Atari to leave the console market, and restructure itself as a third-party developer. After Hasbro Interactive acquired all Atari Corporation properties, it released the Jaguar patents into the public domain in 1999, and declared it an open platform. Since its discontinuation, hobbyists have produced games for the system.\nHistory.\nDevelopment.\nAtari Corporation's previous home video game console, the 7800, was released in 1986. It was considered an \"also-ran\" and far behind rival Nintendo. Around 1989, work began on a new console leveraging technology from Atari ST computers. It was originally named the Super XE, following the XE Game System, and eventually became the Panther using either 16 or 32-bit architecture. A more advanced system codenamed Jaguar also began work.\nBoth the Jaguar and Panther were developed by the members of Flare Technology, a company formed by Martin Brennan and John Mathieson. The team had claimed that they could not only make a console superior to the Genesis or the Super NES, but they could also be cost-effective. Atari was impressed by Flare's work on the Konix Multisystem, and persuaded them to close Flare and form a new company called Flare II, to be funded by Atari.\nWork on the Jaguar design progressed faster than expected, so Atari canceled the Panther project in 1991 to focus on the more promising Jaguar. Rumors were already circulating of a 1992 launch and its 32-bit or even 64-bit architecture. By this time the Atari ST had long been surpassed in popularity by the Amiga, while both Atari and Commodore became victims of Wintel, which became the dominant computer platform. Atari's support for legacy 8-bit products was canceled to fully focus on developing Jaguar, and ST computers were canceled during the Jaguar's release in 1993.\nThe Jaguar was unveiled in at the Summer Consumer Electronics Show in June 1993, calling it a \"multi-media entertainment system\".\nLaunch.\nThe Jaguar was launched on November 23, 1993, at , under a manufacturing deal with IBM. The system was initially available only in the test markets of New York City and San Francisco, with the slogan \"Get bit by Jaguar\", claiming superiority over competing 16-bit and 32-bit systems. During this test launch, Atari sold all units hoping it would rally support for the system. A nationwide release followed six months later, in early 1994. The Jaguar struggled to attain a substantial user base. Atari reported shipping 17,000 units as part of the test market in 1993. By the end of 1994, it reported that it had sold approximately 100,000 units.\n\"Computer Gaming World\" wrote in January 1994 that the Jaguar was \"a great machine in search of a developer/customer base\", as Atari had to \"overcome the stigma of its name (lack of marketing and customer support, as well as poor developer relations in the past)\". Atari had \"ventured late into third-party software support\" for the Jaguar, but competing console 3DO's \"18 month public relations blitz\" resulted in \"an avalanche of software support\". The small size and poor quality of the Jaguar's game library became the most commonly cited reason for tepid adoption, because early releases like \"Trevor McFur in the Crescent Galaxy\", \"Raiden\", and \"\" also received poor reviews, the latter two for failing to take full advantage of the Jaguar's hardware. Jaguar did eventually earn praise with games such as \"Tempest 2000\", \"Doom\", and \"Wolfenstein 3D\". The most successful game during the Jaguar's first year was \"Alien vs. Predator\". However, these occasional successes were seen as insufficient while the Jaguar's competitors were receiving a continual stream of critically acclaimed software; \"GamePro\" concluded its rave review of \"Alien vs. Predator\" by remarking \"If Atari can turn out a dozen more games like \"AvP\", Jaguar owners could truly rest easy and enjoy their purchase.\" \"Next Generation\" commented that \"thus far, Atari has spectacularly failed to deliver on the software side, leaving many to question the actual quality and capability of the hardware. With only one or two exceptions \u2013 \"Tempest 2000\" is cited most frequently \u2013 there have just been no truly great games for the Jaguar up to now.\" It further noted that though Atari is well known by older gamers, the company had much less overall brand recognition than Sega, Sony, Nintendo, or even The 3DO Company. However, they argued that with its low price point, the Jaguar might still compete if Atari could improve the software situation.\nBit count controversy.\nAtari tried to downplay competing consoles by proclaiming the Jaguar was the only \"64-bit\" system; in its marketing in the American market the company used the tagline \"do the math!\", in reference to the 64 number. This claim is questioned by some, because the Motorola 68000 CPU and the Tom and Jerry coprocessors execute 32-bit instruction sets. Atari's reasoning that the 32-bit Tom and Jerry chips work in tandem to add up to a 64-bit system was ridiculed in a mini-editorial by \"Electronic Gaming Monthly\", which commented that \"If Sega did the math for the Sega Saturn the way Atari did the math for their 64-bit Jaguar system, the Sega Saturn would be a 112-bit monster of a machine.\" \"Next Generation\", in a mostly negative review of the Jaguar, maintained that it is a true 64-bit system, because the data path from the DRAM to the CPU and Tom and Jerry chips is 64 bits wide.\nArrival of Saturn and PlayStation.\nIn early 1995, Atari announced that it had dropped the price of the Jaguar to , to be more competitive. Atari ran infomercials with enthusiastic salesmen touting the game system for most of 1995, but did not sell the remaining stock.\nIn 1995, CEO Sam Tramiel declared the Jaguar at least as powerful than the newly launched Saturn, and slightly weaker than the upcoming PlayStation. \"Next Generation\" received a deluge of letters in response to Tramiel's comments, particularly his threat to bring Sony to court for price dumping if the PlayStation entered the U.S. market at a retail price below $300. Many readers found this threat hollow and hypocritical, since Tramiel noted in the same interview that Atari was selling the Jaguar at a loss. The editor responded that price dumping does not have to do with a product being priced below cost, but its being priced much lower in one country than anotherwhich, as Tramiel said, is illegal. Tramiel and \"Next Generation\" agreed that the PlayStation's Japanese price converts to approximately $500. His remark, that the small number of third party Jaguar games was good for Atari's profitability, angered Jaguar owners already frustrated at the small library.\nAtari's 1995 annual report noted: In addition, Atari had severely limited financial resources, and so could not create the level of marketing which has historically backed successful gaming consoles.\nDecline.\nFigures from the NPD Group showed that at the end of year 1995, the Jaguar had statistically a share of zero percent of the \"sold through\" units (which are systems purchased by consumers) in the 32-bit market, which was also lower than the one percent held by its struggling rival 3DO.\nBy November 1995, mass layoffs and insider statements were fueling journalistic speculation that Atari had ceased both development and manufacturing for the Jaguar and was simply trying to sell off existing stock before exiting the video game industry. Although Atari continued to deny these theories going into 1996, core Jaguar developers such as High Voltage Software and Beyond Games stated that they were no longer receiving communications from Atari regarding future Jaguar projects.\nIn its 10-K405 SEC Filing, filed April 12, 1996, Atari informed stockholders that its revenues had declined by more than half, from $38.7 million in 1994 to $14.6 million in 1995, then gave them the news on the truly dire nature of the Jaguar:\nThe filing confirmed that Atari had abandoned the Jaguar in November 1995 and in the subsequent months were concerned chiefly with liquidating its inventory of Jaguar products. On April 8, 1996, Atari Corporation agreed to merge with JTS, Inc. in a reverse takeover, thus forming JTS Corporation. The merger was finalized on July 30.\nAfter the merger, the bulk of Jaguar inventory remained unsold and would be finally moved out to Tiger Software, a private liquidator, on December 23, 1996. On March 13, 1998, JTS sold the Atari name and properties to Hasbro Interactive.\nTechnical specifications.\nFrom the Jaguar Software Reference manual, page 1:\nDesign specs for the console allude to the GPU or DSP being capable of acting as a CPU, leaving the Motorola 68000 to read controller inputs. Atari's Leonard Tramiel also specifically suggested that the 68000 not be used by developers. In practice, however, many developers use the Motorola 68000 to drive gameplay logic due to the greater developer familiarity of the 68000 and the adequacy of the 68000 for certain types of games. Most critically, a flaw in the memory controller means that certain obscure conventions must be followed for the RISC chips to be able to execute code from RAM.\nThe system was notoriously difficult to program for, because its multi-processor design is complex, development tools were released in an unfinished state, and the hardware had crippling bugs.\nCOJAG arcade games.\nAtari Games licensed the Jaguar's chipset for use in its arcade games. The system, named CoJag (for \"Coin-Op Jaguar\"), replaced the 68000 with a 68020 or MIPS R3000-based CPU (depending on the board version), added more RAM, a full 64-bit wide ROM bus (Jaguar ROM bus is 32-bit), and optionally a hard drive (some games such as \"Freeze\" are ROM only). It runs the lightgun games \"Area 51\" and \"Maximum Force\", which were released as dedicated cabinets or as the \"Area 51\" and \"Maximum Force\" combo machine. Other games were developed but never released: \"3 On 3 Basketball\", \"Fishin' Frenzy\", \"Freeze\", and \"Vicious Circle\".\nPeripherals.\nPrior to the launch of the console in November 1993, Atari had announced a variety of peripherals to be released over the console's lifespan. This included a CD-ROM-based console, dial-up Internet access with support for online games, a virtual reality headset, and an MPEG-2 video card. However, due to the poor sales and eventual commercial failure of the Jaguar, most of the peripherals in development were canceled. The only peripherals and add-ons released by Atari for the Jaguar are a redesigned controller, an adapter for four players, a CD console add-on, and a link cable for local area network (LAN) gaming.\nThe redesigned second controller, the ProController by Atari, added three more face buttons and two triggers. It was created in response to the criticism of the original controller, said to lack enough buttons for fighting games in particular. It was never bundled with the system. The Team Tap multitap adds 4-controller support, compatible only with the optionally bundled \"White Men Can't Jump\" and \"NBA Jam Tournament Edition\". Eight player gameplay with two Team Taps is possible but unsupported by those games. For LAN multiplayer support, the Jaglink Interface links two Jaguar consoles through a modular extension and a UTP phone cable. It is compatible with three games: \"AirCars\", \"BattleSphere\", and \"Doom\".\nIn 1994 at the CES, Atari announced that it had partnered with Phylon, Inc. to create the Jaguar Voice/Data Communicator. The unit was delayed and an estimated 100 units were produced, but eventually in 1995 was canceled. The Jaguar Voice Modem or JVM utilizes a 19.9\u00a0kbit/s dial up modem to answer incoming phone calls and store up to 18 phone numbers. Players directly dial each other for online play, only compatible with \"Ultra Vortek\".\nJaguar CD.\nThe Jaguar CD is a CD-ROM peripheral for games. It was released in September 1995, two years after the Jaguar's launch. Thirteen CD games were released during its manufacturing lifetime, with more being made later by homebrew developers. Each Jaguar CD unit has a Virtual Light Machine, which displays light patterns corresponding to music, if the user inserts an audio CD into the console. It was developed by Jeff Minter, after experimenting with graphics during the development of \"Tempest 2000\". The program was deemed a spiritual successor to the Atari Video Music, a visualizer released in 1976.\nThe Memory Track is a cartridge accessory for the Jaguar CD, providing Jaguar CD games with 128\u00a0K EEPROM for persistent storage of data such as preferences and saved games. The Jaguar Duo (codenamed Jaguar III) was a proposal to integrate the Jaguar CD to make a new console, a concept similar to the TurboDuo and Genesis CDX. A prototype, described by journalists as resembling a bathroom scale, was unveiled at the 1995 Winter Consumer Electronics Show, but the console was canceled before production.\nJaguar VR.\nA virtual reality headset compatible with the console, tentatively titled the Jaguar VR, was unveiled by Atari at the 1995 Winter Consumer Electronics Show. The development of the peripheral was a response to Nintendo's virtual reality console, the Virtual Boy, which had been announced the previous year. The headset was developed in cooperation with Virtuality, which had previously created many virtual reality arcade systems, and was already developing a similar headset for practical purposes, named Project Elysium, for IBM. The peripheral was targeted for a commercial release before Christmas 1995. However, the deal with Virtuality was abandoned in October 1995. After Atari's merger with JTS in 1996, all prototypes of the headset were allegedly destroyed. However, two working units, one low-resolution prototype with red and grey-colored graphics and one high-resolution prototype with blue and grey-colored graphics, have since been recovered, and are regularly showcased at retrogaming-themed conventions and festivals. Only one game was developed for the Jaguar VR prototype: a 3D-rendered version of the 1980 arcade game \"Missile Command\", titled \"Missile Command 3D\", and a demo of Virtuality's \"Zone Hunter\" was created.\nUnlicensed peripherals.\nAn unofficial expansion peripheral for the Jaguar dubbed the \"Catbox\" was released by the Rockford, Illinois company ICD. It was originally slated to be released early in the Jaguar's life, in the second quarter of 1994, but was not actually released until mid-1995. The ICD CatBox plugs directly into the AV/DSP connectors located in the rear of the Jaguar console and provides three main functions. These are audio, video, and communications. It features six output formats, three for audio (Line level stereo, RGB monitor, headphone jack with volume control) and three for video (composite, S-Video, and RGB analog component video) making the Jaguar compatible with multiple high quality monitor systems and multiple monitors at the same time. It is capable of communications methods known as CatNet and RS-232 and DSP pass through, allowing the user to connect two or more Jaguars together for multiplayer games either directly or with modems. The ICD CatBox features a polished stainless steel casing and red LEDs in the jaguar's eyes on the logo that indicate communications activity. An IBM AT-type null modem cable may be used to connect two Jaguars together. The CatBox is also compatible with Atari's Jaglink Interface peripheral.\nReception.\nReviewing the Jaguar just a few weeks prior to its launch, \"GamePro\" gave it a \"thumbs sideways\". They praised the power of the hardware but criticized the controller, and were dubious of how the software lineup would turn out, commenting that Atari's failure to secure support from key third party publishers such as Capcom was a bad sign. They concluded that \"Like the 3DO, the Jaguar is a risky investment \u2013 just not quite as expensive.\"\nThe Jaguar won \"GameFan\"s \"Best New System\" award for 1993.\nThe small size and poor quality of the Jaguar's game library became the most commonly cited reason for its failure in the marketplace. The pack-in game \"Cybermorph\" was one of the first polygon-based games for consoles, but was criticized for design flaws and a weak color palette, and compared unfavorably with the SNES's \"Star Fox\". Other early releases like \"Trevor McFur in the Crescent Galaxy\", \"Raiden\", and \"\" also received poor reviews, the latter two for failing to take full advantage of the Jaguar's hardware. Jaguar did eventually earn praise with games such as \"Tempest 2000\", \"Doom\", and \"Wolfenstein 3D\". The most successful title during the Jaguar's first year was \"Alien vs. Predator\". However, these occasional successes were seen as insufficient while the Jaguar's competitors were receiving a continual stream of critically acclaimed software; \"GamePro\" concluded their rave review of \"Alien vs. Predator\" by remarking \"If Atari can turn out a dozen more games like AvP, Jaguar owners could truly rest easy and enjoy their purchase.\" In late 1995 reviews of the Jaguar, \"Game Players\" remarked, \"The Jaguar suffers from several problems, most importantly the lack of good software.\" and \"Next Generation\" likewise commented that \"thus far, Atari has spectacularly failed to deliver on the software side, leaving many to question the actual quality and capability of the hardware. With only one or two exceptions \u2013 \"Tempest 2000\" is cited most frequently \u2013 there have just been no truly great games for the Jaguar up to now.\" They further noted that though Atari is well known by older gamers, the company had much less overall brand recognition than Sega, Sony, Nintendo, or even The 3DO Company. However, they argued that with its low price point, the Jaguar might still compete if Atari could improve the software situation. They gave the system two out of five stars. \"Game Players\" also stated though it is 64-bit, the Jaguar is much less powerful than the 3DO, Saturn, and PlayStation, even when supplemented with the Jaguar CD. With such a small library of games to challenge the incumbent 16-bit game consoles, Jaguar's appeal never grew beyond a small gaming audience. Digital Spy commented: \"Like many failed hardware ventures, it still maintains something of a cult following but can only be considered a misstep for Atari.\"\nIn 2006, IGN editor Craig Harris rated the original Jaguar controller as the worst game controller ever, criticizing the unwarranted recycling of the 1980s \"phone keypad\" format and the small number of action buttons, which he found particularly unwise given that Atari was actively trying to court fighting game fans to the system. Ed Semrad of \"Electronic Gaming Monthly\" commented that many Jaguar games gratuitously used all of the controller's phone keypad buttons, making the controls much more difficult than they needed to be. \"GamePro\"s The Watch Dog remarked, \"The controller usually doesn't use the keypad, and for games that use the keypad extensively (\"Alien vs. Predator\", \"Doom\"), a keypad overlay is used to minimize confusion. But yes, it is a lot of buttons for nuttin'.\" Atari added more action buttons for its Pro Controller, to improve performance in fighting games in particular.\nLegacy.\nTelegames continued to publish games for the Jaguar after it was discontinued, and for a time was the only company to do so. On May 14, 1999, Hasbro Interactive announced that it had released all patents to the Jaguar, declaring it an open platform, and enabling extensive homebrew development. Following the announcement, Songbird Productions joined Telegames in releasing unfinished Jaguar games alongside new games to satisfy the cult following. Hasbro Interactive, along with all the Atari properties, was sold to Infogrames on January 29, 2001.\nIn the United Kingdom in 2001, Telegames and retailer Game made a deal to bring the Jaguar to Game's retail outlets. It was initially sold for \u00a329.99 new and software ranged between \u00a39.99 for more common games such as \"Doom\" and \"Ruiner Pinball\" and \u00a339.99 for rarer releases such as \"Defender 2000\" and \"Checkered Flag\". The machine had a presence in the stores until 2007, when remaining consoles were sold off for \u00a39.99 and games were sold for as low as 97p.\nIn 2022, the compilation \"Atari 50\" was released with a collection of Jaguar games, as one of the first instances of Jaguar software being officially rereleased by Atari. Due to the unique design of the original Jaguar controller, the games feature reworked control layouts to allow them to work with modern hardware.\nMolds.\nIn 1997, Imagin Systems, a manufacturer of dental imaging equipment, purchased the Jaguar cartridge and console molds, including the molds for the CD add-on, from JTS. With minor modification, they fit its HotRod camera, and the cartridge molds were reused to create an optional memory expansion card. In a retrospective, Imagin founder Steve Mortenson praised the design, but admitted that their device came at the time of the dental industry's transition to USB, and apart from a few prototypes, the molds went unused.\nIn December 2014, the molds were purchased from Imagin Systems by Mike Kennedy, owner of the Kickstarter funded \"Retro Videogame Magazine\", to propose a new crowdfunded video game console, the Retro VGS, later rebranded the Coleco Chameleon with a licensing agreement with Coleco. The purchase of the molds was far cheaper than designing and manufacturing entirely new molds, and Kennedy described their acquisition as \"the entire reason [the Retro VGS] is possible\". However, the project was terminated in March 2016 following criticism of Kennedy and doubts regarding demand for the proposed console. Two \"prototypes\" were discovered to be fakes and Coleco withdrew from the project. After the project's termination, the molds were sold to Albert Yarusso, the founder of the AtariAge website."}
{"id": "2783", "revid": "31196195", "url": "https://en.wikipedia.org/wiki?curid=2783", "title": "Atari Lynx", "text": "The Atari Lynx is a 16-bit fourth-generation hand-held game console released by Atari Corporation in September 1989 in North America and 1990 in Europe and Japan. It was the first handheld game console with a color liquid-crystal display. Powered by a 4\u00a0MHz 65C02 8-bit CPU and a custom 16-bit blitter, the Lynx was more advanced than Nintendo's monochrome Game Boy, released two months earlier. It also competed with Sega's Game Gear and NEC's TurboExpress, released the following year.\nThe system was developed at Epyx by two former designers of the Amiga personal computers. The project was called the Handy Game or simply Handy. In 1991, Atari replaced the Lynx with a smaller model internally referred to as the Lynx II. Atari published a total of 73 games for the Lynx before it was discontinued in 1995.\nHistory.\nThe Lynx system was originally developed by Epyx as the Handy Game. In 1986, two former Amiga designers, RJ Mical and Dave Needle, had been asked by a former manager at Amiga, Dave Morse, to design a portable gaming system. Morse now worked at Epyx, a game software company with a recent string of hit games. Morse's son had asked him if he could make a portable gaming system, prompting a meeting with Mical and Needle to discuss the idea. Morse convinced Mical and Needle and they were hired by Epyx to be a part of the design team. Planning and design of the console began in 1986 and was completed in 1987. Epyx first showed the Handy system at the Winter Consumer Electronics Show (CES) in January 1989. Facing financial difficulties, Epyx sought partners. Nintendo, Sega, and other companies declined, but Atari and Epyx eventually agreed that Atari would handle production and marketing, and Epyx would handle software development. Epyx declared bankruptcy by the end of the year, so Atari essentially owned the entire project. Both Atari and others had to purchase Amigas from Atari arch-rival Commodore in order to develop Lynx software.\nThe Handy was designed to run games from the cartridge format, and the game data must be copied from ROM to RAM before it can be used. Thus, less RAM is then available and each game's initial loading is slow. There are trace remnants of a cassette tape interface physically capable of being programmed to read a tape. Lynx developers have noted that \"there is still reference of the tape and some hardware addresses\" and an updated vintage Epyx manual describes the bare existence of what could be utilized for tape support. A 2009 retrospective interview with Mical clarifies that there is no truth to some early reports claiming that games were loaded from tape, and elaborates, \"We did think about hard disk a little.\"\nThe networking system was originally developed to run over infrared links and codenamed RedEye. This was changed to a cable-based networking system before the final release as the infrared beam was too easily interrupted when players walked through the beam, according to Peter Engelbrite. Engelbrite developed the first recordable eight-player co-op game, and the only eight-player game for the Lynx, \"Todd's Adventures in Slime World\".\nAtari changed the internal speaker and removed the thumb stick on the control pad. At Summer 1989 CES, Atari's press demonstration included the \"Portable Color Entertainment System\", which was changed to \"Lynx\" when distributed to resellers, initially retailing in the US at .\nIts launch was successful. Atari reported that it had sold 90% of the 50,000 units shipped in the launch month in the U.S. with a limited launch in New York. US sales in 1990 were approximately 500,000 units according to the Associated Press. In late 1991, it was reported that Atari sales estimates were about 800,000, which Atari claimed was within its expected projections. Lifetime sales by 1995 amount to fewer than 7\u00a0million units when combined with the Game Gear. In comparison, 16 million Game Boy units were sold by 1995 because of its superior durability, pricing, battery life, and game library, notably the pack-in hit \"Tetris\".\nAs with the console units, the game cartridge design evolved over the first year of the console's release. The first generation of cartridges are flat, and designed to be stackable for ease of storage. However, this design proved to be very difficult to remove from the console and was replaced by a second design. This style, called \"tabbed\" or \"ridged\", adds two small tabs on the underside to aid in removal. The original flat style cartridges can be stacked on top of the newer cartridges, but the newer cartridges can not be easily stacked on each other, nor were they stored easily. Thus a third style, the \"curved lip\" style was produced, and all official and third-party cartridges during the console's lifespan were released (or re-released) using this style.\nIn May 1991, Sega launched its Game Gear portable gaming handheld with a color screen. In comparison to the Lynx it had shorter battery life (3\u20134 hours as opposed to 4-5 for the Lynx), but it is slightly smaller, has significantly more games, and cost $30 less than the Lynx at launch.\nRetailers such as Game and Toys \"R\" Us continued to sell the Lynx well into the mid-1990s on the back of the Atari Jaguar launch, helped by magazines such as \"Ultimate Future Games\" which continued to cover the Lynx alongside the new generation of 32-bit and 64-bit consoles.\nLynx II.\nIn July 1991, Atari introduced a new version of the Lynx, internally called the \"Lynx II\", with a new marketing campaign, new packaging, slightly improved hardware, better battery life, and a sleeker look. It has rubber hand grips and a clearer backlit color screen with a power save option (which turns off the backlighting). The monaural headphone jack of the original Lynx was replaced with one wired for stereo. The Lynx II was available without any accessories, dropping the price to .\nDecline.\nIn 1993, Atari started shifting its focus away from the Lynx in order to prepare for the launch of the Jaguar; a few games were released during that time, including \"Battlezone 2000\". Support for the Lynx was formally discontinued in 1995.\nAfter the respective launches of the Sega Saturn and Sony PlayStation caused the commercial failure of the Jaguar, Atari ceased all game development and hardware manufacturing by early 1996 and would later merge with JTS, Inc. on July 30 of that year.\nFeatures.\nThe Atari Lynx has a backlit color LCD display, switchable right- and left-handed (upside down) configuration, and the ability to network with other units via Comlynx cable. The maximum stable connection allowed is eight players. Each Lynx needs a copy of the game, and one cable can connect two machines. The cables can be connected into a chain.\nThe Lynx was cited as the \"first gaming console with hardware support for zooming and distortion of sprites\". With a 4096 color palette and integrated maths and graphics co-processors (including a sprite engine unit), its color graphics display was said to be the key defining feature in the system's competition against Nintendo's monochromatic Game Boy. The fast pseudo-3D graphics features were made possible on a minimal hardware system by co-designer Dave Needle having \"invented the technique for planar expansion/shrinking capability\" and using stretched triangles instead of full polygons.\nLegacy.\nTelegames released several games in the late 1990s, including a port of \"Raiden\" and a platformer called \"Fat Bobby\" in 1997, and an action sports game called \"Hyperdrome\" in 1999.\nOn March 13, 1998, nearly three years after the Lynx's discontinuation, JTS Corporation sold all of the Atari assets to Hasbro Interactive for $5 million. On May 14, 1999, Hasbro, which held on to those properties until selling Hasbro Interactive to Infogrames in 2001, released into the public domain all rights to the Jaguar, opening up the platform for anyone to publish software on without Hasbro's interference. Internet theories say that the Lynx's rights may have been released to the public at the same time as the Jaguar, but this is clearly disputed. Nevertheless, since discontinuation, the Lynx, like the Jaguar, has continued to receive support from a grassroots community which would go on to produce many successful homebrew games such as \"T-Tris\" (the first Lynx game with a save-game feature), \"Alpine Games\", and \"Zaku\".\nIn 2008, Atari was honored at the 59th Annual Technology &amp; Engineering Emmy Awards for pioneering the development of handheld games with the Lynx.\nIn 2022, the compilation \"Atari 50\" released with a handful of popular Lynx titles, marking the first time that classic Lynx software would be officially rereleased by Atari."}
{"id": "2784", "revid": "45984039", "url": "https://en.wikipedia.org/wiki?curid=2784", "title": "Ahimsa", "text": " (, IAST: , ) is the ancient Indian principle of nonviolence which applies to actions towards all living beings. It is a key virtue in Indian religions like Jainism, Buddhism, Hinduism, and Sikhism.\n (also spelled Ahinsa) is one of the cardinal virtues of Jainism, where it is the first of the Pancha Mahavrata. It is also one of the central precepts of Hinduism and is the first of the five precepts of Buddhism. is inspired by the premise that all living beings have the spark of the divine spiritual energy; therefore, to hurt another being is to hurt oneself. \n is also related to the notion that all acts of violence have karmic consequences. While ancient scholars of Brahmanism had already investigated and refined the principles of \n, the concept reached an extraordinary development in the ethical philosophy of Jainism. Mahavira, the twenty-fourth and the last of Jainism, further strengthened the idea in . About , Valluvar emphasized and moral vegetarianism as virtues for an individual, which formed the core of his teachings in the Kural. Perhaps the most popular advocate of the principle of in modern times was Mahatma Gandhi.\n's precept that humans should 'cause no injury' to another living being includes one's deeds, words, and thoughts. Classical Hindu texts like the Mahabharata and the Ramayana, as well as modern scholars, disagree about what the principle of dictates when one is faced with war and other situations that require self-defence. In this way, historical Indian literature has contributed to modern theories of just war and self-defence.\nEtymology.\nThe word \u2014sometimes spelled \u2014is derived from the Sanskrit root , meaning to strike; is injury or harm, while (prefixed with the alpha privative), its opposite, is \"non-harming\" or \"nonviolence\".\nOrigins.\nReverence for can be found in Jain, Hindu, and Buddhist canonical texts. Lord Parshvanatha (the 23rd of 24 Tirthankaras of Jainism) is said to have preached as one of the four vows. No other Indian religion has developed the non-violence doctrine and its implications on everyday life as much as has Jainism.\nHinduism.\nAncient Vedic texts.\n as an ethical concept evolved in the Vedic texts. The oldest scriptures indirectly mention . Over time, the Hindu scripts revised ritual practices, and the concept of was increasingly refined and emphasized until became the highest virtue by the late Vedic era (about ). For example, hymn 10.22.25 in the Rig Veda uses the words (truthfulness) and in a prayer to deity Indra; later, the Yajur Veda dated to be between and , states, \"may all beings look at me with a friendly eye, may I do likewise, and may we look at each other with the eyes of a friend\".\nThe term appears in the text Taittiriya Shakha of the Yajurveda (TS 5.2.8.7), where it refers to non-injury to the sacrificer himself. It occurs several times in the \"Shatapatha Brahmana\" in the sense of \"non-injury\". The doctrine is a late Vedic era development in Brahmanical culture. The earliest reference to the idea of non-violence to animals (), apparently in a moral sense, is in the Kapisthala Katha Samhita of the Yajurveda (KapS 31.11), which may have been written in about .\nJohn Bowker states the word appears but is uncommon in the principal Upanishads. Kaneda gives examples of the word in these Upanishads. Other scholars suggest as an ethical concept started evolving in the Vedas, becoming an increasingly central concept in Upanishads.\nThe Ch\u0101ndogya Upani\u1e63ad, dated to , one of the oldest Upanishads, has the earliest evidence for the Vedic era use of the word in the sense familiar in Hinduism (a code of conduct). It bars violence against \"all creatures\" (), and the practitioner of is said to escape from the cycle of rebirths (CU 8.15.1). Some scholars state that this mention may have been an influence of Jainism on Vedic Hinduism. Others scholar state that this relationship is speculative, and though Jainism is an ancient tradition the oldest traceable texts of Jainism tradition are from many centuries after the Vedic era ended.\nCh\u0101ndogya Upani\u1e63ad also names , along with (truthfulness), (sincerity), (charity), and (penance/meditation), as one of five essential virtues (CU 3.17.4).\nThe Sandilya Upanishad lists ten forbearances: , , , , , , , , , and . According to Kaneda, the term is an important spiritual doctrine shared by Hinduism, Buddhism, and Jainism. It means 'non-injury' and 'non-killing'. It implies the total avoidance of harming any living creature by deeds, words, and thoughts.\nThe Epics.\nThe Mahabharata, one of the epics of Hinduism, has multiple mentions of the phrase (), which literally means: non-violence is the highest moral virtue. For example, Anushasana Parva has the verse:\n&lt;poem&gt;\n&lt;/poem&gt;\nThe above passage from Mahabharata emphasises the cardinal importance of in Hinduism, and literally means:\n&lt;poem&gt;\n is the highest , is the highest self-control, \n is the greatest gift, is the best practice, \n is the highest sacrifice, is the finest strength, \n is the greatest friend, is the greatest happiness, \n is the highest truth, and is the greatest teaching.\n&lt;/poem&gt;\nSome other examples where the phrase are discussed include Adi Parva, Vana Parva, and Anushasana Parva. The Bhagavad Gita, among other things, discusses the doubts and questions about appropriate response when one faces systematic violence or war. These verses develop the concepts of lawful violence in self-defence and the theories of just war. However, there is no consensus on this interpretation. Gandhi, for example, considers this debate about non-violence and lawful violence as a mere metaphor for the internal war within each human being, when he or she faces moral questions.\nSelf-defence, criminal law, and war.\nThe classical texts of Hinduism devote numerous chapters to discussing what people who practice the virtue of can and must do when faced with war, violent threat, or the need to sentence someone convicted of a crime. These discussions have led to theories of just war, ideas of reasonable self-defense, and views of proportionate punishment. Arthashastra discusses, among other things, what constitutes proportionate response and punishment.\nThe precepts of in Hinduism require that war must be avoided, with sincere and truthful dialogue. Force must be the last resort. If war becomes necessary, its cause must be just, its purpose virtuous, its objective to restrain the wicked, its aim peace, and its method lawful. War can only be started and stopped by a legitimate authority. Weapons must be proportionate to the opponent and the aim of war, not indiscriminate tools of destruction. All strategies and weapons used in the war must be to defeat the opponent, not to cause misery to the opponent; for example, the use of arrows is allowed, but the use of arrows smeared with painful poison is not allowed. Warriors must use judgment in the battlefield. Cruelty to the opponent during war is forbidden. Wounded, unarmed opponent warriors must not be attacked or killed; they must be brought to your realm and given medical treatment. Children, women, and civilians must not be injured. While the war is in progress, sincere dialogue for peace must continue.\nDifferent interpretations of ancient Hindu texts have been offered in matters of self-defense. For example, T\u00e4htinen suggests self-defense is appropriate, criminals are not protected by the rule of , and Hindu scriptures support violence against an armed attacker. is not meant to imply pacifism.\nAlternative theories of self-defense, inspired by , build principles similar to ideas of just war. Aikido, pioneered in Japan, illustrates one such set of principles for self-defense. Morihei Ueshiba, the founder of Aikido, described his inspiration as Ahimsa. According to this interpretation of in self-defense, one must not assume that the world is free of aggression. One must presume that some people will, out of ignorance, error, or fear, attack others or intrude into their space, physically or verbally. The aim of self-defense, suggested Ueshiba, must be to neutralize the attacker's aggression and avoid conflict. The best defense is one with which the victim is protected and the attacker is respected and not injured if possible. Under and Aikido, there are no enemies, and appropriate self-defense focuses on neutralizing the immaturity, assumptions, and aggressive strivings of the attacker.\nT\u00e4htinen concludes that Hindus have no misgivings about the death penalty; their position is that evil-doers who deserve death should be killed and that a king, in particular, is obliged to punish criminals and should not hesitate to kill them, even if they happen to be his brothers and sons.\nOther scholars conclude that Hindu scriptures suggest that sentences for any crime must be fair, proportional, and not cruel.\nNon-human life.\nThe Hindu precept of \"cause no injury\" applies to animals and all life forms. This precept is not found in the oldest verses of Vedas (), but increasingly becomes one of the central ideas in post-Vedic period. In the oldest layer of the Vedas, such as the \"Rigveda\", ritual sacrifices of animals and cooking of meat to feed guests are mentioned. This included goat, ox, horse, and others. However, the text is not uniform in its prescriptions. Some verses praise meat as food, while other verses in the Vedas recommend \"abstention from meat\", in particular, \"beef\". According to Marvin Harris, the Vedic literature is inconsistent, with some verses suggesting ritual slaughter and meat consumption, while others suggesting a taboo on meat-eating.\nHindu texts dated to initially mention meat as food, then evolve to suggest that only meat obtained through ritual sacrifice can be eaten, thereafter evolving to the stance that one should eat no meat because it hurts animals, with verses describing the noble life as one that lives on flowers, roots, and fruits alone. The late Vedic-era literature () condemns all killings of men, cattle, birds, and horses, and prays to god Agni to punish those who kill.\nLater texts of Hinduism declare as one of the primary virtues, declare any killing or harming any life as against (moral life). Finally, the discussion in the Upanishads and Hindu Epics shifts to whether a human being can ever live his or her life without harming animal and plant life in some way, which and when plants or animal meat may be eaten, whether violence against animals causes human beings to become less compassionate, and if and how one may exert least harm to non-human life consistent with , given the constraints of life and human needs. The Mahabharata permits hunting by warriors, but opposes it in the case of hermits who must be strictly non-violent. Sushruta Samhita, a Hindu text written in , in Chapter XLVI suggests proper diet as a means of treating certain illnesses, and recommends various fishes and meats for different ailments and for pregnant women, and the Charaka Samhita describes meat as superior to all other kinds of food for convalescents.\nAcross the texts of Hinduism, there is a profusion of ideas about the virtue of when applied to non-human life, but without a universal consensus. Alsdorf claims the debate and disagreements between supporters of vegetarian lifestyle and meat eaters was significant. Even suggested exceptions \u2013 ritual slaughter and hunting \u2013 were challenged by advocates of . In the Mahabharata both sides present various arguments to substantiate their viewpoints. Moreover, a hunter defends his profession in a long discourse.\nMany of the arguments proposed in favor of non-violence to animals refer to the bliss one feels, the rewards it entails before or after death, the danger and harm it prevents, as well as to the karmic consequences of violence.\nThe ancient Hindu texts discuss and non-animal life. They discourage wanton destruction of nature including of wild and cultivated plants. Hermits (sannyasins) were urged to live on a fruitarian diet so as to avoid the destruction of plants. Scholars claim the principles of ecological nonviolence are innate in the Hindu tradition, and its conceptual fountain has been as its cardinal virtue.\nThe classical literature of the Indian religions, such as Hinduism and Jainism, exists in many Indian languages. For example, the \"Tirukkural,\" written in three volumes, likely between , dedicates verses 251\u2013260 and 321\u2013333 of its first volume to the virtue of , emphasizing on moral vegetarianism and non-killing (). However, the \"Tirukkural\" also glorifies soldiers and their valour during war, and states that it is king's duty to punish criminals and implement \"death sentence for the wicked\".\nIn 1960, H. Jay Dinshah founded the American Vegan Society (AVS), linking veganism to the concept of .\nModern times.\nIn the 19th and 20th centuries, prominent figures of Indian spirituality such as Shrimad Rajchandra and Swami Vivekananda emphasised the importance of Ahimsa.\nMohandas Karamchand Gandhi successfully promoted the principle of to all spheres of life, in particular to politics (). His non-violent resistance movement had an immense impact on India, impressed public opinion in Western countries, and influenced the leaders of various civil and political rights movements such as the American civil rights movement's Martin Luther King Jr. and James Bevel. In Gandhi's thought, precludes not only the act of inflicting a physical injury but also mental states like evil thoughts and hatred, and unkind behavior such as harsh words, dishonesty, and lying, all of which he saw as manifestations of violence incompatible with . Gandhi believed to be a creative energy force, encompassing all interactions leading one's self to find , \"Divine Truth\". Sri Aurobindo criticized the Gandhian concept of as unrealistic and not universally applicable; he adopted a pragmatic non-pacifist position, saying that the justification of violence depends on the specific circumstances of the given situation.\nGandhi took the religious principle of \"ahimsa,\" and turned it into a non-violent tool for mass action. He used it to fight not only colonial rule, but social evils such as racial discrimination and untouchability as well.\nGandhi stated his belief that \" is in Hinduism, it is in Christianity as well as in Islam.\" He added, \"Nonviolence is common to all religions, but it has found the highest expression and application in Hinduism (I do not regard Jainism or Buddhism as separate from Hinduism).\" When questioned whether violence and nonviolence are taught in Quran, he stated, \"I have heard from many Muslim friends that the Koran teaches the use of nonviolence. (...\u00a0The) argument about nonviolence in the Holy Koran is an interpolation, not necessary for my thesis.\"\nStudying 's history and philosophy influenced Albert Schweitzer's principle of \"reverence for life\". He commended Indian traditions for their ethics of , considering the prohibition against killing and harming \"one of the greatest events in the spiritual history of humankind\". However, he noted that \"not-killing\" and \"not-harming\" might be unfeasible in certain situations, like self-defense, or ethically complex, as in cases of prolonged famine.\nYoga.\n is imperative for practitioners of Pata\u00f1jali's eight limb Raja yoga system. It is included in the first limb and is the first of five (self restraints) which, together with the second limb, make up the code of ethical conduct in Yoga philosophy. Commentators on the Yoga Sutras II.30 emphasize that \"ahimsa\" is the most important and foundational \"yama\" of the five \"yamas\". Vijnanabhiksu uses the analogy of an elephant to convey its importance, while Vyasa defines it as refraining from harming any living being at any time, emphasizing that all other \"yamas\" support and purify \"ahimsa\".\n is also one of the ten in Hatha Yoga according to verse 1.1.17 of its classic manual \"Hatha Yoga Pradipika\". The significance of as the first restraint in the first limb of Yoga (), is that it defines the necessary foundation for progress through Yoga. It is a precursor to , implying that success in can be had only if the self is purified in thought, word, and deed through the self-restraint of .\nJainism.\nIn Jainism, the understanding and implementation of is more radical, scrupulous, and comprehensive than in any other religion. Killing any living being out of passions like attachment is considered (to injure) and abstaining from such an act is (noninjury). The vow of is considered the foremost among the \"five vows of Jainism\". Other vows like truth () are meant for safeguarding the vow of .\nIn the practice of , the requirements are less strict for the lay persons () who have undertaken (Smaller Vows) than for the Jain monastics who are bound by the Mahavrata \"Great Vows\".\nThe statement (or, \"Non-injury/nonviolence/harmlessness is the supreme/ultimate/paramount/highest/absolute duty/virtue/attribute/religion\") is often found inscribed on the walls of the Jain temples. As in Hinduism, the aim is to prevent the accumulation of harmful karma.\nWhen Mahavira revived and reorganised the Jain faith in , was already an established, strictly observed rule. Rishabhanatha (\u0100din\u0101tha), the first Jain Tirthankara, whom modern Western historians consider to be a historical figure, followed by Parshvanatha (P\u0101r\u015bvan\u0101tha) the twenty-third Tirthankara lived in about . He founded the community to which Mahavira's parents belonged. Ahimsa was already part of the \"Fourfold Restraint\" (\"Caujjama\"), the vows taken by Parshva's followers. In the times of Mahavira and in the following centuries, Jains were at odds with both Buddhists and followers of the Vedic religion or Hindus, whom they accused of negligence and inconsistency in the implementation of . According to the Jain tradition either lacto vegetarianism or veganism is prescribed.\nThe Jain concept of is characterised by several aspects. Killing of animals for food is absolutely ruled out. Jains also make considerable efforts not to injure plants in everyday life as far as possible. Though they admit that plants must be destroyed for the sake of food, they accept such violence only inasmuch as it is indispensable for human survival, and there are special instructions for preventing unnecessary violence against plants. Jain monks and nuns go out of their way so as not to hurt even small insects and other minuscule animals. Both the renouncers and the laypeople of Jain faith reject meat, fish, alcohol, and honey as these are believed to harm large or minuscule life forms.\nJain scholars have debated the potential injury to other life forms during one's occupation. Certain Jain texts (according to Padmannabh Jaini, a Jainism scholar) forbid people of its faith from husbandry, agriculture, and trade in animal-derived products. Some Jains abstain from farming because it inevitably entails unintentional killing or injuring of many small animals, such as worms and insects. These teachings, in part, have led the Jain community to focus on trade, merchant, clerical, and administrative occupations to minimize (occupational violence against all life forms). For the layperson, the teaching has been of with \u2013 that is, reducing violence through proper intention and being careful in every action on a daily basis to minimize violence to all life forms.\nThe Jain texts, unlike most Hindu and Buddhist texts on just war, have been inconsistent. For its monastic community \u2013 and \u2013 the historically accepted practice has been to \"willingly sacrifice one's own life\" to the attacker, to not retaliate, so that the mendicant may keep the First Great Vow of \"total nonviolence\". Jain literature of , for example, describes a king ready for war and being given lessons about non-violence by the Jain acharya (spiritual teacher). In and thereafter, in an era of violent raids, destruction of temples, the slaughter of agrarian communities and ascetics by Islamic armies, Jain scholars reconsidered the First Great Vow of mendicants and its parallel for the laypeople. The medieval texts of this era, such as by Jinadatta Suri, recommended both the mendicants and the laypeople to fight and kill if that would prevent greater and continued violence on humans and other life forms (). Such exemptions to is a relatively rare teaching in Jain texts, states Dundas.\nMahatma Gandhi stated, \"No religion in the World has explained the principle of so deeply and systematically as is discussed with its applicability in every human life in Jainism. As and when the benevolent principle of or non-violence will be ascribed for practice by the people of the world to achieve their end of life in this world and beyond, Jainism is sure to have the uppermost status and Mah\u0101v\u012bra is sure to be respected as the greatest authority on \".\nBuddhism.\nIn Buddhist texts (or its P\u0101li cognate ) is part of the Five Precepts (), the first of which has been to abstain from killing. This precept of is applicable to both the Buddhist layperson and the monastic community.\nThe precept is not a commandment, and transgressions did not for laypersons, but their power has been in the Buddhist belief in karmic consequences and their impact in afterlife during rebirth. Killing, in Buddhist belief, could lead to rebirth in the hellish realm, and for a longer time in more severe conditions if the murder victim was a monk. Saving animals from slaughter for meat is believed to be a way to acquire merit for better rebirth. These moral precepts have been voluntarily self-enforced in lay Buddhist culture through the associated belief in karma and rebirth. Buddhist texts not only recommend , but suggest avoiding trading goods that contribute to or are a result of violence:\nUnlike with lay Buddhists, transgressions by monks do invite sanctions. Full expulsion of a monk from follows instances of killing, just like any other serious offense against the monastic code of conduct.\nWar.\nViolent ways of punishing criminals and prisoners of war were not explicitly condemned in Buddhism, but peaceful ways of conflict resolution and punishment with the least amount of injury were encouraged. The early texts condemn the mental states that lead to violent behavior.\nNonviolence is an overarching theme within the P\u0101li Canon. While the early texts condemn killing in the strongest terms, and portray the ideal ruler as a pacifist, such a ruler is nonetheless flanked by an army. It seems that the Buddha's teaching on nonviolence was not interpreted or put into practice in an uncompromisingly pacifist or anti-military service way by early Buddhists. The early texts assume war to be a fact of life, and well-skilled soldiers are viewed as necessary for defensive warfare. In Pali texts, injunctions to abstain from violence and involvement with military affairs are directed at members of the ; later Mahayana texts, which often generalise monastic norms to laity, require this of lay people as well.\nThe early texts do not contain just-war ideology as such. Some argue that a in the \"Gamani Samyuttam\" rules out all military service. In this passage, a soldier asks the Buddha if it is true that, as he has been told, soldiers slain in battle are reborn in a heavenly realm. The Buddha reluctantly replies that if he is killed in battle while his mind is seized with the intention to kill, he will undergo an unpleasant rebirth. In the early texts, a person's mental state at the time of death is generally viewed as having a great impact on the next birth.\nSome Buddhists point to other early texts as justifying defensive war. One example is the \"Kosala Samyutta\", in which King Pasenadi of Kosala, a righteous king favored by the Buddha, learns of an impending attack on his kingdom. He arms himself in defence, and leads his army into battle to protect his kingdom from attack. He lost this battle but won the war. King Pasenadi eventually defeated Emperor Aj\u0101tasattu and captured him alive. He thought that, although this King of Magadha has transgressed against his kingdom, he had not transgressed against him personally, and Aj\u0101tasattu was still his nephew. He released Aj\u0101tasattu and did not harm him. Upon his return, the Buddha said (among other things) that Pasenadi \"is a friend of virtue, acquainted with virtue, intimate with virtue\", while the opposite is said of the aggressor, King Aj\u0101tasattu.\nAccording to Theravada commentaries, there are five requisite factors that must all be fulfilled for an act to be both an act of killing and to be karmically negative. These are: (1) the presence of a living being, human or animal; (2) the knowledge that the being is a living being; (3) the intent to kill; (4) the act of killing by some means; and (5) the resulting death. Some Buddhists have argued on this basis that the act of killing is complicated, and its ethicality is predicated upon intent. Some have argued that in defensive postures, for example, the primary intention of a soldier is not to kill, but to defend against aggression, and the act of killing in that situation would have minimal negative karmic repercussions.\nAccording to Babasaheb Ambedkar, there is circumstantial evidence encouraging from the Buddha's doctrine, \"Love all, so that you may not wish to kill any.\" Gautama Buddha distinguished between a principle and a rule. He did not make a matter of rule, but suggested it as a matter of principle. This gives Buddhists freedom to act.\nLaws.\nMaurya Emperor Ashoka banned animal sacrifice, hunting, slaughter of \"all four-footed creatures that are neither useful nor edible\" and specific animal species, female goats, sheep and pigs nursing their young as well as their young up to the age of six months. Fishing was banned during Chaturmasya and Uposatha. Slave trade in the Maurya Empire was also banned by Ashoka.\nThe emperors of the Sui dynasty, Tang dynasty, and early Song dynasty banned killing in the Lunar calendar's 1st, 5th, and 9th months. Empress Wu Tse-Tien banned killing for more than half a year in 692. Some rulers banned fishing for a period of time each year.\nThere were also bans after the death of emperors, after Buddhist and Taoist prayers, and after natural disasters such as Shanghai's 1926 summer drought, as well as an eight-day ban beginning August 12, 1959, after the August 7 flood (), the last big flood before the 88 Taiwan Flood.\nPeople avoid killing during some festivals, like the Taoist Ghost Festival, the Nine Emperor Gods Festival, and the Vegetarian Festival, as well as during others.\nReferences.\nSources.\nAttribution:"}
{"id": "2785", "revid": "28916033", "url": "https://en.wikipedia.org/wiki?curid=2785", "title": "Annals of Mathematics", "text": "The Annals of Mathematics is a mathematical journal published every two months by Princeton University and the Institute for Advanced Study.\nHistory.\nThe journal was established as \"The Analyst\" in 1874 and with Joel E. Hendricks as the founding editor-in-chief. It was \"intended to afford a medium for the presentation and analysis of any and all questions of interest or importance in pure and applied Mathematics, embracing especially all new and interesting discoveries in theoretical and practical astronomy, mechanical philosophy, and engineering\". It was published in Des Moines, Iowa, and was the earliest American mathematics journal to be published continuously for more than a year or two. This incarnation of the journal ceased publication after its tenth year, in 1883, giving as an explanation Hendricks' declining health, but Hendricks made arrangements to have it taken over by new management, and it was continued from March 1884 as the \"Annals of Mathematics\". The new incarnation of the journal was edited by Ormond Stone (University of Virginia). It moved to Harvard in 1899 before reaching its current home in Princeton in 1911.\nAn important period for the journal was 1928\u20131958 with Solomon Lefschetz as editor. Norman Steenrod characterized Lefschetz' impact as editor as follows: \"The importance to American mathematicians of a first-class journal is that it sets high standards for them to aim at. In this somewhat indirect manner, Lefschetz profoundly affected the development of mathematics in the United States.\"\nPrinceton University continued to publish the \"Annals\" on its own until 1933, when the Institute for Advanced Study took joint editorial control. Since 1998, it has been available in an electronic edition, alongside its regular print edition. The electronic edition was available without charge, as an open access journal, but since 2008, this is no longer the case. Issues from before 2003 were transferred to the non-free JSTOR archive, and articles are not freely available until 5 years after publication.\nAbstracting and indexing.\nThe journal is abstracted and indexed in the Science Citation Index, Current Contents/Physical, Chemical &amp; Earth Sciences, and Scopus. According to the \"Journal Citation Reports\", the journal has a 2020 impact factor of 5.246, ranking it third out of 330 journals in the category \"Mathematics\"."}
{"id": "2786", "revid": "40976397", "url": "https://en.wikipedia.org/wiki?curid=2786", "title": "Andrei Sakharov", "text": "Andrei Dmitrievich Sakharov (; 21 May 192114 December 1989) was a Soviet physicist and a Nobel Peace Prize laureate, which he was awarded in 1975 for emphasizing human rights around the world.\nAlthough he spent his career in physics in the Soviet program of nuclear weapons, overseeing the development of thermonuclear weapons, Sakharov also did fundamental work in understanding particle physics, magnetism, and physical cosmology. Sakharov is mostly known for his political activism for individual freedom, human rights, civil liberties and reforms in the Soviet Union, for which he was deemed a dissident and faced persecution from the Soviet establishment.\nIn his memory, the Sakharov Prize was established and is awarded annually by the European Parliament for people and organizations dedicated to human rights and freedoms.\nBiography.\nFamily background and early life.\nAndrei Dmitrievich Sakharov was born in Moscow on 21 May 1921, to a Russian family. His father, Dmitri Ivanovich Sakharov, was a physics professor at the Second Moscow State University and an amateur pianist. His grandfather, Ivan, was a lawyer in the former Russian Empire who had displayed respect for social awareness and humanitarian principles (including advocating the abolition of capital punishment). Sakharov's mother, Yekaterina Alekseevna Sofiano, was a daughter of Aleksey Semenovich Sofiano, a general in the Tsarist Russian Army with Greek heritage.\nSakharov's parents and paternal grandmother, Maria Petrovna, largely shaped his personality; his mother and grandmother were members of the Russian Orthodox Church, although his father was a non-believer. When Andrei was about thirteen, he realized that he did not believe in God. However, despite being an atheist, he did believe in a \"guiding principle\" that transcends the physical laws.\nAfter schooling, Sakharov studied physics at the Moscow State University in 1938 and, following evacuation in 1941 during the Eastern Front with Germany, he graduated in A\u015fgabat in Turkmenistan. In 1943, he married Klavdia Alekseyevna Vikhireva, with whom he raised two daughters and a son. Klavdia would later die in 1969. In 1945, he joined the Theoretical Department of Physical Institute of the Russian Academy of Sciences under Igor Tamm in Moscow. In 1947, Sakharov was successful in defending his thesis for the Doctor of Sciences (lit. \"Doktor Nauk\"), which covered the topic of nuclear transmutation.\nSoviet program of nuclear weapons.\nAfter World War II, he researched cosmic rays. In mid-1948 he participated in the Soviet atomic bomb project under Igor Kurchatov and Igor Tamm. Sakharov's study group at FIAN in 1948 came up with a second concept in August\u2013September 1948. Adding a shell of natural, unenriched uranium around the deuterium would increase the deuterium concentration at the uranium-deuterium boundary and the overall yield of the device, because the natural uranium would capture neutrons and itself fission as part of the thermonuclear reaction. This idea of a layered fission-fusion-fission bomb led Sakharov to call it the \"sloika\", or layered cake. The first Soviet atomic device was tested on August 29, 1949. After moving to Sarov in 1950, Sakharov played a key role in the development of the first megaton-range Soviet hydrogen bomb using a design known as \"Sakharov's Third Idea\" in Russia and the Teller\u2013Ulam design in the United States. Before his \"Third Idea\", Sakharov tried a \"layer cake\" of alternating layers of fission and fusion fuel. The results were disappointing, yielding no more than a typical fission bomb. However the design was seen to be worth pursuing because deuterium is abundant and uranium is scarce, and he had no idea how powerful the US design was. Sakharov realised that in order to cause the explosion of one side of the fuel to symmetrically compress the fusion fuel, a mirror could be used to reflect the radiation. The details had not been officially declassified in Russia when Sakharov was writing his memoirs, but in the Teller\u2013Ulam design, soft X-rays emitted by the fission bomb were focused onto a cylinder of lithium deuteride to compress it symmetrically. This is called radiation implosion. The Teller\u2013Ulam design also had a secondary fission device inside the fusion cylinder to assist with the compression of the fusion fuel and generate neutrons to convert some of the lithium to tritium, producing a mixture of deuterium and tritium. Sakharov's idea was first tested as RDS-37 in 1955. A larger variation of the same design which Sakharov worked on was the 50\u00a0Mt Tsar Bomba of October 1961, which was the most powerful nuclear device ever detonated.\nSakharov saw \"striking parallels\" between his fate and those of J. Robert Oppenheimer and Edward Teller in the US. Sakharov believed that in this \"tragic confrontation of two outstanding people\", both deserved respect, because \"each of them was certain he had right on his side and was morally obligated to go to the end in the name of truth.\" While Sakharov strongly disagreed with Teller over nuclear testing in the atmosphere and the Strategic Defense Initiative, he believed that American academics had been unfair to Teller's resolve to get the H-bomb for the United States since \"all steps by the Americans of a temporary or permanent rejection of developing thermonuclear weapons would have been seen either as a clever feint, or as the manifestation of stupidity. In both cases, the reaction would have been the same \u2013 avoid the trap and immediately take advantage of the enemy's stupidity.\"\nSakharov never felt that by creating nuclear weapons he had \"known sin\", in Oppenheimer's expression. He later wrote:\nSupport for peaceful use of nuclear technology.\nIn 1950 he proposed an idea for a controlled nuclear fusion reactor, the tokamak, which is still the basis for the majority of work in the area. Sakharov, in association with Tamm, proposed confining extremely hot ionized plasma by torus shaped magnetic fields for controlling thermonuclear fusion that led to the development of the tokamak device.\nMagneto-implosive generators.\nIn 1951 he invented and tested the first explosively pumped flux compression generators, compressing magnetic fields by explosives. He called these devices MK (for \"MagnetoKumulative\") generators. The radial MK-1 produced a pulsed magnetic field of 25 megagauss (2500 teslas). The resulting helical MK-2 generated 1000 million amperes in 1953.\nSakharov then tested a MK-driven \"plasma cannon\" where a small aluminum ring was vaporized by huge eddy currents into a stable, self-confined toroidal plasmoid and was accelerated to 100\u00a0km/s. Sakharov later suggested replacing the copper coil in MK generators with a large superconductor solenoid to magnetically compress and focus underground nuclear explosions into a shaped charge effect. He theorized this could focus 1023 protons per second on a 1\u00a0mm2 surface.\nParticle physics and cosmology.\nAfter 1965 Sakharov returned to fundamental science and began working on particle physics and physical cosmology.\nHe tried to explain the baryon asymmetry of the universe; in that regard, he was the first to give a theoretical motivation for proton decay. Proton decay was suggested by Wigner in 1949 and 1952.\nProton decay experiments had been performed since 1954 already. Sakharov was the first to consider CPT-symmetric events occurring \"before\" the Big Bang:We can visualize that neutral spinless maximons (or photons) are produced at \"t\" &lt; 0 from contracting matter having an excess of antiquarks, that they pass \"one through the other\" at the instant \"t\" = 0 when the density is infinite, and decay with an excess of quarks when \"t\" &gt; 0, realizing total CPT symmetry of the universe. All the phenomena at t &lt; 0 are assumed in this hypothesis to be CPT reflections of the phenomena at t &gt; 0. His legacy in this domain are the famous conditions named after him: Baryon number violation, C-symmetry and CP-symmetry violation, and interactions out of thermal equilibrium.\nSakharov was also interested in explaining why the curvature of the universe is so small. This led him to consider cyclic models, where the universe oscillates between contraction and expansion phases. In those models, after a certain number of cycles the curvature naturally becomes infinite even if it had not started this way: Sakharov considered three starting points, a flat universe with a slightly negative cosmological constant, a universe with a positive curvature and a zero cosmological constant, and a universe with a negative curvature and a slightly negative cosmological constant. Those last two models feature what Sakharov calls a reversal of the time arrow, which can be summarized as follows: He considers times t &gt; 0 after the initial Big Bang singularity at t = 0 (which he calls \"Friedman singularity\" and denotes \u03a6) as well as times t &lt; 0 before that singularity. He then assumes that entropy increases when time increases for t &gt; 0 as well as when time decreases for t &lt; 0, which constitutes his reversal of time. Then he considers the case when the universe at t &lt; 0 is the image of the universe at t &gt; 0 under CPT symmetry but also the case when it is not so: the universe has a non-zero CPT charge at t = 0 in this case. Sakharov considers a variant of this model where the reversal of the time arrow occurs at a point of maximum entropy instead of happening at the singularity. In those models there is no dynamic interaction between the universe at t &lt; 0 and t &gt; 0.\nIn his first model the two universes did not interact, except via local matter accumulation whose density and pressure become high enough to connect the two sheets through a bridge without spacetime between them, but with a continuity of geodesics beyond the Schwarzschild radius with no singularity, allowing an exchange of matter between the two conjugated sheets, based on an idea after Igor Dmitriyevich Novikov. Novikov called such singularities a \"collapse\" and an \"anticollapse\", which are an alternative to the couple black hole and white hole in the wormhole model. Sakharov also proposed the idea of induced gravity as an alternative theory of quantum gravity.\nTurn to activism.\nSince the late 1950s Sakharov had become concerned about the moral and political implications of his work. Politically active during the 1960s, Sakharov was against nuclear proliferation. Pushing for the end of atmospheric tests, he played a role in the 1963 Partial Test Ban Treaty, signed in Moscow.\nSakharov was also involved in an event with political consequences in 1964, when the Soviet Academy of Sciences nominated for full membership Nikolai Nuzhdin, a follower of Trofim Lysenko (initiator of the Stalin-supported anti-genetics campaign Lysenkoism). Contrary to normal practice, Sakharov, a member of the academy, publicly spoke out against full membership for Nuzhdin and held him responsible for \"the defamation, firing, arrest, even death, of many genuine scientists.\" In the end, Nuzhdin was not elected, but the episode prompted Nikita Khrushchev to order the KGB to gather compromising material on Sakharov.\nThe major turn in Sakharov's political evolution came in 1967, when anti-ballistic missile defense became a key issue in US\u2013Soviet relations. In a secret detailed letter to the Soviet leadership of July 21, 1967, Sakharov explained the need to \"take the Americans at their word\" and accept their proposal for a \"bilateral rejection by the USA and the Soviet Union of the development of antiballistic missile defense\" because an arms race in the new technology would otherwise increase the likelihood of nuclear war. He also asked permission to publish his manuscript, which accompanied the letter, in a newspaper to explain the dangers posed by that kind of defense. The government ignored his letter and refused to let him initiate a public discussion of ABMs in the Soviet press.\nSince 1967, after the Six Day War and the beginning of the Arab-Israeli conflict, he actively supported Israel, as he reported more than once in the press, and also maintained friendly relations with refuseniks who later made aliyah.\nIn May 1968, Sakharov completed an essay, \"Reflections on Progress, Peaceful Coexistence, and Intellectual Freedom\". He described the anti-ballistic missile defense as a major threat of world nuclear war. After the essay was circulated in \"samizdat\" and then published outside the Soviet Union, Sakharov was banned from conducting any military-related research and returned to FIAN to study fundamental theoretical physics.\nFor 12 years, until his exile to Gorky (Nizhny Novgorod) in January 1980, Sakharov assumed the role of a widely recognized and open dissident in Moscow. He stood vigil outside closed courtrooms, wrote appeals on behalf of more than 200 individual prisoners, and continued to write essays about the need for democratization.\nIn 1970, Sakharov was among the three founding members of the Committee on Human Rights in the USSR, along with Valery Chalidze and Andrei Tverdokhlebov. The Committee wrote appeals, collected signatures for petitions and succeeded in affiliating with several international human rights organizations. Its work was the subject of many KGB reports and brought Sakharov under increasing pressure from the government.\nSakharov married a fellow human rights activist, Yelena Bonner, in 1972.\nBy 1973, Sakharov was meeting regularly with Western correspondents and holding press conferences in his apartment. He appealed to the US Congress to approve the 1974 Jackson-Vanik Amendment to a trade bill, which coupled trade tariffs to the Kremlin's willingness to allow freer emigration for Soviet Jews.\nAttacked by Soviet establishment from 1972.\nIn 1972, Sakharov became the target of sustained pressure from his fellow scientists in the Soviet Academy of Sciences and the Soviet press. The writer Aleksandr Solzhenitsyn came to his defence.\nIn 1973 and 1974, the Soviet media campaign continued, targeting both Sakharov and Solzhenitsyn for their pro-Western, anti-socialist positions.\nSakharov later described that it took \"years\" for him to \"understand how much substitution, deceit, and lack of correspondence with reality there was\" in the Soviet ideals. \"At first I thought, despite everything that I saw with my own eyes, that the Soviet State was a breakthrough into the future, a kind of prototype for all countries\". Then he came, in his words, to \"the theory of symmetry: all governments and regimes to a first approximation are bad, all peoples are oppressed, and all are threatened by common dangers.\":\nSakharov's ideas on social development led him to put forward the principle of human rights as a new basis of all politics. In his works, he declared that \"the principle 'what is not prohibited is allowed' should be understood literally\", and defied what he saw as unwritten ideological rules imposed by the Communist Party on the society in spite of a democratic Soviet Constitution (1936):\nIn a letter written from exile, he cheered up a fellow physicist and free market advocate with the words: \"Fortunately, the future is unpredictable and also \u2013 because of quantum effects \u2013 uncertain.\" For Sakharov, the indeterminacy of the future supported his belief that he could and should take personal responsibility for it.\nNobel Peace Prize (1975).\nIn 1973, Sakharov was nominated for the Nobel Peace Prize, and in 1974, he was awarded the Prix mondial Cino Del Duca.\nSakharov was awarded the Nobel Peace Prize in 1975. The Norwegian Nobel Committee called him \"a spokesman for the conscience of mankind\". In the words of the Nobel Committee's citation: \"In a convincing manner Sakharov has emphasised that Man's inviolable rights provide the only safe foundation for genuine and enduring international cooperation.\"\nSakharov was not allowed to leave the Soviet Union to collect the prize. His wife, Yelena Bonner, read his speech at the ceremony in Oslo, Norway. On the day the prize was awarded, Sakharov was in Vilnius, where the human rights activist Sergei Kovalev was being tried. In his Nobel lecture, \"Peace, Progress, Human Rights\", Sakharov called for an end to the arms race, greater respect for the environment, international cooperation, and universal respect for human rights. He included a list of prisoners of conscience and political prisoners in the Soviet Union and stated that he shared the prize with them.\nBy 1976, the head of the KGB, Yuri Andropov, was prepared to call Sakharov \"Domestic Enemy Number One\" before a group of KGB officers.\nInternal exile (1980\u20131986).\nSakharov was arrested on 22 January 1980, following his public protests against the Soviet intervention in Afghanistan in 1979, and was sent to the city of Gorky, now Nizhny Novgorod, a city that was off limits to foreigners.\nBetween 1980 and 1986, Sakharov was kept under Soviet police surveillance. In his memoirs, he mentioned that their apartment in Gorky was repeatedly subjected to searches and heists. Sakharov was named the 1980 Humanist of the Year by the American Humanist Association.\nIn May 1984, Sakharov's wife, Yelena Bonner, was detained, and Sakharov began a hunger strike, demanding permission for his wife to travel to the United States for heart surgery. He was forcibly hospitalized and force-fed. He was held in isolation for four months. In August 1984, Bonner was sentenced by a court to five years of exile in Gorky.\nIn April 1985, Sakharov started a new hunger strike for his wife to travel abroad for medical treatment. He again was taken to a hospital and force-fed. In August, the Politburo discussed what to do about Sakharov. He remained in the hospital until October 1985, when his wife was allowed to travel to the United States. She had heart surgery in the United States and returned to Gorky in June 1986.\nIn December 1985, the European Parliament established the Sakharov Prize for Freedom of Thought, to be given annually for outstanding contributions to human rights.\nOn 19 December 1986, Mikhail Gorbachev, who had initiated the policies of perestroika and glasnost, called Sakharov to tell him that he and his wife could return to Moscow.\nPolitical leader.\nIn 1988, Sakharov was given the International Humanist Award by the International Humanist and Ethical Union. He helped to initiate the first independent legal political organizations and became prominent in the Soviet Union's growing political opposition. In March 1989, Sakharov was elected to the new parliament, the All-Union Congress of People's Deputies and co-led the democratic opposition, the Inter-Regional Deputies Group. In November the head of the KGB reported to Gorbachev on Sakharov's encouragement and support for the coal miners' strike in Vorkuta.\nIn December 1988, Sakharov visited Armenia and Azerbaijan on a fact-finding mission. He concluded, \"For Azerbaijan the issue of Karabakh is a matter of ambition, for the Armenians of Karabakh, it is a matter of life and death\".\nDeath.\nSoon after 9 p.m. on 14 December 1989, Sakharov went to his study to take a nap before preparing an important speech he was to deliver the next day in the Congress. His wife went to wake him at 11 p.m. as he had requested but she found Sakharov dead on the floor. According to the notes of Yakov Rapoport, a senior pathologist present at the autopsy, it is most likely that Sakharov died of an arrhythmia consequent to dilated cardiomyopathy at the age of 68. He was interred in the Vostryakovskoye Cemetery in Moscow.\nInfluence.\nMemorial prizes.\nThe Sakharov Prize for Freedom of Thought was established in 1988 by the European Parliament in his honour, and is the highest tribute to human rights endeavours awarded by the European Union. It is awarded annually by the parliament to \"those who carry the spirit of Soviet dissident Andrei Sakharov\"; to \"Laureates who, like Sakharov, dedicate their lives to peaceful struggle for human rights.\"\nAn Andrei Sakharov prize has also been awarded by the American Physical Society every second year since 2006 \"to recognize outstanding leadership and/or achievements of scientists in upholding human rights\".\nThe Andrei Sakharov Prize for Writer's Civic Courage was established in October 1990.\nIn 2004, with the approval of Yelena Bonner, an annual Sakharov Prize for journalism was established for reporters and commentators in Russia. Funded by former Soviet dissident Pyotr Vins, now a businessman in the US, the prize is administered by the Glasnost Defence Foundation in Moscow. The prize \"for journalism as an act of conscience\" has been won over the years by famous journalists such as Anna Politkovskaya and young reporters and editors working far from Russia's media capital, Moscow. The 2015 winner was Yelena Kostyuchenko.\nAndrei Sakharov Archives and Human Rights Center.\nThe Andrei Sakharov Archives and Human Rights Center, established at Brandeis University in 1993, are now housed at Harvard University.\nThe documents from that archive were published by the Yale University Press in 2005. These documents are available online.\nMost of documents of the archive are letters from the head of the KGB to the Central Committee about activities of Soviet dissidents and recommendations about the interpretation in newspapers. The letters cover the period from 1968 to 1991 (Brezhnev stagnation). The documents characterize not only Sakharov's activity, but that of other dissidents, as well as that of highest-position apparatchiks and the KGB. No Russian equivalent of the KGB archive is available.\nHonours and awards.\nIn 1980, Sakharov was stripped of all Soviet awards for \"anti-Soviet activities\". Later, during glasnost, he declined the return of his awards and, consequently, Mikhail Gorbachev did not sign the necessary decree."}
{"id": "2787", "revid": "82429", "url": "https://en.wikipedia.org/wiki?curid=2787", "title": "Astrobiology", "text": "Astrobiology (also xenology or exobiology) is a scientific field within the life and environmental sciences that studies the origins, early evolution, distribution, and future of life in the universe by investigating its deterministic conditions and contingent events. As a discipline, astrobiology is founded on the premise that life may exist beyond Earth.\nResearch in astrobiology comprises three main areas: the study of habitable environments in the Solar System and beyond, the search for planetary biosignatures of past or present extraterrestrial life, and the study of the origin and early evolution of life on Earth.\nThe field of astrobiology has its origins in the 20th century with the advent of space exploration and the discovery of exoplanets. Early astrobiology research focused on the search for extraterrestrial life and the study of the potential for life to exist on other planets. In the 1960s and 1970s, NASA began its astrobiology pursuits within the Viking program, which was the first US mission to land on Mars and search for signs of life. This mission, along with other early space exploration missions, laid the foundation for the development of astrobiology as a discipline.\nRegarding habitable environments, astrobiology investigates potential locations beyond Earth that could support life, such as Mars, Europa, and exoplanets, through research into the extremophiles populating austere environments on Earth, like volcanic and deep sea environments. Research within this topic is conducted utilising the methodology of the geosciences, especially geobiology, for astrobiological applications.\nThe search for biosignatures involves the identification of signs of past or present life in the form of organic compounds, isotopic ratios, or microbial fossils. Research within this topic is conducted utilising the methodology of planetary and environmental science, especially atmospheric science, for astrobiological applications, and is often conducted through remote sensing and in situ missions.\nAstrobiology also concerns the study of the origin and early evolution of life on Earth to try to understand the conditions that are necessary for life to form on other planets. This research seeks to understand how life emerged from non-living matter and how it evolved to become the diverse array of organisms we see today. Research within this topic is conducted utilising the methodology of paleosciences, especially paleobiology, for astrobiological applications.\nAstrobiology is a rapidly developing field with a strong interdisciplinary aspect that holds many challenges and opportunities for scientists. Astrobiology programs and research centres are present in many universities and research institutions around the world, and space agencies like NASA and ESA have dedicated departments and programs for astrobiology research.\nOverview.\nThe term astrobiology was first proposed by the Russian astronomer Gavriil Tikhov in 1953. It is etymologically derived from the Greek , \"star\"; , \"life\"; and , \"-logia\", \"study\". A close synonym is exobiology from the Greek \u0388\u03be\u03c9, \"external\"; , \"life\"; and , \"-logia\", \"study\", coined by American molecular biologist Joshua Lederberg; exobiology is considered to have a narrow scope limited to search of life external to Earth. Another associated term is xenobiology, from the Greek \u03be\u03ad\u03bd\u03bf\u03c2, \"foreign\"; , \"life\"; and -\u03bb\u03bf\u03b3\u03af\u03b1, \"study\", coined by American science fiction writer Robert Heinlein in his work \"The Star Beast\"; xenobiology is now used in a more specialised sense, referring to 'biology based on foreign chemistry', whether of extraterrestrial or terrestrial (typically synthetic) origin.\nWhile the potential for extraterrestrial life, especially intelligent life, has been explored throughout human history within philosophy and narrative, the question is a verifiable hypothesis and thus a valid line of scientific inquiry; planetary scientist David Grinspoon calls it a field of natural philosophy, grounding speculation on the unknown in known scientific theory.\nThe modern field of astrobiology can be traced back to the 1950s and 1960s with the advent of space exploration, when scientists began to seriously consider the possibility of life on other planets. In 1957, the Soviet Union launched Sputnik 1, the first artificial satellite, which marked the beginning of the Space Age. This event led to an increase in the study of the potential for life on other planets, as scientists began to consider the possibilities opened up by the new technology of space exploration. In 1959, NASA funded its first exobiology project, and in 1960, NASA founded the Exobiology Program, now one of four main elements of NASA's current Astrobiology Program. In 1971, NASA funded Project Cyclops, part of the search for extraterrestrial intelligence, to search radio frequencies of the electromagnetic spectrum for interstellar communications transmitted by extraterrestrial life outside the Solar System. In the 1960s-1970s, NASA established the Viking program, which was the first US mission to land on Mars and search for metabolic signs of present life; the results were inconclusive.\nIn the 1980s and 1990s, the field began to expand and diversify as new discoveries and technologies emerged. The discovery of microbial life in extreme environments on Earth, such as deep-sea hydrothermal vents, helped to clarify the feasibility of potential life existing in harsh conditions. The development of new techniques for the detection of biosignatures, such as the use of stable isotopes, also played a significant role in the evolution of the field.\nThe contemporary landscape of astrobiology emerged in the early 21st century, focused on utilising Earth and environmental science for applications within comparate space environments. Missions included the ESA's Beagle 2, which failed minutes after landing on Mars, NASA's \"Phoenix\" lander, which probed the environment for past and present planetary habitability of microbial life on Mars and researched the history of water, and NASA's \"Curiosity\" rover, currently probing the environment for past and present planetary habitability of microbial life on Mars.\nTheoretical foundations.\nPlanetary habitability.\nAstrobiological research makes a number of simplifying assumptions when studying the necessary components for planetary habitability.\nCarbon and Organic Compounds: Carbon is the fourth most abundant element in the universe and the energy required to make or break a bond is at just the appropriate level for building molecules which are not only stable, but also reactive. The fact that carbon atoms bond readily to other carbon atoms allows for the building of extremely long and complex molecules. As such, astrobiological research presumes that the vast majority of life forms in the Milky Way galaxy are based on carbon chemistries, as are all life forms on Earth. However, theoretical astrobiology entertains the potential for other organic molecular bases for life, thus astrobiological research often focuses on identifying environments that have the potential to support life based on the presence of organic compounds.\nLiquid water: Liquid water is a common molecule that provides an excellent environment for the formation of complicated carbon-based molecules, and is generally considered necessary for life as we know it to exist. Thus, astrobiological research presumes that extraterrestrial life similarly depends upon access to liquid water, and often focuses on identifying environments that have the potential to support liquid water. Some researchers posit environments of water-ammonia mixtures as possible solvents for hypothetical types of biochemistry.\nEnvironmental stability: Where organisms adaptively evolve to the conditions of the environments in which they reside, environmental stability is considered necessary for life to exist. This presupposes the necessity of a stable temperature, pressure, and radiation levels; resultantly, astrobiological research focuses on planets orbiting Sun-like red dwarf stars. This is because very large stars have relatively short lifetimes, meaning that life might not have time to emerge on planets orbiting them; very small stars provide so little heat and warmth that only planets in very close orbits around them would not be frozen solid, and in such close orbits these planets would be tidally locked to the star; whereas the long lifetimes of red dwarfs could allow the development of habitable environments on planets with thick atmospheres. This is significant as red dwarfs are extremely common. (\"See also\": Habitability of red dwarf systems).\nEnergy source: It is assumed that any life elsewhere in the universe would also require an energy source. Previously, it was assumed that this would necessarily be from a sun-like star, however with developments within extremophile research contemporary astrobiological research often focuses on identifying environments that have the potential to support life based on the availability of an energy source, such as the presence of volcanic activity on a planet or moon that could provide a source of heat and energy.\nIt is important to note that these assumptions are based on our current understanding of life on Earth and the conditions under which it can exist. As our understanding of life and the potential for it to exist in different environments evolves, these assumptions may change.\nMethods.\nStudying terrestrial extremophiles.\nAstrobiological research concerning the study of habitable environments in our solar system and beyond utilises methods within the geosciences. Research within this branch primarily concerns the geobiology of organisms that can survive in extreme environments on Earth, such as in volcanic or deep sea environments, to understand the limits of life, and the conditions under which life might be able to survive on other planets. This includes, but is not limited to:\nDeep-sea extremophiles: Researchers are studying organisms that live in the extreme environments of deep-sea hydrothermal vents and cold seeps. These organisms survive in the absence of sunlight, and some are able to survive in high temperatures and pressures, and use chemical energy instead of sunlight to produce food.\nDesert extremophiles: Researchers are studying organisms that can survive in extreme dry, high temperature conditions, such as in deserts.\nMicrobes in extreme environments: Researchers are investigating the diversity and activity of microorganisms in environments such as deep mines, subsurface soil, cold glaciers and polar ice, and high-altitude environments.\nResearching Earth's present environment.\nResearch also regards the long-term survival of life on Earth, and the possibilities and hazards of life on other planets, including:\nBiodiversity and ecosystem resilience: Scientists are studying how the diversity of life and the interactions between different species contribute to the resilience of ecosystems and their ability to recover from disturbances.\nClimate change and extinction: Researchers are investigating the impacts of climate change on different species and ecosystems, and how they may lead to extinction or adaptation. This includes the evolution of Earth's climate and geology, and their potential impact on the habitability of the planet in the future, especially for humans.\nHuman impact on the biosphere: Scientists are studying the ways in which human activities, such as deforestation, pollution, and the introduction of invasive species, are affecting the biosphere and the long-term survival of life on Earth.\nLong-term preservation of life: Researchers are exploring ways to preserve samples of life on Earth for long periods of time, such as cryopreservation and genomic preservation, in the event of a catastrophic event that could wipe out most of life on Earth.\nFinding biosignatures on other worlds.\nEmerging astrobiological research concerning the search for planetary biosignatures of past or present extraterrestrial life utilise methodologies within planetary sciences. These include: \nThe study of microbial life in the subsurface of Mars: Scientists are using data from Mars rover missions to study the composition of the subsurface of Mars, searching for biosignatures of past or present microbial life.\nThe study of liquid bodies on icy moons: Discoveries of surface and subsurface bodies of liquid on moons such as Europa, Titan and Enceladus showed possible habitability zones, making them viable targets for the search for extraterrestrial life. , missions like \"Europa Clipper\" and \"Dragonfly\" are planned to search for biosignatures within these environments. \nThe study of the atmospheres of planets: Scientists are studying the potential for life to exist in the atmospheres of planets, with a focus on the study of the physical and chemical conditions necessary for such life to exist, namely the detection of organic molecules and biosignature gases; for example, the study of the possibility of life in the atmospheres of exoplanets that orbit red dwarfs and the study of the potential for microbial life in the upper atmosphere of Venus.\nTelescopes and remote sensing of exoplanets: The discovery of thousands of exoplanets has opened up new opportunities for the search for biosignatures. Scientists are using telescopes such as the James Webb Space Telescope and the Transiting Exoplanet Survey Satellite to search for biosignatures on exoplanets. They are also developing new techniques for the detection of biosignatures, such as the use of remote sensing to search for biosignatures in the atmosphere of exoplanets.\nTalking to extraterrestrials.\nSETI and CETI: Scientists search for signals from intelligent extraterrestrial civilizations using radio and optical telescopes within the discipline of extraterrestrial intelligence communications (CETI). CETI focuses on composing and deciphering messages that could theoretically be understood by another technological civilization. Communication attempts by humans have included broadcasting mathematical languages, pictorial systems such as the Arecibo message, and computational approaches to detecting and deciphering 'natural' language communication. While some high-profile scientists, such as Carl Sagan, have advocated the transmission of messages, theoretical physicist Stephen Hawking warned against it, suggesting that aliens may raid Earth for its resources.\nInvestigating the early Earth.\nEmerging astrobiological research concerning the study of the origin and early evolution of life on Earth utilises methodologies within the palaeosciences. These include:\nThe study of the early atmosphere: Researchers are investigating the role of the early atmosphere in providing the right conditions for the emergence of life, such as the presence of gases that could have helped to stabilise the climate and the formation of organic molecules.\nThe study of the early magnetic field: Researchers are investigating the role of the early magnetic field in protecting the Earth from harmful radiation and helping to stabilise the climate. This research has immense astrobiological implications where the subjects of current astrobiological research like Mars lack such a field.\nThe study of prebiotic chemistry: Scientists are studying the chemical reactions that could have occurred on the early Earth that led to the formation of the building blocks of life- amino acids, nucleotides, and lipids- and how these molecules could have formed spontaneously under early Earth conditions. \nThe study of impact events: Scientists are investigating the potential role of impact events- especially meteorites- in the delivery of water and organic molecules to early Earth.\nThe study of the primordial soup: Researchers are investigating the conditions and ingredients that were present on the early Earth that could have led to the formation of the first living organisms, such as the presence of water and organic molecules, and how these ingredients could have led to the formation of the first living organisms. This includes the role of water in the formation of the first cells and in catalysing chemical reactions.\nThe study of the role of minerals: Scientists are investigating the role of minerals like clay in catalysing the formation of organic molecules, thus playing a role in the emergence of life on Earth.\nThe study of the role of energy and electricity: Scientists are investigating the potential sources of energy and electricity that could have been available on the early Earth, and their role in the formation of organic molecules, thus the emergence of life.\nThe study of the early oceans: Scientists are investigating the composition and chemistry of the early oceans and how it may have played a role in the emergence of life, such as the presence of dissolved minerals that could have helped to catalyse the formation of organic molecules.\nThe study of hydrothermal vents: Scientists are investigating the potential role of hydrothermal vents in the origin of life, as these environments may have provided the energy and chemical building blocks needed for its emergence.\nThe study of plate tectonics: Scientists are investigating the role of plate tectonics in creating a diverse range of environments on the early Earth.\nThe study of the early biosphere: Researchers are investigating the diversity and activity of microorganisms in the early Earth, and how these organisms may have played a role in the emergence of life.\nThe study of microbial fossils: Scientists are investigating the presence of microbial fossils in ancient rocks, which can provide clues about the early evolution of life on Earth and the emergence of the first organisms.\nResearch.\nThe systematic search for possible life outside Earth is a valid multidisciplinary scientific endeavor. However, hypotheses and predictions as to its existence and origin vary widely, and at the present, the development of hypotheses firmly grounded on science may be considered astrobiology's most concrete practical application. It has been proposed that viruses are likely to be encountered on other life-bearing planets, and may be present even if there are no biological cells.\nResearch outcomes.\n, no evidence of extraterrestrial life has been identified. Examination of the Allan Hills 84001 meteorite, which was recovered in Antarctica in 1984 and originated from Mars, is thought by David McKay, as well as few other scientists, to contain microfossils of extraterrestrial origin; this interpretation is controversial.\nYamato 000593, the second largest meteorite from Mars, was found on Earth in 2000. At a microscopic level, spheres are found in the meteorite that are rich in carbon compared to surrounding areas that lack such spheres. The carbon-rich spheres may have been formed by biotic activity according to some NASA scientists.\nOn 5 March 2011, Richard B. Hoover, a scientist with the Marshall Space Flight Center, speculated on the finding of alleged microfossils similar to cyanobacteria in CI1 carbonaceous meteorites in the fringe \"Journal of Cosmology\", a story widely reported on by mainstream media. However, NASA formally distanced itself from Hoover's claim. According to American astrophysicist Neil deGrasse Tyson: \"At the moment, life on Earth is the only known life in the universe, but there are compelling arguments to suggest we are not alone.\"\nElements of astrobiology.\nAstronomy.\nMost astronomy-related astrobiology research falls into the category of extrasolar planet (exoplanet) detection, the hypothesis being that if life arose on Earth, then it could also arise on other planets with similar characteristics. To that end, a number of instruments designed to detect Earth-sized exoplanets have been considered, most notably NASA's Terrestrial Planet Finder (TPF) and ESA's Darwin programs, both of which have been cancelled. NASA launched the Kepler mission in March 2009, and the French Space Agency launched the COROT space mission in 2006. There are also several less ambitious ground-based efforts underway.\nThe goal of these missions is not only to detect Earth-sized planets but also to directly detect light from the planet so that it may be studied spectroscopically. By examining planetary spectra, it would be possible to determine the basic composition of an extrasolar planet's atmosphere and/or surface. Given this knowledge, it may be possible to assess the likelihood of life being found on that planet. A NASA research group, the Virtual Planet Laboratory, is using computer modeling to generate a wide variety of virtual planets to see what they would look like if viewed by TPF or Darwin. It is hoped that once these missions come online, their spectra can be cross-checked with these virtual planetary spectra for features that might indicate the presence of life.\nAn estimate for the number of planets with intelligent \"communicative\" extraterrestrial life can be gleaned from the Drake equation, essentially an equation expressing the probability of intelligent life as the product of factors such as the fraction of planets that might be habitable and the fraction of planets on which life might arise:\nwhere:\nHowever, whilst the rationale behind the equation is sound, it is unlikely that the equation will be constrained to reasonable limits of error any time soon. The problem with the formula is that it is not used to generate or support hypotheses because it contains factors that can never be verified. The first term, R*, number of stars, is generally constrained within a few orders of magnitude. The second and third terms, \"fp\", stars with planets and \"fe\", planets with habitable conditions, are being evaluated for the star's neighborhood. Drake originally formulated the equation merely as an agenda for discussion at the Green Bank conference, but some applications of the formula had been taken literally and related to simplistic or pseudoscientific arguments. Another associated topic is the Fermi paradox, which suggests that if intelligent life is common in the universe, then there should be obvious signs of it.\nAnother active research area in astrobiology is planetary system formation. It has been suggested that the peculiarities of the Solar System (for example, the presence of Jupiter as a protective shield) may have greatly increased the probability of intelligent life arising on Earth.\nBiology.\nBiology cannot state that a process or phenomenon, by being mathematically possible, has to exist forcibly in an extraterrestrial body. Biologists specify what is speculative and what is not. The discovery of extremophiles, organisms able to survive in extreme environments, became a core research element for astrobiologists, as they are important to understand four areas in the limits of life in planetary context: the potential for panspermia, forward contamination due to human exploration ventures, planetary colonization by humans, and the exploration of extinct and extant extraterrestrial life.\nUntil the 1970s, life was thought to be entirely dependent on energy from the Sun. Plants on Earth's surface capture energy from sunlight to photosynthesize sugars from carbon dioxide and water, releasing oxygen in the process that is then consumed by oxygen-respiring organisms, passing their energy up the food chain. Even life in the ocean depths, where sunlight cannot reach, was thought to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. The world's ability to support life was thought to depend on its access to sunlight. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible \"Alvin\", scientists discovered colonies of giant tube worms, clams, crustaceans, mussels, and other assorted creatures clustered around undersea volcanic features known as black smokers. These creatures thrive despite having no access to sunlight, and it was soon discovered that they form an entirely independent ecosystem. Although most of these multicellular lifeforms need dissolved oxygen (produced by oxygenic photosynthesis) for their aerobic cellular respiration and thus are not completely independent from sunlight by themselves, the basis for their food chain is a form of bacterium that derives its energy from oxidization of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. Other lifeforms entirely decoupled from the energy from sunlight are green sulfur bacteria which are capturing geothermal light for anoxygenic photosynthesis or bacteria running chemolithoautotrophy based on the radioactive decay of uranium. This chemosynthesis revolutionized the study of biology and astrobiology by revealing that life need not be sunlight-dependent; it only requires water and an energy gradient in order to exist.\nBiologists have found extremophiles that thrive in ice, boiling water, acid, alkali, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. This opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats. Characterization of these organisms, their environments and their evolutionary pathways, is considered a crucial component to understanding how life might evolve elsewhere in the universe. For example, some organisms able to withstand exposure to the vacuum and radiation of outer space include the lichen fungi \"Rhizocarpon geographicum\" and \"Rusavskia elegans\", the bacterium \"Bacillus safensis\", \"Deinococcus radiodurans\", \"Bacillus subtilis\", yeast \"Saccharomyces cerevisiae\", seeds from \"Arabidopsis thaliana\" ('mouse-ear cress'), as well as the invertebrate animal Tardigrade. While tardigrades are not considered true extremophiles, they are considered extremotolerant microorganisms that have contributed to the field of astrobiology. Their extreme radiation tolerance and presence of DNA protection proteins may provide answers as to whether life can survive away from the protection of the Earth's atmosphere.\nJupiter's moon, Europa, and Saturn's moon, Enceladus, are now considered the most likely locations for extant extraterrestrial life in the Solar System due to their subsurface water oceans where radiogenic and tidal heating enables liquid water to exist.\nThe origin of life, known as abiogenesis, distinct from the evolution of life, is another ongoing field of research. Oparin and Haldane postulated that the conditions on the early Earth were conducive to the formation of organic compounds from inorganic elements and thus to the formation of many of the chemicals common to all forms of life we see today. The study of this process, known as prebiotic chemistry, has made some progress, but it is still unclear whether or not life could have formed in such a manner on Earth. The alternative hypothesis of panspermia is that the first elements of life may have formed on another planet with even more favorable conditions (or even in interstellar space, asteroids, etc.) and then have been carried over to Earth.\nThe cosmic dust permeating the universe contains complex organic compounds (\"amorphous organic solids with a mixed aromatic-aliphatic structure\") that could be created naturally, and rapidly, by stars. Further, a scientist suggested that these compounds may have been related to the development of life on Earth and said that, \"If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.\"\nMore than 20% of the carbon in the universe may be associated with polycyclic aromatic hydrocarbons (PAHs), possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets. PAHs are subjected to interstellar medium conditions and are transformed through hydrogenation, oxygenation and hydroxylation, to more complex organics\u2014\"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\".\nIn October 2020, astronomers proposed the idea of detecting life on distant planets by studying the shadows of trees at certain times of the day to find patterns that could be detected through observation of exoplanets.\nPhilosophy.\nDavid Grinspoon called astrobiology a field of natural philosophy. Astrobiology intersects with philosophy by raising questions about the nature and existence of life beyond Earth. Philosophical implications include the definition of life itself, issues in the philosophy of mind and cognitive science in case intelligent life is found, epistemological questions about the nature of proof, ethical considerations of space exploration, along with the broader impact of discovering extraterrestrial life on human thought and society.\nDun\u00e9r has emphasized philosophy of astrobiology as an ongoing existential exercise in individual and collective self-understanding, whose major task is constructing and debating concepts such as the concept of life. Key issues, for Dun\u00e9r, are questions of resource money and monetary planning, epistemological questions regarding astrobiological knowledge, linguistics issues about interstellar communication, cognitive issues such as the definition of intelligence, along with the possibility of interplanetary contamination.\nPersson also emphasized key philosophical questions in astrobiology. They include ethical justification of resources, the question of life in general, the epistemological issues and knowledge about being alone in the universe, ethics towards extraterrestrial life, the question of politics and governing uninhabited worlds, along with questions of ecology.\nFor von Hegner, the question of astrobiology and the possibility of astrophilosophy differs. For him, the discipline needs to bifurcate into astrobiology and astrophilosophy since discussions made possible by astrobiology, but which have been astrophilosophical in nature, have existed as long as there have been discussions about extraterrestrial life. Astrobiology is a self-corrective interaction among observation, hypothesis, experiment, and theory, pertaining to the exploration of all natural phenomena. Astrophilosophy consists of methods of dialectic analysis and logical argumentation, pertaining to the clarification of the nature of reality. \u0160ekrst argues that astrobiology requires the affirmation of astrophilosophy, but not as a separate cognate to astrobiology. The stance of conceptual speciesm, according to \u0160ekrst, permeates astrobiology since the very name astrobiology tries to talk about not just biology, but about life in a general way, which includes terrestrial life as a subset. This leads us to either redefine philosophy, or consider the need for astrophilosophy as a more general discipline, to which philosophy is just a subset that deals with questions such as the nature of the human mind and other anthropocentric questions.\nMost of the philosophy of astrobiology deals with two main questions: the question of life and the ethics of space exploration. Kolb specifically emphasizes the question of viruses, for which the question whether they are alive or not is based on the definitions of life that include self-replication. Schneider tried to defined exo-life, but concluded that we often start with our own prejudices and that defining extraterrestrial life seems futile using human concepts. For Dick, astrobiology relies on metaphysical assumption that there is extraterrestrial life, which reaffirms questions in the philosophy of cosmology, such as fine-tuning or the anthropic principle.\nRare Earth hypothesis.\nThe Rare Earth hypothesis postulates that multicellular life forms found on Earth may actually be more of a rarity than scientists assume. According to this hypothesis, life on Earth (and more, multi-cellular life) is possible because of a conjunction of the right circumstances (galaxy and location within it, planetary system, star, orbit, planetary size, atmosphere, etc.); and the chance for all those circumstances to repeat elsewhere may be rare. It provides a possible answer to the Fermi paradox which wonders: if extraterrestrial aliens are common, why aren't they obvious? It is apparently in opposition to the principle of mediocrity, assumed by famed astronomers Frank Drake, Carl Sagan, and others. The principle of mediocrity suggests that life on Earth is not exceptional, and it is more than likely to be found on innumerable other worlds.\nMissions.\nResearch into the environmental limits of life and the workings of extreme ecosystems is ongoing, enabling researchers to better predict what planetary environments might be most likely to harbor life. Missions such as the \"Phoenix\" lander, Mars Science Laboratory, ExoMars, Mars 2020 rover to Mars, and the \"Cassini\" probe to Saturn's moons aim to further explore the possibilities of life on other planets in the Solar System.\nThe two Viking landers each carried four types of biological experiments to the surface of Mars in the late 1970s. These were the only Mars landers to carry out experiments looking specifically for metabolism by current microbial life on Mars. The landers used a robotic arm to collect soil samples into sealed test containers on the craft. The two landers were identical, so the same tests were carried out at two places on Mars' surface; \"Viking 1\" near the equator and \"Viking 2\" further north. The result was inconclusive, and is still disputed by some scientists.\nNorman Horowitz was the chief of the Jet Propulsion Laboratory bioscience section for the Mariner and Viking missions from 1965 to 1976. Horowitz considered that the great versatility of the carbon atom makes it the element most likely to provide solutions, even exotic solutions, to the problems of survival of life on other planets. However, he also considered that the conditions found on Mars were incompatible with carbon based life.\n\"Beagle 2\" was an unsuccessful British Mars lander that formed part of the European Space Agency's 2003 Mars Express mission. Its primary purpose was to search for signs of life on Mars, past or present. Although it landed safely, it was unable to correctly deploy its solar panels and telecom antenna.\nEXPOSE is a multi-user facility mounted in 2008 outside the International Space Station dedicated to astrobiology. EXPOSE was developed by the European Space Agency (ESA) for long-term spaceflights that allow exposure of organic chemicals and biological samples to outer space in low Earth orbit.\nThe Mars Science Laboratory (MSL) mission landed the \"Curiosity\" rover that is currently in operation on Mars. It was launched 26 November 2011, and landed at Gale Crater on 6 August 2012. Mission objectives are to help assess Mars' habitability and in doing so, determine whether Mars is or has ever been able to support life, collect data for a future human mission, study Martian geology, its climate, and further assess the role that water, an essential ingredient for life as we know it, played in forming minerals on Mars.\nThe \"Tanpopo\" mission is an orbital astrobiology experiment investigating the potential interplanetary transfer of life, organic compounds, and possible terrestrial particles in the low Earth orbit. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of microbial life as well as prebiotic organic compounds. Early mission results show evidence that some clumps of microorganism can survive for at least one year in space. This may support the idea that clumps greater than 0.5 millimeters of microorganisms could be one way for life to spread from planet to planet.\n\"ExoMars\" is a robotic mission to Mars to search for possible biosignatures of Martian life, past or present. This astrobiological mission was under development by the European Space Agency (ESA) in partnership with the Russian Federal Space Agency (Roscosmos); it was planned for a 2022 launch; however, technical and funding issues and the Russian invasion of Ukraine have forced ESA to repeatedly delay the rover's delivery to 2028.\nMars 2020 successfully landed its rover \"Perseverance\" in Jezero Crater on 18 February 2021. It will investigate environments on Mars relevant to astrobiology, investigate its surface geological processes and history, including the assessment of its past habitability and potential for preservation of biosignatures and biomolecules within accessible geological materials. The Science Definition Team is proposing the rover collect and package at least 31 samples of rock cores and soil for a later mission to bring back for more definitive analysis in laboratories on Earth. The rover could make measurements and technology demonstrations to help designers of a human expedition understand any hazards posed by Martian dust and demonstrate how to collect carbon dioxide (CO2), which could be a resource for making molecular oxygen (O2) and rocket fuel.\n\"Europa Clipper\" is a mission launched by NASA on 14 October 2024 that will conduct detailed reconnaissance of Jupiter's moon Europa beginning in 2030, and will investigate whether its internal ocean could harbor conditions suitable for life. It will also aid in the selection of future landing sites.\n\"Dragonfly\" is a NASA mission scheduled to land on Titan in 2036 to assess its microbial habitability and study its prebiotic chemistry. Dragonfly is a rotorcraft lander that will perform controlled flights between multiple locations on the surface, which allows sampling of diverse regions and geological contexts.\nProposed concepts.\n\"Icebreaker Life\" is a lander mission that was proposed for NASA's Discovery Program for the 2021 launch opportunity, but it was not selected for development. It would have had a stationary lander that would be a near copy of the successful 2008 \"Phoenix\" and it would have carried an upgraded astrobiology scientific payload, including a 1-meter-long core drill to sample ice-cemented ground in the northern plains to conduct a search for organic molecules and evidence of current or past life on Mars. One of the key goals of the \"Icebreaker Life\" mission is to test the hypothesis that the ice-rich ground in the polar regions has significant concentrations of organics due to protection by the ice from oxidants and radiation.\n\"Journey to Enceladus and Titan\" (\"JET\") is an astrobiology mission concept to assess the habitability potential of Saturn's moons Enceladus and Titan by means of an orbiter.\n\"Enceladus Life Finder\" (\"ELF\") is a proposed astrobiology mission concept for a space probe intended to assess the habitability of the internal aquatic ocean of Enceladus, Saturn's sixth-largest moon.\n\"Life Investigation For Enceladus\" (\"LIFE\") is a proposed astrobiology sample-return mission concept. The spacecraft would enter into Saturn orbit and enable multiple flybys through Enceladus' icy plumes to collect icy plume particles and volatiles and return them to Earth on a capsule. The spacecraft may sample Enceladus' plumes, the E ring of Saturn, and the upper atmosphere of Titan.\n\"Oceanus\" is an orbiter proposed in 2017 for the New Frontiers mission No. 4. It would travel to the moon of Saturn, Titan, to assess its habitability. \"Oceanus\" objectives are to reveal Titan's organic chemistry, geology, gravity, topography, collect 3D reconnaissance data, catalog the organics and determine where they may interact with liquid water.\n\"Explorer of Enceladus and Titan\" (E2T) is an orbiter mission concept that would investigate the evolution and habitability of the Saturnian satellites Enceladus and Titan. The mission concept was proposed in 2017 by the European Space Agency."}
{"id": "2789", "revid": "15420072", "url": "https://en.wikipedia.org/wiki?curid=2789", "title": "Alice Sheldon", "text": ""}
{"id": "2790", "revid": "12396222", "url": "https://en.wikipedia.org/wiki?curid=2790", "title": "Air show", "text": "An air show (or airshow, air fair, air tattoo) is a public event where aircraft are exhibited. They often include aerobatics demonstrations, without they are called \"static air shows\" with aircraft parked on the ground.\nThe largest air show measured by number of exhibitors and size of exhibit space is Le Bourget, followed by Farnborough, with the Dubai Airshow and Singapore Airshow both claiming third place. The largest air show or fly-in by number of participating aircraft is EAA AirVenture Oshkosh, with approximately 10,000 aircraft participating annually. The biggest military airshow in the world is the Royal International Air Tattoo, at RAF Fairford in England. On the other hand, FIDAE in II Air Brigade of the FACH, next to the Arturo Merino Ben\u00edtez International Airport in Santiago, Chile, is the largest aerospace fair in Latin America and the Southern Hemisphere.\nOutline.\nSome airshows are held as a business venture or as a trade event where aircraft, avionics and other services are promoted to potential customers. Many air shows are held in support of local, national or military charities. Military air firms often organise air shows at military airfields as a public relations exercise to thank the local community, promote military careers and raise the profile of the military.\nAir \"seasons\" vary around the world. The United States enjoys a long season that generally runs from March to November, covering the spring, summer, and fall seasons. Other countries often have much shorter seasons. In Japan air shows are generally events held at Japan Air Self-Defense Force bases regularly throughout the year. The European season usually starts in late April or Early May and is usually over by mid October. The Middle East, Australia, and New Zealand hold their events between January and March. However, for many acts, the \"off-season\" does not mean a period of inactivity; pilots and performers use this time for maintenance and practice.\nThe type of displays seen at shows are constrained by a number of factors, including the weather and visibility. Most aviation authorities now publish rules and guidance on minimum display heights and criteria for differing conditions. In addition to the weather, pilots and organizers must also consider local airspace restrictions. Most exhibitors will plan \"full\", \"rolling\" and \"flat\" display for varying weather and airspace conditions.\nThe types of shows vary greatly. Some are large scale military events with large flying displays and ground exhibitions while others held at small local airstrips can often feature just one or two hours of flying with just a few stalls on the ground. Air displays can be held during day or night with the latter becoming increasingly popular. Air shows often, but do not always, take place over airfields; some have been held over the grounds of stately homes or castles and over the sea at coastal resorts.\nThe first public international airshow, at which many types of aircraft were displayed and flown, was the Grande Semaine d'Aviation de la Champagne, held Aug. 22\u201329, 1909 in Reims. This had been preceded by what may have been the first ever gathering of enthusiasts, June 28 \u2013 July 19 of the same year at the airfield at La Brayelle, near Douai.\nAttractions.\nBefore World War II, air shows were associated with long-distance air races, often lasting many days and covering thousands of miles. While the Reno Air Races keep this tradition alive, most air shows today primarily feature a series of aerial demos of short duration.\nMost air shows feature warbirds, aerobatics, and demonstrations of modern military aircraft, and many air shows offer a variety of other aeronautical attractions as well, such as wing-walking, radio-controlled aircraft, water/slurry drops from firefighting aircraft, simulated helicopter rescues and sky diving.\nSpecialist aerobatic aircraft have powerful piston engines, light weight and big control surfaces, making them capable of very high roll rates and accelerations. A skilled pilot will be able to climb vertically, perform very tight turns, tumble his aircraft end-over-end and perform manoeuvres during loops.\nLarger airshows can be headlined by military jet demonstration teams, such as the United States Navy Blue Angels, United States Air Force Thunderbirds, Royal Canadian Air Force Snowbirds, Royal Air Force Red Arrows, and Swiss Air Force Patrouille Suisse, among many others.\nSolo military demos, also known as tactical demos, feature one aircraft. The demonstration focuses on the capabilities of modern military aircraft. The display will usually demonstrate the aircraft's very short (and often very loud) rolls, fast speeds, slow approach speeds, as well as their ability to quickly make tight turns, to climb quickly, and their ability to be precisely controlled at a large range of speeds. Manoeuvres include aileron rolls, barrel rolls, hesitation rolls, Cuban-8s, tight turns, high-alpha flight, a high-speed pass, double Immelmans, and touch-and-gos. Tactical demos may include simulated bomb drops, sometimes with pyrotechnics on the ground for effect. Aircraft with special characteristics that give them unique capabilities will often display those in their demos; For example, Russian fighters with thrust vectoring may be used to perform the cobra maneuver or the Kulbit, while VTOL aircraft such as the Harrier may display such vertical capabilities or perform complex maneuvers with them. Some military air shows also feature demonstrations of aircraft ordnance in airstrikes and close air support, using either blanks or live munitions.\nSafety.\nAir shows may present some risk to spectators and aviators. Accidents have occurred, sometimes with a large loss of life, such as the 1988 Ramstein air show disaster (70 deaths) in Germany and the 2002 Sknyliv air show disaster (77 deaths) in Ukraine.\nBecause of these accidents, the various aviation authorities around the world have set rules and guidance for those running and participating in air displays. For example, after the breakup of an aircraft at 1952 Farnborough air show (31 deaths), the separation between display and spectators was increased. Air displays are often monitored by aviation authorities to ensure safe procedures.\nIn the United Kingdom, local authorities will first need to approve any application for an event to which the public is admitted. The first priority must be to arrange insurance cover and details can be obtained from local authorities. An added complication is a whole raft of legislation concerning health &amp; safety, in particular corporate manslaughter, which can involve the event organiser being charged with a criminal offence if any of the insurances and risk assessments are not fully completed well in advance of the event.\nRules govern the distance from the crowds that aircraft must fly. These vary according to the rating of the pilot/crew, the type of aircraft and the way the aircraft is being flown. For instance, slower, lighter aircraft are usually allowed closer and lower to the crowd than larger, faster types. Also, a fighter jet flying straight and level will be able to do so closer to the crowd and lower than if it were performing a roll or a loop.\nPilots can get authorizations for differing types of displays (e.g., limbo flying, basic aerobatics to unlimited aerobatics) and to differing minimum base heights above the ground. To gain such authorisations, the pilots will have to demonstrate to an examiner that they can perform to those limits without endangering themselves, ground crew or spectators.\nDespite display rules and guidances, accidents have continued to happen. However, air show accidents are rare and where there is proper supervision air shows have impressive safety records. Each year, organizations such as International Council of Air Shows and European Airshow Council meet and discuss various subjects including air show safety where accidents are discussed and lessons learned."}
{"id": "2792", "revid": "40552684", "url": "https://en.wikipedia.org/wiki?curid=2792", "title": "Anthropic principle", "text": "The anthropic principle, also known as the observation selection effect, is the proposition that the range of possible observations that could be made about the universe is limited by the fact that observations are possible only in the type of universe that is capable of developing intelligent life. Proponents of the anthropic principle argue that it explains why the universe has the age and the fundamental physical constants necessary to accommodate intelligent life. If either had been significantly different, no one would have been around to make observations. Anthropic reasoning has been used to address the question as to why certain measured physical constants take the values that they do, rather than some other arbitrary values, and to explain a perception that the universe appears to be finely tuned for the existence of life.\nThere are many different formulations of the anthropic principle. Philosopher Nick Bostrom counts thirty, but the underlying principles can be divided into \"weak\" and \"strong\" forms, depending on the types of cosmological claims they entail.\nDefinition and basis.\nThe principle was formulated as a response to a series of observations that the laws of nature and parameters of the universe have values that are consistent with conditions for life as it is known rather than values that would not be consistent with life on Earth. The anthropic principle states that this is an\" a posteriori\" necessity, because if life were impossible, no living entity would be there to observe it, and thus it would not be known. That is, it must be possible to observe \"some\" universe, and hence, the laws and constants of any such universe must accommodate that possibility.\nThe term \"anthropic\" in \"anthropic principle\" has been argued to be a misnomer. While singling out the currently observable kind of carbon-based life, none of the finely tuned phenomena require human life or some kind of carbon chauvinism. Any form of life or any form of heavy atom, stone, star, or galaxy would do; nothing specifically human or anthropic is involved.\nThe anthropic principle has given rise to some confusion and controversy, partly because the phrase has been applied to several distinct ideas. All versions of the principle have been accused of discouraging the search for a deeper physical understanding of the universe. Critics of the weak anthropic principle point out that its lack of falsifiability entails that it is non-scientific and therefore inherently not useful. Stronger variants of the anthropic principle which are not tautologies can still make claims considered controversial by some; these would be contingent upon empirical verification.\nAnthropic observations.\nIn 1961, Robert Dicke noted that the age of the universe, as seen by living observers, cannot be random. Instead, biological factors constrain the universe to be more or less in a \"golden age\", neither too young nor too old. If the universe was one tenth as old as its present age, there would not have been sufficient time to build up appreciable levels of metallicity (levels of elements besides hydrogen and helium) especially carbon, by nucleosynthesis. Small rocky planets did not yet exist. If the universe were 10 times older than it actually is, most stars would be too old to remain on the main sequence and would have turned into white dwarfs, aside from the dimmest red dwarfs, and stable planetary systems would have already come to an end. Thus, Dicke explained the coincidence between large dimensionless numbers constructed from the constants of physics and the age of the universe, a coincidence that inspired Dirac's varying-\"G\" theory.\nDicke later reasoned that the density of matter in the universe must be almost exactly the critical density needed to prevent the Big Crunch (the \"Dicke coincidences\" argument). The most recent measurements may suggest that the observed density of baryonic matter, and some theoretical predictions of the amount of dark matter, account for about 30% of this critical density, with the rest contributed by a cosmological constant. Steven Weinberg gave an anthropic explanation for this fact: he noted that the cosmological constant has a remarkably low value, some 120 orders of magnitude smaller than the value particle physics predicts (this has been described as the \"worst prediction in physics\"). However, if the cosmological constant were only several orders of magnitude larger than its observed value, the universe would suffer catastrophic inflation, which would preclude the formation of stars, and hence life.\nThe observed values of the dimensionless physical constants (such as the fine-structure constant) governing the four fundamental interactions are balanced as if fine-tuned to permit the formation of commonly found matter and subsequently the emergence of life. A slight increase in the strong interaction (up to 50% for some authors) would bind the dineutron and the diproton and convert all hydrogen in the early universe to helium; likewise, an increase in the weak interaction also would convert all hydrogen to helium. Water, as well as sufficiently long-lived stable stars, both essential for the emergence of life as it is known, would not exist. More generally, small changes in the relative strengths of the four fundamental interactions can greatly affect the universe's age, structure, and capacity for life.\nOrigin.\nThe phrase \"anthropic principle\" first appeared in Brandon Carter's contribution to a 1973 Krak\u00f3w symposium honouring Copernicus's 500th birthday. Carter, a theoretical astrophysicist, articulated the Anthropic Principle in reaction to the Copernican Principle, which states that humans do not occupy a privileged position in the Universe. Carter said: \"Although our situation is not necessarily \"central\", it is inevitably privileged to some extent.\" Specifically, Carter disagreed with using the Copernican principle to justify the Perfect Cosmological Principle, which states that all large regions \"and times\" in the universe must be statistically identical. The latter principle underlies the steady-state theory, which had recently been falsified by the 1965 discovery of the cosmic microwave background radiation. This discovery was unequivocal evidence that the universe has changed radically over time (for example, via the Big Bang).\nCarter defined two forms of the anthropic principle, a \"weak\" one which referred only to anthropic selection of privileged spacetime locations in the universe, and a more controversial \"strong\" form that addressed the values of the fundamental constants of physics.\nRoger Penrose explained the weak form as follows:\nOne reason this is plausible is that there are many other places and times in which humans could have evolved. But when applying the strong principle, there is only one universe, with one set of fundamental parameters, so what exactly is the point being made? Carter offers two possibilities: First, humans can use their own existence to make \"predictions\" about the parameters. But second, \"as a last resort\", humans can convert these predictions into \"explanations\" by assuming that there \"is\" more than one universe, in fact a large and possibly infinite collection of universes, something that is now called the multiverse (\"world ensemble\" was Carter's term), in which the parameters (and perhaps the laws of physics) vary across universes. The strong principle then becomes an example of a selection effect, exactly analogous to the weak principle. Postulating a multiverse is certainly a radical step, but taking it could provide at least a partial answer to a question seemingly out of the reach of normal science: \"Why do the fundamental laws of physics take the particular form we observe and not another?\"\nSince Carter's 1973 paper, the term \"anthropic principle\" has been extended to cover a number of ideas that differ in important ways from his. Particular confusion was caused by the 1986 book \"The Anthropic Cosmological Principle\" by John D. Barrow and Frank Tipler, which distinguished between a \"weak\" and \"strong\" anthropic principle in a way very different from Carter's, as discussed in the next section.\nCarter was not the first to invoke some form of the anthropic principle. In fact, the evolutionary biologist Alfred Russel Wallace anticipated the anthropic principle as long ago as 1904: \"Such a vast and complex universe as that which we know exists around us, may have been absolutely required [...] in order to produce a world that should be precisely adapted in every detail for the orderly development of life culminating in man.\" In 1957, Robert Dicke wrote: \"The age of the Universe 'now' is not random but conditioned by biological factors [...] [changes in the values of the fundamental constants of physics] would preclude the existence of man to consider the problem.\"\nLudwig Boltzmann may have been one of the first in modern science to use anthropic reasoning. Prior to knowledge of the Big Bang Boltzmann's thermodynamic concepts painted a picture of a universe that had inexplicably low entropy. Boltzmann suggested several explanations, one of which relied on fluctuations that could produce pockets of low entropy or Boltzmann universes. While most of the universe is featureless in this model, to Boltzmann, it is unremarkable that humanity happens to inhabit a Boltzmann universe, as that is the only place where intelligent life could be.\nVariants.\nWeak anthropic principle (WAP) (Carter): \"... our location in the universe is \"necessarily\" privileged to the extent of being compatible with our existence as observers.\" For Carter, \"location\" refers to our location in time as well as space.\nStrong anthropic principle (SAP) (Carter): \"[T]he universe (and hence the fundamental parameters on which it depends) must be such as to admit the creation of observers within it at some stage. To paraphrase Descartes, \"cogito ergo mundus talis est\".\"The Latin tag (\"I think, therefore the world is such [as it is]\") makes it clear that \"must\" indicates a deduction from the fact of our existence; the statement is thus a truism.\nIn their 1986 book, \"The anthropic cosmological principle\", John Barrow and Frank Tipler depart from Carter and define the WAP and SAP as follows:\nWeak anthropic principle (WAP) (Barrow and Tipler): \"The observed values of all physical and cosmological quantities are not equally probable but they take on values restricted by the requirement that there exist sites where carbon-based life can evolve and by the requirements that the universe be old enough for it to have already done so.\"Unlike Carter they restrict the principle to carbon-based life, rather than just \"observers\". A more important difference is that they apply the WAP to the fundamental physical constants, such as the fine-structure constant, the number of spacetime dimensions, and the cosmological constant\u2014topics that fall under Carter's SAP.\nStrong anthropic principle (SAP) (Barrow and Tipler): \"The Universe must have those properties which allow life to develop within it at some stage in its history.\"This looks very similar to Carter's SAP, but unlike the case with Carter's SAP, the \"must\" is an imperative, as shown by the following three possible elaborations of the SAP, each proposed by Barrow and Tipler:\nThe philosophers John Leslie and Nick Bostrom reject the Barrow and Tipler SAP as a fundamental misreading of Carter. For Bostrom, Carter's anthropic principle just warns us to make allowance for \"anthropic bias\"\u2014that is, the bias created by anthropic selection effects (which Bostrom calls \"observation\" selection effects)\u2014the necessity for observers to exist in order to get a result. He writes:\nStrong self-sampling assumption (SSSA) (Bostrom): \"Each observer-moment should reason as if it were randomly selected from the class of all observer-moments in its reference class.\" Analysing an observer's experience into a sequence of \"observer-moments\" helps avoid certain paradoxes; but the main ambiguity is the selection of the appropriate \"reference class\": for Carter's WAP this might correspond to all real or potential observer-moments in our universe; for the SAP, to all in the multiverse. Bostrom's mathematical development shows that choosing either too broad or too narrow a reference class leads to counter-intuitive results, but he is not able to prescribe an ideal choice.\nAccording to J\u00fcrgen Schmidhuber, the anthropic principle essentially just says that the conditional probability of finding yourself in a universe compatible with your existence is always 1. It does not allow for any additional nontrivial predictions such as \"gravity won't change tomorrow\". To gain more predictive power, additional assumptions on the prior distribution of alternative universes are necessary.\nPlaywright and novelist Michael Frayn describes a form of the strong anthropic principle in his 2006 book \"The Human Touch\", which explores what he characterises as \"the central oddity of the Universe\":\nCharacter of anthropic reasoning.\nCarter chose to focus on a tautological aspect of his ideas, which has resulted in much confusion. In fact, anthropic reasoning interests scientists because of something that is only implicit in the above formal definitions, namely that humans should give serious consideration to there being other universes with different values of the \"fundamental parameters\"\u2014that is, the dimensionless physical constants and initial conditions for the Big Bang. Carter and others have argued that life would not be possible in most such universes. In other words, the universe humans live in is fine tuned to permit life. Collins &amp; Hawking (1973) characterized Carter's then-unpublished big idea as the postulate that \"there is not one universe but a whole infinite ensemble of universes with all possible initial conditions\". If this is granted, the anthropic principle provides a plausible explanation for the fine tuning of our universe: the \"typical\" universe is not fine-tuned, but given enough universes, a small fraction will be capable of supporting intelligent life. Ours must be one of these, and so the observed fine tuning should be no cause for wonder.\nAlthough philosophers have discussed related concepts for centuries, in the early 1970s the only genuine physical theory yielding a multiverse of sorts was the many-worlds interpretation of quantum mechanics. This would allow variation in initial conditions, but not in the truly fundamental constants. Since that time a number of mechanisms for producing a multiverse have been suggested: see the review by Max Tegmark. An important development in the 1980s was the combination of inflation theory with the hypothesis that some parameters are determined by symmetry breaking in the early universe, which allows parameters previously thought of as \"fundamental constants\" to vary over very large distances, thus eroding the distinction between Carter's weak and strong principles. At the beginning of the 21st century, the string landscape emerged as a mechanism for varying essentially all the constants, including the number of spatial dimensions.\nThe anthropic idea that fundamental parameters are selected from a multitude of different possibilities (each actual in some universe or other) contrasts with the traditional hope of physicists for a theory of everything having no free parameters. As Albert Einstein said: \"What really interests me is whether God had any choice in the creation of the world.\" In 2002, some proponents of the leading candidate for a \"theory of everything\", string theory, proclaimed \"the end of the anthropic principle\" since there would be no free parameters to select. In 2003, however, Leonard Susskind stated: \"...\u00a0it seems plausible that the landscape is unimaginably large and diverse. This is the behavior that gives credence to the anthropic principle.\"&lt;ref name=\"arXiv:hep-th/0302219\"&gt;&lt;/ref&gt;\nThe modern form of a design argument is put forth by intelligent design. Proponents of intelligent design often cite the fine-tuning observations that (in part) preceded the formulation of the anthropic principle by Carter as a proof of an intelligent designer. Opponents of intelligent design are not limited to those who hypothesize that other universes exist; they may also argue, anti-anthropically, that the universe is less fine-tuned than often claimed, or that accepting fine tuning as a brute fact is less astonishing than the idea of an intelligent creator. Furthermore, even accepting fine tuning, Sober (2005) and Ikeda and Jefferys, argue that the anthropic principle as conventionally stated actually undermines intelligent design.\nPaul Davies's book \"The Goldilocks Enigma\" (2006) reviews the current state of the fine-tuning debate in detail, and concludes by enumerating the following responses to that debate:\nOmitted here is Lee Smolin's model of cosmological natural selection, also known as \"fecund universes\", which proposes that universes have \"offspring\" that are more plentiful if they resemble our universe. Also see Gardner (2005).\nClearly each of these hypotheses resolve some aspects of the puzzle, while leaving others unanswered. Followers of Carter would admit only option 3 as an anthropic explanation, whereas 3 through 6 are covered by different versions of Barrow and Tipler's SAP (which would also include 7 if it is considered a variant of 4, as in Tipler 1994).\nThe anthropic principle, at least as Carter conceived it, can be applied on scales much smaller than the whole universe. For example, Carter (1983) inverted the usual line of reasoning and pointed out that when interpreting the evolutionary record, one must take into account cosmological and astrophysical considerations. With this in mind, Carter concluded that given the best estimates of the age of the universe, the evolutionary chain culminating in \"Homo sapiens\" probably admits only one or two low probability links.\nObservational evidence.\nNo possible observational evidence bears on Carter's WAP, as it is merely advice to the scientist and asserts nothing debatable. The obvious test of Barrow's SAP, which says that the universe is \"required\" to support life, is to find evidence of life in universes other than ours. Any other universe is, by most definitions, unobservable (otherwise it would be included in \"our\" portion of \"this\" universe). Thus, in principle Barrow's SAP cannot be falsified by observing a universe in which an observer cannot exist.\nPhilosopher John Leslie states that the Carter SAP (with multiverse) predicts the following:\nHogan has emphasised that it would be very strange if all fundamental constants were strictly determined, since this would leave us with no ready explanation for apparent fine tuning. In fact, humans might have to resort to something akin to Barrow and Tipler's SAP: there would be no option for such a universe \"not\" to support life.\nProbabilistic predictions of parameter values can be made given:\nThe probability of observing value \"X\" is then proportional to . A generic feature of an analysis of this nature is that the expected values of the fundamental physical constants should not be \"over-tuned\", i.e. if there is some perfectly tuned predicted value (e.g. zero), the observed value need be no closer to that predicted value than what is required to make life possible. The small but finite value of the cosmological constant can be regarded as a successful prediction in this sense.\nOne thing that would \"not\" count as evidence for the anthropic principle is evidence that the Earth or the Solar System occupied a privileged position in the universe, in violation of the Copernican principle (for possible counterevidence to this principle, see Copernican principle), unless there was some reason to think that that position was a necessary condition for our existence as observers.\nApplications of the principle.\nThe nucleosynthesis of carbon-12.\nFred Hoyle may have invoked anthropic reasoning to predict an astrophysical phenomenon. He is said to have reasoned, from the prevalence on Earth of life forms whose chemistry was based on carbon-12 nuclei, that there must be an undiscovered resonance in the carbon-12 nucleus facilitating its synthesis in stellar interiors via the triple-alpha process. He then calculated the energy of this undiscovered resonance to be 7.6 million electronvolts. Willie Fowler's research group soon found this resonance, and its measured energy was close to Hoyle's prediction.\nHowever, in 2010 Helge Kragh argued that Hoyle did not use anthropic reasoning in making his prediction, since he made his prediction in 1953 and anthropic reasoning did not come into prominence until 1980. He called this an \"anthropic myth\", saying that Hoyle and others made an after-the-fact connection between carbon and life decades after the discovery of the resonance.\nCosmic inflation.\nDon Page criticized the entire theory of cosmic inflation as follows. He emphasized that initial conditions that made possible a thermodynamic arrow of time in a universe with a Big Bang origin, must include the assumption that at the initial singularity, the entropy of the universe was low and therefore extremely improbable. Paul Davies rebutted this criticism by invoking an inflationary version of the anthropic principle. While Davies accepted the premise that the initial state of the visible universe (which filled a microscopic amount of space before inflating) had to possess a very low entropy value\u2014due to random quantum fluctuations\u2014to account for the observed thermodynamic arrow of time, he deemed this fact an advantage for the theory. That the tiny patch of space from which our observable universe grew had to be extremely orderly, to allow the post-inflation universe to have an arrow of time, makes it unnecessary to adopt any \"ad hoc\" hypotheses about the initial entropy state, hypotheses other Big Bang theories require.\nString theory.\nString theory predicts a large number of possible universes, called the \"backgrounds\" or \"vacua\". The set of these vacua is often called the \"multiverse\" or \"anthropic landscape\" or \"string landscape\". Leonard Susskind has argued that the existence of a large number of vacua puts anthropic reasoning on firm ground: only universes whose properties are such as to allow observers to exist are observed, while a possibly much larger set of universes lacking such properties go unnoticed.\nSteven Weinberg believes the anthropic principle may be appropriated by cosmologists committed to nontheism, and refers to that principle as a \"turning point\" in modern science because applying it to the string landscape \"may explain how the constants of nature that we observe can take values suitable for life without being fine-tuned by a benevolent creator\". Others\u2014most notably David Gross but also Lubo\u0161 Motl, Peter Woit, and Lee Smolin\u2014argue that this is not predictive. Max Tegmark, Mario Livio, and Martin Rees argue that only some aspects of a physical theory need be observable and/or testable for the theory to be accepted, and that many well-accepted theories are far from completely testable at present.\nJ\u00fcrgen Schmidhuber (2000\u20132002) points out that Ray Solomonoff's theory of universal inductive inference and its extensions already provide a framework for maximizing our confidence in any theory, given a limited sequence of physical observations, and some prior distribution on the set of possible explanations of the universe.\nZhi-Wei Wang and Samuel L. Braunstein proved that life's existence in the universe depends on various fundamental constants. It suggests that without a complete understanding of these constants, one might incorrectly perceive the universe as being intelligently designed for life. This perspective challenges the view that our universe is unique in its ability to support life.\nDimensions of spacetime.\nThere are two kinds of dimensions: spatial (bidirectional) and temporal (unidirectional). Let the number of spatial dimensions be \"N\" and the number of temporal dimensions be \"T\". That and , setting aside the compactified dimensions invoked by string theory and undetectable to date, can be explained by appealing to the physical consequences of letting \"N\" differ from 3 and \"T\" differ from 1. The argument is often of an anthropic character and possibly the first of its kind, albeit before the complete concept came into vogue.\nThe implicit notion that the dimensionality of the universe is special is first attributed to Gottfried Wilhelm Leibniz, who in the Discourse on Metaphysics suggested that the world is \"\". Immanuel Kant argued that 3-dimensional space was a consequence of the inverse square law of universal gravitation. While Kant's argument is historically important, John D. Barrow said that it \"gets the punch-line back to front: it is the three-dimensionality of space that explains why we see inverse-square force laws in Nature, not vice-versa\" (Barrow 2002:204).\nIn 1920, Paul Ehrenfest showed that if there is only a single time dimension and more than three spatial dimensions, the orbit of a planet about its Sun cannot remain stable. The same is true of a star's orbit around the center of its galaxy. Ehrenfest also showed that if there are an even number of spatial dimensions, then the different parts of a wave impulse will travel at different speeds. If there are formula_1 spatial dimensions, where \"k\" is a positive whole number, then wave impulses become distorted. In 1922, Hermann Weyl claimed that Maxwell's theory of electromagnetism can be expressed in terms of an action only for a four-dimensional manifold. Finally, Tangherlini showed in 1963 that when there are more than three spatial dimensions, electron orbitals around nuclei cannot be stable; electrons would either fall into the nucleus or disperse.\nMax Tegmark expands on the preceding argument in the following anthropic manner. If \"T\" differs from 1, the behavior of physical systems could not be predicted reliably from knowledge of the relevant partial differential equations. In such a universe, intelligent life capable of manipulating technology could not emerge. Moreover, if , Tegmark maintains that protons and electrons would be unstable and could decay into particles having greater mass than themselves. (This is not a problem if the particles have a sufficiently low temperature.) Lastly, if , gravitation of any kind becomes problematic, and the universe would probably be too simple to contain observers. For example, when , nerves cannot cross without intersecting. Hence anthropic and other arguments rule out all cases except and , which describes the world around us.\nOn the other hand, in view of creating black holes from an ideal monatomic gas under its self-gravity, Wei-Xiang Feng showed that -dimensional spacetime is the marginal dimensionality. Moreover, it is the unique dimensionality that can afford a \"stable\" gas sphere with a \"positive\" cosmological constant. However, a self-gravitating gas cannot be stably bound if the mass sphere is larger than ~1021 solar masses, due to the small positivity of the cosmological constant observed.\nIn 2019, James Scargill argued that complex life may be possible with two spatial dimensions. According to Scargill, a purely scalar theory of gravity may enable a local gravitational force, and 2D networks may be sufficient for complex neural networks.\nMetaphysical interpretations.\nSome of the metaphysical disputes and speculations include, for example, attempts to back Pierre Teilhard de Chardin's earlier interpretation of the universe as being Christ centered (compare Omega Point), expressing a \"creatio evolutiva\" instead the elder notion of \"creatio continua\". From a strictly secular, humanist perspective, it allows as well to put human beings back in the center, an anthropogenic shift in cosmology. Karl W. Giberson has laconically stated that\nWilliam Sims Bainbridge disagreed with de Chardin's optimism about a future Omega point at the end of history, arguing that logically, humans are trapped at the Omicron point, in the middle of the Greek alphabet rather than advancing to the end, because the universe does not need to have any characteristics that would support our further technical progress, if the anthropic principle merely requires it to be suitable for our evolution to this point.\n\"The anthropic cosmological principle\".\nA thorough extant study of the anthropic principle is the book \"The anthropic cosmological principle\" by John D. Barrow, a cosmologist, and Frank J. Tipler, a cosmologist and mathematical physicist. This book sets out in detail the many known anthropic coincidences and constraints, including many found by its authors. While the book is primarily a work of theoretical astrophysics, it also touches on quantum physics, chemistry, and earth science. An entire chapter argues that \"Homo sapiens\" is, with high probability, the only intelligent species in the Milky Way.\nThe book begins with an extensive review of many topics in the history of ideas the authors deem relevant to the anthropic principle, because the authors believe that principle has important antecedents in the notions of teleology and intelligent design. They discuss the writings of Fichte, Hegel, Bergson, and Alfred North Whitehead, and the Omega Point cosmology of Teilhard de Chardin. Barrow and Tipler carefully distinguish teleological reasoning from \"eutaxiological\" reasoning; the former asserts that order must have a consequent purpose; the latter asserts more modestly that order must have a planned cause. They attribute this important but nearly always overlooked distinction to an obscure 1883 book by L. E. Hicks.\nSeeing little sense in a principle requiring intelligent life to emerge while remaining indifferent to the possibility of its eventual extinction, Barrow and Tipler propose the final anthropic principle (FAP): Intelligent information-processing must come into existence in the universe, and, once it comes into existence, it will never die out.\nBarrow and Tipler submit that the FAP is both a valid physical statement and \"closely connected with moral values\". FAP places strong constraints on the structure of the universe, constraints developed further in Tipler's \"The Physics of Immortality\". One such constraint is that the universe must end in a Big Crunch, which seems unlikely in view of the tentative conclusions drawn since 1998 about dark energy, based on observations of very distant supernovas.\nIn his review of Barrow and Tipler, Martin Gardner ridiculed the FAP by quoting the last two sentences of their book as defining a completely ridiculous anthropic principle (CRAP):\nReception and controversies.\nCarter has frequently expressed regret for his own choice of the word \"anthropic\", because it conveys the misleading impression that the principle involves \"humans in particular\", to the exclusion of non-human intelligence more broadly. Others have criticised the word \"principle\" as being too grandiose to describe straightforward applications of selection effects.\nA common criticism of Carter's SAP is that it is an easy \"deus ex machina\" that discourages searches for physical explanations. To quote Penrose again: \"It tends to be invoked by theorists whenever they do not have a good enough theory to explain the observed facts.\"\nCarter's SAP and Barrow and Tipler's WAP have been dismissed as truisms or trivial tautologies\u2014that is, statements true solely by virtue of their logical form and not because a substantive claim is made and supported by observation of reality. As such, they are criticized as an elaborate way of saying, \"If things were different, they would be different\", which is a valid statement, but does not make a claim of some factual alternative over another.\nCritics of the Barrow and Tipler SAP claim that it is neither testable nor falsifiable, and thus is not a scientific statement but rather a philosophical one. The same criticism has been leveled against the hypothesis of a multiverse, although some argue that it does make falsifiable predictions. A modified version of this criticism is that humanity understands so little about the emergence of life, especially intelligent life, that it is effectively impossible to calculate the number of observers in each universe. Also, the prior distribution of universes as a function of the fundamental constants is easily modified to get any desired result.\nMany criticisms focus on versions of the strong anthropic principle, such as Barrow and Tipler's \"anthropic cosmological principle\", which are teleological notions that tend to describe the existence of life as a \"necessary prerequisite\" for the observable constants of physics. Similarly, Stephen Jay Gould, Michael Shermer, and others claim that the stronger versions of the anthropic principle seem to reverse known causes and effects. Gould compared the claim that the universe is fine-tuned for the benefit of our kind of life to saying that sausages were made long and narrow so that they could fit into modern hotdog buns, or saying that ships had been invented to house barnacles. These critics cite the vast physical, fossil, genetic, and other biological evidence consistent with life having been fine-tuned through natural selection to adapt to the physical and geophysical environment in which life exists. Life appears to have adapted to the universe, and not vice versa.\nSome applications of the anthropic principle have been criticized as an argument by lack of imagination, for tacitly assuming that carbon compounds and water are the only possible chemistry of life (sometimes called \"carbon chauvinism\"; see also alternative biochemistry). The range of fundamental physical constants consistent with the evolution of carbon-based life may also be wider than those who advocate a fine-tuned universe have argued. For instance, Harnik et al. propose a Weakless Universe in which the weak nuclear force is eliminated. They show that this has no significant effect on the other fundamental interactions, provided some adjustments are made in how those interactions work. However, if some of the fine-tuned details of our universe were violated, that would rule out complex structures of any kind\u2014stars, planets, galaxies, etc.\nLee Smolin has offered a theory designed to improve on the lack of imagination that has been ascribed to anthropic principles. He puts forth his fecund universes theory, which assumes universes have \"offspring\" through the creation of black holes whose offspring universes have values of physical constants that depend on those of the mother universe.\nThe philosophers of cosmology John Earman, Ernan McMullin, and Jes\u00fas Moster\u00edn contend that \"in its weak version, the anthropic principle is a mere tautology, which does not allow us to explain anything or to predict anything that we did not already know. In its strong version, it is a gratuitous speculation\". A further criticism by Moster\u00edn concerns the flawed \"anthropic\" inference from the assumption of an infinity of worlds to the existence of one like ours:"}
{"id": "2793", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2793", "title": "Application program", "text": ""}
{"id": "2795", "revid": "1273116153", "url": "https://en.wikipedia.org/wiki?curid=2795", "title": "Australian Army", "text": "The Australian Army is the principal land warfare force of Australia. It is a part of the Australian Defence Force (ADF), along with the Royal Australian Navy and the Royal Australian Air Force. The Army is commanded by the Chief of Army (CA), who is subordinate to the Chief of the Defence Force (CDF) who commands the ADF. The CA is also directly responsible to the Minister for Defence, with the Department of Defence administering the ADF and the Army.\nFormed in 1901, as the Commonwealth Military Forces, through the amalgamation of the colonial forces of Australia following the Federation of Australia. Although Australian soldiers have been involved in a number of minor and major conflicts throughout Australia's history, only during the Second World War has Australian territory come under direct attack.\nThe Australian Army was initially composed almost completely of part-time soldiers, where the vast majority were in units of the Citizens Military Force (CMF or Militia) (1901\u20131980) during peacetime, with limits set on the regular Army. Since all reservists were barred from forcibly serving overseas, volunteer expeditionary forces (1st AIF, ANMEF, 2nd AIF) were formed to enable the Army to send large numbers of soldiers to serve overseas during periods of war. This period lasted from federation until post-1947, when a standing peacetime regular army was formed and the Australian Army Reserve (1980\u2013present) began to decline in importance.\nDuring its history, the Australian Army has fought in a number of major wars, including the Second Boer War, the First and Second World Wars, Korean War, Malayan Emergency, Indonesia-Malaysia Confrontation, Vietnam War, the War in Afghanistan (2001\u20132021) and the Iraq War. Since 1947, the Australian Army has also been involved in many peacekeeping operations, usually under the auspices of the United Nations. Today, it participates in multilateral and unilateral military exercises and provides emergency disaster relief and humanitarian aid in response to domestic and international crises.\nHistory.\nFormation.\nFormed in March 1901, following federation, the Australian Army initially consisted of the six, disbanded and separate, colonial military forces' land components. Due to the Army being continuation of the colonial armies, it became immediately embroiled in conflict as contingents had been committed to fight for the United Kingdom of Great Britain and Ireland in the Second Boer War. The Army gained command of these contingents and even supplied federal units to reinforce their commitment at the request of the British government.\nThe \"Defence Act 1903,\" established the operation and command structure of the Australian Army. In 1911, the Universal Service Scheme was implemented, introducing conscription for the first time in Australia, with males aged 14\u201326 assigned into cadet and CMF units; though the scheme did not prescribe or allow overseas service outside the states and territories of Australia. This restriction would be primarily, and continually, bypassed through the process of raising separate volunteer forces until the mid-20th century; this solution was not without its drawbacks, as it caused logistical dilemmas.\nWorld War I.\nAfter the declaration of war on the Central Powers, the Australian Army raised the all volunteer First Australian Imperial Force (AIF) which had an initial recruitment of 52,561 out of a promised 20,000 men. A smaller expeditionary force, the Australian Naval and Military Expeditionary Force (ANMEF), dealt with the issue of the German Pacific holdings. ANMEF recruitment began on 10 August 1914, and operations started 10 days later. On 11 September, the ANMEF landed at Rabaul to secure German New Guinea, with no German outposts in the Pacific left by November 1914. During the AIF's preparations to depart Australia, the Ottoman Empire joined the Central Powers; thereby receiving declarations of war from the Allies of World War I in early November 1914.\nAfter initial recruitment and training, the AIF departed for Egypt where they underwent further preparations, and where the Australian and New Zealand Army Corps (ANZAC) was formed. Their presence in Egypt was due to the planned Gallipoli campaign, an invasion of the Ottoman Empire via Gallipoli. On 25 April, the AIF landed at ANZAC Cove, which signaled the start of Australia's contribution to the campaign. Following little initial success, fighting quickly devolved into trench warfare, which precipitated a stalemate. On 15 December 1915, after eight months of fighting, the evacuation of Gallipoli commenced; it was completed 5 days later with no casualties recorded. After regrouping in Egypt, the AIF was split into two groups and further expanded with reinforcements. This division would see a majority of the Australian Light Horse fight the Ottomans in Arabia and the Levant, whereas\nthe rest of the AIF would go to the Western Front.\nWestern Front.\nThe AIF arrived in France with the 1st, 2nd, 4th and 5th Divisions; which comprised, in part, I ANZAC Corps and, in full, II ANZAC Corps. The 3rd Division would not arrive until November 1916, as it underwent training in England after its transfer from Australia. In July 1916, the AIF commenced operations with the Battle of the Somme, and more specifically with the Attack at Fromelles. Soon after, the 1st, 2nd and 4th Divisions became tied down in actions at the Battle of Pozi\u00e8res and Mouquet Farm. In around six weeks, the operations caused 28,000 Australian casualties. Due to these losses and pressure from the United Kingdom to maintain the AIF's manpower, Prime Minister Billy Hughes introduced the first conscription plebiscite. It was defeated by a narrow margin and created a bitter divide on the issue of conscription throughout the 20th century.\nFollowing the German withdrawal to the Hindenburg Line in March 1917, which was better defended and eased manpower restraints, the first Australian assault on the Hindenburg Line occurred on 11 April 1917 with the First Battle of Bullecourt. On 20 September, the Australian contingent joined the Third Battle of Ypres with the Battle of Menin Road, and continued on to fight in the Battle of Polygon Wood, which lasted until 3 October; in total, these tow operations cost roughly 11,000 in Australian casualties. Until 15 November 1917, multiple attacks at the Battle of Broodseinde Ridge and the Battle of Passchendaele occurred, but, failed to take their objectives following the start of the rain and subsequent muddying of the fields.\nOn 21 March 1918, the Germans attempted a breakout through the Michael Offensive, which was part of the much larger German spring offensive; the AIF suffered 15,000 casualties due to this effort. During this operation, Australian troops conducted a series of local defences and offensives to hold and retake Villers\u2013Brettoneux over the period 4 to 25 April 1918. After the cessation of offensives by the German Army, the Australian Corps began participating in \"Peaceful penetration\" operations, which were localised raids designed to harass and gain small tracts of territory; these proved so effective that several major operational objectives were captured.\nOn 4 July 1918, the Battle of Hamel saw the first successful use of tanks alongside Australians, with the battleplan of John Monash completed three minutes over the planned 90 minute operation. Following this success, the Battle of Amiens was launched on 8 August 1918, in conjunction with the Canadian Corps and the British III Corps, and concluded on 12 August 1918; General Erich Ludendorff described it as \"the black day of the German Army\". On 29 August 1918, following territorial advances and pursuits, the AIF attacked P\u00e8ronne and subsequently initiated the Battle of Mont St Quentin. Another operation around \u00c9pehy was planned for 18 September 1918, which aimed to retake the British trenches and, potentially, capture their most ambitious objective of the Hindenburg's outpost line \u2013 which they achieved.\nFollowing news of a three-month furlough for certain soldiers, seven AIF battalions were disbanded; consequently, members of these battalions mutinied. Soon after the penetration of the Hindenburg Line, plans for the breakthrough of the main trench, with the Australian Corps as the vanguard, were completed. However, due to manpower issues, only the 3rd and 5th Divisions participated, with the American Expeditionary Forces' 27th and 30th Divisions given as reinforcements. On 29 September, following a three day long bombardment, the Battle of the Hindenburg Line commenced, wherein the corps attacked and captured more of the line. On 5 October 1918, after furious fighting, the Australian Corps was withdrawn from the front, as the entire corps had been operating continuously since 8 August 1918. They would not return to the battlefield, as Germany signed the Armistice of 11 November 1918 that ultimately ended the war on the Western Front.\nMiddle East.\nThe Australian mounted units, composed of the ANZAC Mounted Division and eventually the Australian Mounted Division, participated in the Sinai and Palestine campaign. They were originally stationed there to protect the Suez Canal from the Turks, and following the threat of its capture passing, they started offensive operations and helped in the re-conquest of the Sinai Desert. This was followed by the Battles of Gaza, wherein on the 31 October 1917 the 4th and 12th Light Horse took Beersheba through the last charge of the Light Horse. They continued on to capture Jerusalem on 10 December 1917 and then eventually Damascus on 1 October 1918 whereby, a few days later on 10 October 1918, the Ottoman Empire surrendered.\nInterbellum.\nRepatriation efforts were implemented between the armistice and the end of 1919, which occurred after the disbandment of the Australian Imperial Force. In 1921, CMF units were renumbered to that of the AIF, to perpetuate the honours and numerical identities of the units involved in WW1. During this period there was a complacency towards matters of defence, due to the devastating effects of the previous war on the Australian psyche. Following the election of Prime Minister James Scullin in 1929, two events occurred that substantially affected the armed forces: conscription was abolished and the economic effects of the Great Depression started to be felt in Australia. The economic ramifications of the depression led to decisions that decreased defence expenditure and manpower for the army. Since conscription was repealed, to reflect the new volunteer nature of the Citizens Forces, the CMF was renamed to the Militia.\nWorld War II.\nFollowing the declaration of war on Nazi Germany and her allies by the United Kingdom, and the subsequent confirmation by Prime Minister Robert Menzies on 3 September 1939, the Australian Army raised the Second Australian Imperial Force, a 20,000-strong volunteer expeditionary force, which initially consisted of the 6th Division; later increased to include the 7th and 9th Divisions, alongside the 8th Division which was sent to Singapore. In October 1939, compulsory military training recommenced for unmarried men aged 21, who had to complete three months of training.\nThe 2nd AIF commenced its first operations in North Africa with Operation Compass, that began with the Battle of Bardia. This was followed by supplying Australian units to defend against the Axis in the Battle of Greece. After the evacuation of Greece, Australian troops took part in the Battle of Crete which, though more successful, still failed and another withdrawal was ordered. During the Greek Campaign, the Allies were pushed back to Egypt and the Siege of Tobruk began. Tobruk's primary defence personnel were Australians of the 9th Division; the so-called 'Rats of Tobruk'. Additionally, the AIF participated in the Syria\u2013Lebanon campaign. The 9th Division fought in the First and Second Battle of El Alamein before also being shipped home to fight the Japanese.\nPacific.\nIn December 1941, following the Bombing of Pearl Harbor, Australia declared war on Japan. Consequently, the AIF was requested to return home, as the subsequent rapid conquest of Southeast Asia extremely concerned Australian policymakers, and the militia was mobilised. After the Fall of Singapore, and the consequent capture of the entire 8th Division as POWs, this concern only grew. These events hastened the relief of the Rats of Tobruk, while the other divisions were immediately recalled to reinforce New Guinea. General conscription was reintroduced, though service was again limited to Australian possessions, which caused tension between the AIF and Militia. This was in addition to the CMF's perceived inferior fighting ability, with these grievances earning the Militia their nicknames of \"koalas\" and \"chocos\" or \"chocolate soldiers\".\nThe Imperial Japanese Navy's failure in the Battle of the Coral Sea, was the impetus for the Imperial Japanese Army to try to capture Port Moresby via the Owen Stanley Range. On 21 July 1942, the Japanese began the Kokoda Campaign after landing at Gona; attempts to defeat them by Australian battalions were met with eventual success. Resultant offensive operations concluded with the Japanese being driven out of New Guinea entirely. In parallel with these defences, the Battle of Milne Bay was waged, and when the Japanese were repulsed, it was considered their first significant reversal for the war. In November 1942, the campaign ended after the Japanese withdrawal, with Australian advances leading to the Battle of Buna\u2013Gona.\nIn early 1943, the Salamaua\u2013Lae campaign began, with operations against the entrenched Japanese aimed towards recapturing the eponymous towns. This culminated in the capture of Lae, held by the 7th Division in early September 1943, from a successful combined amphibious landing at Lae and an airborne landing at Nadzab. The seaborne assault was notable as it was the first large\u2013scale amphibious operation since Gallipoli. Subsequently, Salamaua was taken days later on 11 September 1943, by a separate joint Australia\u2013US attack. The Battle of Lae was additionally part of the wider Huon Peninsula campaign. Following Lae's capture, the Battle of Finschhafen commenced with a relatively swift control of objectives, with subsequent Japanese counterattacks beaten off. On 17 November 1943, a major offensive that began with the Battle of Sattelberg, continued with the Battle of Wareo, and concluded with the Battle of Sio on 15 January 1944, was unleashed. The momentum of this advance was continued by the 8th Brigade, as they pursued the enemy in retreat, which culminated with the Battle of Madang.\nIn mid-1944, Australian forces took over the garrisoning of Torokina from the US with this changeover giving Australian command responsibility over the Bougainville campaign. Soon after arriving in November of the same year, the commander of II Corps, Lieutenant-General Stanley Savige, began an offensive to retake the island with the 3rd Division alongside the 11th and 23rd Brigades. The campaign lasted until the Japanese surrender, with controversy surrounding its little apparent significance to the war's conclusion, and the number of casualties incurred; this was one of Australia's most costliest campaigns in the Second World War.\nIn October 1944, Australian participation in the Aitape\u2013Wewak campaign began with the replacement of US forces at Aitape with the Australian 6th Division. US forces had previously captured the position, and had held it passively, though Australian command found this unsuitable. On 2 November 1944, the 2/6th Cavalry Commando Regiment was tasked with patrolling the area, wherein minor engagements were reported. In early December, the commandos were sent inland to establish access to the Torricelli Range, while the 19th Brigade handled patrolling, consequently, the amount of fierce fighting and territory secured increased. Following this success, thought was given for the capture of Maprik and Wewak, though supply became a major issue in this period. On 10 February 1945, the campaign's major offensive was underway, which resulted in both falling in quick succession on 22 April 1945. Smaller operations to secure the area continued, and all significant actions ceased by July.\nThe Borneo campaign was a series of three distinct amphibious operations that were undertaken by the 7th and 9th Divisions. The campaign began with the Battle of Tarakan on 1 May 1945, followed six weeks later by the Battle of Labuan, and concluded with the Battle of Balikpapan. The purpose of capturing Tarakan was to establish airfields, and the island was taken seven weeks following the initial amphibious landing. On 10 June 1945, the operation at Labuan commenced, and was tasked to secure resources and a naval base, and would continue until Japan's surrender. On 1 July 1945, the Balikpapan engagement commenced, with all its major objectives being acquired by war's end; this operation remains the largest amphibious operation undertaken by Australian forces, with 33,000 Australian servicemen participating. On 15 August 1945, Japan surrendered, ending the Second World War.\nCold War.\nKorean War.\nAfter the surrender of Japan, Australia provided a contingent to the British Commonwealth Occupation Force (BCOF) which included the 34th Brigade. The units that composed the brigade would eventually become the nucleus of the regular army, with the battalions and brigade being renumbered to reflect this change. Following the start of the Korean War, the Australian Army committed troops to fight against the North Korean forces; the units came from the Australian contribution to BCOF. The 3rd Battalion, Royal Australian Regiment (3RAR) arrived in Pusan on 28 September 1950. Australian troop numbers would increase and continue to be deployed up until the armistice, with 3RAR being eventually joined by the 1st Battalion, Royal Australian Regiment (1RAR). For a brief period, between 1951 and 1959, the Menzies Government reinstituted conscription and compulsory military training with the National Service Scheme, which required all males of eighteen years of age to serve for specified period in either the Australian Regular Army (ARA) or CMF.\nMalayan Emergency.\nThe Australian military entered the Malayan Emergency (1948\u20131960) in October 1955, committing the 2nd Battalion, Royal Australian Regiment (2RAR) to fight alongside Commonwealth forces. The 2RAR fought against the Malayan National Liberation Army (MNLA), a communist led guerrilla army whose goal was to turn Malaya into a socialist republic, and whose leaders had previously been trained and funded by Britain to resist the Japanese occupation of Malaya. Australian military operations in Malaya consisted of patrolling actions and guarding infrastructure, though they rarely saw combat as the emergency was nearly over by the time of their deployment. All three original Royal Australian Regiment battalions would complete at least one tour before the end of operations. In August 1963, Australia ended deployments to Malaya, three years after the emergency's official end.\nIndonesia\u2013Malaysia confrontation.\nIn 1962, the Borneo Confrontation began, due to Indonesia's opposition to the formation of Malaysia. It was an undeclared war that entailed a series of border conflicts between Indonesian-backed forces and British\u2013Malaysian allies. Initial Australian support in the conflict began, and continued throughout, with the training and supply of Malaysian troops; Australian soldiers only saw combat during defensive operations. In January 1965, permission was granted for the deployment of 3RAR, with extensive operations conducted in Sarawak from March until their withdrawal in July 1965. The subsequent deployment of 4th Battalion, Royal Australian Regiment (4RAR), in April 1966, was less intensive, with the battalion withdrawn in August. This is not to mention the efforts of several other corps and units in the conflict.\nVietnam War.\nThe Australian Army commenced its involvement in the Vietnam War by sending military advisors in 1962, which was then increased by sending in combat troops, specifically 1RAR, on 27 May 1965. Just before the official start of hostilities, the Australian Army was augmented with the reintroduction of conscription, which was based on a 'birthday ballot' selection process for all registered 20-year-old males. These men were required to register, unless they gave a legitimate reason for their exemption, else they faced penalties. This scheme would prove to be one of the most controversial implementations of conscription in Australia, with large protests against its adoption.\nIn March 1966, the Australian Army increased its commitment again with the replacement of 1RAR with the 1st Australian Task Force, a force in which all nine battalions of the Royal Australian Regiment would serve. One of the heaviest actions of the war occurred in August 1966, with the Battle of Long Tan, wherein D Company, 6th Battalion, Royal Australian Regiment (6RAR) successfully fended off an enemy force, estimated at 2,000 men, for four hours. In 1968, Australian forces defended against the Tet Offensive, a Viet Cong military operation, and repulsed them with few casualties. The contribution of personnel to the war was gradually wound down, starting in late-1970 and ending in 1972; the official declaration of the end of Australia's involvement in the war was made on 11 January 1973.\nActivities in Africa.\nFollowing the Vietnam War, there was a significant hiatus of operational activity by the Australian Army. In late 1979, in the largest deployment of the decade, the Army committed 151 troops to the Commonwealth Monitoring Force, which monitored the transition of Rhodesia to universal suffrage. A decade later in 1989, Australia deployed 300 army engineer personnel as the Australian contribution to the United Nations Transition Assistance Group in Namibia. The mission helped transition the country to independence from South African control.\nRecent history (1990\u2013present).\nPeacekeeping.\nFollowing the invasion of Kuwait by Iraq in August 1990, a coalition of countries sponsored by the United Nations Security Council, of which Australia was a part, gave a deadline for Iraq to withdraw from Kuwait of the 15 January 1991. Iraq refused to retreat and thus full conflict and the Gulf War began two days later on 17 January 1991. In January 1993, the Australian Army deployed 26 personnel on an ongoing rotational basis to the Multinational Force and Observers (MFO), as part of a non-United Nations peacekeeping organisation that observes and enforces the peace treaty between Israel and Egypt.\nAustralia's largest peacekeeping deployment began in 1999 with the International Force for East Timor, while other ongoing operations include peacekeeping in the Sinai (as part of MFO), and the United Nations Truce Supervision Organization (as part of Operation Paladin since 1956). Humanitarian relief after the 2004 Indian Ocean earthquake in Aceh Province, Indonesia, Operation Sumatra Assist, ended on 24 March 2005.\nAfghanistan and Iraq.\nFollowing the 11 September 2001 terrorist attacks, Australia promised troops to any military operations that the US commenced in response to the attacks. Subsequently, the Australian Army committed combat troops to Afghanistan in Operation Slipper. This combat role continued until the end of 2013 when it was replaced by a training contingent operating under Operation Highroad until 2021.\nAfter the Gulf War the UN imposed heavy restrictions on Iraq to stop them producing any Weapon of mass destruction. In the early 21st century, the US accused Iraq of possessing these weapons, and requested that the UN invade the country in response, a motion which Australia supported. The UN denied this motion, however, it did not stop a coalition, that Australia joined, invading the country; thus starting the Iraq War on 19 March 2003.\nBetween April 2015 and June 2020, the Army deployed a 300-strong element to Iraq, designated as Task Group Taji, as part of Operation Okra. In support of a capacity building mission, Task Group Taji's main role was to provide training to Iraqi forces, during which Australian troops have served alongside counterparts from New Zealand.\nIn 2020 an investigation of allegations of war crimes committed during Australian military operations in Afghanistan was concluded with the release of the Brereton Report. The report identified 25 ADF personnel that were involved directly or indirectly in the murder of 39 civilians and prisoners, with 19 referred to the Australian Federal Police to be criminally investigated. A 'warrior culture' in the SAS was specifically criticised with investigators 'frustrated by outright deceit by those who knew the truth and, not infrequently, misguided resistance to inquiries and investigations by their superiors'.\nOrganisation.\n1st (Australian) Division.\nBeginning 1 July 2023, the division was renamed the 1st Australian Division. The 1st, 3rd and 7th Brigades were placed under the direct control of the division's headquarters. This reform aimed to improve the connections between the divisional headquarters and the brigades it commands during deployments.\nForces Command.\nForces Command controls for administrative purposes all non-combat assets of the Australian Army. Its focus is on unifying all training establishments to create a base for scaling and mobilisation:\nAdditionally, Forces Command includes the following training and support establishments:\n2nd (Australian) Division.\nAdministers the reserve forces from its headquarters located in Sydney.\nAviation.\nArmy Aviation Command is responsible for the Australian Army's helicopters and training, aviation safety and unmanned aerial vehicles (UAV). Army Aviation Command comprises:\nSpecial Forces.\nSpecial Operations Command is a command formation of equal status to the other commands in the ADF and includes all of Army's special forces units. Special Operations Command comprises:\nColours, standards and guidons.\nInfantry, and some other combat units of the Australian Army carry flags called the King's Colour and the Regimental Colour, known as \"the Colours\". Armoured units carry Standards and Guidons \u2013 flags smaller than Colours and traditionally carried by Cavalry, Lancer, Light Horse and Mounted Infantry units. The 1st Armoured Regiment is the only unit in the Australian Army to carry a Standard, in the tradition of heavy armoured units. Artillery units' guns are considered to be their Colours, and on parade are provided with the same respect. Non-combat units (combat service support corps) do not have Colours, as Colours are battle flags and so are only available to combat units. As a substitute, many have Standards or Banners. Units awarded battle honours have them emblazoned on their Colours, Standards and Guidons. They are a link to the unit's past and a memorial to the fallen. Artillery do not have Battle Honours \u2013 their single Honour is \"Ubique\" which means \"Everywhere\" \u2013 although they can receive Honour Titles.\nThe Army is the guardian of the National Flag and as such, unlike the Royal Australian Air Force, does not have a flag or Colours. The Army, instead, has a banner, known as the Army Banner. To commemorate the centenary of the Army, the Governor General Sir William Deane, presented the Army with a new Banner at a parade in front of the Australian War Memorial on 10 March 2001. The banner was presented to the Regimental Sergeant Major of the Army (RSM-A), Warrant Officer Peter Rosemond.\nThe Army Banner bears the Australian Coat of Arms on the obverse, with the dates \"1901\u20132001\" in gold in the upper hoist. The reverse bears the Rising Sun badge of the Australian Army, flanked by seven campaign honours on small gold-edged scrolls: South Africa, World War I, World War II, Korea, Malaya-Borneo, South Vietnam, and Peacekeeping. The banner is trimmed with gold fringe, has gold and crimson cords and tassels, and is mounted on a pike with the usual British royal crest finial.\nPersonnel.\nStrength.\nAs of June 2022 the Army had 28,387 permanent (regular) members and 20,742 reservists (part-time); all of whom are volunteers. As of June 2022, women made up 15.11% of the Army, with a target set for 18% 2025. Gender based restrictions for frontline combat or training roles were lifted in January 2013. Also as of June 2022, Indigenous Australians made up 3.7% of the Army.\nRank and insignia.\nThe ranks of the Australian Army are based on the ranks of the British Army, and carry mostly the same actual insignia. For officers the ranks are identical except for the shoulder title \"Australia\". The Non-Commissioned Officer insignia are the same up until Warrant Officer, where they are stylised for Australia (for example, using the Australian, rather than the British coat of arms).\nThe ranks of the Australian Army are as follows:\nUniforms and Dress.\nThe Australian Army uniforms are detailed in the Australian Army Dress Manual and are grouped into nine general categories, each ranging from ceremonial dress, to general duties dress, to battle dress (in addition there are a number of special categories specific to uniforms that are only worn when posted to specific locations, like ADFA or RMC-D), these are further divided into individual 'Dress Orders' denoted by alphabetical suffixes that detail the specific items of clothing, embellishment and accoutrements, i.e. \"Dress Order No. 1A - 'Ceremonial Parade Service Dress',\" Dress Order No. 2G - 'General Duty Office Dress', Dress Order No 4C 'Combat Dress (AMCU)' . The slouch hat or beret are the regular service and general duties hat, while the field hat, or combat helmet is for use in the field while training, on exercise, or on operations. In December 2013 the Chief of Army reversed a previous ban on berets as general duties headwear for all personnel except Special Forces personnel (SASR, CDO Regiments). Australian Multi-cam Camouflage Uniform is the camouflage pattern for Australian Army camouflage uniforms, and was introduced in 2014, replacing the Disruptive Pattern Camouflage Uniform (DPCU), and Disruptive Pattern Desert Uniform (DPDU) for all Australian Army orders of dress.\nBases.\nThe Army's operational headquarters, Forces Command, is located at Victoria Barracks in Sydney. The Australian Army's three regular brigades are based at Robertson Barracks near Darwin, Lavarack Barracks in Townsville, and Gallipoli Barracks in Brisbane. The Deployable Joint Force Headquarters is also located at Gallipoli Barracks.\nOther important Army bases include the Army Aviation Centre near Oakey, Queensland, Holsworthy Barracks near Sydney, Lone Pine Barracks in Singleton, New South Wales and Woodside Barracks near Adelaide, South Australia. The SASR is based at Campbell Barracks Swanbourne, a suburb of Perth, Western Australia.\nPuckapunyal, north of Melbourne, houses the Australian Army's Combined Arms Training Centre, Land Warfare Development Centre, and three of the five principal Combat Arms schools. Further barracks include Steele Barracks in Sydney, Keswick Barracks in Adelaide, and Irwin Barracks at Karrakatta in Perth. Dozens of Australian Army Reserve depots are located across Australia.\nAustralian Army Journal.\nSince June 1948, the Australian Army has published its own journal titled the \"Australian Army Journal\". The journal's first editor was Colonel Eustace Keogh, and initially, it was intended to assume the role that the \"Army Training Memoranda\" had filled during the Second World War, although its focus, purpose, and format has shifted over time. Covering a broad range of topics including essays, book reviews and editorials, with submissions from serving members as well as professional authors, the journal's stated goal is to provide \"...the primary forum for Army's professional discourse... [and]... debate within the Australian Army... [and improve the]... intellectual rigor of that debate by adhering to a strict and demanding standard of quality\". In 1976, the journal was placed on hiatus as the \"Defence Force Journal\" began publication; however, publishing of the \"Australian Army Journal\" began again in 1999 and since then the journal has been published largely on a quarterly basis, with only minimal interruptions."}
{"id": "2799", "revid": "2902776", "url": "https://en.wikipedia.org/wiki?curid=2799", "title": "American Registry for Internet Numbers", "text": "The American Registry for Internet Numbers (ARIN) is the regional Internet registry for the United States, Canada, and many Caribbean and North Atlantic islands. ARIN manages the distribution of Internet number resources, including IPv4 and IPv6 address space and AS numbers. ARIN opened for business on December 22, 1997 after incorporating on April 18, 1997. ARIN is a nonprofit corporation with headquarters in Chantilly, Virginia, United States.\nARIN is one of five regional Internet registries in the world. Like the other regional Internet registries, ARIN:\nServices.\nARIN provides services related to the technical coordination and management of Internet number resources. The nature of these services is described in ARIN's mission statement:\nThese services are grouped in three areas: Registration, Organization, and Policy Development.\nRegistration services.\nRegistration services pertain to the technical coordination and inventory management of Internet number resources. Services include:\nFor information on requesting Internet number resources from ARIN, see https://www.arin.net/resources/index.html. This section includes the request templates, specific distribution policies, and guidelines for requesting and managing Internet number resources.\nOrganization services.\nOrganization services pertain to interaction between stakeholders, ARIN members, and ARIN. Services include:\nPolicy development services.\nPolicy development services facilitate the development of policy for the technical coordination and management of Internet number resources.\nAll ARIN policies are set by the community. Everyone is encouraged to participate in the policy development process at public policy meetings and on the Public Policy Mailing List. The ARIN Board of Trustees ratifies policies only after:\nMembership is not required to participate in ARIN's policy development process or to apply for Internet number resources.\nServices include:\nOrganizational structure.\nARIN consists of the Internet community within its region, its members, a 7-member Board of Trustees, a 15-member Advisory Council, and a professional staff of about 50. The board of trustees and Advisory Council are elected by ARIN members for three-year terms.\nBoard of trustees.\nThe ARIN membership elects the Board of Trustees (BoT), which has ultimate responsibility for the business affairs and financial health of ARIN, and manages ARIN's operations in a manner consistent with the guidance received from the Advisory Council and the goals set by the registry's members. The BoT is responsible for determining the disposition of all revenues received to ensure all services are provided in an equitable manner. The BoT ratifies proposals generated from the membership and submitted through the Advisory Council. Executive decisions are carried out following approval by the BoT. The BoT consists of 7 members consisting of a President and CEO, a chairman, a Treasurer, and others.\nAdvisory Council.\nIn addition to the BoT, ARIN has an advisory council that advises ARIN and the BoT on IP address allocation policy and related matters. Adhering to the procedures in the Internet Resource Policy Evaluation Process, the advisory council forwards consensus-based policy proposals to the BoT for ratification. The advisory council consists of 15 elected members consisting of a chair, Vice Chair, and others.\nHistory.\nThe organization was formed in December 1997 to \"provide IP registration services as an independent, nonprofit corporation.\" Until this time, IP address registration (outside of RIPE and APNIC regions) was done in accordance with policies set by the IETF by Network Solutions corporation as part of the InterNIC project. The National Science Foundation approved the plan for the creation of the not-for-profit organization to \"give the users of IP numbers (mostly Internet service providers, corporations and other large institutions) a voice in the policies by which they are managed and allocated within the North American region.\". As part of the transition, Network Solutions corporation transitioned these tasks as well as initial staff and computer infrastructure to ARIN.\nThe initial Board of Trustees consisted of Scott Bradner, John Curran, Kim Hubbard, Don Telage, Randy Bush, Raymundo Vega Aguilar, and Jon Postel (IANA) as an ex-officio member.\nThe first president of ARIN was Kim Hubbard, from 1997 until 2000. Kim was succeeded by Raymond \"Ray\" Plzak until the end of 2008. Trustee John Curran was acting president until July 1, 2009, when he assumed the CEO role permanently.\nUntil late 2002 it served Mexico, Central America, South America and all of the Caribbean. LACNIC now handles parts of the Caribbean, Mexico, Central America, and South America. Also, Sub-Saharan Africa was part of its region until April 2005, when AfriNIC was officially recognized by ICANN as the fifth regional Internet registry.\nOn 24 September 2015 ARIN has declared exhaustion of the ARIN IPv4 addresses pool.\nIn 2022, ARIN changed its membership structure to allow end user customers to be considered service members, and in 2024 ARIN updated its membership structure to that ASN-only customers are treated as service members as well, while allowing any of these customers to participate in ARIN governance by requesting General Membership. As a result, all organizations with Internet number resources under an ARIN agreement can participate in ARIN's governance.\nService region.\nThe countries in the ARIN service region are:\nFormer service regions.\nARIN formerly covered Angola, Botswana, Burundi, Republic of Congo, Democratic Republic of Congo, Eswatini, Lesotho, Malawi, Mozambique, Namibia, Rwanda, South Africa, Tanzania, Zambia, and Zimbabwe until AfriNIC was formed.\nARIN formerly covered Argentina, Aruba, Belize, Bolivia, Brazil, Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Dutch West Indies, Ecuador, El Salvador, Falkland Islands (UK), French Guiana, Guatemala, Guyana, Haiti, Honduras, Mexico, Nicaragua, Panama, Paraguay, Peru, South Georgia and the South Sandwich Islands, Suriname, Trinidad and Tobago, Uruguay, and Venezuela until LACNIC was formed."}
{"id": "2800", "revid": "26074453", "url": "https://en.wikipedia.org/wiki?curid=2800", "title": "Asimov (disambiguation)", "text": "Isaac Asimov (1920\u20131992) was a writer.\nAsimov may also refer to:"}
{"id": "2802", "revid": "782009", "url": "https://en.wikipedia.org/wiki?curid=2802", "title": "Akihabara", "text": " is a neighborhood in the Chiyoda ward of Tokyo, Japan, generally considered to be the area surrounding Akihabara Station (nicknamed \"Akihabara Electric Town\"). This area is part of the and Kanda-Sakumach\u014d districts of Chiyoda. There is an administrative district called Akihabara (part of Tait\u014d ward), located north of Akihabara Electric Town surrounding Akihabara Neribei Park.\nThe name Akihabara is a shortening of , which comes from , named after a fire-controlling deity of a firefighting shrine built after the area was destroyed by a fire in 1869. Akihabara gained the nickname shortly after World War II for being a major shopping center for household electronic goods and the post-war black market.\nAkihabara is considered by many to be the centre of Japanese \"otaku\" culture, and is a major shopping district for video games, anime, manga, electronics and computer-related goods. Icons from popular anime and manga are displayed prominently on the shops in the area, and numerous maid caf\u00e9s and some arcades are found throughout the district.\nGeography.\nThe main area of Akihabara is located on a street just west of Akihabara Station. There is an administrative district called Akihabara north of Akihabara Electric Town surrounding Akihabara Neribei Park. This district is part of Tait\u014d ward.\nHistory.\nAkihabara was once near a city gate of Edo and served as a passage between the city and northwestern Japan. This made the region a home to many craftsmen and tradesmen, as well as some low-class samurai. One of Tokyo's frequent fires destroyed the area in 1869, and the people decided to replace the buildings of the area with a shrine called Chinkasha (now known as Akiba Shrine , ), in an attempt to prevent the spread of future fires. The locals nicknamed the shrine Akiba after the deity that could control fire, and the area around it became known as Akibagahara, later Akihabara. After Akihabara Station was built in 1888, the shrine was moved to the Tait\u014d ward, where it resides today.\nSince its opening in 1890, Akihabara Station became a major freight transit point, which allowed a vegetable and fruit market to spring up. In the 1920s, the station saw a large volume of passengers after opening for public transport. After World War II, the black market thrived in the absence of a strong government. This disconnection of Akihabara from government authority allowed the district to grow as a market city. In the 1930s, this climate turned Akihabara into a market region specializing in household electronics, such as washing machines, refrigerators, televisions, and stereos, earning Akihabara the nickname \"Electric Town\".\nAs household electronics began to lose their futuristic appeal in the 1980s, the shops of Akihabara shifted their focus to home computers, at a time when they were only used by specialists and hobbyists. This brought in a new type of consumer, computer nerds or \"otaku\". The market in Akihabara latched onto their new customer base that was focused on anime, manga, and video games. The connection between Akihabara and \"otaku\" has grown to the point that the region is a center for \"otaku\" culture.\n\"Otaku\" culture.\nThe streets of Akihabara are covered with anime and manga icons, and cosplayers line the sidewalks handing out advertisements, especially for maid caf\u00e9s. Release events, special events, and conventions are common in Akihabara. Architects design the stores of Akihabara to be opaque and closed, to reflect the desire of many \"otaku\" to live in their anime worlds rather than display their interests.\nAkihabara's role as a free market has allowed a large amount of amateur work to find an audience. \"Doujinshi\" (amateur or fanmade manga) has been growing in Akihabara since the 1970s."}
{"id": "2804", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2804", "title": "Application layer firewalls", "text": ""}
{"id": "2807", "revid": "6999507", "url": "https://en.wikipedia.org/wiki?curid=2807", "title": "Active Directory", "text": "Active Directory (AD) is a directory service developed by Microsoft for Windows domain networks. Windows Server operating systems include it as a set of processes and services. Originally, only centralized domain management used Active Directory. However, it ultimately became an umbrella title for various directory-based identity-related services.\nA domain controller is a server running the Active Directory Domain Services (AD DS) role. It authenticates and authorizes all users and computers in a Windows domain-type network, assigning and enforcing security policies for all computers and installing or updating software. For example, when a user logs into a computer which is part of a Windows domain, Active Directory checks the submitted username and password and determines whether the user is a system administrator or a non-admin user. Furthermore, it allows the management and storage of information, provides authentication and authorization mechanisms, and establishes a framework to deploy other related services: Certificate Services, Active Directory Federation Services, Lightweight Directory Services, and Rights Management Services.\nActive Directory uses Lightweight Directory Access Protocol (LDAP) versions 2 and 3, Microsoft's version of Kerberos, and DNS.\nRobert R. King defined it in the following way:\nHistory.\nLike many information-technology efforts, Active Directory originated out of a democratization of design using Requests for Comments (RFCs). The Internet Engineering Task Force (IETF) oversees the RFC process and has accepted numerous RFCs initiated by widespread participants. For example, LDAP underpins Active Directory. Also, X.500 directories and the Organizational Unit preceded the Active Directory concept that uses those methods. The LDAP concept began to emerge even before the founding of Microsoft in April 1975, with RFCs as early as 1971. RFCs contributing to LDAP include RFC 1823 (on the LDAP API, August 1995), RFC 2307, RFC 3062, and RFC 4533.\nMicrosoft previewed Active Directory in 1999, released it first with Windows 2000 Server edition, and revised it to extend functionality and improve administration in Windows Server 2003. Active Directory support was also added to Windows 95, Windows 98, and Windows NT 4.0 via patch, with some unsupported features. Additional improvements came with subsequent versions of Windows Server. In Windows Server 2008, Microsoft added further services to Active Directory, such as Active Directory Federation Services. The part of the directory in charge of managing domains, which was a core part of the operating system, was renamed Active Directory Domain Services (ADDS) and became a server role like others. \"Active Directory\" became the umbrella title of a broader range of directory-based services. According to Byron Hynes, everything related to identity was brought under Active Directory's banner.\nActive Directory Services.\nActive Directory Services consist of multiple directory services. The best known is Active Directory Domain Services, commonly abbreviated as AD DS or simply AD.\nDomain Services.\nActive Directory Domain Services (AD DS) is the foundation of every Windows domain network. It stores information about domain members, including devices and users, verifies their credentials, and defines their access rights. The server running this service is called a domain controller. A domain controller is contacted when a user logs into a device, accesses another device across the network, or runs a line-of-business Metro-style app sideloaded into a machine.\nOther Active Directory services (excluding LDS, as described below) and most Microsoft server technologies rely on or use Domain Services; examples include Group Policy, Encrypting File System, BitLocker, Domain Name Services, Remote Desktop Services, Exchange Server, and SharePoint Server.\nThe self-managed Active Directory DS must be distinct from managed Azure AD DS, a cloud product.\nLightweight Directory Services.\nActive Directory Lightweight Directory Services (AD LDS), previously called \"Active Directory Application Mode\" (ADAM), implements the LDAP protocol for AD DS. It runs as a service on Windows Server and offers the same functionality as AD DS, including an equal API. However, AD LDS does not require the creation of domains or domain controllers. It provides a Data Store for storing directory data and a \"Directory Service\" with an LDAP Directory Service Interface. Unlike AD DS, multiple AD LDS instances can operate on the same server.\nCertificate Services.\nActive Directory Certificate Services (AD CS) establishes an on-premises public key infrastructure. It can create, validate, revoke and perform other similar actions, public key certificates for internal uses of an organization. These certificates can be used to encrypt files (when used with Encrypting File System), emails (per S/MIME standard), and network traffic (when used by virtual private networks, Transport Layer Security protocol or IPSec protocol).\nAD CS predates Windows Server 2008, but its name was simply Certificate Services.\nAD CS requires an AD DS infrastructure.\nFederation Services.\nActive Directory Federation Services (AD FS) is a single sign-on service. With an AD FS infrastructure in place, users may use several web-based services (e.g. internet forum, blog, online shopping, webmail) or network resources using only one set of credentials stored at a central location, as opposed to having to be granted a dedicated set of credentials for each service. AD FS uses many popular open standards to pass token credentials such as SAML, OAuth or OpenID Connect. AD FS supports encryption and signing of SAML assertions. AD FS's purpose is an extension of that of AD DS: The latter enables users to authenticate with and use the devices that are part of the same network, using one set of credentials. The former enables them to use the same set of credentials in a different network.\nAs the name suggests, AD FS works based on the concept of federated identity.\nAD FS requires an AD DS infrastructure, although its federation partner may not.\nRights Management Services.\nActive Directory Rights Management Services (AD RMS), previously known as Rights Management Services or RMS before Windows Server 2008, is server software that allows for information rights management, included with Windows Server. It uses encryption and selective denial to restrict access to various documents, such as corporate e-mails, Microsoft Word documents, and web pages. It also limits the operations authorized users can perform on them, such as viewing, editing, copying, saving, or printing. IT administrators can create pre-set templates for end users for convenience, but end users can still define who can access the content and what actions they can take.\nLogical structure.\nActive Directory is a service comprising a database and executable code. It is responsible for managing requests and maintaining the database. The Directory System Agent is the executable part, a set of Windows services and processes that run on Windows 2000 and later. Accessing the objects in Active Directory databases is possible through various interfaces such as LDAP, ADSI, messaging API, and Security Accounts Manager services.\nObjects used.\nActive Directory structures consist of information about objects classified into two categories: resources (such as printers) and security principals (which include user or computer accounts and groups). Each security principal is assigned a unique security identifier (SID). An object represents a single entity, such as a user, computer, printer, or group, along with its attributes. Some objects may even contain other objects within them. Each object has a unique name, and its definition is a set of characteristics and information by a schema, which determines the storage in the Active Directory.\nAdministrators can extend or modify the schema using the schema object when needed. However, because each schema object is integral to the definition of Active Directory objects, deactivating or changing them can fundamentally alter or disrupt a deployment. Modifying the schema affects the entire system automatically, and new objects cannot be deleted, only deactivated. Changing the schema usually requires planning.\nForests, trees, and domains.\nIn an Active Directory network, the framework that holds objects has different levels: the forest, tree, and domain. Domains within a deployment contain objects stored in a single replicable database, and the DNS name structure identifies their domains, the namespace. A domain is a logical group of network objects such as computers, users, and devices that share the same Active Directory database.\nOn the other hand, a tree is a collection of domains and domain trees in a contiguous namespace linked in a transitive trust hierarchy. The forest is at the top of the structure, a collection of trees with a standard global catalog, directory schema, logical structure, and directory configuration. The forest is a secure boundary that limits access to users, computers, groups, and other objects.\nOrganizational units.\nThe objects held within a domain can be grouped into organizational units (OUs). OUs can provide hierarchy to a domain, ease its administration, and can resemble the organization's structure in managerial or geographical terms. OUs can contain other OUs\u2014domains are containers in this sense. Microsoft recommends using OUs rather than domains for structure and simplifying the implementation of policies and administration. The OU is the recommended level at which to apply group policies, which are Active Directory objects formally named group policy objects (GPOs), although policies can also be applied to domains or sites (see below). The OU is the level at which administrative powers are commonly delegated, but delegation can be performed on individual objects or attributes as well.\nOrganizational units do not each have a separate namespace. As a consequence, for compatibility with Legacy NetBios implementations, user accounts with an identical SamAccountName are not allowed within the same domain even if the accounts objects are in separate OUs. This is because SamAccountName, a user object attribute, must be unique within the domain. However, two users in different OUs can have the same common name (CN), the name under which they are stored in the directory itself such as \"fred.staff-ou.domain\" and \"fred.student-ou.domain\", where \"staff-ou\" and \"student-ou\" are the OUs.\nIn general, the reason for this lack of allowance for duplicate names through hierarchical directory placement is that Microsoft primarily relies on the principles of NetBIOS, which is a flat-namespace method of network object management that, for Microsoft software, goes all the way back to Windows NT 3.1 and MS-DOS LAN Manager. Allowing for duplication of object names in the directory, or completely removing the use of NetBIOS names, would prevent backward compatibility with legacy software and equipment. However, disallowing duplicate object names in this way is a violation of the LDAP RFCs on which Active Directory is supposedly based.\nAs the number of users in a domain increases, conventions such as \"first initial, middle initial, last name\" (Western order) or the reverse (Eastern order) fail for common family names like \"Li\" (\u674e), \"Smith\" or \"Garcia\". Workarounds include adding a digit to the end of the username. Alternatives include creating a separate ID system of unique employee/student ID numbers to use as account names in place of actual users' names and allowing users to nominate their preferred word sequence within an acceptable use policy.\nBecause duplicate usernames cannot exist within a domain, account name generation poses a significant challenge for large organizations that cannot be easily subdivided into separate domains, such as students in a public school system or university who must be able to use any computer across the network.\nShadow groups.\nIn Microsoft's Active Directory, OUs do not confer access permissions, and objects placed within OUs are not automatically assigned access privileges based on their containing OU. It represents a design limitation specific to Active Directory, and other competing directories, such as Novell NDS, can set access privileges through object placement within an OU.\nActive Directory requires a separate step for an administrator to assign an object in an OU as a group member also within that OU. Using only the OU location to determine access permissions is unreliable since the entity might not have been assigned to the group object for that OU yet.\nA common workaround for an Active Directory administrator is to write a custom PowerShell or Visual Basic script to automatically create and maintain a \"user group\" for each OU in their Directory. The scripts run periodically to update the group to match the OU's account membership. However, they cannot instantly update the security groups anytime the directory changes, as occurs in competing directories, as security is directly implemented into the Directory. Such groups are known as \"shadow groups\". Once created, these shadow groups are selectable in place of the OU in the administrative tools. Microsoft's Server 2008 reference documentation mentions shadow groups but does not provide instructions on creating them. Additionally, there are no available server methods or console snap-ins for managing these groups.\nAn organization must determine the structure of its information infrastructure by dividing it into one or more domains and top-level OUs. This decision is critical and can base on various models such as business units, geographical locations, IT service, object type, or a combination of these models. The immediate purpose of organizing OUs is to simplify administrative delegation and, secondarily, to apply group policies. While OUs serve as an administrative boundary, the forest itself is the only security boundary. All other domains must trust any administrator in the forest to maintain security.\nPartitions.\nThe Active Directory database is organized in \"partitions\", each holding specific object types and following a particular replication pattern. Microsoft often refers to these partitions as 'naming contexts. The 'Schema' partition defines object classes and attributes within the forest. The 'Configuration' partition contains information on the physical structure and configuration of the forest (such as the site topology). Both replicate all domains in the forest. The 'Domain' partition holds all objects created in that domain and replicates only within it.\nPhysical structure.\n\"Sites\" are physical (rather than logical) groupings defined by one or more IP subnets. AD also defines connections, distinguishing low-speed (e.g., WAN, VPN) from high-speed (e.g., LAN) links. Site definitions are independent of the domain and OU structure and are shared across the forest. Sites play a crucial role in managing network traffic created by replication and directing clients to their nearest domain controllers (DCs). Microsoft Exchange Server 2007 uses the site topology for mail routing. Administrators can also define policies at the site level.\nThe Active Directory information is physically held on one or more peer domain controllers, replacing the NT PDC/BDC model. Each DC has a copy of the Active Directory. Member servers joined to Active Directory that are not domain controllers are called Member Servers. In the domain partition, a group of objects acts as copies of domain controllers set up as global catalogs. These global catalog servers offer a comprehensive list of all objects in the forest.\nGlobal Catalog servers replicate all objects from all domains to themselves, providing an international listing of entities in the forest. However, to minimize replication traffic and keep the GC's database small, only selected attributes of each object are replicated, called the \"partial attribute set\" (PAS). The PAS can be modified by modifying the schema and marking features for replication to the GC. Earlier versions of Windows used NetBIOS to communicate. Active Directory is fully integrated with DNS and requires TCP/IP\u2014DNS. To fully operate, the DNS server must support SRV resource records, also known as service records.\nReplication.\nActive Directory uses multi-master replication to synchronize changes, meaning replicas pull changes from the server where the change occurred rather than being pushed to them. The Knowledge Consistency Checker (KCC) uses defined sites to manage traffic and create a replication topology of site links. Intra-site replication occurs frequently and automatically due to change notifications, which prompt peers to begin a pull replication cycle. Replication intervals between different sites are usually less consistent and don't usually use change notifications. However, it's possible to set it up to be the same as replication between locations on the same network if needed.\nEach DS3, T1, and ISDN link can have a cost, and the KCC alters the site link topology accordingly. Replication may occur transitively through several site links on same-protocol \"site link bridges\" if the price is low. However, KCC automatically costs a direct site-to-site link lower than transitive connections. A bridgehead server in each zone can send updates to other DCs in the exact location to replicate changes between sites. To configure replication for Active Directory zones, activate DNS in the domain based on the site.\nTo replicate Active Directory, Remote Procedure Calls (RPC) over IP (RPC/IP) are used. SMTP is used to replicate between sites but only for modifications in the Schema, Configuration, or Partial Attribute Set (Global Catalog) GCs. It's not suitable for reproducing the default Domain partition.\nImplementation.\nGenerally, a network utilizing Active Directory has more than one licensed Windows server computer. Backup and restore of Active Directory are possible for a network with a single domain controller. However, Microsoft recommends more than one domain controller to provide automatic failover protection of the directory. Domain controllers are ideally single-purpose for directory operations only and should not run any other software or role.\nSince certain Microsoft products, like SQL Server and Exchange, can interfere with the operation of a domain controller, isolation of these products on additional Windows servers is advised. Combining them can complicate the configuration and troubleshooting of the domain controller or the other installed software more complex. If planning to implement Active Directory, a business should purchase multiple Windows server licenses to have at least two separate domain controllers. Administrators should consider additional domain controllers for performance or redundancy and individual servers for tasks like file storage, Exchange, and SQL Server since this will guarantee that all server roles are adequately supported.\nOne way to lower the physical hardware costs is by using virtualization. However, for proper failover protection, Microsoft recommends not running multiple virtualized domain controllers on the same physical hardware.\nDatabase.\nThe Active-Directory database, the \"directory store\", in Windows 2000 Server uses the JET Blue-based Extensible Storage Engine (ESE98). Each domain controller's database is limited to 16 terabytes and 2 billion objects (but only 1 billion security principals). Microsoft has created NTDS databases with more than 2 billion objects. NT4's Security Account Manager could support up to 40,000 objects. It has two main tables: the \"data table\" and the \"link table\". Windows Server 2003 added a third main table for security descriptor single instancing.\nPrograms may access the features of Active Directory via the COM interfaces provided by \"Active Directory Service Interfaces\".\nTrusting.\nTo allow users in one domain to access resources in another, Active Directory uses trusts.\nTrusts inside a forest are automatically created when domains are created. The forest sets the default boundaries of trust, and implicit, transitive trust is automatic for all domains within a forest.\nManagement tools.\nMicrosoft Active Directory management tools include:\nThese management tools may not provide enough functionality for efficient workflow in large environments. Some third-party tools extend the administration and management capabilities. They provide essential features for a more convenient administration process, such as automation, reports, integration with other services, etc.\nUnix integration.\nVarying levels of interoperability with Active Directory can be achieved on most Unix-like operating systems (including Unix, Linux, Mac OS X or Java and Unix-based programs) through standards-compliant LDAP clients, but these systems usually do not interpret many attributes associated with Windows components, such as Group Policy and support for one-way trusts.\nThird parties offer Active Directory integration for Unix-like platforms, including:\nThe schema additions shipped with Windows Server 2003 R2 include attributes that map closely enough to RFC 2307 to be generally usable. The reference implementation of RFC 2307, nss_ldap and pam_ldap provided by PADL.com, support these attributes directly. The default schema for group membership complies with RFC 2307bis (proposed). Windows Server 2003 R2 includes a Microsoft Management Console snap-in that creates and edits the attributes.\nAn alternative option is to use another directory service as non-Windows clients authenticate to this while Windows Clients authenticate to Active Directory. Non-Windows clients include 389 Directory Server (formerly Fedora Directory Server, FDS), ViewDS v7.2 XML Enabled Directory, and Sun Microsystems Sun Java System Directory Server. The latter two are both able to perform two-way synchronization with Active Directory and thus provide a \"deflected\" integration.\nAnother option is to use OpenLDAP with its \"translucent\" overlay, which can extend entries in any remote LDAP server with additional attributes stored in a local database. Clients pointed at the local database see entries containing both the remote and local attributes, while the remote database remains completely untouched.\nAdministration (querying, modifying, and monitoring) of Active Directory can be achieved via many scripting languages, including PowerShell, VBScript, JScript/JavaScript, Perl, Python, and Ruby. Free and non-free Active Directory administration tools can help to simplify and possibly automate Active Directory management tasks.\nSince October 2017 Amazon AWS offers integration with Microsoft Active Directory."}
{"id": "2808", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2808", "title": "Atom Bomb", "text": ""}
{"id": "2809", "revid": "45546346", "url": "https://en.wikipedia.org/wiki?curid=2809", "title": "Arian (disambiguation)", "text": "Arianism is a nontrinitarian Christological doctrine.\nArian may also refer to:\nPeople.\nSurname.\nArian is a surname that originated in Ancient Persia"}
{"id": "2810", "revid": "12824384", "url": "https://en.wikipedia.org/wiki?curid=2810", "title": "Aldona of Lithuania", "text": "Aldona (baptized \"Ona\" or \"Anna\"; her pagan name, Aldona, is known only from the writings of Maciej Stryjkowski; \u2013 26 May 1339) was Queen consort of Poland (1333\u20131339), and a princess of the Grand Duchy of Lithuania. She was the daughter of Gediminas, Grand Duke of Lithuania.\nBiography.\nAldona married Casimir III of Poland, when he was 15 or 16 years old. The bride was probably of about the same age. The marriage took place on 30 April or 16 October 1325 and was a purely political maneuver to strengthen the first Polish\u2013Lithuanian coalition against the Teutonic Knights. Casimir was seeking allies in the dispute over Pomerania with the Order. Gediminas had just undertaken an unsuccessful attempt to Christianize Lithuania. This coalition was a prelude to the Union of Krewo in 1385, and the Union of Lublin in 1569, which resulted in the creation of a new state, the Polish\u2013Lithuanian Commonwealth. The details of the agreement are not known; however, it is known that Gediminas released all Polish captives, some 25,000 people, who returned to Poland. The importance of the marriage was attested by the fact that W\u0142adys\u0142aw abandoned his earlier plans to marry his son to Jutta of Bohemia. The alliance was put into effect when joint Polish\u2013Lithuanian forces organized an attack against the Margraviate of Brandenburg in 1326. However, the coalition was not strong and collapsed c. 1330. Yet, there is no evidence of fighting between Poland and Lithuania while Aldona was alive. Aldona died suddenly at the end of May 1339, and was buried in Krak\u00f3w.\nAldona was remembered for her piety and devotion to music. She was accompanied by court musicians wherever she went. It was even suggested by Jan D\u0142ugosz that the cymbals which were played in procession before her represented a pagan Lithuanian tradition. Her husband Casimir is known for his romantic affairs: after Aldona's death he married three more times.\nIssue.\nAldona had two daughters:\nIn popular culture.\nFilm.\nQueen Aldona Anna is one of the main characters in the first season of Polish historical TV drama series \"Korona Kr\u00f3l\u00f3w\" (\"The Crown of the Kings\"). She is played by Marta Bry\u0142a."}
{"id": "2812", "revid": "31254146", "url": "https://en.wikipedia.org/wiki?curid=2812", "title": "Aron Nimzowitsch", "text": "Aron Nimzowitsch (; , \"Aron Isayevich Nimtsovich\"; 7 November 1886\u00a0\u2013 16 March 1935) was a Latvian-born Danish chess player and writer. In the late 1920s, Nimzowitsch was one of the best chess players in the world. He was the foremost figure amongst the hypermoderns and wrote a very influential book on chess theory: \"My System\" (1925\u20131927). Nimzowitsch's seminal work \"Chess Praxis\", originally published in German in 1929, was purchased by a pre-teen and future World Champion Tigran Petrosian and was to have a great influence on his development as a chess player.\nLife.\nBorn in Riga, then part of the Russian Empire, the Jewish Yiddish-speaking Nimzowitsch came from a wealthy family, where he learned chess from his father Shaya Abramovich Nimzowitsch (1860, Pinsk \u2013 1918), who was a timber merchant. By 1897, the family lived in Dvinsk. Mother's name: Esphir Nohumovna Nimzowitsch (born Rabinovich, 1865, Polotsk \u2013 1937), sister \u2013 Tsilya-Kreyna Pevzner, brothers Yakov, Osey and Benno. In 1904, he travelled to Berlin to study philosophy, but set aside his studies soon and began a career as a professional chess player that same year. He won his first international tournament at Munich 1906. Then, he tied for first with Alexander Alekhine at Saint Petersburg 1913/14 (the eighth All-Russian Masters' Tournament).\nDuring the 1917 Russian Revolution, Nimzowitsch was in the Baltic war zone. He escaped being drafted into one of the armies by feigning madness, insisting that a fly was on his head. He then escaped to Berlin, and gave his first name as Arnold, possibly to avoid anti-Semitic persecution.\nNimzowitsch eventually moved to Copenhagen in 1922, where he lived for the rest of his life in one small rented room. In Copenhagen, he won the Nordic Championship twice, in 1924 and in 1934. He obtained Danish citizenship and lived in Denmark until his death in 1935.\nChess career.\nThe height of Nimzowitsch's career was the late 1920s and early 1930s. Chessmetrics places him as the third best player in the world from 1927 to 1931, behind Alexander Alekhine and Jos\u00e9 Capablanca. His most notable successes were first-place finishes at Copenhagen 1923, Marienbad 1925, Dresden 1926, Hanover 1926, the Carlsbad 1929 chess tournament, and second place behind Alekhine at the San Remo 1930 chess tournament. Nimzowitsch never developed a knack for match play, though; his best match success was a draw with Alekhine, but the match consisted of only two games and took place in 1914, thirteen years before Alekhine became world champion.\nNimzowitsch never beat Capablanca (+0\u22125=6), but fared better against Alekhine (+3\u22129=9). He even beat Alekhine with the black pieces, in their short 1914 match at St. Petersburg. One of Nimzowitsch's most famous games is his celebrated immortal zugzwang game against S\u00e4misch at Copenhagen 1923. Another game on this theme is his win over Paul Johner at Dresden 1926. When in form, Nimzowitsch was very dangerous with the black pieces, scoring many fine wins over top players.\nLegacy.\nNimzowitsch is considered one of the most important players and writers in chess history. His works influenced numerous other players, including Savielly Tartakower, Milan Vidmar, Richard R\u00e9ti, Akiba Rubinstein, Mikhail Botvinnik, Bent Larsen, Viktor Korchnoi and Tigran Petrosian, and his influence is still felt today.\nHe wrote three books on chess strategy: \"Mein System (My System)\", 1925; \"Die Praxis meines Systems (The Practice of My System)\", 1929, commonly known as \"Chess Praxis\"; and \"Die Blockade\" (\"The Blockade\"), 1925, although much in this book is generally held to be a rehash of material already presented in \"Mein System\". \"Mein System\" is considered to be one of the most influential chess books of all time. It sets out Nimzowitsch's most important ideas, while his second most influential work, \"Chess Praxis\", elaborates upon these ideas, adds a few new ones, and has immense value as a stimulating collection of Nimzowitsch's own games accompanied by his idiosyncratic, hyperbolic commentary which is often as entertaining as instructive.\nNimzowitsch's chess theories, when first propounded, flew in the face of widely held orthodoxies enunciated by the dominant theorist of the era, Siegbert Tarrasch, and his disciples. Tarrasch's rigid generalizations drew on the earlier work of Wilhelm Steinitz, and were upheld by Tarrasch's sharp tongue when dismissing the opinions of doubters. While the greatest players of the time, among them Alekhine, Emanuel Lasker and Capablanca, clearly did not allow their play to be hobbled by blind adherence to general concepts that the center had to be controlled by pawns, that development had to happen in support of this control, that rooks always belong on open files, that wing openings were unsound\u2014core ideas of Tarrasch's chess philosophy as popularly understood\u2014beginners were taught to think of these generalizations as unalterable principles.\nNimzowitsch supplemented many of the earlier simplistic assumptions about chess strategy by enunciating in his turn a further number of general concepts of defensive play aimed at achieving one's own goals by preventing realization of the opponent's plans. Notable in his \"system\" were concepts such as overprotection of pieces and pawns under attack, control of the center by pieces instead of pawns, blockading of opposing pieces (notably the passed pawns) and prophylaxis. His aforementioned game versus Paul Johner in 1926 (listed in the notable games below) is a great example of Nimzowitsch's concept of 'first restrain, then blockade and finally destroy'. He manoeuvres the black queen from its starting point to h7 to form a part of king-side blockade along with the knight on f6 and h-pawn to stop any attacking threats from White. He was also a leading exponent of the fianchetto development of bishops. Perhaps most importantly, he formulated the terminology still in use for various complex chess strategies. Others had used these ideas in practice, but he was the first to present them systematically as a lexicon of themes accompanied by extensive taxonomical observations.\nRaymond Keene writes that Nimzowitsch \"was one of the world's leading grandmasters for a period extending over a quarter of a century, and for some of that time he was the obvious challenger for the world championship. ... [He was also] a great and profound chess thinker second only to Steinitz, and his works \u2013 \"Die Blockade\", \"My System\" and \"Chess Praxis\" \u2013 established his reputation as one of the father figures of modern chess.\" GM Robert Byrne called him \"perhaps the most brilliant theoretician and teacher in the history of the game.\" GM Jan Hein Donner called Nimzowitsch \"a man who was too much of an artist to be able to prove he was right and who was regarded as something of a madman in his time. He would be understood only long after his death.\"\nMany chess openings and variations are named after Nimzowitsch, the most famous being the Nimzo-Indian Defence (1.d4 Nf6 2.c4 e6 3.Nc3 Bb4) and the less often played Nimzowitsch Defence (1.e4 Nc6). Nimzowitsch biographer GM Raymond Keene and others have referred to 1.Nf3 followed by 2.b3 as the Nimzowitsch\u2013Larsen Attack. Keene wrote a book about the opening with that title. These openings all exemplify Nimzowitsch's ideas about controlling the center with pieces instead of pawns. He was also vital in the development of two important systems in the French Defence, the (in some places called the Nimzowitsch Variation; its moves are 1.e4 e6 2.d4 d5 3.Nc3 Bb4) and the (1.e4 e6 2.d4 d5 3.e5). He also pioneered two provocative variations of the Sicilian Defence: the , 1.e4 c5 2.Nf3 Nf6, which invites 3.e5 Nd5 (similar to Alekhine's Defence) and 1.e4 c5 2.Nf3 Nc6 3.d4 cxd4 4.Nxd4 d5?! (the latter regarded as dubious today). International Master John L. Watson has dubbed the line 1.c4 Nf6 2.Nc3 e6 3.Nf3 Bb4 the \"Nimzo-English\", employing this designation in Chapter 11 of his book \"Mastering the Chess Openings, Volume 3\".\nPersonality.\nThere are many entertaining anecdotes regarding Nimzowitsch\u2014some less savory than others. An article by Hans Kmoch and Fred Reinfeld entitled \"Unconventional Surrender\" on page 55 of the February 1950 \"Chess Review\" tells of the \"... example of Nimzowitsch, who ... once missed first prize in a tournament in Berlin by losing to S\u00e4misch, and when it became clear he was going to lose the game, Nimzowitsch stood up on the table and shouted, 'Gegen diesen Idioten muss ich verlieren!' ('I must lose to this idiot!')\".\nNimzowitsch was annoyed by his opponents' smoking. A popular, but probably apocryphal, story is that once when an opponent laid an unlit cigar on the table, he complained to the tournament arbiters, \"He is threatening to smoke, and as an old player you must know that the threat is stronger than the execution.\"\nNimzowitsch had lengthy and somewhat bitter dogmatic conflicts with Tarrasch over whose ideas constituted 'proper' chess.\nNimzowitsch's vanity and faith in his ideas of overprotection provoked Hans Kmoch to write a parody about him in February 1928 in the \"Wiener Schachzeitung\". This consisted of a mock game against the fictional player \"Systemsson\", supposedly played and annotated by Nimzowitsch himself. The annotations gleefully exaggerate the idea of overprotection, as well as asserting the true genius of the wondrous idea. Kmoch was in fact a great admirer of Nimzowitsch, and Nimzowitsch was amused at the effort.\nKmoch also wrote an article about his nine years with Nimzowitsch: Nimzovich suffered from the delusion that he was unappreciated and that the reason was malice. All it took to make him blossom, as I later learned, was a little praise. His paranoia was most evident when he dined in company. He always thought he was served much smaller portions than everyone else. He didn't care about the actual amount but only about the imagined affront. I once suggested that he and I order what the other actually wanted and, when the food was served, exchange plates. After we had done so, he shook his head in disbelief, still thinking that he had received the smaller portion.\nNimzowitsch's colleague Tartakower observed of him, \"He pretends to be crazy in order to drive us all crazy.\"\nDeath.\nAlthough he had long suffered from heart trouble, his early death was unexpected; taken ill suddenly at the end of 1934, he lay bedridden for three months before dying of pneumonia. He is buried in Bispebjerg Cemetery in Copenhagen."}
{"id": "2813", "revid": "6326132", "url": "https://en.wikipedia.org/wiki?curid=2813", "title": "Aragonese language", "text": "Aragonese ( ; in Aragonese) is a Romance language spoken in several dialects by about 12,000 people as of 2011, in the Pyrenees valleys of Aragon, Spain, primarily in the comarcas of Somontano de Barbastro, Jacetania, Alto G\u00e1llego, Sobrarbe, and Ribagorza/Ribagor\u00e7a. It is the only modern language which survived from medieval Navarro-Aragonese in a form distinct from Spanish.\nHistorically, people referred to the language as ('talk' or 'speech'). Native Aragonese people usually refer to it by the names of its local dialects such as (from Valle de Hecho) or (from the Benasque Valley).\nHistory.\nAragonese, which developed in portions of the Ebro basin, can be traced back to the High Middle Ages. It spread throughout the Pyrenees to areas where languages similar to modern Basque might have been previously spoken. The Kingdom of Aragon (formed by the counties of Aragon, Sobrarbe and Ribagorza) expanded southward from the mountains, pushing the Moors farther south in the \"Reconquista\" and spreading the Aragonese language.\nThe union of the Catalan counties and the Kingdom of Aragon which formed the 12th-century Crown of Aragon did not merge the languages of the two territories; Catalan continued to be spoken in the east and Navarro-Aragonese in the west, with the boundaries blurred by dialectal continuity. The Aragonese \"Reconquista\" in the south ended with the cession of Murcia by James I of Aragon to the Kingdom of Castile as dowry for an Aragonese princess.\nThe best-known proponent of the Aragonese language was Johan Ferrandez d'Heredia, the Grand Master of the Knights Hospitaller in Rhodes at the end of the 14th century. He wrote an extensive catalog of works in Aragonese and translated several works from Greek into Aragonese (the first in medieval Europe).\nThe spread of Castilian (Spanish), the Castilian origin of the Trast\u00e1mara dynasty, and the similarity between Castilian (Spanish) and Aragonese facilitated the recession of the latter. A turning point was the 15th-century coronation of the Castilian Ferdinand I of Aragon, also known as Ferdinand of Antequera.\nIn the early 18th century, after the defeat of the allies of Aragon in the War of the Spanish Succession, Philip V ordered the prohibition of the Aragonese language in schools and the establishment of Castilian (Spanish) as the only official language in Aragon. This was ordered in the Aragonese Nueva Planta decrees of 1707.\nIn recent times, Aragonese was mostly regarded as a group of rural dialects of Spanish. Compulsory education undermined its already weak position; for example, pupils were punished for using it. However, the 1978 Spanish transition to democracy heralded literary works and studies of the language.\nModern Aragonese.\nAragonese is the native language of the Aragonese mountain ranges of the Pyrenees, in the \"comarcas\" of Somontano, Jacetania, Sobrarbe, and Ribagorza. Cities and towns in which Aragonese is spoken are Huesca, Graus, Monz\u00f3n, Barbastro, Bielsa, Chist\u00e9n, Fonz, Echo, Estadilla, Benasque, Campo, Sabi\u00f1\u00e1nigo, Jaca, Plan, Ans\u00f3, Ayerbe, Broto, and El Grado.\nIt is spoken as a second language by inhabitants of Zaragoza, Huesca, Ejea de los Caballeros, or Teruel. According to recent polls, there are about 25,500 speakers (2011) including speakers living outside the native area. In 2017, the Direcci\u00f3n General de Pol\u00edtica Ling\u00fc\u00edstica de Arag\u00f3n estimated there were 10,000 to 12,000 active speakers of Aragonese.\nIn 2009, the Languages Act of Aragon (Law 10/2009) recognized the \"native language, original and historic\" of Aragon. The language received several linguistic rights, including its use in public administration. Some of the legislation was repealed by a new law in 2013 (Law 3/2013). [See Languages Acts of Aragon for more information on the subject]\nPhonology.\nTraits.\nAragonese has many historical traits in common with Catalan. Some are conservative features that are also shared with the Asturleonese languages and Galician\u2013Portuguese, where Spanish innovated in ways that did not spread to nearby languages.\nOrthography.\nBefore 2023, Aragonese had three orthographic standards:\nDuring the 16th century, Aragonese Moriscos wrote \"aljamiado\" texts (Romance texts in Arabic script), possibly because of their inability to write in Arabic. The language in these texts has a mixture of Aragonese and Castilian traits, and they are among the last known written examples of the Aragonese formerly spoken in central and southern Aragon.\nIn 2023, a new orthographic standard has been published by the \"Academia Aragonesa de la Lengua\". This version is close to the Academia de l'Aragon\u00e9s orthography, but with the following differences: is always spelled \u27e8cu\u27e9, e. g. \"cuan, cuesti\u00f3n\" (exception is made for some loanwords: \"quad, quadr\u00edvium, quark, qu\u00e1sar, qu\u00e1ter, qu\u00f3rum\"); is spelled \u27e8ny\u27e9 or \u27e8\u00f1\u27e9 by personal preference; final \u27e8z\u27e9 is not written as \u27e8tz\u27e9.\nThe marginal phoneme (only in loanwords, e. g. \"jabugo\") is spelled j in the Uesca, Academia de l'Aragon\u00e9s and Academia Aragonesa de la Lengua standards (not mentioned in the SLA standard). Additionally, the Academia de l'Aragon\u00e9s and Academia Aragonesa de la Lengua orthographies allow the letter j in some loanwords internationally known with it (e. g. \"jazz, jacuzzi\", which normally have in the Aragonese pronunciation) and also mention the letters k and w, also used only in loanwords (w may represent or ).\nGrammar.\nAragonese grammar has a lot in common with Occitan and Catalan, but also Spanish.\nArticles.\nThe definite article in Aragonese has undergone dialect-related changes, with definite articles in Old Aragonese similar to their present Spanish equivalents. There are two main forms:\nThese forms are used in the eastern and some central dialects.\nThese forms are used in the western and some central dialects.\nLexicology.\nNeighboring Romance languages have influenced Aragonese. Catalan and Occitan influenced Aragonese for many years. Since the 15th century, Spanish has most influenced Aragonese; it was adopted throughout Aragon as the first language, limiting Aragonese to the northern region surrounding the Pyrenees. French has also influenced Aragonese; Italian loanwords have entered through other languages (such as Catalan), and Portuguese words have entered through Spanish. Germanic words came with the conquest of the region by Germanic peoples during the fifth century, and English has introduced a number of new words into the language.\nGender.\nWords that were part of the Latin second declension\u2014as well as words that joined it later on\u2014are usually masculine:\nWords that were part of the Latin first declension are usually feminine:\nSome Latin neuter plural nouns joined the first declension as singular feminine nouns: \nWords ending in \"-or\" are feminine:\nThe names of fruit trees usually end in \"-era\" (a suffix derived from Latin \"-aria\") and are usually feminine:\nThe genders of river names vary:\nPronouns.\nJust like most other Occitano-Romance languages, Aragonese has partitive and locative clitic pronouns derived from the Latin and : \"/\" and \"/\"\"/\"; unlike Ibero-Romance.\nSuch pronouns are present in most major Romance languages (Catalan and , Occitan and , French and , and Italian and \"/\").\n\"/\" is used for:\n\"/\"\"/\" is used for:\nLiterature.\nAragonese was not written until the 12th and 13th centuries; the history \"\", , , and date from this period; an Aragonese version of the \"Chronicle of the Morea\" also exists, differing also in its content and written in the late 14th century called .\nEarly modern period.\nSince 1500, Spanish has been the cultural language of Aragon; many Aragonese wrote in Spanish, and during the 17th century the Argensola brothers went to Castile to teach Spanish.\nAragonese became a popular village language. During the 17th century, popular literature in the language began to appear. In a 1650 Huesca literary contest, Aragonese poems were submitted by Mat\u00edas Pradas, Isabel de Rodas and \"Fileno, monta\u00f1\u00e9s\".\nContemporary literature.\nThe 19th and 20th centuries have seen a renaissance of Aragonese literature in several dialects. In 1844, Braulio Foz's novel was published in the Almud\u00e9var (southern) dialect. The 20th century featured Domingo Miral's costumbrist comedies and Veremundo M\u00e9ndez Coarasa's poetry, both in Hecho (western) Aragonese; Cleto Torrodellas' poetry and Ton\u00f3n de Baldomera's popular writings in the Graus (eastern) dialect and Arnal Cavero's costumbrist stories and Juana Coscujuela's novel , also in the southern dialect.\nAragonese in modern education.\nThe 1997 Aragonese law of languages stipulated that Aragonese (and Catalan) speakers had a right to the teaching of and in their own language. Following this, Aragonese lessons started in schools in the 1997\u20131998 academic year. It was originally taught as an extra-curricular, non-evaluable voluntary subject in four schools. However, whilst legally schools can choose to use Aragonese as the language of instruction, as of the 2013\u20132014 academic year, there are no recorded instances of this option being taken in primary or secondary education. In fact, the only current scenario in which Aragonese is used as the language of instruction is in the Aragonese philology university course, which is optional, taught over the summer and in which only some of the lectures are in Aragonese.\nPre-school education.\nIn pre-school education, students whose parents wish them to be taught Aragonese receive between thirty minutes to one hour of Aragonese lessons a week. In the 2014\u20132015 academic year there were 262 students recorded in pre-school Aragonese lessons.\nPrimary school education.\nThe subject of Aragonese now has a fully developed curriculum in primary education in Aragon. Despite this, in the 2014\u20132015 academic year there were only seven Aragonese teachers in the region across both pre-primary and primary education and none hold permanent positions, whilst the number of primary education students receiving Aragonese lessons was 320.\nAs of 2017 there were 1068 reported Aragonese language students and 12 Aragonese language instructors in Aragon.\nSecondary school education.\nThere is no officially approved program or teaching materials for the Aragonese language at the secondary level, and though two non-official textbooks are available ( (Ben\u00edtez, 2007) and (Campos, 2014)) many instructors create their own learning materials. Further, most schools with Aragonese programs that have the possibility of being offered as an examinative subject have elected not to do so.\nAs of 2007 it is possible to use Aragonese as a language of instruction for multiple courses; however, no program is yet to instruct any curricular or examinative courses in Aragonese.\nAs of the 2014\u20132015 academic year there were 14 Aragonese language students at the secondary level.\nHigher education.\nAragonese is not currently a possible field of study for a bachelor's or postgraduate degree in any official capacity, nor is Aragonese used as a medium of instruction. A bachelor's or master's degree may be obtained in Magisterio (teaching) at the University of Zaragoza; however, no specialization in Aragonese language is currently available. As such those who wish to teach Aragonese at the pre-school, primary, or secondary level must already be competent in the language by being a native speaker or by other means. Further, prospective instructors must pass an ad hoc exam curated by the individual schools at which they wish to teach in order to prove their competence, as there are no recognized standard competency exams for the Aragonese language.\nSince the 1994\u20131995 academic year, Aragonese has been an elective subject within the bachelor's degree for primary school education at the University of Zaragoza's Huesca campus.\nThe University of Zaragoza's Huesca campus also offers a \"Diploma de Especializaci\u00f3n\" (These are studies that require a previous university degree and have a duration of between 30 and 59 ECTS credits.) in Aragonese Philology with 37 ECTS credits."}
{"id": "2815", "revid": "1348858", "url": "https://en.wikipedia.org/wiki?curid=2815", "title": "Advanced Mobile Phone System", "text": "Advanced Mobile Phone System (AMPS) was an analog mobile phone system standard originally developed by Bell Labs and later modified in a cooperative effort between Bell Labs and Motorola. It was officially introduced in the Americas on October 13, 1983, and was deployed in many other countries too, including Israel in 1986, Australia in 1987, Singapore in 1988, and Pakistan in 1990. It was the primary analog mobile phone system in North America (and other locales) through the 1980s and into the 2000s. As of February 18, 2008, carriers in the United States were no longer required to support AMPS and companies such as AT&amp;T and Verizon Communications have discontinued this service permanently. AMPS was discontinued in Australia in September 2000, in India by October 2004, in Israel by January 2010, and Brazil by 2010.\nHistory.\nThe first cellular network efforts began at Bell Labs and with research conducted at Motorola. \nIn 1960, John F. Mitchell\nbecame Motorola's chief engineer for its mobile-communication products, and oversaw the development and marketing of the first pager to use transistors.\nMotorola had long produced mobile telephones for automobiles, but these large and heavy models consumed too much power to allow their use without the automobile's engine running. Mitchell's team, which included Dr. Martin Cooper, developed portable cellular telephony. Cooper and Mitchell were among the Motorola employees granted a patent for this work in 1973. The first call on the prototype connected, reportedly, to a wrong number.\nWhile Motorola was developing a cellular phone, from 1968 to 1983 Bell Labs worked out a system called Advanced Mobile Phone System (AMPS), which became the first cellular network standard in the United States. The Bell system deployed ASTM in Chicago, Illinois, first as an equipment test serving approximately 100 units in 1978, and subsequently as a service test planned for 2,000 billed units. Motorola and others designed and built the cellular phones for this and other cellular systems. Louis M. Weinberg, a marketing director at AT&amp;T, was named the first president of the AMPS corporation. He served in this position during the startup of the AMPS subsidiary of AT&amp;T.\nMartin Cooper, a former general manager for the systems division at Motorola, led a team that produced the first cellular handset in 1973 and made the first phone call from it. In 1983 Motorola introduced the DynaTAC 8000x, the first commercially available cellular phone small enough to be easily carried. He later introduced the so-called Bag Phone.\nIn 1992, the first smartphone, called IBM Simon, used AMPS. Frank Canova led its design at IBM and it was demonstrated that year at the COMDEX computer-industry trade-show. A refined version of the product was marketed to consumers in 1994 by BellSouth under the name Simon Personal Communicator. The Simon was the first device that can be properly referred to as a \"smartphone\", even though that term was not yet coined.\nTechnology.\nAMPS is a first-generation cellular technology that uses separate frequencies, or \"channels\", for each conversation. It therefore required considerable bandwidth for a large number of users. In general terms, AMPS was very similar to the older \"0G\" Improved Mobile Telephone Service it replaced, but used considerably more computing power to select frequencies, hand off conversations to land lines, and handle billing and call setup.\nWhat really separated AMPS from older systems is the \"back end\" call setup functionality. In AMPS, the cell centers could flexibly assign channels to handsets based on signal strength, allowing the same frequency to be re-used, without interference, if locations were separated enough. The channels were grouped so a specific set was different of the one used on the cell nearby. This allowed a larger number of phones to be supported over a geographical area. AMPS pioneers coined the term \"cellular\" because of its use of small hexagonal \"cells\" within a system.\nAMPS suffered from many weaknesses compared to today's digital technologies. As an analog standard, it was susceptible to static and noise, and there was no protection from 'eavesdropping' using a scanner or an older TV set that could tune into channels 70\u201383.\nCloning.\nIn the 1990s, an epidemic of \"cloning\" cost the cellular carriers millions of dollars. An eavesdropper with specialized equipment could intercept a handset's ESN (Electronic Serial Number) and MDN or CTN (Mobile Directory Number or Cellular Telephone Number). The Electronic Serial Number, a 12-digit number sent by the handset to the cellular system for billing purposes, uniquely identified that phone on the network. The system then allowed or disallowed calls and/or features based on its customer file. A person intercepting an ESN/MDN pair could clone the combination onto a different phone and use it in other areas for making calls without paying.\nCellular phone cloning became possible with off-the-shelf technology in the 1990s. Would-be cloners required three key items :\nThe radio, when tuned to the proper frequency, would receive the signal transmitted by the cell phone to be cloned, containing the phone's ESN/MDN pair. This signal would feed into the sound-card audio-input of the PC, and Banpaia would decode the ESN/MDN pair from this signal and display it on the screen. The hacker could then copy that data into the Oki 900 phone and reboot it, after which the phone network could not distinguish the Oki from the original phone whose signal had been received. This gave the cloner, through the Oki phone, the ability to use the mobile-phone service of the legitimate subscriber whose phone was cloned \u2013 just as if that phone had been physically stolen, except that the subscriber retained his or her phone, unaware that the phone had been cloned\u2014at least until that subscriber received his or her next bill.\nThe problem became so large that some carriers required the use of a PIN before making calls. Eventually, the cellular companies initiated a system called RF Fingerprinting, whereby it could determine subtle differences in the signal of one phone from another and shut down some cloned phones. Some legitimate customers had problems with this though if they made certain changes to their own phone, such as replacing the battery and/or antenna.\nThe Oki 900 could listen in to AMPS phone-calls right out-of-the-box with no hardware modifications.\nStandards.\nAMPS was originally standardized by American National Standards Institute (ANSI) as EIA/TIA/IS-3. EIA/TIA/IS-3 was superseded by EIA/TIA-553 and TIA interim standard with digital technologies, the cost of wireless service is so low that the problem of cloning has virtually disappeared.\nFrequency bands.\nAMPS cellular service operated in the 850 MHz Cellular band. For each market area, the United States Federal Communications Commission (FCC) allowed two licensees (networks) known as \"A\" and \"B\" carriers. Each carrier within a market used a specified \"block\" of frequencies consisting of 21 control channels and 395 voice channels. Originally, the B (wireline) side license was usually owned by the local phone company, and the A (non-wireline) license was given to wireless telephone providers.\nAt the inception of cellular in 1983, the FCC had granted each carrier within a market 333 channel pairs (666 channels total). By the late 1980s, the cellular industry's subscriber base had grown into the millions across America and it became necessary to add channels for additional capacity. In 1989, the FCC granted carriers an expansion from the previous 666 channels to the final 832 (416 pairs per carrier). The additional frequencies were from the band held in reserve for future (inevitable) expansion. These frequencies were immediately adjacent to the existing cellular band. These bands had previously been allocated to UHF TV channels 70\u201383.\nEach duplex channel was composed of 2 frequencies. 416 of these were in the 824\u2013849\u00a0MHz range for transmissions from mobile stations to the base stations, paired with 416 frequencies in the 869\u2013894\u00a0MHz range for transmissions from base stations to the mobile stations. Each cell site used a different subset of these channels than its neighbors to avoid interference. This significantly reduced the number of channels available at each site in real-world systems. Each AMPS channel had a one way bandwidth of 30\u00a0kHz, for a total of 60\u00a0kHz for each duplex channel.\nLaws were passed in the US which prohibited the FCC type acceptance and sale of any receiver which could tune the frequency ranges occupied by analog AMPS cellular services. Though the service is no longer offered, these laws remain in force (although they may no longer be enforced).\nNarrowband AMPS.\nIn 1991, Motorola proposed an AMPS enhancement known as narrowband AMPS (NAMPS or N-AMPS).\nDigital AMPS.\nLater, many AMPS networks were partially converted to D-AMPS, often referred to as TDMA (though TDMA is a generic term that applies to many 2G cellular systems). D-AMPS, commercially deployed since 1993, was a digital, 2G standard used mainly by AT&amp;T Mobility and U.S. Cellular in the United States, Rogers Wireless in Canada, Telcel in Mexico, Telecom Italia Mobile (TIM) in Brazil, VimpelCom in Russia, Movilnet in Venezuela, and Cellcom in Israel. In most areas, D-AMPS is no longer offered and has been replaced by more advanced digital wireless networks.\nSuccessor technologies.\nAMPS and D-AMPS have now been phased out in favor of either CDMA2000 or GSM, which allow for higher capacity data transfers for services such as WAP, Multimedia Messaging System (MMS), and wireless Internet access. There are some phones capable of supporting AMPS, D-AMPS and GSM all in one phone (using the GAIT standard).\nAnalog AMPS being replaced by digital.\nIn 2002, the FCC decided to no longer require A and B carriers to support AMPS service as of February 18, 2008. All AMPS carriers have converted to a digital standard such as CDMA2000 or GSM. Digital technologies such as GSM and CDMA2000 support multiple voice calls on the same channel and offer enhanced features such as two-way text messaging and data services.\nUnlike in the United States, the Canadian Radio-television and Telecommunications Commission (CRTC) and Industry Canada have not set any requirement for maintaining AMPS service in Canada. Rogers Wireless has dismantled their AMPS (along with IS-136) network; the networks were shut down May 31, 2007. Bell Mobility and Telus Mobility, who operated AMPS networks in Canada, announced that they would observe the same timetable as outlined by the FCC in the United States, and as a result would not begin to dismantle their AMPS networks until after February 2008.\nOnStar relied heavily on North American AMPS service for its subscribers because, when the system was developed, AMPS offered the most comprehensive wireless coverage in the US. In 2006, ADT asked the FCC to extend the AMPS deadline due to many of their alarm systems still using analog technology to communicate with the control centers. Cellular companies who own an A or B license (such as Verizon and Alltel) were required to provide analog service until February 18, 2008. After that point, however, most cellular companies were eager to shut down AMPS and use the remaining channels for digital services. OnStar transitioned to digital service with the help of data transport technology developed by Airbiquity, but warned customers who could not be upgraded to digital service that their service would permanently expire on January 1, 2008."}
{"id": "2819", "revid": "47842174", "url": "https://en.wikipedia.org/wiki?curid=2819", "title": "Aerodynamics", "text": " Aerodynamics ( \"aero\" (air) + (dynamics)) is the study of the motion of air, particularly when affected by a solid object, such as an airplane wing. It involves topics covered in the field of fluid dynamics and its subfield of gas dynamics, and is an important domain of study in aeronautics. The term \"aerodynamics\" is often used synonymously with gas dynamics, the difference being that \"gas dynamics\" applies to the study of the motion of all gases, and is not limited to air. The formal study of aerodynamics began in the modern sense in the eighteenth century, although observations of fundamental concepts such as aerodynamic drag were recorded much earlier. Most of the early efforts in aerodynamics were directed toward achieving heavier-than-air flight, which was first demonstrated by Otto Lilienthal in 1891. Since then, the use of aerodynamics through mathematical analysis, empirical approximations, wind tunnel experimentation, and computer simulations has formed a rational basis for the development of heavier-than-air flight and a number of other technologies. Recent work in aerodynamics has focused on issues related to compressible flow, turbulence, and boundary layers and has become increasingly computational in nature.\nHistory.\nModern aerodynamics only dates back to the seventeenth century, but aerodynamic forces have been harnessed by humans for thousands of years in sailboats and windmills, and images and stories of flight appear throughout recorded history, such as the Ancient Greek legend of Icarus and Daedalus. Fundamental concepts of continuum, drag, and pressure gradients appear in the work of Aristotle and Archimedes.\nIn 1726, Sir Isaac Newton became the first person to develop a theory of air resistance, making him one of the first aerodynamicists. Dutch-Swiss mathematician Daniel Bernoulli followed in 1738 with \"Hydrodynamica\" in which he described a fundamental relationship between pressure, density, and flow velocity for incompressible flow known today as Bernoulli's principle, which provides one method for calculating aerodynamic lift. In 1757, Leonhard Euler published the more general Euler equations which could be applied to both compressible and incompressible flows. The Euler equations were extended to incorporate the effects of viscosity in the first half of the 1800s, resulting in the Navier\u2013Stokes equations. The Navier\u2013Stokes equations are the most general governing equations of fluid flow but are difficult to solve for the flow around all but the simplest of shapes.\nIn 1799, Sir George Cayley became the first person to identify the four aerodynamic forces of flight (weight, lift, drag, and thrust), as well as the relationships between them, and in doing so outlined the path toward achieving heavier-than-air flight for the next century. In 1871, Francis Herbert Wenham constructed the first wind tunnel, allowing precise measurements of aerodynamic forces. Drag theories were developed by Jean le Rond d'Alembert, Gustav Kirchhoff, and Lord Rayleigh. In 1889, Charles Renard, a French aeronautical engineer, became the first person to reasonably predict the power needed for sustained flight. Otto Lilienthal, the first person to become highly successful with glider flights, was also the first to propose thin, curved airfoils that would produce high lift and low drag. Building on these developments as well as research carried out in their own wind tunnel, the Wright brothers flew the first powered airplane on December 17, 1903.\nDuring the time of the first flights, Frederick W. Lanchester, Martin Kutta, and Nikolai Zhukovsky independently created theories that connected circulation of a fluid flow to lift. Kutta and Zhukovsky went on to develop a two-dimensional wing theory. Expanding upon the work of Lanchester, Ludwig Prandtl is credited with developing the mathematics behind thin-airfoil and lifting-line theories as well as work with boundary layers.\nAs aircraft speed increased designers began to encounter challenges associated with air compressibility at speeds near the speed of sound. The differences in airflow under such conditions lead to problems in aircraft control, increased drag due to shock waves, and the threat of structural failure due to aeroelastic flutter. The ratio of the flow speed to the speed of sound was named the Mach number after Ernst Mach who was one of the first to investigate the properties of the supersonic flow. Macquorn Rankine and Pierre Henri Hugoniot independently developed the theory for flow properties before and after a shock wave, while Jakob Ackeret led the initial work of calculating the lift and drag of supersonic airfoils. Theodore von K\u00e1rm\u00e1n and Hugh Latimer Dryden introduced the term transonic to describe flow speeds between the critical Mach number and Mach 1 where drag increases rapidly. This rapid increase in drag led aerodynamicists and aviators to disagree on whether supersonic flight was achievable until the sound barrier was broken in 1947 using the Bell X-1 aircraft.\nBy the time the sound barrier was broken, aerodynamicists' understanding of the subsonic and low supersonic flow had matured. The Cold War prompted the design of an ever-evolving line of high-performance aircraft. Computational fluid dynamics began as an effort to solve for flow properties around complex objects and has rapidly grown to the point where entire aircraft can be designed using computer software, with wind-tunnel tests followed by flight tests to confirm the computer predictions. Understanding of supersonic and hypersonic aerodynamics has matured since the 1960s, and the goals of aerodynamicists have shifted from the behaviour of fluid flow to the engineering of a vehicle such that it interacts predictably with the fluid flow. Designing aircraft for supersonic and hypersonic conditions, as well as the desire to improve the aerodynamic efficiency of current aircraft and propulsion systems, continues to motivate new research in aerodynamics, while work continues to be done on important problems in basic aerodynamic theory related to flow turbulence and the existence and uniqueness of analytical solutions to the Navier\u2013Stokes equations.\nFundamental concepts.\nUnderstanding the motion of air around an object (often called a flow field) enables the calculation of forces and moments acting on the object. In many aerodynamics problems, the forces of interest are the fundamental forces of flight: lift, drag, thrust, and weight. Of these, lift and drag are aerodynamic forces, i.e. forces due to air flow over a solid body. Calculation of these quantities is often founded upon the assumption that the flow field behaves as a continuum. Continuum flow fields are characterized by properties such as flow velocity, pressure, density, and temperature, which may be functions of position and time. These properties may be directly or indirectly measured in aerodynamics experiments or calculated starting with the equations for conservation of mass, momentum, and energy in air flows. Density, flow velocity, and an additional property, viscosity, are used to classify flow fields.\nFlow classification.\nFlow velocity is used to classify flows according to speed regime. Subsonic flows are flow fields in which the air speed field is always below the local speed of sound. Transonic flows include both regions of subsonic flow and regions in which the local flow speed is greater than the local speed of sound. Supersonic flows are defined to be flows in which the flow speed is greater than the speed of sound everywhere. A fourth classification, hypersonic flow, refers to flows where the flow speed is much greater than the speed of sound. Aerodynamicists disagree on the precise definition of hypersonic flow.\nCompressible flow accounts for varying density within the flow. Subsonic flows are often idealized as incompressible, i.e. the density is assumed to be constant. Transonic and supersonic flows are compressible, and calculations that neglect the changes of density in these flow fields will yield inaccurate results.\nViscosity is associated with the frictional forces in a flow. In some flow fields, viscous effects are very small, and approximate solutions may safely neglect viscous effects. These approximations are called inviscid flows. Flows for which viscosity is not neglected are called viscous flows. Finally, aerodynamic problems may also be classified by the flow environment. External aerodynamics is the study of flow around solid objects of various shapes (e.g. around an airplane wing), while internal aerodynamics is the study of flow through passages inside solid objects (e.g. through a jet engine).\nContinuum assumption.\nUnlike liquids and solids, gases are composed of discrete molecules which occupy only a small fraction of the volume filled by the gas. On a molecular level, flow fields are made up of the collisions of many individual of gas molecules between themselves and with solid surfaces. However, in most aerodynamics applications, the discrete molecular nature of gases is ignored, and the flow field is assumed to behave as a continuum. This assumption allows fluid properties such as density and flow velocity to be defined everywhere within the flow.\nThe validity of the continuum assumption is dependent on the density of the gas and the application in question. For the continuum assumption to be valid, the mean free path length must be much smaller than the length scale of the application in question. For example, many aerodynamics applications deal with aircraft flying in atmospheric conditions, where the mean free path length is on the order of micrometers and where the body is orders of magnitude larger. In these cases, the length scale of the aircraft ranges from a few meters to a few tens of meters, which is much larger than the mean free path length. For such applications, the continuum assumption is reasonable. The continuum assumption is less valid for extremely low-density flows, such as those encountered by vehicles at very high altitudes (e.g. 300,000\u00a0ft/90\u00a0km) or satellites in Low Earth orbit. In those cases, statistical mechanics is a more accurate method of solving the problem than is continuum aerodynamics. The Knudsen number can be used to guide the choice between statistical mechanics and the continuous formulation of aerodynamics.\nConservation laws.\nThe assumption of a fluid continuum allows problems in aerodynamics to be solved using fluid dynamics conservation laws. Three conservation principles are used: \nTogether, these equations are known as the Navier\u2013Stokes equations, although some authors define the term to only include the momentum equation(s). The Navier\u2013Stokes equations have no known analytical solution and are solved in modern aerodynamics using computational techniques. Because computational methods using high speed computers were not historically available and the high computational cost of solving these complex equations now that they are available, simplifications of the Navier\u2013Stokes equations have been and continue to be employed. The Euler equations are a set of similar conservation equations which neglect viscosity and may be used in cases where the effect of viscosity is expected to be small. Further simplifications lead to Laplace's equation and potential flow theory. Additionally, Bernoulli's equation is a solution in one dimension to both the momentum and energy conservation equations.\nThe ideal gas law or another such equation of state is often used in conjunction with these equations to form a determined system that allows the solution for the unknown variables.\nBranches of aerodynamics.\nAerodynamic problems are classified by the flow environment or properties of the flow, including flow speed, compressibility, and viscosity. \"External\" aerodynamics is the study of flow around solid objects of various shapes. Evaluating the lift and drag on an airplane or the shock waves that form in front of the nose of a rocket are examples of external aerodynamics. \"Internal\" aerodynamics is the study of flow through passages in solid objects. For instance, internal aerodynamics encompasses the study of the airflow through a jet engine or through an air conditioning pipe.\nAerodynamic problems can also be classified according to whether the flow speed is below, near or above the speed of sound. A problem is called subsonic if all the speeds in the problem are less than the speed of sound, transonic if speeds both below and above the speed of sound are present (normally when the characteristic speed is approximately the speed of sound), supersonic when the characteristic flow speed is greater than the speed of sound, and hypersonic when the flow speed is much greater than the speed of sound. Aerodynamicists disagree over the precise definition of hypersonic flow; a rough definition considers flows with Mach numbers above 5 to be hypersonic.\nThe influence of viscosity on the flow dictates a third classification. Some problems may encounter only very small viscous effects, in which case viscosity can be considered to be negligible. The approximations to these problems are called inviscid flows. Flows for which viscosity cannot be neglected are called viscous flows.\nIncompressible aerodynamics.\nAn incompressible flow is a flow in which density is constant in both time and space. Although all real fluids are compressible, a flow is often approximated as incompressible if the effect of the density changes cause only small changes to the calculated results. This is more likely to be true when the flow speeds are significantly lower than the speed of sound. Effects of compressibility are more significant at speeds close to or above the speed of sound. The Mach number is used to evaluate whether the incompressibility can be assumed, otherwise the effects of compressibility must be included.\nSubsonic flow.\nSubsonic (or low-speed) aerodynamics describes fluid motion in flows which are much lower than the speed of sound everywhere in the flow. There are several branches of subsonic flow but one special case arises when the flow is inviscid, incompressible and irrotational. This case is called potential flow and allows the differential equations that describe the flow to be a simplified version of the equations of fluid dynamics, thus making available to the aerodynamicist a range of quick and easy solutions.\nIn solving a subsonic problem, one decision to be made by the aerodynamicist is whether to incorporate the effects of compressibility. Compressibility is a description of the amount of change of density in the flow. When the effects of compressibility on the solution are small, the assumption that density is constant may be made. The problem is then an incompressible low-speed aerodynamics problem. When the density is allowed to vary, the flow is called compressible. In air, compressibility effects are usually ignored when the Mach number in the flow does not exceed 0.3 (about 335 feet (102\u00a0m) per second or 228 miles (366\u00a0km) per hour at 60\u00a0\u00b0F (16\u00a0\u00b0C)). Above Mach 0.3, the problem flow should be described using compressible aerodynamics.\nCompressible aerodynamics.\nAccording to the theory of aerodynamics, a flow is considered to be compressible if the density changes along a streamline. This means that \u2013 unlike incompressible flow \u2013 changes in density are considered. In general, this is the case where the Mach number in part or all of the flow exceeds 0.3. The Mach 0.3 value is rather arbitrary, but it is used because gas flows with a Mach number below that value demonstrate changes in density of less than 5%. Furthermore, that maximum 5% density change occurs at the stagnation point (the point on the object where flow speed is zero), while the density changes around the rest of the object will be significantly lower. Transonic, supersonic, and hypersonic flows are all compressible flows.\nTransonic flow.\nThe term Transonic refers to a range of flow velocities just below and above the local speed of sound (generally taken as Mach 0.8\u20131.2). It is defined as the range of speeds between the critical Mach number, when some parts of the airflow over an aircraft become supersonic, and a higher speed, typically near Mach 1.2, when all of the airflow is supersonic. Between these speeds, some of the airflow is supersonic, while some of the airflow is not supersonic.\nSupersonic flow.\nSupersonic aerodynamic problems are those involving flow speeds greater than the speed of sound. Calculating the lift on the Concorde during cruise can be an example of a supersonic aerodynamic problem.\nSupersonic flow behaves very differently from subsonic flow. Fluids react to differences in pressure; pressure changes are how a fluid is \"told\" to respond to its environment. Therefore, since sound is, in fact, an infinitesimal pressure difference propagating through a fluid, the speed of sound in that fluid can be considered the fastest speed that \"information\" can travel in the flow. This difference most obviously manifests itself in the case of a fluid striking an object. In front of that object, the fluid builds up a stagnation pressure as impact with the object brings the moving fluid to rest. In fluid traveling at subsonic speed, this pressure disturbance can propagate upstream, changing the flow pattern ahead of the object and giving the impression that the fluid \"knows\" the object is there by seemingly adjusting its movement and is flowing around it. In a supersonic flow, however, the pressure disturbance cannot propagate upstream. Thus, when the fluid finally reaches the object it strikes it and the fluid is forced to change its properties \u2013 temperature, density, pressure, and Mach number\u2014in an extremely violent and irreversible fashion called a shock wave. The presence of shock waves, along with the compressibility effects of high-flow velocity (see Reynolds number) fluids, is the central difference between the supersonic and subsonic aerodynamics regimes.\nHypersonic flow.\nIn aerodynamics, hypersonic speeds are speeds that are highly supersonic. In the 1970s, the term generally came to refer to speeds of Mach 5 (5 times the speed of sound) and above. The hypersonic regime is a subset of the supersonic regime. Hypersonic flow is characterized by high temperature flow behind a shock wave, viscous interaction, and chemical dissociation of gas.\nAssociated terminology.\nThe incompressible and compressible flow regimes produce many associated phenomena, such as boundary layers and turbulence.\nBoundary layers.\nThe concept of a boundary layer is important in many problems in aerodynamics. The viscosity and fluid friction in the air is approximated as being significant only in this thin layer. This assumption makes the description of such aerodynamics much more tractable mathematically.\nTurbulence.\nIn aerodynamics, turbulence is characterized by chaotic property changes in the flow. These include low momentum diffusion, high momentum convection, and rapid variation of pressure and flow velocity in space and time. Flow that is not turbulent is called laminar flow.\nAerodynamics in other fields.\nEngineering design.\nAerodynamics is a significant element of vehicle design, including road cars and trucks where the main goal is to reduce the vehicle drag coefficient, and racing cars, where in addition to reducing drag the goal is also to increase the overall level of downforce. Aerodynamics is also important in the prediction of forces and moments acting on sailing vessels. It is used in the design of mechanical components such as hard drive heads. Structural engineers resort to aerodynamics, and particularly aeroelasticity, when calculating wind loads in the design of large buildings, bridges, and wind turbines.\nThe aerodynamics of internal passages is important in heating/ventilation, gas piping, and in automotive engines where detailed flow patterns strongly affect the performance of the engine.\nEnvironmental design.\nUrban aerodynamics are studied by town planners and designers seeking to improve amenity in outdoor spaces, or in creating urban microclimates to reduce the effects of urban pollution. The field of environmental aerodynamics describes ways in which atmospheric circulation and flight mechanics affect ecosystems.\nAerodynamic equations are used in numerical weather prediction.\nBall-control in sports.\nSports in which aerodynamics are of crucial importance include soccer, table tennis, cricket, baseball, and golf, in which most players can control the trajectory of the ball using the \"Magnus effect\".\nFurther reading.\nGeneral aerodynamics\nSubsonic aerodynamics\nTransonic aerodynamics\nSupersonic aerodynamics\nHypersonic aerodynamics\nHistory of aerodynamics\nAerodynamics related to engineering\n\"Ground vehicles\"\n\"Fixed-wing aircraft\"\n\"Helicopters\"\n\"Missiles\"\n\"Model aircraft\"\nRelated branches of aerodynamics\n\"Aerothermodynamics\"\n\"Aeroelasticity\"\n\"Boundary layers\"\n\"Turbulence\""}
{"id": "2820", "revid": "16185737", "url": "https://en.wikipedia.org/wiki?curid=2820", "title": "Andreas Schl\u00fcter", "text": "Andreas Schl\u00fcter (1659 \u2013 c. June 1714) was a German baroque sculptor and architect, active in the Holy Roman Empire of the German Nation, the Polish\u2013Lithuanian Commonwealth, and the Russian Tsardom.\nBiography.\nAndreas Schl\u00fcter was born probably in Hamburg. His early life is obscure as at least three different persons of that name are documented. The records of St. Michaelis Church, Hamburg show that an Andreas Schl\u00fcter, son of sculptor Gerhart Schl\u00fcter, had been baptized there on 22 May 1664. Documents from Danzig/Gda\u0144sk (Royal Prussia) reported that an Andreas Schl\u00fcter \"(senior)\" had worked 1640\u20131652 in Danzig's Jopengasse lane (today's ulica Piwna). Possibly born in 1640, an \"Andres Schliter\" is recorded as apprentice on 9 May 1656 by the mason's guild. Other sources state 1659 as year of birth.\nHe probably did spend several years abroad as Journeyman. His first work, in 1675, may have been epitaphs of the Dukes Sambor and Mestwin in the dome of Pelplin monastery.\nSchl\u00fcter's first known work was the decoration of the facade of the Danzig Royal Chapel, in 1681. He later created statues for King John III Sobieski's Wilan\u00f3w Palace in Warsaw and sepulchral sculptures in Zhovkva. In 1689, he moved to Warsaw and made the pediment reliefs and sculptural work of Krasi\u0144ski Palace.\nSchl\u00fcter was invited to Berlin in 1694 by Eberhard von Danckelmann to work as court sculptor at the armory (\"Zeughaus\") for Elector Frederick III. His sculpted decorations are a masterpiece of baroque expression and pathos. While the more visible reliefs on the outside had to praise fighting, the statues of dying warriors in the interior denounced war and gave an indication of his pacifist religious beliefs (he is said to have been a Mennonite). Travelling through Italy in 1696, he studied the work of masters like Michelangelo Buonarroti and Gian Lorenzo Bernini.\nSchl\u00fcter also worked as an architect and built many state buildings in Berlin in his role as \"Hofbaumeister\" (Court Architect), which he lost when one tower showed signs of a weak fundament. He also served as director of the Prussian Academy of Arts from 1702 to 1704, after which he began concentrating on sculpting again, as \"Hofbildhauer\" (Court Sculptor). His most important equestrian sculpture is that of the \"Great Elector\", Frederick William of Brandenburg, cast in 1708 and placed at \"Lange Br\u00fccke\" near the Berlin City Palace, now situated in the honor court before Charlottenburg Palace.\nThe Berlin City Palace, and many of his works, were partially destroyed by bombing in World War II and by the subsequent Communist regime. A similar fate probably befell the Amber Room, made between 1701 and 1709, Schl\u00fcter's most famous work of architecture.\nIn 1713, Schl\u00fcter's fame brought him to work for Tsar Peter I of Russia in Saint Petersburg, where he died of an illness after creating several designs. Together with Johann Friedrich Braunstein, he designed the Grand Palace and Monplaisir Palace in Peterhof Palace Complex. Also the city's oldest building, Kikin Hall, and the reliefs at the Summer Palace are attributed to him. This way he became an important figure of Petrine Baroque."}
{"id": "2821", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2821", "title": "Axiomatic Set Theory", "text": ""}
{"id": "2822", "revid": "45087769", "url": "https://en.wikipedia.org/wiki?curid=2822", "title": "Ash", "text": "Ash is the solid remnants of fires. Specifically, \"ash\" refers to all non-aqueous, non-gaseous residues that remain after something burns. In analytical chemistry, to analyse the mineral and metal content of chemical samples, ash is the non-gaseous, non-liquid residue after complete combustion.\nAshes as the end product of incomplete combustion are mostly mineral, but usually still contain an amount of combustible organic or other oxidizable residues. The best-known type of ash is wood ash, as a product of wood combustion in campfires, fireplaces, etc. The darker the wood ashes, the higher the content of remaining charcoal from incomplete combustion. The ashes are of different types. Some ashes contain natural compounds that make soil fertile. Others have chemical compounds that can be toxic but may break up in soil from chemical changes and microorganism activity.\nLike soap, ash is also a disinfecting agent (alkaline). The World Health Organization recommends ash or sand as alternative for handwashing when soap is not available. Before industrialization, ash soaked in water was the primary means of obtaining potash.\nNatural occurrence.\nAsh occurs naturally from any fire that burns vegetation, and may disperse in the soil to fertilise it, or clump under it for long enough to carbonise into coal.\nComposition.\nThe composition of the ash varies depending on the product burned and its origin. The \"ash content\" or \"mineral content\" of a product is derived its incineration under temperatures ranging from to .\nWood and plant matter.\nThe composition of ash derived from wood and other plant matter varies based on plant species, parts of the plants (such as bark, trunk, or young branches with foliage), type of soil, and time of year. The composition of these ashes also differ greatly depending on mode of combustion.\nWood ashes, in addition to residual carbonaceous materials (unconsumed embers, activated carbons impregnated with carbonaceous particles, tars, various gases, etc.), contain a between 20% and 50% calcium in the form of calcium oxide and are generally rich in potassium carbonate. Ashes derived from grasses, and the Gramineae family in particular, are rich in silica. The color of the ash comes from small proportions of inorganic minerals such as iron oxides and manganese. The oxidized metal elements that constitute wood ash are mostly considered alkaline.\nFor example, ash collected from wood boilers is composed of\nThe pH of the ash is between 10 and 13, mostly due to the fact that the oxides of calcium, potassium, and sodium are strong bases. Acidic components such as carbon dioxide, phosphoric acid, silicic acid, and sulfuric acid are rarely present and, in the presence of the previously mentioned bases, are generally found in the form of salts, respectively carbonates, phosphates, silicates and sulphates.\nStrictly speaking, calcium and potassium salts produce the aforementioned calcium oxide (also known as quicklime) and potassium during the combustion of organic matter. But, in practice, quicklime is only obtained via lime-kiln, and potash (from potassium carbonate) or baking soda (from sodium carbonate) is extracted from the ashes.\nOther substances such as sulfur, chlorine, iron or sodium only appear in small quantities. Still others are rarely found in wood, such as aluminum, zinc, and boron. (depending on the trace elements drawn from the soil by the incinerated plants).\nMineral content in ash depends on the species of tree burned, even in the same soil conditions. More chloride is found in conifer trees than broadleaf trees, with seven times as much found in spruces than in oak trees. There is twice as much phosphoric acid in the European aspen than in oaks and twice as much magnesium in elm trees than in the Scotch pine.\nAsh composition also varies by which part of the tree was burnt. Silicon and calcium salts are more abundant in bark than in wood, while potassium salts are primarily found in wood. Compositional variation also occurred based on the season in which the tree died.\nSpecific types.\nCremation ashes.\nCremation ashes, also called cremated remains or \"cremains,\" are the bodily remains left from cremation. They often take the form of a grey powder resembling coarse sand. While often referred to as \"ashes\", the remains primarily consist of powdered bone fragments due to the cremation process, which eliminates the body's organic materials. People often store these ashes in containers like urns, although they are also sometimes buried or scattered in specific locations.\nFood ashes.\nIn food processing, mineral and ash content is used to characterize the presence of organic and inorganic components in food for monitoring quality, nutritional quantification and labeling, analyzing microbiological stability, and more. This process can be used to measure minerals like calcium, sodium, potassium, and phosphorus as well as metal content such as lead, mercury, cadmium, and aluminum.\nJoss paper ash.\nAnalysis of the contents of ash samples from Vietnam and Singapore shows that joss paper burning can emit many pollutants detrimental to air quality. There is a significant amount of heavy metals in the dust fume and bottom ash, e.g., aluminium, iron, manganese, copper, lead, zinc and cadmium.\n\"Burning of joss paper accounted for up to 42% of the atmospheric rBC [refractory black carbon] mass, higher than traffic (14-17%), crop residue (10-17%), coal (18-20%) during the Hanyi festival in northwest China\", according to a 2022 study, \"the overall air quality can be worsened due to the practice of uncontrolled burning of joss paper during the festival, which is not just confined to the people who do the burning,\" and \"burning joss paper during worship activities is common in China and most Asian countries with similar traditions.\"\nWildfire ash.\nHigh levels of heavy metals, including lead, arsenic, cadmium, and copper were found in the ash debris following the 2007 Californian wildfires. A national clean-up campaign was organised ... In the devastating California Camp Fire (2018) that killed 85 people, lead levels increased by around 50 times in the hours following the fire at a site nearby (Chico). Zinc concentration also increased significantly in Modesto, 150 miles away. Heavy metals such as manganese and calcium were found in numerous California fires as well.\nUses.\nFertilizer.\nAshes have been used since the Neolithic period as fertilizer because they are rich in minerals, especially potash and essential nutrients. They are the main fertilizer in slash-and-burn agriculture, which eventually evolved into controlled burn and forest clearing practices. People in ancient history already possessed extensive knowledge of the nutrients produced by (from social 10th textbook)(manufacturing industries )different ashes. For clay soil in particular, using ash without modification or using , ash whose minerals have been washed with water, was necessary.\nLaundry.\nBecause ashes contain potash, they can be used to make biodegradable laundry detergent. The demand for organic products has led to renewed interest for laundry using ash derived from wood. The French word for laundry is from the Latin word , which means a substance made from ash and used to wash laundry. This usage also developed into a small, traditional architectural structure to the west of Rh\u00f4ne mainstem: the , a masonry structure built with stone or cob, that looks like a cabinet and that carries dirty laundry and fireplace ash; when the is full, the laundry and ash are moved to a laundry container and boiled in water.\nLaundry using ash derived from wood has the benefit of being free, easy to produce, sustainable, and as efficient as standard laundry washing methods.\nEffect on precipitation.\n\"Particles of dust or smoke in the atmosphere are essential for precipitation. These particles, called 'condensation nuclei,' provide a surface for water vapor to condense upon. This helps water droplets gather together and become large enough to fall to the earth\""}
{"id": "2823", "revid": "12336988", "url": "https://en.wikipedia.org/wiki?curid=2823", "title": "Antiderivative", "text": "In calculus, an antiderivative, inverse derivative, primitive function, primitive integral or indefinite integral of a continuous function is a differentiable function whose derivative is equal to the original function . This can be stated symbolically as . The process of solving for antiderivatives is called antidifferentiation (or indefinite integration), and its opposite operation is called \"differentiation\", which is the process of finding a derivative. Antiderivatives are often denoted by capital Roman letters such as and .\nAntiderivatives are related to definite integrals through the second fundamental theorem of calculus: the definite integral of a function over a closed interval where the function is Riemann integrable is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval.\nIn physics, antiderivatives arise in the context of rectilinear motion (e.g., in explaining the relationship between position, velocity and acceleration). The discrete equivalent of the notion of antiderivative is antidifference.\nExamples.\nThe function formula_1 is an antiderivative of formula_2, since the derivative of formula_3 is formula_4. Since the derivative of a constant is zero, formula_4 will have an infinite number of antiderivatives, such as formula_6, etc. Thus, all the antiderivatives of formula_4 can be obtained by changing the value of in formula_8, where is an arbitrary constant known as the constant of integration. The graphs of antiderivatives of a given function are vertical translations of each other, with each graph's vertical location depending upon the value .\nMore generally, the power function formula_9 has antiderivative formula_10 if , and formula_11 if .\nIn physics, the integration of acceleration yields velocity plus a constant. The constant is the initial velocity term that would be lost upon taking the derivative of velocity, because the derivative of a constant term is zero. This same pattern applies to further integrations and derivatives of motion (position, velocity, acceleration, and so on). Thus, integration produces the relations of acceleration, velocity and displacement:\nformula_12\nUses and properties.\nAntiderivatives can be used to compute definite integrals, using the fundamental theorem of calculus: if is an antiderivative of the continuous function over the interval formula_13, then:\nformula_14\nBecause of this, each of the infinitely many antiderivatives of a given function may be called the \"indefinite integral\" of \"f\" and written using the integral symbol with no bounds:\nformula_15\nIf is an antiderivative of , and the function is defined on some interval, then every other antiderivative of differs from by a constant: there exists a number such that formula_16 for all . is called the constant of integration. If the domain of is a disjoint union of two or more (open) intervals, then a different constant of integration may be chosen for each of the intervals. For instance\nformula_17\nis the most general antiderivative of formula_18 on its natural domain formula_19\nEvery continuous function has an antiderivative, and one antiderivative is given by the definite integral of with variable upper boundary:\nformula_20\nfor any in the domain of . Varying the lower boundary produces other antiderivatives, but not necessarily all possible antiderivatives. This is another formulation of the fundamental theorem of calculus.\nThere are many elementary functions whose antiderivatives, even though they exist, cannot be expressed in terms of elementary functions. Elementary functions are polynomials, exponential functions, logarithms, trigonometric functions, inverse trigonometric functions and their combinations under composition and linear combination. Examples of these nonelementary integrals are\nFor a more detailed discussion, see also Differential Galois theory.\nTechniques of integration.\nFinding antiderivatives of elementary functions is often considerably harder than finding their derivatives (indeed, there is no pre-defined method for computing indefinite integrals). For some elementary functions, it is impossible to find an antiderivative in terms of other elementary functions. To learn more, see elementary functions and nonelementary integral.\nThere exist many properties and techniques for finding antiderivatives. These include, among others:\nComputer algebra systems can be used to automate some or all of the work involved in the symbolic techniques above, which is particularly useful when the algebraic manipulations involved are very complex or lengthy. Integrals which have already been derived can be looked up in a table of integrals.\nOf non-continuous functions.\nNon-continuous functions can have antiderivatives. While there are still open questions in this area, it is known that:\nAssuming that the domains of the functions are open intervals:"}
{"id": "2824", "revid": "44987666", "url": "https://en.wikipedia.org/wiki?curid=2824", "title": "The ABC Song", "text": "\"The ABC Song\" is the best-known song used to recite the English alphabet in alphabetical order. It is commonly used to teach the alphabet to children in English-speaking countries. \"The ABC Song\" was first copyrighted in 1835 by Boston music publisher Charles Bradlee. The melody is from a 1761 French music book and is also used in other nursery rhymes like \"Twinkle, Twinkle, Little Star\", while the author of the lyrics is unknown. Songs set to the same melody are also used to teach the alphabets of other languages.\nHistory.\nThe melody of \"The ABC Song\" was first published in the French book of music \"Les Amusements d'une Heure et Demy\" (\"\") (1761) without lyrics. It was adapted in Mozart's Twelve Variations and used in many nursery rhymes around the world, including \"Ah! vous dirai-je, maman\", \"Twinkle, Twinkle, Little Star\" and later \"Baa, Baa, Black Sheep\", before being used in this song. The author of the lyrics is unknown.\n\"The ABC Song\" was first copyrighted in 1835 by Boston music publisher Charles Bradlee under the title \"The A.B.C., a German air with variations for the flute with an easy accompaniment for the piano forte.\" The melody was attributed to 18th-century composer Louis Le Maire.\n\"The ABC Song\" is commonly used in preschools across English-speaking countries. Due to the speed at which '\"L, M, N, O, P\"' is spoken, it is a common misconception among children still learning the alphabet to believe that it is in fact its own letter called \"elemenopee\". Some have proposed teaching slower versions of the song to avoid this issue, but attempts to do so have been criticized for lacking the end rhymes and the \"'L, M, N, O, P\"\" part being an essential part of the song. The television series \"Sesame Street\" has covered the song many times, collaborating with popular artists such as Stevie Wonder, Katy Perry, Nina Simone and Usher.\nComposition and variations.\nLyrics: \"(each line represents two measures, or eight beats)\"\n&lt;score lang=\"lilypond\" sound=\"1\"&gt;\\relative c' {\n \\key c \\major \\time 4/4\n c4 c4 g'4 g4 \\bar \"|\" a4 a4 g2 \\bar \"|\"\n f4 f4 e4 e4 \\bar \"|\" d8 d8 d8 d8 c2 \\bar \"|\" \\break\n g'4 g4 f2 \\bar \"|\" e4 e4 d2 \\bar \"|\"\n g8 g8 g4 f2 \\bar \"|\" e4 e4 d2 \\bar \"|\" \\break\n c4 c4 g'4 g4 \\bar \"|\" a4 a4 g2 \\bar \"|\"\n f4 f4 e4 e4 \\bar \"|\" d4 d4 c2 \\bar \"|.\"\n \\addlyrics {\n A B C D E F G,\n H I J K L M N O P,\n Q R S, T U V,\n W \u3000 \u3000 X, Y and Z.\n Now I know my A B Cs.\n Next time, won't you sing with me?\n }&lt;/score&gt;\nLyrics for the alternate Zed version: \"(each line represents two measures or eight beats)\"\n&lt;score lang=\"lilypond\" sound=\"1\"&gt;\n \\relative c' {\n \\key c \\major \\time 4/4\n c4 c4 g'4 g4 \\bar \"|\" a4 a4 g2 \\bar \"|\"\n f4 f4 e4 e4 \\bar \"|\" d4 d4 c2 \\bar \"|\" \\break\n g'4 g4 f4 f4 \\bar \"|\" e4 e4 d2 \\bar \"|\"\n g4 g8 g8 f4 f4 \\bar \"|\" e4 e4 d2 \\bar \"|\" \\break\n c4 c4 g'4 g4 \\bar \"|\" a4 a4 g2 \\bar \"|\"\n f4 f4 e4 e4 \\bar \"|\" d4 d4 c2 \\bar \"|.\"\n \\addlyrics {\n A B C D E F G,\n H I J K L M N,\n O P Q R S T U,\n V W \u3000 \u3000 X Y and Z.\n Now I know my A B Cs.\n Next time, won't you sing with me?\n&lt;/score&gt;\nPronunciation of \"Z\".\nIn American English, the dialect in mind by the composer, the letter name for Z is pronounced /zi\u02d0/ (\"Zee\"), but in most other anglophone countries, the letter name is pronounced /z\u025bd/ (\"Zed\"). In such dialects, the absent \"Zee\"-rhyme is generally not missed, although while singing the song, some children may accommodate for \"Zee\" which they would otherwise not use on a regular basis. Variants of the song exist to accommodate \"Zed\". One such variation is shown below:\n&lt;score sound=\"1\"&gt;{ \\time 4/4 c'4 c' g' g' | a' a' g'2 | f'4 f' e' e' | d' d' c'2 | g'4 g' f' f' | e' e' d'2 | g'4 \\times 2/3 { f'8 f' f' } e'4 d' | c' r r2 | \\bar \"|.\" } \\addlyrics { A B C D E F G H I J K L M N O P Q R S T U V dub- a- U X Y \"Z(ed)\" }&lt;/score&gt;\nThis version does not have a closing line, and the tune is modified accordingly. The W is not lengthened in this version.\nBackwards alphabet.\nSeveral versions exist covering the alphabet backward, i.e., Z to A. One version is shown below.\nThe e-d-c-b part is as fast as the l-m-n-o part in the normal alphabet song.\nVersions for other languages.\nThe same melody used for \"The ABC Song\" has also been used for the German, French, and Arabic alphabets. A French-language version of the song is also taught in Canada, with generally no alterations to the melody except in the final line that requires adjustment to accommodate the two-syllable pronunciation of the French \"y\"."}
{"id": "2826", "revid": "73920", "url": "https://en.wikipedia.org/wiki?curid=2826", "title": "Antigonid dynasty", "text": "The Antigonid dynasty (; ) was a Macedonian Greek royal house which ruled the kingdom of Macedon during the Hellenistic period. Founded by Antigonus I Monophthalmus, a general and successor of Alexander the Great, the dynasty first came to power after the Battle of Salamis in 306 BC and ruled much of Hellenistic Greece from 294 until their defeat at the Battle of Pydna in 168 BC (Third Macedonian War), after which Macedon came under the control of the Roman Republic. \nThe wars of the Diadochi witnessed the fall of the Argead dynasty in Macedon resulting in a power vacuum, which the Antigonid and Antipatrid dynasties sought to occupy. The Antigonid family first rose to power when Demetrius I Poliorcetes, son of Antigonus I, ousted Cassander's governor of Athens in 306 BC giving his father control over a land spanning from the Aegean Sea to the Middle East. Despite the subsequent instability and loss of the Asian territory, the family managed to maintain its power in mainland Greece and the islands, with Antigonus II Gonatas ultimately solidifying Antigonid rule over Hellenistic Macedon \u2013a territory also known as the Antigonid Empire. Antigonus III Doson further expanded Macedonian influence in southern Greece reestablishing the Hellenic Alliance with himself as the president. Under Philip V, Antigonid Macedon first came into conflict with Rome, which had become a decisive power in the eastern Mediterranean. In the second century BC, the last Antigonid king, Perseus, became known as the champion of Greek resistance against Rome, albeit Rome's control over Antigonid Greece began to steadily expand, culminating in the fall of the dynasty in 168. \nHistory.\nThe beginning of Hellenistic Greece was defined by the struggle between the Antipatrid dynasty, led first by Cassander (r.\u2009305 \u2013 297 BC), son of Antipater, and the Antigonid dynasty, led by Antigonus I Monophthalmus (r.\u2009306 \u2013 301 BC) and his son, the future king Demetrius I Poliorcetes (r.\u2009294 \u2013 288 BC). After the power crisis in Macedon, which culminated in Philip III's and Euridice's death, Cassander managed to seize control from Olympias and began to establish his authority in the kingdom; in 316 BC he buried Philip III and Euridice at Aegae and married Philip II's daughter, Thessalonica, thus becoming a member of the Argead dynasty. In 310/309 BC, Cassander commanded Glaucias to secretly assassinate the 14-year-old Alexander IV, son of Alexander the Great, and his mother Roxane and the Macedonian Argead dynasty became extinct. \nIn 307 BC, Demetrius I successfully ousted Cassander's governor of Athens, Demetrius of Phalerum, and after defeating Ptolemy I at the Battle of Salamis in 306 BC he conquered the island Cyprus. Following that victory, Demetrius' father, Antigonus I, assumed the title of \"Basileus\" (\"King\" of Alexander's Empire) by the assembled armies and gained control over the Aegean, the eastern Mediterranean, and most of the Middle East. While Antigonus and Demetrius attempted to recreate Philip II's Hellenic league with themselves as dual hegemons, a revived coalition of Cassander, Ptolemy I Soter, Seleucus I Nicator, and Lysimachus decisively defeated the Antigonids at the Battle of Ipsus in 301 BC, during which Antigonus I was killed. Demetrius I survived the battle and in 294 BC \u2013during the struggles between Casander's sons Alexander V and Antipater I\u2013 he managed to seize control of Athens and establish himself as king of Macedon. In 288 BC, he was driven out by Pyrrhus and Lysimachus and eventually died as a prisoner of Seleucus I Nicator. After a long period of instability, Demetrius' son Antigonus II Gonatas was able to establish the family's control over the old Kingdom of Macedon, as well as over most of the Greek city-states by 276 BC.\nLegacy.\nThe Antigonid was one of four dynasties established by Alexander's successors, the others being the Seleucid dynasty, Ptolemaic dynasty and Antipatrid dynasty. The last scion of the dynasty, Perseus of Macedon, who reigned between 179 and 168 BC, proved unable to stop the advancing Roman legions and Macedon's defeat at the Battle of Pydna signaled the end of the dynasty.\nDynasty.\nThe ruling members of the Antigonid dynasty were:\nThe Greek rebel against Rome and last King of Macedonia, Andriscus, claimed to be the son of Perseus."}
{"id": "2827", "revid": "1640548", "url": "https://en.wikipedia.org/wiki?curid=2827", "title": "Abingdon", "text": "Abingdon may refer to:"}
{"id": "2828", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=2828", "title": "Abipones", "text": ""}
{"id": "2830", "revid": "1251723947", "url": "https://en.wikipedia.org/wiki?curid=2830", "title": "Abjuration", "text": "Abjuration is the solemn repudiation, abandonment, or renunciation by or upon oath, often the renunciation of citizenship or some other right or privilege. The term comes from the Latin \"abjurare\", \"to forswear\".\nAbjuration of the realm.\nAbjuration of the realm was a type of abjuration in ancient English law. The person taking the oath swore to leave the country directly and promptly, never to return to the kingdom unless by permission of the sovereign. This was often taken by fugitives who had taken sanctuary:\nEnglish Commonwealth.\nNear the start of the English Civil War, on 18 August 1643 Parliament passed \"An Ordinance for Explanation of a former Ordinance for Sequestration of Delinquents Estates with some Enlargements.\" The enlargements included an oath which became known as the \"Oath of Abjuration\":\nIn 1656\u20137, it was reissued in what was for Catholics an even more objectionable form. Everyone was to be \"adjudged a Papist\" who refused this oath, and the consequent penalties began with the confiscation of two-thirds of the recusant's goods, and went on to deprive him of almost every civic right.\nThe Catholic Encyclopaedia makes the point that the oath and the penalties were so severe that it stopped the efforts of the Gallicanizing party among the English Catholics, who had been ready to offer forms of submission similar to the old oath of Allegiance, which was condemned anew about this time by Pope Innocent X.\nScotland.\nDuring The Killing Time of the 1680s an Abjuration Oath could be put to suspects where they were given the option to abjure or renounce their allegiances. The terms of the oath were deliberately designed to offend the consciences of the Presbyterian Covenanters. Those who would not swear \"whether they have arms, or not\" could be \"immediately killed\" by field trial \"before two witnesses\" on a charge of high treason. John Brown was included among those executed in this judicial process by John Graham (Bluidy Clavers) on 1 May 1685. The wives and children of such men could also be put out of their houses if they had spoken to the suspect or refused the oath themselves.\nGreat Britain and Ireland.\nIn England (and after 1707 Great Britain) the Oath of Abjuration denied the royal title of James II's heirs (i.e. the direct Catholic descendant of the House of Stuart exiled after the Glorious Revolution in 1688). In England, an Oath of Abjuration was taken by Members of Parliament, clergy, and laymen, pledging to support the current British monarch and repudiated the right of the Stuarts and other claimants to the throne. This oath was imposed under William III, George I and George III. It was superseded by the oath of allegiance. In Ireland, the oath was imposed of state officeholders, teachers, lawyers, and on the clergy of the established church in from 1703, the following year it was on all Irish voters and from 1709 it could be demanded of any adult male by a magistrate.\nBilino Polje abjuration.\nThe Bilino Polje abjuration, also known as \"Confessio Christianorum bosniensis\", was an act of alleged heresy abjuration by clergy of the Bosnian Church in presence of the Bosnian ruler, Ban Kulin, and Giovanni da Casamari. It affirmed the primacy of the pope and related to errors of practice, stemming from ignorance, rather than heretical doctrines. It was signed by seven Bosnian priors, on 8 April 1203 at Bilino Polje field, near today town of Zenica, in Bosnia and Herzegovina. The same document was brought to Buda, in 30 April by Giovanni da Casamari, Ban Kulin and two abbots, where it was examined by Emeric, King of Hungary, and the high clergy.\nThe Netherlands.\nAnother famous abjuration was brought about by the Plakkaat van Verlatinghe of July 26, 1581, the formal Act of Abjuration or declaration of independence of the Low Countries from the Spanish king, Philip II. This oath was the climax of the Eighty Years' War (Dutch Revolt)."}
{"id": "2831", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2831", "title": "Abkhasia", "text": ""}
{"id": "2833", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=2833", "title": "Abitibi", "text": "Abitibi may refer to:"}
{"id": "2834", "revid": "30211467", "url": "https://en.wikipedia.org/wiki?curid=2834", "title": "A Vindication of the Rights of Woman", "text": "A Vindication of the Rights of Woman: with Strictures on Political and Moral Subjects (1792), written by British philosopher and women's rights advocate Mary Wollstonecraft (1759\u20131797), is one of the earliest works of feminist philosophy. In it, Wollstonecraft responds to those educational and political theorists of the eighteenth century who did not believe women should receive a rational education. She argues that women ought to have an education commensurate with their position in society, claiming that women are essential to the nation because they educate its children and because they could be \"companions\" to their husbands, rather than mere wives. Instead of viewing women as ornaments to society or property to be traded in marriage, Wollstonecraft maintains that they are human beings deserving of the same fundamental rights as men.\nWollstonecraft was prompted to write the \"Rights of Woman\" after reading Charles Maurice de Talleyrand-P\u00e9rigord's 1791 report to the French National Assembly, which stated that women should only receive domestic education. From her reaction to this specific event, she launched a broad attack against double standards, indicting men for encouraging women to indulge in excessive emotion. Wollstonecraft hurried to complete the work in direct response to ongoing events; she intended to write a more thoughtful second volume but died before completing it.\nWhile Wollstonecraft does call for equality between the sexes in particular areas of life, especially morality, she does not explicitly state that men and women are equal. Her ambiguous statements regarding the equality of the sexes have made it difficult to classify Wollstonecraft as a modern feminist; the word itself did not emerge until decades after her death.\nAlthough it is commonly assumed that the \"Rights of Woman\" was unfavourably received, this is a modern misconception based on the belief that Wollstonecraft was as reviled during her lifetime as she became after the publication of William Godwin's \"Memoirs of the Author of A Vindication of the Rights of Woman\" (1798). The \"Rights of Woman\" was generally received well when it was first published in 1792. Biographer Emily W. Sunstein called it \"perhaps the most original book of [Wollstonecraft's] century\". Wollstonecraft's work had significant impact on advocates for women's rights in the nineteenth century, particularly the 1848 Seneca Falls Convention which produced the Declaration of Sentiments laying out the aims of the women's suffrage movement in the United States.\nHistorical context.\n\"A Vindication of the Rights of Woman\" was written against the tumultuous background of the French Revolution and the debates that it spawned in Britain. In a lively and sometimes vicious pamphlet war, now referred to as the Revolution controversy, British political commentators addressed topics ranging from representative government to human rights to the separation of church and state, many of these issues having been raised in France first. Wollstonecraft first entered this fray in 1790 with \"A Vindication of the Rights of Men\", a response to Edmund Burke's \"Reflections on the Revolution in France\" (1790). In his \"Reflections\", Burke criticized the view of many British thinkers and writers who had welcomed the early stages of the French revolution. While they saw the revolution as analogous to Britain's own Glorious Revolution in 1688, which had restricted the powers of the monarchy, Burke argued that the appropriate historical analogy was the English Civil War (1642\u20131651) in which CharlesI had been executed in 1649. He viewed the French revolution as the violent overthrow of a legitimate government. In \"Reflections\" he argues that citizens do not have the right to revolt against their government because civilization is the result of social and political consensus; its traditions cannot be continually challengedthe result would be anarchy. One of the key arguments of Wollstonecraft's \"Rights of Men\", published just six weeks after Burke's \"Reflections\", is that rights cannot be based on tradition; rights, she argues, should be conferred because they are reasonable and just, regardless of their basis in tradition.\nWhen Charles Maurice de Talleyrand-P\u00e9rigord presented his \"Rapport sur l'instruction publique\" (1791) to the National Assembly in France, Wollstonecraft was galvanized to respond. In his recommendations for a national system of education, Talleyrand had written:\nLet us bring up women, not to aspire to advantages which the Constitution denies them, but to know and appreciate those which it guarantees them... Men are destined to live on the stage of the world. A public education suits them: it early places before their eyes all the scenes of life: only the proportions are different. The paternal home is better for the education of women; they have less need to learn to deal with the interests of others, than to accustom themselves to a calm and secluded life.\nWollstonecraft dedicated the \"Rights of Woman\" to Talleyrand: \"Having read with great pleasure a pamphlet which you have lately published, I dedicate this volume to you; to induce you to reconsider the subject, and maturely weigh what I have advanced respecting the rights of woman and national education.\" At the end of 1791, French feminist Olympe de Gouges had published her Declaration of the Rights of Woman and of the Female Citizen, and the question of women's rights became central to political debates in both France and Britain.\nThe \"Rights of Woman\" is an extension of Wollstonecraft's arguments in the \"Rights of Men\". In the \"Rights of Men\", as the title suggests, she is concerned with the rights of particular men (eighteenth-century British men) while in the \"Rights of Woman\", she is concerned with the rights afforded to \"woman\", an abstract category. She does not isolate her argument to eighteenth-century women or British women. The first chapter of the \"Rights of Woman\" addresses the issue of natural rights and asks who has those inalienable rights and on what grounds. She answers that since natural rights are given by God, for one segment of society to deny them to another segment is a sin. \"The Rights of Woman\" thus engages not only specific events in France and in Britain but also larger questions being raised by political philosophers such as John Locke and Jean-Jacques Rousseau.\nThemes.\nThe \"Rights of Woman\" is a long (almost 87,000 words) essay that introduces all of its major topics in the opening chapters and then repeatedly returns to them, each time from a different point of view. It also adopts a hybrid tone that combines rational argument with the fervent rhetoric of sensibility. Wollstonecraft did not employ the formal argumentation or logical prose style common to eighteenth-century philosophical writing.\nHysteria was once seen as a physical phenomenonphysicians and anatomists believed that the more \"sensitive\" people's \"nerves\", the more emotionally affected they would be by their surroundings. Since women were thought to have keener nerves than men, it was believed that women were more emotional than men. The emotional excess associated with sensibility also theoretically produced an ethic of compassion: those with sensibility could easily sympathise with people in pain. Thus historians have credited the discourse of sensibility and those who promoted it with the increased humanitarian efforts, such as the movement to abolish the slave trade. But sensibility also paralysed those who had too much of it; as scholar G. J. Barker-Benfield explains, \"an innate refinement of nerves was also identifiable with greater suffering, with weakness, and a susceptibility to disorder\".\nBy the time Wollstonecraft was writing the \"Rights of Woman\", sensibility had already been under sustained attack for a number of years. Sensibility, which had initially promised to draw individuals together through sympathy, was now viewed as \"profoundly separatist\"; novels, plays, and poems that employed the language of sensibility asserted individual rights, sexual freedom, and unconventional familial relationships based only upon feeling. Furthermore, as Janet Todd, another scholar of sensibility, argues, \"to many in Britain the cult of sensibility seemed to have feminized the nation, given women undue prominence, and emasculated men\".\nRational education.\nOne of Wollstonecraft's central arguments in the \"Rights of Woman\" is that women should be educated in a rational manner to give them the opportunity to contribute to society. In the eighteenth century, it was often assumed by educational philosophers and conduct book writers, who wrote what one might think of as early self-help books, that women were incapable of rational or abstract thought. Women, it was believed, were too susceptible to sensibility and too fragile to be able to think clearly. Wollstonecraft, along with other female reformers such as Catharine Macaulay and Hester Chapone, maintained that women were indeed capable of rational thought and deserved to be educated. She argued this point in her own conduct book, \"Thoughts on the Education of Daughters\" (1787), in her children's book, \"Original Stories from Real Life\" (1788), as well as in the \"Rights of Woman\".\nStating in her preface that \"my main argument is built on this simple principle, that if [woman] be not prepared by education to become the companion of man, she will stop the progress of knowledge and virtue; for truth must be common to all\", Wollstonecraft contends that society will degenerate without educated women, particularly because mothers are the primary educators of young children. She attributes the problem of uneducated women to men and \"a false system of education, gathered from the books written on this subject by men who [consider] females rather as women than human creatures\". Women are capable of rationality; it only appears that they are not, because men have refused to educate them and encouraged them to be frivolous (Wollstonecraft describes silly women as \"spaniels\" and \"toys\").\nWollstonecraft attacks conduct book writers such as James Fordyce and John Gregory as well as educational philosophers such as Jean-Jacques Rousseau who argue that a woman does not need a rational education. (Rousseau argues in \"\" [1762] that women should be educated for the pleasure of men; Wollstonecraft, infuriated by this argument, attacks not only it but also Rousseau himself.) Intent on illustrating the limitations that contemporary educational theory placed upon women, Wollstonecraft writes, \"taught from their infancy that beauty is woman's sceptre, the mind shapes itself to the body, and, roaming round its gilt cage, only seeks to adorn its prison\", implying that without this damaging ideology, which encourages young women to focus their attention on beauty and outward accomplishments, they could achieve much more. Wives could be the rational \"companions\" of their husbands and even pursue careers should they so choose: \"women might certainly study the art of healing, and be physicians as well as nurses. And midwifery, decency seems to allot to them... they might, also, study politics... Business of various kinds, they might likewise pursue.\"\nFor Wollstonecraft, \"the most perfect education\" is \"an exercise of the understanding as is best calculated to strengthen the body and form the heart. Or, in other words, to enable the individual to attach such habits of virtue as will render it independent.\" In addition to her broad philosophical arguments, Wollstonecraft lays out a specific plan for national education to counter Talleyrand's. In Chapter 12, \"On National Education\", she proposes that children be sent to free day schools as well as given some education at home \"to inspire a love of home and domestic pleasures\". She also maintains that schooling should be co-educational, contending that men and women, whose marriages are \"the cement of society\", should be \"educated after the same model\".\nFeminism.\nIt is debatable to what extent the \"Rights of Woman\" is a feminist text; because the definitions of \"feminist\" vary, different scholars have come to different conclusions. The words \"feminist\" and \"feminism\" were not coined until the 1890s, and there was no feminist movement to speak of during Wollstonecraft's lifetime. \"Rights of Woman\" is often considered the source or original, \"the ur-document of modern liberal feminism\". In the introduction to her work on Wollstonecraft's thought, Barbara Taylor writes:\nDescribing [Wollstonecraft's philosophy] as feminist is problematic, and I do it only after much consideration. The label is of course anachronistic... Treating Wollstonecraft's thought as an anticipation of nineteenth and twentieth-century feminist argument has meant sacrificing or distorting some of its key elements. Leading examples of this... have been the widespread neglect of her religious beliefs, and the misrepresentation of her as a bourgeois liberal, which together have resulted in the displacement of a religiously inspired utopian radicalism by a secular, class-partisan reformism as alien to Wollstonecraft's political project as her dream of a divinely promised age of universal happiness is to our own. Even more important however has been the imposition on Wollstonecraft of a heroic-individualist brand of politics utterly at odds with her own ethically driven case for women's emancipation. Wollstonecraft's leading ambition for women was that they should attain virtue, and it was to this end that she sought their liberation.\nIn the \"Rights of Woman\", Wollstonecraft does not make the claim for gender equality using the same arguments or the same language that late nineteenth- and twentieth-century feminists later would. For instance, rather than unequivocally stating that men and women are equal, Wollstonecraft contends that men and women are equal in the eyes of God, which means that they are both subject to the same moral law. For Wollstonecraft, men and women are equal in the most important areas of life. While such an idea may not seem revolutionary to twenty-first-century readers, its implications were revolutionary during the eighteenth century. For example, it implied that both men and womennot just womenshould be modest and respect the sanctity of marriage. Wollstonecraft's argument exposed the sexual double standard of the late eighteenth century and demanded that men adhere to the same virtues demanded of women.\nHowever, Wollstonecraft's arguments for equality stand in contrast to her statements respecting the superiority of masculine strength and valour. Wollstonecraft states:\nLet it not be concluded, that I wish to invert the order of things; I have already granted, that, from the constitution of their bodies, men seem to be designed by Providence to attain a greater degree of virtue. I speak collectively of the whole sex; but I see not the shadow of a reason to conclude that their virtues should differ in respect to their nature. In fact, how can they, if virtue has only one eternal standard? I must therefore, if I reason consequentially, as strenuously maintain that they have the same simple direction, as that there is a God.\nWollstonecraft calls on men, rather than women, to initiate the social and political changes she outlines in the \"Rights of Woman\". Because women are uneducated, they cannot alter their own situationmen must come to their aid. Wollstonecraft writes at the end of her chapter \"Of the Pernicious Effects Which Arise from the Unnatural Distinctions Established in Society\":\nI then would fain convince reasonable men of the importance of some of my remarks; and prevail on them to weigh dispassionately the whole tenor of my observations... I appeal to their understandings; and, as a fellow-creature, claim, in the name of my sex, some interest in their hearts. I entreat them to assist to emancipate their companion, to make her a help meet for them! Would men but generously snap our chains, and be content with rational fellowship instead of slavish obedience, they would find us more observant daughters, more affectionate sisters, more faithful wives, more reasonable mothersin a word, better citizens.\nWollstonecraft's last novel, \"\" (1798), the fictionalized sequel to the \"Rights of Woman\", is usually considered her most radical feminist work.\nSensibility.\nOne of Wollstonecraft's most scathing criticisms in the \"Rights of Woman\" is against false and excessive sensibility, particularly in women. She argues that women who succumb to sensibility are \"blown about by every momentary gust of feeling\"; because these women are \"the prey of their senses\", they cannot think rationally. Not only do they do harm to themselves but they also do harm to all of civilization: these are not women who can refine civilizationthese are women who will destroy it. But reason and feeling are not independent for Wollstonecraft; rather, she believes that they should inform each other. For Wollstonecraft the passions underpin all reason. This was a theme that she would return to throughout her career, but particularly in her novels ' (1788) and '. For the eighteenth-century Scottish philosopher David Hume, reason is dominated by the passions. He held that passions rather than reason govern human behaviour, famously proclaiming in \"A Treatise of Human Nature\" that \"Reason is, and ought only to be the slave of the passions\".\nAs part of her argument that women should not be overly influenced by their feelings and emotions, Wollstonecraft emphasises that they should not be constrained by or made slaves to their bodies or their sexual feelings. This particular argument has led many modern feminists to suggest that Wollstonecraft intentionally avoids granting women any sexual desire. Cora Kaplan argues that the \"negative and prescriptive assault on female sexuality\" is a leitmotif of the \"Rights of Woman\". For example, Wollstonecraft advises her readers to \"calmly let passion subside into friendship\" in the ideal companionate marriage (that is, in the ideal of a love-based marriage that was developing at the time). It would be better, she writes, when \"two virtuous young people marry . . . if some circumstances checked their passion\". According to Wollstonecraft, \"love and friendship cannot subsist in the same bosom\". As Mary Poovey explains, \"Wollstonecraft betrays her fear that female desire might in fact court man's lascivious and degrading attentions, that the subordinate position women have been given might even be deserved. Until women can transcend their fleshly desires and fleshly forms, they will be hostage to the body.\" If women are not interested in sexuality, they cannot be dominated by men. Wollstonecraft worries that women are consumed with \"romantic wavering\", that is, they are interested only in satisfying their lusts. Because the \"Rights of Woman\" eliminates sexuality from a woman's life, Kaplan contends, it \"expresses a violent antagonism to the sexual\" while at the same time \"exaggerat[ing] the importance of the sensual in the everyday life of women\". Wollstonecraft was so determined to wipe sexuality from her picture of the ideal woman that she ended up foregrounding it by insisting upon its absence. But as Kaplan and others have remarked, Wollstonecraft may have been forced to make this sacrifice: \"it is important to remember that the notion of woman as politically enabled and independent [was] fatally linked [during the eighteenth century] to the unrestrained and vicious exercise of her sexuality.\"\nRepublicanism.\nClaudia Johnson, a prominent Wollstonecraft scholar, has called the \"Rights of Woman\" \"a republican manifesto\". Johnson contends that Wollstonecraft is hearkening back to the Commonwealth tradition of the seventeenth century and attempting to reestablish a republican ethos. In Wollstonecraft's version, there would be strong, but separate, masculine and feminine roles for citizens. According to Johnson, Wollstonecraft \"denounces the collapse of proper sexual distinction as the leading feature of her age, and as the grievous consequence of sentimentality itself. The problem undermining society in her view is feminized men\". If men feel free to adopt both the masculine position and the sentimental feminine position, she argues, women have no position open to them in society. Johnson therefore sees Wollstonecraft as a critic, in both the \"Rights of Men\" and the \"Rights of Woman\", of the \"masculinization of sensitivity\" in such works as Edmund Burke's \"Reflections on the Revolution in France\".\nIn the \"Rights of Woman\" Wollstonecraft adheres to a version of republicanism that includes a belief in the eventual overthrow of all titles, including the monarchy. She also suggests that all men and women should be represented in government. But the bulk of her \"political criticism\", as Chris Jones, a Wollstonecraft scholar, explains, \"is couched predominantly in terms of morality\". Her definition of virtue focuses on the individual's happiness rather than, for example, the good of society. This is reflected in her explanation of natural rights. Because rights ultimately proceed from God, Wollstonecraft maintains that there are duties, tied to those rights, incumbent upon each and every person. For Wollstonecraft, the individual is taught republicanism and benevolence within the family; domestic relations and familial ties are crucial to her understanding of social cohesion and patriotism.\nClass.\nIn many ways the \"Rights of Woman\" is inflected by a bourgeois view of the world, as is its direct predecessor the \"Rights of Men\". Wollstonecraft addresses her text to the middle class, which she calls the \"most natural state\". She also frequently praises modesty and industry, virtues which, at the time, were associated with the middle class. From her position as a middle-class writer arguing for a middle-class ethos, Wollstonecraft also attacks the wealthy, criticizing them using the same arguments she employs against women. She points out the \"false-refinement, immorality, and vanity\" of the rich, calling them \"weak, artificial beings, raised above the common wants and affections of their race, in a premature unnatural manner [who] undermine the very foundation of virtue, and spread corruption through the whole mass of society\".\nBut Wollstonecraft's criticisms of the wealthy do not necessarily reflect a concomitant sympathy for the poor. For her, the poor are fortunate because they will never be trapped by the snares of wealth: \"Happy is it when people have the cares of life to struggle with; for these struggles prevent their becoming a prey to enervating vices, merely from idleness!\" She contends that charity has only negative consequences because, as Jones puts it, she \"sees it as sustaining an unequal society while giving the appearance of virtue to the rich\".\nIn her national plan for education, she retains class distinctions (with an exception for the intelligent), suggesting that: \"After the age of nine, girls and boys, intended for domestic employments, or mechanical trades, ought to be removed to other schools, and receive instruction, in some measure appropriated to the destination of each individual... The young people of superior abilities, or fortune, might now be taught, in another school, the dead and living languages, the elements of science, and continue the study of history and politics, on a more extensive scale, which would not exclude polite literature.\"\nRhetoric and style.\nIn attempting to navigate the cultural expectations of female writers and the generic conventions of political and philosophical discourse, Wollstonecraft, as she does throughout her \"oeuvre\", constructs a unique blend of masculine and feminine styles in the \"Rights of Woman\". She uses the language of philosophy, referring to her work as a \"treatise\" with \"arguments\" and \"principles\". However, Wollstonecraft also uses a personal tone, employing \"I\" and \"you\", dashes and exclamation marks, and autobiographical references to create a distinctly feminine voice in the text. The \"Rights of Woman\" further hybridizes its genre by weaving together elements of the conduct book, the short essay, and the novel, genres often associated with women, while at the same time claiming that these genres could be used to discuss philosophical topics such as rights.\nAlthough Wollstonecraft argues against excessive sensibility, the rhetoric of the \"Rights of Woman\" is at times heated and attempts to provoke the reader. Many of the most emotional comments in the book are directed at Rousseau. For example, after excerpting a long passage from \"\" (1762), Wollstonecraft pithily states, \"I shall make no other comments on this ingenious passage, than just to observe, that it is the philosophy of lasciviousness.\" A mere page later, after indicting Rousseau's plan for female education, she writes \"I must relieve myself by drawing another picture.\" These terse exclamations are meant to draw the reader to her side of the argument (it is assumed that the reader will agree with them). While she claims to write in a plain style so that her ideas will reach the broadest possible audience, she actually combines the plain, rational language of the political treatise with the poetic, passionate language of sensibility to demonstrate that one can combine rationality and sensibility in the same self.\nIn her efforts to vividly describe the condition of women within society, Wollstonecraft employs several different analogies. She often compares women to slaves, arguing that their ignorance and powerlessness places them in that position. But at the same time, she also compares them to \"capricious tyrants\" who use cunning and deceit to manipulate the men around them. At one point, she reasons that a woman can become either a slave or tyrant, which she describes as two sides of the same coin. Wollstonecraft also compares women to soldiers; like military men, they are valued only for their appearance and obedience. And like the rich, women's \"softness\" has \"debased mankind\".\nRevision.\nWollstonecraft was forced to write the \"Rights of Woman\" hurriedly to respond to Talleyrand and ongoing events. Upon completing the work, she wrote to her friend William Roscoe: \"I am dissatisfied with myself for not having done justice to the subject... Do not suspect me of false modestyI mean to say that had I allowed myself more time I could have written a better book, in every sense of the word... I intend to finish the next volume before I begin to print, for it is not pleasant to have the Devil coming for the conclusion of a sheet fore it is written.\" When Wollstonecraft revised the \"Rights of Woman\" for the second edition, she took the opportunity not only to fix small spelling and grammar mistakes but also to bolster the feminist claims of her argument. She changed some of her statements regarding female and male difference to reflect a greater equality between the sexes.\nWollstonecraft never wrote the second part to the \"Rights of Woman,\" although William Godwin published her \"Hints\", which were \"chiefly designed to have been incorporated in the second part of the \"Vindication of the Rights of Woman\", in the posthumous collection of her works. However, she did begin writing the novel \", which most scholars consider a fictionalized sequel to the \"Rights of Woman\". It was unfinished at her death and also included in the \"Posthumous Works\" published by Godwin.\nReception and legacy.\nWhen it was first published in 1792, the \"Rights of Woman\" was reviewed favourably by the \"Analytical Review\", the \"General Magazine\", the \"Literary Magazine\", \"New York Magazine\", and the \"Monthly Review\", although the assumption persists that \"Rights of Woman\" received hostile reviews. It was almost immediately released in a second edition in 1792, several American editions appeared, and it was translated into French. Taylor writes that \"it was an immediate success\". Moreover, other writers such as Mary Hays and Mary Robinson specifically alluded to Wollstonecraft's text in their own works. Hays cited the \"Rights of Woman\" in her novel \"Memoirs of Emma Courtney\" (1796) and modelled her female characters after Wollstonecraft's ideal woman.\nAlthough female conservatives such as Hannah More excoriated Wollstonecraft personally, they actually shared many of the same values. As the scholar Anne Mellor has shown, both More and Wollstonecraft wanted a society founded on \"Christian virtues of rational benevolence, honesty, personal virtue, the fulfillment of social duty, thrift, sobriety, and hard work\". During the early 1790s, many writers within British society were engaged in an intense debate regarding the position of women in society. For example, the respected poet and essayist Anna Laetitia Barbauld and Wollstonecraft sparred back and forth; Barbauld published several poems responding to Wollstonecraft's work and Wollstonecraft commented on them in footnotes to the \"Rights of Woman\". The work also provoked outright hostility. The bluestocking Elizabeth Carter was unimpressed with the work. Thomas Taylor, the Neoplatonist translator who had been a landlord to the Wollstonecraft family in the late 1770s, swiftly wrote a satire called \"A Vindication of the Rights of Brutes\": if women have rights, why not animals too?\nAfter Wollstonecraft died in 1797, her husband William Godwin published his \"Memoirs of the Author of A Vindication of the Rights of Woman\" (1798). He revealed much about her private life that had previously not been known to the public: her illegitimate child, her love affairs, and her attempts at suicide. While Godwin believed he was portraying his wife with love, sincerity, and compassion, contemporary readers were shocked by Wollstonecraft's unorthodox lifestyle and she became a reviled figure. Richard Polwhele targeted her in particular in his anonymous long poem \"The Unsex'd Females\" (1798), a defensive reaction to women's literary self-assertion: Hannah More is Christ to Wollstonecraft's Satan. His poem was \"well known\" among the responses to \"A Vindication\".\nWollstonecraft's ideas became associated with her life story and women writers felt that it was dangerous to mention her in their texts. Hays, who had previously been a close friend and an outspoken advocate for Wollstonecraft and her \"Rights of Woman\", for example, did not include her in the collection of \"Illustrious and Celebrated Women\" she published in 1803. Maria Edgeworth specifically distances herself from Wollstonecraft in her novel \"Belinda\" (1802); she caricatures Wollstonecraft as a radical feminist in the character of Harriet Freke. But, like Jane Austen, she does not reject Wollstonecraft's ideas. Both Edgeworth and Austen argue that women are crucial to the development of the nation; moreover, they portray women as rational beings who should choose companionate marriage.\nThe negative views towards Wollstonecraft persisted for over a century. The \"Rights of Woman\" was not reprinted until the middle of the nineteenth century and it still retained an aura of ill-repute. George Eliot wrote \"there is in some quarters a vague prejudice against the \"Rights of Woman\" as in some way or other a reprehensible book, but readers who go to it with this impression will be surprised to find it eminently serious, severely moral, and withal rather heavy\". The suffragist (i.e. moderate reformer, as opposed to suffragette) Millicent Garrett Fawcett wrote the introduction to the centenary edition of the \"Rights of Woman\", cleansing the memory of Wollstonecraft and claiming her as the foremother of the struggle for the vote. While the \"Rights of Woman\" may have paved the way for feminist arguments, twentieth-century feminists have tended to use Wollstonecraft's life story, rather than her texts, for inspiration; her unorthodox lifestyle convinced them to try new \"experiments in living\", as Virginia Woolf termed it in her famous essay on Wollstonecraft. However, there is some evidence that the \"Rights of Woman\" may be influencing current feminists. Ayaan Hirsi Ali, a feminist who is critical of Islam's dictates regarding women, cites the \"Rights of Woman\" in her autobiography \"Infidel\", writing that she was \"inspired by Mary Wollstonecraft, the pioneering feminist thinker who told women they had the same ability to reason as men did and deserved the same rights\". Miriam Schneir also includes this text in her anthology \"\", labelling it as one of the essential feminist works. Further evidence of the enduring legacy of Wollstonecraft's \"A Vindication\" may be seen by direct references in recent historical fiction set: for example, in \"The Silk Weaver\" (1998) set in the late eighteenth century among Dublin silk weavers, author Gabrielle Warnock (1998) intervenes as narrator to hold up \u2018Rights of Woman\u2019 for the reader to reflect upon the politics, morals, and feelings of her female characters. In \"Death Comes to Pemberley\" (2011), set in 1803, P. D. James has one male character reference \"Rights of Woman\" in reproving another (Darcy) for denying voice to the woman in matters that concern her."}
{"id": "2835", "revid": "46807820", "url": "https://en.wikipedia.org/wiki?curid=2835", "title": "Afghan Hound", "text": "The Afghan Hound is a hound distinguished by its thick, fine, silky coat, and a tail with a ring curl at the end. The breed originates in the cold mountains of Afghanistan. Its local name is () or (). Other names for this breed are T\u0101z\u012b, Balkh Hound, Baluchi Hound, and Barakzai Hound.\nThe American Kennel Club (AKC) describes the breed as among the most eye-catching of all. The Afghan Hound is an \"aloof and dignified aristocrat of sublime beauty.\" Despite their regal appearance, the Afghan possesses an \"endearing streak of silliness and a profound loyalty.\"\nAdmired since ancient times for their beauty, the Afghan Hound's distinctive coat was developed as protection from the harsh mountain climate. Their huge paw-pads served as shock absorbers on the rocky terrain.\nHistory.\nThe Afghan Hound has been identified as a basal breed that predates the emergence of the modern breeds in the 19th century. It is most closely related to the Saluki.\nConnections with other types and breeds from the same area may provide clues to the history. A name for a desert coursing Afghan Hound, Tazi (Sag-e-Tazi), suggests a shared ancestry with the very similar Tazy breed from the Caspian Sea area of Russia and Turkmenistan. Other types or breeds of similar appearance are the Taigan from the mountainous Tian Shan region on the Chinese border of Afghanistan, and the Barakzay, or Kurram Valley Hound.\nOnce out of Afghanistan, the history of the Afghan Hound breed became entwined with that of the very earliest dog shows and the Kennel Club (UK). Various sighthounds were brought to England in the 1800s by army officers returning from British India and were exhibited at dog shows, which were then just becoming popular, under various names, such as Barukzy hounds. They were also called \"Persian Greyhounds\" by the English, in reference to their own indigenous sighthound.\nOne dog in particular, Zardin, was brought in 1907 from India by Captain John Barff. Zardin became the early ideal for the breed type still referred to as the Persian Greyhound. Zardin was the basis of the writing for the first breed standard in 1912, but this breeding cycle was stopped by World War I.\nOut of the longhaired sighthound types known in Afghanistan, two main strains make up the modern Afghan Hound breed. The first were a group of hounds brought to Scotland from Balochistan by Major and Mrs. G. Bell-Murray and Miss Jean C. Manson in 1920, and they are known as the Bell-Murray strain. These dogs were of the \"steppe\" or \u201cdesert\u201d type and were less heavily coated.\nThe second strain was a group of dogs from a kennel in Kabul owned by Mrs. Mary Amps, which she shipped to England in 1925. She and her husband came to Kabul after the Afghan war in 1919, and the foundation sire of her kennel (named Ghazni) in Kabul was a dog that closely resembled Zardin. Her Ghazni strain were the more heavily coated mountain type. Most of the Afghans in the United States were developed from the Ghazni strain from England. The first Afghans in Australia were imported from the United States in 1934, also of the Ghazni strain. The mountain and steppe strains became mixed into the modern Afghan Hound breed, and a new standard was written in 1948, which is still used today.\nThe Afghan Hound can also come with a much more \"patterned\" coat. This descends from the Bell-Murray's and the Ghazni lines, and is displayed in much lighter feathering of coat, deeper saddle (often actually looking like a saddle) and much shorter hair on the face and neck. It is believed that these particular Afghan Hounds were a product of much hotter parts of the country.\nThe beauty of Afghan Hound dogs caused them to become highly desirable show dogs and pets, and they are recognised by all of the major kennel clubs in the English-speaking world. One of the Amps Ghazni, Sirdar, won best in show at Crufts in 1928 and 1930. An Afghan Hound was featured on the cover of Life Magazine on November 26, 1945. Afghan Hounds were the most popular in Australia in the 1970s, and won most of the major shows. An Afghan Hound won Best in Show (BIS) at the 1996 World Dog Show in Budapest. Afghan Hounds were BIS at the Westminster Kennel Club Dog Show in 1957 and again in 1983.\nThe Afghan Hound breed is no longer used for hunting, although it can be seen in the sport of lure coursing.\nOn August 3, 2005, Korean scientist Hwang Woo-Suk announced that his team of researchers had become the first team to successfully clone a dog, an Afghan Hound named Snuppy. In 2006 Hwang Woo-Suk was dismissed from his university position for fabricating data in his research. Snuppy, nonetheless, was a genuine clone, and thus the first cloned dog in history.\nDescription.\nThe dogs in this breed occur in many different coat colors. A study that mapped the genes of Afghan Hounds and discussed the effect of genes on coat colour in the breed was published in the Journal of Heredity in 2010.\nThe Afghan Hound is tall, standing in height and weighing . The coat may be any colour, but white markings, particularly on the head, are discouraged; many individuals have a black facial mask. A specimen may have a beard on the lower jaw, known as a \"mandarin\". Some Afghan Hounds are almost white, but parti-color hounds (white with islands of red or black) are penalized in the AKC standard, but not by the FCI.\nTheir long, fine-textured coat requires considerable care and grooming. The long topknot and the shorter-haired saddle on the back of the dog are distinctive features of the Afghan Hound coat. The high hipbones and unique small ring on the end of the tail are also characteristics of the breed.\nThe temperament of the typical Afghan Hound can be aloof and dignified, but happy and clownish when playing. This breed, as tends to be the case with sighthounds, has a high prey drive and may kill small animals and livestock. Genomic studies have pointed to the Afghan Hound as one of the oldest of dog breeds.\nThe breed has a reputation among dog trainers of having a relatively slow \"obedience intelligence\"; Stanley Coren, in his book \"The Intelligence of Dogs\", ranked the breed last among 138 breeds mentioned in ability to understand and obey commands, requiring more than 80 repetitions to understand a new command and obeying on the first command less than 25% of the time. Coren noted that Afghan Hounds were consistently ranked among the least obedient dog breeds among all of the trainers he consulted, with a majority (121 out of 199) ranking the Afghan Hound in the lowest ten breeds out of 133 listed.\nVariants.\nKhalag Tazi.\nThe Khalag Tazi is a variety of the Afghan Hound introduced to Europe in 1920, when an Indian Army officer, Major G Bell-Murray, brought some animals back from Afghanistan.\nBakhmull.\nBakhmull (also Bakhmull Tazi or Tazi Bakhmull, also called the Aboriginal Afghan Hound) is a long-haired variety of sighthound. It has been bred mostly in Russia and claimed to represent an Afghan Hound aboriginal to Afghanistan. In Pashto the word means \"velvet\", applied in reference to the dog's silky coat, which is rather abundant and long on the whole body, except the \"saddle\" (middle to lower back), front parts of all four legs, and the muzzle. Its color is always fawn, ivory, or white, with a darker \"saddle\", thus it produces an impression of a (yellowish) dog whose coat color matches the khaki sandstone and limestone of the Hindu Kush mountain landscape and deserts. The following colors are not permissible: red, red with white spots, black, and black with white spots.\nSince the 1980s, the centre of Bakhmull breeding has been Russia, beginning in Moscow, then spreading to various other places in the CIS. The foundation stock was brought to Russia in the 1970s by military men returning from Afghanistan. Natalia Gherasiova (a breeder, of the Blue Dale el Bark Bakhmull kennel, and dog show judge) established the National Bakhmull Club, affiliated with the Russian Federation for Hunting Dogs (RFOS) and Russian Kynological Federation (RKF). A breed standard was first published in 1985, and a shared RFOS\u2013RKF revision was produced in 1997.\nBakhmulls hunt solo and in couples. Although its coat is long, it does not require much grooming. Paws are well protected from injuries by \"feathering\" (thick additional paw fur). Its long, velvety coat and its stamina makes more suitable than many breeds for harsh weather. The breed standard calls for \"aristocratic gait and a beautiful head with gazelle-like ... eyes\". The eyes should be large, brown, slanting upwards, and of almond shape, with rims outlined black. Black coloration is required on the nose and lips for both white and fawn bakhmulls. The dog's height should be between , for bitches. The height at the withers is higher than at the croup.\nHealth.\nLifespan.\nA UK study found a life expectancy of 11.1 years for the breed compared to an average of 12.7 for purebreeds and 12 for crossbreeds.\nHealth concerns.\nMajor health issues are allergies, cancer, and hip dysplasia. Like other sighthounds, the Afghan Hound is sensitive to anesthesia, as sighthounds have relatively low levels of body fat. Afghan Hounds are also among the dog breeds most likely to develop chylothorax, a rare condition which causes the thoracic ducts to leak, allowing large quantities of chyle fluid to enter the dog's chest cavity. This condition commonly results in a lung-lobe torsion (in which the dog's lung twists within the chest cavity, requiring emergency surgery), due to the breed's typically deep, \"barrel\"-shaped chest. If not corrected through surgery, chylothorax can ultimately cause fibrosing pleuritis, or a hardening of the organs, due to scar tissue forming around the organs to protect them from the chyle fluid. Chylothorax is often fatal.\nAmong other health problems are laryngeal paralysis, dilated cardiomyopathy (twice as common in males as females), and dermatological issues such as testosterone-responsive dermatosis of male dogs (often seen in castrated males), nasal depigmentation (also known as Dudley nose), and skin tumours. Afghans are also prone to Central diabetes insipidus (CDI), hypothyroidism and tricholemmoma, a rare condition which mainly affects older dogs in the Middle Ages. Ocular conditions that can occur include medial canthal pocket syndrome (breed predisposition due to shape of head), corneal dystrophy, cataract and generalized progressive retinal atrophy (GPRA). Afghan myelopathy (causing pelvic limb ataxia) is sometimes reported.\nIn popular culture.\nPablo Picasso said that his 1967 statue located in Chicago's Daley Plaza represented the head of an Afghan Hound named Kabul.\nThe Afghan hound has been represented in multiple animated feature films and TV shows, including Universal Pictures' \"Balto\" (Sylvie), Disney's \"\" (Ruby), Hasbro Studios's \"Pound Puppies\" (Twiggy) and ABC Kids' \"Bluey\" (Indy). An Afghan hound also appeared in the films \"One Hundred and One Dalmatians,\" \"101 Dalmatians,\" \"102 Dalmatians, and \". Other examples include Prince Amir of Kinjan from \"What-a-Mess,\" Persia from \"Road Rovers,\" Burt from \"Foofur,\" Laila from \"Roadside Romeo,\" and Brainy Barker from \"Krypto the Superdog\". Malory Archer in the show \"Archer\" also had an Afghan hound named Duchess at some point in her childhood.\nIn the 1941 novel \"Between the Acts\", Virginia Woolf uses an Afghan hound named Sohrab to represent aspects of one of the book's human characters.\nThe Afghan Hound features prominently in the avant-garde music video of French band M83's, \"Set in Stone (M83 Remix)\"."}
{"id": "2836", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=2836", "title": "Azawakh", "text": "The Azawakh is a breed of dog from West Africa. With ancient origins, it is raised throughout the Sahelian zone of Mali, Niger, and Burkina Faso. This region includes the Azawagh Valley for which the breed is named. While commonly associated with the nomadic Tuareg people, the dogs are also bred and owned by other ethnic groups, such as the Peulh, Bella, and Hausa. The Azawakh is more related to the Sloughi than it is to the Saluki.\nDescription.\nAppearance.\nSlim and elegant, with bone structure and muscles showing through thin skin. Eyes are almond-shaped.\nThe coat is very short and almost absent on the belly. Its bone structure shows clearly through the skin and musculature. Its muscles are \"dry\", meaning that they are quite flat, unlike the Greyhound and Whippet. In this respect it is similar in type to the Saluki.\nColours.\nColours permitted by the F\u00e9d\u00e9ration Cynologique Internationale (FCI) breed standard are clear sand to dark fawn/brown, red and brindle (with or without a dark mask), with white bib, tail tip, and white on all feet (which can be tips of toes to high stockings). Since 2015 white stockings that go above the elbow joint are considered disqualifying features in the FCI member countries, as is a white collar or half collar (Irish marked).\nSome conservationists support the idea that in Africa, Azawakhs are still found in a variety of colours such as red, blue fawn (that is, with a lilac cast), grizzle, and, rarely, blue and black with various white markings including Irish marked (white collar) and particolour (mostly white). Because of this wide color variation in the native population, the American standard used by the AKC and UKC allows any color combination found in Africa.\nMovement.\nThe Azawakh's light, supple, lissome gait is a notable breed characteristic, as is an upright double suspension gallop.\nTemperament.\nBred by the Tuareg, Fula and various other nomads of the Sahara and sub-Saharan Sahel in the countries of Mali, Niger, Burkina Faso, and southern Algeria, the breed known by the tuaregs as \u201dOska\u201d was used there as a guard dog and to hunt gazelle and hare at speeds up to . The austerity of the Sahel environment has ensured that only the most fit dogs survive and has accentuated the breed's ruggedness and independence. Unlike some other sighthounds, the Azawakh is more of a pack hunter and they bump down the quarry with hindquarters when it has been tired out. In role of a guard dog, if an Azawakh senses danger it will bark to alert the other members of the pack, and they will gather together as a pack under the lead of the alpha dog, then chase off or attack the predator. \nUnlike other sighthounds, the primary function of the Azawakh in its native land is that of a guard dog. It develops an intense bond with its owner, and tend to be reserved with strangers.\nAzawakh have high energy and tremendous endurance. They are excellent training companions for runners. Many Azawakh dislike rain and cold weather.\nAzawakh are pack oriented and form complex social hierarchies. They have tremendous memories and are able to recognize each other after long periods of separation. They can often be found sleeping on top of each other for warmth and companionship.\nBreed history.\nThe breed is relatively uncommon in Europe and North America but there is a growing band of devotees. Azawakh may be registered with the FCI in the USA via the Federaci\u00f3n Can\u00f3fila de Puerto Rico (FCPR). European FCI clubs and the AKC recognize the FCPR as an acceptable registry. The AKC recognized the Azawakh a member of the Hound group in 2019. The American Azawakh Association (AAA) is the AKC Parent Club for the Azawakh. Azawakh may be registered with the UKC and ARBA. The breed is not yet registered by CKC. Azawakh are eligible for ASFA and AKC lure coursing and NOFCA open field coursing events."}
{"id": "2838", "revid": "1264144041", "url": "https://en.wikipedia.org/wiki?curid=2838", "title": "Acrylic paint", "text": "Acrylic paint is a fast-drying paint made of pigment suspended in acrylic polymer emulsion and plasticizers, silicone oils, defoamers, stabilizers, or metal soaps. Most acrylic paints are water-based, but become water-resistant when dry. Depending on how much the paint is diluted with water, or modified with acrylic gels, mediums, or pastes, the finished acrylic painting can resemble a watercolor, a gouache, or an oil painting, or it may have its own unique characteristics not attainable with other media.\nWater-based acrylic paints are used as latex house paints, as latex is the technical term for a suspension of polymer microparticles in water. Interior latex house paints tend to be a combination of binder (sometimes acrylic, vinyl, PVA, and others), filler, pigment, and water. Exterior latex house paints may also be a co-polymer blend, but the best exterior water-based paints are 100% acrylic, because of its elasticity and other factors. Vinyl, however, costs half of what 100% acrylic resins cost, and polyvinyl acetate (PVA) is even cheaper, so paint companies make many different combinations of them to match the market.\nHistory.\nOtto R\u00f6hm invented acrylic resin, which was quickly transformed into acrylic paint. As early as 1934, the first usable acrylic resin dispersion was developed by German chemical company BASF, and patented by Rohm and Haas. The synthetic paint was first used in the 1940s, combining some of the properties of oil and watercolor. Between 1946 and 1949, Leonard Bocour and Sam Golden invented a solution acrylic paint under the brand Magna paint. These were mineral spirit-based paints.\nWater-based acrylic paints were subsequently sold as latex house paints.\nSoon after the water-based acrylic binders were introduced as house paints, artists and companies alike began to explore the potential of the new binders. Diego Rivera, David Alfaro Siqueiros, and Jos\u00e9 Clemente Orozco were the first ones who experimented with acrylic paint. This is because they were very impressed with the durability of the acrylic paint. Because of this, artists and companies alike began to produce Politec Acrylic Artists' Colors in Mexico in 1953. According to \"The Times\" newspaper, Lancelot Ribeiro pioneered the use of acrylic paints in the UK because of his \"increasing impatience\" by the 1960s over the time it took for oil paints to dry, as also its \"lack of brilliance in its colour potential.\" He took to the new synthetic plastic bases that commercial paints were beginning to use and soon got help from manufacturers like ICI, Courtaulds, and Geigy. The companies supplied him samples of their latest paints in quantities that he was using three decades later, according to the paper. Initially, the firms thought the PVA compounds would not be needed in commercially viable quantities. But they quickly recognised the potential demand and \"so Ribeiro became the godfather of generations of artists using acrylics as an alternative to oils.\"\nIn 1956, Jos\u00e9 L. Guti\u00e9rrez produced \"Politec Acrylic Artists' Colors\" in Mexico, and Henry Levison of Cincinnati-based Permanent Pigments Co. produced Liquitex colors. These two product lines were the first acrylic emulsion artists' paints, with modern high-viscosity paints becoming available in the early 1960s. Meanwhile, on the other side of the globe, 1958 saw the inception of Vynol Paints Pty Ltd (now Derivan) in Australia, who started producing a water-based artist acrylic called Vynol Colour, followed by Matisse Acrylics in the 1960s. Following that development, Golden came up with a waterborne acrylic paint called \"Aquatec\". In 1963, George Rowney (part of Daler-Rowney since 1983) was the first manufacturer to introduce artists' acrylic paints in Europe, under the brand name \"Cryla\".\nPainting with acrylics.\nAcrylic painters can modify the appearance, hardness, flexibility, texture, and other characteristics of the paint surface by using acrylic medium or simply by adding water. Watercolor and oil painters also use various mediums, but the range of acrylic mediums is much greater. Acrylics have the ability to bond to many different surfaces, and mediums can be used to modify their binding characteristics. Acrylics can be used on paper, canvas, and a range of other materials; however, their use on engineered woods such as medium-density fiberboard can be problematic because of the porous nature of those surfaces. In these cases, it is recommended that the surface first be sealed with an appropriate sealer. The process of sealing acrylic painting is called varnishing. Artists use removable varnishes over isolation coat to protect paintings from dust, UV, scratches, etc. This process is similar to varnishing an oil painting.\nAcrylics can be applied in thin layers or washes to create effects that resemble watercolors and other water-based mediums. They can also be used to build thick layers of paint \u2014 gel and molding paste are sometimes used to create paintings with relief features. Acrylic paints are also used in hobbies such as trains, cars, houses, DIY projects, and human models. People who make such models use acrylic paint to build facial features on dolls or raised details on other types of models. Wet acrylic paint is easily removed from paintbrushes and skin with water, whereas oil paints require the use of a hydrocarbon.\nAcrylics are the most common paints used in grattage, a surrealist technique that began to be used with the advent of this type of paint. Acrylics are used for this purpose because they easily scrape or peel from a surface.\nPainting techniques.\nAcrylic artists' paints may be thinned with water or acrylic medium and used as washes in the manner of watercolor paints, but unlike watercolor the washes are not rehydratable once dry. For this reason, acrylics do not lend themselves to the color lifting techniques of gum arabic-based watercolor paints. Instead, the paint is applied in layers, sometimes diluting with water or acrylic medium to allow layers underneath to partially show through. Using an acrylic medium gives the paint more of a rich and glossy appearance, whereas using water makes the paint look more like watercolor and have a matte finish.\nAcrylic paints with gloss or matte finishes are common, although a satin (semi-matte) sheen is most common. Some brands exhibit a range of finishes (e.g. heavy-body paints from Golden, Liquitex, Winsor &amp; Newton and Daler-Rowney); Politec acrylics are fully matte. As with oils, pigment amounts and particle size or shape can affect the paint sheen. Matting agents can also be added during manufacture to dull the finish. If desired, the artist can mix different media with their paints and use topcoats or varnishes to alter or unify sheen.\nWhen dry, acrylic paint is generally non-removable from a solid surface if it adheres to the surface. Water or mild solvents do not re-solubilize it, although isopropyl alcohol can lift some fresh paint films off. Toluene and acetone can remove paint films, but they do not lift paint stains very well and are not selective. The use of a solvent to remove paint may result in removal of all of the paint layers (acrylic gesso, et cetera). Oils and warm, soapy water can remove acrylic paint from skin. Acrylic paint can be removed from nonporous plastic surfaces such as miniatures or models using cleaning products such as Dettol (containing chloroxylenol 4.8% v/w).\nAn acrylic sizing should be used to prime canvas in preparation for painting with acrylic paints, to prevent Support Induced Discoloration (SID). Acrylic paint contains surfactants that can pull up discoloration from a raw canvas, especially in transparent glazed or translucent gelled areas. Gesso alone will not stop SID; a sizing must be applied before using a gesso.\nThe viscosity of acrylic can be successfully reduced by using suitable extenders that maintain the integrity of the paint film. There are retarders to slow drying and extend \"workability\" time, and \"flow releases\" to increase color-blending ability.\nProperties.\nGrades.\nCommercial acrylic paints come in two grades by manufacturers:\nDifferences between acrylic and oil paint.\nThe vehicle and binder of oil paints is linseed oil (or another drying oil), whereas acrylic paint has water as the vehicle for an emulsion (suspension) of acrylic polymer, which serves as the binder. Thus, oil paint is said to be \"oil-based\", whereas acrylic paint is \"water-based\" (or sometimes \"water-borne\").\nThe main practical difference between most acrylics and oil paints is the inherent drying time. Oils allow for more time to blend colors and apply even glazes over underpaintings. This slow-drying aspect of oil can be seen as an advantage for certain techniques, but it impedes an artist trying to work quickly. The fast evaporation of water from regular acrylic paint films can be slowed with the use of acrylic retarders. Retarders are generally glycol or glycerin-based additives. The addition of a retarder slows the evaporation rate of the water.\nOil paints may require the use of solvents such as mineral spirits or turpentine to thin the paint and clean up. These solvents generally have some level of toxicity and can be found objectionable. Relatively recently, water-miscible oil paints have been developed for artists' use. Oil paint films can gradually yellow and lose their flexibility over time creating cracks in the paint film; the \"fat over lean\" rule must be observed to ensure its durability.\nOil paint has a higher pigment load than acrylic paint. As linseed oil contains a smaller molecule than acrylic paint, oil paint is able to absorb substantially more pigment. Oil provides a refractive index that is less clear than acrylic dispersions, which imparts a unique \"look and feel\" to the resultant paint film. Not all the pigments of oil paints are available in acrylics and vice versa, as each medium has different chemical sensitivities. Some historical pigments are alkali sensitive, and therefore cannot be made in an acrylic emulsion; others are just too difficult to formulate. Approximate \"hue\" color formulations, that do not contain the historical pigments, are typically offered as substitutes.\nBecause of acrylic paint's more flexible nature and more consistent drying time between layers, an artist does not have to follow the same rules of oil painting, where more medium must be applied to each layer to avoid cracking. It usually takes 10\u201320 minutes for one to two layers of acrylic paint to dry, depending on the brand, quality, and humidity levels of the surrounding environment. Some professional grades of acrylic paint can take 20\u201330 minutes or even more than an hour. Although canvas needs to be properly primed before painting with oils to prevent the paint medium from eventually rotting the canvas, acrylic can be safely applied straight to the canvas. The rapid drying of acrylic paint tends to discourage blending of color and use of wet-in-wet technique as in oil painting. Even though acrylic retarders can slow drying time to several hours, it remains a relatively fast-drying medium and adding too much acrylic retarder can prevent the paint from ever drying properly.\nMeanwhile, acrylic paint is very elastic, which prevents cracking from occurring. Acrylic paint's binder is acrylic polymer emulsion \u2013 as this binder dries, the paint remains flexible.\nAnother difference between oil and acrylic paints is the versatility offered by acrylic paints. Acrylics are very useful in mixed media, allowing the use of pastel (oil and chalk), charcoal and pen (among others) on top of the dried acrylic painted surface. Mixing other bodies into the acrylic is possible\u2014sand, rice, and even pasta may be incorporated in the artwork. Mixing artist or student grade acrylic paint with household acrylic emulsions is possible, allowing the use of premixed tints straight from the tube or tin, and thereby presenting the painter with a vast color range at their disposal. This versatility is also illustrated by the variety of additional artistic uses for acrylics. Specialized acrylics have been manufactured and used for linoblock printing (acrylic block printing ink has been produced by Derivan since the early 1980s), face painting, airbrushing, watercolor-like techniques, and fabric screen printing.\nAnother difference between oil and acrylic paint is the cleanup. Acrylic paint can be cleaned out of a brush with any soap, while oil paint needs a specific type to be sure to get all the oil out of the brushes. Also, it is easier to let a palette with oil paint dry and then scrape the paint off, whereas one can easily clean wet acrylic paint with water.\nDifference between acrylic and watercolor paint.\nThe biggest difference is that acrylic paint is opaque, whereas watercolor paint is translucent in nature. Watercolors take about 5 to 15 minutes to dry while acrylics take about 10 to 20 minutes. In order to change the tone or shade of a watercolor pigment, one changes the percentage of water mixed in to the color. For brighter colors, one adds more water. For darker colors, one adds less water. In order to create lighter or darker colors with acrylic paints, one adds white or black.\nAnother difference is that watercolors must be painted onto a porous surface, primarily watercolor paper. Acrylic paints can be used on many different surfaces.\nBoth acrylic and watercolor are easy to clean up with water. Acrylic paint should be cleaned with soap and water immediately following use. Watercolor paint can be cleaned with just water."}
{"id": "2839", "revid": "139266", "url": "https://en.wikipedia.org/wiki?curid=2839", "title": "Angular momentum", "text": "Angular momentum (sometimes called moment of momentum or rotational momentum) is the rotational analog of linear momentum. It is an important physical quantity because it is a conserved quantity\u00a0\u2013 the total angular momentum of a closed system remains constant. Angular momentum has both a direction and a magnitude, and both are conserved. Bicycles and motorcycles, flying discs, rifled bullets, and gyroscopes owe their useful properties to conservation of angular momentum. Conservation of angular momentum is also why hurricanes form spirals and neutron stars have high rotational rates. In general, conservation limits the possible motion of a system, but it does not uniquely determine it.\nThe three-dimensional angular momentum for a point particle is classically represented as a pseudovector , the cross product of the particle's position vector (relative to some origin) and its momentum vector; the latter is in Newtonian mechanics. Unlike linear momentum, angular momentum depends on where this origin is chosen, since the particle's position is measured from it.\nAngular momentum is an extensive quantity; that is, the total angular momentum of any composite system is the sum of the angular momenta of its constituent parts. For a continuous\u00a0rigid body or a fluid, the total angular momentum is the volume integral of angular momentum density (angular momentum per unit volume in the limit as volume shrinks to zero) over the entire body.\nSimilar to conservation of linear momentum, where it is conserved if there is no external force, angular momentum is conserved if there is no external torque. Torque can be defined as the rate of change of angular momentum, analogous to force. The net \"external\" torque on any system is always equal to the \"total\" torque on the system; the sum of all internal torques of any system is always 0 (this is the rotational analogue of Newton's third law of motion). Therefore, for a closed system (where there is no net external torque), the \"total\" torque on the system must be 0, which means that the total angular momentum of the system is constant.\nThe change in angular momentum for a particular interaction is called angular impulse, sometimes twirl. Angular impulse is the angular analog of (linear) impulse.\nExamples.\nThe trivial case of the angular momentum formula_1 of a body in an orbit is given by\nformula_2\nwhere formula_3 is the mass of the orbiting object, formula_4 is the orbit's frequency and formula_5 is the orbit's radius.\nThe angular momentum formula_1 of a uniform rigid sphere rotating around its axis, instead, is given by\nformula_7\nwhere formula_3 is the sphere's mass, formula_4 is the frequency of rotation and formula_5 is the sphere's radius.\nThus, for example, the orbital angular momentum of the Earth with respect to the Sun is about 2.66 \u00d7 1040 kg\u22c5m2\u22c5s\u22121, while its rotational angular momentum is about 7.05 \u00d7 1033 kg\u22c5m2\u22c5s\u22121.\nIn the case of a uniform rigid sphere rotating around its axis, if, instead of its mass, its density is known, the angular momentum formula_1 is given by\nformula_12\nwhere formula_13 is the sphere's density, formula_4 is the frequency of rotation and formula_5 is the sphere's radius.\nIn the simplest case of a spinning disk, the angular momentum formula_1 is given by\nformula_17\nwhere formula_3 is the disk's mass, formula_4 is the frequency of rotation and formula_5 is the disk's radius.\nIf instead the disk rotates about its diameter (e.g. coin toss), its angular momentum formula_1 is given by\nformula_22\nDefinition in classical mechanics.\nJust as for angular velocity, there are two special types of angular momentum of an object: the spin angular momentum is the angular momentum about the object's centre of mass, while the orbital angular momentum is the angular momentum about a chosen center of rotation. The Earth has an orbital angular momentum by nature of revolving around the Sun, and a spin angular momentum by nature of its daily rotation around the polar axis. The total angular momentum is the sum of the spin and orbital angular momenta. In the case of the Earth the primary conserved quantity is the total angular momentum of the solar system because angular momentum is exchanged to a small but important extent among the planets and the Sun. The orbital angular momentum vector of a point particle is always parallel and directly proportional to its orbital angular velocity vector \u03c9, where the constant of proportionality depends on both the mass of the particle and its distance from origin. The spin angular momentum vector of a rigid body is proportional but not always parallel to the spin angular velocity vector \u03a9, making the constant of proportionality a second-rank tensor rather than a scalar.\nOrbital angular momentum in two dimensions.\nAngular momentum is a vector quantity (more precisely, a pseudovector) that represents the product of a body's rotational inertia and rotational velocity (in radians/sec) about a particular axis. However, if the particle's trajectory lies in a single plane, it is sufficient to discard the vector nature of angular momentum, and treat it as a scalar (more precisely, a pseudoscalar). Angular momentum can be considered a rotational analog of linear momentum. Thus, where linear momentum is proportional to mass and linear speed \nformula_23\nangular momentum is proportional to moment of inertia and angular speed measured in radians per second.\nformula_24\nUnlike mass, which depends only on amount of matter, moment of inertia depends also on the position of the axis of rotation and the distribution of the matter. Unlike linear velocity, which does not depend upon the choice of origin, orbital angular velocity is always measured with respect to a fixed origin. Therefore, strictly speaking, should be referred to as the angular momentum \"relative to that center\".\nIn the case of circular motion of a single particle, we can use formula_25 and formula_26 to expand angular momentum as formula_27 reducing to:\nformula_28\nthe product of the radius of rotation and the linear momentum of the particle formula_29, where formula_30 is the linear (tangential) speed.\nThis simple analysis can also apply to non-circular motion if one uses the component of the motion perpendicular to the radius vector:\nformula_31\nwhere formula_32 is the perpendicular component of the motion. Expanding, formula_33 rearranging, formula_34 and reducing, angular momentum can also be expressed,\nformula_35\nwhere formula_36 is the length of the \"moment arm\", a line dropped perpendicularly from the origin onto the path of the particle. It is this definition, , to which the term \"moment of momentum\" refers.\nScalar angular momentum from Lagrangian mechanics.\nAnother approach is to define angular momentum as the conjugate momentum (also called canonical momentum) of the angular coordinate formula_37 expressed in the Lagrangian of the mechanical system. Consider a mechanical system with a mass formula_38 constrained to move in a circle of radius formula_5 in the absence of any external force field. The kinetic energy of the system is\nformula_40\nAnd the potential energy is\nformula_41\nThen the Lagrangian is\nformula_42\nThe \"generalized momentum\" \"canonically conjugate to\" the coordinate formula_37 is defined by\nformula_44\nOrbital angular momentum in three dimensions.\nTo completely define orbital angular momentum in three dimensions, it is required to know the rate at which the position vector sweeps out angle, the direction perpendicular to the instantaneous plane of angular displacement, and the mass involved, as well as how this mass is distributed in space. By retaining this vector nature of angular momentum, the general nature of the equations is also retained, and can describe any sort of three-dimensional motion about the center of rotation \u2013 circular, linear, or otherwise. In vector notation, the orbital angular momentum of a point particle in motion about the origin can be expressed as:\nformula_45\nwhere\nThis can be expanded, reduced, and by the rules of vector algebra, rearranged:\nformula_52\nwhich is the cross product of the position vector formula_48 and the linear momentum formula_54 of the particle. By the definition of the cross product, the formula_55 vector is perpendicular to both formula_48 and formula_57. It is directed perpendicular to the plane of angular displacement, as indicated by the right-hand rule \u2013 so that the angular velocity is seen as counter-clockwise from the head of the vector. Conversely, the formula_55 vector defines the plane in which formula_48 and formula_57 lie.\nBy defining a unit vector formula_61 perpendicular to the plane of angular displacement, a scalar angular speed formula_62 results, where\nformula_63 and\nformula_64 where formula_65 is the perpendicular component of the motion, as above.\nThe two-dimensional scalar equations of the previous section can thus be given direction:\nformula_66\nand formula_67 for circular motion, where all of the motion is perpendicular to the radius formula_5.\nIn the spherical coordinate system the angular momentum vector expresses as\nAnalogy to linear momentum.\nAngular momentum can be described as the rotational analog of linear momentum. Like linear momentum it involves elements of mass and displacement. Unlike linear momentum it also involves elements of position and shape.\nMany problems in physics involve matter in motion about some certain point in space, be it in actual rotation about it, or simply moving past it, where it is desired to know what effect the moving matter has on the point\u2014can it exert energy upon it or perform work about it? Energy, the ability to do work, can be stored in matter by setting it in motion\u2014a combination of its inertia and its displacement. Inertia is measured by its mass, and displacement by its velocity. Their product,\nformula_70\nis the matter's momentum. Referring this momentum to a central point introduces a complication: the momentum is not applied to the point directly. For instance, a particle of matter at the outer edge of a wheel is, in effect, at the end of a lever of the same length as the wheel's radius, its momentum turning the lever about the center point. This imaginary lever is known as the \"moment arm\". It has the effect of multiplying the momentum's effort in proportion to its length, an effect known as a \"moment\". Hence, the particle's momentum referred to a particular point,\nformula_71\nis the \"angular momentum\", sometimes called, as here, the \"moment of momentum\" of the particle versus that particular center point. The equation formula_72 combines a moment (a mass formula_38 turning moment arm formula_5) with a linear (straight-line equivalent) speed formula_75. Linear speed referred to the central point is simply the product of the distance formula_5 and the angular speed formula_62 versus the point: formula_78 another moment. Hence, angular momentum contains a double moment: formula_79 Simplifying slightly, formula_80 the quantity formula_81 is the particle's moment of inertia, sometimes called the second moment of mass. It is a measure of rotational inertia.\nThe above analogy of the translational momentum and rotational momentum can be expressed in vector form:\nThe direction of momentum is related to the direction of the velocity for linear movement. The direction of angular momentum is related to the angular velocity of the rotation.\nBecause moment of inertia is a crucial part of the spin angular momentum, the latter necessarily includes all of the complications of the former, which is calculated by multiplying elementary bits of the mass by the squares of their distances from the center of rotation. Therefore, the total moment of inertia, and \nthe angular momentum, is a complex function of the configuration of the matter about the center of rotation and the orientation of the rotation for the various bits.\nFor a rigid body, for instance a wheel or an asteroid, the orientation of rotation is simply the position of the rotation axis versus the matter of the body. It may or may not pass through the center of mass, or it may lie completely outside of the body. For the same body, angular momentum may take a different value for every possible axis about which rotation may take place. It reaches a minimum when the axis passes through the center of mass.\nFor a collection of objects revolving about a center, for instance all of the bodies of the Solar System, the orientations may be somewhat organized, as is the Solar System, with most of the bodies' axes lying close to the system's axis. Their orientations may also be completely random.\nIn brief, the more mass and the farther it is from the center of rotation (the longer the moment arm), the greater the moment of inertia, and therefore the greater the angular momentum for a given angular velocity. In many cases the moment of inertia, and hence the angular momentum, can be simplified by,\nformula_84where formula_85 is the radius of gyration, the distance from the axis at which the entire mass formula_38 may be considered as concentrated.\nSimilarly, for a point mass formula_38 the moment of inertia is defined as,\nformula_88where formula_5 is the radius of the point mass from the center of rotation, and for any collection of particles formula_90 as the sum,\nformula_91\nAngular momentum's dependence on position and shape is reflected in its units versus linear momentum: kg\u22c5m2/s or N\u22c5m\u22c5s for angular momentum versus kg\u22c5m/s or N\u22c5s for linear momentum. When calculating angular momentum as the product of the moment of inertia times the angular velocity, the angular velocity must be expressed in radians per second, where the radian assumes the dimensionless value of unity. (When performing dimensional analysis, it may be productive to use which treats radians as a base unit, but this is not done in the International system of units). The units if angular momentum can be interpreted as torque\u22c5time. An object with angular momentum of can be reduced to zero angular velocity by an angular impulse of .\nThe plane perpendicular to the axis of angular momentum and passing through the center of mass is sometimes called the \"invariable plane\", because the direction of the axis remains fixed if only the interactions of the bodies within the system, free from outside influences, are considered. One such plane is the invariable plane of the Solar System.\nAngular momentum and torque.\nNewton's second law of motion can be expressed mathematically,\nformula_92\nor force = mass \u00d7 acceleration. The rotational equivalent for point particles may be derived as follows:\nformula_93\nwhich means that the torque (i.e. the time derivative of the angular momentum) is\nformula_94\nBecause the moment of inertia is formula_95, it follows that formula_96, and formula_97 which, reduces to\nformula_98\nThis is the rotational analog of Newton's second law. Note that the torque is not necessarily proportional or parallel to the angular acceleration (as one might expect). The reason for this is that the moment of inertia of a particle can change with time, something that cannot occur for ordinary mass.\nConservation of angular momentum.\nGeneral considerations.\nA rotational analog of Newton's third law of motion might be written, \"In a closed system, no torque can be exerted on any matter without the exertion on some other matter of an equal and opposite torque about the same axis.\" Hence, \"angular momentum can be exchanged between objects in a closed system, but total angular momentum before and after an exchange remains constant (is conserved)\".\nSeen another way, a rotational analogue of Newton's first law of motion might be written, \"A rigid body continues in a state of uniform rotation unless acted upon by an external influence.\" Thus \"with no external influence to act upon it, the original angular momentum of the system remains constant\".\nThe conservation of angular momentum is used in analyzing \"central force motion\". If the net force on some body is directed always toward some point, the \"center\", then there is no torque on the body with respect to the center, as all of the force is directed along the radius vector, and none is perpendicular to the radius. Mathematically, torque formula_99 because in this case formula_48 and formula_101 are parallel vectors. Therefore, the angular momentum of the body about the center is constant. This is the case with gravitational attraction in the orbits of planets and satellites, where the gravitational force is always directed toward the primary body and orbiting bodies conserve angular momentum by exchanging distance and velocity as they move about the primary. Central force motion is also used in the analysis of the Bohr model of the atom.\nFor a planet, angular momentum is distributed between the spin of the planet and its revolution in its orbit, and these are often exchanged by various mechanisms. The conservation of angular momentum in the Earth\u2013Moon system results in the transfer of angular momentum from Earth to Moon, due to tidal torque the Moon exerts on the Earth. This in turn results in the slowing down of the rotation rate of Earth, at about 65.7 nanoseconds per day, and in gradual increase of the radius of Moon's orbit, at about 3.82\u00a0centimeters per year.\nThe conservation of angular momentum explains the angular acceleration of an ice skater as they bring their arms and legs close to the vertical axis of rotation. By bringing part of the mass of their body closer to the axis, they decrease their body's moment of inertia. Because angular momentum is the product of moment of inertia and angular velocity, if the angular momentum remains constant (is conserved), then the angular velocity (rotational speed) of the skater must increase.\nThe same phenomenon results in extremely fast spin of compact stars (like white dwarfs, neutron stars and black holes) when they are formed out of much larger and slower rotating stars.\nConservation is not always a full explanation for the dynamics of a system but is a key constraint. For example, a spinning top is subject to gravitational torque making it lean over and change the angular momentum about the nutation axis, but neglecting friction at the point of spinning contact, it has a conserved angular momentum about its spinning axis, and another about its precession axis. Also, in any planetary system, the planets, star(s), comets, and asteroids can all move in numerous complicated ways, but only so that the angular momentum of the system is conserved.\nNoether's theorem states that every conservation law is associated with a symmetry (invariant) of the underlying physics. The symmetry associated with conservation of angular momentum is rotational invariance. The fact that the physics of a system is unchanged if it is rotated by any angle about an axis implies that angular momentum is conserved.\nRelation to Newton's second law of motion.\nWhile angular momentum total conservation can be understood separately from Newton's laws of motion as stemming from Noether's theorem in systems symmetric under rotations, it can also be understood simply as an efficient method of calculation of results that can also be otherwise arrived at directly from Newton's second law, together with laws governing the forces of nature (such as Newton's third law, Maxwell's equations and Lorentz force). Indeed, given initial conditions of position and velocity for every point, and the forces at such a condition, one may use Newton's second law to calculate the second derivative of position, and solving for this gives full information on the development of the physical system with time. Note, however, that this is no longer true in quantum mechanics, due to the existence of particle spin, which is angular momentum that cannot be described by the cumulative effect of point-like motions in space.\nAs an example, consider decreasing of the moment of inertia, e.g. when a figure skater is pulling in their hands, speeding up the circular motion. In terms of angular momentum conservation, we have, for angular momentum \"L\", moment of inertia \"I\" and angular velocity \"\u03c9\":\nformula_102\nUsing this, we see that the change requires an energy of:\nformula_103\nso that a decrease in the moment of inertia requires investing energy.\nThis can be compared to the work done as calculated using Newton's laws. Each point in the rotating body is accelerating, at each point of time, with radial acceleration of:\nformula_104\nLet us observe a point of mass \"m\", whose position vector relative to the center of motion is perpendicular to the z-axis at a given point of time, and is at a distance \"z\". The centripetal force on this point, keeping the circular motion, is:\nformula_105\nThus the work required for moving this point to a distance \"dz\" farther from the center of motion is:\nformula_106\nFor a non-pointlike body one must integrate over this, with \"m\" replaced by the mass density per unit \"z\". This gives:\nformula_107\nwhich is exactly the energy required for keeping the angular momentum conserved.\nNote, that the above calculation can also be performed per mass, using kinematics only. Thus the phenomena of figure skater accelerating tangential velocity while pulling their hands in, can be understood as follows in layman's language: The skater's palms are not moving in a straight line, so they are constantly accelerating inwards, but do not gain additional speed because the accelerating is always done when their motion inwards is zero. However, this is different when pulling the palms closer to the body: The acceleration due to rotation now increases the speed; but because of the rotation, the increase in speed does not translate to a significant speed inwards, but to an increase of the rotation speed.\nStationary-action principle.\nIn classical mechanics it can be shown that the rotational invariance of action functionals implies conservation of angular momentum. The action is defined in classical physics as a functional of positions, formula_108 often represented by the use of square brackets, and the final and initial times. It assumes the following form in cartesian coordinates:formula_109where the repeated indices indicate summation over the index. If the action is invariant of an infinitesimal transformation, it can be mathematically stated as: formula_110.\nUnder the transformation, formula_111, the action becomes:\nformula_112\nwhere we can employ the expansion of the terms up-to first order in formula_113:\nformula_114giving the following change in action:\nformula_115\nSince all rotations can be expressed as matrix exponential of skew-symmetric matrices, i.e. as formula_116 where formula_117 is a skew-symmetric matrix and formula_118 is angle of rotation, we can express the change of coordinates due to the rotation formula_119, up-to first order of infinitesimal angle of rotation, formula_120 as:\nformula_121\nCombining the equation of motion and rotational invariance of action, we get from the above equations that:formula_122Since this is true for any matrix formula_123 that satisfies formula_124 it results in the conservation of the following quantity:\nformula_125\nas formula_126. This corresponds to the conservation of angular momentum throughout the motion.\nLagrangian formalism.\nIn Lagrangian mechanics, angular momentum for rotation around a given axis, is the conjugate momentum of the generalized coordinate of the angle around the same axis. For example, formula_127, the angular momentum around the z axis, is:\nformula_128\nwhere formula_129 is the Lagrangian and formula_130 is the angle around the z axis.\nNote that formula_131, the time derivative of the angle, is the angular velocity formula_132. Ordinarily, the Lagrangian depends on the angular velocity through the kinetic energy: The latter can be written by separating the velocity to its radial and tangential part, with the tangential part at the x-y plane, around the z-axis, being equal to:\nformula_133\nwhere the subscript i stands for the i-th body, and \"m\", \"v\"\"T\" and \"\u03c9\"\"z\" stand for mass, tangential velocity around the z-axis and angular velocity around that axis, respectively.\nFor a body that is not point-like, with density \"\u03c1\", we have instead:\nformula_134\nwhere integration runs over the area of the body, and \"I\"z is the moment of inertia around the z-axis.\nThus, assuming the potential energy does not depend on \"\u03c9\"\"z\" (this assumption may fail for electromagnetic systems), we have the angular momentum of the \"i\"th object:\nformula_135\nWe have thus far rotated each object by a separate angle; we may also define an overall angle \"\u03b8\"z by which we rotate the whole system, thus rotating also each object around the z-axis, and have the overall angular momentum:\nformula_136\nFrom Euler\u2013Lagrange equations it then follows that:\nformula_137\nSince the lagrangian is dependent upon the angles of the object only through the potential, we have:\nformula_138\nwhich is the torque on the \"i\"th object.\nSuppose the system is invariant to rotations, so that the potential is independent of an overall rotation by the angle \"\u03b8\"z (thus it may depend on the angles of objects only through their differences, in the form formula_139). We therefore get for the total angular momentum:\nformula_140\nAnd thus the angular momentum around the z-axis is conserved.\nThis analysis can be repeated separately for each axis, giving conversation of the angular momentum vector. However, the angles around the three axes cannot be treated simultaneously as generalized coordinates, since they are not independent; in particular, two angles per point suffice to determine its position. While it is true that in the case of a rigid body, fully describing it requires, in addition to three translational degrees of freedom, also specification of three rotational degrees of freedom; however these cannot be defined as rotations around the Cartesian axes (see Euler angles). This caveat is reflected in quantum mechanics in the non-trivial commutation relations of the different components of the angular momentum operator.\nHamiltonian formalism.\nEquivalently, in Hamiltonian mechanics the Hamiltonian can be described as a function of the angular momentum. As before, the part of the kinetic energy related to rotation around the z-axis for the \"i\"th object is:\nformula_141\nwhich is analogous to the energy dependence upon momentum along the z-axis, formula_142.\nHamilton's equations relate the angle around the z-axis to its conjugate momentum, the angular momentum around the same axis:\nformula_143\nThe first equation gives\nformula_144\nAnd so we get the same results as in the Lagrangian formalism.\nNote, that for combining all axes together, we write the kinetic energy as:\nformula_145 is tiny by everyday standards, about 10\u221234 J s, and therefore this quantization does not noticeably affect the angular momentum of macroscopic objects. However, it is very important in the microscopic world. For example, the structure of electron shells and subshells in chemistry is significantly affected by the quantization of angular momentum.\nQuantization of angular momentum was first postulated by Niels Bohr in his model of the atom and was later predicted by Erwin Schr\u00f6dinger in his Schr\u00f6dinger equation.\nUncertainty.\nIn the definition formula_146, six operators are involved: The position operators formula_147, formula_148, formula_149, and the momentum operators formula_150, formula_151, formula_152. However, the Heisenberg uncertainty principle tells us that it is not possible for all six of these quantities to be known simultaneously with arbitrary precision. Therefore, there are limits to what can be known or measured about a particle's angular momentum. It turns out that the best that one can do is to simultaneously measure both the angular momentum vector's magnitude and its component along one axis.\nThe uncertainty is closely related to the fact that different components of an angular momentum operator do not commute, for example formula_153. (For the precise commutation relations, see angular momentum operator.)\nTotal angular momentum as generator of rotations.\nAs mentioned above, orbital angular momentum L is defined as in classical mechanics: formula_146, but \"total\" angular momentum J is defined in a different, more basic way: J is defined as the \"generator of rotations\". More specifically, J is defined so that the operator\nformula_155\nis the rotation operator that takes any system and rotates it by angle formula_37 about the axis formula_157. (The \"exp\" in the formula refers to operator exponential.) To put this the other way around, whatever our quantum Hilbert space is, we expect that the rotation group SO(3) will act on it. There is then an associated action of the Lie algebra so(3) of SO(3); the operators describing the action of so(3) on our Hilbert space are the (total) angular momentum operators.\nThe relationship between the angular momentum operator and the rotation operators is the same as the relationship between Lie algebras and Lie groups in mathematics. The close relationship between angular momentum and rotations is reflected in Noether's theorem that proves that angular momentum is conserved whenever the laws of physics are rotationally invariant.\nAngular momentum in electrodynamics.\nWhen describing the motion of a charged particle in an electromagnetic field, the canonical momentum P (derived from the Lagrangian for this system) is not gauge invariant. As a consequence, the canonical angular momentum L = r \u00d7 P is not gauge invariant either. Instead, the momentum that is physical, the so-called \"kinetic momentum\" (used throughout this article), is (in SI units)\nformula_158\nwhere \"e\" is the electric charge of the particle and A the magnetic vector potential of the electromagnetic field. The gauge-invariant angular momentum, that is \"kinetic angular momentum\", is given by\nformula_159\nThe interplay with quantum mechanics is discussed further in the article on canonical commutation relations.\nAngular momentum in optics.\nIn \"classical Maxwell electrodynamics\" the Poynting vector\nis a linear momentum density of electromagnetic field.\nformula_160\nThe angular momentum density vector formula_161 is given by a vector product\nas in classical mechanics:\nformula_162\nThe above identities are valid \"locally\", i.e. in each space point formula_48 in a given moment formula_164.\nAngular momentum in nature and the cosmos.\nTropical cyclones and other related weather phenomena involve conservation of angular momentum in order to explain the dynamics. Winds revolve slowly around low pressure systems, mainly due to the coriolis effect. If the low pressure intensifies and the slowly circulating air is drawn toward the center, the molecules must speed up in order to conserve angular momentum. By the time they reach the center, the speeds become destructive.\nJohannes Kepler determined the laws of planetary motion without knowledge of conservation of momentum. However, not long after his discovery their derivation was determined from conservation of angular momentum. Planets move more slowly the further they are out in their elliptical orbits, which is explained intuitively by the fact that orbital angular momentum is proportional to the radius of the orbit. Since the mass does not change and the angular momentum is conserved, the velocity drops.\nTidal acceleration is an effect of the tidal forces between an orbiting natural satellite (e.g. the Moon) and the primary planet that it orbits (e.g. Earth). The gravitational torque between the Moon and the tidal bulge of Earth causes the Moon to be constantly promoted to a slightly higher orbit (~3.8\u00a0cm per year) and Earth to be decelerated (by \u221225.858 \u00b1 0.003\u2033/cy\u00b2) in its rotation (the length of the day increases by ~1.7 ms per century, +2.3 ms from tidal effect and \u22120.6 ms from post-glacial rebound). The Earth loses angular momentum which is transferred to the Moon such that the overall angular momentum is conserved.\nAngular momentum in engineering and technology.\nExamples of using conservation of angular momentum for practical advantage are abundant. In engines such as steam engines or internal combustion engines, a flywheel is needed to efficiently convert the lateral motion of the pistons to rotational motion.\nInertial navigation systems explicitly use the fact that angular momentum is conserved with respect to the inertial frame of space. Inertial navigation is what enables submarine trips under the polar ice cap, but are also crucial to all forms of modern navigation.\nRifled bullets use the stability provided by conservation of angular momentum to be more true in their trajectory. The invention of rifled firearms and cannons gave their users significant strategic advantage in battle, and thus were a technological turning point in history.\nHistory.\nIsaac Newton, in the \"Principia\", hinted at angular momentum in his examples of the first law of motion,A top, whose parts by their cohesion are perpetually drawn aside from rectilinear motions, does not cease its rotation, otherwise than as it is retarded by the air. The greater bodies of the planets and comets, meeting with less resistance in more free spaces, preserve their motions both progressive and circular for a much longer time.He did not further investigate angular momentum directly in the \"Principia\", saying:From such kind of reflexions also sometimes arise the circular motions of bodies about their own centres. But these are cases which I do not consider in what follows; and it would be too tedious to demonstrate every particular that relates to this subject.However, his geometric proof of the law of areas is an outstanding example of Newton's genius, and indirectly proves angular momentum conservation in the case of a central force.\nLaw of Areas.\nNewton's derivation.\nAs a planet orbits the Sun, the line between the Sun and the planet sweeps out equal areas in equal intervals of time. This had been known since Kepler expounded his second law of planetary motion. Newton derived a unique geometric proof, and went on to show that the attractive force of the Sun's gravity was the cause of all of Kepler's laws.\nDuring the first interval of time, an object is in motion from point A to point B. Undisturbed, it would continue to point c during the second interval. When the object arrives at B, it receives an impulse directed toward point S. The impulse gives it a small added velocity toward S, such that if this were its only velocity, it would move from B to V during the second interval. By the rules of velocity composition, these two velocities add, and point C is found by construction of parallelogram BcCV. Thus the object's path is deflected by the impulse so that it arrives at point C at the end of the second interval. Because the triangles SBc and SBC have the same base SB and the same height Bc or VC, they have the same area. By symmetry, triangle SBc also has the same area as triangle SAB, therefore the object has swept out equal areas SAB and SBC in equal times.\nAt point C, the object receives another impulse toward S, again deflecting its path during the third interval from d to D. Thus it continues to E and beyond, the triangles SAB, SBc, SBC, SCd, SCD, SDe, SDE all having the same area. Allowing the time intervals to become ever smaller, the path ABCDE approaches indefinitely close to a continuous curve.\nNote that because this derivation is geometric, and no specific force is applied, it proves a more general law than Kepler's second law of planetary motion. It shows that the Law of Areas applies to any central force, attractive or repulsive, continuous or non-continuous, or zero.\nConservation of angular momentum in the law of areas.\nThe proportionality of angular momentum to the area swept out by a moving object can be understood by realizing that the bases of the triangles, that is, the lines from S to the object, are equivalent to the radius, and that the heights of the triangles are proportional to the perpendicular component of velocity. Hence, if the area swept per unit time is constant, then by the triangular area formula , the product and therefore the product are constant: if and the base length are decreased, and height must increase proportionally. Mass is constant, therefore angular momentum is conserved by this exchange of distance and velocity.\nIn the case of triangle SBC, area is equal to (SB)(VC). Wherever C is eventually located due to the impulse applied at B, the product (SB)(VC), and therefore remain constant. Similarly so for each of the triangles.\nAnother areal proof of conservation of angular momentum for any central force uses Mamikon's sweeping tangents theorem.\nAfter Newton.\nLeonhard Euler, Daniel Bernoulli, and Patrick d'Arcy all understood angular momentum in terms of conservation of areal velocity, a result of their analysis of Kepler's second law of planetary motion. It is unlikely that they realized the implications for ordinary rotating matter.\nIn 1736 Euler, like Newton, touched on some of the equations of angular momentum in his \"Mechanica\" without further developing them.\nBernoulli wrote in a 1744 letter of a \"moment of rotational motion\", possibly the first conception of angular momentum as we now understand it.\nIn 1799, Pierre-Simon Laplace first realized that a fixed plane was associated with rotation\u2014his \"invariable plane\".\nLouis Poinsot in 1803 began representing rotations as a line segment perpendicular to the rotation, and elaborated on the \"conservation of moments\".\nIn 1852 L\u00e9on Foucault used a gyroscope in an experiment to display the Earth's rotation.\nWilliam J. M. Rankine's 1858 \"Manual of Applied Mechanics\" defined angular momentum in the modern sense for the first time:...\u00a0a line whose length is proportional to the magnitude of the angular momentum, and whose direction is perpendicular to the plane of motion of the body and of the fixed point, and such, that when the motion of the body is viewed from the extremity of the line, the radius-vector of the body seems to have right-handed rotation.In an 1872 edition of the same book, Rankine stated that \"The term \"angular momentum\" was introduced by Mr. Hayward,\" probably referring to R.B. Hayward's article \"On a Direct Method of estimating Velocities, Accelerations, and all similar Quantities with respect to Axes moveable in any manner in Space with Applications,\" which was introduced in 1856, and published in 1864. Rankine was mistaken, as numerous publications feature the term starting in the late 18th to early 19th centuries. However, Hayward's article apparently was the first use of the term and the concept seen by much of the English-speaking world. Before this, angular momentum was typically referred to as \"momentum of rotation\" in English."}
{"id": "2840", "revid": "30519056", "url": "https://en.wikipedia.org/wiki?curid=2840", "title": "Plum pudding model", "text": "The plum pudding model was the first scientific model of the atom to describe an internal structure. It was first proposed by J. J. Thomson in 1904 following his discovery of the electron in 1897, and was rendered obsolete by Ernest Rutherford's discovery of the atomic nucleus in 1911. The model tried to account for two properties of atoms then known: that there are electrons, and that atoms have no net electric charge. Logically there had to be an equal amount of positive charge to balance out the negative charge of the electrons. As Thomson had no idea as to the source of this positive charge, he tentatively proposed that it was everywhere in the atom, and that the atom was spherical. This was the mathematically simplest hypothesis to fit the available evidence, or lack thereof. In such a sphere, the negatively charged electrons would distribute themselves in a more or less even manner throughout the volume, simultaneously repelling each other while being attracted to the positive sphere's center.\nDespite Thomson's efforts, his model couldn't account for emission spectra and valencies. Based on experimental studies of alpha particle scattering (in the gold foil experiment), Ernest Rutherford developed an alternative model for the atom featuring a compact nucleus where the positive charge is concentrated.\nThomson's model is popularly referred to as the \"plum pudding model\" with the notion that the electrons are distributed uniformly like raisins in a plum pudding. Neither Thomson nor his colleagues ever used this analogy. It seems to have been coined by popular science writers to make the model easier to understand for the layman. The analogy is perhaps misleading because Thomson likened the positive sphere to a liquid rather than a solid since he thought the electrons moved around in it.\nSignificance.\nThomson's model marks the moment when the development of atomic theory passed from chemists to physicists. While atomic theory was widely accepted by chemists by the end of the 19th century, physicists remained skeptical because the atomic model lacked any properties which concerned their field, such as electric charge, magnetic moment, volume, or absolute mass. Thomson himself was a physicist and his atomic model was a byproduct of his investigations of cathode rays, by which he discovered electrons. Thomson hypothesized that the quantity, arrangement, and motions of electrons in the atom could explain its physical and chemical properties, such as emission spectra, valencies, reactivity, and ionization. He was on the right track, though his approach was based on classical mechanics and he did not have the insight to incorporate quantized energy into it.\nBackground.\nThroughout the 19th century evidence from chemistry and statistical mechanics accumulated that matter was composed of atoms. The structure of the atom was discussed, and by the end of the century the leading model was the vortex theory of the atom, proposed by William Thomson (later Lord Kelvin) in 1867. By 1890, J.J. Thomson had his own version called the \"nebular atom\" hypothesis, in which atoms were composed of immaterial vortices and suggested similarities between the arrangement of vortices and periodic regularity found among the chemical elements.\nThomson's discovery of the electron in 1897 changed his views. Thomson called them \"corpuscles\" (particles), but they were more commonly called \"electrons\", the name G. J. Stoney had coined for the \"fundamental unit quantity of electricity\" in 1891. However even late in 1899, few scientists believed in subatomic particles.\nAnother emerging scientific theme of the 19th century was the discovery and study of radioactivity. Thomson discovered the electron by studying cathode rays, and in 1900 Henri Becquerel determined that the radiation from uranium, now called beta particles, had the same charge/mass ratio as cathode rays. These beta particles were believed to be electrons travelling at high speed. The particles were used by Thomson to probe atoms to find evidence for his atomic theory. The other form of radiation critical to this era of atomic models was alpha particles. Heavier and slower than beta particles, these were the key tool used by Rutherford to find evidence against Thomson's model.\nIn addition to the emerging atomic theory, the electron, and radiation, the last element of history was the many studies of atomic spectra published in the late 19th century. Part of the attraction of the vortex model was its possible role in describing the spectral data as vibrational responses to electromagnetic radiation. Neither Thomson's model nor its successor, Rutherford's model, made progress towards understanding atomic spectra. That would have to wait until Niels Bohr built the first quantum-based atom model.\nDevelopment.\nThomson's model was the first to assign a specific inner structure to an atom, though his earliest descriptions did not include mathematical formulas.\nFrom 1897 through 1913, Thomson proposed a series of increasingly detailed \"polyelectron\" models for the atom. His first versions were qualitative culminating in his 1906 paper and follow on summaries. Thomson's model changed over the course of its initial publication, finally becoming a model with much more mobility containing electrons revolving in the dense field of positive charge rather than a static structure. Thomson attempted unsuccessfully to reshape his model to account for some of the major spectral lines experimentally known for several elements.\n1897 Corpuscles inside atoms.\nIn a paper titled \"Cathode Rays\", Thomson demonstrated that cathode rays are not light but made of negatively charged particles which he called \"corpuscles\". He observed that cathode rays can be deflected by electric and magnetic fields, which does not happen with light rays. In a few paragraphs near the end of this long paper Thomson discusses the possibility that atoms were made of these \"corpuscles\", calling them \"primordial atoms\". Thomson believed that the intense electric field around the cathode caused the surrounding gas molecules to split up into their component \"corpuscles\", thereby generating cathode rays. Thomson thus showed evidence that atoms were divisible, though he did not attempt to describe their structure at this point.\nThomson notes that he was not the first scientist to propose that atoms are divisible, making reference to William Prout who in 1815 found that the atomic weights of various elements were multiples of hydrogen's atomic weight and hypothesised that all atoms were made of hydrogen atoms fused together. Prout's hypothesis was dismissed by chemists when by the 1830s it was found that some elements seemed to have a non-integer atomic weight\u2014e.g. chlorine has an atomic weight of about 35.45. But the idea continued to intrigue scientists. The discrepancies were eventually explained with the discovery of isotopes in 1912.\nA few months after Thomson's paper appeared, George FitzGerald suggested that the corpuscle identified by Thomson from cathode rays and proposed as parts of an atom was a \"free electron\", as described by physicist Joseph Larmor and Hendrik Lorentz. While Thomson did not adopt the terminology, the connection convinced other scientists that cathode rays were particles, an important step in their eventual acceptance of an atomic model based on sub-atomic particles.\nIn 1899 Thomson reiterated his atomic model in a paper that showed that negative electricity created by ultraviolet light landing on a metal (known now as the photoelectric effect) has the same mass-to-charge ratio as cathode rays; then he applied his previous method for determining the charge on ions to the negative electric particles created by ultraviolet light. He estimated that the electron's mass was 0.0014 times that of the hydrogen ion (as a fraction: ). In the conclusion of this paper he writes:\n1904 Mechanical model of the atom.\nThomson provided his first detailed description of the atom in his 1904 paper \"On the Structure of the Atom\".\nThomson starts with a short description of his model\n... the atoms of the elements consist of a number of negatively electrified corpuscles enclosed in a sphere of uniform positive electrification, ...\nPrimarily focused on the electrons, Thomson adopted the positive sphere from Kelvin's atom model proposed a year earlier. \nHe then gives a detailed mechanical analysis of such a system, distributing the electrons uniformly around a ring. The attraction of the positive electrification is balanced by the mutual repulsion of the electrons. His analysis focuses on stability, looking for cases where small changes in position are countered by restoring forces.\nAfter discussing his many formulae for stability he turned to analysing patterns in the number of electrons in various concentric rings of stable configurations. These regular patterns Thomson argued are analogous to the periodic law of chemistry behind the structure of the periodic table. This concept, that a model based on subatomic particles could account for chemical trends, encouraged interest in Thomson's model and influenced future work even if the details Thomson's electron assignments turned out to be incorrect.\nThomson at this point believed that all the mass of the atom was carried by the electrons. This would mean that even a small atom would have to contain thousands of electrons, and the positive electrification that encapsulated them was without mass.\n1905 lecture on electron arrangements.\nIn a lecture delivered to the Royal Institution of Great Britain in 1905, Thomson explained that it was too computationally difficult for him to calculate the movements of large numbers of electrons in the positive sphere, so he proposed a practical experiment. This involved magnetised pins pushed into cork discs and set afloat in a basin of water. The pins were oriented such that they repelled each other. Above the centre of the basin was suspended an electromagnet that attracted the pins. The equilibrium arrangement the pins took informed Thomson on what arrangements the electrons in an atom might take.\nFor instance, he observed that while five pins would arrange themselves in a stable pentagon around the centre, six pins could not form a stable hexagon. Instead, one pin would move to the centre and the other five would form a pentagon around the centre pin, and this arrangement was stable. As he added more pins, they would arrange themselves in concentric rings around the centre.\nThe experiment functioned in two dimensions instead of three, but Thomson inferred the electrons in the atom arranged themselves in concentric shells and the could move within these shells but did not move from one shell to another them except when electrons were added or subtracted from the atom.\n1906 Estimating electrons per atom.\nBefore 1906 Thomson considered the atomic weight to be due to the mass of the electrons (which he continued to call \"corpuscles\"). Based on his own estimates of the electron mass, an atom would need tens of thousands electrons to account for the mass. In 1906 he used three different methods, X-ray scattering, beta ray absorption, or optical properties of gases, to estimate that \"number of corpuscles is not greatly different from the atomic weight\". This reduced the number of electrons to tens or at most a couple of hundred and that in turn meant that the positive sphere in Thomson's model contained most of the mass of the atom. This meant that Thomson's mechanical stability work from 1904 and the comparison to the periodic table were no longer valid. Moreover, the alpha particle, so important to the next advance in atomic theory by Rutherford, would no longer be viewed as an atom containing thousands of electrons.\nIn 1907, Thomson published \"The Corpuscular Theory of Matter\" which reviewed his ideas on the atom's structure and proposed further avenues of research.\nIn Chapter 6, he further elaborates his experiment using magnetised pins in water, providing an expanded table. For instance, if 59 pins were placed in the pool, they would arrange themselves in concentric rings of the order 20-16-13-8-2 (from outermost to innermost).\nIn Chapter 7, Thomson summarised his 1906 results on the number of electrons in an atom. He included one important correction: he replaced the beta-particle analysis with one based on the cathode ray experiments of August Becker, giving a result in better agreement with other approaches to the problem. Experiments by other scientists in this field had shown that atoms contain far fewer electrons than Thomson previously thought. Thomson now believed the number of electrons in an atom was a small multiple of its atomic weight: \"the number of corpuscles in an atom of any element is proportional to the atomic weight of the element \u2014 it is a multiple, and not a large one, of the atomic weight of the element.\" This meant that almost all of the atom's mass had to be carried by the positive sphere, whatever it was made of.\nThomson in this book estimated that a hydrogen atom is 1,700 times heavier than an electron (the current measurement is 1,837). Thomson noted that no scientist had yet found a positively charged particle smaller than a hydrogen ion. He also wrote that the positive charge of an atom is a multiple of a basic unit of positive charge, equal to the negative charge of an electron. Thomson refused to jump to the conclusion that the basic unit of positive charge has a mass equal to that of the hydrogen ion, arguing that scientists first had to know how many electrons an atom contains. For all he could tell, a hydrogen ion might still contain a few electrons\u2014perhaps two electrons and three units of positive charge.\n1910 Multiple scattering.\nThomson's difficulty with beta scattering in 1906 lead him to renewed interest in the topic. He encouraged J. Arnold Crowther to experiment with beta scattering through thin foils and, in 1910, Thomson produced a new theory of beta scattering. The two innovations in this paper was the introduction of scattering from the positive sphere of the atom and analysis that multiple or compound scattering was critical to the final results. This theory and Crowther's experimental results would be confronted by Rutherford's theory and Geiger and Mardsen new experiments with alpha particles.\nAnother innovation in Thomson's 1910 paper was that he modelled how an atom might deflect an incoming beta particle if the positive charge of the atom existed in discrete units of equal but arbitrary size, spread evenly throughout the atom, separated by empty space, with each unit having a positive charge equal to the electron's negative charge. Thomson therefore came close to deducing the existence of the proton, which was something Rutherford eventually did. In Rutherford's model of the atom, the protons are clustered in a very small nucleus, but in Thomson's alternative model, the positive units were spread throughout the atom.\nThomson's 1910 scattering model.\nIn his 1910 paper \"On the Scattering of rapidly moving Electrified Particles\", Thomson presented equations that modelled how beta particles scatter in a collision with an atom. His work was based on beta scattering studies by James Crowther.\nDeflection by the positive sphere.\nThomson typically assumed the positive charge in the atom was uniformly distributed throughout its volume, encapsulating the electrons. In his 1910 paper, Thomson presented the following equation which isolated the effect of this positive sphere:\nformula_1\nwhere \"k\" is the Coulomb constant, \"q\"e is the charge of the beta particle, \"q\"g is the charge of the positive sphere, \"m\" is the mass of the beta particle, and \"R\" is the radius of the sphere. Because the atom is many thousands of times heavier than the beta particle, no correction for recoil is needed.\nThomson did not explain how this equation was developed, but the historian John L. Heilbron provided an educated guess he called a \"straight-line\" approximation. Consider a beta particle passing through the positive sphere with its initial trajectory at a lateral distance \"b\" from the centre. The path is assumed to have a very small deflection and therefore is treated here as a straight line.\nInside a sphere of uniformly distributed positive charge the force exerted on the beta particle at any point along its path through the sphere would be directed along the radius with magnitude:\nformula_2\nThe component of force perpendicular to the trajectory and thus deflecting the path of the particle would be:\nformula_3\nThe lateral change in momentum \"p\"y is therefore\nformula_4\nThe resulting angular deflection, formula_5, is given by\nformula_6\nwhere \"p\"x is the average horizontal momentum taken to be equal to the incoming momentum. Since we already know the deflection is very small, we can treat formula_7 as being equal to formula_5.\nTo find the average deflection angle formula_9, the angle for each value of \"b\" and the corresponding \"L\" are added across the face sphere, then divided by the cross-section area. formula_10 per Pythagorean theorem.\nformula_11\nformula_12\nThis matches Thomson's formula in his 1910 paper.\nDeflection by the electrons.\nThomson modelled the collisions between a beta particle and the electrons of an atom by calculating the deflection of one collision then multiplying by a factor for the number of collisions as the particle crosses the atom.\nFor the electrons within an arbitrary distance \"s\" of the beta particle's path, their mean distance will be . Therefore, the average deflection per electron will be\nformula_13\nwhere \"q\"e is the elementary charge, \"k\" is the Coulomb constant, \"m\" and \"v\" are the mass and velocity of the beta particle.\nThe factor for the number of collisions was known to be the square root of the number of possible electrons along path. The number of electrons depends upon the density of electrons along the particle path times the path length \"L\".\nThe net deflection caused by all the electrons within this arbitrary cylinder of effect around the beta particle's path is\nformula_14\nwhere \"N\"0 is the number of electrons per unit volume and formula_15 is the volume of this cylinder.\nSince Thomson calculated the deflection would be very small, he treats \"L\" as a straight line. Therefore formula_16 where \"b\" is the distance of this chord from the centre. The mean of formula_17 is given by the integral\nformula_18\nWe can now replace formula_17 in the equation for formula_20 to obtain the mean deflection formula_21:\nformula_22\nformula_23\nwhere \"N\" is the number of electrons in the atom, equal to formula_24.\nDeflection by the positive charge in discrete units.\nIn his 1910 paper, Thomson proposed an alternative model in which the positive charge exists in discrete units separated by empty space, with those units being evenly distributed throughout the atom's volume.\nIn this concept, the average scattering angle of the beta particle is given by:\nformula_25\nwhere \"\u03c3\" is the ratio of the volume occupied by the positive charge to the volume of the whole atom. Thomson did not explain how he arrived at this equation.\nNet deflection.\nTo find the combined effect of the positive charge and the electrons on the beta particle's path, Thomson provided the following equation:\nformula_26\nDemise of the plum pudding model.\nThomson probed the structure of atoms through beta particle scattering, whereas his former student Ernest Rutherford was interested in alpha particle scattering. Beta particles are electrons emitted by radioactive decay, whereas alpha particles are essentially helium atoms, also emitted in process of decay. Alpha particles have considerably more momentum than beta particles and Rutherford found that matter scatters alpha particles in ways that Thomson's plum pudding model could not predict.\nBetween 1908 and 1913, Ernest Rutherford, Hans Geiger, and Ernest Marsden collaborated on a series of experiments in which they bombarded thin metal foils with a beam of alpha particles and measured the intensity versus scattering angle of the particles. They found that the metal foil could scatter alpha particles by more than 90\u00b0. This should not have been possible according to the Thomson model: the scattering into large angles should have been negligible. The odds of a beta particle being scattered by more than 90\u00b0 under such circumstances is astronomically small, and since alpha particles typically have much more momentum than beta particles, their deflection should be smaller still. The Thomson models simply could not produce electrostatic forces of sufficient strength to cause such large deflection. The charges in the Thomson model were too diffuse. This led Rutherford to discard the Thomson for a new model where the positive charge of the atom is concentrated in a tiny nucleus.\nRutherford went on to make more compelling discoveries. In Thomson's model, the positive charge sphere was just an abstract component, but Rutherford found something concrete to attribute the positive charge to: particles he dubbed \"protons\". Whereas Thomson believed that the electron count was roughly correlated to the atomic weight, Rutherford showed that (in a neutral atom) it is exactly equal to the atomic number.\nThomson hypothesised that the arrangement of the electrons in the atom somehow determined the spectral lines of a chemical element. He was on the right track, but it had nothing to do with how atoms circulated in a sphere of positive charge. Scientists eventually discovered that it had to do with how electrons absorb and release energy in discrete quantities, moving through energy levels which correspond to emission and absorption spectra. Thomson had not incorporated quantum mechanics into his atomic model, which at the time was a very new field of physics. Niels Bohr and Erwin Schroedinger later incorporated quantum mechanics into the atomic model.\nRutherford's nuclear model.\nRutherford's 1911 paper on alpha particle scattering showed that Thomson's scattering model could not explain the large angle scattering and it showed that multiple scattering was not necessary to explain the data. However, in the years immediately following its publication few scientists took note. The scattering model predictions were not considered definitive evidence against Thomson's plum pudding model. Thomson and Rutherford had pioneered scattering as a technique to probe atoms, its reliability and value were unproven. Before Rutherford's paper the alpha particle was considered an atom, not a compact mass. It was not clear why it should be a good probe. Moreover, Rutherford's paper did not discuss the atomic electrons vital to practical problems like chemistry or atomic spectroscopy. Rutherford's nuclear model would only become widely accepted after the work of Niels Bohr.\nMathematical Thomson problem.\nThe Thomson problem in mathematics seeks the optimal distribution of equal point charges on the surface of a sphere. Unlike the original Thomson atomic model, the sphere in this purely mathematical model does not have a charge, and this causes all the point charges to move to the surface of the sphere by their mutual repulsion. There is still no general solution to Thomson's original problem of how electrons arrange themselves within a sphere of positive charge.\nOrigin of the nickname.\nThe first known writer to compare Thomson's model to a plum pudding was an anonymous reporter in an article for the British pharmaceutical magazine \"The Chemist and Druggist\" in August 1906.\nThe analogy was never used by Thomson nor his colleagues. It seems to have been a conceit of popular science writers to make the model easier to understand for the layman."}
{"id": "2842", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=2842", "title": "Atom/Bohr model", "text": ""}
{"id": "2844", "revid": "30519056", "url": "https://en.wikipedia.org/wiki?curid=2844", "title": "History of atomic theory", "text": "Atomic theory is the scientific theory that matter is composed of particles called atoms. The definition of the word \"atom\" has changed over the years in response to scientific discoveries. Initially, it referred to a hypothetical concept of there being some fundamental particle of matter, too small to be seen by the naked eye, that could not be divided. Then the definition was refined to being the basic particles of the chemical elements, when chemists observed that elements seemed to combine with each other in ratios of small whole numbers. Then physicists discovered that these particles had an internal structure of their own and therefore perhaps did not deserve to be called \"atoms\", but renaming atoms would have been impractical by that point.\nAtomic theory is one of the most important scientific developments in history, crucial to all the physical sciences. At the start of \"The Feynman Lectures on Physics\", physicist and Nobel laureate Richard Feynman offers the atomic hypothesis as the single most prolific scientific concept.\nPhilosophical atomism.\nThe basic idea that matter is made up of tiny indivisible particles is an old idea that appeared in many ancient cultures. The word \"atom\" is derived from the ancient Greek word \"atomos\", which means \"uncuttable\". This ancient idea was based in philosophical reasoning rather than scientific reasoning. Modern atomic theory is not based on these old concepts. In the early 19th century, the scientist John Dalton noticed that chemical substances seemed to combine with each other by discrete and consistent units of weight, and he decided to use the word \"atom\" to refer to these units.\nGroundwork.\nWorking in the late 17th century, Robert Boyle developed the concept of a chemical element as substance different from a compound.\nNear the end of the 18th century, a number of important developments in chemistry emerged without referring to the notion of an atomic theory. The first was Antoine Lavoisier who showed that compounds consist of elements in constant proportion, redefining an element as a substance which scientists could not decompose into simpler substances by experimentation. This brought an end to the ancient idea of the elements of matter being fire, earth, air, and water, which had no experimental support. Lavoisier showed that water can be decomposed into hydrogen and oxygen, which in turn he could not decompose into anything simpler, thereby proving these are elements. Lavoisier also defined the law of conservation of mass, which states that in a chemical reaction, matter does not appear nor disappear into thin air; the total mass remains the same even if the substances involved were transformed. Finally, there was the law of definite proportions, established by the French chemist Joseph Proust in 1797, which states that if a compound is broken down into its constituent chemical elements, then the masses of those constituents will always have the same proportions by weight, regardless of the quantity or source of the original compound. This definition distinguished compounds from mixtures.\nDalton's law of multiple proportions.\nJohn Dalton studied data gathered by himself and by other scientists. He noticed a pattern that later came to be known as the law of multiple proportions: in compounds which contain two particular elements, the amount of Element A per measure of Element B will differ across these compounds by ratios of small whole numbers. This suggested that each element combines with other elements in multiples of a basic quantity.\nIn 1804, Dalton explained his atomic theory to his friend and fellow chemist Thomas Thomson, who published an explanation of Dalton's theory in his book \"A System of Chemistry\" in 1807. According to Thomson, Dalton's idea first occurred to him when experimenting with \"olefiant gas\" (ethylene) and \"carburetted hydrogen gas\" (methane). Dalton found that \"carburetted hydrogen gas\" contains twice as much hydrogen per measure of carbon as \"olefiant gas\", and concluded that a molecule of \"olefiant gas\" is one carbon atom and one hydrogen atom, and a molecule of \"carburetted hydrogen gas\" is one carbon atom and two hydrogen atoms. In reality, an ethylene molecule has two carbon atoms and four hydrogen atoms (C2H4), and a methane molecule has one carbon atom and four hydrogen atoms (CH4). In this particular case, Dalton was mistaken about the formulas of these compounds, and it wasn't his only mistake. But in other cases, he got their formulas right, as in the following examples:\nExample 1 \u2014 tin oxides: Dalton identified two types of tin oxide. One is a grey powder that Dalton referred to as \"the protoxide of tin\", which is 88.1% tin and 11.9% oxygen. The other is a white powder which Dalton referred to as \"the deutoxide of tin\", which is 78.7% tin and 21.3% oxygen. Adjusting these figures, in the grey powder there is about 13.5\u00a0g of oxygen for every 100\u00a0g of tin, and in the white powder there is about 27\u00a0g of oxygen for every 100\u00a0g of tin. 13.5 and 27 form a ratio of 1:2. These compounds are known today as tin(II) oxide (SnO) and tin(IV) oxide (SnO2). In Dalton's terminology, a \"protoxide\" is a molecule containing a single oxygen atom, and a \"deutoxide\" molecule has two. The modern equivalents of his terms would be \"monoxide\" and \"dioxide\".\nExample 2 \u2014 iron oxides: Dalton identified two oxides of iron. There is one type of iron oxide that is a black powder which Dalton referred to as \"the protoxide of iron\", which is 78.1% iron and 21.9% oxygen. The other iron oxide is a red powder, which Dalton referred to as \"the intermediate or red oxide of iron\" which is 70.4% iron and 29.6% oxygen. Adjusting these figures, in the black powder there is about 28\u00a0g of oxygen for every 100\u00a0g of iron, and in the red powder there is about 42\u00a0g of oxygen for every 100\u00a0g of iron. 28 and 42 form a ratio of 2:3. These compounds are iron(II) oxide and iron(III) oxide and their formulas are Fe2O2 and Fe2O3 respectively (iron(II) oxide's formula is normally written as FeO, but here it is written as Fe2O2 to contrast it with the other oxide). Dalton described the \"intermediate oxide\" as being \"2 atoms protoxide and 1 of oxygen\", which adds up to two atoms of iron and three of oxygen. That averages to one and a half atoms of oxygen for every iron atom, putting it midway between a \"protoxide\" and a \"deutoxide\".\nExample 3 \u2014 nitrogen oxides: Dalton was aware of three oxides of nitrogen: \"nitrous oxide\", \"nitrous gas\", and \"nitric acid\". These compounds are known today as nitrous oxide, nitric oxide, and nitrogen dioxide respectively. \"Nitrous oxide\" is 63.3% nitrogen and 36.7% oxygen, which means it has 80 g of oxygen for every 140 g of nitrogen. \"Nitrous gas\" is 44.05% nitrogen and 55.95% oxygen, which means there is 160 g of oxygen for every 140 g of nitrogen. \"Nitric acid\" is 29.5% nitrogen and 70.5% oxygen, which means it has 320 g of oxygen for every 140 g of nitrogen. 80\u00a0g, 160\u00a0g, and 320\u00a0g form a ratio of 1:2:4. The formulas for these compounds are N2O, NO, and NO2.\nDalton defined an atom as being the \"ultimate particle\" of a chemical substance, and he used the term \"compound atom\" to refer to \"ultimate particles\" which contain two or more elements. This is inconsistent with the modern definition, wherein an atom is the basic particle of a chemical element and a molecule is an agglomeration of atoms. The term \"compound atom\" was confusing to some of Dalton's contemporaries as the word \"atom\" implies indivisibility, but he responded that if a carbon dioxide \"atom\" is divided, it ceases to be carbon dioxide. The carbon dioxide \"atom\" is indivisible in the sense that it cannot be divided into smaller carbon dioxide particles.\nDalton made the following assumptions on how \"elementary atoms\" combined to form \"compound atoms\" (what we today refer to as molecules). When two elements can only form one compound, he assumed it was one atom of each, which he called a \"binary compound\". If two elements can form two compounds, the first compound is a binary compound and the second is a \"ternary compound\" consisting of one atom of the first element and two of the second. If two elements can form three compounds between them, then the third compound is a \"quaternary\" compound containing one atom of the first element and three of the second. Dalton thought that water was a \"binary compound\", i.e. one hydrogen atom and one oxygen atom. Dalton did not know that in their natural gaseous state, the ultimate particles of oxygen, nitrogen, and hydrogen exist in pairs (O2, N2, and H2). Nor was he aware of valencies. These properties of atoms were discovered later in the 19th century.\nBecause atoms were too small to be directly weighed using the methods of the 19th century, Dalton instead expressed the weights of the myriad atoms as multiples of the hydrogen atom's weight, which Dalton knew was the lightest element. By his measurements, 7 grams of oxygen will combine with 1 gram of hydrogen to make 8 grams of water with nothing left over, and assuming a water molecule to be one oxygen atom and one hydrogen atom, he concluded that oxygen's atomic weight is 7. In reality it is 16. Aside from the crudity of early 19th century measurement tools, the main reason for this error was that Dalton didn't know that the water molecule in fact has two hydrogen atoms, not one. Had he known, he would have doubled his estimate to a more accurate 14. This error was corrected in 1811 by Amedeo Avogadro. Avogadro proposed that equal volumes of any two gases, at equal temperature and pressure, contain equal numbers of molecules (in other words, the mass of a gas's particles does not affect the volume that it occupies). Avogadro's hypothesis, now usually called Avogadro's law, provided a method for deducing the relative weights of the molecules of gaseous elements, for if the hypothesis is correct relative gas densities directly indicate the relative weights of the particles that compose the gases. This way of thinking led directly to a second hypothesis: the particles of certain elemental gases were pairs of atoms, and when reacting chemically these molecules often split in two. For instance, the fact that two liters of hydrogen will react with just one liter of oxygen to produce two liters of water vapor (at constant pressure and temperature) suggested that a single oxygen molecule splits in two in order to form two molecules of water. The formula of water is H2O, not HO. Avogadro measured oxygen's atomic weight to be 15.074.\nOpposition to atomic theory.\nDalton's atomic theory attracted widespread interest but not everyone accepted it at first. The law of multiple proportions was shown not to be a universal law when it came to organic substances, whose molecules can be quite large. For instance, in oleic acid there is 34\u00a0g of hydrogen for every 216\u00a0g of carbon, and in methane there is 72\u00a0g of hydrogen for every 216\u00a0g of carbon. 34 and 72 form a ratio of 17:36, which is not a ratio of small whole numbers. We know now that carbon-based substances can have very large molecules, larger than any the other elements can form. Oleic acid's formula is C18H34O2 and methane's is CH4. The law of multiple proportions by itself was not complete proof, and atomic theory was not universally accepted until the end of the 19th century.\nOne problem was the lack of uniform nomenclature. The word \"atom\" implied indivisibility, but Dalton defined an atom as being the ultimate particle of any chemical substance, not just the elements or even matter per se. This meant that \"compound atoms\" such as carbon dioxide could be divided, as opposed to \"elementary atoms\". Dalton disliked the word \"molecule\", regarding it as \"diminutive\". Amedeo Avogadro did the opposite: he exclusively used the word \"molecule\" in his writings, eschewing the word \"atom\", instead using the term \"elementary molecule\". J\u00f6ns Jacob Berzelius used the term \"organic atoms\" to refer to particles containing three or more elements, because he thought this only existed in organic compounds. Jean-Baptiste Dumas used the terms \"physical atoms\" and \"chemical atoms\"; a \"physical atom\" was a particle that cannot be divided by physical means such as temperature and pressure, and a \"chemical atom\" was a particle that could not be divided by chemical reactions.\nThe modern definitions of \"atom\" and \"molecule\"\u2014an atom being the basic particle of an element, and a molecule being an agglomeration of atoms\u2014were established in the late half of the 19th century. A key event was the Karlsruhe Congress in Germany in 1860. As the first international congress of chemists, its goal was to establish some standards in the community. A major proponent of the modern distinction between atoms and molecules was Stanislao Cannizzaro.\nCannizzaro criticized past chemists such as Berzelius for not accepting that the particles of certain gaseous elements are actually pairs of atoms, which led to mistakes in their formulation of certain compounds. Berzelius believed that hydrogen gas and chlorine gas particles are solitary atoms. But he observed that when one liter of hydrogen reacts with one liter of chlorine, they form two liters of hydrogen chloride instead of one. Berzelius decided that Avogadro's law does not apply to compounds. Cannizzaro preached that if scientists just accepted the existence of single-element molecules, such discrepancies in their findings would be easily resolved. But Berzelius did not even have a word for that. Berzelius used the term \"elementary atom\" for a gas particle which contained just one element and \"compound atom\" for particles which contained two or more elements, but there was nothing to distinguish H2 from H since Berzelius did not believe in H2. So Cannizzaro called for a redefinition so that scientists could understand that a hydrogen \"molecule\" can split into two hydrogen \"atoms\" in the course of a chemical reaction.\nA second objection to atomic theory was philosophical. Scientists in the 19th century had no way of directly observing atoms. They inferred the existence of atoms through indirect observations, such as Dalton's law of multiple proportions. Some scientists adopted positions aligned with the philosophy of positivism, arguing that scientists should not attempt to deduce the deeper reality of the universe, but only systemize what patterns they could directly observe.\nThis generation of anti-atomists can be grouped in two camps. \nThe \"equivalentists\", like Marcellin Berthelot, believed the theory of equivalent weights was adequate for scientific purposes. This generalization of Proust's law of definite proportions summarized observations. For example, 1 gram of hydrogen will combine with 8 grams of oxygen to form 9 grams of water, therefore the \"equivalent weight\" of oxygen is 8 grams. The \"energeticist\", like Ernst Mach and Wilhelm Ostwald, were philosophically opposed to hypothesis about reality altogether. In their view, only energy as part of thermodynamics should be the basis of physical models.\nThese positions were eventually quashed by two important advancements that happened later in the 19th century: the development of the periodic table and the discovery that molecules have an internal architecture that determines their properties.\nIsomerism.\nScientists discovered some substances have the exact same chemical content but different properties. For instance, in 1827, Friedrich W\u00f6hler discovered that silver fulminate and silver cyanate are both 107 parts silver, 12 parts carbon, 14 parts nitrogen, and 16 parts oxygen (we now know their formulas as both AgCNO). In 1830 J\u00f6ns Jacob Berzelius introduced the term \"isomerism\" to describe the phenomenon. In 1860, Louis Pasteur hypothesized that the molecules of isomers might have the same set of atoms but in different arrangements.\nIn 1874, Jacobus Henricus van 't Hoff proposed that the carbon atom bonds to other atoms in a tetrahedral arrangement. Working from this, he explained the structures of organic molecules in such a way that he could predict how many isomers a compound could have. Consider, for example, pentane (C5H12). In van 't Hoff's way of modelling molecules, there are three possible configurations for pentane, and scientists did go on to discover three and only three isomers of pentane.\nIsomerism was not something that could be fully explained by alternative theories to atomic theory, such as radical theory and the theory of types.\nMendeleev's periodic table.\nDmitrii Mendeleev noticed that when he arranged the elements in a row according to their atomic weights, there was a certain periodicity to them. For instance, the second element, lithium, had similar properties to the ninth element, sodium, and the sixteenth element, potassium \u2014 a period of seven. Likewise, beryllium, magnesium, and calcium were similar and all were seven places apart from each other on Mendeleev's table. Using these patterns, Mendeleev predicted the existence and properties of new elements, which were later discovered in nature: scandium, gallium, and germanium. Moreover, the periodic table could predict how many atoms of other elements that an atom could bond with \u2014 e.g., germanium and carbon are in the same group on the table and their atoms both combine with two oxygen atoms each (GeO2 and CO2). Mendeleev found these patterns validated atomic theory because it showed that the elements could be categorized by their atomic weight. Inserting a new element into the middle of a period would break the parallel between that period and the next, and would also violate Dalton's law of multiple proportions.\nThe elements on the periodic table were originally arranged in order of increasing atomic weight. However, in a number of places chemists chose to swap the positions of certain adjacent elements so that they appeared in a group with other elements with similar properties. For instance, tellurium is placed before iodine even though tellurium is heavier (127.6 vs 126.9) so that iodine can be in the same column as the other halogens. The modern periodic table is based on atomic number, which is equivalent to the nuclear charge, a change had to wait for the discovery of the nucleus. \nIn addition, an entire row of the table was not shown\nbecause the noble gases had not been discovered when Mendeleev devised his table.\nStatistical mechanics.\nIn 1738, Swiss physicist and mathematician Daniel Bernoulli postulated that the pressure of gases and heat were both caused by the underlying motion of particles. Using his model he could predict the ideal gas law at constant temperature and suggested that the temperature was proportional to the velocity of the particles. These results were largely ignored for a century.\nJames Clerk Maxwell, a vocal proponent of atomism, revived the kinetic theory in 1860 and 1867. His key insight was that the velocity of particles in a gas would vary around an average value, introducing the concept of a distribution function. Ludwig Boltzmann and Rudolf Clausius expanded his work on gases and the laws of thermodynamics especially the second law relating to entropy. In the 1870s, Josiah Willard Gibbs extended the laws of entropy and thermodynamics and coined the term \"statistical mechanics.\"\nBoltzmann defended the atomistic hypothesis against major detractors from the time like Ernst Mach or energeticists like Wilhelm Ostwald, who considered that energy was the elementary quantity of reality.\nAt the beginning of the 20th century, Albert Einstein independently reinvented Gibbs' laws, because they had only been printed in an obscure American journal. Einstein later commented that had he known of Gibbs' work, he would \"not have published those papers at all, but confined myself to the treatment of some few points [that were distinct].\" All of statistical mechanics and the laws of heat, gas, and entropy took the existence of atoms as a necessary postulate.\nBrownian motion.\nIn 1827, the British botanist Robert Brown observed that dust particles inside pollen grains floating in water constantly jiggled about for no apparent reason. In 1905, Einstein theorized that this Brownian motion was caused by the water molecules continuously knocking the grains about, and developed a mathematical model to describe it. This model was validated experimentally in 1908 by French physicist Jean Perrin, who used Einstein's equations to measure the size of atoms.\nDiscovery of the electron.\nAtoms were thought to be the smallest possible division of matter until 1899 when J. J. Thomson discovered the electron through his work on cathode rays.\nA Crookes tube is a sealed glass container in which two electrodes are separated by a vacuum. When a voltage is applied across the electrodes, cathode rays are generated, creating a glowing patch where they strike the glass at the opposite end of the tube. Through experimentation, Thomson discovered that the rays could be deflected by electric fields and magnetic fields, which meant that these rays were not a form of light but were composed of very light charged particles, and their charge was negative. Thomson called these particles \"corpuscles\". He measured their mass-to-charge ratio to be several orders of magnitude smaller than that of the hydrogen atom, the smallest atom. This ratio was the same regardless of what the electrodes were made of and what the trace gas in the tube was.\nIn contrast to those corpuscles, positive ions created by electrolysis or X-ray radiation had mass-to-charge ratios that varied depending on the material of the electrodes and the type of gas in the reaction chamber, indicating they were different kinds of particles.\nIn 1898, Thomson measured the charge on ions to be roughly 6 \u00d7 10\u221210 electrostatic units (2 \u00d7 10\u221219 Coulombs). In 1899, he showed that negative electricity created by ultraviolet light landing on a metal (known now as the photoelectric effect) has the same mass-to-charge ratio as cathode rays; then he applied his previous method for determining the charge on ions to the negative electric particles created by ultraviolet light. By this combination he showed that electron's mass was 0.0014 times that of hydrogen ions. These \"corpuscles\" were so light yet carried so much charge that Thomson concluded they must be the basic particles of electricity, and for that reason other scientists decided that these \"corpuscles\" should instead be called electrons following an 1894 suggestion by George Johnstone Stoney for naming the basic unit of electrical charge.\nIn 1904, Thomson published a paper describing a new model of the atom. Electrons reside within atoms, and they transplant themselves from one atom to the next in a chain in the action of an electrical current. When electrons do not flow, their negative charge logically must be balanced out by some source of positive charge within the atom so as to render the atom electrically neutral. Having no clue as to the source of this positive charge, Thomson tentatively proposed that the positive charge was everywhere in the atom, the atom being shaped like a sphere\u2014this was the mathematically simplest model to fit the available evidence (or lack of it). The balance of electrostatic forces would distribute the electrons throughout this sphere in a more or less even manner. Thomson further explained that ions are atoms that have a surplus or shortage of electrons.\nThomson's model is popularly known as the plum pudding model, based on the idea that the electrons are distributed throughout the sphere of positive charge with the same density as raisins in a plum pudding. Neither Thomson nor his colleagues ever used this analogy. It seems to have been a conceit of popular science writers. The analogy suggests that the positive sphere is like a solid, but Thomson likened it to a liquid, as he proposed that the electrons moved around in it in patterns governed by the electrostatic forces. More to the point, the positive electrification in Thomson's model was an abstraction, he did not propose anything concrete like a particle. Thomson's model was incomplete, it could not predict any of the known properties of the atom such as emission spectra or valencies.\nIn 1906, Robert A. Millikan and Harvey Fletcher performed the oil drop experiment in which they measured the charge of an electron to be about -1.6 \u00d7 10\u221219, a value now defined as -1 \"e\". Since the hydrogen ion and the electron were known to be indivisible and a hydrogen atom is neutral in charge, it followed that the positive charge in hydrogen was equal to this value, i.e. 1 \"e\".\nDiscovery of the nucleus.\nThomson's plum pudding model was challenged in 1911 by one of his former students, Ernest Rutherford, who presented a new model to explain new experimental data. The new model proposed a concentrated center of charge and mass that was later dubbed the atomic nucleus.\nErnest Rutherford and his colleagues Hans Geiger and Ernest Marsden came to have doubts about the Thomson model after they encountered difficulties when they tried to build an instrument to measure the charge-to-mass ratio of alpha particles (these are positively-charged particles emitted by certain radioactive substances such as radium). The alpha particles were being scattered by the air in the detection chamber, which made the measurements unreliable. Thomson had encountered a similar problem in his work on cathode rays, which he solved by creating a near-perfect vacuum in his instruments. Rutherford didn't think he'd run into this same problem because alpha particles usually have much more momentum than electrons. According to Thomson's model of the atom, the positive charge in the atom is not concentrated enough to produce an electric field strong enough to deflect an alpha particle. Yet there was scattering, so Rutherford and his colleagues decided to investigate this scattering carefully.\nBetween 1908 and 1913, Rutherford and his colleagues performed a series of experiments in which they bombarded thin foils of metal with a beam of alpha particles. They spotted alpha particles being deflected by angles greater than 90\u00b0. According to Thomson's model, all of the alpha particles should have passed through with negligible deflection. Rutherford deduced that the positive charge of the atom is not distributed throughout the atom's volume as Thomson believed, but is concentrated in a tiny nucleus at the center. This nucleus also carries most of the atom's mass. Only such an intense concentration of charge, anchored by its high mass, could produce an electric field strong enough to deflect the alpha particles as observed. Rutherford's model, being supported primarily by scattering data unfamiliar to many scientists, did not catch on until Niels Bohr joined Rutherford's lab and developed a new model for the electrons.\nRutherford model predicted that the scattering of alpha particles would be proportional to the square of the atomic charge. Geiger and Marsden's based their analysis on setting the charge to half of the atomic weight of the foil's material (gold, aluminium, etc.). Amateur physicist Antonius van den Broek noted that there was a more precise relation between the charge and the element's numeric sequence in the order of atomic weights. The sequence number came be called the atomic number and it replaced atomic weight in organizing the periodic table.\nBohr model.\nRutherford deduced the existence of the atomic nucleus through his experiments but he had nothing to say about how the electrons were arranged around it. In 1912, Niels Bohr joined Rutherford's lab and began his work on a quantum model of the atom.\nMax Planck in 1900 and Albert Einstein in 1905 had postulated that light energy is emitted or absorbed in discrete amounts known as quanta (singular, \"quantum\"). This led to a series of atomic models with some quantum aspects, such as that of Arthur Erich Haas in 1910 and the 1912 John William Nicholson atomic model with quantized angular momentum as \"h\"/2. The dynamical structure of these models was still classical, but in 1913, Bohr abandon the classical approach. He started his Bohr model of the atom with a quantum hypothesis: an electron could only orbit the nucleus in particular circular orbits with fixed angular momentum and energy, its distance from the nucleus (i.e., their radii) being proportional to its energy. Under this model an electron could not lose energy in a continuous manner; instead, it could only make instantaneous \"quantum leaps\" between the fixed energy levels. When this occurred, light was emitted or absorbed at a frequency proportional to the change in energy (hence the absorption and emission of light in discrete spectra).\nIn a trilogy of papers Bohr described and applied his model to derive the Balmer series of lines in the atomic spectrum of hydrogen and the related spectrum of He+. He also used he model to describe the structure of the periodic table and aspects of chemical bonding. Together these results lead to Bohr's model being widely accepted by the end of 1915.\nBohr's model was not perfect. It could only predict the spectral lines of hydrogen, not those of multielectron atoms. Worse still, it could not even account for all features of the hydrogen spectrum: as spectrographic technology improved, it was discovered that applying a magnetic field caused spectral lines to multiply in a way that Bohr's model couldn't explain. In 1916, Arnold Sommerfeld added elliptical orbits to the Bohr model to explain the extra emission lines, but this made the model very difficult to use, and it still couldn't explain more complex atoms.\nDiscovery of isotopes.\nWhile experimenting with the products of radioactive decay, in 1913 radiochemist Frederick Soddy discovered that there appeared to be more than one variety of some elements. The term isotope was coined by Margaret Todd as a suitable name for these varieties.\nThat same year, J. J. Thomson conducted an experiment in which he channeled a stream of neon ions through magnetic and electric fields, striking a photographic plate at the other end. He observed two glowing patches on the plate, which suggested two different deflection trajectories. Thomson concluded this was because some of the neon ions had a different mass. The nature of this differing mass would later be explained by the discovery of neutrons in 1932: all atoms of the same element contain the same number of protons, while different isotopes have different numbers of neutrons.\nDiscovery of the proton.\nBack in 1815, William Prout observed that the atomic weights of the known elements were multiples of hydrogen's atomic weight, so he hypothesized that all atoms are agglomerations of hydrogen, a particle which he dubbed \"the protyle\". Prout's hypothesis was put into doubt when some elements were found to deviate from this pattern\u2014e.g. chlorine atoms on average weigh 35.45 daltons\u2014but when isotopes were discovered in 1913, Prout's observation gained renewed attention.\nIn 1898, J. J. Thomson found that the positive charge of a hydrogen ion was equal to the negative charge of a single electron.\nIn an April 1911 paper concerning his studies on alpha particle scattering, Ernest Rutherford estimated that the charge of an atomic nucleus, expressed as a multiplier of hydrogen's nuclear charge (\"q\"e), is roughly half the atom's atomic weight.\nIn June 1911, Van den Broek noted that on the periodic table, each successive chemical element increased in atomic weight on average by 2, which in turn suggested that each successive element's nuclear charge increased by 1 \"q\"e. In 1913, van den Broek further proposed that the electric charge of an atom's nucleus, expressed as a multiplier of the elementary charge, is equal to the element's sequential position on the periodic table. Rutherford defined this position as being the element's atomic number.\nIn 1913, Henry Moseley measured the X-ray emissions of all the elements on the periodic table and found that the frequency of the X-ray emissions was a mathematical function of the element's atomic number and the charge of a hydrogen nucleus .\nIn 1917 Rutherford bombarded nitrogen gas with alpha particles and observed hydrogen ions being emitted from the gas. Rutherford concluded that the alpha particles struck the nuclei of the nitrogen atoms, causing hydrogen ions to split off.\nThese observations led Rutherford to conclude that the hydrogen nucleus was a singular particle with a positive charge equal to that of the electron's negative charge. The name \"proton\" was suggested by Rutherford at an informal meeting of fellow physicists in Cardiff in 1920.\nThe charge number of an atomic nucleus was found to be equal to the element's ordinal position on the periodic table. The nuclear charge number thus provided a simple and clear-cut way of distinguishing the chemical elements from each other, as opposed to Lavoisier's classic definition of a chemical element being a substance that cannot be broken down into simpler substances by chemical reactions. The charge number or proton number was thereafter referred to as the atomic number of the element. In 1923, the International Committee on Chemical Elements officially declared the atomic number to be the distinguishing quality of a chemical element.\nDuring the 1920s, some writers defined the atomic number as being the number of \"excess protons\" in a nucleus. Before the discovery of the neutron, scientists believed that the atomic nucleus contained a number of \"nuclear electrons\" which cancelled out the positive charge of some of its protons. This explained why the atomic weights of most atoms were higher than their atomic numbers. Helium, for instance, was thought to have four protons and two nuclear electrons in the nucleus, leaving two excess protons and a net nuclear charge of 2+. After the neutron was discovered, scientists realized the helium nucleus in fact contained two protons and two neutrons.\nDiscovery of the neutron.\nPhysicists in the 1920s believed that the atomic nucleus contained protons plus a number of \"nuclear electrons\" that reduced the overall charge. These \"nuclear electrons\" were distinct from the electrons that orbited the nucleus. This incorrect hypothesis would have explained why the atomic numbers of the elements were less than their atomic weights, and why radioactive elements emit electrons (beta radiation) in the process of nuclear decay. Rutherford even hypothesized that a proton and an electron could bind tightly together into a \"neutral doublet\". Rutherford wrote that the existence of such \"neutral doublets\" moving freely through space would provide a more plausible explanation for how the heavier elements could have formed in the genesis of the Universe, given that it is hard for a lone proton to fuse with a large atomic nucleus because of the repulsive electric field.\nIn 1928, Walter Bothe observed that beryllium emitted a highly penetrating, electrically neutral radiation when bombarded with alpha particles. It was later discovered that this radiation could knock hydrogen atoms out of paraffin wax. Initially it was thought to be high-energy gamma radiation, since gamma radiation had a similar effect on electrons in metals, but James Chadwick found that the ionization effect was too strong for it to be due to electromagnetic radiation, so long as energy and momentum were conserved in the interaction. In 1932, Chadwick exposed various elements, such as hydrogen and nitrogen, to the mysterious \"beryllium radiation\", and by measuring the energies of the recoiling charged particles, he deduced that the radiation was actually composed of electrically neutral particles which could not be massless like the gamma ray, but instead were required to have a mass similar to that of a proton. Chadwick called this new particle \"the neutron\" and believed that it to be a proton and electron fused together because the neutron had about the same mass as a proton and an electron's mass is negligible by comparison. Neutrons are not in fact a fusion of a proton and an electron.\nModern quantum mechanical models.\nIn 1924, Louis de Broglie proposed that all particles\u2014particularly subatomic particles such as electrons\u2014have an associated wave. Erwin Schr\u00f6dinger, fascinated by this idea, developed an equation that describes an electron as a wave function instead of a point. This approach predicted many of the spectral phenomena that Bohr's model failed to explain, but it was difficult to visualize, and faced opposition. One of its critics, Max Born, proposed instead that Schr\u00f6dinger's wave function did not describe the physical extent of an electron (like a charge distribution in classical electromagnetism), but rather gave the probability that an electron would, when measured, be found at a particular point. This reconciled the ideas of wave-like and particle-like electrons: the behavior of an electron, or of any other subatomic entity, has both wave-like and particle-like aspects, and whether one aspect or the other is observed depend upon the experiment.\nA consequence of describing particles as waveforms rather than points is that it is mathematically impossible to calculate with precision both the position and momentum of a particle at a given point in time. This became known as the uncertainty principle, a concept first introduced by Werner Heisenberg in 1927.\nSchr\u00f6dinger's wave model for hydrogen replaced Bohr's model, with its neat, clearly defined circular orbits. The modern model of the atom describes the positions of electrons in an atom in terms of probabilities. An electron can potentially be found at any distance from the nucleus, but, depending on its energy level and angular momentum, exists more frequently in certain regions around the nucleus than others; this pattern is referred to as its atomic orbital. The orbitals come in a variety of shapes\u2014sphere, dumbbell, torus, etc.\u2014with the nucleus in the middle. The shapes of atomic orbitals are found by solving the Schr\u00f6dinger equation. Analytic solutions of the Schr\u00f6dinger equation are known for very few relatively simple model Hamiltonians including the hydrogen atom and the hydrogen molecular ion. Beginning with the helium atom\u2014which contains just two electrons\u2014numerical methods are used to solve the Schr\u00f6dinger equation.\nQualitatively the shape of the atomic orbitals of multi-electron atoms resemble the states of the hydrogen atom. The Pauli principle requires the distribution of these electrons within the atomic orbitals such that no more than two electrons are assigned to any one orbital; this requirement profoundly affects the atomic properties and ultimately the bonding of atoms into molecules."}
{"id": "2846", "revid": "211905", "url": "https://en.wikipedia.org/wiki?curid=2846", "title": "Ai", "text": "AI most frequently refers to artificial intelligence, which is intelligence demonstrated by machines.\nAi, AI or A.I. may also refer to:"}
{"id": "2847", "revid": "24946975", "url": "https://en.wikipedia.org/wiki?curid=2847", "title": "Aung San Suu Kyi", "text": "Aung San Suu Kyi (born 19 June 1945), sometimes abbreviated to Suu Kyi, is a Burmese politician who served as State Counsellor of Myanmar and Minister of Foreign Affairs from 2016 to 2021. She has served as the general secretary of the National League for Democracy (NLD) since the party's founding in 1988 and was registered as its chairperson while it was a legal party from 2011 to 2023. She played a vital role in Myanmar's transition from military junta to partial democracy in the 2010s.\nThe youngest daughter of Aung San, Father of the Nation of modern-day Myanmar, and Khin Kyi, Aung San Suu Kyi was born in Rangoon, British Burma. After graduating from the University of Delhi in 1964 and St Hugh's College, Oxford in 1968, she worked at the United Nations for three years. She married Michael Aris in 1972, with whom she had two children.\nAung San Suu Kyi rose to prominence in the 8888 Uprising of 8\u00a0August 1988 and became the General Secretary of the NLD, which she had newly formed with the help of several retired army officials who criticised the military junta. In the 1990 general election, NLD won 81% of the seats in Parliament, but the results were nullified, as the State Peace and Development Council (SPDC), the military government, refused to hand over power, resulting in an international outcry. She had been detained before the elections and remained under house arrest for almost 15 of the 21 years from 1989 to 2010, becoming one of the world's most prominent political prisoners. In 1999, \"Time\" magazine named her one of the \"Children of Gandhi\" and his spiritual heir to nonviolence. She survived an assassination attempt in the 2003 Depayin massacre when at least 70 people associated with the NLD were killed.\nHer party boycotted the 2010 general election, resulting in a decisive victory for the military-backed Union Solidarity and Development Party (USDP). Aung San Suu Kyi became a Pyithu Hluttaw MP while her party won 43 of the 45 vacant seats in the 2012 by-elections. In the 2015 general election, her party won a landslide victory, taking 86% of the seats in the Pyidaungsu Hluttaw\u2014well more than the 67% supermajority needed to ensure that its preferred candidates were elected president and vice president in the Presidential Electoral College. Although she was prohibited from becoming the president due to a clause in the Myanmar constitution\u2014her late husband and children are foreign citizens\u2014she assumed the newly created role of State Counsellor of Myanmar, a role akin to a prime minister or a head of government.\nWhen she ascended to the office of state counsellor, Aung San Suu Kyi drew criticism from several countries, organisations and figures over Myanmar's inaction in response to the genocide of the Rohingya people in Rakhine State and refusal to acknowledge that the Myanmar's military had committed massacres. Under her leadership, Myanmar also drew criticism for prosecutions of journalists. In 2019, Aung San Suu Kyi appeared in the International Court of Justice where she defended the Myanmar military against allegations of genocide against the Rohingya.\nAung San Suu Kyi, whose party had won the November 2020 Myanmar general election, was arrested on 1\u00a0February 2021 following a coup d'\u00e9tat that returned the Tatmadaw to power and sparked protests across the country. Several charges were filed against her, and on 6\u00a0December 2021, she was sentenced to four years in prison on two of them. Later, on 10\u00a0January 2022, she was sentenced to an additional four years on another set of charges. On 12\u00a0October 2022, she was convicted of two further charges of corruption and she was sentenced to two terms of three years' imprisonment to be served concurrent to each other. On 30\u00a0December 2022, her trials ended with another conviction and an additional sentence of seven years' imprisonment for corruption. Aung San Suu Kyi's final sentence was of 33 years in prison, later reduced to 27 years. The United Nations, most European countries, and the United States condemned the arrests, trials, and sentences as politically motivated.\nName.\n\"Aung San Suu Kyi\", like other Burmese names, includes no surname, but is only a personal name, in her case derived from three relatives: \"Aung San\" from her father, \"Suu\" from her paternal grandmother, and \"Kyi\" from her mother Khin Kyi.\nIn Myanmar, Aung San Suu Kyi is often referred to as \"Daw\" Aung San Suu Kyi. \"Daw\", literally meaning \"aunt\", is not part of her name but is an honorific for any older and revered woman, akin to \"Madam\". She is sometimes addressed as Daw Suu or Amay Suu (\"Mother Suu\") by her supporters.\nPersonal life.\nAung San Suu Kyi was born on 19 June 1945 in Rangoon (now Yangon), British Burma. According to Peter Popham, she was born in a small village outside Rangoon called Hmway Saung. Her father, Aung San, allied with the Japanese during World War II. Aung San founded the modern Burmese army and negotiated Burma's independence from the United Kingdom in 1947; he was assassinated by his rivals in the same year. She is a niece of Thakin Than Tun who was the husband of Khin Khin Gyi, the elder sister of her mother Khin Kyi.\nShe grew up with her mother, Khin Kyi, and two brothers, Aung San Lin and Aung San Oo, in Rangoon. Aung San Lin died at the age of eight when he drowned in an ornamental lake on the grounds of the house. Her elder brother emigrated to San Diego, California, becoming a United States citizen. After Aung San Lin's death, the family moved to a house by Inya Lake where Aung San Suu Kyi met people of various backgrounds, political views, and religions. She was educated in Methodist English High School (now Basic Education High School No. 1 Dagon) for much of her childhood in Burma, where she was noted as having a talent for learning languages. She speaks four languages: Burmese, English (with a British accent), French, and Japanese. She is a Theravada Buddhist.\nAung San Suu Kyi's mother, Khin Kyi, gained prominence as a political figure in the newly formed Burmese government. She was appointed Burmese ambassador to India and Nepal in 1960, and Aung San Suu Kyi followed her there. She studied in the Convent of Jesus and Mary School in New Delhi, and graduated from Lady Shri Ram College, a constituent college of the University of Delhi in New Delhi, with a degree in politics in 1964. Suu Kyi continued her education at St Hugh's College, Oxford, obtaining a B.A. degree in Philosophy, Politics and Economics in 1967, graduating with a third-class degree that was promoted per tradition to an MA in 1968. After graduating, she lived in New York City with family friend Ma Than E, who was once a popular Burmese pop singer. She worked at the United Nations for three years, primarily on budget matters, writing daily to her future husband, Dr. Michael Aris. On 1\u00a0January 1972, Aung San Suu Kyi and Aris, a scholar of Tibetan culture and literature, living abroad in Bhutan, were married. The following year, she gave birth to their first son, Alexander Aris, in London; their second son, Kim Aris, was born in 1977. Between 1985 and 1987, Aung San Suu Kyi was working toward a Master of Philosophy degree in Burmese literature as a research student at the School of Oriental and African Studies (SOAS), University of London. She was elected as an Honorary Fellow of St Hugh's in 1990. For two years, she was a Fellow at the Indian Institute of Advanced Studies (IIAS) in Shimla, India. She also worked for the government of the Union of Burma.\nIn 1988, Aung San Suu Kyi returned to Burma to tend for her ailing mother. Aris' visit in Christmas 1995 was the last time that he and Aung San Suu Kyi met, as she remained in Burma and the Burmese dictatorship denied him any further entry visas. Aris was diagnosed with prostate cancer in 1997 which was later found to be terminal. Despite appeals from prominent figures and organisations, including the United States, UN Secretary-General Kofi Annan and Pope John Paul II, the Burmese government would not grant Aris a visa, saying that they did not have the facilities to care for him, and instead urged Aung San Suu Kyi to leave the country to visit him. She was at that time temporarily free from house arrest but was unwilling to depart, fearing that she would be refused re-entry if she left, as she did not trust the military junta's assurance that she could return.\nAris died on his 53rd birthday on 27 March 1999. Since 1989, when his wife was first placed under house arrest, he had seen her only five times, the last of which was for Christmas in 1995. She was also separated from her children, who live in the United Kingdom, until 2011.\nOn 2 May 2008, after Cyclone Nargis hit Burma, Aung San Suu Kyi's dilapidated lakeside bungalow lost its roof and electricity, while the cyclone also left entire villages in the Irrawaddy delta submerged. Plans to renovate and repair the house were announced in August 2009. Aung San Suu Kyi was released from house arrest on 13\u00a0November 2010.\nPolitical career.\nPolitical beginning.\nCoincidentally, when Aung San Suu Kyi returned to Burma in 1988, the long-time military leader of Burma and head of the ruling party, General Ne Win, stepped down. Mass demonstrations for democracy followed that event on 8\u00a0August 1988 (8-8-88, a day seen as auspicious), which were violently suppressed in what came to be known as the 8888 Uprising. On 24\u00a0August 1988, she made her first public appearance at the Yangon General Hospital, addressing protestors from a podium. On 26\u00a0August, she addressed half a million people at a mass rally in front of the Shwedagon Pagoda in the capital, calling for a democratic government. However, in September 1988, a new military junta took power.\nInfluenced by both Mahatma Gandhi's philosophy of non-violence and also by the Buddhist concepts, Aung San Suu Kyi entered politics to work for democratisation, helped found the National League for Democracy on 27\u00a0September 1988, but was put under house arrest on 20\u00a0July 1989. She was offered freedom if she left the country, but she refused. Despite her philosophy of non-violence, a group of ex-military commanders and senior politicians who joined NLD during the crisis believed that she was too confrontational and left NLD. However, she retained enormous popularity and support among NLD youths with whom she spent most of her time.\nDuring the crisis, the previous democratically elected Prime Minister of Burma, U Nu, initiated to form an interim government and invited opposition leaders to join him. Indian Prime Minister Rajiv Gandhi had signaled his readiness to recognize the interim government. However, Aung San Suu Kyi categorically rejected U Nu's plan by saying \"the future of the opposition would be decided by masses of the people\". Ex-Brigadier General Aung Gyi, another influential politician at the time of the 8888 crisis and the first chairman in the history of the NLD, followed the suit and rejected the plan after Aung San Suu Kyi's refusal. Aung Gyi later accused several NLD members of being communists and resigned from the party.\n1990 general election and Nobel Peace Prize.\nIn 1990, the military junta called a general election, in which the National League for Democracy (NLD) received 59% of the votes, guaranteeing NLD 80% of the parliament seats. Some claim that Aung San Suu Kyi would have assumed the office of Prime Minister. Instead, the results were nullified and the military refused to hand over power, resulting in an international outcry. Aung San Suu Kyi was placed under house arrest at her home on University Avenue () in Rangoon, during which time she was awarded the Sakharov Prize for Freedom of Thought in 1990, and the Nobel Peace Prize one year later. Her sons Alexander and Kim accepted the Nobel Peace Prize on her behalf. Aung San Suu Kyi used the Nobel Peace Prize's US$1.3\u00a0million prize money to establish a health and education trust for the Burmese people. Around this time, Aung San Suu Kyi chose nonviolence as an expedient political tactic, stating in 2007, \"I do not hold to nonviolence for moral reasons, but for political and practical reasons.\"\nThe decision of the Nobel Committee mentions:\nIn 1995 Aung San Suu Kyi delivered the keynote address at the Fourth World Conference on Women in Beijing.\n1996 attack.\nOn 9 November 1996, the motorcade that Aung San Suu Kyi was traveling in with other National League for Democracy leaders Tin Oo and Kyi Maung, was attacked in Yangon. About 200 men swooped down on the motorcade, wielding metal chains, metal batons, stones and other weapons. The car that Aung San Suu Kyi was in had its rear window smashed, and the car with Tin Oo and Kyi Maung had its rear window and two backdoor windows shattered. It is believed the offenders were members of the Union Solidarity and Development Association (USDA) who were allegedly paid Ks.500/- (@ USD $0.50) each to participate. The NLD lodged an official complaint with the police, and according to reports the government launched an investigation, but no action was taken. (Amnesty International 120297)\nHouse arrest.\nAung San Suu Kyi was placed under house arrest for a total of 15 years over a 21-year period, on numerous occasions, since she began her political career, during which time she was prevented from meeting her party supporters and international visitors. In an interview, she said that while under house arrest she spent her time reading philosophy, politics and biographies that her husband had sent her. She also passed the time playing the piano and was occasionally allowed visits from foreign diplomats as well as from her personal physician.\nAlthough under house arrest, Aung San Suu Kyi was granted permission to leave Burma under the condition that she never return, which she refused: \"As a mother, the greater sacrifice was giving up my sons, but I was always aware of the fact that others had given up more than me. I never forget that my colleagues who are in prison suffer not only physically, but mentally for their families who have no security outside\u2014in the larger prison of Burma under authoritarian rule.\"\nThe media were also prevented from visiting Aung San Suu Kyi, as occurred in 1998 when journalist Maurizio Giuliano, after photographing her, was stopped by customs officials who then confiscated all his films, tapes and some notes. In contrast, Aung San Suu Kyi did have visits from government representatives, such as during her autumn 1994 house arrest when she met the leader of Burma, Senior General Than Shwe and General Khin Nyunt on 20\u00a0September in the first meeting since she had been placed in detention. On several occasions during her house arrest, she had periods of poor health and as a result was hospitalised.\nThe Burmese government detained and kept Aung San Suu Kyi imprisoned because it viewed her as someone \"likely to undermine the community peace and stability\" of the country, and used both Article 10(a) and 10(b) of the 1975 State Protection Act (granting the government the power to imprison people for up to five years without a trial), and Section 22 of the \"Law to Safeguard the State Against the Dangers of Those Desiring to Cause Subversive Acts\" as legal tools against her. She continuously appealed her detention, and many nations and figures continued to call for her release and that of 2,100 other political prisoners in the country. On 12\u00a0November 2010, days after the junta-backed Union Solidarity and Development Party (USDP) won elections conducted after a gap of 20 years, the junta finally agreed to sign orders allowing Aung San Suu Kyi's release, and her house arrest term came to an end on 13\u00a0November 2010.\nUnited Nations involvement.\nThe United Nations (UN) has attempted to facilitate dialogue between the junta and Aung San Suu Kyi. On 6\u00a0May 2002, following secret confidence-building negotiations led by the UN, the government released her; a government spokesman said that she was free to move \"because we are confident that we can trust each other\". Aung San Suu Kyi proclaimed \"a new dawn for the country\". However, on 30\u00a0May 2003 in an incident similar to the 1996 attack on her, a government-sponsored mob attacked her caravan in the northern village of Depayin, murdering and wounding many of her supporters. Aung San Suu Kyi fled the scene with the help of her driver, Kyaw Soe Lin, but was arrested upon reaching Ye-U. The government imprisoned her at Insein Prison in Rangoon. After she underwent a hysterectomy in September 2003, the government again placed her under house arrest in Rangoon.\nThe results from the UN facilitation have been mixed; Razali Ismail, UN special envoy to Burma, met with Aung San Suu Kyi. Ismail resigned from his post the following year, partly because he was denied re-entry to Burma on several occasions. Several years later in 2006, Ibrahim Gambari, UN Undersecretary-General (USG) of Department of Political Affairs, met with Aung San Suu Kyi, the first visit by a foreign official since 2004. He also met with her later the same year. On 2\u00a0October 2007 Gambari returned to talk to her again after seeing Than Shwe and other members of the senior leadership in Naypyidaw. State television broadcast Aung San Suu Kyi with Gambari, stating that they had met twice. This was Aung San Suu Kyi's first appearance in state media in the four years since her current detention began.\nThe United Nations Working Group for Arbitrary Detention published an Opinion that Aung San Suu Kyi's deprivation of liberty was arbitrary and in contravention of Article 9 of the Universal Declaration of Human Rights 1948, and requested that the authorities in Burma set her free, but the authorities ignored the request at that time. The U.N. report said that according to the Burmese Government's reply, \"Daw Aung San Suu Kyi has not been arrested, but has only been taken into protective custody, for her own safety\", and while \"it could have instituted legal action against her under the country's domestic legislation\u00a0... it has preferred to adopt a magnanimous attitude, and is providing her with protection in her own interests\".\nSuch claims were rejected by Brigadier-General Khin Yi, Chief of Myanmar Police Force (MPF). On 18\u00a0January 2007, the state-run paper \"New Light of Myanmar\" accused Aung San Suu Kyi of tax evasion for spending her Nobel Prize money outside the country. The accusation followed the defeat of a US-sponsored United Nations Security Council resolution condemning Burma as a threat to international security; the resolution was defeated because of strong opposition from China, which has strong ties with the military junta (China later voted against the resolution, along with Russia and South Africa).\nIn November 2007, it was reported that Aung San Suu Kyi would meet her political allies National League for Democracy along with a government minister. The ruling junta made the official announcement on state TV and radio just hours after UN special envoy Ibrahim Gambari ended his second visit to Burma. The NLD confirmed that it had received the invitation to hold talks with Aung San Suu Kyi. However, the process delivered few concrete results.\nOn 3 July 2009, UN Secretary-General Ban Ki-moon went to Burma to pressure the junta into releasing Aung San Suu Kyi and to institute democratic reform. However, on departing from Burma, Ban Ki-moon said he was \"disappointed\" with the visit after junta leader Than Shwe refused permission for him to visit Aung San Suu Kyi, citing her ongoing trial. Ban said he was \"deeply disappointed that they have missed a very important opportunity\".\n2007 anti-government protests.\nProtests led by Buddhist monks during Saffron Revolution began on 19\u00a0August 2007 following steep fuel price increases, and continued each day, despite the threat of a crackdown by the military.\nOn 22 September 2007, although still under house arrest, Aung San Suu Kyi made a brief public appearance at the gate of her residence in Yangon to accept the blessings of Buddhist monks who were marching in support of human rights. It was reported that she had been moved the following day to Insein Prison (where she had been detained in 2003), but meetings with UN envoy Ibrahim Gambari near her Rangoon home on 30\u00a0September and 2\u00a0October established that she remained under house arrest.\n2009 trespass incident.\nOn 3 May 2009, an American man, identified as John Yettaw, swam across Inya Lake to her house uninvited and was arrested when he made his return trip three days later. He had attempted to make a similar trip two years earlier, but for unknown reasons was turned away. He later claimed at trial that he was motivated by a divine vision requiring him to notify her of an impending terrorist assassination attempt. On 13\u00a0May, Aung San Suu Kyi was arrested for violating the terms of her house arrest because the swimmer, who pleaded exhaustion, was allowed to stay in her house for two days before he attempted the swim back. Aung San Suu Kyi was later taken to Insein Prison, where she could have faced up to five years' confinement for the intrusion. The trial of Aung San Suu Kyi and her two maids began on 18\u00a0May and a small number of protesters gathered outside. Diplomats and journalists were barred from attending the trial; however, on one occasion, several diplomats from Russia, Thailand and Singapore and journalists were allowed to meet Aung San Suu Kyi. The prosecution had originally planned to call 22 witnesses. It also accused John Yettaw of embarrassing the country. During the ongoing defence case, Aung San Suu Kyi said she was innocent. The defence was allowed to call only one witness (out of four), while the prosecution was permitted to call 14 witnesses. The court rejected two character witnesses, NLD members Tin Oo and Win Tin, and permitted the defence to call only a legal expert. According to one unconfirmed report, the junta was planning to, once again, place her in detention, this time in a military base outside the city. In a separate trial, Yettaw said he swam to Aung San Suu Kyi's house to warn her that her life was \"in danger\". The national police chief later confirmed that Yettaw was the \"main culprit\" in the case filed against Aung San Suu Kyi. According to aides, Aung San Suu Kyi spent her 64th birthday in jail sharing biryani rice and chocolate cake with her guards.\nHer arrest and subsequent trial received worldwide condemnation by the UN Secretary General Ban Ki-moon, the United Nations Security Council, Western governments, South Africa, Japan and the Association of Southeast Asian Nations, of which Burma is a member. The Burmese government strongly condemned the statement, as it created an \"unsound tradition\" and criticised Thailand for meddling in its internal affairs. The Burmese Foreign Minister Nyan Win was quoted in the state-run newspaper \"New Light of Myanmar\" as saying that the incident \"was trumped up to intensify international pressure on Burma by internal and external anti-government elements who do not wish to see the positive changes in those countries' policies toward Burma\". Ban responded to an international campaign by flying to Burma to negotiate, but Than Shwe rejected all of his requests.\nOn 11 August 2009, the trial concluded with Aung San Suu Kyi being sentenced to imprisonment for three years with hard labour. This sentence was commuted by the military rulers to further house arrest of 18 months. On 14\u00a0August, US Senator Jim Webb visited Burma, visiting with junta leader General Than Shwe and later with Aung San Suu Kyi. During the visit, Webb negotiated Yettaw's release and deportation from Burma. Following the verdict of the trial, lawyers of Aung San Suu Kyi said they would appeal against the 18-month sentence. On 18\u00a0August, United States President Barack Obama asked the country's military leadership to set free all political prisoners, including Aung San Suu Kyi. In her appeal, Aung San Suu Kyi had argued that the conviction was unwarranted. However, her appeal against the August sentence was rejected by a Burmese court on 2\u00a0October 2009. Although the court accepted the argument that the 1974 constitution, under which she had been charged, was null and void, it also said the provisions of the 1975 security law, under which she has been kept under house arrest, remained in force. The verdict effectively meant that she would be unable to participate in the elections scheduled to take place in 2010\u2014the first in Burma in two decades. Her lawyer stated that her legal team would pursue a new appeal within 60 days.\nLate 2000s: International support for release.\nAung San Suu Kyi has received vocal support from Western nations in Europe, Australia and North and South America, as well as India, Israel, Japan the Philippines and South Korea. In December 2007, the US House of Representatives voted unanimously 400\u20130 to award Aung San Suu Kyi the Congressional Gold Medal; the Senate concurred on 25\u00a0April 2008. On 6\u00a0May 2008, President George W. Bush signed legislation awarding Aung San Suu Kyi the Congressional Gold Medal. She is the first recipient in American history to receive the prize while imprisoned. More recently, there has been growing criticism of her detention by Burma's neighbours in the Association of Southeast Asian Nations (ASEAN), particularly from Indonesia, Thailand, the Philippines and Singapore. At one point Malaysia warned Burma that it faced expulsion from ASEAN as a result of the detention of Aung San Suu Kyi. Other nations including South Africa, Bangladesh and the Maldives also called for her release. The United Nations has urged the country to move towards inclusive national reconciliation, the restoration of democracy, and full respect for human rights. In December 2008, the United Nations General Assembly passed a resolution condemning the human rights situation in Burma and calling for Aung San Suu Kyi's release\u201480 countries voting for the resolution, 25 against and 45 abstentions. Other nations, such as China and Russia, are less critical of the regime and prefer to cooperate only on economic matters. Indonesia has urged China to push Burma for reforms. However, Samak Sundaravej, former Prime Minister of Thailand, criticised the amount of support for Aung San Suu Kyi, saying that \"Europe uses Aung San Suu Kyi as a tool. If it's not related to Aung San Suu Kyi, you can have deeper discussions with Myanmar.\"\nVietnam, however, did not support calls by other ASEAN member states for Myanmar to free Aung San Suu Kyi, state media reported Friday, 14\u00a0August 2009. The state-run Vi\u1ec7t Nam News said Vietnam had no criticism of Myanmar's decision 11\u00a0August 2009 to place Aung San Suu Kyi under house arrest for the next 18 months, effectively barring her from elections scheduled for 2010. \"It is our view that the Aung San Suu Kyi trial is an internal affair of Myanmar\", Vietnamese government spokesman Le Dung stated on the website of the Ministry of Foreign Affairs. In contrast with other ASEAN member states, Dung said Vietnam has always supported Myanmar and hopes it will continue to implement the \"roadmap to democracy\" outlined by its government.\nNobel Peace Prize winners (Archbishop Desmond Tutu, the Dalai Lama, Shirin Ebadi, Adolfo P\u00e9rez Esquivel, Mairead Corrigan, Rigoberta Mench\u00fa, Prof. Elie Wiesel, US President Barack Obama, Betty Williams, Jody Williams and former US President Jimmy Carter) called for the rulers of Burma to release Aung San Suu Kyi to \"create the necessary conditions for a genuine dialogue with Daw Aung San Suu Kyi and all concerned parties and ethnic groups to achieve an inclusive national reconciliation with the direct support of the United Nations\". Some of the money she received as part of the award helped fund higher education grants to Burmese students through the London-based charity Prospect Burma.\nIt was announced prior to the 2010 Burmese general election that Aung San Suu Kyi may be released \"so she can organize her party\", However, Aung San Suu Kyi was not allowed to run. On 1\u00a0October 2010 the government announced that she would be released on 13\u00a0November 2010.\nUS President Barack Obama personally advocated the release of all political prisoners, especially Aung San Suu Kyi, during the US-ASEAN Summit of 2009.\nThe US Government hoped that successful general elections would be an optimistic indicator of the Burmese government's sincerity towards eventual democracy. The Hatoyama government which spent 2.82\u00a0billion yen in 2008, has promised more Japanese foreign aid to encourage Burma to release Aung San Suu Kyi in time for the elections; and to continue moving towards democracy and the rule of law.\nIn a personal letter to Aung San Suu Kyi, UK Prime Minister Gordon Brown cautioned the Burmese government of the potential consequences of rigging elections as \"condemning Burma to more years of diplomatic isolation and economic stagnation\".\nAung San Suu Kyi met with many heads of state and opened a dialog with the Minister of Labor Aung Kyi (not to be confused with Aung San Suu Kyi). She was allowed to meet with senior members of her NLD party at the State House, however these meetings took place under close supervision.\n2010 release.\nOn the evening of 13 November 2010, Aung San Suu Kyi was released from house arrest. This was the date her detention had been set to expire according to a court ruling in August 2009 and came six days after a widely criticised general election. She appeared in front of a crowd of her supporters, who rushed to her house in Rangoon when nearby barricades were removed by the security forces. Aung San Suu Kyi had been detained for 15 of the past 21 years. The government newspaper \"New Light of Myanmar\" reported the release positively, saying she had been granted a pardon after serving her sentence \"in good conduct\". \"The New York Times\" suggested that the military government may have released Aung San Suu Kyi because it felt it was in a confident position to control her supporters after the election.\nHer son Kim Aris was granted a visa in November 2010 to see his mother shortly after her release, for the first time in 10 years. He visited again on 5\u00a0July 2011, to accompany her on a trip to Bagan, her first trip outside Yangon since 2003. Her son visited again on 8\u00a0August 2011, to accompany her on a trip to Pegu, her second trip.\nDiscussions were held between Aung San Suu Kyi and the Burmese government during 2011, which led to a number of official gestures to meet her demands. In October, around a tenth of Burma's political prisoners were freed in an amnesty and trade unions were legalised.\nIn November 2011, following a meeting of its leaders, the NLD announced its intention to re-register as a political party to contend 48 by-elections necessitated by the promotion of parliamentarians to ministerial rank. Following the decision, Aung San Suu Kyi held a telephone conference with US President Barack Obama, in which it was agreed that Secretary of State Hillary Clinton would make a visit to Burma, a move received with caution by Burma's ally China. On 1\u00a0December 2011, Aung San Suu Kyi met with Hillary Clinton at the residence of the top-ranking US diplomat in Yangon.\nOn 21 December 2011, Thai Prime Minister Yingluck Shinawatra met Aung San Suu Kyi in Yangon, marking Aung San Suu Kyi's \"first-ever meeting with the leader of a foreign country\".\nOn 5 January 2012, British Foreign Minister William Hague met Aung San Suu Kyi and his Burmese counterpart. This represented a significant visit for Aung San Suu Kyi and Burma. Aung San Suu Kyi studied in the UK and maintains many ties there, whilst Britain is Burma's largest bilateral donor.\nDuring Aung San Suu Kyi's visit to Europe, she visited the Swiss parliament, collected her 1991 Nobel Prize in Oslo and her honorary degree from the University of Oxford.\n2012 by-elections.\nIn December 2011, there was speculation that Aung San Suu Kyi would run in the 2012 national by-elections to fill vacant seats. On 18\u00a0January 2012, Aung San Suu Kyi formally registered to contest a Pyithu Hluttaw (lower house) seat in the Kawhmu Township constituency in special parliamentary elections to be held on 1\u00a0April 2012. The seat was previously held by Soe Tint, who vacated it after being appointed Construction Deputy Minister, in the 2010 election. She ran against Union Solidarity and Development Party candidate Soe Min, a retired army physician and native of Twante Township.\nOn 3 March 2012, at a large campaign rally in Mandalay, Aung San Suu Kyi unexpectedly left after 15 minutes, because of exhaustion and airsickness.\nIn an official campaign speech broadcast on Burmese state television's MRTV on 14\u00a0March 2012, Aung San Suu Kyi publicly campaigned for reform of the 2008 Constitution, removal of restrictive laws, more adequate protections for people's democratic rights, and establishment of an independent judiciary. The speech was leaked online a day before it was broadcast. A paragraph in the speech, focusing on the Tatmadaw's repression by means of law, was censored by authorities.\nAung San Suu Kyi also called for international media to monitor the by-elections, while publicly pointing out irregularities in official voter lists, which include deceased individuals and exclude other eligible voters in the contested constituencies. On 21\u00a0March 2012, Aung San Suu Kyi was quoted as saying \"Fraud and rule violations are continuing and we can even say they are increasing.\"\nWhen asked whether she would assume a ministerial post if given the opportunity, she said the following:\nOn 26 March 2012, Aung San Suu Kyi suspended her nationwide campaign tour early, after a campaign rally in Myeik (Mergui), a coastal town in the south, citing health problems due to exhaustion and hot weather.\nOn 1 April 2012, the NLD announced that Aung San Suu Kyi had won the vote for a seat in Parliament. A news broadcast on state-run MRTV, reading the announcements of the Union Election Commission, confirmed her victory, as well as her party's victory in 43 of the 45 contested seats, officially making Aung San Suu Kyi the Leader of the Opposition in the Pyidaungsu Hluttaw.\nAlthough she and other MP-elects were expected to take office on 23\u00a0April when the Hluttaws resumed session, National League for Democracy MP-elects, including Aung San Suu Kyi, said they might not take their oaths because of its wording; in its present form, parliamentarians must vow to \"safeguard\" the constitution. In an address on Radio Free Asia, she said \"We don't mean we will not attend the parliament, we mean we will attend only after taking the oath\u00a0... Changing that wording in the oath is also in conformity with the Constitution. I don't expect there will be any difficulty in doing it.\"\nOn 2 May 2012, National League for Democracy MP-elects, including Aung San Suu Kyi, took their oaths and took office, though the wording of the oath was not changed. According to the \"Los Angeles Times\", \"Suu Kyi and her colleagues decided they could do more by joining as lawmakers than maintaining their boycott on principle.\"\nOn 9\u00a0July 2012, she attended the Parliament for the first time as a lawmaker.\n2015 general election.\nOn 16 June 2012, Aung San Suu Kyi was finally able to deliver her Nobel acceptance speech (Nobel lecture) at Oslo's City Hall, two decades after being awarded the peace prize. In September 2012, Aung San Suu Kyi received in person the United States Congressional Gold Medal, which is the highest Congressional award. Although she was awarded this medal in 2008, at the time she was under house arrest, and was unable to receive the medal. Aung San Suu Kyi was greeted with bipartisan support at Congress, as part of a coast-to-coast tour in the United States. In addition, Aung San Suu Kyi met President Barack Obama at the White House. The experience was described by Aung San Suu Kyi as \"one of the most moving days of my life\". In 2014, she was listed as the 61st-most-powerful woman in the world by \"Forbes\".\nOn 6 July 2012, Aung San Suu Kyi announced on the World Economic Forum's website that she wanted to run for the presidency in Myanmar's 2015 elections. The current Constitution, which came into effect in 2008, bars her from the presidency because she is the widow and mother of foreigners\u2014provisions that appeared to be written specifically to prevent her from being eligible.\nThe NLD won a sweeping victory in those elections, winning at least 255 seats in the House of Representatives and 135 seats in the House of Nationalities. In addition, Aung San Suu Kyi won re-election to the House of Representatives. Under the 2008 constitution, the NLD needed to win at least a two-thirds majority in both houses to ensure that its candidate would become president. Before the elections, Aung San Suu Kyi announced that even though she is constitutionally barred from the presidency, she would hold the real power in any NLD-led government. On 30\u00a0March 2016 she became Minister for the President's Office, for Foreign Affairs, for Education and for Electric Power and Energy in President Htin Kyaw's government; later she relinquished the latter two ministries and President Htin Kyaw appointed her State Counsellor, a position akin to a Prime Minister created especially for her. The position of State Counsellor was approved by the House of Nationalities on 1\u00a0April 2016 and the House of Representatives on 5\u00a0April 2016. The next day, her role as State Counsellor was established.\nState counsellor and foreign minister (2016\u20132021).\nAs soon as she became foreign minister, she invited Chinese Foreign Minister Wang Yi, Canadian Foreign Minister Stephane Dion and Italian Foreign Minister Paolo Gentiloni in April and Japanese Foreign Minister Fumio Kishida in May and discussed how to have good diplomatic relationships with these countries.\nInitially, upon accepting the State Counsellor position, she granted amnesty to the students who were arrested for opposing the National Education Bill, and announced the creation of the commission on Rakhine State, which had a long record of persecution of the Muslim Rohingya minority. However, soon Aung San Suu Kyi's government did not manage with the ethnic conflicts in Shan and Kachin states, where thousands of refugees fled to China, and by 2017 the persecution of the Rohingya by the government forces escalated to the point that it is not uncommonly called a genocide. Aung San Suu Kyi, when interviewed, has denied the allegations of ethnic cleansing. She has also refused to grant citizenship to the Rohingya, instead taking steps to issue ID cards for residency but no guarantees of citizenship.\nHer tenure as State Counsellor of Myanmar has drawn international criticism for her failure to address her country's economic and ethnic problems, particularly the plight of the Rohingya following the 25 August 2017 ARSA attacks (described as \"certainly one of the biggest refugee crises and cases of ethnic cleansing since the Second World War\"), for the weakening of freedom of the press and for her style of leadership, described as imperious and \"distracted and out of touch\".\nDuring the COVID-19 pandemic in Myanmar, Suu Kyi chaired a National Central Committee responsible for coordinating the country's pandemic response.\nResponse to the genocide of Rohingya Muslims and refugees.\nIn 2017, critics called for Aung San Suu Kyi's Nobel prize to be revoked, citing her silence over the genocide of Rohingya people in Myanmar.\nSome activists criticised Aung San Suu Kyi for her silence on the 2012 Rakhine State riots (later repeated during the 2015 Rohingya refugee crisis), and her indifference to the plight of the Rohingya, Myanmar's persecuted Muslim minority. In 2012, she told reporters she did not know if the Rohingya could be regarded as Burmese citizens. In a 2013 interview with the BBC's Mishal Husain, Aung San Suu Kyi did not condemn violence against the Rohingya and denied that Muslims in Myanmar have been subject to ethnic cleansing, insisting that the tensions were due to a \"climate of fear\" caused by \"a worldwide perception that global Muslim power is 'very great. She did condemn \"hate of any kind\" in the interview. According to Peter Popham, in the aftermath of the interview, she expressed anger at being interviewed by a Muslim. Husain had challenged Aung San Suu Kyi that almost all of the impact of violence was against the Rohingya, in response to Aung San Suu Kyi's claim that violence was happening on both sides, and Peter Popham described her position on the issue as one of purposeful ambiguity for political gain.\nHowever, she said that she wanted to work towards reconciliation and she cannot take sides as violence has been committed by both sides. According to \"The Economist\", her \"halo has even slipped among foreign human-rights lobbyists, disappointed at her failure to make a clear stand on behalf of the Rohingya minority\". However, she has spoken out \"against a ban on Rohingya families near the Bangladeshi border having more than two children\".\nIn a 2015 BBC News article, reporter Jonah Fisher suggested that Aung San Suu Kyi's silence over the Rohingya issue is due to a need to obtain support from the majority Bamar ethnicity as she is in \"the middle of a general election campaign\". In May 2015, the Dalai Lama publicly called upon her to do more to help the Rohingya in Myanmar, claiming that he had previously urged her to address the plight of the Rohingya in private during two separate meetings and that she had resisted his urging. In May 2016, Aung San Suu Kyi asked the newly appointed United States Ambassador to Myanmar, Scot Marciel, not to refer to the Rohingya by that name as they \"are not recognized as among the 135 official ethnic groups\" in Myanmar. This followed Bamar protests at Marciel's use of the word \"Rohingya\".\nIn 2016, Aung San Suu Kyi was accused of failing to protect Myanmar's Rohingya Muslims during the Rohingya genocide. State crime experts from Queen Mary University of London warned that Aung San Suu Kyi is \"legitimising genocide\" in Myanmar. Despite continued persecution of the Rohingya well into 2017, Aung San Suu Kyi was \"not even admitting, let alone trying to stop, the army's well-documented campaign of rape, murder and destruction against Rohingya villages\". On 4\u00a0September 2017, Yanghee Lee, the UN's special rapporteur on human rights in Myanmar, criticised Aung San Suu Kyi's response to the \"really grave\" situation in Rakhine, saying: \"The de facto leader needs to step in\u2014that is what we would expect from any government, to protect everybody within their own jurisdiction.\" The BBC reported that \"Her comments came as the number of Rohingya fleeing to Bangladesh reached 87,000, according to UN estimates\", adding that \"her sentiments were echoed by Nobel Peace laureate Malala Yousafzai, who said she was waiting to hear from Ms Suu Kyi\u2014who has not commented on the crisis since it erupted\". The next day George Monbiot, writing in \"The Guardian\", called on readers to sign a change.org petition to have the Nobel peace prize revoked, criticising her silence on the matter and asserting \"whether out of prejudice or out of fear, she denies to others the freedoms she rightly claimed for herself. Her regime excludes\u2014and in some cases seeks to silence\u2014the very activists who helped to ensure her own rights were recognised.\" The Nobel Foundation replied that there existed no provision for revoking a Nobel Prize. Archbishop Desmond Tutu, a fellow peace prize holder, also criticised Aung San Suu Kyi's silence: in an open letter published on social media, he said: \"If the political price of your ascension to the highest office in Myanmar is your silence, the price is surely too steep\u00a0... It is incongruous for a symbol of righteousness to lead such a country.\" On 13\u00a0September it was revealed that Aung San Suu Kyi would not be attending a UN General Assembly debate being held the following week to discuss the humanitarian crisis, with a Myanmar's government spokesman stating \"perhaps she has more pressing matters to deal with\".\nIn October 2017, Oxford City Council announced that, following a unanimous cross-party vote, the honour of Freedom of the City, granted in 1997 in recognition of her \"long struggle for democracy\", was to be withdrawn following evidence emerging from the United Nations which meant that she was \"no longer worthy of the honour\". A few days later, Munsur Ali, a councillor for City of London Corporation, tabled a motion to rescind the Freedom of the City of London: the motion was supported by Catherine McGuinness, chair of the corporation's policy and resources committee, who expressed \"distress\u00a0... at the situation in Burma and the atrocities committed by the Burmese military\". On 13\u00a0November 2017, Bob Geldof returned his Freedom of the City of Dublin award in protest over Aung San Suu Kyi also holding the accolade, stating that he does not \"wish to be associated in any way with an individual currently engaged in the mass ethnic cleansing of the Rohingya people of north-west Burma\". Calling Aung San Suu Kyi a \"handmaiden to genocide\", Geldof added that he would take pride in his award being restored if it is first stripped from her. The Dublin City Council voted 59\u20132 (with one abstention) to revoke Aung San Suu Kyi's Freedom of the City award over Myanmar's treatment of the Rohingya people in December 2017, though Lord Mayor of Dublin M\u00edche\u00e1l Mac Donncha denied the decision was influenced by protests by Geldof and members of U2. At the same meeting, the Councillors voted 37\u20137 (with 5 abstentions) to remove Geldof's name from the Roll of Honorary Freemen.\nIn March 2018, the United States Holocaust Memorial Museum revoked Aung San Suu Kyi's Elie Wiesel Award, awarded in 2012, citing her failure \"to condemn and stop the military's brutal campaign\" against Rohingya Muslims.\nIn May 2018, Aung San Suu Kyi was considered complicit in the crimes against Rohingyas in a report by Britain's International Development Committee.\nIn August 2018, it was revealed that Aung San Suu Kyi would be stripped of her Freedom of Edinburgh award over her refusal to speak out against the crimes committed against the Rohingya. She had received the award in 2005 for promoting peace and democracy in Burma. This will be only the second time that anyone has ever been stripped of the award, after Charles Stewart Parnell lost it in 1890 due to a salacious affair. Also in August, a UN report, while describing the violence as genocide, added that Aung San Suu Kyi did as little as possible to prevent it.\nIn early October 2018, both the Canadian Senate and its House of Commons voted unanimously to strip Aung San Suu Kyi of her honorary citizenship. This decision was caused by the Government of Canada's determination that the treatment of the Rohingya by Myanmar's government amounts to genocide.\nOn 11 November 2018, Amnesty International announced it was revoking her Ambassador of Conscience award.\nIn December 2019, Aung San Suu Kyi appeared in the International Court of Justice at The Hague where she defended the Burmese military against allegations of genocide against the Rohingya. In a speech of over 3,000 words, Aung San Suu Kyi did not use the term \"Rohingya\" in describing the ethnic group. She stated that the allegations of genocide were \"incomplete and misleading\", claiming that the situation was actually a Burmese military response to attacks by the Arakan Rohingya Salvation Army (ARSA). She also questioned how there could be \"genocidal intent\" when the Burmese government had opened investigations and also encouraged Rohingya to return after being displaced. However, experts have largely criticised the Burmese investigations as insincere, with the military declaring itself innocent and the government preventing a visit from investigators from the United Nations. Many Rohingya have also not returned due to perceiving danger and a lack of rights in Myanmar.\nIn January 2020, the International Court of Justice decided that there was a \"real and imminent risk of irreparable prejudice to the rights\" of the Rohingya. The court also took the view that the Burmese government's efforts to remedy the situation \"do not appear sufficient\" to protect the Rohingya. Therefore, the court ordered the Burmese government to take \"all measures within its power\" to protect the Rohingya from genocidal actions. The court also instructed the Burmese government to preserve evidence and report back to the court at timely intervals about the situation.\nArrests and prosecution of journalists.\nIn December 2017, two Reuters journalists, Wa Lone and Kyaw Soe Oo, were arrested while investigating the Inn Din massacre of Rohingyas. Suu Kyi publicly commented in June 2018 that the journalists \"weren't arrested for covering the Rakhine issue\", but because they had broken Myanmar's Official Secrets Act. As the journalists were then on trial for violating the Official Secrets Act, Aung San Suu Kyi's presumption of their guilt was criticised by rights groups for potentially influencing the verdict. American diplomat Bill Richardson said that he had privately discussed the arrest with Suu Kyi, and that Aung San Suu Kyi reacted angrily and labelled the journalists \"traitors\". A police officer testified that he was ordered by superiors to use entrapment to frame and arrest the journalists; he was later jailed and his family evicted from their home in the police camp. The judge found the journalists guilty in September 2018 and to be jailed for seven years. Aung San Suu Kyi reacted to widespread international criticism of the verdict by stating: \"I don't think anyone has bothered to read\" the judgement as it had \"nothing to do with freedom of expression at all\", but the Official Secrets Act. She also challenged critics to \"point out where there has been a miscarriage of justice\", and told the two Reuters journalists that they could appeal their case to a higher court.\nIn September 2018, the Office of the United Nations High Commissioner for Human Rights issued a report that since Aung San Suu Kyi's party, the NLD, came to power, the arrests and criminal prosecutions of journalists in Myanmar by the government and military, under laws which are too vague and broad, have \"made it impossible for journalists to do their job without fear or favour.\"\n2021 arrest and trial.\nOn 1 February 2021, Aung San Suu Kyi was arrested and deposed by the Myanmar's military, along with other leaders of her National League for Democracy (NLD) party, after Myanmar's military declared the November 2020 general election results fraudulent. A 1\u00a0February court order authorised her detainment for 15 days, stating that soldiers searching her Naypyidaw villa had uncovered imported communications equipment lacking proper paperwork. Aung San Suu Kyi was transferred to house arrest on the same evening, and on 3\u00a0February was formally charged with illegally importing ten or more walkie-talkies. She faces up to three years in prison for the charges. According to \"The New York Times\", the charge \"echoed previous accusations of esoteric legal crimes (and) arcane offenses\" used by the military against critics and rivals. As of 9\u00a0February, Aung San Suu Kyi continues to be held incommunicado, without access to international observers or legal representation of her choice.\nUS President Joe Biden raised the threat of new sanctions as a result of the Myanmar's military coup. In a statement, the UN Secretary-General Ant\u00f3nio Guterres believes \"These developments represent a serious blow to democratic reforms in Myanmar.\" Volkan Bozk\u0131r, President of the UN General Assembly, also voiced his concerns, having tweeted \"Attempts to undermine democracy and rule of law are unacceptable\", and called for the \"immediate release\" of the detained NLD party leaders.\nOn 1 April 2021, Aung San Suu Kyi was charged with the fifth offence in relation to violating the official secrets act. According to her lawyer, it is the most serious charge brought against her after the coup and could carry a sentence of up to 14 years in prison if convicted. On 12\u00a0April 2021, Aung San Suu Kyi was hit with another charge, this time \"under section 25 of the natural disaster management law\". According to her lawyer, it is her sixth indictment. She appeared in court via video link and now faces five charges in the capital Naypyidaw and one in Yangon.\nOn 28 April 2021, the National Unity Government (NUG), in which Aung San Suu Kyi symbolically retained her position, anticipated that there would be no talks with the junta until all political prisoners, including her, are set free. This move by her supporters come after an ASEAN-supported consensus with the junta leadership in the past days. However, on 8\u00a0May 2021, the junta designated NUG as a terrorist organisation and warned citizens not to cooperate, nor to give aid to the parallel government, stripping Aung San Suu Kyi of her symbolic position. On 10\u00a0May 2021, her lawyer said she would appear in court in person for the first time since her arrest after the Supreme Court ruled that she could attend in person and meet her lawyers. She had been previously only allowed to do so remotely from her home. On 21\u00a0May 2021, a military junta commission was formed to dissolve Aung San Suu Kyi's National League for Democracy (NLD) on grounds of election fraud in the November 2020 election. On 22\u00a0May 2021, during his first interview since the coup, junta leader Min Aung Hlaing reported that she was in good health at her home and that she would appear in court in a matter of days. On 23\u00a0May 2021, the European Union expressed support for Aung San Suu Kyi's party and condemned the commission aimed at dissolving the party, echoing the NLD's statement released earlier in the week.\nOn 24 May 2021, Aung San Suu Kyi appeared in person in court for the first time since the coup to face the \"incitement to sedition\" charge against her. During the 30-minute hearing, she said that she was not fully aware of what was going on outside as she had no access to full information from the outside and refused to respond on the matters. She was also quoted on the possibility of her party's forced dissolution as \"Our party grew out of the people so it will exist as long as people support it.\" In her meeting with her lawyers, Aung San Suu Kyi also wished people \"good health\".\nOn 2 June 2021, it was reported that the military had moved her (as well as Win Myint) from their homes to an unknown location.\nOn 10 June 2021, Aung San Suu Kyi was charged with corruption, the most serious charge brought against her, which carries a maximum penalty of 15 years' imprisonment. Aung San Suu Kyi's lawyers say the charges are made to keep her out of the public eye.\nOn 14 June 2021, the trial against Aung San Suu Kyi began. Any conviction would prevent her from running for office again. Aung San Suu Kyi's lawyers attempted to have prosecution testimony against her on the sedition charge disqualified but the motion was denied by the judge.\nOn 13 September 2021, court proceedings were to resume against her, but it was postponed due to Aung San Suu Kyi presenting \"minor health issues\" that impeded her from attending the court in person.\nOn 4 October 2021, Aung San Suu Kyi asked the judge to reduce her times of court appearances because of her fragile health. Aung San Suu Kyi described her health as \"strained\".\nIn November, the Myanmar courts deferred the first verdicts in the trial without further explanation or giving dates. In the same month, she was again charged with corruption, related to the purchase and rental of a helicopter, bringing the total of charges to nearly a dozen.\nOn 6 December 2021, Suu Kyi was sentenced to 4 years in jail. Suu Kyi, who is still facing multiple charges and further sentences, was sentenced on the charge of inciting dissent and violating COVID-19 protocols. Following a partial pardon by the chief of the military government, Aung San Suu Kyi's four-year sentence was reduced to two years' imprisonment.\nOn 10 January 2022, the military court in Myanmar sentenced Suu Kyi to an additional four years in prison on a number of charges including \"importing and owning walkie-talkies\" and \"breaking coronavirus rules\". The trials, which are closed to the public, the media, and any observers, were described as a \"courtroom circus of secret proceedings on bogus charges\" by the deputy director for Asia of Human Rights Watch.\nOn 27 April 2022, Aung San Suu Kyi was sentenced to five years in jail on corruption charges.\nOn 22 June 2022, junta authorities ordered that all further legal proceedings against Suu Kyi will take place in prison venues, instead of a courtroom. No explanation of the decision was given. Citing unidentified sources, the BBC reported that Suu Kyi was also moved on 22\u00a0June from house arrest, where she had had close companions, to solitary confinement in a specially-built area inside a prison in Naypyidaw. This is the same prison in which Win Myint had similarly been placed in solitary confinement. The military confirmed that Suu Kyi had been moved to prison.\nOn 15 August 2022, sources following Aung San Suu Kyi's court proceedings said that she was sentenced to an additional six years' imprisonment after being found guilty on four corruption charges, bringing her overall sentences to 17 years in prison. In September 2022, she was convicted of election fraud and breaching the state's secrets act and sentenced to a total of six years in prison for both convictions, increasing her overall sentence to 23 years in prison. By 12 October 2022, she had been sentenced to 26 years imprisonment on ten charges in total, including five corruption charges. On 30\u00a0December 2022, her trials ended with another conviction and an additional sentence of seven years' imprisonment for corruption. Aung San Suu Kyi's final sentence is of 33 years in prison.\nOn 12 July 2023, Thailand's foreign minister Don Pramudwinai said at the ASEAN Foreign Ministers' Meeting in Jakarta that he met with Aung San Suu Kyi during his visit to Myanmar. On 1\u00a0August 2023, the military junta granted Suu Kyi a partial pardon, reducing her sentence to a total of 27 years in prison. Prior to the pardon, she was moved from prison to a VIP government residence, according to an official from NLD party.\nHowever, it was reported that since the beginning of September 2023, she is back in prison. The exact time when she was sent back to prison is unknown. Since January, Aung San Suu Kyi and her lawyers are trying to get six corruption charges overturned. To this date, the requests are repeatedly denied.\nOn 16 April 2024, the military announced that Aung San Suu Kyi had been transferred to house arrest due to a heat wave. However, pro-democracy publications such as \"The Irrawaddy\" claimed that she remains in prison, with air conditioners being added to her cell.\nPolitical beliefs.\nAsked what democratic models Myanmar could look to, she said: \"We have many, many lessons to learn from various places, not just the Asian countries like South Korea, Taiwan, Mongolia, and Indonesia.\" She also cited \"Eastern Europe and countries, which made the transition from communist autocracy to democracy in the 1980s and 1990s, and the Latin American countries, which made the transition from military governments. And we cannot of course forget South Africa, because although it wasn't a military regime, it was certainly an authoritarian regime.\" She added: \"We wish to learn from everybody who has achieved a transition to democracy, and also\u00a0... our great strong point is that, because we are so far behind everybody else, we can also learn which mistakes we should avoid.\"\nIn a nod to the deep US political divide between Republicans led by Mitt Romney and the Democrats by Barack Obama\u2014then battling to win the 2012 presidential election\u2014she stressed, \"Those of you who are familiar with American politics I'm sure understand the need for negotiated compromise.\"\nIn popular culture.\nThe life of Aung San Suu Kyi and her husband Michael Aris is portrayed in Luc Besson's 2011 film \"The Lady\", in which they are played by Michelle Yeoh and David Thewlis. Yeoh visited Aung San Suu Kyi in 2011 before the film's release in November. In the John Boorman's 1995 film \"Beyond Rangoon\", Aung San Suu Kyi was played by Adelle Lutz.\nIrish songwriters Damien Rice and Lisa Hannigan released in 2005 the single \"Unplayed Piano\", in support of the Free Aung San Suu Kyi 60th Birthday Campaign that was happening at the time.\nIrish rock band U2 wrote the song \"Walk On\" in tribute to Aung San Suu Kyi. It is the fourth track on their tenth studio album, \"All That You Can't Leave Behind\" (2000), and would later be issued as a single. Lead singer Bono is wearing a t-shirt with her image and name on the front in their official video of the song. \"Walk On\" won Record of the Year at the 2002 Grammy Awards, that also featured U2 performing the song. Bono publicised her plight during the U2 360\u00b0 Tour, 2009\u20132011.\nSaxophonist Wayne Shorter composed a song titled \"Aung San Suu Kyi\". It appears on his albums \"1+1\" (with pianist Herbie Hancock) and \"Footprints Live!\".\nHealth problems.\nAung San Suu Kyi underwent surgery for a gynecological condition in September 2003 at Asia Royal Hospital during her house arrest. She also underwent minor foot surgery in December 2013 and eye surgery in April 2016. In June 2012, her doctor Tin Myo Win said that she had no serious health problems, but weighed only , had low blood pressure, and could become weak easily.\nAfter being arrested and detained on 1 February 2021, there were concerns that Aung San Suu Kyi's health was deteriorating. However, according to military's spokesperson Zaw Min Tun, special attention is given to her health and living condition. Don Pramudwinai also said that \"she was in good health, both physically and mentally\".\nAlthough a junta spokesperson claimed that she is in good health, since being sent back to prison in September 2023, it is reported that her health condition is worsening and she is \"suffering of toothache and unable to eat\". Her request to see a dentist had been denied. Her son is urging the junta to allow Aung San Suu Kyi to receive medical assistance.\nExternal links.\n (withdrawn 2018)"}
{"id": "2851", "revid": "1271818753", "url": "https://en.wikipedia.org/wiki?curid=2851", "title": "Abraham Joshua Heschel", "text": "Abraham Joshua Heschel (January 11, 1907\u00a0\u2013 December 23, 1972) was a Polish-American rabbi and one of the leading Jewish theologians and Jewish philosophers of the 20th century. Heschel, a professor of Jewish mysticism at the Jewish Theological Seminary of America, authored a number of widely read books on Jewish philosophy and was a leader in the U.S. civil rights movement.\nBiography.\nAbraham Joshua Heschel was born in Warsaw in 1907, the youngest of six children of Moshe Mordechai Heschel and Reizel Perlow Heschel. He was descended from preeminent European rabbis on both sides of his family. His paternal great-great-grandfather and namesake was Rebbe Avraham Yehoshua Heshel of Apt in present-day Poland. His mother was also a descendant of Avraham Yehoshua Heshel and other Hasidic dynasties. His siblings were Sarah, Dvora Miriam, Esther Sima, Gittel, and Jacob. Their father Moshe died of influenza in 1916 when Abraham was nine. He was tutored by a Gerrer Hasid who introduced him to the thought of Rabbi Menachem Mendel of Kotzk.\nAfter a traditional yeshiva education and studying for Orthodox rabbinical ordination (semicha), Heschel pursued his doctorate at the University of Berlin and rabbinic ordination at the non-denominational Hochschule f\u00fcr die Wissenschaft des Judentums. There he studied under notable scholars including Hanoch Albeck, Ismar Elbogen, Julius Guttmann, Alexander Guttmann, and Leo Baeck. His mentor in Berlin was David Koigen. Heschel later taught Talmud at the Hochschule. He joined a Yiddish poetry group, Jung Vilna, and in 1933, published a volume of Yiddish poems, \"Der Shem Hamefoyrosh: Mentsch,\" dedicated to his father.\nIn late October 1938, while living in a rented room in the home of a Jewish family in Frankfurt, Heschel was arrested by the Gestapo and deported to Poland in the Polenaktion. He spent ten months lecturing on Jewish philosophy and Torah at Warsaw's Institute for Jewish Studies. Six weeks before the German invasion of Poland, Heschel fled Warsaw for London with the help of Julian Morgenstern, president of Hebrew Union College, and Alexander Guttmann, an eventual colleague at the Hebrew Union College, who secretly re-wrote Heschel's ordination certificate to meet American visa requirements.\nHeschel's sister Esther was killed in a German bombing. His mother was murdered by the Nazis, and two other sisters, Gittel and Devorah, died in Nazi concentration camps. He never returned to Germany, Austria or Poland. He once wrote, \"If I should go to Poland or Germany, every stone, every tree would remind me of contempt, hatred, murder, of children killed, of mothers burned alive, of human beings asphyxiated.\"\nHeschel arrived in New York City in March 1940. He soon left for Cincinnati, serving on the faculty of Hebrew Union College (HUC), the main seminary of Reform Judaism, for five years. In 1946 he returned to New York, taking a position with the Jewish Theological Seminary of America (JTS), the main seminary of Conservative Judaism. He remained with JTS as professor of Jewish ethics and Mysticism until his death in 1972. At the time of his death, Heschel lived near JTS at 425 Riverside Drive in Manhattan.\nHeschel married Sylvia Straus, a concert pianist, on December 10, 1946, in Los Angeles. Their daughter, Susannah Heschel, became a Jewish scholar in her own right.\nIdeology.\nHeschel explicated many facets of Jewish thought, including studies on medieval Jewish philosophy, Kabbalah, and Hasidic philosophy. According to some scholars, he was more interested in spirituality than in critical text study; the latter was a specialty of many scholars at JTS. He was not given a graduate assistant for many years and he was mainly relegated to teach in the education school or the Rabbinical school, not in the academic graduate program. Heschel became friendly with his colleague Mordecai Kaplan. Though they differed in their approaches to Judaism, they had a very cordial relationship and visited each other's homes from time to time.\nHeschel believed that the teachings of the Hebrew prophets were a clarion call for social action in the United States and, inspired by this belief, he worked for African Americans' civil rights and spoke out against the Vietnam War.\nHe also criticized what he specifically called \"pan-halakhism,\" or an exclusive focus upon religiously compatible behavior to the neglect of the non-legalistic dimension of rabbinic tradition.\nHeschel is notable as a recent proponent of what one scholar calls the \"Nachmanidean\" school of Jewish thought - emphasizing the mutually dependent relationship between God and man - as opposed to the \"Maimonidean\" school in which God is independent and unchangeable. In Heschel's language, the \"Maimonidean\" perspective is associated with Rabbi Yishmael and the \"Nachmanidean\" perspective with Rabbi Akiva; according to Heschel neither perspective should be adopted in isolation, but rather both are interwoven with the other.\nHeschel described kabbalah as an outgrowth of classical rabbinic sources that describe God's dependence on man to implement the divine plan for the world. This contrasts with scholars like Gershon Scholem who saw kabbalah as reflecting the influence of non-Jewish thought. While Scholem's school focused on the metaphysics and history of kabbalistic thought, Heschel focused on kabbalistic descriptions of the human religious experience. In recent years, a growing body of kabbalah scholarship has followed Heschel's emphasis on the mystical experience of kabbalah and on its continuity with earlier Jewish sources.\nInfluence outside Judaism.\nHeschel is a widely read Jewish theologian whose most influential works include \"Man Is Not Alone\", \"God in Search of Man\", \"The Sabbath,\" and \"The Prophets\". As a representative of American Jews at the Second Vatican Council, Heschel persuaded the Catholic Church to eliminate or modify passages in its liturgy that demeaned Jews, or referred to an expected conversion of the Jewish people to Christianity. His theological works argued that religious experience is a fundamentally human impulse, not just a Jewish one. He believed that no religious community could claim a monopoly on religious truth. For these and other reasons, Martin Luther King Jr. called Heschel \"a truly great prophet.\" Heschel actively participated in the Civil Rights movement, and was a participant in the third Selma to Montgomery march, accompanying Dr. King and John Lewis.\nPublished works.\n\"Man Is Not Alone\" (1951).\n\"Man Is Not Alone: A Philosophy of Religion\" offers Heschel's views on how people can comprehend God. Judaism views God as being radically different from humans, so Heschel explores the ways that Judaism teaches that a person may have an encounter with the ineffable. A recurring theme in this work is the radical amazement people feel when experiencing the presence of the Divine. Heschel then explores the problems of doubts and faith, what Judaism means by teaching that God is one, the essence of humanity and the problem of human needs, the definition of religion in general and Judaism in particular, and human yearning for spirituality. He offers his views as to Judaism being a pattern for life.\n\"The Sabbath\" (1951).\n\"The Sabbath: Its Meaning for Modern Man\" is a work on the nature and celebration of Shabbat, the Jewish Sabbath. It is rooted in the thesis that Judaism is a religion of time, not space, and that the Sabbath symbolizes the sanctification of time. For Heschel, \"Technical civilization is man's conquest of space. It is a triumph frequently achieved by sacrificing an essential ingredient of existence, namely, time.\u201d While he wrote that \u201cto enhance our power in the world of space is our main objective,\u201d he also warned that while \u201cwe have often suffered from degradation by poverty, now we are threatened with degradation through power.\"\n\"God in Search of Man\" (1955).\n\"God in Search of Man: A Philosophy of Judaism\" is a companion volume to \"Man Is Not Alone\" in which Heschel discusses the nature of religious thought, how thought becomes faith, and how faith creates responses in the believer. He discusses ways people can seek God's presence and the radical amazement we receive in return. He offers a criticism of nature worship, a study of humanity's metaphysical loneliness, and his view that we can consider God in search of humanity. The first section concludes with a study of Jews as a chosen people. Section two deals with the idea of revelation and what it means for one to be a prophet. This section gives us his idea of revelation as an event instead of a process. This relates to Israel's commitment to God. Section three discusses his views on how a Jew should understand the nature of Judaism as a religion. He discusses and rejects the idea that mere faith (without law) alone is enough but then cautions rabbis against adding too many restrictions to Jewish law. He discusses the need to correlate ritual observance with spirituality and love and the importance of Kavanah (intention) when performing mitzvot. He discusses religious behaviorism\u2014when people strive for external compliance with the law, yet disregard the importance of inner devotion.\n\"The Prophets\" (1962).\nThis work started as Heschel's PhD thesis in German, which he later expanded and translated into English. Originally published in a two-volume edition, \"The Prophets\" studies the books of the Hebrew prophets. It covers their lives and the historical context of their missions, summarizes their work, and discusses their psychological state. Heschel puts forward a central idea in his theology: that the prophetic (and, ultimately, Jewish) view of God is best understood not as anthropomorphic (that God takes human form) but as anthropopathic\u2014that God has human feelings.\nIn \"The Prophets\", Heschel describes the Jewish prophets' unique aspect compared to similar figures. Whereas other nations have soothsayers and diviners who attempt to discover the will of their gods, Heschel asserts, the Hebrew prophets are characterized by their experience of what he calls theotropism\u2014God turning towards humanity. Heschel argues for the view of Hebrew prophets as receivers of the \"Divine Pathos,\" of the wrath and sorrow of God over his nation that has forsaken him. In this view, prophets do not speak for God so much as they remind their audience of God's voice for the voiceless, the poor, and the oppressed.\nHe writes:\n\"Torah min HaShamayim\" (1962).\nMany consider Heschel's \"Torah min HaShamayim BeAspaklariya shel HaDorot\", (\"Torah from Heaven in the mirror of the generations\") to be his masterwork. The three volumes of this work are a study of classical rabbinic theology and aggadah, as opposed to halakha (Jewish law). It explores the views of the rabbis in the Mishnah, Talmud, and Midrash about the nature of Torah, the revelation of God to humankind, prophecy, and the ways that Jews have used scriptural exegesis to expand and understand these core Jewish texts. In this work, Heschel views the 2nd-century sages Rabbi Akiva and Ishmael ben Elisha as paradigms for the two dominant world-views in Jewish theology\nTwo Hebrew volumes were published during his lifetime by Soncino Press, and the third was published posthumously by JTS Press in the 1990s. A new edition, including an expanded third volume, due to manuscripts that were found and edited by Dr. Dror Bondi, was published by Magid Press in 2021. An English translation of all three volumes, with notes, essays, and appendices, was translated and edited by Rabbi Gordon Tucker, entitled \"Heavenly Torah: As Refracted Through the Generations\". It can be the subject of intense study and analysis, providing insight into the relationship between God and humans beyond the world of Judaism and all monotheisms.\n\"Who is Man?\" (1965).\nHere, Heschel discusses the nature and role of man. In these three lectures, originally delivered in somewhat different form as The Raymond Fred West Memorial Lectures at Stanford University in May 1963, Dr. Heschel inquires into the logic of being human: What is meant by being human? What are the grounds on which to justify a human being's claim to being human? The author says, \u201cWe have never been as openmouthed and inquisitive, never as astonished and embarrassed at our ignorance about man. We know what he makes, but we do not know what he is or what to expect of him. Is it not conceivable that our entire civilization is built upon a misinterpretation of man? Or that the tragedy of man is due to the fact that he is a being who has forgotten the question: Who is Man? The failure to identify himself, to know what is authentic human existence, leads him to assume a false identity, to pretend to be what he is unable to be or to not accepting what is at the very root of his being. Ignorance about man is not lack of knowledge, but false knowledge.\u201d\n\"Prophetic Inspiration After the Prophets\" (1966).\nHeschel wrote a series of articles, originally in Hebrew, on the existence of prophecy in Judaism after the destruction of the Holy Temple in Jerusalem in 70 CE. These essays were translated into English and published as \"Prophetic Inspiration After the Prophets: Maimonides and Others\" by the American Judaica publisher Ktav.\nThe publisher of this book states, \"The standard Jewish view is that prophecy ended with the ancient prophets, somewhere early in the Second Temple era. Heschel demonstrated that this view is not altogether accurate. Belief in the possibility of continued prophetic inspiration, and belief in its actual occurrence existed throughout much of the medieval period, and it even exists in modern times. Heschel's work on prophetic inspiration in the Middle Ages originally appeared in two long Hebrew articles. In them, he concentrated on the idea that prophetic inspiration was even possible in post-Talmudic times, and, indeed, it had taken place at various schools in various times, from the Geonim to Maimonides and beyond.\"\nAwards and commemoration.\n1970: National Jewish Book Award in the Jewish Thought category for \"Israel: An Echo of Eternity\"\nFive schools have been named for Heschel: in Buenos Aires, Argentina the rabbinical school of the Seminario Rabinico Latinoamericano; on the Upper West Side of New York City, the A J Heschel School; in California the Abraham Joshua Heschel Day School is located in Northridge, while the Heschel West Day School is located in Agoura Hills; and The Toronto Heschel School in Toronto, Ontario, Canada.\nIn 2009, a Missouri highway was named \"Dr. Abraham Joshua Heschel Highway\" to subvert the plans of a Springfield, Missouri-area Neo-Nazi group who cleaned the stretch of highway as part of an \"Adopt-A-Highway\" program. Heschel's daughter, Susannah, has objected to the adoption of her father's name in this context.\nHeschel's papers are held in the Rubenstein Rare Book &amp; Manuscript Library at Duke University.\nOn 17 October 2022, John Paul II Catholic University of Lublin inaugurated the Abraham J. Heschel Center for Catholic-Jewish Relations, attended by Catholic and Jewish figures, including Rabbi Abraham Skorka, Susannah Heschel, Latin Patriarch of Jerusalem Archbishop Pierbattista Pizzaballa, and Archbishop Stanis\u0142aw Budzik of Lublin. Pope Francis has welcomed the establishment of the Heschel Center."}
{"id": "2853", "revid": "35874198", "url": "https://en.wikipedia.org/wiki?curid=2853", "title": "Aberdeen Bestiary", "text": "The Aberdeen Bestiary (Aberdeen University Library, Univ Lib. MS 24) is a 12th-century English illuminated manuscript bestiary that was first listed in 1542 in the inventory of the Old Royal Library at the Palace of Westminster. Due to similarities, it is often considered to be the \"sister\" manuscript of the Ashmole Bestiary. The connection between the ancient Greek didactic text \"Physiologus\" and similar bestiary manuscripts is also often noted. Information about the manuscript's origins and patrons are circumstantial, although the manuscript most likely originated from the 13th century and was owned by a wealthy ecclesiastical patron from northern or southern England. Currently, the Aberdeen Bestiary resides in the Aberdeen University Library in Scotland.\nHistory.\nThe Aberdeen Bestiary and the Ashmole Bestiary are considered by Xenia Muratova, a professor of Art History, to be \"the work of different artists belonging to the same artistic milieu.\" Due to their \"striking similarities\" they are often compared and described by scholars as being \"sister manuscripts.\" The medievalist scholar M. R. James considered the Aberdeen Bestiary \"a replica of Ashmole 1511\" a view echoed by many other art historians.\nProvenance.\nThe original patron of both the Aberdeen and Ashmole Bestiary was considered to be a high-ranking member of society such as a prince, king or another high ranking church official or monastery. However, since the section related to monastery life that was commonly depicted within the Aviarium manuscript was missing the original patron remains uncertain but it appears less likely to be a church member. The Aberdeen Bestiary was kept in Church and monastic settings for a majority of its history. However at some point it entered into the English royal collections library. The royal Westminster Library shelf stamp of Henry VIII of England is stamped on the side of the bestiary. How King Henry acquired the manuscript remains unknown although it was probably taken from a monastery. The manuscript appears to have been well-read by the family based on the amount of reading wear on the edges of the pages. Around the time King James of Scotland became the King of England the bestiary was passed along to Marischal College in Aberdeen, Scotland. The manuscript is in fragmented condition as many illuminations on folios were removed individually as miniatures likely not for monetary but possibly for personal reasons. The manuscript currently is in the Aberdeen Library in Scotland where it has remained since 1542.\nDescription.\nMaterials.\nThe Aberdeen bestiary is a gilded, decorated manuscript featuring large miniatures and some of the finest pigment, parchment and gold leaf from its time. Some portions of the manuscript such as folio eight recto even feature tarnished silver leaf. The original patron was wealthy enough to afford such materials so that the artists and scribes could enjoy creative freedom while creating the manuscripts. The artists were professionally trained and experimented with new techniques - such as heavy washes mixed with light washes and dark thick lines and use of contrasting color. The aqua color that is in the Aberdeen Bestiary is not present in the Ashmole Bestiary. The Aberdeen manuscript is loaded with filigree flora design and \"champie\" style gold leaf initials. Canterbury is considered to be the original location of manufacture as the location was well known for manufacturing high-end luxury books during the thirteen century. Its similarities with the Canterbury Paris Psalter tree style also further draws evidence of this relation.\nStyle.\nThe craftsmanship of both Ashmole and Aberdeen bestiary suggest similar artists and scribes. Both the Ashmole and Aberdeen bestiary were probably made within 10 years of each other due to their stylistic and material similarities and the fact that both are crafted with the finest materials of their time. Stylistically both manuscripts are very similar but the Aberdeen has figures that are both more voluminous and less energetic than those of the Ashmole Bestiary. The color usage has been suggested as potentially Biblical in meaning as color usage had different interpretations in the early 13th century. The overall style of the human figures as well as color usage is very reminiscent of Roman mosaic art especially with the attention to detail in the drapery. Circles and ovals semi-realistically depict highlights throughout the manuscript. The way that animals are shaded in a Romanesque fashion with the use of bands to depict volume and form, which is similar to an earlier 12th-century Bury Bible made at Bury St.Edmunds. This Bestiary also shows stylistic similarities with the Paris Psalters of Canterbury. The Aviary section is similar to the Aviariium which is a well-known 12th century monastic text. The deviation from traditional color usage can be seen in the tiger, satyr, and unicorn folios as well as many other folios. The satyr in the Aberdeen Bestiary when compared to the satyr section of the slightly older Worksop bestiary is almost identical. There are small color notes in the Aberdeen Bestiary that are often seen in similar manuscripts dating between 1175 and 1250 which help indicate that it was made near the year 1200 or 1210. These notes are similar to many other side notes written on the sides of pages throughout the manuscript and were probably by the painter to remind himself of special circumstances, these note occur irregularly throughout the text.\nIlluminations.\nFolio page 1 to 3 recto depicts the Genesis 1:1-25 which is represented with a large full page illumination Biblical Creation scene in the manuscript. Folio 5 recto shows Adam, a large figure surrounded by gold leaf and towering over others, with the theme of 'Adam naming the animals' - this starts the compilation of the bestiary portion within the manuscript. Folio 5 verso depicts quadrupeds, livestock, wild beasts, and the concept of the herd. Folio 7 to 18 recto depicts large cats and other beasts such as wolves, foxes and dogs. Many pages from the start of the manuscript's bestiary section such as 11 verso featuring a hyena shows small pin holes which were likely used to map out and copy artwork to a new manuscript. Folio 20 verso to 28 recto depicts livestock such as sheep, horses, and goats. Small animals like cats and mice are depicted on folio 24 to 25. Pages 25 recto to 63 recto feature depictions of birds and folio 64 recto to 80 recto depicts reptiles, worms and fish. 77 recto to 91 verso depicts trees and plants and other elements of nature such as the nature of man. The end folios of the manuscript from 93 recto to 100 recto depicts the nature of stones and rocks.\nSeventeen of the Aberdeen manuscript pages are pricked for transfer in a process called pouncing such as clearly seen in the hyena folio as well as folio 3 recto and 3 verso depicting Genesis 1:26-1:28, 31, 1:1-2. The pricking must have been done shortly after the creation of the Adam and Eve folio pages since there is not damage done to nearby pages. Other pages used for pouncing include folio 7 recto to 18 verso which is the beginning of the beasts portion of the manuscript and likely depicted a lions as well as other big cats such as leopards, panthers and their characteristic as well as other large wild and domesticated beasts.\nMissing Folios.\nOn folio 6 recto there was likely intended to be a depiction of a lion as in the Ashmole bestiary, but in this instance the pages were left blank although there are markings of margin lines. In comparison to the Ashmole bestiary, on 9 verso some leaves are missing which should have likely contained imagery of the antelope (\"Antalops\"), unicorn (\"Unicornis\"), lynx (\"Lynx\"), griffin (\"Gryps\"), part of elephant (\"Elephans\"). Near folio 21 verso two illuminations of the ox (\"Bos\"), camel (\"Camelus\"), dromedary (\"Dromedarius\"), ass (\"Asinus\"), onager (\"Onager\") and part of horse (\"Equus\") are also assumed to be missing. Also missing from folio 15 recto on are some leaves which should have contained crocodile (\"Crocodilus\"), manticore (\"Mantichora\") and part of parandrus (\"Parandrus\"). These missing folios are assumed from comparisons between the Ashmole and other related bestiaries."}
{"id": "2855", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2855", "title": "ALADI", "text": ""}
{"id": "2856", "revid": "82432", "url": "https://en.wikipedia.org/wiki?curid=2856", "title": "Latin American Integration Association", "text": "The Latin American Integration Association / Asociaci\u00f3n Latinoamericana de Integraci\u00f3n / Associa\u00e7\u00e3o Latino-Americana de Integra\u00e7\u00e3o (LAIA / ALADI) is an international and regional scope organization. It was created on 12 August 1980 by the 1980 Montevideo Treaty, replacing the Latin American Free Trade Association (LAFTA/ALALC). Currently, it has 13 member countries, and any of the Latin American States may apply for accession.\nObjectives.\nThe development of the integration process developed within the framework of the ALADI aims at promoting the harmonious and balanced socio-economic development of the region, and its long-term objective is the gradual and progressive establishment of a Latin-American single market.\nIntegration mechanisms.\nThe ALADI promotes the establishment of an area of economic preferences within the region, in order to create a Latin-American common market, through three mechanisms:\nThe Relatively Less Economically Developed Countries of the region (Bolivia, Ecuador and Paraguay) benefit from a preferential system, through the lists of markets opening offered by the countries in favor of the Relatively Less Economically Developed Countries; special programs of cooperation (business rounds, pre-investment, financing, technological support); and countervailing measures in favor of the land-locked countries, the full participation of such countries in the integration process is sought.\nThe ALADI includes in its legal structure the strongest sub-regional, plurilateral and bilateral integration agreements arising in growing numbers in the continent. As a result, the ALADI \u2013 as an institutional and legal framework or \u201cumbrella\u201d of the regional integration- develops actions in order to support and foster these efforts for the progressive establishment of a common economic space.\nAccession of other Latin American countries.\nThe 1980 Montevideo Treaty is open to the accession of any Latin-American country. On 26 August 1999, the first accession to the 1980 Montevideo Treaty was executed, with the incorporation of the Republic of Cuba as a member country of the ALADI. On 10 May 2012, the Republic of Panama became the thirteenth member country of the ALADI. Likewise, the accession of the Republic of Nicaragua was accepted in the Sixteenth Meeting of the Council of Ministers (Resolution 75 (XVI)), held on 11 August 2011. \nCurrently, Nicaragua moves towards the fulfillment of conditions for becoming a member country of the ALADI.\nThe ALADI opens its field of actions for the rest of Latin America through multilateral links or partial agreements with other countries and integration areas of the continent (Article 25). The Latin-American Integration Association also contemplates the horizontal cooperation with other integration movements in the world and partial actions with third developing countries or their respective integration areas (Article 27).\nInstitutional structure.\nThe Council of Ministers is the supreme body of the ALADI, and adopts the decisions for the superior political management of the integration process.\nIt is constituted by the Ministers of Foreign Affairs of the member countries. Notwithstanding, when one of such member countries assigns the competence of the integration affairs to a different Minister or Secretary of State, the member countries may be represented, with full powers, by the respective Minister or Secretary. It is convened by the Committee of Representatives, meets and makes decisions with the presence of all the member countries.\nIt is in charge, among others, of analyzing the functioning of the integration process in all its aspects, promoting the convergence of the partial scope agreements seeking their progressive multilateralization, and promoting greater scope actions as regards economic integration. It is made up of Plenipotentiaries of the member countries.\nIt is the permanent political body and negotiating forum of the ALADI, where all the initiatives for the fulfillment of the objectives established by the 1980 Montevideo Treaty are analyzed and agreed on. It is composed of a Permanent Representative of each member country with right to one vote and an Alternate Representative. It meets regularly every 15 days and its Resolutions are adopted by the affirmative vote of two thirds of the member countries.\nIt is the technical body of the ALADI, and it may propose, evaluate, study and manage for the fulfillment of the objectives of the ALADI. It is composed of technical and administrative personnel, and directed by a Secretary-General, who has the support of two Undersecretaries, elected for a three-year period, renewable for the same term."}
{"id": "2858", "revid": "30394555", "url": "https://en.wikipedia.org/wiki?curid=2858", "title": "Aircraft spotting", "text": "Aircraft spotting, or planespotting, is a hobby consisting of observing and tracking aircraft, which is usually accomplished by photography or videography. Besides monitoring aircraft, planespotting enthusiasts (who are usually called planespotters) also record information regarding airports, air traffic control communications, airline routes, and more.\nHistory and evolution.\nAviation enthusiasts have been watching airplanes and other aircraft since aviation began. However, as a hobby (distinct from active/wartime work), planespotting did not appear until the second half of the 20th century.\nDuring World War II and the subsequent Cold War some countries encouraged their citizens to become \"planespotters\" in an \"observation corps\" or similar public body for reasons of public security. Britain had the Royal Observer Corps which operated between 1925 and 1995. A journal called \"The Aeroplane Spotter\" was published in January 1940. The publication included a glossary that was refined in 2010 and published online.\nThe development of technology and global resources enabled a revolution in plane-spotting. Point and shoot cameras, DSLRs &amp; walkie talkies significantly changed the hobby. With the help of the internet, websites such as FlightAware and Flightradar24 have made it possible for spotters to track and locate specific aircraft from all across the world. Websites specifically for aircraft, such as airliners.net, and social networking services, such as Twitter, Facebook and Instagram, allow spotters to record their sightings and upload their photos or see pictures of aircraft spotted by other people worldwide. \nTechniques.\nWhen spotting aircraft, observers generally notice the key attributes of an aircraft, such as a distinctive noise from its engine, the number of contrails it is producing, or its callsign. Observers can also assess the size of the aircraft and the number, type, and position of its engines. Another distinctive attribute is the position of wings relative to the fuselage and the degree to which they are swept rearwards. The wings may be above the fuselage, below it, or fixed at midpoint. The number of wings indicates whether it is a monoplane, biplane or triplane. The position of the tailplane relative to the fin(s) and the shape of the fin are other attributes. The configuration of the landing gear can be distinctive, as well as the size and shape of the cockpit and passenger windows along with the layout of emergency exits and doors.\nOther features include the speed, cockpit placement, colour scheme or special equipment that changes the silhouette of the aircraft. Taken together these traits will enable the identification of an aircraft. If the observer is familiar with the airfield being used by the aircraft and its normal traffic patterns, he or she is more likely to leap quickly to a decision about the aircraft's identity \u2013 they may have seen the same type of aircraft from the same angle many times. This is particularly prevalent if the aircraft spotter is spotting commercial aircraft, operated by airlines that have a limited fleet.\nSpotters use equipment such as ADS-B decoders to track the movements of aircraft. The two most famous devices used are the AirNav Systems RadarBox and Kinetic Avionics SBS series. Both of them read and process the radar data and show the movements on a computer screen. Another tool that spotters can use are apps such as FlightRadar24 or Flightaware, where they can look at arrival and departure schedules and track the location of aircraft that have their transponder on. Most of the decoders also allow the exporting of logs from a certain route or airport.\nSpotting styles.\nSome spotters will note and compile the markings, a national insignia or airline livery or logo, a squadron badge or code letters in the case of a military aircraft. Published manuals allow more information to be deduced, such as the delivery date or the manufacturer's construction number. Camouflage markings differ, depending on the surroundings in which that aircraft is expected to operate.\nIn general, most spotters attempt to see as many aircraft of a given type, a particular airline, or a particular subset of aircraft such as business jets, commercial airliners, military and/or general aviation aircraft. Some spotters attempt to see every airframe and are known as \"frame spotters.\" Others are keen to see every registration worn by each aircraft.\nAncillary activities might include listening-in to air traffic control transmissions (using radio scanners, where that is legal), liaising with other \"spotters\" to clear up uncertainties as to what aircraft have been seen at specific times or in particular places. Several internet mailing list groups have been formed to help communicate aircraft seen at airports, queries and anomalies. These groups can cater to certain regions, certain aircraft types, or may appeal to a wider audience. The result is that information on aircraft movements can be delivered worldwide in a real-time fashion to spotters.\nThe hobbyist might travel long distances to visit different airports, to see an unusual aircraft, or to view the remains of aircraft withdrawn from use. Air shows usually draw large numbers of spotters as they are opportunities to enter airfields and air bases worldwide that are usually closed to the public and to see displayed aircraft at close range. Some aircraft may be placed in the care of museums (see Aviation archaeology) \u2013 or perhaps be cannibalized in order to repair a similar aircraft already preserved.\nAircraft registrations can be found in books, with online resources, or in monthly magazines from enthusiast groups. Most spotters maintained books of different aircraft fleets and would underline or check each aircraft seen. Each year, a revised version of the books would be published and the spotter would need to re-underline every aircraft seen. With the development of commercial aircraft databases spotters were finally able to record their sightings in an electronic database and produce reports that emulated the underlined books.\nLegal ramifications.\nThe legal repercussions of the hobby were dramatically shown in November 2001 when fourteen aircraft spotters (twelve British, two Dutch) were arrested by Greek police after being observed at an open day at the Greek Air Force base at Kalamata. They were charged with espionage and faced a possible 20-year prison sentence if found guilty. After being held for six weeks, they were eventually released on $11,696 (\u00a39,000) bail, and the charges reduced to the misdemeanor charge of illegal information collection. They returned for their trial in April, 2002 and were found guilty, with eight of the group sentenced to three years, the rest for one year. At their appeal a year later, all were acquitted.\nAs airport watch groups.\nIn the wake of the targeting of airports by terrorists, enthusiasts' organisations and police in the UK have cooperated in creating a code of conduct for planespotters, in a similar vein to guidelines devised for train spotters. By asking enthusiasts to contact police if spotters believe they see or hear something suspicious, this is an attempt to allow enthusiasts to continue their hobby while increasing security around airports. Birmingham and Stansted pioneered this approach in Britain and prior to the 2012 London Olympics, RAF Northolt introduced a \"Flightwatch\" scheme based on the same cooperative principles. These changes are also being made abroad in countries such as Australia, where aviation enthusiasts are reporting suspicious or malicious actions to police.\nThe organisation of such groups has now been echoed in parts of North America. For example, the Bensenville, Illinois police department have sponsored an \"Airport Watch\" group at the Chicago O'Hare Airport. Members are issued identification cards and given training to accurately record and report unusual activities around the airport perimeter. (Members are not permitted airside.) Meetings are attended and supported by the FBI, Chicago Department of Aviation and the TSA who also provide regular training to group members. The Bensenville program was modeled on similar programs in Toronto, Ottawa and Minneapolis.\nIn 2009, a similar airport watch group was organized between airport security and local aircraft spotters at Montr\u00e9al\u2013Pierre Elliott Trudeau International Airport. As of 2016, the group has 46 members and a special phone number to use to contact police if suspicious activity is seen around the airport area.\nExtraordinary rendition.\nFollowing the events of 9/11, information collected by planespotters helped uncover what is known as \"extraordinary rendition\" by the CIA. Information on unusual movements of rendition aircraft provided data that was mapped by critical geographers such as Trevor Paglen and the Institute for Applied Autonomy. These data and maps led first to news reports and then to a number of governmental and inter-governmental investigations."}
{"id": "2861", "revid": "37479218", "url": "https://en.wikipedia.org/wiki?curid=2861", "title": "Advertising", "text": "Advertising is the practice and techniques employed to bring attention to a product or service. Advertising aims to present a product or service in terms of utility, advantages and qualities of interest to consumers. It is typically used to promote a specific good or service, but there are a wide range of uses, the most common being commercial advertisement.\nCommercial advertisements often seek to generate increased consumption of their products or services through \"branding\", which associates a product name or image with certain qualities in the minds of consumers. On the other hand, ads that intend to elicit an immediate sale are known as direct-response advertising. Non-commercial entities that advertise more than consumer products or services include political parties, interest groups, religious organizations, and governmental agencies. Non-profit organizations may use free modes of persuasion, such as a public service announcement. Advertising may also help to reassure employees or shareholders that a company is viable or successful.\nIn the 19th century, soap businesses were among the first to employ large-scale advertising campaigns. Thomas J. Barratt was hired by Pears to be its brand manager\u2014the first of its kind\u2014and in addition to creating slogans and images he recruited West End stage actress and socialite Lillie Langtry to become the poster-girl for Pears, making her the first celebrity to endorse a commercial product. Modern advertising originated with the techniques introduced with tobacco advertising in the 1920s, most significantly with the campaigns of Edward Bernays, considered the founder of modern, \"Madison Avenue\" advertising.\nWorldwide spending on advertising in 2015 amounted to an estimated . Advertising's projected distribution for 2017 was 40.4% on TV, 33.3% on digital, 9% on newspapers, 6.9% on magazines, 5.8% on outdoor and 4.3% on radio. Internationally, the largest (\"Big Five\") advertising agency groups are Omnicom, WPP, Publicis, Interpublic, and Dentsu.\nIn Latin, \"advertere\" means \"to turn towards\".\nHistory.\nEgyptians used papyrus to make sales messages and wall posters. Commercial messages and political campaign displays have been found in the ruins of Pompeii and ancient Arabia. Lost and found advertising on papyrus was common in ancient Greece and ancient Rome. Wall or rock painting for commercial advertising is another manifestation of an ancient advertising form, which is present to this day in many parts of Asia, Africa, and South America. The tradition of wall painting can be traced back to Indian rock art paintings that date back to 4000 BC.\nIn ancient China, the earliest advertising known was oral, as recorded in the Classic of Poetry (11th to 7th centuries BC) of bamboo flutes played to sell confectionery. Advertisement usually takes the form of calligraphic signboards and inked papers. A copper printing plate dated back to the Song dynasty used to print posters in the form of a square sheet of paper with a rabbit logo with \"Jinan Liu's Fine Needle Shop\" and \"We buy high-quality steel rods and make fine-quality needles, to be ready for use at home in no time\" written above and below is considered the world's earliest identified printed advertising medium.\nIn Europe, as the towns and cities of the Middle Ages began to grow, and the general population was unable to read, instead of signs that read \"cobbler\", \"miller\", \"tailor\", or \"blacksmith\", images associated with their trade would be used such as a boot, a suit, a hat, a clock, a diamond, a horseshoe, a candle or even a bag of flour. Fruits and vegetables were sold in the city square from the backs of carts and wagons and their proprietors used street callers (town criers) to announce their whereabouts. The first compilation of such advertisements was gathered in \"Les Crieries de Paris\", a thirteenth-century poem by Guillaume de la Villeneuve.\n18th-19th century: Newspaper Advertising.\nIn the 18th century, advertisements started to appear in weekly newspapers in England. These early print advertisements were used mainly to promote books and newspapers, which became increasingly affordable with advances in the printing press; and medicines, which were increasingly sought after. However, false advertising and so-called \"quack\" advertisements became a problem, which ushered in the regulation of advertising content.\nIn the United States, newspapers grew quickly in the first few decades of the 19th century, in part due to advertising. By 1822, the United States had more newspaper readers than any other country. About half of the content of these newspapers consisted of advertising, usually local advertising, with half of the daily newspapers in the 1810s using the word \"advertiser\" in their name.\nIn August 1859, British pharmaceutical firm Beechams created a slogan for Beecham's Pills: \"Beechams Pills: Worth a guinea a box\", which is considered to be the world's first advertising slogan. The Beechams adverts would appear in newspapers all over the world, helping the company become a global brand. The phrase was said to be uttered by a satisfied lady purchaser from St Helens, Lancashire, the founder's hometown.\nIn June 1836, the French newspaper \"La Presse\" was the first to include paid advertising in its pages, allowing it to lower its price, extend its readership and increase its profitability and the formula was soon copied by all titles. Around 1840, Volney B. Palmer established the roots of the modern day advertising agency in Philadelphia. In 1842 Palmer bought large amounts of space in various newspapers at a discounted rate then resold the space at higher rates to advertisers. The actual ad \u2013 the copy, layout, and artwork \u2013 was still prepared by the company wishing to advertise; in effect, Palmer was a space broker. The situation changed when the first full-service advertising agency of N.W. Ayer &amp; Son was founded in 1869 in Philadelphia. Ayer &amp; Son offered to plan, create, and execute complete advertising campaigns for its customers. By 1900 the advertising agency had become the focal point of creative planning, and advertising was firmly established as a profession.\n Around the same time, in France, Charles-Louis Havas extended the services of his news agency, Havas to include advertisement brokerage, making it the first French group to organize. At first, agencies were brokers for advertisement space in newspapers.\nLate 19th century: Modern Advertising.\nThe late 19th and early 20th centuries saw the rise of modern advertising, driven by industrialization and the growth of consumer goods. This era saw the dawn of ad agencies, employing more cunning methods\u2014 persuasive diction and psychological tactics. Thomas J. Barratt of London has been called \"the father of modern advertising\". Working for the Pears soap company, Barratt created an effective advertising campaign for the company products, which involved the use of targeted slogans, images and phrases. One of his slogans, \"Good morning. Have you used Pears' soap?\" was famous in its day and into the 20th century. In 1882, Barratt recruited English actress and socialite Lillie Langtry to become the poster girl for Pears, making her the first celebrity to endorse a commercial product.\nBecoming the company's brand manager in 1865, listed as the first of its kind by the \"Guinness Book of Records\", Barratt introduced many of the crucial ideas that lie behind successful advertising and these were widely circulated in his day. He constantly stressed the importance of a strong and exclusive brand image for Pears and of emphasizing the product's availability through saturation campaigns. He also understood the importance of constantly reevaluating the market for changing tastes and mores, stating in 1907 that \"tastes change, fashions change, and the advertiser has to change with them. An idea that was effective a generation ago would fall flat, stale, and unprofitable if presented to the public today. Not that the idea of today is always better than the older idea, but it is different \u2013 it hits the present taste.\"\nEnhanced advertising revenues was one effect of the Industrial Revolution in Britain. Thanks to the revolution and the consumers it created, by the mid-19th century biscuits and chocolate became products for the masses, and British biscuit manufacturers were among the first to introduce branding to distinguish grocery products. One the world's first global brands, Huntley &amp; Palmers biscuits were sold in 172 countries in 1900, and their global reach was reflected in their advertisements.\n20th century.\nAs a result of massive industrialization, advertising increased dramatically in the United States. In 1919 it was 2.5 percent of gross domestic product (GDP) in the US, and it averaged 2.2 percent of GDP between then and at least 2007, though it may have declined dramatically since the Great Recession.\nIndustry could not benefit from its increased productivity without a substantial increase in consumer spending. This contributed to the development of mass marketing designed to influence the population's economic behavior on a larger scale. In the 1910s and 1920s, advertisers in the U.S. adopted the doctrine that human instincts could be targeted and harnessed \u2013 \"sublimated\" into the desire to purchase commodities. Edward Bernays, a nephew of Sigmund Freud, became associated with the method and is sometimes called the founder of modern advertising and public relations. Bernays claimed that:In other words, selling products by appealing to the rational minds of customers (the main method used prior to Bernays) was much less effective than selling products based on the unconscious desires that Bernays felt were the true motivators of human action. \"Sex sells\" became a controversial issue, with techniques for titillating and enlarging the audience posing a challenge to conventional morality.\nIn the 1920s, under Secretary of Commerce Herbert Hoover, the American government promoted advertising. Hoover himself delivered an address to the Associated Advertising Clubs of the World in 1925 called 'Advertising Is a Vital Force in Our National Life.\" In October 1929, the head of the U.S. Bureau of Foreign and Domestic Commerce, Julius Klein, stated \"Advertising is the key to world prosperity.\" This was part of the \"unparalleled\" collaboration between business and government in the 1920s, according to a 1933 European economic journal.\nThe tobacco companies became major advertisers in order to sell packaged cigarettes. The tobacco companies pioneered the new advertising techniques when they hired Bernays to create positive associations with tobacco smoking.\nAdvertising was also used as a vehicle for cultural assimilation, encouraging workers to exchange their traditional habits and community structure in favor of a shared \"modern\" lifestyle. An important tool for influencing immigrant workers was the American Association of Foreign Language Newspapers (AAFLN). The AAFLN was primarily an advertising agency but also gained heavily centralized control over much of the immigrant press.\nAt the turn of the 20th century, advertising was one of the few career choices for women. Since women were responsible for most household purchasing done, advertisers and agencies recognized the value of women's insight during the creative process. In fact, the first American advertising to use a sexual sell was created by a woman \u2013 for a soap product. Although tame by today's standards, the advertisement featured a couple with the message \"A skin you love to touch\".\nIn the 1920s, psychologists Walter D. Scott and John B. Watson contributed applied psychological theory to the field of advertising. Scott said, \"Man has been called the reasoning animal but he could with greater truthfulness be called the creature of suggestion. He is reasonable, but he is to a greater extent suggestible\". He demonstrated this through his advertising technique of a direct command to the consumer.\nRadio from the 1920s.\nIn the early 1920s, the first radio stations were established by radio equipment manufacturers, followed by non-profit organizations such as schools, clubs and civic groups who also set up their own stations. Retailer and consumer goods manufacturers quickly recognized radio's potential to reach consumers in their home and soon adopted advertising techniques that would allow their messages to stand out; slogans, mascots, and jingles began to appear on radio in the 1920s and early television in the 1930s.\nThe rise of mass media communications allowed manufacturers of branded goods to bypass retailers by advertising directly to consumers. This was a major paradigm shift which forced manufacturers to focus on the brand and stimulated the need for superior insights into consumer purchasing, consumption and usage behavior; their needs, wants and aspirations. The earliest radio drama series were sponsored by soap manufacturers and the genre became known as a \"soap opera.\" Before long, radio station owners realized they could increase advertising revenue by selling 'air-time' in small time allocations which could be sold to multiple businesses. By the 1930s, these \"advertising spots\", as the packets of time became known, were being sold by the station's geographical sales representatives, ushering in an era of national radio advertising.\nBy the 1940s, manufacturers began to recognize the way in which consumers were developing personal relationships with their brands in a social/psychological/anthropological sense. Advertisers began to use motivational research and consumer research to gather insights into consumer purchasing. Strong branded campaigns for Chrysler and Exxon/Esso, using insights drawn research methods from psychology and cultural anthropology, led to some of the most enduring campaigns of the 20th century.\nCommercial television in the 1950s.\nIn the early 1950s, the DuMont Television Network began the modern practice of selling advertisement time to multiple sponsors. Previously, DuMont had trouble finding sponsors for many of their programs and compensated by selling smaller blocks of advertising time to several businesses. This eventually became the standard for the commercial television industry in the United States. However, it was still a common practice to have single sponsor shows, such as The United States Steel Hour. In some instances the sponsors exercised great control over the content of the show \u2013 up to and including having one's advertising agency actually writing the show. The single sponsor model is much less prevalent now, a notable exception being the Hallmark Hall of Fame.\nCable television from the 1980s.\nThe late 1980s and early 1990s saw the introduction of cable television and particularly MTV. Pioneering the concept of the music video, MTV ushered in a new type of advertising: the consumer tunes in \"for\" the advertising message, rather than it being a by-product or afterthought. As cable and satellite television became increasingly prevalent, specialty channels emerged, including channels entirely devoted to advertising, such as QVC, Home Shopping Network, and ShopTV Canada.\nInternet from the 1990s.\nWith the advent of the ad server, online advertising grew, contributing to the \"dot-com\" boom of the 1990s. Entire corporations operated solely on advertising revenue, offering everything from coupons to free Internet access. At the turn of the 21st century, some websites, including the search engine Google, changed online advertising by personalizing ads based on web browsing behavior. This has led to other similar efforts and an increase in interactive advertising. Online advertising introduced new opportunities for targeting and engagement, with platforms like Google and Facebook leading the charge. This shift has significantly altered the advertising landscape, making digital advertising a dominant force in the industry.\nThe share of advertising spending relative to GDP has changed little across large changes in media since 1925. In 1925, the main advertising media in America were newspapers, magazines, signs on streetcars, and outdoor posters. Advertising spending as a share of GDP was about 2.9 percent. By 1998, television and radio had become major advertising media; by 2017, the balance between broadcast and online advertising had shifted, with online spending exceeding broadcast. Nonetheless, advertising spending as a share of GDP was slightly lower \u2013 about 2.4 percent.\nGuerrilla marketing involves unusual approaches such as staged encounters in public places, giveaways of products such as cars that are covered with brand messages, and interactive advertising where the viewer can respond to become part of the advertising message. This type of advertising is unpredictable, which causes consumers to buy the product or idea. This reflects an increasing trend of interactive and \"embedded\" ads, such as via product placement, having consumers vote through text messages, and various campaigns utilizing social network services such as Facebook or Twitter.\nThe advertising business model has also been adapted in recent years. In media for equity, advertising is not sold, but provided to start-up companies in return for equity. If the company grows and is sold, the media companies receive cash for their shares.\nDomain name registrants (usually those who register and renew domains as an investment) sometimes \"park\" their domains and allow advertising companies to place ads on their sites in return for per-click payments. These ads are typically driven by pay per click search engines like Google or Yahoo, but ads can sometimes be placed directly on targeted domain names through a domain lease or by making contact with the registrant of a domain name that describes a product. Domain name registrants are generally easy to identify through WHOIS records that are publicly available at registrar websites.\nClassification.\nAdvertising may be categorized in a variety of ways, including by style, target audience, geographic scope, medium, or purpose. For example, in print advertising, classification by style can include display advertising (ads with design elements sold by size) vs. classified advertising (ads without design elements sold by the word or line). Advertising may be local, national or global. An ad campaign may be directed toward consumers or to businesses. The purpose of an ad may be to raise awareness (brand advertising), or to elicit an immediate sale (direct response advertising). The term above the line (ATL) is used for advertising involving mass media; more targeted forms of advertising and promotion are referred to as below the line (BTL). The two terms date back to 1954 when Procter &amp; Gamble began paying their advertising agencies differently from other promotional agencies. In the 2010s, as advertising technology developed, a new term, through the line (TTL) began to come into use, referring to integrated advertising campaigns.\nTraditional media.\nVirtually any medium can be used for advertising. Commercial advertising media can include wall paintings, billboards, street furniture components, printed flyers and rack cards, radio, cinema and television adverts, web banners, mobile telephone screens, shopping carts, web popups, skywriting, bus stop benches, human billboards and forehead advertising, magazines, newspapers, town criers, sides of buses, banners attached to or sides of airplanes (\"logojets\"), in-flight advertisements on seatback tray tables or overhead storage bins, taxicab doors, roof mounts and passenger screens, musical stage shows, subway platforms and trains, elastic bands on disposable diapers, doors of bathroom stalls, stickers on apples in supermarkets, shopping cart handles (grabertising), the opening section of streaming audio and video, posters, and the backs of event tickets and supermarket receipts. Any situation in which an \"identified\" sponsor pays to deliver their message through a medium is advertising.\nNew media approaches.\nA new advertising approach is known as advanced advertising, which is data-driven advertising, using large quantities of data, precise measuring tools and precise targeting. Advanced advertising also makes it easier for companies which sell ad space to attribute customer purchases to the ads they display or broadcast.\nIncreasingly, other media are overtaking many of the \"traditional\" media such as television, radio and newspaper because of a shift toward the usage of the Internet for news and music as well as devices like digital video recorders (DVRs) such as TiVo.\nOnline advertising began with unsolicited bulk e-mail advertising known as \"e-mail spam\". Spam has been a problem for e-mail users since 1978. As new online communication channels became available, advertising followed. The first banner ad appeared on the World Wide Web in 1994. Prices of Web-based advertising space are dependent on the \"relevance\" of the surrounding web content and the traffic that the website receives.\nIn online display advertising, display ads generate awareness quickly. Unlike search, which requires someone to be aware of a need, display advertising can drive awareness of something new and without previous knowledge. Display works well for direct response. The display is not only used for generating awareness, it is used for direct response campaigns that link to a landing page with a clear 'call to action'.\nAs the mobile phone became a new mass medium in 1998 when the first paid downloadable content appeared on mobile phones in Finland, mobile advertising followed, also first launched in Finland in 2000. By 2007 the value of mobile advertising had reached $2 billion and providers such as Admob delivered billions of mobile ads.\nMore advanced mobile ads include banner ads, coupons, Multimedia Messaging Service picture and video messages, advergames and various engagement marketing campaigns. A particular feature driving mobile ads is the 2D barcode, which replaces the need to do any typing of web addresses, and uses the camera feature of modern phones to gain immediate access to web content. 83 percent of Japanese mobile phone users already are active users of 2D barcodes.\nSome companies have proposed placing messages or corporate logos on the side of booster rockets and the International Space Station.\nUnpaid advertising (also called \"publicity advertising\"), can include personal recommendations (\"bring a friend\", \"sell it\"), spreading buzz, or achieving the feat of equating a brand with a common noun (in the United States, \"Xerox\" = \"photocopier\", \"Kleenex\" = tissue, \"Vaseline\" = petroleum jelly, \"Hoover\" = vacuum cleaner, and \"Band-Aid\" = adhesive bandage). However, some companies oppose the use of their brand name to label an object. Equating a brand with a common noun also risks turning that brand into a generic trademark \u2013 turning it into a generic term which means that its legal protection as a trademark is lost. \nEarly in its life, The CW aired short programming breaks called \"Content Wraps\", to advertise one company's product during an entire commercial break. The CW pioneered \"content wraps\" and some products featured were Herbal Essences, Crest, Guitar Hero II, CoverGirl, and Toyota.\nA new promotion concept has appeared, \"ARvertising\", advertising on augmented reality technology.\nControversy exists on the effectiveness of subliminal advertising (see mind control), and the pervasiveness of mass messages (propaganda).\nRise in new media.\nWith the Internet came many new advertising opportunities. Pop-up, Flash, banner, pop-under, advergaming, and email advertisements (all of which are often unwanted or spam in the case of email) are now commonplace. Particularly since the rise of \"entertaining\" advertising, some people may like an advertisement enough to wish to watch it later or show a friend. In general, the advertising community has not yet made this easy, although some have used the Internet to widely distribute their ads to anyone willing to see or hear them. In the last three quarters of 2009, mobile and Internet advertising grew by 18% and 9% respectively, while older media advertising saw declines: \u221210.1% (TV), \u221211.7% (radio), \u221214.8% (magazines) and \u221218.7% (newspapers). Between 2008 and 2014, U.S. newspapers lost more than half their print advertising revenue.\nNiche marketing.\nAnother significant trend regarding future of advertising is the growing importance of the niche market using niche or targeted ads. Also brought about by the Internet and the theory of the long tail, advertisers will have an increasing ability to reach specific audiences. In the past, the most efficient way to deliver a message was to blanket the largest mass market audience possible. However, usage tracking, customer profiles and the growing popularity of niche content brought about by everything from blogs to social networking sites, provide advertisers with audiences that are smaller but much better defined, leading to ads that are more relevant to viewers and more effective for companies' marketing products. Among others, Comcast Spotlight is one such advertiser employing this method in their video on demand menus. These advertisements are targeted to a specific group and can be viewed by anyone wishing to find out more about a particular business or practice, from their home. This causes the viewer to become proactive and actually choose what advertisements they want to view.\nNiche marketing could also be helped by bringing the issue of color into advertisements. Different colors play major roles when it comes to marketing strategies, for example, seeing the blue can promote a sense of calmness and gives a sense of security which is why many social networks such as Facebook use blue in their logos.\nGoogle AdSense is an example of niche marketing. Google calculates the primary purpose of a website and adjusts ads accordingly; it uses keywords on the page (or even in emails) to find the general ideas of topics disused and places ads that will most likely be clicked on by viewers of the email account or website visitors.\nCrowdsourcing.\nThe concept of crowdsourcing has given way to the trend of user-generated advertisements. User-generated ads are created by people, as opposed to an advertising agency or the company themselves, often resulting from brand sponsored advertising competitions. For the 2007 Super Bowl, the Frito-Lays division of PepsiCo held the \"Crash the Super Bowl\" contest, allowing people to create their own Doritos commercials. Chevrolet held a similar competition for their Tahoe line of SUVs. Due to the success of the Doritos user-generated ads in the 2007 Super Bowl, Frito-Lays relaunched the competition for the 2009 and 2010 Super Bowl. The resulting ads were among the most-watched and most-liked Super Bowl ads. In fact, the winning ad that aired in the 2009 Super Bowl was ranked by the USA Today Super Bowl Ad Meter as the top ad for the year while the winning ads that aired in the 2010 Super Bowl were found by Nielsen's BuzzMetrics to be the \"most buzzed-about\". Another example of companies using crowdsourcing successfully is the beverage company Jones Soda that encourages consumers to participate in the label design themselves.\nThis trend has given rise to several online platforms that host user-generated advertising competitions on behalf of a company. Founded in 2007, Zooppa has launched ad competitions for brands such as Google, Nike, Hershey's, General Mills, Microsoft, NBC Universal, Zinio, and Mini Cooper. Crowdsourcing remains controversial, as the long-term impact on the advertising industry is still unclear.\nGlobalization.\nAdvertising has gone through five major stages of development: domestic, export, international, multi-national, and global. For global advertisers, there are four, potentially competing, business objectives that must be balanced when developing worldwide advertising: building a brand while speaking with one voice, developing economies of scale in the creative process, maximizing local effectiveness of ads, and increasing the company's speed of implementation. Born from the evolutionary stages of global marketing are the three primary and fundamentally different approaches to the development of global advertising executions: exporting executions, producing local executions, and importing ideas that travel.\nAdvertising research is key to determining the success of an ad in any country or region. The ability to identify which elements and/or moments of an ad contribute to its success is how economies of scale are maximized. Once one knows what works in an ad, that idea or ideas can be imported by any other market. Market research measures, such as , and provide insight into what is working in an ad in any country or region because the measures are based on the visual, not verbal, elements of the ad.\nForeign public messaging.\nForeign governments, particularly those that own marketable commercial products or services, often promote their interests and positions through the advertising of those goods because the target audience is not only largely unaware of the forum as a vehicle for foreign messaging but also willing to receive the message while in a mental state of absorbing information from advertisements during television commercial breaks, while reading a periodical, or while passing by billboards in public spaces. A prime example of this messaging technique is advertising campaigns to promote international travel. While advertising foreign destinations and services may stem from the typical goal of increasing revenue by drawing more tourism, some travel campaigns carry the additional or alternative intended purpose of promoting good sentiments or improving existing ones among the target audience towards a given nation or region. It is common for advertising promoting foreign countries to be produced and distributed by the tourism ministries of those countries, so these ads often carry political statements and/or depictions of the foreign government's desired international public perception. Additionally, a wide range of foreign airlines and travel-related services which advertise separately from the destinations, themselves, are owned by their respective governments; examples include, though are not limited to, the Emirates airline (Dubai), Singapore Airlines (Singapore), Qatar Airways (Qatar), China Airlines (Taiwan/Republic of China), and Air China (People's Republic of China). By depicting their destinations, airlines, and other services in a favorable and pleasant light, countries market themselves to populations abroad in a manner that could mitigate prior public impressions.\nDiversification.\nIn the realm of advertising agencies, continued industry diversification has seen observers note that \"big global clients don't need big global agencies any more\". This is reflected by the growth of non-traditional agencies in various global markets, such as Canadian business TAXI and SMART in Australia and has been referred to as \"a revolution in the ad world\".\nNew technology.\nThe ability to record shows on digital video recorders (such as TiVo) allow watchers to record the programs for later viewing, enabling them to fast forward through commercials. Additionally, as more seasons of pre-recorded box sets are offered for sale of television programs; fewer people watch the shows on TV. However, the fact that these sets are sold, means the company will receive additional profits from these sets.\nTo counter this effect, a variety of strategies have been employed. Many advertisers have opted for product placement on TV shows like Survivor. Other strategies include integrating advertising with internet-connected program guidess (EPGs), advertising on companion devices (like smartphones and tablets) during the show, and creating mobile apps for TV programs. Additionally, some like brands have opted for social television sponsorship.\nThe emerging technology of drone displays has recently been used for advertising purposes.\nEducation.\nIn recent years there have been several media literacy initiatives, and more specifically concerning advertising, that seek to empower citizens in the face of media advertising campaigns.\nAdvertising education has become popular with bachelor, master and doctorate degrees becoming available in the emphasis. A surge in advertising interest is typically attributed to the strong relationship advertising plays in cultural and technological changes, such as the advance of online social networking. A unique model for teaching advertising is the student-run advertising agency, where advertising students create campaigns for real companies. Organizations such as the American Advertising Federation establish companies with students to create these campaigns.\nPurposes.\nAdvertising is at the front of delivering the proper message to customers and prospective customers. The purpose of advertising is to inform the consumers about their product and convince customers that a company's services or products are the best, enhance the image of the company, point out and create a need for products or services, demonstrate new uses for established products, announce new products and programs, reinforce the salespeople's individual messages, draw customers to the business, and to hold existing customers.\nSales promotions and brand loyalty.\nSales promotions are another way to advertise. Sales promotions are double purposed because they are used to gather information about what type of customers one draws in and where they are, and to jump start sales. Sales promotions include things like contests and games, sweepstakes, product giveaways, samples coupons, loyalty programs, and discounts. The ultimate goal of sales promotions is to stimulate potential customers to action.\nCriticisms.\nWhile advertising can be seen as necessary for economic growth, it is not without social costs. Unsolicited commercial e-mail and other forms of spam have become so prevalent as to have become a major nuisance to users of these services, as well as being a financial burden on internet service providers. Advertising is increasingly invading public spaces, such as schools, which some critics argue is a form of child exploitation. This increasing difficulty in limiting exposure to specific audiences can result in negative backlash for advertisers. In tandem with these criticisms, the advertising industry has seen low approval rates in surveys and negative cultural portrayals. A 2021 study found that for more than 80% of brands, advertising had a negative return on investment. Unsolicited ads have been criticized as attention theft.\nOne of the most controversial criticisms of advertisement in the present day is that of the predominance of advertising of foods high in sugar, fat, and salt specifically to children. Critics claim that food advertisements targeting children are exploitive and are not sufficiently balanced with proper nutritional education to help children understand the consequences of their food choices. Additionally, children may not understand that they are being sold something, and are therefore more impressionable. Michelle Obama has criticized large food companies for advertising unhealthy foods largely towards children and has requested that food companies either limit their advertising to children or advertise foods that are more in line with dietary guidelines. The other criticisms include the change that are brought by those advertisements on the society and also the deceiving ads that are aired and published by the corporations. Cosmetic and health industry are the ones which exploited the highest and created reasons of concern. Political advertisement and their regulations have been scrutinized for misinformation, ethics and political bias.\nRegulation.\nThere have been increasing efforts to protect the public interest by regulating the content and the influence of advertising. Some examples include restrictions for advertising alcohol, tobacco or gambling imposed in many countries, as well as the bans around advertising to children, which exist in parts of Europe. Advertising regulation focuses heavily on the veracity of the claims and as such, there are often tighter restrictions placed around advertisements for food and healthcare products.\nThe advertising industries within some countries rely less on laws and more on systems of self-regulation. Advertisers and the media agree on a code of advertising standards that they attempt to uphold. The general aim of such codes is to ensure that any advertising is 'legal, decent, honest and truthful'. Some self-regulatory organizations are funded by the industry, but remain independent, with the intent of upholding the standards or codes like the Advertising Standards Authority in the UK.\nIn the UK, most forms of outdoor advertising, such as the display of billboards, are regulated by the UK Town and County Planning system. The display of an advertisement without consent from the Planning Authority is a criminal offense liable to a fine of \u00a32,500 per offense. In the US, where some communities believe that outdoor advertising are a blight on landscapes, attempts to ban billboard advertising in the open countryside occurred in the 1960s, leading to the \"Highway Beautification Act\". Cities such as S\u00e3o Paulo have introduced an outright ban, with London also having specific legislation to control unlawful displays.\nSome governments restrict the languages that can be used in advertisements, but advertisers may employ tricks to try avoiding them. In France for instance, advertisers sometimes print English words in bold and French translations in fine print to deal with Article 120 of the 1994 Toubon Law limiting the use of English.\nThe advertising of pricing information is another topic of concern for governments. In the United States for instance, it is common for businesses to only mention the existence and amount of applicable taxes at a later stage of a transaction. In Canada and New Zealand, taxes can be listed as separate items, as long as they are quoted up-front. In most other countries, the advertised price must include all applicable taxes, enabling customers to easily know how much it will cost them.\nTheory.\nHierarchy-of-effects models.\nVarious competing models of hierarchies of effects attempt to provide a theoretical underpinning to advertising practice.\nMarketing mix.\nThe marketing mix was proposed by professor E. Jerome McCarthy in the 1960s. It consists of four basic elements called the \"four Ps\". Product is the first P representing the actual product. Price represents the process of determining the value of a product. Place represents the variables of getting the product to the consumer such as distribution channels, market coverage and movement organization. The last P stands for Promotion which is the process of reaching the target market and convincing them to buy the product.\nIn the 1990s, the concept of four Cs was introduced as a more customer-driven replacement of four P's. There are two theories based on four Cs: Lauterborn's four Cs (\"consumer\", \"cost\", \"communication\", \"convenience\")\n and Shimizu's four Cs (\"commodity\", \"cost\", \"communication\", \"channel\") in the 7Cs Compass Model (Co-marketing). Communications can include advertising, sales promotion, public relations, publicity, personal selling, corporate identity, internal communication, SNS, and MIS.\nResearch.\nAdvertising research is a specialized form of research that works to improve the effectiveness and efficiency of advertising. It entails numerous forms of research which employ different methodologies. Advertising research includes pre-testing (also known as copy testing) and post-testing of ads and/or campaigns.\nPre-testing includes a wide range of qualitative and quantitative techniques, including: focus groups, in-depth target audience interviews (one-on-one interviews), small-scale quantitative studies and physiological measurement. The goal of these investigations is to better understand how different groups respond to various messages and visual prompts, thereby providing an assessment of how well the advertisement meets its communications goals.\nPost-testing employs many of the same techniques as pre-testing, usually with a focus on understanding the change in awareness or attitude attributable to the advertisement. With the emergence of digital advertising technologies, many firms have begun to continuously post-test ads using real-time data. This may take the form of A/B split-testing or multivariate testing.\nContinuous ad tracking and the Communicus System are competing examples of post-testing advertising research types.\nSemiotics.\nMeanings between consumers and marketers depict signs and symbols that are encoded in everyday objects. Semiotics is the study of signs and how they are interpreted. Advertising has many hidden signs and meanings within brand names, logos, package designs, print advertisements, and television advertisements. Semiotics aims to study and interpret the message being conveyed in (for example) advertisements. Logos and advertisements can be interpreted at two levels \u2013 known as the surface level and the underlying level. The surface level uses signs creatively to create an image or personality for a product. These signs can be images, words, fonts, colors, or slogans. The underlying level is made up of hidden meanings. The combination of images, words, colors, and slogans must be interpreted by the audience or consumer. The \"key to advertising analysis\" is the signifier and the signified. The signifier is the object and the signified is the mental concept. A product has a signifier and a signified. The signifier is the color, brand name, logo design, and technology. The signified has two meanings known as denotative and connotative. The denotative meaning is the meaning of the product. A television's denotative meaning might be that it is high definition. The connotative meaning is the product's deep and hidden meaning. A connotative meaning of a television would be that it is top-of-the-line.\nApple's commercials used a black silhouette of a person that was the age of Apple's target market. They placed the silhouette in front of a blue screen so that the picture behind the silhouette could be constantly changing. However, the one thing that stays the same in these ads is that there is music in the background and the silhouette is listening to that music on a white iPod through white headphones. Through advertising, the white color on a set of earphones now signifies that the music device is an iPod. The white color signifies almost all of Apple's products.\nThe semiotics of gender plays a key influence on the way in which signs are interpreted. When considering gender roles in advertising, individuals are influenced by three categories. Certain characteristics of stimuli may enhance or decrease the elaboration of the message (if the product is perceived as feminine or masculine). Second, the characteristics of individuals can affect attention and elaboration of the message (traditional or non-traditional gender role orientation). Lastly, situational factors may be important to influence the elaboration of the message.\nThere are two types of marketing communication claims-objective and subjective. Objective claims stem from the extent to which the claim associates the brand with a tangible product or service feature. For instance, a camera may have auto-focus features. Subjective claims convey emotional, subjective, impressions of intangible aspects of a product or service. They are non-physical features of a product or service that cannot be directly perceived, as they have no physical reality. For instance the brochure has a beautiful design. Males tend to respond better to objective marketing-communications claims while females tend to respond better to subjective marketing communications claims.\nVoiceovers are commonly used in advertising. Most voiceovers are done by men, with figures of up to 94% having been reported. There have been more female voiceovers in recent years, but mainly for food, household products, and feminine-care products.\nGender effects on comprehension.\nAccording to a 1977 study by David Statt, females process information comprehensively, while males process information through heuristic devices such as procedures, methods or strategies for solving problems, which could have an effect on how they interpret advertising. According to this study, men prefer to have available and apparent cues to interpret the message, whereas females engage in more creative, associative, imagery-laced interpretation. Later research by a Danish team found that advertising attempts to persuade men to improve their appearance or performance, whereas its approach to women aims at transformation toward an impossible ideal of female presentation. In Paul Suggett's article \"The Objectification of Women in Advertising\" he discusses the negative impact that these women in advertisements, who are too perfect to be real, have on women, as well as men, in real life. Advertising's manipulation of women's aspiration to these ideal types as portrayed in film, in erotic art, in advertising, on stage, within music videos and through other media exposures requires at least a conditioned rejection of female reality and thereby takes on a highly ideological cast. Studies show that these expectations of women and young girls negatively affect their views about their bodies and appearances. These advertisements are directed towards men. Not everyone agrees: one critic viewed this monologic, gender-specific interpretation of advertising as excessively skewed and politicized. There are some companies like Dove and aerie that are creating commercials to portray more natural women, with less post production manipulation, so more women and young girls are able to relate to them.\nMore recent research by Martin (2003) reveals that males and females differ in how they react to advertising depending on their mood at the time of exposure to the ads and on the affective tone of the advertising. When feeling sad, males prefer happy ads to boost their mood. In contrast, females prefer happy ads when they are feeling happy. The television programs in which ads are embedded influence a viewer's mood state. Susan Wojcicki, author of the article \"Ads that Empower Women don't just Break Stereotypes\u2014They're also Effective\" discusses how advertising to women has changed since the first Barbie commercial, where a little girl tells the doll that, she wants to be just like her. Little girls grow up watching advertisements of scantily clad women advertising things from trucks to burgers and Wojcicki states that this shows girls that they are either arm candy or eye candy.\nAlternatives.\nOther approaches to revenue include donations, paid subscriptions, microtransactions, and data monetization. Websites and applications are \"ad-free\" when not using advertisements at all for revenue. For example, the online encyclopedia Wikipedia provides free content by receiving funding from charitable donations.\nReferences.\nNotes"}
{"id": "2862", "revid": "25118920", "url": "https://en.wikipedia.org/wiki?curid=2862", "title": "AI-complete", "text": "In the field of artificial intelligence (AI), tasks that are hypothesized to require artificial general intelligence to solve are informally known as AI-complete or AI-hard. Calling a problem AI-complete reflects the belief that it cannot be solved by a simple specific algorithm. \nIn the past, problems supposed to be AI-complete included computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. AI-complete were notably considered useful for testing the presence of humans, as CAPTCHAs aim to do, and in computer security to circumvent brute-force attacks.\nHistory.\nThe term was coined by Fanya Montalvo by analogy with NP-complete and NP-hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller's 1987 PhD dissertation and in Eric Raymond's 1991 Jargon File.\nExpert systems, that were popular in the 1980s, were able to solve very simple and/or restricted versions of AI-complete problems, but never in their full generality. When AI researchers attempted to \"scale up\" their systems to handle more complicated, real-world situations, the programs tended to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation: they would fail as unexpected circumstances outside of its original problem context would begin to appear. When human beings are dealing with new situations in the world, they are helped by their awareness of the general context: they know what the things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. Expert systems lacked this adaptability and were brittle when facing new situations.\nDeepMind published a work in May 2022 in which they trained a single model to do several things at the same time. The model, named Gato, can \"play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.\" Similarly, some tasks once considered to be AI-complete, like machine translation, are among the capabilities of large language models.\nAI-complete problems.\nAI-complete problems have been hypothesized to include:\nFormalization.\nComputational complexity theory deals with the relative computational difficulty of computable functions. By definition, it does not cover problems whose solution is unknown or has not been characterized formally. Since many AI problems have no formalization yet, conventional complexity theory does not enable a formal definition of AI-completeness.\nResearch.\nRoman Yampolskiy\nsuggests that a problem formula_1 is AI-Complete if it has two properties:\nOn the other hand, a problem formula_3 is AI-Hard if and only if there is an AI-Complete problem formula_1 that is polynomial time Turing-reducible to formula_3. This also gives as a consequence the existence of AI-Easy problems, that are solvable in polynomial time by a deterministic Turing machine with an oracle for some problem.\nYampolskiy has also hypothesized that the Turing Test is a defining feature of AI-completeness.\nGroppe and Jain classify problems which require artificial general intelligence to reach human-level machine performance as AI-complete, while only restricted versions of AI-complete problems can be solved by the current AI systems. For \u0160ekrst, getting a polynomial solution to AI-complete problems would not necessarily be equal to solving the issue of artificial general intelligence, while emphasizing the lack of computational complexity research being the limiting factor towards achieving artificial general intelligence.\nFor Kwee-Bintoro and Velez, solving AI-complete problems would have strong repercussions on the society. "}
{"id": "2863", "revid": "69423", "url": "https://en.wikipedia.org/wiki?curid=2863", "title": "American Telephone and Telegraph Company", "text": ""}
{"id": "2864", "revid": "45880546", "url": "https://en.wikipedia.org/wiki?curid=2864", "title": "Archaeoastronomy", "text": "Archaeoastronomy (also spelled archeoastronomy) is the interdisciplinary or multidisciplinary study of how people in the past \"have understood the phenomena in the sky, how they used these phenomena and what role the sky played in their cultures\". Clive Ruggles argues it is misleading to consider archaeoastronomy to be the study of ancient astronomy, as modern astronomy is a scientific discipline, while archaeoastronomy considers symbolically rich cultural interpretations of phenomena in the sky by other cultures. It is often twinned with \"ethnoastronomy\", the anthropological study of skywatching in contemporary societies. Archaeoastronomy is also closely associated with historical astronomy, the use of historical records of heavenly events to answer astronomical problems and the history of astronomy, which uses written records to evaluate past astronomical practice.\nArchaeoastronomy uses a variety of methods to uncover evidence of past practices including archaeology, anthropology, astronomy, statistics and probability, and history. Because these methods are diverse and use data from such different sources, integrating them into a coherent argument has been a long-term difficulty for archaeoastronomers. Archaeoastronomy fills complementary niches in landscape archaeology and cognitive archaeology. Material evidence and its connection to the sky can reveal how a wider landscape can be integrated into beliefs about the cycles of nature, such as Mayan astronomy and its relationship with agriculture. Other examples which have brought together ideas of cognition and landscape include studies of the cosmic order embedded in the roads of settlements.\nArchaeoastronomy can be applied to all cultures and all time periods. The meanings of the sky vary from culture to culture; nevertheless there are scientific methods which can be applied across cultures when examining ancient beliefs. It is perhaps the need to balance the social and scientific aspects of archaeoastronomy which led Clive Ruggles to describe it as \"a field with academic work of high quality at one end but uncontrolled speculation bordering on lunacy at the other\".\nHistory.\nTwo hundred years before John Michell wrote the above, there were no archaeoastronomers and there were no professional archaeologists, but there were astronomers and antiquarians. Some of their works are considered precursors of archaeoastronomy; antiquarians interpreted the astronomical orientation of the ruins that dotted the English countryside as William Stukeley did of Stonehenge in 1740, while John Aubrey in 1678 and Henry Chauncy in 1700 sought similar astronomical principles underlying the orientation of churches. Late in the nineteenth century astronomers such as Richard Proctor and Charles Piazzi Smyth investigated the astronomical orientations of the pyramids.\nThe term \"archaeoastronomy\" was advanced by Elizabeth Chesley Baity (following the suggestion of Euan MacKie) in 1973, but as a topic of study it may be much older, depending on how archaeoastronomy is defined. Clive Ruggles says that Heinrich Nissen, working in the mid-nineteenth century was arguably the first archaeoastronomer. Rolf Sinclair says that Norman Lockyer, working in the late 19th and early 20th centuries, could be called the 'father of archaeoastronomy'. Euan MacKie would place the origin even later, stating: \"...the genesis and modern flowering of archaeoastronomy must surely lie in the work of Alexander Thom in Britain between the 1930s and the 1970s\".\nIn the 1960s the work of the engineer Alexander Thom and that of the astronomer Gerald Hawkins, who proposed that Stonehenge was a Neolithic computer, inspired new interest in the astronomical features of ancient sites. The claims of Hawkins were largely dismissed, but this was not the case for Alexander Thom's work, whose survey results of megalithic sites hypothesized widespread practice of accurate astronomy in the British Isles. Euan MacKie, recognizing that Thom's theories needed to be tested, excavated at the Kintraw standing stone site in Argyllshire in 1970 and 1971 to check whether the latter's prediction of an observation platform on the hill slope above the stone was correct. There was an artificial platform there and this apparent verification of Thom's long alignment hypothesis (Kintraw was diagnosed as an accurate winter solstice site) led him to check Thom's geometrical theories at the Cultoon stone circle in Islay, also with a positive result. MacKie therefore broadly accepted Thom's conclusions and published new prehistories of Britain. In contrast a re-evaluation of Thom's fieldwork by Clive Ruggles argued that Thom's claims of high accuracy astronomy were not fully supported by the evidence. Nevertheless, Thom's legacy remains strong, Edwin C. Krupp wrote in 1979, \"Almost singlehandedly he has established the standards for archaeo-astronomical fieldwork and interpretation, and his amazing results have stirred controversy during the last three decades.\" His influence endures and practice of statistical testing of data remains one of the methods of archaeoastronomy.\nThe approach in the New World, where anthropologists began to consider more fully the role of astronomy in Amerindian civilizations, was markedly different. They had access to sources that the prehistory of Europe lacks such as ethnographies and the historical records of the early colonizers. Following the pioneering example of Anthony Aveni, this allowed New World archaeoastronomers to make claims for motives which in the Old World would have been mere speculation. The concentration on historical data led to some claims of high accuracy that were comparatively weak when compared to the statistically led investigations in Europe.\nThis came to a head at a meeting sponsored by the International Astronomical Union (IAU) in Oxford in 1981. The methodologies and research questions of the participants were considered so different that the conference proceedings were published as two volumes. Nevertheless, the conference was considered a success in bringing researchers together and Oxford conferences have continued every four or five years at locations around the world. The subsequent conferences have resulted in a move to more interdisciplinary approaches with researchers aiming to combine the contextuality of archaeological research, which broadly describes the state of archaeoastronomy today, rather than merely establishing the existence of ancient astronomies, archaeoastronomers seek to explain why people would have an interest in the night sky.\nRelations to other disciplines.\nArchaeoastronomy has long been seen as an interdisciplinary field that uses written and unwritten evidence to study the astronomies of other cultures. As such, it can be seen as connecting other disciplinary approaches for investigating ancient astronomy: astroarchaeology (an obsolete term for studies that draw astronomical information from the alignments of ancient architecture and landscapes), history of astronomy (which deals primarily with the written textual evidence), and ethnoastronomy (which draws on the ethnohistorical record and contemporary ethnographic studies).\nReflecting Archaeoastronomy's development as an interdisciplinary subject, research in the field is conducted by investigators trained in a wide range of disciplines. Authors of recent doctoral dissertations have described their work as concerned with the fields of archaeology and cultural anthropology; with various fields of history including the history of specific regions and periods, the history of science and the history of religion; and with the relation of astronomy to art, literature and religion. Only rarely did they describe their work as astronomical, and then only as a secondary category.\nBoth practicing archaeoastronomers and observers of the discipline approach it from different perspectives. Other researchers relate archaeoastronomy to the history of science, either as it relates to a culture's observations of nature and the conceptual framework they devised to impose an order on those observations or as it relates to the political motives which drove particular historical actors to deploy certain astronomical concepts or techniques. Art historian Richard Poss took a more flexible approach, maintaining that the astronomical rock art of the North American Southwest should be read employing \"the hermeneutic traditions of western art history and art criticism\" Astronomers, however, raise different questions, seeking to provide their students with identifiable precursors of their discipline, and are especially concerned with the important question of how to confirm that specific sites are, indeed, intentionally astronomical.\nThe reactions of professional archaeologists to archaeoastronomy have been decidedly mixed. Some expressed incomprehension or even hostility, varying from a rejection by the archaeological mainstream of what they saw as an archaeoastronomical fringe to an incomprehension between the cultural focus of archaeologists and the quantitative focus of early archaeoastronomers. Yet archaeologists have increasingly come to incorporate many of the insights from archaeoastronomy into archaeology textbooks and, as mentioned above, some students wrote archaeology dissertations on archaeoastronomical topics.\nSince archaeoastronomers disagree so widely on the characterization of the discipline, they even dispute its name. All three major international scholarly associations relate archaeoastronomy to the study of culture, using the term \"Astronomy in Culture\" or a translation. Michael Hoskin sees an important part of the discipline as fact-collecting, rather than theorizing, and proposed to label this aspect of the discipline \"Archaeotopography.\" Ruggles and Saunders proposed \"Cultural Astronomy\" as a unifying term for the various methods of studying folk astronomies. Others have argued that astronomy is an inaccurate term, what are being studied are cosmologies and people who object to the use of logos have suggested adopting the Spanish \"cosmovisi\u00f3n\".\nWhen debates polarise between techniques, the methods are often referred to by a colour code, based on the colours of the bindings of the two volumes from the first Oxford Conference, where the approaches were first distinguished. Green (Old World) archaeoastronomers rely heavily on statistics and are sometimes accused of missing the cultural context of what is a social practice. Brown (New World) archaeoastronomers in contrast have abundant ethnographic and historical evidence and have been described as 'cavalier' on matters of measurement and statistical analysis. Finding a way to integrate various approaches has been a subject of much discussion since the early 1990s.\nMethodology.\nThere is no one way to do archaeoastronomy. The divisions between archaeoastronomers tend not to be between the physical scientists and the social scientists. Instead, it tends to depend on the location and/or kind of data available to the researcher. In the Old World, there is little data but the sites themselves; in the New World, the sites were supplemented by ethnographic and historic data. The effects of the isolated development of archaeoastronomy in different places can still often be seen in research today. Research methods can be classified as falling into one of two approaches, though more recent projects often use techniques from both categories.\nGreen archaeoastronomy.\nGreen archaeoastronomy is named after the cover of the book \"Archaeoastronomy in the Old World\". It is based primarily on statistics and is particularly apt for prehistoric sites where the social evidence is relatively scant compared to the historic period. The basic methods were developed by Alexander Thom during his extensive surveys of British megalithic sites.\nThom wished to examine whether or not prehistoric peoples used high-accuracy astronomy. He believed that by using horizon astronomy, observers could make estimates of dates in the year to a specific day. The observation required finding a place where on a specific date the Sun set into a notch on the horizon. A common theme is a mountain that blocked the Sun, but on the right day would allow the tiniest fraction to re-emerge on the other side for a 'double sunset'. The animation below shows two sunsets at a hypothetical site, one the day before the summer solstice and one at the summer solstice, which has a double sunset.\nTo test this idea he surveyed hundreds of stone rows and circles. Any individual alignment could indicate a direction by chance, but he planned to show that together the distribution of alignments was non-random, showing that there was an astronomical intent to the orientation of at least some of the alignments. His results indicated the existence of eight, sixteen, or perhaps even thirty-two approximately equal divisions of the year. The two solstices, the two equinoxes and four cross-quarter days, days halfway between a solstice and the equinox were associated with the medieval Celtic calendar. While not all these conclusions have been accepted, it has had an enduring influence on archaeoastronomy, especially in Europe.\nEuan MacKie has supported Thom's analysis, to which he added an archaeological context by comparing Neolithic Britain to the Mayan civilization to argue for a stratified society in this period. To test his ideas he conducted a couple of excavations at proposed prehistoric observatories in Scotland. Kintraw is a site notable for its four-meter high standing stone. Thom proposed that this was a foresight to a point on the distant horizon between Beinn Shianaidh and Beinn o'Chaolias on Jura. This, Thom argued, was a notch on the horizon where a double sunset would occur at midwinter. However, from ground level, this sunset would be obscured by a ridge in the landscape, and the viewer would need to be raised by two meters: another observation platform was needed. This was identified across a gorge where a platform was formed from small stones. The lack of artifacts caused concern for some archaeologists and the petrofabric analysis was inconclusive, but further research at Maes Howe and on the Bush Barrow Lozenge led MacKie to conclude that while the term 'science' may be anachronistic, Thom was broadly correct upon the subject of high-accuracy alignments.\nIn contrast Clive Ruggles has argued that there are problems with the selection of data in Thom's surveys. Others have noted that the accuracy of horizon astronomy is limited by variations in refraction near the horizon. A deeper criticism of Green archaeoastronomy is that while it can answer \"whether\" there was likely to be an interest in astronomy in past times, its lack of a social element means that it struggles to answer \"why\" people would be interested, which makes it of limited use to people asking questions about the society of the past. Keith Kintigh wrote: \"To put it bluntly, in many cases it doesn't matter much to the progress of anthropology whether a particular archaeoastronomical claim is right or wrong because the information doesn't inform the current interpretive questions.\" Nonetheless, the study of alignments remains a staple of archaeoastronomical research, especially in Europe.\nBrown archaeoastronomy.\nIn contrast to the largely alignment-oriented statistically led methods of green archaeoastronomy, brown archaeoastronomy has been identified as being closer to the history of astronomy or to cultural history, insofar as it draws on historical and ethnographic records to enrich its understanding of early astronomies and their relations to calendars and ritual. The many records of native customs and beliefs made by Spanish chroniclers and ethnographic researchers means that brown archaeoastronomy is often associated with studies of astronomy in the Americas.\nOne famous site where historical records have been used to interpret sites is Chichen Itza. Rather than analyzing the site and seeing which targets appear popular, archaeoastronomers have instead examined the ethnographic records to see what features of the sky were important to the Mayans and then sought archaeological correlates. One example which could have been overlooked without historical records is the Mayan interest in the planet Venus. This interest is attested to by the Dresden codex which contains tables with information about Venus's appearances in the sky. These cycles would have been of astrological and ritual significance as Venus was associated with Quetzalcoatl or Xolotl. Associations of architectural features with settings of Venus can be found in Chichen Itza, Uxmal, and probably some other Mesoamerican sites.\nThe Temple of the Warriors bears iconography depicting feathered serpents associated with Quetzalcoatl or Kukulcan. This means that the building's alignment towards the place on the horizon where Venus first appears in the evening sky (when it coincides with the rainy season) may be meaningful. However, since both the date and the azimuth of this event change continuously, a solar interpretation of this orientation is much more likely.\nAnthony Aveni claims that another building associated with the planet Venus in the form of Kukulcan, and the rainy season at Chichen Itza is the Caracol. This is a building with a circular tower and doors facing the cardinal directions. The base faces the most northerly setting of Venus. Additionally the pillars of a stylobate on the building's upper platform were painted black and red. These are colours associated with Venus as an evening and morning star. However the windows in the tower seem to have been little more than slots, making them poor at letting light in, but providing a suitable place to view out. In their discussion of the credibility of archaeoastronomical sites, Cotte and Ruggles considered the interpretation that the Caracol is an observatory site was debated among specialists, meeting the second of their four levels of site credibility.\nAveni states that one of the strengths of the brown methodology is that it can explore astronomies invisible to statistical analysis and offers the astronomy of the Incas as another example. The empire of the Incas was conceptually divided using \"ceques\", radial routes emanating from the capital at Cusco. Thus there are alignments in all directions which would suggest there is little of astronomical significance, However, ethnohistorical records show that the various directions do have cosmological and astronomical significance with various points in the landscape being significant at different times of the year. In eastern Asia archaeoastronomy has developed from the history of astronomy and much archaeoastronomy is searching for material correlates of the historical record. This is due to the rich historical record of astronomical phenomena which, in China, stretches back into the Han dynasty, in the second century BC.\nA criticism of this method is that it can be statistically weak. Schaefer in particular has questioned how robust the claimed alignments in the Caracol are. Because of the wide variety of evidence, which can include artefacts as well as sites, there is no one way to practice archaeoastronomy. Despite this it is accepted that archaeoastronomy is not a discipline that sits in isolation. Because archaeoastronomy is an interdisciplinary field, whatever is being investigated should make sense both archaeologically and astronomically. Studies are more likely to be considered sound if they use theoretical tools found in archaeology like analogy and homology and if they can demonstrate an understanding of accuracy and precision found in astronomy. Both quantitative analyses and interpretations based on ethnographic analogies and other contextual evidence have recently been applied in systematic studies of architectural orientations in the Maya area and in other parts of Mesoamerica.\nSource materials.\nBecause archaeoastronomy is about the many and various ways people interacted with the sky, there are a diverse range of sources giving information about astronomical practices.\nAlignments.\nA common source of data for archaeoastronomy is the study of alignments. This is based on the assumption that the axis of alignment of an archaeological site is meaningfully oriented towards an astronomical target. Brown archaeoastronomers may justify this assumption through reading historical or ethnographic sources, while green archaeoastronomers tend to prove that alignments are unlikely to be selected by chance, usually by demonstrating common patterns of alignment at multiple sites.\nAn alignment is calculated by measuring the azimuth, the angle from north, of the structure and the altitude of the horizon it faces The azimuth is usually measured using a theodolite or a compass. A compass is easier to use, though the deviation of the Earth's magnetic field from true north, known as its magnetic declination must be taken into account. Compasses are also unreliable in areas prone to magnetic interference, such as sites being supported by scaffolding. Additionally a compass can only measure the azimuth to a precision of a half a degree.\nA theodolite can be considerably more accurate if used correctly, but it is also considerably more difficult to use correctly. There is no inherent way to align a theodolite with North and so the scale has to be calibrated using astronomical observation, usually the position of the Sun. Because the position of celestial bodies changes with the time of day due to the Earth's rotation, the time of these calibration observations must be accurately known, or else there will be a systematic error in the measurements. Horizon altitudes can be measured with a theodolite or a clinometer.\nArtifacts.\nFor artifacts such as the Sky Disc of Nebra, alleged to be a Bronze Age artefact depicting the cosmos, the analysis would be similar to typical post-excavation analysis as used in other sub-disciplines in archaeology. An artefact is examined and attempts are made to draw analogies with historical or ethnographical records of other peoples. The more parallels that can be found, the more likely an explanation is to be accepted by other archaeologists.\nA more mundane example is the presence of astrological symbols found on some shoes and sandals from the Roman Empire. The use of shoes and sandals is well known, but Carol van Driel-Murray has proposed that astrological symbols etched onto sandals gave the footwear spiritual or medicinal meanings. This is supported through citation of other known uses of astrological symbols and their connection to medical practice and with the historical records of the time.\nAnother well-known artefact with an astronomical use is the Antikythera mechanism. In this case analysis of the artefact, and reference to the description of similar devices described by Cicero, would indicate a plausible use for the device. The argument is bolstered by the presence of symbols on the mechanism, allowing the disc to be read.\nArt and inscriptions.\nArt and inscriptions may not be confined to artefacts, but also appear painted or inscribed on an archaeological site. Sometimes inscriptions are helpful enough to give instructions to a site's use. For example, a Greek inscription on a stele (from Itanos) has been translated as:\"Patron set this up for Zeus Epopsios. Winter solstice. Should anyone wish to know: off 'the little pig' and the stele the sun turns.\" From Mesoamerica come Mayan and Aztec codices. These are folding books made from Amatl, processed tree bark on which are glyphs in Mayan or Aztec script. The Dresden codex contains information regarding the Venus cycle, confirming its importance to the Mayans.\nMore problematic are those cases where the movement of the Sun at different times and seasons causes light and shadow interactions with petroglyphs. A widely known example is the Sun Dagger of Fajada Butte at which a glint of sunlight passes over a spiral petroglyph. The location of a dagger of light on the petroglyph varies throughout the year. At the summer solstice a dagger can be seen through the heart of the spiral; at the winter solstice two daggers appear to either side of it. It is proposed that this petroglyph was created to mark these events. Recent studies have identified many similar sites in the US Southwest and Northwestern Mexico. It has been argued that the number of solstitial markers at these sites provides statistical evidence that they were intended to mark the solstices. The Sun Dagger site on Fajada Butte in Chaco Canyon, New Mexico, stands out for its explicit light markings that record all the key events of both the solar and lunar cycles: summer solstice, winter solstice, equinox, and the major and minor lunar standstills of the Moon's 18.6 year cycle. In addition at two other sites on Fajada Butte, there are five light markings on petroglyphs recording the summer and winter solstices, equinox and solar noon. Numerous buildings and interbuilding alignments of the great houses of Chaco Canyon and outlying areas are oriented to the same solar and lunar directions that are marked at the Sun Dagger site.\nIf no ethnographic nor historical data are found which can support this assertion then acceptance of the idea relies upon whether or not there are enough petroglyph sites in North America that such a correlation could occur by chance. It is helpful when petroglyphs are associated with existing peoples. This allows ethnoastronomers to question informants as to the meaning of such symbols.\nEthnographies.\nAs well as the materials left by peoples themselves, there are also the reports of other who have encountered them. The historical records of the Conquistadores are a rich source of information about the pre-Columbian Americans. Ethnographers also provide material about many other peoples.\nAnthony Aveni uses the importance of zenith passages as an example of the importance of ethnography. For peoples living between the tropics of Cancer and Capricorn there are two days of the year when the noon Sun passes directly overhead and casts no shadow. In parts of Mesoamerica this was considered a significant day as it would herald the arrival of rains, and so play a part in the cycle of agriculture. This knowledge is still considered important amongst Mayan Indians living in Central America today. The ethnographic records suggested to archaeoastronomers that this day may have been important to the ancient Mayans. There are also shafts known as 'zenith tubes' which illuminate subterranean rooms when the Sun passes overhead found at places like Monte Alb\u00e1n and Xochicalco. It is only through the ethnography that we can speculate that the timing of the illumination was considered important in Mayan society. Alignments to the sunrise and sunset on the day of the zenith passage have been claimed to exist at several sites. However, it has been shown that, since there are very few orientations that can be related to these phenomena, they likely have different explanations.\nEthnographies also caution against over-interpretation of sites. At a site in Chaco Canyon can be found a pictograph with a star, crescent and hand. It has been argued by some astronomers that this is a record of the 1054 Supernova. However recent reexaminations of related 'supernova petroglyphs' raises questions about such sites in general. Cotte and Ruggles used the Supernova petroglyph as an example of a completely refuted site and anthropological evidence suggests other interpretations. The Zuni people, who claim a strong ancestral affiliation with Chaco, marked their sun-watching station with a crescent, star, hand and sundisc, similar to those found at the Chaco site.\nEthnoastronomy is also an important field outside of the Americas. For example, anthropological work with Aboriginal Australians is producing much information about their Indigenous astronomies and about their interaction with the modern world.\nRecreating the ancient sky.\nOnce the researcher has data to test, it is often necessary to attempt to recreate ancient sky conditions to place the data in its historical environment.\nDeclination.\nTo calculate what astronomical features a structure faced a coordinate system is needed. The stars provide such a system. On a clear night observe the stars spinning around the celestial pole can be observed. This point is +90\u00b0 of the North Celestial Pole or \u221290\u00b0 observing the Southern Celestial Pole. The concentric circles the stars trace out are lines of celestial latitude, known as \"declination\". The arc connecting the points on the horizon due East and due West (if the horizon is flat) and all points midway between the Celestial Poles is the Celestial Equator which has a declination of 0\u00b0. The visible declinations vary depending where you are on the globe. Only an observer on the North Pole of Earth would be unable to see any stars from the Southern Celestial Hemisphere at night (see diagram below). Once a declination has been found for the point on the horizon that a building faces it is then possible to say whether a specific body can be seen in that direction.\nSolar positioning.\nWhile the stars are fixed to their declinations the Sun is not. The rising point of the Sun varies throughout the year. It swings between two limits marked by the solstices a bit like a pendulum, slowing as it reaches the extremes, but passing rapidly through the midpoint. If an archaeoastronomer can calculate from the azimuth and horizon height that a site was built to view a declination of +23.5\u00b0 then he or she need not wait until 21 June to confirm the site does indeed face the summer solstice. For more information see History of solar observation.\nLunar positioning.\nThe Moon's appearance is considerably more complex. Its motion, like the Sun, is between two limits\u2014known as \"luni\"stices rather than \"sol\"stices. However, its travel between lunistices is considerably faster. It takes a sidereal month to complete its cycle rather than the year-long trek of the Sun. This is further complicated as the lunistices marking the limits of the Moon's movement move on an 18.6 year cycle. For slightly over nine years the extreme limits of the Moon are outside the range of sunrise. For the remaining half of the cycle the Moon never exceeds the limits of the range of sunrise. However, much lunar observation was concerned with the \"phase\" of the Moon. The cycle from one New Moon to the next runs on an entirely different cycle, the Synodic month. Thus when examining sites for lunar significance the data can appear sparse due to the extremely variable nature of the Moon. See Moon for more details.\nStellar positioning.\nFinally there is often a need to correct for the apparent movement of the stars. On the timescale of human civilisation the stars have largely maintained the same position relative to each other. Each night they appear to rotate around the celestial poles due to the Earth's rotation about its axis. However, the Earth spins rather like a spinning top. Not only does the Earth rotate, it wobbles. The Earth's axis takes around 25,800 years to complete one full wobble. The effect to the archaeoastronomer is that stars did not rise over the horizon in the past in the same places as they do today. Nor did the stars rotate around Polaris as they do now.\nThe movement of the Earth's axis was already noticed by the Sumerians over six thousand years ago, when they were able to observe the star Canopus culminating directly above the horizon on the southern meridian for the first time in their oldest and southernmost city Eridu. For several decades, Canopus was not yet visible in the neighbouring town of Ur to the north-east of Eridu, and therefore, it was called the \"Star of the City of Eridu\" in Sumerian.\nIn the case of the Egyptian pyramids, it has been shown they were aligned towards Thuban, a faint star in the constellation of Draco. The effect can be substantial over relatively short lengths of time, historically speaking. For instance a person born on 25 December in Roman times would have been born with the Sun in the constellation Capricorn. In the modern period a person born on the same date would have the Sun in Sagittarius due to the precession of the equinoxes.\nTransient phenomena.\nAdditionally there are often transient phenomena, events which do not happen on an annual cycle. Most predictable are events like eclipses. In the case of solar eclipses these can be used to date events in the past. A solar eclipse mentioned by Herodotus enables us to date a battle between the Medes and the Lydians, which following the eclipse failed to happen, to 28 May, 585 BC.\nSome comets are predictable, most famously Halley's Comet. Yet as a class of object they remain unpredictable and can appear at any time. Some have extremely lengthy orbital periods which means their past appearances and returns cannot be predicted. Others may have only ever passed through the Solar System once and so are inherently unpredictable.\nMeteor showers should be predictable, but some meteors are cometary debris and so require calculations of orbits which are currently impossible to complete. Other events noted by ancients include aurorae, sun dogs and rainbows all of which are as impossible to predict as the ancient weather, but nevertheless may have been considered important phenomena.\nMajor topics of archaeoastronomical research.\nThe use of calendars.\nA common justification for the need for astronomy is the need to develop an accurate calendar for agricultural reasons. Ancient texts like Hesiod's Works and Days, an ancient farming manual, would appear to partially confirm this: astronomical observations are used in combination with ecological signs, such as bird migrations to determine the seasons. Ethnoastronomical studies of the Hopi of the southwestern United States indicate that they carefully observed the rising and setting positions of the Sun to determine the proper times to plant crops. However, ethnoastronomical work with the Mursi of Ethiopia shows that their luni-solar calendar was somewhat haphazard, indicating the limits of astronomical calendars in some societies. All the same, calendars appear to be an almost universal phenomenon in societies as they provide tools for the regulation of communal activities.\nOne such example is the \"Tzolk'in\" calendar of 260 days. Together with the 365-day year, it was used in pre-Columbian Mesoamerica, forming part of a comprehensive calendrical system, which combined a series of astronomical observations and ritual cycles. Archaeoastronomical studies throughout Mesoamerica have shown that the orientations of most structures refer to the Sun and were used in combination with the 260-day cycle for scheduling agricultural activities and the accompanying rituals. The distribution of dates and intervals marked by orientations of monumental ceremonial complexes in the area along the southern Gulf Coast in Mexico, dated to about 1100 to 700 BCE, represents the earliest evidence of the use of this cycle. \nOther peculiar calendars include ancient Greek calendars. These were nominally lunar, starting with the New Moon. In reality the calendar could pause or skip days with confused citizens inscribing dates by both the civic calendar and \"ton theoi\", by the moon. The lack of any universal calendar for ancient Greece suggests that coordination of panhellenic events such as games or rituals could be difficult and that astronomical symbolism may have been used as a politically neutral form of timekeeping. Orientation measurements in Greek temples and Byzantine churches have been associated to deity's name day, festivities, and special events.\nMyth and cosmology.\nAnother motive for studying the sky is to understand and explain the universe. In these cultures myth was a tool for achieving this, and the explanations, while not reflecting the standards of modern science, are cosmologies.\nThe Incas arranged their empire to demonstrate their cosmology. The capital, Cusco, was at the centre of the empire and connected to it by means of ceques, conceptually straight lines radiating out from the centre. These ceques connected the centre of the empire to the four \"suyus\", which were regions defined by their direction from Cusco. The notion of a quartered cosmos is common across the Andes. Gary Urton, who has conducted fieldwork in the Andean villagers of Misminay, has connected this quartering with the appearance of the Milky Way in the night sky. In one season it will bisect the sky and in another bisect it in a perpendicular fashion.\nThe importance of observing cosmological factors is also seen on the other side of the world. The Forbidden City in Beijing is laid out to follow cosmic order though rather than observing four directions. The Chinese system was composed of five directions: North, South, East, West and Centre. The Forbidden City occupied the centre of ancient Beijing. One approaches the Emperor from the south, thus placing him in front of the circumpolar stars. This creates the situation of the heavens revolving around the person of the Emperor. The Chinese cosmology is now better known through its export as feng shui.\nThere is also much information about how the universe was thought to work stored in the mythology of the constellations. The Barasana of the Amazon plan part of their annual cycle based on observation of the stars. When their constellation of the Caterpillar-Jaguar (roughly equivalent to the modern Scorpius) falls they prepare to catch the pupating caterpillars of the forest as they fall from the trees. The caterpillars provide food at a season when other foods are scarce.\nA more well-known source of constellation myth are the texts of the Greeks and Romans. The origin of their constellations remains a matter of vigorous and occasionally fractious debate.\nThe loss of one of the sisters, Merope, in some Greek myths may reflect an astronomical event wherein one of the stars in the Pleiades disappeared from view by the naked eye.\nGiorgio de Santillana, professor of the History of Science in the School of Humanities at the Massachusetts Institute of Technology, and Hertha von Dechend, professor at Goethe University Frankfurt, argued that the old mythological stories handed down from antiquity were not random fictitious tales but were accurate depictions of celestial cosmology clothed in tales to aid their oral transmission. The chaos, monsters and violence in ancient myths are representative of the forces that shape each age. They argued that ancient myths are the remains of preliterate, late Neolithic astronomy that was lost. Santillana and von Dechend argued in their book \"Hamlet's Mill: An Essay on Myth and the Frame of Time\" (1969) that ancient myths have no historical or factual basis other than a cosmological one encoding astronomical phenomena, especially the precession of the equinoxes. Santillana and von Dechend's approach is not widely accepted.\nDisplays of power.\nBy including celestial motifs in clothing it becomes possible for the wearer to make claims the power on Earth is drawn from above. It has been said that the Shield of Achilles described by Homer is also a catalogue of constellations. In North America shields depicted in Comanche petroglyphs appear to include Venus symbolism.\nSolsticial alignments also can be seen as displays of power. When viewed from a ceremonial plaza on the Island of the Sun (the mythical origin place of the Sun) in Lake Titicaca, the Sun was seen to rise at the June solstice between two towers on a nearby ridge. The sacred part of the island was separated from the remainder of it by a stone wall and ethnographic records indicate that access to the sacred space was restricted to members of the Inca ruling elite. Ordinary pilgrims stood on a platform outside the ceremonial area to see the solstice Sun rise between the towers.\nIn Egypt the temple of Amun-Re at Karnak has been the subject of much study. Evaluation of the site, taking into account the change over time of the obliquity of the ecliptic show that the Great Temple was aligned on the rising of the midwinter Sun. The length of the corridor down which sunlight would travel would have limited illumination at other times of the year.\nIn a later period the Serapeum of Alexandria was also said to have contained a solar alignment so that, on a specific sunrise, a shaft of light would pass across the lips of the statue of Serapis thus symbolising the Sun saluting the god.\nMajor sites of archaeoastronomical interest.\nClive Ruggles and Michel Cotte recently edited a book on heritage sites of astronomy and archaeoastronomy which discussed a worldwide sample of astronomical and archaeoastronomical sites and provided criteria for the classification of archaeoastronomical sites.\nNewgrange.\nNewgrange is a passage tomb in the Republic of Ireland dating from around 3,300 to 2,900 BC For a few days around the Winter Solstice light shines along the central passageway into the heart of the tomb. What makes this notable is not that light shines in the passageway, but that it does not do so through the main entrance. Instead it enters via a hollow box above the main doorway discovered by Michael O'Kelly. It is this roofbox which strongly indicates that the tomb was built with an astronomical aspect in mind. In their discussion of the credibility of archaeoastronomical sites, Cotte and Ruggles gave Newgrange as an example of a Generally accepted site, the highest of their four levels of credibility. Clive Ruggles notes:\nEgypt.\nSince the first modern measurements of the precise cardinal orientations of the Giza pyramids by Flinders Petrie, various astronomical methods have been proposed for the original establishment of these orientations. It was recently proposed that this was done by observing the positions of two stars in the Plough / Big Dipper which was known to Egyptians as the thigh. It is thought that a vertical alignment between these two stars checked with a plumb bob was used to ascertain where north lay. The deviations from true north using this model reflect the accepted dates of construction.\nSome have argued that the pyramids were laid out as a map of the three stars in the belt of Orion, although this theory has been criticized by reputable astronomers. The site was instead probably governed by a spectacular hierophany which occurs at the summer solstice, when the Sun, viewed from the Sphinx terrace, forms\u2014together with the two giant pyramids\u2014the symbol Akhet, which was also the name of the Great Pyramid. Further, the south east corners of all the three pyramids align towards the temple of Heliopolis, as first discovered by the Egyptologist Mark Lehner.\nThe astronomical ceiling of the tomb of Senenmut (BC) contains the Celestial Diagram depicting circumpolar constellations in the form of discs. Each disc is divided into 24 sections suggesting a 24-hour time period. Constellations are portrayed as sacred deities of Egypt. The observation of lunar cycles is also evident.\nEl Castillo.\nEl Castillo, also known as Kukulc\u00e1n's Pyramid, is a Mesoamerican step-pyramid built in the centre of Mayan center of Chichen Itza in Mexico. Several architectural features have suggested astronomical elements. Each of the stairways built into the sides of the pyramid has 91 steps. Along with the extra one for the platform at the top, this totals 365 steps, which is possibly one for each day of the year (365.25) or the number of lunar orbits in 10,000 rotations (365.01).\nA visually striking effect is seen every March and September as an unusual shadow occurs around the equinoxes. Light and shadow phenomena have been proposed to explain a possible architectural hierophany involving the sun at Chich\u00e9n Itz\u00e1 in a Maya Toltec structure dating to about 1000 CE. A shadow appears to descend the west balustrade of the northern stairway. The visual effect is of a serpent descending the stairway, with its head at the base in light. Additionally the western face points to sunset around 25 May, traditionally the date of transition from the dry to the rainy season. The intended alignment was, however, likely incorporated in the northern (main) facade of the temple, as it corresponds to sunsets on May 20 and July 24, recorded also by the central axis of Castillo at Tulum. The two dates are separated by 65 and 300 days, and it has been shown that the solar orientations in Mesoamerica regularly correspond to dates separated by calendrically significant intervals (multiples of 13 and 20 days). In their discussion of the credibility of archaeoastronomical sites, Cotte and Ruggles used the \"equinox hierophany\" at Chich\u00e9n Itz\u00e1 as an example of an Unproven site, the third of their four levels of credibility.\nStonehenge.\nMany astronomical alignments have been claimed for Stonehenge, a complex of megaliths and earthworks in the Salisbury Plain of England. The most famous of these is the midsummer alignment, where the Sun rises over the Heel Stone. However, this interpretation has been challenged by some archaeologists who argue that the midwinter alignment, where the viewer is outside Stonehenge and sees the Sun setting in the henge, is the more significant alignment, and the midsummer alignment may be a coincidence due to local topography. In their discussion of the credibility of archaeoastronomical sites, Cotte and Ruggles gave Stonehenge as an example of a Generally accepted site, the highest of their four levels of credibility.\nAs well as solar alignments, there are proposed lunar alignments. The four station stones mark out a rectangle. The short sides point towards the midsummer sunrise and midwinter sunset. The long sides if viewed towards the south-east, face the most southerly rising of the Moon. Anthony Aveni notes that these lunar alignments have never gained the acceptance that the solar alignments have received.\nMaeshowe.\nThis is an architecturally outstanding Neolithic chambered tomb on the mainland of Orkney, Scotland\u2014probably dating to the early 3rd millennium BC, and where the setting Sun at midwinter shines down the entrance passage into the central chamber (see Newgrange). In the 1990s further investigations were carried out to discover whether this was an accurate or an approximate solar alignment. Several new aspects of the site were discovered. In the first place the entrance passage faces the hills of the island Hoy, about 10 miles away. Secondly, it consists of two straight lengths, angled at a few degrees to each other. Thirdly, the outer part is aligned towards the midwinter sunset position on a level horizon just to the left of Ward Hill on Hoy. Fourthly the inner part points directly at the Barnhouse standing stone about 400m away and then to the right end of the summit of Ward Hill, just before it dips down to the notch between it at Cuilags to the right. This indicated line points to sunset on the first Sixteenths of the solar year (according to A. Thom) before and after the winter solstice and the notch at the base of the right slope of the Hill is at the same declination. Fourthly a similar 'double sunset' phenomenon is seen at the right end of Cuilags, also on Hoy; here the date is the first Eighth of the year before and after the winter solstice, at the beginning of November and February respectively\u2014the Old Celtic festivals of Samhain and Imbolc. This alignment is not indicated by an artificial structure but gains plausibility from the other two indicated lines. Maeshowe is thus an extremely sophisticated calendar site which must have been positioned carefully in order to use the horizon foresights in the ways described.\nUxmal.\nUxmal is a Mayan city in the Puuc Hills of Yucat\u00e1n Peninsula, Mexico. The Governor's Palace at Uxmal is often used as an exemplar of why it is important to combine ethnographic and alignment data. The palace is aligned with an azimuth of 118\u00b0 on the pyramid of Cehtzuc. This alignment corresponds approximately to the southernmost rising and, with a much greater precision, to the northernmost setting of Venus; both phenomena occur once every eight years. By itself this would not be sufficient to argue for a meaningful connection between the two events. The palace has to be aligned in one direction or another and why should the rising of Venus be any more important than the rising of the Sun, Moon, other planets, Sirius \"et cetera\"? The answer given is that not only does the palace point towards significant points of Venus, it is also covered in glyphs which stand for Venus and Mayan zodiacal constellations. Moreover, the great northerly extremes of Venus always occur in late April or early May, coinciding with the onset of the rainy season. The Venus glyphs placed in the cheeks of the Maya rain god Chac, most likely referring to the concomitance of these phenomena, support the west-working orientation scheme.\nChaco Canyon.\nIn Chaco Canyon, the center of the ancient Pueblo culture in the American Southwest, numerous solar and lunar light markings and architectural and road alignments have been documented. These findings date to the 1977 discovery of the Sun Dagger site by Anna Sofaer. Three large stone slabs leaning against a cliff channel light and shadow markings onto two spiral petroglyphs on the cliff wall, marking the solstices, equinoxes and the lunar standstills of the 18.6 year cycle of the moon. Subsequent research by the Solstice Project and others demonstrated that numerous building and interbuilding alignments of the great houses of Chaco Canyon are oriented to solar, lunar and cardinal directions. In addition, research shows that the Great North Road, a thirty-five mile engineered \"road\", was constructed not for utilitarian purposes but rather to connect the ceremonial center of Chaco Canyon with the direction north.\nLascaux Cave.\nIn recent years, new research has suggested that the Lascaux cave paintings in France may incorporate prehistoric star charts. Michael Rappenglueck of the University of Munich argues that some of the non-figurative dot clusters and dots within some of the figurative images correlate with the constellations of Taurus, the Pleiades and the grouping known as the \"Summer Triangle\". Based on her own study of the astronomical significance of Bronze Age petroglyphs in the Vall\u00e9e des Merveilles and her extensive survey of other prehistoric cave painting sites in the region\u2014most of which appear to have been selected because the interiors are illuminated by the setting Sun on the day of the winter solstice\u2014French researcher Chantal J\u00e8gues-Wolkiewiez has further proposed that the gallery of figurative images in the Great Hall represents an extensive star map and that key points on major figures in the group correspond to stars in the main constellations as they appeared in the Paleolithic. Appliying phylogenetics to myths of the Cosmic Hunt, Julien d'Huy suggested that the palaeolithic version of this story could be the following: there is an animal that is a horned herbivore, especially an elk. One human pursues this ungulate. The hunt locates or gets to the sky. The animal is alive when it is transformed into a constellation. It forms the Big Dipper. This story may be represented in the famous Lascaux shaft 'scene'\nFringe archaeoastronomy.\nArchaeoastronomy owes something of a poor reputation among scholars due to its occasional misuse to advance a range of pseudo-historical accounts. During the 1930s, Otto S. Reuter compiled a study entitled \"Germanische Himmelskunde\", or \"Teutonic Skylore\". The astronomical orientations of ancient monuments claimed by Reuter and his followers would place the ancient Germanic peoples ahead of the Ancient Near East in the field of astronomy, demonstrating the intellectual superiority of the \"Aryans\" (Indo-Europeans) over the Semites.\nMore recently I. J. Gallagher, R. L. Pyle, and B. Fell interpreted inscriptions in West Virginia as a description in Celtic Ogham alphabet of the supposed winter solstitial marker at the site. The controversial translation was supposedly validated by a problematic archaeoastronomical indication in which the winter solstice Sun shone on an inscription of the Sun at the site. Subsequent analyses criticized its cultural inappropriateness, as well as its linguistic and archaeoastronomical claims, to describe it as an example of \"cult archaeology\".\nArchaeoastronomy is sometimes related to the fringe discipline of Archaeocryptography, when its followers attempt to find underlying mathematical orders beneath the proportions, size, and placement of archaeoastronomical sites such as Stonehenge and the Pyramid of Kukulc\u00e1n at Chichen Itza.\nIndia.\nSince the 19th century, numerous scholars have sought to use archaeoastronomical calculations to demonstrate the antiquity of Ancient Indian Vedic culture, computing the dates of astronomical observations ambiguously described in ancient poetry to as early as 4000 BC. David Pingree, a historian of Indian astronomy, condemned \"the scholars who perpetrate wild theories of prehistoric science and call themselves archaeoastronomers\".\nOrganisations.\nThere are currently several academic organisations for scholars of archaeoastronomy (including ethnoastronomy and Indigenous astronomy). \nISAACthe International Society for Archaeoastronomy and Astronomy in Culturewas founded in 1996 as the global society for the field. It sponsors the Oxford conferences and the Journal of Astronomy in Culture. \nSEAC \u2013 La Soci\u00e9t\u00e9 Europ\u00e9enne pour l'Astronomie dans la Culture was founded in 1992 with a focus on broader Europe. SEAC holds annual conferences in Europe and publishes refereed conference proceedings on an annual basis. \nSIACLa Sociedad Interamericana de Astronom\u00eda en la Cultura was founded in 2003 with a focus on Latin America. \nSCAAS - The Society for Cultural Astronomy in the American Southwest was founded in 2009 as a regional organisation focusing on the astronomies of the native peoples of the Southwestern United States; it has since held seven meetings and workshops. \nAAAC \u2013 the Australian Association for Astronomy in Culture was founded in 2020 in Australia, focusing on Aboriginal and Torres Strait Islander astronomy. \nThe Romanian Society for Cultural Astronomy was founded in 2019, holding an annual international conference and publishing the first monograph on archaeo- and ethnoastronomy in Romania (2019).\nSMART \u2013 the Society of M\u0101ori Astronomy Research and Traditions was founded in Aotearoa/New Zealand in 2013, focusing on Maori astronomy.\nNative Skywatchers was founded in 2007 in Minnesota, USA to promote Native American star knowledge, particularly of the Lakota and Ojibwe peoples of the northern US and Canada.\nPublications.\nAdditionally the \"Journal for the History of Astronomy\" publishes many archaeoastronomical papers. For twenty-seven volumes (from 1979 to 2002) it published an annual supplement \"Archaeoastronomy\". \nThe \"Journal of Astronomical History and Heritage\", \"Culture &amp; Cosmos\", and the \"Journal of Skyscape Archaeology\" also publish papers on archaeoastronomy.\nAcademic programs.\nNational projects and university programs including, or dedicated to, cultural astronomy are found globally. They include:\nThe Sophia Centre for Cosmology in Culture at the University of Wales - Trinity Saint David in Lampeter, UK.\nThe Cultural Astronomy Program at the University of Melbourne in Australia.\nThe Tata Institute of Fundamental Research made interesting findings in this field."}
{"id": "2865", "revid": "8926256", "url": "https://en.wikipedia.org/wiki?curid=2865", "title": "Andrzej Sapkowski", "text": "Andrzej Sapkowski (; born 21 June 1948) is a Polish fantasy writer. He is best known for his series of books \"The Witcher\", which revolves around the eponymous monster-hunter, Geralt of Rivia. The saga has been popularized through television, stage, comic books, video games and translated into 37 languages making him the second most-translated Polish science fiction and fantasy writer after Stanis\u0142aw Lem. \nDescribed as the \"Polish Tolkien\", he wrote multiple novels short story collections, which sold over 30 million copies worldwide. The influence of Slavic mythology is seen as a characteristic feature of many of his works. He is a five-time recipient of the Zajdel Award, Poland's most popular science fiction and fantasy prize, as well as many other awards and honors including David Gemmell Award, World Fantasy Life Achievement Award and the Gloria Artis Medal for Merit to Culture.\nEarly life.\nHe was born on 21 June 1948 in \u0141\u00f3d\u017a, in central Poland. His father served in the Polish People's Army and participated in the Battle of Berlin. After the end of World War II, his parents lived near Nowa S\u00f3l before settling in \u0141\u00f3d\u017a. He attended the Boles\u0142aw Prus High School No. 21. He also studied economics at the University of \u0141\u00f3d\u017a, and before turning to writing, he had worked as a senior sales representative for a foreign trade company. He started his literary career as a translator, in particular, of science fiction. Among the first works translated by him was \"The Words of Guru\" by Cyril M. Kornbluth.\nCareer.\nMajor works.\nHe says he wrote his first short story, \"The Witcher\" (1986), (\"Wied\u017amin\", also translated \"The Hexer\" or \"Spellmaker\"), on a whim, in order to enter a contest by Polish science fiction and fantasy magazine \"Fantastyka\". In an interview, he said that being a businessman at the time and thus familiar with marketing, he knew how to sell, and indeed, he won third prize. The story was published in \"Fantastyka\" in 1986 and was enormously successful both with readers and critics. Sapkowski has created a cycle of tales based on the world of \"The Witcher\", comprising three collections of short stories and eight novels. This cycle and his other works have made him one of the best-known fantasy authors in Poland in the 1990s.\nThe main character of \"The Witcher\" is Geralt of Rivia, trained as a monster hunter since childhood. Geralt exists in a morally ambiguous universe, yet manages to maintain his own coherent code of ethics. At the same time cynical and noble, Geralt has been compared to Raymond Chandler's signature character Philip Marlowe. The world in which these adventures take place is heavily influenced by Slavic mythology.\nIn her review of \"Blood of Elves\", Alice Wybrew of \"Total Sci-Fi\" writes that \"Moving effortlessly between moments of wrought emotion and staggeringly effective action, to lengthy periods of political discussion and war stratagems, Sapkowski addresses every aspect of a good fantasy novel eloquently and with ease. His style reads as easily as David Gemmel, but hits harder and deeper than his late fantasy comrade. Creating a world that is both familiar and comfortable, it is through his inventive use of character manipulation that he generates a new and realistic experience\". Alex Jay of \"Polygon\" further observes that within Sapkowski's fantasy tales, \"there are parallels to the complicated history of ethnic strife and resistance to oppression in Central and Eastern Europe\". The depictions of the disputes between nonhumans and humans \"echo real-world disputes over territory and citizenship that draw dividing lines according to race, nationality, or ethnicity\".\nIn 2001, he published the \"Manuscript Found in a Dragon's Cave\", an original and personal guide to fantasy literature. It was written in the form of an encyclopaedia and the author discusses in it the history of the literary genre, well-known fantasy heroes, descriptions of magic terminology as well as major works of notable writers including J. R. R. Tolkien's \"The Hobbit\" and \"The Lord of the Rings\", Robert E. Howard's \"Conan\", C. S. Lewis's \"The Chronicles of Narnia\", Ursula K. Le Guin's \"Earthsea\", Roger Zelazny's \"The Chronicles of Amber\", J. K. Rowling's \"Harry Potter\", and George R. R. Martin's \"A Song of Ice and Fire\". \nSapkowski's next book series was the \"Hussite Trilogy\" set in the 15th century at the time of the Hussite Wars with Reinmar of Bielawa as the main protagonist. Mariusz Czubaj writes:\nAlthough the \"Hussite Trilogy\" proved less popular compared to \"The Witcher\", it has been described as the author's \"magnum opus\". Published between 2002 and 2006, the series was released as an audiobook in 2019.\nIn August 2023, Sapkowski announced he was working on a new novel from \"The Witcher\" universe during an on-line meeting with his Ukrainian fans. He added that his work on the book \"may take a year, but no longer\" giving it a potential expected publication date at some point in 2024.\nLegal dispute with CD Projekt.\nIn October 2018, he sent an open letter to CD Projekt demanding 60 million zloty ($16.1 million) in royalty payments from the company for using the Witcher universe in their computer games. The letter was written despite the fact that Sapkowski had sold the video game rights to the Witcher for a single sum, rather than through a royalties contract. Sapkowski and his lawyers based their claims on Article 44 of the Copyright and Related Rights Act.\nCD Projekt released a statement claiming that the author's demands are groundless and that the company had legitimately and legally acquired copyright to Sapkowski's works. His decision was criticized by many commentators and gaming journalists including Dmitry Glukhovsky, the author of \"Metro 2033\", who described him as \"an old fool\" and noted that without the gaming franchise, the Witcher series \"would never get this crazy international readership\" and would have remained popular only in Central and Eastern Europe. \nOn 20 December 2019, the writer and the company resolved the dispute with an amicable settlement. The company stated this deal was made in an effort \"to maintain good relations with authors of works which have inspired CD Projekt Red's own creations.\" The details of this arrangement were not made public.\nPersonal life.\nSapkowski resides in his hometown of \u0141\u00f3d\u017a in central Poland. He had a son named Krzysztof (1972\u20132019), who was an avid reader of the Polish \"Fantastyka\" magazine, and for whom he wrote the first \"Witcher\" story, who has since deceased.\nSapkowski is a member of the Polish Writers Association. In an interview, he mentioned that his favorite writers included Ernest Hemingway, Mikhail Bulgakov, Raymond Chandler and Umberto Eco. \nIn 2005, Stanis\u0142aw Bere\u015b conducted a lengthy interview with Sapkowski that was eventually published in a book form as \"Historia i fantastyka\".\nTranslations and adaptations of Sapkowski's works.\nSapkowski's books have been translated into Bulgarian, Chinese, Croatian, Czech, Dutch, English, Estonian, Finnish, French, Georgian, German, Greek, Hebrew, Hungarian, Italian, Korean, Lithuanian, Norwegian, Persian, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Ukrainian. An English translation of \"The Last Wish\" short story collection was published by Gollancz in 2007. From 2008, the Witcher saga is published by Gollancz. The English translation of Sapkowski's novel \"Blood of Elves\" won the David Gemmell Legend Award in 2009.\nIn the years 1993\u20131995, a six-issue comic book series entitled \"The Witcher\" was released in the \"Komiks\" magazine by Pr\u00f3szy\u0144ski i S-ka publishing house. The comic was written by Maciej Parowski and illustrated by Bogus\u0142aw Polch. The comics were the first attempt to portray the Witcher universe outside the novels. Since 2014, a comic book series \"The Witcher\" has been published by the American publisher Dark Horse Comics. The stories presented in the series are mostly originals, written not by Andrzej Sapkowski but by other writers; the exception being volume 2, Fox Children, which adapted a story from the anthology \"Season of Storms\". \nIn 2001, a television series based on the \"Witcher\" cycle was released in Poland and internationally, entitled \"Wied\u017amin\" (\"The Hexer\"). A film by the same title was compiled from excerpts of the television series but both have been critical and box office failures.\nIn 2009, Russian heavy metal band Esse staged \"The Road with No Return\", a rock opera based on the works by Sapkowski. Yevgeny Pronin is the author of the libretto and the composer of much of the opera's music. The premiere of the opera took place the same year in Rostov-on-Don and was subsequently released as a DVD in 2012.\nThe Polish game developer, CD Projekt Red, created a role-playing game series based on \"The Witcher\" universe. The first game, titled simply \"The Witcher\", was first released in October 2007. The sequel, ' was released in 2011. The third game in the trilogy, ', was released in May 2015. The game shipped over 40 million copies, making it one of the best selling video games of all time.\nIn May 2017, Netflix commissioned \"The Witcher\", an English-language adaptation of the book series. \"The Witcher\" television series premiered on Netflix on 20 December 2019. Sapkowski served for a while as a creative consultant on the project. The popularity of the Netflix show led to Sapkowski topping Amazon's list of best-selling authors ahead of J.K. Rowling and Stephen King. A spin-off anime \"\", produced by Lauren Schmidt Hissrich, premiered in 2021. \nIn September 2017, a musical \"Wied\u017amin\" (The Witcher) directed by Wojciech Ko\u015bcielniak was premiered at the Musical Theatre in Gdynia. \n\"\" is a fantasy miniseries created by Declan de Barra and Lauren Schmidt Hissrich adapted from \"The Witcher\" book series which serves as a prequel to the Netflix television series. It was released on Netflix in December 2022.\nAwards and recognition.\nSapkowski is a recipient of numerous awards and honours both Polish and foreign including: \nBibliography.\nThe Witcher Saga.\nNew book.\nShortly before the release of \"Rozdro\u017ce kruk\u00f3w\", in an interview for the Polish weekly periodical \"Polityka\", Sapkowski announced that it would not be the last book set in the \"Witcher\" universe. Again, he did not provide more details, saying only that \u201cthere'll be a new book\u201d and \u201cif I set the time horizon to three or four years, it'll be without much risk\u201d."}
{"id": "2866", "revid": "47722236", "url": "https://en.wikipedia.org/wiki?curid=2866", "title": "Ammeter", "text": "An ammeter (abbreviation of \"ampere meter\") is an instrument used to measure the current in a circuit. Electric currents are measured in amperes (A), hence the name. For direct measurement, the ammeter is connected in series with the circuit in which the current is to be measured. An ammeter usually has low resistance so that it does not cause a significant voltage drop in the circuit being measured.\nInstruments used to measure smaller currents, in the milliampere or microampere range, are designated as \"milliammeters\" or \"microammeters\". Early ammeters were laboratory instruments that relied on the Earth's magnetic field for operation. By the late 19th\u00a0century, improved instruments were designed which could be mounted in any position and allowed accurate measurements in electric power systems. It is generally represented by letter 'A' in a circuit. \nHistory.\nThe relation between electric current, magnetic fields and physical forces was first noted by Hans Christian \u00d8rsted in 1820, who observed a compass needle was deflected from pointing North when a current flowed in an adjacent wire. The tangent galvanometer was used to measure currents using this effect, where the restoring force returning the pointer to the zero position was provided by the Earth's magnetic field. This made these instruments usable only when aligned with the Earth's field. Sensitivity of the instrument was increased by using additional turns of wire to multiply the effect \u2013 the instruments were called \"multipliers\".\nThe word \"rheoscope\" as a detector of electrical currents was coined by Sir Charles Wheatstone about 1840 but is no longer used to describe electrical instruments. The word makeup is similar to that of \"rheostat\" (also coined by Wheatstone) which was a device used to adjust the current in a circuit. Rheostat is a historical term for a variable resistance, though unlike rheoscope may still be encountered.\nTypes.\nSome instruments are \"panel meters\", meant to be mounted on some sort of control panel. Of these, the flat, horizontal or vertical type is often called an \"edgewise meter\".\nMoving-coil.\nThe D'Arsonval galvanometer is a moving coil ammeter. It uses magnetic deflection, where current passing through a coil placed in the magnetic field of a permanent magnet causes the coil to move. The modern form of this instrument was developed by Edward Weston, and uses two spiral springs to provide the restoring force. The uniform air gap between the iron core and the permanent magnet poles make the deflection of the meter linearly proportional to current. These meters have linear scales. Basic meter movements can have full-scale deflection for currents from about 25\u00a0microamperes to 10\u00a0milliamperes.\nBecause the magnetic field is polarised, the meter needle acts in opposite directions for each direction of current. A DC\u00a0ammeter is thus sensitive to which polarity it is connected in; most are marked with a positive terminal, but some have centre-zero mechanisms\nand can display currents in either direction. A moving coil meter indicates the average (mean) of a varying current through it,\nwhich is zero for AC. For this reason, moving-coil meters are only usable directly for DC, not AC.\nThis type of meter movement is extremely common for both ammeters and other meters derived from them, such as voltmeters and ohmmeters.\nMoving magnet.\nMoving magnet ammeters operate on essentially the same principle as moving coil, except that the coil is mounted in the meter case, and a permanent magnet moves the needle. Moving magnet Ammeters are able to carry larger currents than moving coil instruments, often several tens of amperes, because the coil can be made of thicker wire and the current does not have to be carried by the hairsprings. Indeed, some Ammeters of this type do not have hairsprings at all, instead using a fixed permanent magnet to provide the restoring force.\nElectrodynamic.\nAn electrodynamic ammeter uses an electromagnet instead of the permanent magnet of the d'Arsonval movement. This instrument can respond to both alternating and direct current and also indicates true RMS for AC. See wattmeter for an alternative use for this instrument.\nMoving-iron.\nMoving iron ammeters use a piece of iron which moves when acted upon by the electromagnetic force of a fixed coil of wire. The moving-iron meter was invented by Austrian engineer Friedrich Drexler in 1884.\nThis type of meter responds to both direct and alternating currents (as opposed to the moving-coil ammeter, which works on direct current only). The iron element consists of a moving vane attached to a pointer, and a fixed vane, surrounded by a coil. As alternating or direct current flows through the coil and induces a magnetic field in both vanes, the vanes repel each other and the moving vane deflects against the restoring force provided by fine helical springs. The deflection of a moving iron meter is proportional to the square of the current. Consequently, such meters would normally have a nonlinear scale, but the iron parts are usually modified in shape to make the scale fairly linear over most of its range. Moving iron instruments indicate the RMS value of any AC waveform applied. Moving iron ammeters are commonly used to measure current in industrial frequency AC circuits.\nHot-wire.\nIn a hot-wire ammeter, a current passes through a wire which expands as it heats. Although these instruments have slow response time and low accuracy, they were sometimes used in measuring radio-frequency current.\nThese also measure true RMS for an applied AC.\nDigital.\nIn much the same way as the analogue ammeter formed the basis for a wide variety of derived meters, including voltmeters, the basic mechanism for a digital meter is a digital voltmeter mechanism, and other types of meter are built around this.\nDigital ammeter designs use a shunt resistor to produce a calibrated voltage proportional to the current flowing. This voltage is then measured by a digital voltmeter, through use of an analog-to-digital converter (ADC); the digital display is calibrated to display the current through the shunt. Such instruments are often calibrated to indicate the RMS value for a sine wave only, but many designs will indicate true RMS within limitations of the wave crest factor.\nIntegrating.\nThere is also a range of devices referred to as integrating ammeters.\nIn these ammeters the current is summed over time, giving as a result the product of current and time; which is proportional to the electrical charge transferred with that current. These can be used for metering energy (the charge needs to be multiplied by the voltage to give energy) or for estimating the charge of a battery or capacitor.\nPicoammeter.\nA picoammeter, or pico ammeter, measures very low electric current, usually from the picoampere range at the lower end to the milliampere range at the upper end. Picoammeters are used where the current being measured is below the limits of sensitivity of other devices, such as multimeters.\nMost picoammeters use a \"virtual short\" technique and have several different measurement ranges that must be switched between to cover multiple decades of measurement. Other modern picoammeters use log compression and a \"current sink\" method that eliminates range switching and associated voltage spikes.\nSpecial design and usage considerations must be observed in order to reduce leakage current which may swamp measurements such as special insulators and driven shields. Triaxial cable is often used for probe connections.\nApplication.\nAmmeters must be connected in series with the circuit to be measured. For relatively small currents (up to a few amperes), an ammeter may pass the whole of the circuit current. For larger direct currents, a shunt resistor carries most of the circuit current and a small, accurately-known fraction of the current passes through the meter movement. For alternating current circuits, a current transformer may be used to provide a convenient small current to drive an instrument, such as 1 or 5 amperes, while the primary current to be measured is much larger (up to thousands of amperes). The use of a shunt or current transformer also allows convenient location of the indicating meter without the need to run heavy circuit conductors up to the point of observation. In the case of alternating current, the use of a current transformer also isolates the meter from the high voltage of the primary circuit. A shunt provides no such isolation for a direct-current ammeter, but where high voltages are used it may be possible to place the ammeter in the \"return\" side of the circuit which may be at low potential with respect to earth.\nAmmeters must not be connected directly across a voltage source since their internal resistance is very low and excess current would flow. Ammeters are designed for a low voltage drop across their terminals, much less than one volt; the extra circuit losses produced by the ammeter are called its \"burden\" on the measured circuit(I).\nOrdinary Weston-type meter movements can measure only milliamperes at most, because the springs and practical coils can carry only limited currents. To measure larger currents, a resistor called a \"shunt\" is placed in parallel with the meter. The resistances of shunts is in the integer to fractional milliohm range. Nearly all of the current flows through the shunt, and only a small fraction flows through the meter. This allows the meter to measure large currents. Traditionally, the meter used with a shunt has a full-scale deflection (FSD) of , so shunts are typically designed to produce a voltage drop of when carrying their full rated current.\nTo make a multi-range ammeter, a selector switch can be used to connect one of a number of shunts across the meter. It must be a make-before-break switch to avoid damaging current surges through the meter movement when switching ranges.\nA better arrangement is the Ayrton shunt or universal shunt, invented by William E. Ayrton, which does not require a make-before-break switch. It also avoids any inaccuracy because of contact resistance. In the figure, assuming for example, a movement with a full-scale voltage of 50\u00a0mV and desired current ranges of 10\u00a0mA, 100\u00a0mA, and 1\u00a0A, the resistance values would be: R1 = 4.5\u00a0ohms, R2 = 0.45\u00a0ohm, R3 = 0.05\u00a0ohm. And if the movement resistance is 1000\u00a0ohms, for example, R1 must be adjusted to 4.525\u00a0ohms.\nSwitched shunts are rarely used for currents above 10 amperes.\nZero-center ammeters are used for applications requiring current to be measured with both polarities, common in scientific and industrial equipment. Zero-center ammeters are also commonly placed in series with a battery. In this application, the charging of the battery deflects the needle to one side of the scale (commonly, the right side) and the discharging of the battery deflects the needle to the other side. A special type of zero-center ammeter for testing high currents in cars and trucks has a pivoted bar magnet that moves the pointer, and a fixed bar magnet to keep the pointer centered with no current. The magnetic field around the wire carrying current to be measured deflects the moving magnet.\nSince the ammeter shunt has a very low resistance, mistakenly wiring the ammeter in parallel with a voltage source will cause a short circuit, at best blowing a fuse, possibly damaging the instrument and wiring, and exposing an observer to injury.\nIn AC circuits, a current transformer can be used to convert the large current in the main circuit into a smaller current more suited to a meter. Some designs of transformer are able to directly convert the magnetic field around a conductor into a small AC current, typically either or at full rated current, that can be easily read by a meter. In a similar way, accurate AC/DC non-contact ammeters have been constructed using Hall effect magnetic field sensors. A portable hand-held clamp-on ammeter is a common tool for maintenance of industrial and commercial electrical equipment, which is temporarily clipped over a wire to measure current. Some recent types have a parallel pair of magnetically soft probes that are placed on either side of the conductor."}
{"id": "2868", "revid": "17790", "url": "https://en.wikipedia.org/wiki?curid=2868", "title": "Amanda Hesser", "text": "Amanda Hesser (born 1971) is an American food writer, editor, cookbook author and entrepreneur. Most notably, she was the food editor of \"The New York Times Magazine\", the editor of \"T Living\", a quarterly publication of \"The New York Times\", author of \"The Essential New York Times Cookbook\" which was a \"New York Times\" bestseller, and co-founder and CEO of Food52.\nBiography.\nAfter finishing her first book, in 1997, Hesser was hired as a food reporter for \"The New York Times\" where she wrote more than 750 stories. While at the \"Times,\" Hesser wrote about the influence of Costco on the wine industry, and how the Farmer Consumer Advisory Committee made decisions for the New York City Greenmarket. She was also among the first to write about Ferran Adri\u00e0 of El Bulli in a major American publication.\nHesser was involved in two cases of conflict of interest while working at the \"Times\". In 2004, she awarded the restaurant Spice Market a three-star rating without disclosing that the year before, the restaurant's owner, Jean-Georges Vongerichten, had provided a complimentary jacket blurb for her book \"Cooking for Mr. Latte\". In 2007, Hesser published a favorable review of \"Vegetable Harvest\" by Patricia Wells without noting that in 1999, Wells had provided a jacket blurb for Hesser's book \"The Cook and the Gardener\". In both cases, the \"Times\" subsequently pointed out the conflicts of interest with editors' notes.\nWhile Hesser left the \"Times\" in March 2008 to focus on the development of Food52, she continued to write the \"Recipe Redux\" feature for the \"Times\" magazine until February 27, 2011.\nAs co-founder and CEO of Food52, she has raised two rounds of investment from parties including Lerer Hippeau Ventures and Bertelsmann Digital Media Investments. Food52 has won numerous notable awards, including the James Beard Foundation Award for Publication of the Year (2012) and the International Association of Culinary Professionals Award for Best Website (2013). In February 2017, noting that 92 percent of the company was white, she and her co-founder Merrill Stubbs \"issued a statement about the ways in which the company intended to redress a lack of racial equality in its workplace.\" By the following January, \"they published a follow-up letter updating readers on the progress of their efforts, stating that their staff had been reduced to being 76 percent white.\"\nHesser was featured in \"Food &amp; Wine\"'s \"40 under 40\" list, was named one of the 50 most influential women in food by \"Gourmet\" magazine, and had a cameo as herself in the film \"Julie &amp; Julia\".\nHesser lives in Brooklyn Heights with her husband, Tad Friend, a staff writer for \"The New Yorker\", and their two children."}
{"id": "2869", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=2869", "title": "Anxiolytic", "text": "An anxiolytic (; also antipanic or anti-anxiety agent) is a medication or other intervention that reduces anxiety. This effect is in contrast to anxiogenic agents which increase anxiety. Anxiolytic medications are used for the treatment of anxiety disorders and their related psychological and physical symptoms.\nNature of anxiety.\nAnxiety is a naturally-occurring emotion and response. When anxiety levels exceed the tolerability of a person, anxiety disorders may occur. People with anxiety disorders can exhibit fear responses, such as defensive behaviors, high levels of alertness, and negative emotions. Those with anxiety disorders may have concurrent psychological disorders, such as depression. Anxiety disorders are classified using six possible clinical assessments:\nDifferent types of anxiety disorders will share some general symptoms while having their own distinctive symptoms. This explains why people with different types of anxiety disorders will respond differently to different classes of anti-anxiety medications.\nEtiology.\nThe etiology of anxiety disorder remains unknown. There are several contributing factors that are still yet to be proved to cause anxiety disorders. These factors include childhood anxiety, drug induction by central stimulant drugs, metabolic diseases or having depressive disorder.\nMedications.\nAnti-anxiety medication is any drug that can be taken or prescribed for the treatment of anxiety disorders, which may be mediated by neurotransmitters like norepinephrine, serotonin, dopamine, and gamma-aminobutyric acid (GABA) in the central nervous system. Anti-anxiety medication can be classified into six types according to their different mechanisms: antidepressants, benzodiazepines, azapirones, antiepileptics, antipsychotics, and beta blockers.\nAntidepressants include selective serotonin reuptake inhibitors (SSRIs), serotonin\u2013norepinephrine reuptake inhibitors (SNRIs), tricyclic antidepressants (TCAs), and monoamine oxidase inhibitors (MAOIs). SSRIs are used in all types of anxiety disorders while SNRIs are used for generalized anxiety disorder (GAD). Both of them are considered as first-line anti-anxiety medications. TCAs are second-line treatment as they cause more significant adverse effects when compared to the first-line treatment. Benzodiazepines are effective in emergent and short-term treatment of anxiety disorders due to their fast onset but carry the risk of dependence. Buspirone is indicated for GAD, which has much slower onset but with the advantage of less sedating and withdrawal effects.\nHistory.\nThe first monoamine oxidase inhibitor (MAOI), iproniazid, was discovered accidentally when developing the new antitubercular drug isoniazid. The drug was found to induce euphoria and improve the patient's appetite and sleep quality.\nThe first tricyclic antidepressant, imipramine, was originally developed and studied to be an antihistamine alongside other first-generation antihistamines of the time, such as promethazine. TCAs can increase the level of norepinephrine and serotonin by inhibiting their reuptake transport proteins. The majority of TCAs exert greater effect on norepinephrine, which leads to side effects like drowsiness and memory loss. \nIn order to be more effective on serotonin agonism and avoid anticholinergic and antihistaminergic side effects, selective serotonin reuptake inhibitors (SSRI) were researched and introduced to treat anxiety disorders. The first SSRI, fluoxetine (Prozac), was discovered in 1974 and approved by FDA in 1987. After that, other SSRIs like sertraline (Zoloft), paroxetine (Paxil), and escitalopram (Lexapro) have entered the market.\nThe first serotonin norepinephrine reuptake inhibitor (SNRI), venlafaxine (Effexor), entered the market in 1993. SNRIs can target serotonin and norepinephrine transporters while avoiding imposing significant effects on other adrenergic (\u03b11, \u03b12, and \u03b2), histamine (H1), muscarinic, dopamine, or postsynaptic serotonin receptors.\nClassifications.\nThere are six groups of anti-anxiety medications available that have been proven to be clinically significant in treatment of anxiety disorders. The groups of medications are as follows.\nAntidepressants.\nMedications that are indicated for both anxiety disorders and depression. Selective serotonin reuptake inhibitors (SSRIs) and serotonin\u2013norepinephrine reuptake inhibitors (SNRIs) are new generations of antidepressants. They have a much lower adverse effect profile than older antidepressants like monoamine oxidase inhibitors (MAOIs) and tricyclic antidepressants (TCAs). Therefore, SSRIs and SNRIs are now the first-line agent in treating long term anxiety disorders, given their applications and significance in all six types of disorders.\nBenzodiazepines.\nBenzodiazepines are used for acute anxiety and could be added along with current use of SSRIs to stabilize a treatment. Long-term use in treatment plans is not recommended. Different kinds of benzodiazepine will vary in its pharmacological profile, including its strength of effect and time taken for metabolism. The choice of the benzodiazepine will depend on the corresponding profiles.\nBenzodiazepines are used for emergent or short-term management. They are not recommended as the first-line anti-anxiety drugs, but they can be used in combination with SSRIs/SNRIs during the initial treatment stage. Indications include panic disorder, sleep disorders, seizures, acute behavioral disturbance, muscle spasm and premedication and sedation for procedures.\nAzapirones.\nBuspirone can be useful in GAD but not particularly effective in treating phobias, panic disorder or social anxiety disorders. It is a safer option for long-term use as it does not cause dependence like benzodiazepines.\nAntiepileptics.\nAntiepileptics are rarely prescribed as an off-label treatment for anxiety disorders and post-traumatic stress disorders. There have been some suggestions that they may help with anxiety symptoms but there is generally a lack of research on its use.\nOne antiepileptic, pregabalin, has been found to be better at treating GAD than a placebo, and comparable effects to benzodiazepines. It has also been shown be potentially efficient in treating social anxiety disorder. Gabapentin has been prescribed off-label for anxiety despite a lack of research evidence supporting such use, although some studies have indicated that it may relieve anxiety symptoms. The potential anxiolytic effect of tiagabine has been observed in some pre-clinical trials, but its effectiveness has not yet been proved. Similarly, there is a lack of research on valproate for the treatment of anxiety disorders.\nAntipsychotics.\nOlanzapine and risperidone are atypical antipsychotics which are also effective in GAD and PTSD treatment. However, there is a higher chance of experiencing adverse effects than the other anti-anxiety medications.\nBeta-adrenoceptor antagonists.\nPropranolol is originally used for high blood pressure and heart diseases. It can also be used to treat anxiety with symptoms like tremor or increased heart rate. They work on the nervous system and alleviate the symptoms as a relief. Propranolol is also commonly used for public speaking when one is nervous.\nMechanism of action.\nSSRIs and SNRIs.\nBoth selective serotonin reuptake inhibitors (SSRI) and serotonin and norepinephrine reuptake inhibitors (SNRI) are reuptake inhibitors of a class of nerve signal transduction chemical called neurotransmitters. Serotonin and norepinephrine are neurotransmitters that are related to nervous control in mood regulation. The level of these neurotransmitters is regulated by the nerve through reuptake to avoid accumulation of the neurotransmitter at the endings of nerve fibers. By reuptaking the neurotransmitter, the level of neuronal activity will go back down and be ready to go back up upon excitation from a new nerve signal. However the neurotransmitter level of patients with anxiety disorders is usually low or the patients\u2019 nerve fibers are insensitive to the neurotransmitters. SSRIs and SNRIs will then block the channel of reuptake and increase the level of the neurotransmitter. The nerve fibers will inhibit further production of neurotransmitters upon the increase. However the prolonged increase will eventually desensitize the nerve about the change in level. Therefore, the action of both SSRIs and SNRIs will take 4\u20136 weeks to exert their full effect.\nBenzodiazepine.\nBenzodiazepines bind selectively to the GABA receptor, which is the receptor protein found in the nervous system and is in control of the nervous response. Benzodiazepine will increase the entry of chloride ions into the cells by improving the binding between GABA and GABA receptors and then the better opening of the channel for chloride ion passage. The high level of chloride ion inside the nerve cells makes the nerve more difficult to depolarize and inhibit further nerve signal transduction. The excitability of the nerves then reduces and the nervous system slows down. Therefore, the drug can alleviate symptoms of anxiety disorder and make the person less nervous.\nClinical use.\nSelective serotonin reuptake inhibitors.\nSelective serotonin reuptake inhibitors (SSRIs) are a class of medications used in the treatment of depression, anxiety disorders, OCD and some personality disorders. SSRIs are the first-line anti-anxiety medications. Serotonin is one of the crucial neurotransmitters in mood enhancement, and increasing serotonin level produces an anti-anxiety effect. SSRIs increase the serotonin level in the brain by inhibiting serotonin uptake pumps on serotonergic systems, without interactions with other receptors and ion channels. SSRIs are beneficial in both acute response and long-term maintenance treatment for both depression and anxiety disorder.\nSSRIs can increase anxiety initially due to negative feedback through the serotonergic autoreceptors; for this reason a concurrent benzodiazepine can be used until the anxiolytic effect of the SSRI occurs.\nThe SSRIs paroxetine and escitalopram are USFDA approved to treat generalized anxiety disorder.\nAdverse effect.\nThe common early side effects of SSRIs include nausea and loose stool, which can be solved by discontinuing the treatment. Headache, dizziness, insomnia are the common early side effects as well.\nSexual dysfunction, anorgasmia, erectile dysfunction, and reduced libido are common adverse side effects of SSRIs. Sometimes they may persist after the cessation of treatment.\nWithdrawal symptoms like dizziness, headache and flu-like symptoms (fatigue/myalgia/loose stool) may occur if SSRI is stopped suddenly. The brain is incapable of upregulating the receptors to sufficient levels especially after discontinuation of the drugs with short half life like paroxetine. Both fluoxetine and its active metabolite have a long half life therefore it causes the least withdrawal symptoms.\nSerotonin\u2013norepinephrine reuptake inhibitors.\nSerotonin\u2013norepinephrine reuptake inhibitor (SNRIs) include venlafaxine and duloxetine drugs. Venlafaxine, in extended release form, and duloxetine, are indicated for the treatment of GAD. SNRIs are as effective as SSRIs in the treatment of anxiety disorders.\nTricyclic antidepressants.\nTricyclic antidepressants (TCAs) have anxiolytic effects; however, side effects are often more troubling or severe and overdose is dangerous. They are considered effective, but have generally been replaced by antidepressants that cause different adverse effects. Examples include imipramine, doxepin, amitriptyline, nortriptyline and desipramine.\nContraindication.\nTCAs may cause drug poisoning in patients with hypotension, cardiovascular diseases and arrhythmias.\nTetracyclic antidepressants.\nMirtazapine has demonstrated anxiolytic effect comparable to SSRIs while rarely causing or exacerbating anxiety. Mirtazapine's anxiety reduction tends to occur significantly faster than SSRIs.\nMonoamine oxidase inhibitors.\nMonoamine oxidase inhibitors (MAOIs) are first-generation antidepressants effective for anxiety treatment but their dietary restrictions, adverse effect profile and availability of newer medications have limited their use. MAOIs include phenelzine, isocarboxazid and tranylcypromine. Pirlindole is a reversible MAOI that lacks dietary restriction.\nBarbiturates.\nBarbiturates are powerful anxiolytics but the risk of abuse and addiction is high. Many experts consider these drugs obsolete for treating anxiety but valuable for the short-term treatment of severe insomnia, though only after benzodiazepines or non-benzodiazepines have failed.\nBenzodiazepines.\nBenzodiazepines are prescribed to quell panic attacks. Benzodiazepines are also prescribed in tandem with an antidepressant for the latent period of efficacy associated with many ADs for anxiety disorder. There is risk of benzodiazepine withdrawal and rebound syndrome if BZDs are rapidly discontinued. Tolerance and dependence may occur. The risk of abuse in this class of medication is smaller than in that of barbiturates. Cognitive and behavioral adverse effects are possible.\nBenzodiazepines include:\nalprazolam (Xanax), bromazepam,\nchlordiazepoxide (Librium),\nclonazepam (Klonopin),\ndiazepam (Valium),\nlorazepam (Ativan),\noxazepam,\ntemazepam, and Triazolam.\nAdverse effect.\nBenzodiazepines lead to central nervous system depression, resulting in common adverse effects like drowsiness, oversedation, light-headedness. Memory impairment can be a common adverse effect especially in elderly, hypersalivation, ataxia, slurred speech, psychomotor effects.\nSympatholytics.\nSympatholytics are a group of anti-hypertensives which inhibit activity of the sympathetic nervous system. Beta blockers reduce anxiety by decreasing heart rate and preventing shaking. Beta blockers include propranolol, oxprenolol, and metoprolol. The alpha-1 antagonist prazosin could be effective for PTSD. The alpha-2 agonists clonidine and guanfacine have demonstrated both anxiolytic and anxiogenic effects.\nMiscellaneous.\nBuspirone.\nBuspirone (Buspar) is a 5-HT1A receptor agonist used to treated generalized anxiety disorder. If an individual has only recently stopped taking benzodiazepines, buspirone will be less effective.\nPregabalin.\nPregabalin (Lyrica) produces anxiolytic effect after one week of use comparable to lorazepam, alprazolam, and venlafaxine with more consistent psychic and somatic anxiety reduction. Unlike BZDs, it does not disrupt sleep architecture nor does it cause\ncognitive or psychomotor impairment.\nHydroxyzine.\nHydroxyzine (Atarax) is an antihistamine originally approved for clinical use by the FDA in 1956. Hydroxyzine has a calming effect which helps ameliorate anxiety. Hydroxyzine efficacy is comparable to benzodiazepines in the treatment of generalized anxiety disorder.\nPhenibut.\nPhenibut (Anvifen, Fenibut, Noofen) is an anxiolytic used in Russia. Phenibut is a GABAB receptor agonist, as well as an antagonist at \u03b12\u03b4 subunit-containing voltage-dependent calcium channels (VDCCs), similarly to gabapentinoids like gabapentin and pregabalin. The medication is not approved by the FDA for use in the United States, but is sold online as a supplement.\nTemgicoluril.\nTemgicoluril (Mebicar) is an anxiolytic produced in Latvia and used in Eastern Europe. Temgicoluril has an effect on the structure of limbic-reticular activity, particularly on the hypothalamus, as well as on all four basic neuromediator systems \u2013 \u03b3 aminobutyric acid (GABA), choline, serotonin and adrenergic activity. Temgicoluril decreases noradrenaline, increases serotonin, and exerts no effect on dopamine.\nFabomotizole.\nFabomotizole (Afobazole) is an anxiolytic drug launched in Russia in the early 2000s. Its mechanism of action is poorly-defined, with GABAergic, NGF and BDNF release promoting, MT1 receptor agonism, MT3 receptor antagonism, and sigma receptor agonism thought to have some involvement.\nBromantane.\nBromantane is a stimulant drug with anxiolytic properties developed in Russia during the late 1980s. Bromantane acts mainly by facilitating the biosynthesis of dopamine, through indirect genomic upregulation of relevant enzymes (tyrosine hydroxylase (TH) and aromatic L-amino acid decarboxylase (AAAD)).\nEmoxypine.\nEmoxypine is an antioxidant that is also a purported anxiolytic. Its chemical structure resembles that of pyridoxine, a form of vitamin B6.\nMenthyl isovalerate.\nMenthyl isovalerate is a flavoring food additive marketed as a sedative and anxiolytic drug in Russia under the name \"Validol\".\nRacetams.\nSome racetam based drugs such as aniracetam can have an antianxiety effect.\nAlpidem.\nAlpidem is a nonbenzodiazepine anxiolytic with similar anxiolytic effectiveness as benzodiazepines but reduced sedation and cognitive, memory, and motor impairment. It was marketed briefly in France but was withdrawn from the market due to liver toxicity.\nEtifoxine.\nEtifoxine has similar anxiolytic effects as benzodiazepine drugs, but does not produce the same levels of sedation and ataxia. Further, etifoxine does not affect memory and vigilance, and does not induce rebound anxiety, drug dependence, or withdrawal symptoms.\nAlcohol.\nAlcohol is sometimes used as an anxiolytic by self-medication. fMRI can measure the anxiolytic effects of alcohol in the human brain.\nAlternatives to medication.\nCognitive behavioral therapy (CBT) is an effective treatment for panic disorder, social anxiety disorder, generalized anxiety disorder, and obsessive\u2013compulsive disorder, while exposure therapy is the recommended treatment for anxiety related phobias. Healthcare providers can guide those with anxiety disorder by referring them to self-help resources. Sometimes medication is combined with psychotherapy but research has not found a benefit of combined pharmacotherapy and psychotherapy versus monotherapy.\nIf CBT is found ineffective, both the Canadian and American medical associations then suggest the use of medication."}
{"id": "2870", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=2870", "title": "Antipsychotic", "text": "Antipsychotics, previously known as neuroleptics and major tranquilizers, are a class of psychotropic medication primarily used to manage psychosis (including delusions, hallucinations, paranoia or disordered thought), principally in schizophrenia but also in a range of other psychotic disorders. They are also the mainstay, together with mood stabilizers, in the treatment of bipolar disorder. Moreover, they are also used as adjuncts in the treatment of treatment-resistant major depressive disorder.\nUse of antipsychotics is associated with reductions in brain tissue volumes, including white matter reduction, an effect which is dose-dependent and time-dependent. A recent controlled trial suggests that second generation antipsychotics combined with intensive psychosocial therapy may potentially prevent pallidal brain volume loss in first episode psychosis.\nThe use of antipsychotics may result in many unwanted side effects such as involuntary movement disorders, gynecomastia, impotence, weight gain and metabolic syndrome. Long-term use can produce adverse effects such as tardive dyskinesia, tardive dystonia, and tardive akathisia.\nFirst-generation antipsychotics (e.g., chlorpromazine, haloperidol, etc.), known as typical antipsychotics, were first introduced in the 1950s, and others were developed until the early 1970s. Second-generation antipsychotics, known as atypical antipsychotics, arrived with the introduction of clozapine in the early 1970s followed by others (e.g., risperidone, olanzapine, etc.). Both generations of medication block receptors in the brain for dopamine, but atypicals block serotonin receptors as well. Third-generation antipsychotics were introduced in the 2000s and offer partial agonism, rather than blockade, of dopamine receptors. \"Neuroleptic\", originating from (\"neuron\") and (\"take hold of\")\u2014thus meaning \"which takes the nerve\"\u2014refers to both common neurological effects and side effects.\nMedical uses.\nAntipsychotics are most frequently used for the following conditions:\nGiven the limited options available to treat the behavioral problems associated with dementia, other pharmacological and non-pharmacological interventions are usually attempted before using antipsychotics. A risk-to-benefit analysis is performed to weigh the risk of the adverse effects of antipsychotics versus: the potential benefit, the adverse effects of alternative interventions, and the risk of failing to intervene when a patient's behavior becomes unsafe. The same can be said for insomnia, in which they are not recommended as first-line therapy. There are evidence-based indications for using antipsychotics in children (e.g., tic disorder, bipolar disorder, psychosis), but the use of antipsychotics outside of those contexts (e.g., to treat behavioral problems) warrants significant caution.\nAntipsychotics are used to treat tics associated with Tourette syndrome. Aripiprazole, an atypical antipsychotic, is used as add-on medication to ameliorate sexual dysfunction as a symptom of selective serotonin reuptake inhibitor (SSRI) antidepressants in women. Quetiapine is used to treat generalized anxiety disorder.\nSchizophrenia.\nAntipsychotic drug treatment is a key component of schizophrenia treatment recommendations by the National Institute of Health and Care Excellence (NICE), the American Psychiatric Association, and the British Society for Psychopharmacology. The main aim of treatment with antipsychotics is to reduce the positive symptoms of psychosis, that include delusions and hallucinations. There is mixed evidence to support a significant impact of antipsychotic use on primary negative symptoms (such as apathy, lack of emotional affect, and lack of interest in social interactions) or on cognitive symptoms (memory impairments, reduced ability to plan and execute tasks). In general, the efficacy of antipsychotic treatment in reducing positive symptoms appears to increase with the severity of baseline symptoms. All antipsychotic medications work relatively the same way: by antagonizing D2 dopamine receptors. However, there are some differences when it comes to typical and atypical antipsychotics. For example, atypical antipsychotic medications have been seen to lower the neurocognitive impairment associated with schizophrenia more than conventional antipsychotics, although the reasoning and mechanics of this are still unclear to researchers.\nApplications of antipsychotic drugs in the treatment of schizophrenia include prophylaxis for those showing symptoms that suggest that they are at high risk of developing psychosis; treatment of first-episode psychosis; maintenance therapy (a form of prophylaxis, maintenance therapy aims to maintain therapeutic benefit and prevent symptom relapse); and treatment of recurrent episodes of acute psychosis. A recent 2024 study found that using high doses of antipsychotics for schizophrenia was linked to a higher risk of mortality. Researchers analyzed data from 32,240 individuals aged 17 to 64 diagnosed with schizophrenia between 2002 and 2012 to arrive at this conclusion. \nPrevention of psychosis and symptom improvement.\nTest batteries such as the PACE (Personal Assessment and Crisis Evaluation Clinic) and COPS (Criteria of Prodromal Syndromes), which measure low-level psychotic symptoms and cognitive disturbances, are used to evaluate people with early, low-level symptoms of psychosis. Test results are combined with family history information to identify patients in the \"high-risk\" group; they are considered to have a 20\u201340% risk of progression to frank psychosis within two years. These patients are often treated with low doses of antipsychotic drugs with the goal of reducing their symptoms and preventing progression to frank psychosis. While generally useful for reducing symptoms, clinical trials to date show little evidence that early use of antipsychotics improves long-term outcomes in those with prodromal symptoms, either alone or in combination with cognitive-behavioral therapy.\nFirst-episode psychosis.\nFirst-episode psychosis (FEP) is the first time that psychotic symptoms are presented. NICE recommends that all people presenting with first-episode psychosis be treated with both an antipsychotic drug and cognitive behavioral therapy (CBT). NICE further recommends that those expressing a preference for CBT alone be informed that combination treatment is more effective. A diagnosis of schizophrenia is not made at this time as it takes longer to be determined by both DSM-5 and ICD-11, and only around 60% of those presenting with a first episode of psychosis will later be diagnosed with schizophrenia.\nThe conversion rate for a first episode of drug induced psychosis to bipolar disorder or schizophrenia is lower, with 30% of people converting to either bipolar disorder or schizophrenia. NICE makes no distinction between substance-induced psychosis and any other form of psychosis. The rate of conversion differs for different classes of drugs.\nPharmacological options for the specific treatment of FEP have been discussed in recent reviews. The goals of treatment for FEP include reducing symptoms and potentially improving long-term treatment outcomes. Randomized clinical trials have provided evidence for the efficacy of antipsychotic drugs in achieving the former goal, with first-generation and second generation antipsychotics showing about equal efficacy. The evidence that early treatment has a favorable effect on long-term outcomes is equivocal.\nRecurrent psychotic episodes.\nPlacebo-controlled trials of both first- and second-generation antipsychotic drugs consistently demonstrate the superiority of active drugs over placebos in suppressing psychotic symptoms. A large meta-analysis of 38 trials of antipsychotic drugs in schizophrenia with acute psychotic episodes showed an effect size of about 0.5. There is little or no difference in efficacy among approved antipsychotic drugs, including both first- and second-generation agents. The efficacy of such drugs is suboptimal. Few patients achieve complete resolution of symptoms. Response rates, calculated using various cutoff values for symptom reduction, are low, and their interpretation is complicated by high placebo response rates and selective publication of clinical trial results.\nMaintenance therapy.\nThe majority of patients treated with an antipsychotic drug will experience a response within four weeks. The goals of continuing treatment are to maintain suppression of symptoms, prevent relapse, improve quality of life, and support engagement in psychosocial therapy.\nMaintenance therapy with antipsychotic drugs is clearly superior to placebo in preventing relapse but is associated with weight gain, movement disorders, and high dropout rates. A 3-year trial following persons receiving maintenance therapy after an acute psychotic episode found that 33% obtained long-lasting symptom reduction, 13% achieved remission, and only 27% experienced satisfactory quality of life. The effect of relapse prevention on long term outcomes is uncertain, as historical studies show little difference in long term outcomes before and after the introduction of antipsychotic drugs.\nWhile maintenance therapy clearly reduces the rate of relapses requiring hospitalization, a large observational study in Finland found that, in people that eventually discontinued antipsychotics, the risk of being hospitalized again for a mental health problem or dying increased the longer they were dispensed (and presumably took) antipsychotics prior to stopping therapy. If people did not stop taking antipsychotics, they remained at low risk for relapse and hospitalization compared to those that did. The authors speculated that the difference may be because the people that discontinued treatment after a longer time had more severe mental illness than those that discontinued antipsychotic therapy sooner.\nA significant challenge in the use of antipsychotic drugs for the prevention of relapse is the poor rate of adherence. In spite of the relatively high rates of adverse effects associated with these drugs, some evidence, including higher dropout rates in placebo arms compared to treatment arms in randomized clinical trials, suggests that most patients who discontinue treatment do so because of suboptimal efficacy. If someone experiences psychotic symptoms due to nonadherence, they may be compelled to receive treatment through a process called involuntary commitment, in which they can be forced to accept treatment (including antipsychotics). A person can also be committed to treatment outside of a hospital, called outpatient commitment.\nAntipsychotics in long-acting injectable (LAI), or \"depot\", form have been suggested as a method of decreasing medication nonadherence (sometimes also called non-compliance). NICE advises LAIs be offered to patients when preventing covert, intentional nonadherence is a clinical priority. LAIs are used to ensure adherence in outpatient commitment. A meta-analysis found that LAIs resulted in lower rates of rehospitalization with a hazard ratio of 0.83; however, these results were not statistically significant (the 95% confidence interval was 0.62 to 1.11).\nBipolar disorder.\nAntipsychotics are routinely used, often in conjunction with mood stabilizers such as lithium/valproate, as a first-line treatment for manic and mixed episodes associated with bipolar disorder. The reason for this combination is the therapeutic delay of the aforementioned mood stabilizers (for valproate therapeutic effects are usually seen around five days after treatment is commenced whereas lithium usually takes at least a week before the full therapeutic effects are seen) and the comparatively rapid antimanic effects of antipsychotic drugs. The antipsychotics have a documented efficacy when used alone in acute mania/mixed episodes.\nAt least five atypical antipsychotics (lumateperone, cariprazine, lurasidone, olanzapine, and quetiapine) have also been found to possess efficacy in the treatment of bipolar depression as a monotherapy, whereas only olanzapine and quetiapine have been proven to be effective broad-spectrum (i.e., against all three types of relapse\u2014manic, mixed and depressive) prophylactic (or \"maintenance\") treatments in patients with bipolar disorder. A recent Cochrane review also found that olanzapine had a less favourable risk/benefit ratio than lithium as a maintenance treatment for bipolar disorder.\nThe American Psychiatric Association and the UK National Institute for Health and Care Excellence recommend antipsychotics for managing acute psychotic episodes in schizophrenia or bipolar disorder, and as a longer-term maintenance treatment for reducing the likelihood of further episodes. They state that response to any given antipsychotic can be variable so that trials may be necessary, and that lower doses are to be preferred where possible. A number of studies have looked at levels of \"compliance\" or \"adherence\" with antipsychotic regimes and found that discontinuation (stopping taking them) by patients is associated with higher rates of relapse, including hospitalization.\nDementia.\nPsychosis and agitation develop in as many as 80 percent of people living in nursing homes. Despite a lack of FDA approval and black-box warnings, atypical antipsychotics are very often prescribed to people with dementia. An assessment for an underlying cause of behavior is needed before prescribing antipsychotic medication for symptoms of dementia. Antipsychotics in old age dementia showed a modest benefit compared to placebo in managing aggression or psychosis, but this is combined with a fairly large increase in serious adverse events. Thus, antipsychotics should not be used routinely to treat dementia with aggression or psychosis, but may be an option in a few cases where there is severe distress or risk of physical harm to others. Psychosocial interventions may reduce the need for antipsychotics. In 2005, the FDA issued an advisory warning of an increased risk of death when atypical antipsychotics are used in dementia. In the subsequent 5 years, the use of atypical antipsychotics to treat dementia decreased by nearly 50%.\nMajor depressive disorder.\nA number of atypical antipsychotics have some benefits when used in addition to other treatments in major depressive disorder. Aripiprazole, quetiapine extended-release, and olanzapine (when used in conjunction with fluoxetine) have received the Food and Drug Administration (FDA) labelling for this indication. There is, however, a greater risk of side effects with their use compared to using traditional antidepressants. The greater risk of serious side effects with antipsychotics is why, e.g., quetiapine was denied approval as monotherapy for major depressive disorder or generalized anxiety disorder, and instead was only approved as an adjunctive treatment in combination with traditional antidepressants.\nA recent study on the use of antipychotics in unipolar depression concluded that the use of those drugs in addition to antidepressants alone leads to a worse disease outcome. This effect is especially pronounced in younger patients with psychotic unipolar depression. Considering the wide use of such combination therapies, further studies on the side effects of antipychotics as an add-on therapy are warranted.\nOther.\nGlobal antipsychotic utilization has seen a steady growth since the introduction of atypical (second-generation) antipsychotics and this is ascribed to off-label use for many other unapproved disorders. Besides the above uses antipsychotics may be used for obsessive\u2013compulsive disorder, post-traumatic stress disorder, personality disorders, Tourette syndrome, autism and agitation in those with dementia. Evidence however does not support the use of atypical antipsychotics in eating disorders or personality disorder. The atypical antipsychotic risperidone may be useful for obsessive\u2013compulsive disorder. The use of low doses of antipsychotics for insomnia, while common, is not recommended as there is little evidence of benefit as well as concern regarding adverse effects. Some of the more serious adverse effects may also occur at the low doses used, such as dyslipidemia and neutropenia, and a recent network meta-analysis of 154 double-blind, randomized controlled trials of drug therapies vs. placebo for insomnia in adults found that quetiapine did not demonstrated any short-term benefits in sleep quality. Low dose antipsychotics may also be used in treatment of impulse-behavioural and cognitive-perceptual symptoms of borderline personality disorder. Despite the lack of evidence supporting the benefit of antipsychotics in people with personality disorders, 1 in 4 who do not have a serious mental illness are prescribed them in UK primary care. Many people receive these medication for over a year, contrary to NICE guidelines.\nIn children they may be used in those with disruptive behavior disorders, mood disorders and pervasive developmental disorders or intellectual disability. Antipsychotics are only weakly recommended for Tourette syndrome, because although they are effective, side effects are common. The situation is similar for those on the autism spectrum.\nMuch of the evidence for the off-label use of antipsychotics (for example, for dementia, OCD, PTSD, personality disorders, Tourette's) was of insufficient scientific quality to support such use, especially as there was strong evidence of increased risks of stroke, tremors, significant weight gain, sedation, and gastrointestinal problems. A UK review of unlicensed usage in children and adolescents reported a similar mixture of findings and concerns. A survey of children with pervasive developmental disorder found that 16.5% were taking an antipsychotic drug, most commonly for irritability, aggression, and agitation. Both risperidone and aripiprazole have been approved by the US FDA for the treatment of irritability in autistic children and adolescents. A review in the UK found that the use of antipsychotics in England doubled between 2000 and 2019. Children were prescribed antipsychotics for conditions for which there is no approval, such as autism.\nAggressive challenging behavior in adults with intellectual disability is often treated with antipsychotic drugs despite lack of an evidence base. A recent randomized controlled trial, however, found no benefit over placebo and recommended that the use of antipsychotics in this way should no longer be regarded as an acceptable routine treatment.\nAntipsychotics may be an option, together with stimulants, in people with ADHD and aggressive behavior when other treatments have not worked. They have not been found to be useful for the prevention of delirium among those admitted to hospital.\nTypicals vs atypicals.\nAside from reduced extrapyramidal symptoms, and with the clear exception of clozapine, it is unclear whether the atypical (second-generation) antipsychotics offer advantages over older, first generation antipsychotics. Amisulpride, olanzapine, risperidone and clozapine may be more effective but are associated with greater side effects. Typical antipsychotics have equal drop-out and symptom relapse rates to atypicals when used at low to moderate dosages.\nClozapine is an effective treatment for those who respond poorly to other drugs (\"treatment-resistant\" or \"refractory\" schizophrenia), but it has the potentially serious side effect of agranulocytosis (lowered white blood cell count) in less than 4% of people.\nDue to bias in the research the accuracy of comparisons of atypical antipsychotics is a concern.\nIn 2005, a US government body, the National Institute of Mental Health published the results of a major independent study (the CATIE project). No other atypical studied (risperidone, quetiapine, and ziprasidone) did better than the first-generation antipsychotic perphenazine on the measures used, nor did they produce fewer adverse effects than the typical antipsychotic perphenazine, although more patients discontinued perphenazine owing to extrapyramidal effects compared to the atypical agents (8% vs. 2% to 4%). This is significant because any patient with tardive dyskinesia was specifically excluded from randomization to perphenazine; i.e., in the CATIE study the patient cohort randomized to receive perphenazne was at lower risk of having extrapyramidal symptoms.\nAtypical antipsychotics do not appear to lead to improved rates of medication adherence compared to typical antipsychotics.\nMany researchers question the first-line prescribing of atypicals over typicals, and some even question the distinction between the two classes. In contrast, other researchers point to the significantly higher risk of tardive dyskinesia and other extrapyramidal symptoms with the typicals and for this reason alone recommend first-line treatment with the atypicals, notwithstanding a greater propensity for metabolic adverse effects in the latter. The UK government organization NICE recently revised its recommendation favoring atypicals, to advise that the choice should be an individual one based on the particular profiles of the individual drug and on the patient's preferences.\nThe re-evaluation of the evidence has not necessarily slowed the bias toward prescribing the atypicals.\nOther uses.\nAntipsychotics, such as risperidone, quetiapine, and olanzapine, have been used as hallucinogen antidotes or \"trip killers\" to block the effects of serotonergic psychedelics like psilocybin and lysergic acid diethylamide (LSD).\nAdverse effects.\nGenerally, more than one antipsychotic drug should not be used at a time because of increased adverse effects.\nSome atypicals are associated with considerable weight gain, diabetes and the risk of metabolic syndrome. Unwanted side effects cause people to stop treatment, resulting in relapses. Risperidone (atypical) has a similar rate of extrapyramidal symptoms to haloperidol (typical). A rare but potentially lethal condition of neuroleptic malignant syndrome (NMS) has been associated with the use of antipsychotics. Through its early recognition, and timely intervention rates have declined. However, an awareness of the syndrome is advised to enable intervention. Another less rare condition of tardive dyskinesia can occur due to long-term use of antipsychotics, developing after months or years of use. It is more often reported with use of typical antipsychotics. Very rarely antipsychotics may cause tardive psychosis.\nClozapine is associated with side effects that include weight gain, tiredness, and hypersalivation. More serious adverse effects include seizures, NMS, neutropenia, and agranulocytosis (lowered white blood cell count) and its use needs careful monitoring.\nClozapine is also associated with thromboembolism (including pulmonary embolism), myocarditis, and cardiomyopathy. A systematic review of clozapine-associated pulmonary embolism indicates that this adverse effect can often be fatal, and that it has an early onset, and is dose-dependent. The findings advised the consideration of using a prevention therapy for venous thromboembolism after starting treatment with clozapine, and continuing this for six months. Constipation is three times more likely to occur with the use of clozapine, and severe cases can lead to ileus and bowel ischemia resulting in many fatalities. Very rare clozapine adverse effects include periorbital edema due to several possible mechanisms (e.g., inhibition of platelet-derived growth factor receptors leading to increased vascular permeability, antagonism of renal dopamine receptors with electrolyte and fluid imbalance and immune-mediated hypersensitivity reactions).\nHowever, the risk of serious adverse effects from clozapine is low, and there are the beneficial effects to be gained of a reduced risk of suicide, and aggression. Typical antipsychotics and atypical risperidone can have a side effect of sexual dysfunction. Clozapine, olanzapine, and quetiapine are associated with beneficial effects on sexual functioning helped by various psychotherapies.\nBy rate.\nCommon (\u2265 1% and up to 50% incidence for \"most\" antipsychotic drugs) adverse effects of antipsychotics include:\nRare/Uncommon (&lt;1% incidence for \"most\" antipsychotic drugs) adverse effects of antipsychotics include:\nLong-term effects.\nSome studies have found decreased life expectancy associated with the use of antipsychotics, and argued that more studies are needed. Antipsychotics may also increase the risk of early death in individuals with dementia. Antipsychotics typically worsen symptoms in people with depersonalisation disorder. Antipsychotic polypharmacy (prescribing two or more antipsychotics at the same time for an individual) is a common practice but not evidence-based or recommended, and there are initiatives to curtail it. Similarly, the use of excessively high doses (often the result of polypharmacy) continues despite clinical guidelines and evidence indicating that it is usually no more effective but is usually more harmful. A meta-analysis of observational studies with over two million individuals has suggested a moderate association of antipsychotic use with breast cancer.\nLoss of grey matter and other brain structural changes over time are observed amongst people diagnosed with schizophrenia. Meta-analyses of the effects of antipsychotic treatment on grey matter volume and the brain's structure have reached conflicting conclusions. A 2012 meta-analysis concluded that grey matter loss is greater in patients treated with first generation antipsychotics relative to those treated with atypicals, and hypothesized a protective effect of atypicals as one possible explanation. A second meta-analysis suggested that treatment with antipsychotics was associated with increased grey matter loss. Animal studies found that monkeys exposed to both first- and second-generation antipsychotics experience significant reduction in brain volume, resulting in an 8-11% reduction in brain volume over a 17\u201327 month period.\nThe National Association of State Mental Health Program Directors said that antipsychotics are not interchangeable, and it recommends including trying at least one weight-neutral treatment for those patients with potential metabolic issues.\nSubtle, long-lasting forms of akathisia are often overlooked or confused with post-psychotic depression, in particular when they lack the extrapyramidal aspect that psychiatrists have been taught to expect when looking for signs of akathisia.\nAdverse effect on cognitive function and increased risk of death in people with dementia along with worsening of symptoms has been described in the literature.\nAntipsychotics, due to acting as dopamine D2 receptor antagonists and thereby stimulating pituitary lactotrophs, may have a risk of prolactinoma with long-term use. This is also responsible for their induction of hyperprolactinemia (high prolactin levels).\nDiscontinuation.\nThe \"British National Formulary\" recommends a gradual withdrawal when discontinuing antipsychotics to avoid acute withdrawal syndrome or rapid relapse. Symptoms of withdrawal commonly include nausea, vomiting, and loss of appetite. Other symptoms may include restlessness, increased sweating, and trouble sleeping. Less commonly there may be a feeling of the world spinning, numbness, or muscle pains. Symptoms generally resolve after a short period of time. \nA randomised controlled trial compared maintenance therapy with gradual dose reduction or discontinuation among people with long-term psychosis. At 2 years, people in the reduction group were twice as likely to relapse (25%) as those in the maintenance group (13%). Moreover, those in the reduction group had no improvement in social functioning (a measure combining people\u2019s ability to look after themselves, work, study and take part in family and social activities), side effects, quality of life, symptoms, or bodyweight. \nThere is tentative evidence that discontinuation of antipsychotics can result in psychosis. It may also result in recurrence of the condition that is being treated. Rarely, tardive dyskinesia can occur when the medication is stopped.\nUnexpected psychotic episodes have been observed in patients withdrawing from clozapine. This is referred to as supersensitivity psychosis, not to be equated with tardive dyskinesia.\nTardive dyskinesia may abate during withdrawal from the antipsychotic agent, or it may persist.\nWithdrawal effects may also occur when switching a person from one antipsychotic to another, (it is presumed due to variations of potency and receptor activity). Such withdrawal effects can include cholinergic rebound, an activation syndrome, and motor syndromes including dyskinesias. These adverse effects are more likely during rapid changes between antipsychotic agents, so making a gradual change between antipsychotics minimises these withdrawal effects. The \"British National Formulary\" recommends a gradual dose reduction when discontinuing antipsychotic treatment to avoid acute withdrawal symptoms or rapid relapse. The process of cross-titration involves gradually increasing the dose of the new medication while gradually decreasing the dose of the old medication.\nCity and Hackney Clinical Commissioning Group found more than 1,000 patients in their area in July 2019 who had not had regular medication reviews or health checks because they were not registered as having serious mental illness. On average they had been taking these drugs for six years. If this is typical of practice in England more than 100,000 patients are probably in the same position.\nList of agents.\nClinically used antipsychotic medications are listed below by drug group. Trade names appear in parentheses. A 2013 review has stated that the division of antipsychotics into first and second generation is perhaps not accurate.\nNotes:\n\"\u2020 indicates drugs that are no longer (or were never) marketed in English-speaking countries. \"\n\"\u2021 denotes drugs that are no longer (or were never to begin with) marketed in the United States. Some antipsychotics are not firmly placed in either first-generation or second-generation classes.\"\n\"# denotes drugs that have been withdrawn worldwide.\"\nDisputed/unknown.\nThis category is for drugs that have been called both first and second-generation, depending on the literature being used.\nThird-generation.\nThird generation antipsychotics are recognized as demonstrating D2 receptor partial agonism as opposed to the D2 and 5HT-2A receptor antagonism of second-generation (atypical) antipsychotics and D2 antagonism of first-generation (typical) antipsychotics.\nMechanism of action.\nAntipsychotic drugs such as haloperidol and chlorpromazine tend to block dopamine D2 receptors in the dopaminergic pathways of the brain. This means that dopamine released in these pathways has less effect. Excess release of dopamine in the mesolimbic pathway has been linked to psychotic experiences. Decreased dopamine release in the prefrontal cortex, and excess dopamine release in other pathways, are associated with psychotic episodes in schizophrenia and bipolar disorder.\nIn addition to the antagonistic effects of dopamine, antipsychotics (in particular atypical antipsychotics) also antagonize 5-HT2A receptors. Different alleles of the 5-HT2A receptor have been associated with schizophrenia and other psychoses, including depression. Higher concentrations of 5-HT2A receptors in cortical and subcortical areas, in particular in the right caudate nucleus have been historically recorded.\nTypical antipsychotics are not particularly selective and also block dopamine receptors in the mesocortical pathway, tuberoinfundibular pathway, and the nigrostriatal pathway. Blocking D2 receptors in these other pathways is thought to produce some unwanted side effects that the typical antipsychotics can produce (see above). They were commonly classified on a spectrum of low potency to high potency, where potency referred to the ability of the drug to bind to dopamine receptors, and not to the effectiveness of the drug. High-potency antipsychotics such as haloperidol, in general, have doses of a few milligrams and cause less sleepiness and calming effects than low-potency antipsychotics such as chlorpromazine and thioridazine, which have dosages of several hundred milligrams. The latter have a greater degree of anticholinergic and antihistaminergic activity, which can counteract dopamine-related side-effects.\nAtypical antipsychotic drugs have a similar blocking effect on D2 receptors; however, most also act on serotonin receptors, especially 5-HT2A and 5-HT2C receptors. Both clozapine and quetiapine appear to bind just long enough to elicit antipsychotic effects but not long enough to induce extrapyramidal side effects and prolactin hypersecretion. 5-HT2A antagonism increases dopaminergic activity in the nigrostriatal pathway, leading to a lowered extrapyramidal side effect liability among the atypical antipsychotics.\nThrough the ability of most antipsychotics to antagonize 5-HT2A serotonin pathways enabling a sensitisation of postsynaptic serotonin receptors, MDMA exposure can be more intense because it has more excitatory receptors to activate. The same effect can be observed with the D2 antagonizing with normal amphetamine (with this just being hypothetical as there is the fact that antipsychotics sensitize receptors, with exact these postsynaptic receptors (5-HT2A, D2) being flooded by the respective neurotransmitter (serotonin, dopamine) from amphetamine exposure).\nHistory.\nThe original antipsychotic drugs were happened upon largely by chance and then tested for their effectiveness. The first, chlorpromazine, was developed as a surgical anesthetic. It was first used on psychiatric patients because of its powerful calming effect; at the time it was regarded as a non-permanent \"pharmacological lobotomy\". Lobotomy at the time was used to treat many behavioral disorders, including psychosis, although its effect was to markedly reduce behavior and mental functioning of all types. However, chlorpromazine proved to reduce the effects of psychosis in a more effective and specific manner than lobotomy, even though it was known to be capable of causing severe sedation. The underlying neurochemistry involved has since been studied in detail, and subsequent antipsychotic drugs have been developed by rational drug design.\nThe discovery of chlorpromazine's psychoactive effects in 1952 led to further research that resulted in the development of antidepressants, anxiolytics, and the majority of other drugs now used in the management of psychiatric conditions. In 1952, Henri Laborit described chlorpromazine only as inducing indifference towards what was happening around them in nonpsychotic, nonmanic patients, and Jean Delay and Pierre Deniker described it as controlling manic or psychotic agitation. The former claimed to have discovered a treatment for agitation in anyone, and the latter team claimed to have discovered a treatment for psychotic illness.\nUntil the 1970s there was considerable debate within psychiatry on the most appropriate term to use to describe the new drugs. In the late 1950s the most widely used term was \"neuroleptic\", followed by \"major tranquilizer\" and then \"ataraxic\". The first recorded use of the term tranquilizer dates from the early nineteenth century. In 1953 Frederik F. Yonkman, a chemist at the Swiss-based Cibapharmaceutical company, first used the term tranquilizer to differentiate reserpine from the older sedatives. The word \"neuroleptic\" was coined in 1955 by Delay and Deniker after their discovery (1952) of the antipsychotic effects of chlorpromazine. It is derived from the (\"neuron\", originally meaning \"sinew\" but today referring to the nerves) and \"\u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03c9\" (\"lamban\u014d\", meaning \"take hold of\"). Thus, the word means \"taking hold of one's nerves\". It was often taken to refer also to common side effects such as reduced activity in general, as well as lethargy and impaired motor control. Although these effects are unpleasant and in some cases harmful, they were at one time, along with akathisia, considered a reliable sign that the drug was working. The term \"ataraxy\" was coined by the neurologist Howard Fabing and the classicist Alister Cameron to describe the observed effect of psychic indifference and detachment in patients treated with chlorpromazine. This term derived from the Greek adjective \"\u1f00\u03c4\u03ac\u03c1\u03b1\u03ba\u03c4\u03bf\u03c2\" (\"ataraktos\"), which means \"not disturbed, not excited, without confusion, steady, calm\". In the use of the terms \"tranquilizer\" and \"ataractic\", medical practitioners distinguished between the \"major tranquilizers\" or \"major ataractics\", which referred to drugs used to treat psychoses, and the \"minor tranquilizers\" or \"minor ataractics\", which referred to drugs used to treat neuroses. While popular during the 1950s, these terms are infrequently used today. They are being abandoned in favor of \"antipsychotic\", which refers to the drug's desired effects. Today, \"minor tranquilizer\" can refer to anxiolytic and/or hypnotic drugs such as the benzodiazepines and nonbenzodiazepines, which are useful as generally short-term management for insomnia together with cognitive behavioral therapy for insomnia. They are potentially addictive sedatives.\nAntipsychotics are broadly divided into two groups, the typical or first-generation antipsychotics and the atypical or second-generation antipsychotics. The difference between first- and second-generation antipsychotics is a subject of debate. The second-generation antipsychotics are generally distinguishable by the presence of 5HT2A receptor antagonism and a corresponding lower propensity for extrapyramidal side effects compared to first-generation antipsychotics.\nSociety and culture.\nTerminology.\nThe term \"major tranquilizer\" was used for older antipsychotic drugs. The term \"neuroleptic\" is often used as a synonym for \"antipsychotic\", even though \u2013 strictly speaking \u2013 the two terms are not interchangeable. \"Antipsychotic\" drugs are a subgroup of \"neuroleptic\" drugs, because the latter have a wider range of effects.\nAntipsychotics are a type of psychoactive or psychotropic medication.\nSales.\nAntipsychotics were once among the biggest selling and most profitable of all drugs, generating $22\u00a0billion in global sales in 2008. By 2003 in the US, an estimated 3.21\u00a0million patients received antipsychotics, worth an estimated $2.82\u00a0billion. Over 2/3 of prescriptions were for the newer, more expensive atypicals, each costing on average $164 per year, compared to $40 for the older types. By 2008, sales in the US reached $14.6\u00a0billion, the biggest selling drugs in the US by therapeutic class.\nIn the five years since July 2017 the number of antipsychotic medicines dispensed in the community in the United Kingdom has increased by 11.2%. There have also been substantial price rises. Risperidone 6\u00a0mg tablets, the largest, increased from \u00a33.09 in July 2017 to \u00a341.16 in June 2022. The NHS is spending an additional \u00a333 million annually on antipsychotics. Haloperidol 500 microgram tablets constituted \u00a314.3 million of this.\nOverprescription.\nAntipsychotics in the nursing home population are often overprescribed, often for the purposes of making it easier to handle dementia patients. Federal efforts to reduce the use of antipsychotics in US nursing homes has led to a nationwide decrease in their usage in 2012.\nLegal.\nAntipsychotics are sometimes administered as part of compulsory psychiatric treatment via inpatient (hospital) commitment or outpatient commitment.\nFormulations.\nThey may be administered orally or, in some cases, through long-acting (depot) injections administered in the dorsgluteal, ventrogluteal or deltoid muscle. Short-acting parenteral formulations also exist, which are generally reserved for emergencies or when oral administration is otherwise impossible. The oral formulations include immediate release, extended release, and orally disintegrating products (which are not sublingual, and can help ensure that medications are swallowed instead of \"cheeked\"). Sublingual products (e.g., asenapine) also exist, which must be held under the tongue for absorption. The first transdermal formulation of an antipsychotic (transdermal asenapine, marketed as Secuado), was FDA-approved in 2019.\nRecreational use.\nCertain second-generation antipsychotics are misused or abused for their sedative, tranquilizing, and (paradoxically) \"hallucinogenic\" effects. The most commonly implicated second-generation antipsychotic is quetiapine. In case reports, quetiapine has been abused in doses taken by mouth (which is how the drug is available from the manufacturer), but also crushed and insufflated or mixed with water for injection into a vein. Olanzapine, another sedating second-generation antipsychotic, has also been misused for similar reasons. There is no standard treatment for antipsychotic abuse, though switching to a second-generation antipsychotic with less abuse potential (e.g., aripiprazole) has been used.\nControversy.\nJoanna Moncrieff has argued that antipsychotic drug treatment is often undertaken as a means of control rather than to treat specific symptoms experienced by the patient.\nUse of this class of drugs has a history of criticism in residential care. As the drugs used can make patients calmer and more compliant, critics claim that the drugs can be overused. Outside doctors can feel under pressure from care home staff. In an official review commissioned by UK government ministers it was reported that the needless use of antipsychotic medication in dementia care was widespread and was linked to 1800 deaths per year. In the US, the government has initiated legal action against the pharmaceutical company Johnson &amp; Johnson for allegedly paying kickbacks to Omnicare to promote its antipsychotic risperidone (Risperdal) in nursing homes.\nThere has also been controversy about the role of pharmaceutical companies in marketing and promoting antipsychotics, including allegations of downplaying or covering up adverse effects, expanding the number of conditions or illegally promoting off-label usage; influencing drug trials (or their publication) to try to show that the expensive and profitable newer atypicals were superior to the older cheaper typicals that were out of patent. Following charges of illegal marketing, settlements by two large pharmaceutical companies in the US set records for the largest criminal fines ever imposed on corporations. One case involved Eli Lilly and Company's antipsychotic Zyprexa, and the other involved Bextra. In the Bextra case, the government also charged Pfizer with illegally marketing another antipsychotic, Geodon. In addition, AstraZeneca faces numerous personal-injury lawsuits from former users of Seroquel (quetiapine), amidst federal investigations of its marketing practices. By expanding the conditions for which they were indicated, Astrazeneca's Seroquel and Eli Lilly's Zyprexa had become the biggest selling antipsychotics in 2008 with global sales of $5.5\u00a0billion and $5.4\u00a0billion respectively.\nHarvard University medical professor Joseph Biederman conducted research on bipolar disorder in children that led to an increase in such diagnoses. A 2008 Senate investigation found that Biederman also received $1.6 million in speaking and consulting fees between 2000 and 2007, some of them undisclosed to Harvard, from companies including makers of antipsychotic drugs prescribed for children with bipolar disorder. Johnson &amp; Johnson gave more than $700,000 to a research center that was headed by Biederman from 2002 to 2005, where research was conducted, in part, on Risperdal, the company's antipsychotic drug. Biederman has responded saying that the money did not influence him and that he did not promote a specific diagnosis or treatment.\nPharmaceutical companies have also been accused of attempting to set the mental health agenda through activities such as funding consumer advocacy groups.\nSpecial populations.\nIt is recommended that persons with dementia who exhibit behavioral and psychological symptoms should not be given antipsychotics before trying other treatments. When taking antipsychotics this population has increased risk of cerebrovascular effects, parkinsonism or extrapyramidal symptoms, sedation, confusion and other cognitive adverse effects, weight gain, and increased mortality. Physicians and caretakers of persons with dementia should try to address symptoms including agitation, aggression, apathy, anxiety, depression, irritability, and psychosis with alternative treatments whenever antipsychotic use can be replaced or reduced. Elderly persons often have their dementia treated first with antipsychotics and this is not the best management strategy."}
{"id": "2871", "revid": "1893265", "url": "https://en.wikipedia.org/wiki?curid=2871", "title": "Akita", "text": " is a Japanese name and may refer to:"}
{"id": "2872", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2872", "title": "Arthur Tudor", "text": ""}
{"id": "2875", "revid": "1267529772", "url": "https://en.wikipedia.org/wiki?curid=2875", "title": "Archduke Charles, Duke of Teschen", "text": "Archduke Charles Louis John Joseph Lawrence of Austria, Duke of Teschen (; 5 September 177130 April 1847) was an Austrian field-marshal, the third son of Emperor Leopold II and his wife, Maria Luisa of Spain. He was also the younger brother of Francis II, Holy Roman Emperor. He was epileptic, but achieved respect both as a commander and as a reformer of the Austrian army. He was considered one of Napoleon's most formidable opponents and one of the greatest generals of the French Revolutionary and Napoleonic Wars. \nHe began his career fighting the revolutionary armies of France. Early in the wars of the First Coalition, he saw victory at Neerwinden in 1793, before being defeated at Wattignies in 1793 and Fleurus in 1794. In 1796, as chief of all Austrian forces on the Rhine, Charles defeated Jean-Baptiste Jourdan at Amberg, W\u00fcrzburg and Limburg, and then won victories at Wetzlar, Emmendingen and Schliengen that forced Jean Victor Marie Moreau to withdraw across the Rhine. He also defeated opponents at Z\u00fcrich, Ostrach, Stockach, and Mannheim in 1799. He reformed Austria's armies to adopt the nation-at-arms principle. In 1809, he entered the War of the Fifth Coalition and inflicted Napoleon's first major setback at Aspern-Essling, before suffering a defeat at the bloody Battle of Wagram. After Wagram, Charles saw no more significant action in the Napoleonic Wars.\nAs a military strategist, Charles was able to successfully execute complex and risky manoeuvres of troops. However, his contemporary Carl von Clausewitz criticised his rigidity and adherence to \"geographic\" strategy. Many Austrians nevertheless remember Charles as a hero of the French Revolutionary and Napoleonic Wars.\nYouth and early career.\nCharles was born in Florence, Tuscany. His father, then Grand Duke of Tuscany, generously permitted Charles's childless aunt Archduchess Maria Christina of Austria and her husband Albert of Saxe-Teschen to adopt and raise the boy in Vienna. Charles spent his youth in Tuscany, at Vienna and in the Austrian Netherlands, where he began his career of military service in the wars of the French Revolution. He commanded a brigade at the Battle of Jemappes (1792), and in the campaign of 1793 distinguished himself at the Action of Aldenhoven and the Battle of Neerwinden. In this year he became Governor of the Habsburg Netherlands, an office he lost with the occupation of the Low Countries by the French revolutionaries in 1794. The year he became Governor he also received the army rank of lieutenant field marshal. Shortly thereafter another promotion saw him made \"Feldzeugmeister\" (equivalent of Lieutenant General). In the remainder of the war in the Low Countries he held high commands, and was present at the Battle of Fleurus (1794).\nIn 1795 he served on the Rhine, and in the following year, he was entrusted with chief control of all the Austrian forces on that river. His conduct of the operations against Jourdan and Moreau in 1796 marked him out at once as one of the greatest generals in Europe. At first, falling back carefully and avoiding a decision, he finally marched away, leaving a mere screen in front of Moreau. Falling upon Jourdan, he beat him in the battles of Amberg (August), W\u00fcrzburg and Limburg (September), and drove him over the Rhine with great loss. He then turned upon Moreau's army, which he defeated and forced out of Germany after the battles of Wetzlar, Emmendingen and Schliengen.\nNapoleonic Wars.\nIn 1797 he was sent to arrest the victorious march of General Bonaparte in Italy, and he conducted the retreat of the over-matched Austrians with the highest skill. In the campaign of 1799 he once more opposed Jourdan, whom he defeated in the battles of Ostrach and Stockach, following up his success by invading Switzerland and defeating Mass\u00e9na in the First Battle of Zurich, after which he re-entered Germany and drove the French once more over the Rhine after winning at Mannheim in 1799.\nIll-health, however, forced him to retire to Bohemia, but he was soon recalled to undertake the task of checking Moreau's advance on Vienna. The result of the Battle of Hohenlinden had, however, foredoomed the attempt, and the archduke had to make the armistice of Steyr. His popularity was now such that the Perpetual Diet of Regensburg, which met in 1802, resolved to erect a statue in his honour and to give him the title of saviour of his country, but Charles refused both distinctions.\nIn the short and disastrous war of 1805 Archduke Charles commanded what was intended to be the main army in Italy, but events made Germany the decisive theatre of operations; Austria sustained defeat on the Danube, and the archduke was defeated by Massena in the Battle of Caldiero. With the conclusion of peace he began his active work of army reorganisation, which was first tested on the field in 1809.\nIn 1806 Francis II (now Francis I of Austria) named the Archduke Charles, already a field marshal, as Commander in Chief of the Austrian army and Head of the Council of War. Supported by the prestige of being the only general who had proved capable of defeating the French, he promptly initiated a far-reaching scheme of reform, which replaced the obsolete methods of the 18th century. The chief characteristics of the new order were the adoption of the nation in arms principle and the adoption of French war organization and tactics. The army reforms were not yet completed by the war of 1809, in which Charles acted as commander in chief, yet even so it proved a far more formidable opponent than the old and was only defeated after a desperate struggle involving Austrian victories and large loss of life on both sides.\nIts initial successes were neutralised by the reverses of Abensberg, Landshut and Eckm\u00fchl but, after the evacuation of Vienna, the archduke won a strong victory at the Battle of Aspern-Essling but soon afterwards lost at the Battle of Wagram after heavy casualties on both sides. At the end of the campaign the archduke gave up all his military offices.\nIn 1808, when Napoleon had crowned his brother Joseph king of Spain, Archduke Charles had said to his brother, Emperor Francis II, \"Now we know what Napoleon wants \u2013 he wants everything\". \nLater life.\nWhen Austria joined the ranks of the allies during the War of the Sixth Coalition, Charles was not given a command and the post of commander-in-chief of the allied Army of Bohemia went to the Prince of Schwarzenberg. Charles spent the rest of his life in retirement, except for a short time in 1815 when he was military governor of the Fortress Mainz. In 1822 he succeeded to the duchy of Saxe-Teschen. In 1830 Charles was a candidate for the throne of Belgium.\nOn 15 September/17 September 1815 in Weilburg, Charles married Princess Henrietta of Nassau-Weilburg (1797\u20131829). She was a daughter of Frederick William of Nassau-Weilburg (1768\u20131816) and his wife Burgravine Louise Isabelle of Kirchberg.\nCharles died at Vienna on 30 April 1847. He is buried in tomb 122 in the New Vault of the Imperial Crypt in Vienna. An equestrian statue was erected to his memory on the Heldenplatz in Vienna in 1860.\nAssessment of his achievements.\nThe caution which the archduke preached so earnestly in his strategic works, he displayed in practice only when the situation seemed to demand it, although his education certainly prejudiced him in favor of the defensive at all costs. He was at the same time capable of forming and executing the most daring offensive strategy, and his tactical skill in the handling of troops, whether in wide turning movements, as at W\u00fcrzburg and Z\u00fcrich, or in masses, as at Aspern and Wagram, was certainly equal to that of most leaders of his time, with only a few exceptions. Arthur Wellesley named Charles as the greatest general of his time. Charles was arguably the best commander ever produced by the House of Habsburg, and undoubtedly the most able Habsburg general of the French Revolutionary and Napoleonic era. Archduke Charles is credited with handing Napoleon his first major defeat. He has been described as the best general Republican France ever fought, with the exception of Alexander Suvorov.\nAccording to the \"Encyclop\u00e6dia Britannica\" Eleventh Edition, his campaign of 1796 is considered almost faultless. That he sustained defeat in 1809 was due in part to the great numerical superiority of the French and their allies, and in part to the condition of his newly reorganized troops. His six weeks' inaction after the victory of Aspern is, however, open to unfavorable criticism. As a military writer, his position in the evolution of the art of war is very important, and his doctrines had naturally the greatest weight. Nevertheless, they cannot but be considered antiquated even in 1806. Caution and the importance of strategic points are the chief features of his system. The rigidity of his geographical strategy may be gathered from the prescription that \"this principle is never to be departed from.\"\nAgain and again he repeated the advice that nothing should be hazarded unless one's army is completely secure, a rule which he himself neglected with such brilliant results in 1796. Strategic points, he says, not the defeat of the enemy's army, decide the fate of one's own country, and must constantly remain the general's main concern, a maxim which was never more remarkably disproved than in the war of 1809. The editor of the archduke's work is able to make but a feeble defense against Clausewitz's reproach that Charles attached more value to ground than to the annihilation of the foe. In his tactical writings the same spirit is conspicuous. His reserve in battle is designed to \"cover a retreat.\"\nThe baneful influence of these antiquated principles was clearly shown in the maintenance of K\u00f6niggr\u00e4tz-Josefstadt in 1866 as a strategic point, which was preferred to the defeat of the separated Prussian armies, and in the strange plans produced in Vienna for the campaign of 1859, and in the almost unintelligible Battle of Montebello in the same year. The theory and the practice of Archduke Charles form one of the most curious contrasts in military history. In the one he is unreal, in the other he displayed, along with the greatest skill, a vivid activity which made him for long the most formidable opponent of Napoleon.\nHe was the 831st Knight of the Order of the Golden Fleece in Austria.\nCreation of the Austrian staff.\nWhen Karl Mack von Leiberich became chief of staff of the army under Prince Josias of Saxe-Coburg-Saalfeld in the Netherlands, he issued the \"Instruktionspunkte fur die gesamte Herren Generals\", the last of 19 points setting out the roles of staff officers, dealing with offensive and defensive operations, while helping the Commander-in-chief. In 1796, Archduke Charles augmented these with his own \"Observationspunkte\", writing of the Chief of Staff: \"he is duty bound to consider all possibilities related to operations and not view himself as merely carrying out those instructions\". On 20 March 1801, Feldmarschalleutnant Duka became the world's first peacetime \"Generalquartiermeister\" at the head of the staff and the wartime role of the Chief of Staff was now focused on planning and operations to assist the Commander. Archduke Charles produced a new Dienstvorschrift on 1 September 1805, which divided the staff into three: 1) Political Correspondence; 2) the Operations Directorate, dealing with planning and intelligence; 3) the Service Directorate, dealing with administration, supply and military justice. The Archduke set out the position of a modern Chief of Staff: \"The Chief of Staff stands at the side of the Commander-in-Chief and is completely at his disposal. His sphere of work connects him with no specific unit\". \"The Commander-in-Chief decides what should happen and how; his chief assistant works out these decisions, so that each subordinate understands his allotted task\". With the creation of the Korps in 1809, each had a staff, whose chief was responsible for directing operations and executing the overall headquarters plan."}
{"id": "2877", "revid": "4497767", "url": "https://en.wikipedia.org/wiki?curid=2877", "title": "Augustine of Canterbury", "text": "Augustine of Canterbury (early 6th century\u00a0\u2013 most likely 26 May 604) was a Christian monk who became the first archbishop of Canterbury in the year 597. He is considered the \"Apostle to the English\". \nAugustine was the prior of a monastery in Rome when Pope Gregory the Great chose him in 595 to lead a mission, usually known as the Gregorian mission, to Britain to Christianize King \u00c6thelberht and his Kingdom of Kent from Anglo-Saxon paganism. Kent was likely chosen because \u00c6thelberht commanded major influence over neighbouring Anglo-Saxon kingdoms in addition to his marriage to Bertha, a Frankish princess, who was expected to exert some influence over her husband. Before reaching Kent, the missionaries had considered turning back, but Gregory urged them on, and in 597, Augustine landed on the Isle of Thanet and proceeded to \u00c6thelberht's main town of Canterbury.\nKing \u00c6thelberht converted to Christianity and allowed the missionaries to preach freely, giving them land to found a monastery outside the city walls. Augustine was consecrated as a bishop and converted many of the king's subjects, including thousands during a mass baptism on Christmas Day in 597. Pope Gregory sent more missionaries in 601, along with encouraging letters and gifts for the churches, although attempts to persuade the native British bishops to submit to Augustine's authority failed. Roman bishops were established at London, and Rochester in 604, and a school was founded to train Anglo-Saxon priests and missionaries. Augustine also arranged the consecration of his successor, Laurence of Canterbury. The archbishop probably died in 604 and was soon revered as a saint.\nBackground to the mission.\nAfter the withdrawal of the Roman legions from their province of Britannia in 410, the inhabitants were left to defend themselves against the attacks of the Saxons. Before the Roman withdrawal, Britannia had been converted to Christianity and produced the ascetic Pelagius. Britain sent three bishops to the Council of Arles in 314, and a Gaulish bishop went to the island in 396 to help settle disciplinary matters. Material remains testify to a growing presence of Christians, at least until around 360. After the Roman legions departed, pagan tribes settled the southern parts of the island while western Britain, beyond the Anglo-Saxon kingdoms, remained Christian. This native British Church developed in isolation from Rome under the influence of missionaries from Ireland and was centred on monasteries instead of bishoprics. Other distinguishing characteristics were its calculation of the date of Easter and the style of the tonsure haircut that clerics wore. Evidence for the survival of Christianity in the eastern part of Britain during this time includes the survival of the cult of Saint Alban and the occurrence in place names of \"eccles\", derived from the Latin \"ecclesia\", meaning \"church\". There is no evidence that these native Christians tried to convert the Anglo-Saxons. The invasions destroyed most remnants of Roman civilisation in the areas held by the Saxons and related tribes, including the economic and religious structures.\nIt was against this background that Pope Gregory I decided to send a mission, often called the Gregorian mission, to convert the Anglo-Saxons to Christianity in 595. The Kingdom of Kent was ruled by \u00c6thelberht, who had married a Christian princess named Bertha before 588, and perhaps earlier than 560. Bertha was the daughter of Charibert I, one of the Merovingian kings of the Franks. As one of the conditions of her marriage, she brought a bishop named Liudhard with her to Kent. Together in Canterbury, they restored a church that dated to Roman timespossibly the current St Martin's Church. \u00c6thelberht was a pagan at this point but allowed his wife freedom of worship. One biographer of Bertha states that under his wife's influence, \u00c6thelberht asked Pope Gregory to send missionaries. The historian Ian N. Wood feels that the initiative came from the Kentish court as well as the queen. Other historians, however, believe that Gregory initiated the mission, although the exact reasons remain unclear. Bede, an 8th-century monk who wrote a history of the English church, recorded a famous story in which Gregory saw fair-haired Saxon slaves from Britain in the Roman slave market and was inspired to try to convert their people. More practical matters, such as the acquisition of new provinces acknowledging the primacy of the papacy, and a desire to influence the emerging power of the Kentish kingdom under \u00c6thelberht, were probably involved. The mission may have been an outgrowth of the missionary efforts against the Lombards who, as pagans and Arian Christians, were not on good relations with the Catholic church in Rome.\nAside from \u00c6thelberht's granting of freedom of worship to his wife, the choice of Kent was probably dictated by a number of other factors. Kent was the dominant power in southeastern Britain. Since the eclipse of King Ceawlin of Wessex in 592, \u00c6thelberht was the \"bretwalda\", or leading Anglo-Saxon ruler; Bede refers to \u00c6thelberht as having imperium (overlordship) south of the River Humber. Trade between the Franks and \u00c6thelberht's kingdom was well established, and the language barrier between the two regions was apparently only a minor obstacle, as the interpreters for the mission came from the Franks. Lastly, Kent's proximity to the Franks allowed support from a Christian area. There is some evidence, including Gregory's letters to Frankish kings in support of the mission, that some of the Franks felt that they had a claim to overlordship over some of the southern British kingdoms at this time. The presence of a Frankish bishop could also have lent credence to claims of overlordship, if Bertha's Bishop Liudhard was felt to be acting as a representative of the Frankish church and not merely as a spiritual advisor to the queen. Frankish influence was not merely political; archaeological remains attest to a cultural influence as well.\nIn 595, Gregory chose Augustine, who was the prior of the Abbey of St Andrew in Rome, to head the mission to Kent. The pope selected monks to accompany Augustine and sought support from the Frankish royalty and clergy in a series of letters, of which some copies survive in Rome. He wrote to King Theuderic II of Burgundy and to King Theudebert II of Austrasia, as well as their grandmother Brunhild, seeking aid for the mission. Gregory thanked King Chlothar II of Neustria for aiding Augustine. Besides hospitality, the Frankish bishops and kings provided interpreters and Frankish priests to accompany the mission. By soliciting help from the Frankish kings and bishops, Gregory helped to assure a friendly reception for Augustine in Kent, as \u00c6thelbert was unlikely to mistreat a mission which visibly had the support of his wife's relatives and people. Moreover, the Franks appreciated the chance to participate in mission that would extend their influence in Kent. Chlothar, in particular, needed a friendly realm across the Channel to help guard his kingdom's flanks against his fellow Frankish kings.\nSources make no mention of why Pope Gregory chose a monk to head the mission. Pope Gregory once wrote to \u00c6thelberht complimenting Augustine's knowledge of the Bible, so Augustine was evidently well educated. Other qualifications included administrative ability, for Gregory was the abbot of St Andrews as well as being pope, which left the day-to-day running of the abbey to Augustine, the prior.\nArrival and first efforts.\nAugustine was accompanied by Laurence of Canterbury, his eventual successor to the archbishopric, and a group of about 40 companions, some of whom were monks. Soon after leaving Rome, the missionaries halted, daunted by the nature of the task before them. They sent Augustine back to Rome to request papal permission to return. Gregory refused and sent Augustine back with letters encouraging the missionaries to persevere. In 597, Augustine and his companions landed in Kent. They achieved some initial success soon after their arrival: \u00c6thelberht permitted the missionaries to settle and preach in his capital of Canterbury where they used the church of St Martin's for services. Neither Bede nor Gregory mentions the date of \u00c6thelberht's conversion, but it probably took place in 597. In the early medieval period, large-scale conversions required the ruler's conversion first, and Augustine is recorded as making large numbers of converts within a year of his arrival in Kent. Also, by 601, Gregory was writing to both \u00c6thelberht and Bertha, calling the king his son and referring to his baptism. A late medieval tradition, recorded by the 15th-century chronicler Thomas Elmham, gives the date of the king's conversion as Whit Sunday, or 2 June 597; there is no reason to doubt this date, although there is no other evidence for it. Against a date in 597 is a letter of Gregory's to Patriarch Eulogius of Alexandria in June 598, which mentions the number of converts made by Augustine, but does not mention any baptism of the king. However, it is clear that by 601 the king had been converted. His baptism likely took place at Canterbury.\nAugustine established his episcopal see at Canterbury. It is not clear when and where Augustine was consecrated as a bishop. Bede, writing about a century later, states that Augustine was consecrated by the Frankish Archbishop \u00c6therius of Arles, Gaul (France) after the conversion of \u00c6thelberht. Contemporary letters from Pope Gregory, however, refer to Augustine as a bishop before he arrived in England. A letter of Gregory's from September 597 calls Augustine a bishop, and one dated ten months later says Augustine had been consecrated on Gregory's command by bishops of the German lands. The historian R. A. Markus discusses the various theories of when and where Augustine was consecrated, and suggests he was consecrated before arriving in England, but argues the evidence does not permit deciding exactly where this took place.\nSoon after his arrival, Augustine founded the monastery of Saints Peter and Paul, which later became St Augustine's Abbey, on land donated by the king. In a letter Gregory wrote to the patriarch of Alexandria in 598, he claimed that more than 10,000 Christians had been baptised; the number may be exaggerated but there is no reason to doubt that a mass conversion took place. However, there were probably some Christians already in Kent before Augustine arrived, remnants of the Christians who lived in Britain in the later Roman Empire. Little literary traces remain of them, however. One other effect of the king's conversion by Augustine's mission was that the Frankish influence on the southern kingdoms of Britain was decreased.\nAfter these conversions, Augustine sent Laurence back to Rome with a report of his success, along with questions about the mission. Bede records the letter and Gregory's replies in chapter 27 of his \"Historia ecclesiastica gentis Anglorum\"; this section of the \"History\" is usually known as the \"Libellus responsionum\". Augustine asked for Gregory's advice on a number of issues, including how to organise the church, the punishment for church robbers, guidance on who was allowed to marry whom, and the consecration of bishops. Other topics were relations between the churches of Britain and Gaul, childbirth and baptism, and when it was lawful for people to receive communion and for a priest to celebrate mass.\nFurther missionaries were sent from Rome in 601. They brought a pallium for Augustine and a present of sacred vessels, vestments, relics, and books. The pallium was the symbol of metropolitan status, and signified that Augustine was now an archbishop unambiguously associated with the Holy See. Along with the pallium, a letter from Gregory directed the new archbishop to consecrate 12 suffragan bishops as soon as possible and to send a bishop to York. Gregory's plan was that there would be two metropolitans, one at York and one at London, with 12 suffragan bishops under each archbishop. As part of this plan, Augustine was expected to transfer his archiepiscopal see to London from Canterbury. This move never happened; no contemporary sources give the reason, but it was probably because London was not part of \u00c6thelberht's domains. Instead, London was part of the kingdom of Essex, ruled by \u00c6thelberht's nephew Saebert of Essex, who converted to Christianity in 604. The historian S. Brechter has suggested that the metropolitan see was indeed moved to London, and that it was only with the abandonment of London as a see after the death of \u00c6thelberht that Canterbury became the archiepiscopal see. This theory contradicts Bede's version of events, however.\nAdditional work.\nIn 604, Augustine founded two more bishoprics in Britain. Two men who had come to Britain with him in 601 were consecrated, Mellitus as Bishop of London and Justus as Bishop of Rochester. Bede relates that Augustine, with the help of the king, \"recovered\" a church built by Roman Christians in Canterbury. It is not clear if Bede meant that Augustine rebuilt the church or that Augustine merely reconsecrated a building that had been used for pagan worship. Archaeological evidence seems to support the latter interpretation; in 1973 the remains of an aisled building dating from the Romano-British period were uncovered just south of the present Canterbury Cathedral. The historian Ian Wood argues that the existence of the \"Libellus\" points to more contact between Augustine and the native Christians because the topics covered in the work are not restricted to conversion from paganism, but also dealt with relations between differing styles of Christianity.\nAugustine failed to extend his authority to the Christians in Wales and Dumnonia to the west. Gregory had decreed that these Christians should submit to Augustine and that their bishops should obey him, apparently believing that more of the Roman governmental and ecclesiastical organisation survived in Britain than was actually the case. According to the narrative of Bede, the Britons in these regions viewed Augustine with uncertainty, and their suspicion was compounded by a diplomatic misjudgement on Augustine's part. In 603, Augustine and \u00c6thelberht summoned the British bishops to a meeting south of the Severn. These guests retired early to confer with their people, who, according to Bede, advised them to judge Augustine based upon the respect he displayed at their next meeting. When Augustine failed to rise from his seat on the entrance of the British bishops, they refused to recognise him as their archbishop. There were, however, deep differences between Augustine and the British church that perhaps played a more significant role in preventing an agreement. At issue were the tonsure, the observance of Easter, and practical and deep-rooted differences in approach to asceticism, missionary endeavours, and how the church itself was organised. Some historians believe that Augustine had no real understanding of the history and traditions of the British church, damaging his relations with their bishops. Also, there were political dimensions involved, as Augustine's efforts were sponsored by the Kentish king, and at this period the Wessex and Mercian kingdoms were expanding to the west, into areas held by the Britons.\nFurther success.\nGregory also instructed Augustine on other matters. Temples were to be consecrated for Christian use, and feasts, if possible, moved to days celebrating Christian martyrs. One religious site was revealed to be a shrine of a local St Sixtus, whose worshippers were unaware of details of the martyr's life or death. They may have been native Christians, but Augustine did not treat them as such. When Gregory was informed, he told Augustine to stop the cult and use the shrine for the Roman St Sixtus.\nGregory legislated on the behaviour of the laity and the clergy. He placed the new mission directly under papal authority and made it clear that English bishops would have no authority over Frankish counterparts nor vice versa. Other directives dealt with the training of native clergy and the missionaries' conduct.\nThe King's School, Canterbury claims Augustine as its founder, which would make it the world's oldest existing school, but the first documentary records of the school date from the 16th century. Augustine did establish a school, and soon after his death Canterbury was able to send teachers out to support the East Anglian mission. Augustine received liturgical books from the pope, but their exact contents are unknown. They may have been some of the new mass books that were being written at this time. The exact liturgy that Augustine introduced to England remains unknown, but it would have been a form of the Latin language liturgy in use at Rome.\nDeath and legacy.\nBefore his death, Augustine consecrated Laurence of Canterbury as his successor to the archbishopric, probably to ensure an orderly transfer of office. Although at the time of Augustine's death, 26 May 604, the mission barely extended beyond Kent, his undertaking introduced a more active missionary style into the British Isles. Despite the earlier presence of Christians in Ireland and Wales, no efforts had been made to try to convert the Saxon invaders. Augustine was sent to convert the descendants of those invaders, and eventually became the decisive influence in Christianity in most of the British Isles. Much of his success came about because of Augustine's close relationship with \u00c6thelberht, which gave the archbishop time to establish himself. Augustine's example also influenced the great missionary efforts of the Anglo-Saxon Church.\nAugustine's body was originally buried in the portico of what is now St Augustine's, Canterbury, but it was later exhumed and placed in a tomb within the abbey church, which became a place of pilgrimage and veneration. After the Norman Conquest the cult of St Augustine was actively promoted. After the Conquest, his shrine in St Augustine's Abbey held a central position in one of the axial chapels, flanked by the shrines of his successors Laurence and Mellitus. King Henry I of England granted St. Augustine's Abbey a six-day fair around the date on which Augustine's relics were translated to his new shrine, from 8 September through 13 September.\nA life of Augustine was written by Goscelin around 1090, but this life portrays Augustine in a different light, compared to Bede's account. Goscelin's account has little new historical content, mainly being filled with miracles and imagined speeches. Building on this account, later medieval writers continued to add new miracles and stories to Augustine's life, often quite fanciful. These authors included William of Malmesbury, who claimed that Augustine founded Cerne Abbey, the author (generally believed to be John Brompton) of a late medieval chronicle containing invented letters from Augustine, and a number of medieval writers who included Augustine in their romances. Another problem with investigating Augustine's saintly cult is the confusion resulting because most medieval liturgical documents mentioning Augustine do not distinguish between Augustine of Canterbury and Augustine of Hippo, a fourth-century saint. Medieval Scandinavian liturgies feature Augustine of Canterbury quite often, however. During the English Reformation, Augustine's shrine was destroyed and his relics were lost.\nAugustine's shrine was re-established in March 2012 at the church of St. Augustine in Ramsgate, Kent, very close to the mission's landing site. St Augustine's Cross, a Celtic cross erected in 1884, marks the spot in Ebbsfleet, Thanet, East Kent, where the newly arrived Augustine is said to have first met and preached to the awaiting King Ethelbert."}
{"id": "2880", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=2880", "title": "Anti-Ballistic Missile", "text": ""}
