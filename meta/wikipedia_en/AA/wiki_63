{"id": "7131", "revid": "48774147", "url": "https://en.wikipedia.org/wiki?curid=7131", "title": "Charles Evers", "text": "James Charles Evers (September 11, 1922July 22, 2020) was an American civil rights activist, businessman, radio personality, and politician. Evers was known for his role in the civil rights movement along with his younger brother Medgar Evers. After serving in World War II, Evers began his career as a disc jockey at WHOC in Philadelphia, Mississippi. In 1954, he was made the National Association for the Advancement of Colored People (NAACP) State Voter Registration chairman. After his brother's assassination in 1963, Evers took over his position as field director of the NAACP in Mississippi. In this role, he organized and led many demonstrations for the rights of African Americans.\nIn 1969, Evers was named \"Man of the Year\" by the NAACP. On June 3, 1969, Evers was elected in Fayette, Mississippi, as the first African-American mayor of a biracial town in Mississippi since the Reconstruction era, following passage of the Voting Rights Act of 1965 which enforced constitutional rights for citizens.\nAt the time of Evers's election as mayor, the town of Fayette had a population of 1,600 of which 75% was African-American and almost 25% white; the white officers on the Fayette city police \"resigned rather than work under a black administration,\" according to the Associated Press. Evers told reporters \"I guess we will just have to operate with an all-black police department for the present. But I am still looking for some whites to join us in helping Fayette grow.\" Evers then outlawed the carrying of firearms within city limits.\nHe ran for governor in 1971 and the United States Senate in 1978, both times as an independent candidate. In 1989, Evers was defeated for re-election after serving sixteen years as mayor. In his later life, he became a Republican, endorsing Ronald Reagan in 1980, and more recently Donald Trump in 2016. This diversity in party affiliations throughout his life was reflected in his fostering of friendships with people from a variety of backgrounds, as well as his advising of politicians from across the political spectrum. After his political career ended, he returned to radio and hosted his own show, \"Let's Talk\". In 2017, Evers was inducted into the National Rhythm &amp; Blues Hall of Fame for his contributions to the music industry.\nEarly life and education.\nCharles Evers was born in Decatur, Mississippi, on September 11, 1922, to James Evers, a laborer, and Jesse Wright Evers, a maid. He was the eldest of four children; Medgar Evers was his younger brother. He attended segregated public schools, which were typically underfunded in Mississippi following the exclusion of African Americans from the political system by disenfranchisement after 1890. Evers graduated from Alcorn State University in Lorman, Mississippi.\nCareer.\nBusiness activities.\nDuring World War II, Charles and Medgar Evers both served in the United States Army. Charles fell in love with a Philippine woman while stationed overseas. He could not marry her and bring her home to Mississippi because the state's constitution prohibited interracial marriages.\nDuring the war he established a brothel in Quezon City which catered to American servicemen. After serving a year of reserve duty following the Korean War, he settled in Philadelphia, Mississippi. In 1949, he began working as a disc jockey at WHOC, making him the first black disc jockey in the state. By the early 1950s, he was managing a hotel, cab company, and burial insurance business in the town. He had a cafe in Philadelphia and influenced over two hundred black citizens to pay their poll tax. Forced to leave due to local white hostility in 1956, he moved to Chicago. Low on money, he began working as a meatpacker in stockyards during the day and as an attendant for the men's restroom at the Conrad Hilton Hotel at nights. He also began pimping and ran a numbers game, taking $500 a week from the latter. He gained enough money to purchase several bars, bootlegged liquor, and sold jukeboxes.\nCivil rights activism.\nIn Mississippi about 1951, brothers Charles and Medgar Evers grew interested in African freedom movements. They were interested in Jomo Kenyatta and the rise of the Kikuyu tribal resistance to colonialism in Kenya, known as the Mau Mau uprising as it moved to open violence. Along with his brother, Charles became active in the Regional Council of Negro Leadership (RCNL), a civil rights organization that promoted self-help and business ownership. He also helped his brother with black voter registration drives. Between 1952 and 1955, Evers often spoke at the RCNL's annual conferences in Mound Bayou, a town founded by freedmen, on such issues as voting rights. His brother Medgar continued to be involved in civil rights, becoming field secretary and head of the National Association for the Advancement of Colored People (NAACP) in Mississippi. While working in Chicago he sent money to him, not specifying the source.\nOn June 12, 1963, Byron De La Beckwith, a member of a Ku Klux Klan chapter, fatally shot Evers's brother, Medgar, in Mississippi as he arrived home from work. Medgar died at the hospital in Jackson. Charles learned of his brother's death several hours later and flew to Jackson the following morning. Deeply upset by the assassination, he heavily involved himself in the planning of his brother's funeral. He decided to relocate to Mississippi to carry on his brother's work. Journalist Jason Berry, who later worked for Charles, said, \"I think he wanted to be a better person. I think Medgar's death was a cathartic experience.\" A decade after his death, Evers and blues musician B.B. King created the Medgar Evers Homecoming Festival, an annual three-day event held the first week of June in Mississippi.\nOver the opposition of more establishment figures in the National Association for the Advancement of Colored People (NAACP) such as Roy Wilkins, Evers took over his brother's post as head of the NAACP in Mississippi. Wilkins never managed a friendly relationship with Evers, and Medgar's widow, Myrlie, also disapproved of Charles' replacing him. A staunch believer in racial integration, he distrusted what he viewed as the militancy and separatism of the Student Nonviolent Coordinating Committee and the Mississippi Freedom Democratic Party, a black-dominated breakaway of the segregationist Mississippi Democratic Party. In 1965 he launched a series of successful black boycotts in southwestern Mississippi which partnered with the Natchez Deacons for Defense and Justice, which won concessions from the Natchez authorities and ratified his unconventional boycott methods. Often accompanied by a group of 65 male followers, he would pressure local blacks in small towns to avoid stores under boycott and directly challenge white business leaders. He also led a voter registration campaign. He coordinated his efforts from the small town of Fayette in Jefferson County. Fayette was a small, economically depressed town of about 2,500 people. About three-fourths of the population was black, and they had long been socially and economically subordinate to the white minority. Evers moved the NAACP's Mississippi field office from Jackson to Fayette to take advantage of the potential of the black majority and achieve political influence in Jefferson and two adjacent counties. He explained, \"My feeling is that Negroes gotta control somewhere in America, and we've dropped anchor in these counties. We are going to control these three counties in the next ten years. There is no question about it.\"\nWith his voter registration drives having made Fayette's number of black registered voters double the size of the white electorate, Evers helped elect a black man to the local school board in 1966. He also established the Medgar Evers Community Center at the outskirts of town, which served as a center for registration efforts, grocery store, restaurant, and dance hall. By early 1968 he had established a network of local NAACP branches in the region. The president of each branch served as Evers' deputies, and he attended all of their meetings. That year he made a bid for the open seat of the 3rd congressional district in the U.S. House of Representatives, facing six white opponents in the Democratic primary. Though low on funds, he led in the primary with a plurality of the votes. The Mississippi Legislature responded by passing a law mandating a runoff primary in the event of no absolute majority in the initial contest, which Evers lost. He also supported Robert F. Kennedy's 1968 presidential campaign, serving as co-director of his Mississippi campaign organization, and was with Kennedy in Los Angeles when he was assassinated.\nMayor of Fayette.\nIn May 1969, Evers ran for the office of Mayor of Fayette and defeated white incumbent R. J. Allen, 386 votes to 255. This made him the first black mayor of a biracial Mississippi town (unlike the all-black Mound Bayou) since Reconstruction. Evers' election as mayor had great symbolic significance statewide and attracted national attention. The NAACP named Evers their 1969 Man of the Year. Evers popularized the slogan, \"Hands that picked cotton can now pick the mayor.\" The local white community was bitter about his victory, but he became intensively popular among Mississippi's blacks. To celebrate his victory, he hosted an inaugural ball in Natchez, which was widely attended by black Mississippians, reporters from around the country, and prominent national liberals including Ramsey Clark, Ted Sorensen, Whitney Young, Julian Bond, Shirley MacLaine, and Paul O'Dwyer. The white-dominated school board refused to let Evers swear-in on property under their jurisdiction, so he took his oath of office in a parking lot.\nEvers appointed a black police force and several black staff members. He also benefitted from an influx of young, white liberal volunteers who wanted to assist a civil rights leader. Many ended up leaving after growing disillusioned with Evers' pursuit of personal financial success and domineering leadership style. Evers sought to make Fayette an upstanding community and a symbolic refuge for black people. Repulsed by the behavior of poor blacks in the town, he ordered the police force to enforce a 25-mile per hour speed limit on local roads, banned cursing in public, and cracked down on truancy. He also prohibited the carrying of firearms in town but kept a gun on himself. He quickly responded to concerns from poor blacks while making white businessmen wait outside of his office. Rhetorically, he would vacillate between messages of racial conciliation and statements of hostility.\nFayette's white population remained bitter about Evers' victory. Many avoided the city hall where they used to socialize and \"The Fayette Chronicle\" regularly criticized him. He argued with the county board of supervisors over his plan to erect busts of his brother, Martin Luther King Jr., and the Kennedys on the courthouse square. He told the press, \"They're cooperating because they haven't blown my head off. This is Mississippi.\" In September 1969, a Klansman drove into Fayette with a collection of weapons, intending to assassinate Evers. A white resident tipped off the mayor and the Klansman was arrested. The Klansman defended his motives by saying, \"I am a Mississippi white man\".\nEvers' moralistic style began to create discontent; in early 1970, most of Fayette's police department resigned, saying the mayor had treated them \"like dogs\". Evers complained that local blacks were \"jealous\" of him. As the judge in the municipal court, he personally issued fines for infractions such as cursing in public. He regularly ignored the input of the town board of aldermen, and town employee Charles Ramberg reported that he said he would fire municipal workers who would not vote for him. During Evers' tenure, Fayette benefitted from several federal grants, and ITT Inc. built an assembly plant in the town, but the region's economy largely remained depressed. By 1981, Jefferson County had the highest unemployment rate in the state.\nWhites' perception that Evers was venal and self-interested persisted and began to spread among the black community. This problem ballooned when in 1974 the Internal Revenue Service arranged for him to be indicted for tax evasion by failing to report $156,000 in income he garnered in the late 1960s. Prosecutors further accused him of depositing town funds in a personal bank account. His attorney told the court that Evers had indeed concealed the income, but argued that the charge was invalid since this had been done before the late 1960s, as the indictment specified. The case resulted in a mistrial, but Evers' reputation permanently suffered. In the late 1970s he used a $5,300 federal grant to renovate a building he owned which he leased to a federal day care program, and used some of the employees for personal business.\nEvers served many terms as mayor of Fayette. Admired by some, he alienated others with his inflexible stands on various issues. Evers did not like to share or delegate power. Evers lost the Democratic primary for mayor in 1981 to Kennie Middleton. Four years later, Evers defeated Middleton in the primaries and won back the office of mayor. In 1989, Evers lost the nomination once again to political rival Kennie Middleton. In his response to the defeat, Evers accepted, said he was tired, and that: \"Twenty years is enough. I'm tired of being out front. Let someone else be out front.\"\n1971 gubernatorial campaign.\nEvers began mulling the possibility of a campaign for the office of governor in 1969. He decided to enter the 1971 gubernatorial election as an independent, kicking off his campaign with a rally in Decatur. He later explained his reason for launching the bid, saying, \"I ran for governor because if someone doesn't start running, there will never be a black man or a black woman governor of the state of Mississippi.\" He endorsed white segregationist Jimmy Swan in the Democratic primary, reasoning that if Swan won the nomination, moderate whites would be more inclined to vote for himself in the general election. He campaigned on a platform of reduced taxes\u2014particularly for lower property taxes on the elderly, improved healthcare, and legalizing gambling along the Gulf Coast. Low on money, his candidacy was largely funded by the sale of campaign buttons and copies of his recently published autobiography. His campaign staff was largely young and inexperienced and lacked organization.\nEvers' rallies drew large crowds of blacks. \"The Clarion-Ledger\", a leading Mississippian conservative newspaper, largely ignored his campaign. To gain attention, he unexpectedly gatecrashed the annual Fisherman's Rodeo in Pascagoula and stopped and spoke to people on the streets of Jackson during their morning commute. Police departments in rural towns were often horrified by the arrival of his campaign caravan. A total of 269 other black candidates were running for office in Mississippi that year, and many of them complained that Evers was self-absorbed and hoarding resources, despite his slim chances of winning. Evers did little to support them.\nIn the general election, Evers faced Democratic nominee Bill Waller and independent segregationist Thomas Pickens Brady. Waller and Evers were personally acquainted with one another, as Waller had prosecuted Beckwith for the murder of Medgar. Despite the fears of public observers, the campaign was largely devoid of overt racist appeals and Evers and Waller avoided negative tactics. Though about 40 percent of the Mississippi electorate in 1971 was black, Evers only secured about 22 percent of the total vote; Waller won with 601,222 votes to Evers' 172,762 and Brady's 6,653. The night of the election, Evers shook the hands of Waller supporters in Jackson and then went to a local television station where his opponent was delivering a victory speech. Learning that Evers had arrived, Waller's nervous aides hurried the governor-elect to his car. Evers approached the car shortly before its departure and told Waller, \"I just wanted to congratulate you.\" Waller replied, \"Whaddya say, Charlie?\" and his wife leaned over and shook Evers' hand.\nLater political career.\nIn 1978, Evers ran as an independent for the U.S. Senate seat vacated by Democrat James Eastland. He finished in third place behind his opponents, Democrat Maurice Dantin and Republican Thad Cochran. He received 24 percent of the vote, likely siphoning off African-American votes that would have otherwise gone to Dantin. Cochran won the election with a plurality of 45 percent of the vote. With the shift in white voters moving into the Republican Party in the state (and the rest of the South), Cochran was continuously re-elected to his Senate seat. After his failed Senate race, Evers briefly switched political parties and became a Republican.\nIn 1983, Evers ran as an independent for governor of Mississippi but lost to the Democrat Bill Allain. Republican Leon Bramlett of Clarksdale, also known as a college All-American football player, finished second with 39 percent of the vote.\nEvers endorsed Ronald Reagan for President of the United States during the 1980 United States presidential election. Evers later attracted controversy for his support of judicial nominee Charles W. Pickering, a Republican, who was nominated by President George H. W. Bush for a seat on the U.S. Court of Appeals. Evers criticized the NAACP and other organizations for opposing Pickering, as he said the candidate had a record of supporting the civil rights movement in Mississippi.\nEvers befriended a range of people from sharecroppers to presidents. He was an informal adviser to politicians as diverse as Lyndon B. Johnson, George C. Wallace, Ronald Reagan and Robert F. Kennedy. Evers severely criticized such national leaders as Roy Wilkins, Stokely Carmichael, H. Rap Brown and Louis Farrakhan over various issues.\nEvers was a member of the Republican Party for 30 years when he spoke warmly of the 2008 election of Barack Obama as the first black President of the United States. During the 2016 presidential election, Evers supported Donald Trump's presidential campaign.\nBooks.\nEvers wrote two autobiographies or memoirs: \"Evers\" (1971), written with Grace Halsell and self-published; and \"Have No Fear,\" written with Andrew Szanton and published by John Wiley &amp; Sons (1997).\nPersonal life.\nEvers was briefly married to Christine Evers until their marriage ended in annulment. In 1951, Evers married Nannie L. Magee, with whom he had four daughters. The couple divorced in June 1974. Evers lived in Brandon, Mississippi, and served as station manager of WMPR 90.1 FM in Jackson.\nOn July 22, 2020, Evers died in Brandon at age 97.\nMedia portrayal.\nEvers was portrayed by Bill Cobbs in the 1996 film \"Ghosts of Mississippi\" (1996)."}
{"id": "7133", "revid": "1201040", "url": "https://en.wikipedia.org/wiki?curid=7133", "title": "Collective nouns/All sorted by collective term", "text": ""}
{"id": "7142", "revid": "29615425", "url": "https://en.wikipedia.org/wiki?curid=7142", "title": "CDMA", "text": ""}
{"id": "7143", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=7143", "title": "Code-division multiple access", "text": "Code-division multiple access (CDMA) is a channel access method used by various radio communication technologies. CDMA is an example of multiple access, where several transmitters can send information simultaneously over a single communication channel. This allows several users to share a band of frequencies (see bandwidth). To permit this without undue interference between the users, CDMA employs spread spectrum technology and a special coding scheme (where each transmitter is assigned a code).\nCDMA optimizes the use of available bandwidth as it transmits over the entire frequency range and does not limit the user's frequency range.\nIt is used as the access method in many mobile phone standards. IS-95, also called \"cdmaOne\", and its 3G evolution CDMA2000, are often simply referred to as \"CDMA\", but UMTS, the 3G standard used by GSM carriers, also uses \"wideband CDMA\", or W-CDMA, as well as TD-CDMA and TD-SCDMA, as its radio technologies. Many carriers (such as AT&amp;T, UScellular and Verizon) shut down 3G CDMA-based networks in 2022 and 2024, rendering handsets supporting only those protocols unusable for calls, even to 911.\nIt can be also used as a channel or medium access technology, like ALOHA for example or as a permanent pilot/signalling channel to allow users to synchronize their local oscillators to a common system frequency, thereby also estimating the channel parameters permanently.\nIn these schemes, the message is modulated on a longer spreading sequence, consisting of several chips (0es and 1es). Due to their very advantageous auto- and crosscorrelation characteristics, these spreading sequences have also been used for radar applications for many decades, where they are called Barker codes (with a very short sequence length of typically 8 to 32).\nFor space-based communication applications, CDMA has been used for many decades due to the large path loss and Doppler shift caused by satellite motion. CDMA is often used with binary phase-shift keying (BPSK) in its simplest form, but can be combined with any modulation scheme like (in advanced cases) quadrature amplitude modulation (QAM) or orthogonal frequency-division multiplexing (OFDM), which typically makes it very robust and efficient (and equipping them with accurate ranging capabilities, which is difficult without CDMA). Other schemes use subcarriers based on binary offset carrier modulation (BOC modulation), which is inspired by Manchester codes and enable a larger gap between the virtual center frequency and the subcarriers, which is not the case for OFDM subcarriers.\nHistory.\nThe technology of code-division multiple access channels has long been known.\nUnited States.\nIn the US, one of the earliest descriptions of CDMA can be found in the summary report of Project Hartwell on \"The Security of Overseas Transport\", which was a summer research project carried out at the Massachusetts Institute of Technology from June to August 1950. Further research in the context of jamming and anti-jamming was carried out in 1952 at Lincoln Lab.\nSoviet Union.\nIn the Soviet Union (USSR), the first work devoted to this subject was published in 1935 by Dmitry Ageev. It was shown that through the use of linear methods, there are three types of signal separation: frequency, time and compensatory. The technology of CDMA was used in 1957, when the young military radio engineer Leonid Kupriyanovich in Moscow made an experimental model of a wearable automatic mobile phone, called LK-1 by him, with a base station. LK-1 has a weight of 3\u00a0kg, 20\u201330\u00a0km operating distance, and 20\u201330 hours of battery life. The base station, as described by the author, could serve several customers. In 1958, Kupriyanovich made the new experimental \"pocket\" model of mobile phone. This phone weighed 0.5\u00a0kg. To serve more customers, Kupriyanovich proposed the device, which he called \"correlator.\" In 1958, the USSR also started the development of the \"Altai\" national civil mobile phone service for cars, based on the Soviet MRT-1327 standard. The phone system weighed . It was placed in the trunk of the vehicles of high-ranking officials and used a standard handset in the passenger compartment. The main developers of the Altai system were VNIIS (Voronezh Science Research Institute of Communications) and GSPI (State Specialized Project Institute). In 1963 this service started in Moscow, and in 1970 Altai service was used in 30 USSR cities.\nSteps in CDMA modulation.\nCDMA is a spread-spectrum multiple-access technique. A spread-spectrum technique spreads the bandwidth of the data uniformly for the same transmitted power. A spreading code is a pseudo-random code in the time domain that has a narrow ambiguity function in the frequency domain, unlike other narrow pulse codes. In CDMA a locally generated code runs at a much higher rate than the data to be transmitted. Data for transmission is combined by bitwise XOR (exclusive OR) with the faster code. The figure shows how a spread-spectrum signal is generated. The data signal with pulse duration of formula_1 (symbol period) is XORed with the code signal with pulse duration of formula_2 (chip period). (Note: bandwidth is proportional to formula_3, where formula_4 = bit time.) Therefore, the bandwidth of the data signal is formula_5 and the bandwidth of the spread spectrum signal is formula_6. Since formula_2 is much smaller than formula_1, the bandwidth of the spread-spectrum signal is much larger than the bandwidth of the original signal. The ratio formula_9 is called the spreading factor or processing gain and determines to a certain extent the upper limit of the total number of users supported simultaneously by a base station.\nEach user in a CDMA system uses a different code to modulate their signal. Choosing the codes used to modulate the signal is very important in the performance of CDMA systems. The best performance occurs when there is good separation between the signal of a desired user and the signals of other users. The separation of the signals is made by correlating the received signal with the locally generated code of the desired user. If the signal matches the desired user's code, then the correlation function will be high and the system can extract that signal. If the desired user's code has nothing in common with the signal, the correlation should be as close to zero as possible (thus eliminating the signal); this is referred to as cross-correlation. If the code is correlated with the signal at any time offset other than zero, the correlation should be as close to zero as possible. This is referred to as auto-correlation and is used to reject multi-path interference.\nAn analogy to the problem of multiple access is a room (channel) in which people wish to talk to each other simultaneously. To avoid confusion, people could take turns speaking (time division), speak at different pitches (frequency division), or speak in different languages (code division). CDMA is analogous to the last example where people speaking the same language can understand each other, but other languages are perceived as noise and rejected. Similarly, in radio CDMA, each group of users is given a shared code. Many codes occupy the same channel, but only users associated with a particular code can communicate.\nIn general, CDMA belongs to two basic categories: synchronous (orthogonal codes) and asynchronous (pseudorandom codes).\nCode-division multiplexing (synchronous CDMA).\nThe digital modulation method is analogous to those used in simple radio transceivers. In the analog case, a low-frequency data signal is time-multiplied with a high-frequency pure sine-wave carrier and transmitted. This is effectively a frequency convolution (Wiener\u2013Khinchin theorem) of the two signals, resulting in a carrier with narrow sidebands. In the digital case, the sinusoidal carrier is replaced by Walsh functions. These are binary square waves that form a complete orthonormal set. The data signal is also binary and the time multiplication is achieved with a simple XOR function. This is usually a Gilbert cell mixer in the circuitry.\nSynchronous CDMA exploits mathematical properties of orthogonality between vectors representing the data strings. For example, the binary string \"1011\" is represented by the vector (1, 0, 1, 1). Vectors can be multiplied by taking their dot product, by summing the products of their respective components (for example, if u = (\"a\", \"b\") and v = (\"c\", \"d\"), then their dot product u\u00b7v = \"ac\" + \"bd\"). If the dot product is zero, the two vectors are said to be \"orthogonal\" to each other. Some properties of the dot product aid understanding of how W-CDMA works. If vectors a and b are orthogonal, then formula_10 and:\nEach user in synchronous CDMA uses a code orthogonal to the others' codes to modulate their signal. An example of 4 mutually orthogonal digital signals is shown in the figure below. Orthogonal codes have a cross-correlation equal to zero; in other words, they do not interfere with each other. In the case of IS-95, 64-bit Walsh codes are used to encode the signal to separate different users. Since each of the 64 Walsh codes is orthogonal to all other, the signals are channelized into 64 orthogonal signals. The following example demonstrates how each user's signal can be encoded and decoded.\nExample.\nStart with a set of vectors that are mutually orthogonal. (Although mutual orthogonality is the only condition, these vectors are usually constructed for ease of decoding, for example columns or rows from Walsh matrices.) An example of orthogonal functions is shown in the adjacent picture. These vectors will be assigned to individual users and are called the \"code\", \"chip code\", or \"chipping code\". In the interest of brevity, the rest of this example uses codes v with only two bits.\nEach user is associated with a different code, say v. A 1 bit is represented by transmitting a positive code v, and a 0 bit is represented by a negative code \u2212v. For example, if v = (\"v\"0, \"v\"1) = (1, \u22121) and the data that the user wishes to transmit is (1, 0, 1, 1), then the transmitted symbols would be\nFor the purposes of this article, we call this constructed vector the \"transmitted vector\".\nEach sender has a different, unique vector v chosen from that set, but the construction method of the transmitted vector is identical.\nNow, due to physical properties of interference, if two signals at a point are in phase, they add to give twice the amplitude of each signal, but if they are out of phase, they subtract and give a signal that is the difference of the amplitudes. Digitally, this behaviour can be modelled by the addition of the transmission vectors, component by component.\nIf sender0 has code (1, \u22121) and data (1, 0, 1, 1), and sender1 has code (1, 1) and data (0, 0, 1, 1), and both senders transmit simultaneously, then this table describes the coding steps:\nBecause signal0 and signal1 are transmitted at the same time into the air, they add to produce the raw signal\nThis raw signal is called an interference pattern. The receiver then extracts an intelligible signal for any known sender by combining the sender's code with the interference pattern. The following table explains how this works and shows that the signals do not interfere with one another:\nFurther, after decoding, all values greater than 0 are interpreted as 1, while all values less than zero are interpreted as 0. For example, after decoding, data0 is (2, \u22122, 2, 2), but the receiver interprets this as (1, 0, 1, 1). Values of exactly 0 mean that the sender did not transmit any data, as in the following example:\nAssume signal0 = (1, \u22121, \u22121, 1, 1, \u22121, 1, \u22121) is transmitted alone. The following table shows the decode at the receiver:\nWhen the receiver attempts to decode the signal using sender1's code, the data is all zeros; therefore the cross-correlation is equal to zero and it is clear that sender1 did not transmit any data.\nAsynchronous CDMA.\nWhen mobile-to-base links cannot be precisely coordinated, particularly due to the mobility of the handsets, a different approach is required. Since it is not mathematically possible to create signature sequences that are both orthogonal for arbitrarily random starting points and which make full use of the code space, unique \"pseudo-random\" or \"pseudo-noise\" sequences called spreading sequences are used in \"asynchronous\" CDMA systems. A spreading sequence is a binary sequence that appears random but can be reproduced in a deterministic manner by intended receivers. These spreading sequences are used to encode and decode a user's signal in asynchronous CDMA in the same manner as the orthogonal codes in synchronous CDMA (shown in the example above). These spreading sequences are statistically uncorrelated, and the sum of a large number of spreading sequences results in \"multiple access interference\" (MAI) that is approximated by a Gaussian noise process (following the central limit theorem in statistics). Gold codes are an example of a spreading sequence suitable for this purpose, as there is low correlation between the codes. If all of the users are received with the same power level, then the variance (e.g., the noise power) of the MAI increases in direct proportion to the number of users. In other words, unlike synchronous CDMA, the signals of other users will appear as noise to the signal of interest and interfere slightly with the desired signal in proportion to number of users.\nAll forms of CDMA use the spread-spectrum spreading factor to allow receivers to partially discriminate against unwanted signals. Signals encoded with the specified spreading sequences are received, while signals with different sequences (or the same sequences but different timing offsets) appear as wideband noise reduced by the spreading factor.\nSince each user generates MAI, controlling the signal strength is an important issue with CDMA transmitters. A CDM (synchronous CDMA), TDMA, or FDMA receiver can in theory completely reject arbitrarily strong signals using different codes, time slots or frequency channels due to the orthogonality of these systems. This is not true for asynchronous CDMA; rejection of unwanted signals is only partial. If any or all of the unwanted signals are much stronger than the desired signal, they will overwhelm it. This leads to a general requirement in any asynchronous CDMA system to approximately match the various signal power levels as seen at the receiver. In CDMA cellular, the base station uses a fast closed-loop power-control scheme to tightly control each mobile's transmit power.\nIn 2019, schemes to precisely estimate the required length of the codes in dependence of Doppler and delay characteristics have been developed. Soon after, machine learning based techniques that generate sequences of a desired length and spreading properties have been published as well. These are highly competitive with the classic Gold and Welch sequences. These are not generated by linear-feedback-shift-registers, but have to be stored in lookup tables.\nAdvantages of asynchronous CDMA over other techniques.\nEfficient practical utilization of the fixed frequency spectrum.\nIn theory CDMA, TDMA and FDMA have exactly the same spectral efficiency, but, in practice, each has its own challenges \u2013 power control in the case of CDMA, timing in the case of TDMA, and frequency generation/filtering in the case of FDMA.\nTDMA systems must carefully synchronize the transmission times of all the users to ensure that they are received in the correct time slot and do not cause interference. Since this cannot be perfectly controlled in a mobile environment, each time slot must have a guard time, which reduces the probability that users will interfere, but decreases the spectral efficiency.\nSimilarly, FDMA systems must use a guard band between adjacent channels, due to the unpredictable Doppler shift of the signal spectrum because of user mobility. The guard bands will reduce the probability that adjacent channels will interfere, but decrease the utilization of the spectrum.\nFlexible allocation of resources.\nAsynchronous CDMA offers a key advantage in the flexible allocation of resources i.e. allocation of spreading sequences to active users. In the case of CDM (synchronous CDMA), TDMA, and FDMA the number of simultaneous orthogonal codes, time slots, and frequency slots respectively are fixed, hence the capacity in terms of the number of simultaneous users is limited. There are a fixed number of orthogonal codes, time slots or frequency bands that can be allocated for CDM, TDMA, and FDMA systems, which remain underutilized due to the bursty nature of telephony and packetized data transmissions. There is no strict limit to the number of users that can be supported in an asynchronous CDMA system, only a practical limit governed by the desired bit error probability since the SIR (signal-to-interference ratio) varies inversely with the number of users. In a bursty traffic environment like mobile telephony, the advantage afforded by asynchronous CDMA is that the performance (bit error rate) is allowed to fluctuate randomly, with an average value determined by the number of users times the percentage of utilization. Suppose there are 2\"N\" users that only talk half of the time, then 2\"N\" users can be accommodated with the same \"average\" bit error probability as \"N\" users that talk all of the time. The key difference here is that the bit error probability for \"N\" users talking all of the time is constant, whereas it is a \"random\" quantity (with the same mean) for 2\"N\" users talking half of the time.\nIn other words, asynchronous CDMA is ideally suited to a mobile network where large numbers of transmitters each generate a relatively small amount of traffic at irregular intervals. CDM (synchronous CDMA), TDMA, and FDMA systems cannot recover the underutilized resources inherent to bursty traffic due to the fixed number of orthogonal codes, time slots or frequency channels that can be assigned to individual transmitters. For instance, if there are \"N\" time slots in a TDMA system and 2\"N\" users that talk half of the time, then half of the time there will be more than \"N\" users needing to use more than \"N\" time slots. Furthermore, it would require significant overhead to continually allocate and deallocate the orthogonal-code, time-slot or frequency-channel resources. By comparison, asynchronous CDMA transmitters simply send when they have something to say and go off the air when they do not, keeping the same signature sequence as long as they are connected to the system.\nSpread-spectrum characteristics of CDMA.\nMost modulation schemes try to minimize the bandwidth of this signal since bandwidth is a limited resource. However, spread-spectrum techniques use a transmission bandwidth that is several orders of magnitude greater than the minimum required signal bandwidth. One of the initial reasons for doing this was military applications including guidance and communication systems. These systems were designed using spread spectrum because of its security and resistance to jamming. Asynchronous CDMA has some level of privacy built in because the signal is spread using a pseudo-random code; this code makes the spread-spectrum signals appear random or have noise-like properties. A receiver cannot demodulate this transmission without knowledge of the pseudo-random sequence used to encode the data. CDMA is also resistant to jamming. A jamming signal only has a finite amount of power available to jam the signal. The jammer can either spread its energy over the entire bandwidth of the signal or jam only part of the entire signal.\nCDMA can also effectively reject narrow-band interference. Since narrow-band interference affects only a small portion of the spread-spectrum signal, it can easily be removed through notch filtering without much loss of information. Convolution encoding and interleaving can be used to assist in recovering this lost data. CDMA signals are also resistant to multipath fading. Since the spread-spectrum signal occupies a large bandwidth, only a small portion of this will undergo fading due to multipath at any given time. Like the narrow-band interference, this will result in only a small loss of data and can be overcome.\nAnother reason CDMA is resistant to multipath interference is because the delayed versions of the transmitted pseudo-random codes will have poor correlation with the original pseudo-random code, and will thus appear as another user, which is ignored at the receiver. In other words, as long as the multipath channel induces at least one chip of delay, the multipath signals will arrive at the receiver such that they are shifted in time by at least one chip from the intended signal. The correlation properties of the pseudo-random codes are such that this slight delay causes the multipath to appear uncorrelated with the intended signal, and it is thus ignored.\nSome CDMA devices use a rake receiver, which exploits multipath delay components to improve the performance of the system. A rake receiver combines the information from several correlators, each one tuned to a different path delay, producing a stronger version of the signal than a simple receiver with a single correlation tuned to the path delay of the strongest signal.\nFrequency reuse is the ability to reuse the same radio channel frequency at other cell sites within a cellular system. In the FDMA and TDMA systems, frequency planning is an important consideration. The frequencies used in different cells must be planned carefully to ensure signals from different cells do not interfere with each other. In a CDMA system, the same frequency can be used in every cell, because channelization is done using the pseudo-random codes. Reusing the same frequency in every cell eliminates the need for frequency planning in a CDMA system; however, planning of the different pseudo-random sequences must be done to ensure that the received signal from one cell does not correlate with the signal from a nearby cell.\nSince adjacent cells use the same frequencies, CDMA systems have the ability to perform soft hand-offs. Soft hand-offs allow the mobile telephone to communicate simultaneously with two or more cells. The best signal quality is selected until the hand-off is complete. This is different from hard hand-offs utilized in other cellular systems. In a hard-hand-off situation, as the mobile telephone approaches a hand-off, signal strength may vary abruptly. In contrast, CDMA systems use the soft hand-off, which is undetectable and provides a more reliable and higher-quality signal.\nCollaborative CDMA.\nA novel collaborative multi-user transmission and detection scheme called collaborative CDMA has been investigated for the uplink that exploits the differences between users' fading channel signatures to increase the user capacity well beyond the spreading length in the MAI-limited environment. The authors show that it is possible to achieve this increase at a low complexity and high bit error rate performance in flat fading channels, which is a major research challenge for overloaded CDMA systems. In this approach, instead of using one sequence per user as in conventional CDMA, the authors group a small number of users to share the same spreading sequence and enable group spreading and despreading operations. The new collaborative multi-user receiver consists of two stages: group multi-user detection (MUD) stage to suppress the MAI between the groups and a low-complexity maximum-likelihood detection stage to recover jointly the co-spread users' data using minimal Euclidean-distance measure and users' channel-gain coefficients. An enhanced CDMA version known as interleave-division multiple access (IDMA) uses the orthogonal interleaving as the only means of user separation in place of signature sequence used in CDMA system."}
{"id": "7144", "revid": "6908984", "url": "https://en.wikipedia.org/wiki?curid=7144", "title": "Internet filter", "text": "An Internet filter is software that restricts or controls the content an Internet user is capable to access, especially when utilized to restrict material delivered over the Internet via the Web, Email, or other means. Such restrictions can be applied at various levels: a government can attempt to apply them nationwide (see Internet censorship), or they can, for example, be applied by an Internet service provider to its clients, by an employer to its personnel, by a school to its students, by a library to its visitors, by a parent to a child's computer, or by an individual user to their own computers. The motive is often to prevent access to content which the computer's owner(s) or other authorities may consider objectionable. When imposed without the consent of the user, content control can be characterised as a form of internet censorship. Some filter software includes time control functions that empowers parents to set the amount of time that child may spend accessing the Internet or playing games or other computer activities.\nTerminology.\nThe term \"content control\" is used on occasion by CNN, \"Playboy\" magazine, the \"San Francisco Chronicle\", and \"The New York Times\". However, several other terms, including \"content filtering software\", \"web content filter\", \"filtering proxy servers\", \"secure web gateways\", \"censorware\", \"content security and control\", \"web filtering software\", \"content-censoring software\", and \"content-blocking software\", are often used. \"Nannyware\" has also been used in both product marketing and by the media. Industry research company Gartner uses \"secure web gateway\" (SWG) to describe the market segment.\nCompanies that make products that selectively block Web sites do not refer to these products as censorware, and prefer terms such as \"Internet filter\" or \"URL Filter\"; in the specialized case of software specifically designed to allow parents to monitor and restrict the access of their children, \"parental control software\" is also used. Some products log all sites that a user accesses and rates them based on content type for reporting to an \"accountability partner\" of the person's choosing, and the term accountability software is used. Internet filters, parental control software, and/or accountability software may also be combined into one product.\nThose critical of such software, however, use the term \"censorware\" freely: consider the Censorware Project, for example. The use of the term \"censorware\" in editorials criticizing makers of such software is widespread and covers many different varieties and applications: Xeni Jardin used the term in a 9 March 2006 editorial in \"The New York Times,\" when discussing the use of American-made filtering software to suppress content in China; in the same month a high school student used the term to discuss the deployment of such software in his school district.\nIn general, outside of editorial pages as described above, traditional newspapers do not use the term \"censorware\" in their reporting, preferring instead to use less overtly controversial terms such as \"content filter\", \"content control\", or \"web filtering\"; \"The New York Times\" and \"The Wall Street Journal\" both appear to follow this practice. On the other hand, Web-based newspapers such as CNET use the term in both editorial and journalistic contexts, for example \"Windows Live to Get Censorware.\"\nTypes of filtering.\nFilters can be implemented in many different ways: by software on a personal computer, via network infrastructure such as proxy servers, DNS servers, or firewalls that provide Internet access. No solution provides complete coverage, so most companies deploy a mix of technologies to achieve the proper content control in line with their policies.\nReasons for filtering.\nThe Internet does not intrinsically provide content blocking, and therefore there is much content on the Internet that is considered unsuitable for children, given that much content is given certifications as suitable for adults only, e.g. 18-rated games and movies.\nInternet service providers (ISPs) that block material containing pornography, or controversial religious, political, or news-related content en route are often utilized by parents who do not permit their children to access content not conforming to their personal beliefs. Content filtering software can, however, also be used to block malware and other content that is or contains hostile, intrusive, or annoying material including adware, spam, computer viruses, worms, trojan horses, and spyware.\nMost content control software is marketed to organizations or parents. It is, however, also marketed on occasion to facilitate self-censorship, for example by people struggling with addictions to online pornography, gambling, chat rooms, etc. Self-censorship software may also be utilised by some in order to avoid viewing content they consider immoral, inappropriate, or simply distracting. A number of accountability software products are marketed as \"self-censorship\" or \"accountability software\". These are often promoted by religious media and at religious gatherings.\nCriticism.\nFiltering errors.\nOverblocking.\nUtilizing a filter that is overly zealous at filtering content, or mislabels content not intended to be censored can result in over-blocking, or over-censoring. Overblocking can filter out material that should be acceptable under the filtering policy in effect, for example health related information may unintentionally be filtered along with porn-related material because of the Scunthorpe problem. Filter administrators may prefer to err on the side of caution by accepting over blocking to prevent any risk of access to sites that they determine to be undesirable. Content-control software was mentioned as blocking access to Beaver College before its name change to Arcadia University. Another example was the filtering of Horniman Museum. As well, over-blocking may encourage users to bypass the filter entirely.\nUnderblocking.\nWhenever new information is uploaded to the Internet, filters can under block, or under-censor, content if the parties responsible for maintaining the filters do not update them quickly and accurately, and a blacklisting rather than a whitelisting filtering policy is in place.\nMorality and opinion.\nMany would not be satisfied with government filtering viewpoints on moral or political issues, agreeing that this could become support for propaganda. Many would also find it unacceptable that an ISP, whether by law or by the ISP's own choice, should deploy such software without allowing the users to disable the filtering for their own connections. In the United States, the First Amendment to the United States Constitution has been cited in calls to criminalise forced internet censorship. (See section below)\nLegal actions.\nIn 1998, a United States federal district court in Virginia ruled (Loudoun v. Board of Trustees of the Loudoun County Library) that the imposition of mandatory filtering in a public library violates the First Amendment.\nIn 1996 the US Congress passed the Communications Decency Act, banning indecency on the Internet. Civil liberties groups challenged the law under the First Amendment, and in 1997 the Supreme Court ruled in their favor. Part of the civil liberties argument, especially from groups like the Electronic Frontier Foundation, was that parents who wanted to block sites could use their own content-filtering software, making government involvement unnecessary.\nIn the late 1990s, groups such as the Censorware Project began reverse-engineering the content-control software and decrypting the blacklists to determine what kind of sites the software blocked. This led to legal action alleging violation of the \"Cyber Patrol\" license agreement. They discovered that such tools routinely blocked unobjectionable sites while also failing to block intended targets.\nSome content-control software companies responded by claiming that their filtering criteria were backed by intensive manual checking. The companies' opponents argued, on the other hand, that performing the necessary checking would require resources greater than the companies possessed and that therefore their claims were not valid.\nThe Motion Picture Association successfully obtained a UK ruling enforcing ISPs to use content-control software to prevent copyright infringement by their subscribers.\nReligious, anti-religious, and political censorship.\nMany types of content-control software have been shown to block sites based on the religious and political leanings of the company owners. Examples include blocking several religious sites (including the Web site of the Vatican), many political sites, and homosexuality-related sites. \"X-Stop\" was shown to block sites such as the Quaker web site, the National Journal of Sexual Orientation Law, The Heritage Foundation, and parts of The Ethical Spectacle. CYBERsitter blocks out sites like National Organization for Women. Nancy Willard, an academic researcher and attorney, pointed out that many U.S. public schools and libraries use the same filtering software that many Christian organizations use. Cyber Patrol, a product developed by The Anti-Defamation League and Mattel's The Learning Company, has been found to block not only political sites it deems to be engaging in 'hate speech' but also human rights web sites, such as Amnesty International's web page about Israel and gay-rights web sites, such as glaad.org.\nContent labeling.\nContent labeling may be considered another form of content-control software. In 1994, the Internet Content Rating Association (ICRA) \u2014 now part of the Family Online Safety Institute \u2014 developed a content rating system for online content providers. Using an online questionnaire a webmaster describes the nature of their web content. A small file is generated that contains a condensed, computer readable digest of this description that can then be used by content filtering software to block or allow that site.\nICRA labels come in a variety of formats. These include the World Wide Web Consortium's Resource Description Framework (RDF) as well as Platform for Internet Content Selection (PICS) labels used by Microsoft's Internet Explorer Content Advisor.\nICRA labels are an example of self-labeling. Similarly, in 2006 the Association of Sites Advocating Child Protection (ASACP) initiated the Restricted to Adults self-labeling initiative. ASACP members were concerned that various forms of legislation being proposed in the United States were going to have the effect of forcing adult companies to label their content. The RTA label, unlike ICRA labels, does not require a webmaster to fill out a questionnaire or sign up to use. Like ICRA the RTA label is free. Both labels are recognized by a wide variety of content-control software.\nThe Voluntary Content Rating (VCR) system was devised by Solid Oak Software for their CYBERsitter filtering software, as an alternative to the PICS system, which some critics deemed too complex. It employs HTML metadata tags embedded within web page documents to specify the type of content contained in the document. Only two levels are specified, \"mature\" and \"adult\", making the specification extremely simple.\nUse in public libraries.\nAustralia.\nThe Australian Internet Safety Advisory Body has information about \"practical advice on Internet safety, parental control and filters for the protection of children, students and families\" that also includes public libraries.\nNetAlert, the software made available free of charge by the Australian government, was allegedly cracked by a 16-year-old student, Tom Wood, less than a week after its release in August 2007. Wood supposedly bypassed the $84 million filter in about half an hour to highlight problems with the government's approach to Internet content filtering.\nThe Australian Government has introduced legislation that requires ISPs to \"restrict access to age restricted content (commercial MA15+ content and R18+ content) either hosted in Australia or provided from Australia\" that was due to commence from 20 January 2008, known as Cleanfeed.\nCleanfeed is a proposed mandatory ISP level content filtration system. It was proposed by the Beazley led Australian Labor Party opposition in a 2006 press release, with the intention of protecting children who were vulnerable due to claimed parental computer illiteracy. It was announced on 31 December 2007 as a policy to be implemented by the Rudd ALP government, and initial tests in Tasmania have produced a 2008 report. Cleanfeed is funded in the current budget, and is moving towards an Expression of Interest for live testing with ISPs in 2008. Public opposition and criticism have emerged, led by the EFA and gaining irregular mainstream media attention, with a majority of Australians reportedly \"strongly against\" its implementation. Criticisms include its expense, inaccuracy (it will be impossible to ensure only illegal sites are blocked) and the fact that it will be compulsory, which can be seen as an intrusion on free speech rights. Another major criticism point has been that although the filter is claimed to stop certain materials, the underground rings dealing in such materials will not be affected. The filter might also provide a false sense of security for parents, who might supervise children less while using the Internet, achieving the exact opposite effect. Cleanfeed is a responsibility of Senator Conroy's portfolio.\nDenmark.\nIn Denmark it is stated policy that it will \"prevent inappropriate Internet sites from being accessed from children's libraries across Denmark\". \"'It is important that every library in the country has the opportunity to protect children against pornographic material when they are using library computers. It is a main priority for me as Culture Minister to make sure children can surf the net safely at libraries,' states Brian Mikkelsen in a press-release of the Danish Ministry of Culture.\"\nUnited States.\nThe use of Internet filters or content-control software varies widely in public libraries in the United States, since Internet use policies are established by the local library board. Many libraries adopted Internet filters after Congress conditioned the receipt of universal service discounts on the use of Internet filters through the Children's Internet Protection Act (CIPA). Other libraries do not install content control software, believing that acceptable use policies and educational efforts address the issue of children accessing age-inappropriate content while preserving adult users' right to freely access information. Some libraries use Internet filters on computers used by children only. Some libraries that employ content-control software allow the software to be deactivated on a case-by-case basis on application to a librarian; libraries that are subject to CIPA are required to have a policy that allows adults to request that the filter be disabled without having to explain the reason for their request.\nMany legal scholars believe that a number of legal cases, in particular \"Reno v. American Civil Liberties Union\", established that the use of content-control software in libraries is a violation of the First Amendment. The Children's Internet Protection Act [CIPA] and the June 2003 case \"United States v. American Library Association\" found CIPA constitutional as a condition placed on the receipt of federal funding, stating that First Amendment concerns were dispelled by the law's provision that allowed adult library users to have the filtering software disabled, without having to explain the reasons for their request. The plurality decision left open a future \"as-applied\" Constitutional challenge, however.\nIn November 2006, a lawsuit was filed against the North Central Regional Library District (NCRL) in Washington State for its policy of refusing to disable restrictions upon requests of adult patrons, but CIPA was not challenged in that matter. In May 2010, the Washington State Supreme Court provided an opinion after it was asked to certify a question referred by the United States District Court for the Eastern District of Washington: \"Whether a public library, consistent with Article I, \u00a7 5 of the Washington Constitution, may filter Internet access for all patrons without disabling Web sites containing constitutionally-protected speech upon the request of an adult library patron.\" The Washington State Supreme Court ruled that NCRL's internet filtering policy did not violate Article I, Section 5 of the Washington State Constitution. The Court said: \"It appears to us that NCRL's filtering policy is reasonable and accords with its mission and these policies and is viewpoint neutral. It appears that no article I, section 5 content-based violation exists in this case. NCRL's essential mission is to promote reading and lifelong learning. As NCRL maintains, it is reasonable to impose restrictions on Internet access in order to maintain an environment that is conducive to study and contemplative thought.\" The case returned to federal court.\nIn March 2007, Virginia passed a law similar to CIPA that requires public libraries receiving state funds to use content-control software. Like CIPA, the law requires libraries to disable filters for an adult library user when requested to do so by the user.\nBypassing filters.\nContent filtering in general can \"be bypassed entirely by tech-savvy individuals.\" Blocking content on a device \"[will not]\u2026guarantee that users won't eventually be able to find a way around the filter.\" Content providers may change URLs or IP addresses to circumvent filtering. Individuals with technical expertise may use a different method by employing multiple domains or URLs that direct to a shared IP address where restricted content is present. This strategy doesn't circumvent IP packet filtering, however can evade DNS poisoning and web proxies. Additionally, perpetrators may use mirrored websites that avoid filters.\nSome software may be bypassed successfully by using alternative protocols such as FTP or telnet or HTTPS, conducting searches in a different language, using a proxy server or a circumventor such as Psiphon. Also cached web pages returned by Google or other searches could bypass some controls as well. Web syndication services may provide alternate paths for content. Some of the more poorly designed programs can be shut down by killing their processes: for example, in Microsoft Windows through the Windows Task Manager, or in Mac OS X using Force Quit or Activity Monitor. Numerous workarounds and counters to workarounds from content-control software creators exist. Google services are often blocked by filters, but these may most often be bypassed by using \"https://\" in place of \"http://\" since content filtering software is not able to interpret content under secure connections (in this case SSL).\nAn encrypted VPN can be used as means of bypassing content control software, especially if the content control software is installed on an Internet gateway or firewall. Other ways to bypass a content control filter include translation sites and establishing a remote connection with an uncensored device.\nProducts and services.\nSome ISPs offer parental control options. Some offer security software which includes parental controls. Mac OS X v10.4 offers parental controls for several applications (Mail, Finder, iChat, Safari &amp; Dictionary). Microsoft's Windows Vista operating system also includes content-control software.\nContent filtering technology exists in two major forms: application gateway or packet inspection. For HTTP access the application gateway is called a web-proxy or just a proxy. Such web-proxies can inspect both the initial request and the returned web page using arbitrarily complex rules and will not return any part of the page to the requester until a decision is made. In addition they can make substitutions in whole or for any part of the returned result. Packet inspection filters do not initially interfere with the connection to the server but inspect the data in the connection as it goes past, at some point the filter may decide that the connection is to be filtered and it will then disconnect it by injecting a TCP-Reset or similar faked packet. The two techniques can be used together with the packet filter monitoring a link until it sees an HTTP connection starting to an IP address that has content that needs filtering. The packet filter then redirects the connection to the web-proxy which can perform detailed filtering on the website without having to pass through all unfiltered connections. This combination is quite popular because it can significantly reduce the cost of the system.\nThere are constraints to IP level packet-filtering, as it may result in rendering all web content associated with a particular IP address inaccessible. This may result in the unintentional blocking of legitimate sites that share the same IP address or domain. For instance, university websites commonly employ multiple domains under one IP address. Moreover, IP level packet-filtering can be surpassed by using a distinct IP address for certain content while still being linked to the same domain or server.\nGateway-based content control software may be more difficult to bypass than desktop software as the user does not have physical access to the filtering device. However, many of the techniques in the Bypassing filters section still work."}
{"id": "7145", "revid": "45070860", "url": "https://en.wikipedia.org/wiki?curid=7145", "title": "Chambered cairn", "text": "A chambered cairn is a burial monument, usually constructed during the Neolithic, consisting of a sizeable (usually stone) chamber around and over which a cairn of stones was constructed. Some chambered cairns are also passage-graves. They are found throughout Britain and Ireland, with the largest number in Scotland.\nTypically, the chamber is larger than a cist, and will contain a larger number of interments, which are either excarnated bones or inhumations (cremations). Most were situated near a settlement, and served as that community's \"graveyard\".\nScotland.\nBackground.\nDuring the early Neolithic (4000\u20133300 BC) architectural forms are highly regionalised with timber and earth monuments predominating in the east and stone-chambered cairns in the west. During the later Neolithic (3300\u20132500 BC) massive circular enclosures and the use of grooved ware and Unstan ware pottery emerge. Scotland has a particularly large number of chambered cairns; they are found in various different types described below. Along with the excavations of settlements such as Skara Brae, Links of Noltland, Barnhouse, Rinyo and Balfarg and the complex site at Ness of Brodgar these cairns provide important clues to the character of civilization in Scotland in the Neolithic. However the increasing use of cropmarks to identify Neolithic sites in lowland areas has tended to diminish the relative prominence of these cairns.\nIn the early phases bones of numerous bodies are often found together and it has been argued that this suggests that in death at least, the status of individuals was played down. During the late Neolithic henge sites were constructed and single burials began to become more commonplace; by the Bronze Age it is possible that even where chambered cairns were still being built they had become the burial places of prominent individuals rather than of communities as a whole.\nClyde-Carlingford court cairns.\nThe Clyde or Clyde-Carlingford type are principally found in northern and western Ireland and southwestern Scotland. They first were identified as a separate group in the Firth of Clyde region, hence the name. Over 100 have been identified in Scotland alone. Lacking a significant passage, they are a form of gallery grave. The burial chamber is normally located at one end of a rectangular or trapezoidal cairn, while a roofless, semi-circular forecourt at the entrance provided access from the outside (although the entrance itself was often blocked), and gives this type of chambered cairn its alternate name of court tomb or court cairn. These forecourts are typically fronted by large stones and it is thought the area in front of the cairn was used for public rituals of some kind. The chambers were created from large stones set on end, roofed with large flat stones and often sub-divided by slabs into small compartments. They are generally considered to be the earliest in Scotland.\nExamples include Cairn Holy I and Cairn Holy II near Newton Stewart, a cairn at Port Charlotte, Islay, which dates to 3900\u20134000 BC, and Monamore, or Meallach's Grave, Arran, which may date from the early fifth millennium BC. Excavations at the Mid Gleniron cairns near Cairnholy revealed a multi-period construction which shed light on the development of this class of chambered cairn.\nOrkney-Cromarty.\nThe Orkney-Cromarty group is by far the largest and most diverse. It has been subdivided into Yarrows, Camster and Cromarty subtypes but the differences are extremely subtle. The design is of dividing slabs at either side of a rectangular chamber, separating it into compartments or stalls. The number of these compartments ranges from 4 in the earliest examples to over 24 in an extreme example on Orkney. The actual shape of the cairn varies from simple circular designs to elaborate 'forecourts' protruding from each end, creating what look like small amphitheatres. It is likely that these are the result of cultural influences from mainland Europe, as they are similar to designs found in France and Spain.\nExamples include Midhowe on Rousay, and both the Unstan Chambered Cairn and Wideford Hill chambered cairn from the Orkney Mainland, both of which date from the mid 4th millennium BC and were probably in use over long periods of time. When the latter was excavated in 1884, grave goods were found that gave their name to Unstan ware pottery. Blackhammer cairn on Rousay is another example dating from the 3rd millennium BC.\nThe Grey Cairns of Camster in Caithness are examples of this type from mainland Scotland. The Tomb of the Eagles on South Ronaldsay is a stalled cairn that shows some similarities with the later Maeshowe type. It was in use for 800 years or more and numerous bird bones were found here, predominantly white-tailed sea eagle.\nMaeshowe.\nThe Maeshowe group, named after the famous Orkney monument, is among the most elaborate. They appear relatively late and only in Orkney and it is not clear why the use of cairns continued in the north when their construction had largely ceased elsewhere in Scotland. They consist of a central chamber from which lead small compartments, into which burials would be placed. The central chambers are tall and steep-sided and have corbelled roofing faced with high quality stone.\nIn addition to Maeshowe itself, which was constructed c. 2700 BC, there are various other examples from the Orkney Mainland. These include Quanterness chambered cairn (3250 BC) in which the remains of 157 individuals were found when excavated in the 1970s, Cuween Hill near Finstown which was found to contain the bones of men, dogs and oxen and Wideford Hill chambered cairn, which dates from 2000 BC.\nExamples from elsewhere in Orkney are the Vinquoy chambered cairn, and the Huntersquoy chambered cairn, both found on the north end of the island of Eday and Quoyness on Sanday constructed about 2900 BC and which is surrounded by an arc of Bronze Age mounds. The central chamber of Holm of Papa Westray South cairn is over 20 metres long.\nBookan.\nThe Bookan type is named after a cairn found to the north-west of the Ring of Brodgar in Orkney, which is now a dilapidated oval mound, about 16 metres in diameter. Excavations in 1861 indicated a rectangular central chamber surrounded by five smaller chambers. Because of the structure's unusual design, it was originally presumed to be an early form. However, later interpretations and further excavation work in 2002 suggested that they have more in common with the later Maeshowe type rather than the stalled Orkney-Cromarty cairns.\nHuntersquoy chambered cairn on Eday is a double storied Orkney\u2013Cromarty type cairn with a Booken-type lower chamber.\nShetland.\nThe Shetland or Zetland group are relatively small passage graves, that are round or heel-shaped in outline. The whole chamber is cross or trefoil-shaped and there are no smaller individual compartments. An example is to be found on the uninhabited island of Vementry on the north side of the West Mainland, where it appears that the cairn may have originally been circular and its distinctive heel shape added as a secondary development, a process repeated elsewhere in Shetland. This probably served to make the cairn more distinctive and the forecourt area more defined.\nHebridean.\nLike the Shetland cairn the Hebridean group appear relatively late in the Neolithic. They are largely found in the Outer Hebrides, although a mixture of cairn types are found here. These passage graves are usually larger than the Shetland type and are round or have funnel-shaped forecourts, although a few are long cairns \u2013 perhaps originally circular but with later tails added. They often have a polygonal chamber and a short passage to one end of the cairn.\nThe Rubha an D\u00f9nain peninsula on the island of Skye provides an example from the 2nd or 3rd millennium BC. Barpa Langass on North Uist is the best preserved chambered cairn in the Hebrides.\nBargrennan.\nBargrennan chambered cairns are a class of passage graves found only in south-west Scotland, in western Dumfries and Galloway and southern Ayrshire. As well as being structurally different from the nearby Clyde cairns, Bargrennan cairns are distinguished by their siting and distribution; they are found in upland, inland areas of Galloway and Ayrshire.\nBronze Age.\nIn addition to the increasing prominence of individual burials, during the Bronze Age regional differences in architecture in Scotland became more pronounced. The Clava cairns date from this period, with about 50 cairns of this type in the Inverness area. Corrimony chambered cairn near Drumnadrochit is an example dated to 2000 BC or older. The only surviving evidence of burial was a stain indicating the presence of a single body. The cairn is surrounded by a circle of 11 standing stones. The cairns at Balnuaran of Clava are of a similar date. The largest of three is the north-east cairn, which was partially reconstructed in the 19th century and the central cairn may have been used as a funeral pyre.\nGlebe cairn in Kilmartin Glen in Argyll dates from 1700 BC and has two stone cists inside one of which a jet necklace was found during 19th century excavations. There are numerous prehistoric sites in the vicinity including Nether Largie North cairn, which was entirely removed and rebuilt during excavations in 1930.\nWales.\nChambered long cairns.\nThere are 18 Scheduled Ancient Monuments listed:"}
{"id": "7146", "revid": "19382112", "url": "https://en.wikipedia.org/wiki?curid=7146", "title": "Currency code", "text": ""}
{"id": "7147", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=7147", "title": "Canadian whisky", "text": "Canadian whisky is a type of whisky produced in Canada. Most Canadian whiskies are blended multi-grain liquors containing a large percentage of corn spirits, and are typically lighter and smoother than other whisky styles. When Canadian distillers began adding small amounts of highly-flavourful rye grain to their mashes, people began demanding this new rye-flavoured whisky, referring to it simply as \"rye\". Today, as for the past two centuries, the terms \"rye whisky\" and \"Canadian whisky\" are used interchangeably in Canada and (as defined in Canadian law) refer to exactly the same product, which generally is made with only a small amount of rye grain.\nCharacteristics.\nHistorically, in Canada, corn-based whisky that had some rye grain added to the mash bill to give it more flavour came to be called \"rye\".\nThe regulations under Canada's \"Food and Drugs Act\" stipulate the minimum conditions that must be met in order to label a product as \"Canadian Whisky\" or \"Canadian Rye Whisky\" (or \"Rye Whisky\")\u2014these are also upheld internationally through geographical indication agreements. These regulations state that whisky must \"be mashed, distilled and aged in Canada\", \"be aged in small wood vessels for not less than three years\", \"contain not less than 40 per cent alcohol by volume\" and \"may contain caramel and flavouring\". Within these parameters Canadian whiskies can vary considerably, especially with the allowance of \"flavouring\"\u2014though the additional requirement that they \"possess the aroma, taste and character generally attributed to Canadian whisky\" can act as a limiting factor.\nCanadian whiskies are most typically blends of whiskies made from a single grain, principally corn and rye, but also sometimes wheat or barley. Mash bills of multiple grains may also be used for some flavouring whiskies. The availability of inexpensive American corn, with its higher proportion of usable starches relative to other cereal grains, has led it to be most typically used to create base whiskies to which flavouring whiskies are blended in. Exceptions to this include the Highwood Distillery which specializes in using wheat and the Alberta Distillers which developed its own proprietary yeast strain that specializes in distilling rye. The flavouring whiskies are most typically rye whiskies, blended into the product to add most of its flavour and aroma. While Canadian whisky may be labelled as a \"rye whisky\" this blending technique only necessitates a small percentage (such as 10%) of rye to create the flavour, whereas much more rye would be required if it were added to a mash bill alongside the more readily distilled corn.\nThe base whiskies are distilled to between 180 and 190 proof which results in few congener by-products (such as fusel alcohol, aldehydes, esters, etc.) and creates a lighter taste. By comparison, an American whisky distilled any higher than 160 proof is labelled as \"light whiskey\". The flavouring whiskies are distilled to a lower proof so that they retain more of the grain's flavour. The relative lightness created by the use of base whiskies makes Canadian whisky useful for mixing into cocktails and highballs. The minimum three year aging in small wood barrels applies to all whiskies used in the blend. As the regulations do not limit the specific type of wood that must be used, a variety of flavours can be achieved by blending whiskies aged in different types of barrels. In addition to new wood barrels, charred or uncharred, flavour can be added by aging whiskies in previously used bourbon or fortified wine barrels for different lengths of time.\nHistory.\nIn the 18th and early 19th centuries, gristmills distilled surplus grains to avoid spoilage. Most of these early whiskies would have been rough, mostly unaged wheat whiskey. Distilling methods and technologies were brought to Canada by American and European immigrants with experience in distilling wheat and rye. This early whisky from improvised stills, often with the grains closest to spoilage, was produced with various, uncontrolled proofs and was consumed, unaged, by the local market. While most distilling capacity was taken up producing rum, a result of Atlantic Canada's position in the British sugar trade, the first commercial scale production of whisky in Canada began in 1801 when John Molson purchased a copper pot still, previously used to produce rum, in Montreal. With his son Thomas Molson, and eventually partner James Morton, the Molsons operated a distillery in Montreal and Kingston and were the first in Canada to export whisky, benefiting from Napoleonic Wars' disruption in supplying French wine and brandies to England. \nGooderham and Worts began producing whisky in 1837 in Toronto as a side business to their wheat milling but surpassed Molson's production by the 1850s as it expanded their operations with a new distillery in what would become the Distillery District. Henry Corby started distilling whisky as a side business from his gristmill in 1859 in what became known as Corbyville and Joseph Seagram began working in his father-in-law's Waterloo flour mill and distillery in 1864, which he would eventually purchase in 1883. Meanwhile, Americans Hiram Walker and J.P. Wiser moved to Canada: Walker to Windsor in 1858 to open a flour mill and distillery and Wiser to Prescott in 1857 to work at his uncle's distillery where he introduced a rye whisky and was successful enough to buy the distillery five years later. The disruption of American Civil War created an export opportunity for Canadian-made whiskies and their quality, particularly those from Walker and Wiser who had already begun the practice of aging their whiskies, sustained that market even after post-war tariffs were introduced. In the 1880s, Canada's National Policy placed high tariffs on foreign alcoholic products as whisky began to be sold in bottles and the federal government instituted a bottled in bond program that provided certification of the time a whisky spent aging and allowed deferral of taxes for that period, which encouraged aging. In 1890 Canada became the first country to enact an aging law for whiskies, requiring them to be aged at least two years. The growing temperance movement culminated in prohibition in 1916 and distilleries had to either specialize in the export market or switch to alternative products, like industrial alcohols which were in demand in support of the war effort.\nWith the deferred revenue and storage costs of the Aging Law acting as a barrier to new entrants and the reduced market due to prohibition, consolidation of Canadian whisky had begun. Henry Corby Jr. modernized and expanded upon his father's distillery and sold it, in 1905, to businessman Mortimer Davis who also purchased the Wiser distillery, in 1918, from the heirs of J.P. Wiser. Davis's salesman Harry Hatch spent time promoting the Corby and Wiser brands and developing a distribution network in the United States which held together as Canadian prohibition ended and American prohibition began. After Hatch's falling out with Davis, Hatch purchased the struggling Gooderham and Worts in 1923 and switched out Davis's whisky for his. Hatch was successful enough to be able to also purchase the Walker distillery, and the popular Canadian Club brand, from Hiram's grandsons in 1926. While American prohibition created risk and instability in the Canadian whisky industry, some benefited from purchasing unused American distillation equipment and from sales to exporters (nominally to foreign countries like Saint Pierre and Miquelon, though actually to bootleggers to the United States). Along with Hatch, the Bronfman family was able to profit from making whisky destined for United States during prohibition, though mostly in Western Canada and were able to open a distillery in LaSalle, Quebec and merge their company, in 1928, with Seagram's which had struggled with transitioning to the prohibition marketplace. Samuel Bronfman became president of the company and, with his dominant personality, began a strategy of increasing their capacity and aging whiskies in anticipation of the end of prohibition. When that did occur, in 1933, Seagram's was in a position to quickly expand; they purchased The British Columbia Distilling Company from the Riefel family in 1935, as well as several American distilleries and introduced new brands, one of them being Crown Royal, in 1939, which would eventually become one of the best-selling Canadian whiskies.\nWhile some capacity was switched to producing industrial alcohols in support of the country's World War II efforts, the industry expanded again after the war until the 1980s. In 1945, Schenley Industries purchased one of those industrial alcohol distilleries in Valleyfield, Quebec, and repurposed several defunct American whiskey brands, like Golden Wedding, Old Fine Copper, and starting in 1972, Gibson's Finest. Seeking to secure their supply of Canadian whisky, Barton Brands also built a new distillery in Collingwood, Ontario, in 1967, where they would produce Canadian Mist, though they sold the distillery and brand only four years later to Brown\u2013Forman. As proximity to the shipping routes (by rail and boat) to the US became less important, large distilleries were established in Alberta and Manitoba. Five years after starting to experiment with whiskies in their Toronto gin distillery, W. &amp; A. Gilbey Ltd. created the Black Velvet blend in 1951 which was so successful a new distillery in Lethbridge, Alberta was constructed in 1973 to produce it. \nAlso in the west, a Calgary-based business group recruited the Riefels from British Columbia to oversee their Alberta Distillers operations in 1948. The company became an innovator in the practice of bulk shipping whiskies to the United States for bottling and the success of their Windsor Canadian brand (produced in Alberta but bottled in the United States) led National Distillers Limited to purchase Alberta Distillers, in 1964, to secure their supply chain. More Alberta investors founded the Highwood Distillery in 1974 in High River, Alberta, which specialized in wheat-based whiskies. Seagram's opened a large, new plant in Gimli, Manitoba, in 1969, which would eventually replace their Waterloo and LaSalle distilleries. In British Columbia, Ernie Potter who had been producing fruit liqueurs from alcohols distilled at Alberta Distillers built his own whisky distillery in Langley in 1958 and produced the Potter's and Century brands of whisky. Hiram Walker's built the Okanagan Distillery in Winfield, British Columbia, in 1970 with the intention of producing Canadian Club but was redirected to fulfill contracts to produce whiskies for Suntory before being closed in 1995.\nAfter decades of expansion, a shift in consumer preferences towards white spirits (such as vodka) in the American market resulted in an excess supply of Canadian whiskies. While this allowed the whiskies to be aged longer, the unexpected storage costs and deferred revenue strained individual companies. With the distillers seeking investors and multinational corporations seeking value brands, a series of acquisitions and mergers occurred. Alberta Distillers was bought in 1987 by Fortune Brands which would go on to become part of Suntory Global Spirits. Hiram Walker was sold in 1987 to Allied Lyons which Pernod Ricard took over in 2006, with Fortune Brands acquiring the Canadian Club brand. Grand Metropolitan had purchased Black Velvet in 1972 but sold the brand in 1999 to Constellation Brands who in turn sold it to Heaven Hill in 2019. Schenley was acquired in 1990 by United Distillers which would go on to become part of Diageo, though Gibson's Finest was sold to William Grant &amp; Sons in 2001. Seagram's was sold in 2000 to Vivendi, which in turn sold its various brands and distilleries to Pernod Ricard and Diageo. Highwood would purchase Potter's in 2006. Despite the consolidation, the Kittling Ridge Distillery in Grimsby, Ontario, began to produce the Forty Creek brand, though it was sold to the Campari Group in 2014. Later, the Sazerac Company would purchase the brands Seagram's VO, Canadian 83 and Five Star from Diageo in 2018.\nIllicit export to the United States.\nCanadian whisky featured prominently in rum-running into the U.S. during Prohibition. Hiram Walker's distillery in Windsor, Ontario, directly across the Detroit River and the international boundary between Canada and the United States, easily served bootleggers using small, fast smuggling boats.\nDistilleries and brands.\nThe following is a listing of distilleries presently producing Canadian whiskys:\nAlberta.\nThere are several distilleries based in Alberta, including the Alberta Distillers, established in 1946 in Calgary, Alberta. The distillery was purchased in 1987 by Fortune Brands which became Beam Suntory in 2011 and Suntory Global Spirits in 2024. The distillery uses a specific strain of yeast which they developed that specializes in fermenting rye. While the distillery exports much of its whisky for bottling in other countries, they also produce the brands Alberta Premium, Alberta Springs, Windsor Canadian, Tangle Ridge, and Canadian Club Chairman's Select.\nBlack Velvet Distillery (formerly the Palliser Distillery) was established in 1973 in Lethbridge, Alberta, and has been owned by Heaven Hill since 2019. It produces the Black Velvet brand, mostly shipped in bulk for bottling in America, with some bottled onsite for Canadians. It also makes Danfield's and the Schenley's Golden Wedding and OFC labels.\nHighwood Distillery (formerly the Sunnyvale Distillery) was established in 1974 in High River, Alberta, the Highwood Distillery specializes in using wheat in their base whiskies. This distillery also produces vodka, rum, gin and liqueurs. Brands of Canadian whisky produced at the Highwood Distillery include Centennial, Century, Ninety, and Potter's. They also produce White Owl whisky which is charcoal-filtered to remove the colouring introduced by aging in wood barrels.\nManitoba.\nGimli Distillery was established in 1968 in Gimli, Manitoba, to produce Seagram brands, the distillery was acquired by Diageo in 2001. The Gimli Distillery is responsible for producing Crown Royal, the best-selling Canadian whisky in the world with 7 million cases shipped in 2017. They also supply some of the whisky used in Seagram's VO and other blends.\nOntario.\nDistilleries were established in Ontario during the mid-19th century, with Gooderham and Worts's beginning operations in Toronto's Distillery District in the 1830s. Distilleries continued to operate from the Distillery District until 1990, when the area was reoriented towards commercial and residential development. Other former distilleries in the province includes one in Corbyville, which hosted a distillery operated by Corby Spirit and Wine. A distillery in Waterloo was operated by Seagram to produce Crown Royal until 1992; although the company still maintains a blending and bottling plant in Amherstburg.\nPresently, there are several major distilleries based in Ontario. The oldest functioning distillery in Ontario is the Hiram Walker Distillery, established in 1858 in Windsor, Ontario, but modernized and expanded upon several times since. The distillery is owned by Pernod Ricard and operated by Corby Spirit and Wine, of which Pernod has a controlling share. Brands produced at the Walker Distillery include Lot 40, Pike Creek, Gooderham and Worts, Hiram Walker's Special Old, Corby's Royal Reserve, and J.P. Wiser's brands. Most of its capacity is used for contract production of the Suntory Global Spirits brand (and former Hiram Walker brand) Canadian Club, in addition to generic Canadian whisky that is exported in bulk and bottled under various labels in other countries.\nCanadian Mist Distillery was established in 1967 in Collingwood, Ontario, the distillery is owned by the Sazerac Company and primarily produces the Canadian Mist brand for export. The distillery also produces whiskies used in the Collingwood brand, introduced 2011, and the Bearface brand, introduced 2018.\nKittling Ridge Distillery was established in 1992 with an associated winery in Grimsby, Ontario, its first whiskies came to market in 2002. The distillery was purchased in 2014 by Campari Group. The distillery produces the Forty Creek brand.\nQuebec.\nOld Montreal Distillery was established in 1929 as a Corby Spirit and Wine distillery, it was acquired by Sazerac Company in 2011 and modernized in 2018. It produces Sazerac brands and has taken over bottling of Caribou Crossing.\nValleyfield Distillery (formerly the Schenley Distillery) was established in 1945 in a former brewery in Salaberry-de-Valleyfield, Quebec, near Montreal, the distillery has been owned by Diageo in 2008. Seagram's VO is bottled here with flavouring whisky from the Gimli Distillery. Otherwise, the Valleyfield Distillery specializes in producing base whiskies distilled from corn for other Diageo products."}
{"id": "7148", "revid": "45406993", "url": "https://en.wikipedia.org/wiki?curid=7148", "title": "Collective noun", "text": "In linguistics, a collective noun is a word referring to a collection of things taken as a whole. Most collective nouns in everyday speech are not specific to one kind of thing. For example, the collective noun \"group\" can be applied to people (\"a group of people\"), or dogs (\"a group of dogs\"), or objects (\"a group of stones\").\nSome collective nouns are specific to one kind of thing, especially terms of venery, which identify groups of specific animals. For example, \"pride\" as a term of venery always refers to lions, never to dogs or cows. Other examples come from popular culture such as a group of owls, which is called a \"parliament\".\nDifferent forms of English handle verb agreement with collective count nouns differently. For example, users of British English generally accept that collective nouns take either singular or plural verb forms depending on context and the metonymic shift that it implies, while in some other forms of English the verb agreement is less flexible.\nDerivation.\nMorphological derivation accounts for many collective words and various languages have common affixes for denoting collective nouns. Because derivation is a slower and less productive word formation process than the more overtly syntactical morphological methods, there are fewer collectives formed this way. As with all derived words, derivational collectives often differ semantically from the original words, acquiring new connotations and even new denotations.\nAffixes.\nProto-Indo-European.\nEarly Proto-Indo-European used the suffix *eh\u2082 to form collective nouns, which evolved into the Latin neuter plural ending -a, as in \"datum/data\". Late Proto-Indo-European used the ending *t, which evolved into the English ending -th, as in \"young/youth\".\nEnglish.\nThe English endings \"-age\" and \"-ade\" often signify a collective. Sometimes, the relationship is easily recognizable: \"baggage, drainage, blockade\". Though the etymology is plain to see, the derived words take on a distinct meaning. This is a productive ending, as evidenced in the recent coin, \"signage\".\nGerman.\nGerman uses the prefix \"ge-\" to create collectives. The root word often undergoes umlaut and suffixation as well as receiving the \"ge-\" prefix. Nearly all nouns created in that way are of neuter gender:\nThere are also several endings that can be used to create collectives, such as \"welt\" and \"masse\".\nDutch.\nDutch has a similar pattern but sometimes uses the (unproductive) circumfix \":\nSwedish.\nThe following Swedish example has different words in the collective form and in the individual form:\nEsperanto.\nEsperanto uses the collective infix -\"ar\"- to produce a large number of derived words:\nMetonymic merging of grammatical number.\nTwo examples of collective nouns are \"team\" and \"government\", which are both words referring to groups of (usually) people. Both \"team\" and \"government\" are \"countable\" nouns (consider: \"one team\", \"two teams\", \"most teams\"; \"one government\", \"two governments\", \"many governments\").\nAgreement in different forms of English.\nConfusion often stems from the way that different forms of English handle agreement with collective nouns\u2014specifically, whether or not to use the collective singular: the singular verb form with a collective noun. The plural verb forms are often used in British English with the singular forms of these countable nouns (e.g., \"The team \"have\" finished the project.\"). Conversely, in the English language as a whole, singular verb forms can often be used with nouns ending in \"-s\" that were once considered plural (e.g., \"Physics \"is\" my favorite academic subject\"). This apparent \"number mismatch\" is a natural and logical feature of human language, and its mechanism is a subtle metonymic shift in the concepts underlying the words.\nIn British English, it is generally accepted that collective nouns can take either singular or plural verb forms depending on the context and the metonymic shift that it implies. For example, \"the team \"is\" in the dressing room\" (\"formal agreement\") refers to \"the team\" as an ensemble, while \"the team \"are\" fighting among themselves\" (\"notional agreement\") refers to \"the team\" as individuals. That is also the British English practice with names of countries and cities in sports contexts (e.g., \"Newcastle \"have\" won the competition.\").\nIn American English, collective nouns almost always take singular verb forms (formal agreement). In cases that a metonymic shift would be revealed nearby, the whole sentence should be recast to avoid the metonymy. (For example, \"The team are fighting among themselves\" may become \"the team \"members\" are fighting among themselves\" or simply \"the team is infighting\".) Collective proper nouns are usually taken as singular (\"Apple is expected to release a new phone this year\"), unless the plural is explicit in the proper noun itself, in which case it is taken as plural (\"The Green Bay Packers are scheduled to play the Minnesota Vikings this weekend\"). More explicit examples of collective proper nouns include \"General Motors is once again the world's largest producer of vehicles\", and \"Texas Instruments is a large producer of electronics here\", and \"British Airways is an airline company in Europe\". Furthermore, \"American Telephone &amp; Telegraph is a telecommunications company in North America\". Such phrases might look plural, but they are not.\nExamples of metonymic shift.\nA good example of such a metonymic shift in the singular-to-plural direction (which exclusively takes place in British English) is the following sentence: \"The team have finished the project.\" In that sentence, the underlying thought is of the individual members of the team working together to finish the project. Their accomplishment is collective, and the emphasis is not on their individual identities, but they are still discrete individuals; the word choice \"team have\" manages to convey both their collective and discrete identities simultaneously. Collective nouns that have a singular form but take a plural verb form are called collective plurals. An example of such a metonymic shift in the plural-to-singular direction is the following sentence: \"Mathematics is my favorite academic subject\". The word \"mathematics\" may have originally been plural in concept, referring to mathematic endeavors, but metonymic shift (the shift in concept from \"the endeavors\" to \"the whole set of endeavors\") produced the usage of \"mathematics\" as a singular entity taking singular verb forms. (A true mass-noun sense of \"mathematics\" followed naturally.)\nNominally singular pronouns can be collective nouns taking plural verb forms, according to the same rules that apply to other collective nouns. For example, it is correct usage in both British English and American English usage to say: \"None are so fallible as those who are sure they're right.\" In that case, the plural verb is used because the context for \"none\" suggests more than one thing or person. This also applies to the use of an adjective as a collective noun: \"The British are coming!\"; \"The poor will always be with you.\"\nOther examples include:\nThis does not, however, affect the tense later in the sentence:\nAbbreviations provide other \"exceptions\" in American usage concerning plurals:\nWhen only the name is plural but not the object, place, or person:\nTerms of venery.\nThe tradition of using \"terms of venery\" or \"nouns of assembly\", collective nouns that are specific to certain kinds of animals, stems from an English hunting tradition of the Late Middle Ages. The fashion of a consciously developed hunting language came to England from France. It was marked by an extensive proliferation of specialist vocabulary, applying different names to the same feature in different animals. The elements can be shown to have already been part of French and English hunting terminology by the beginning of the 14th century. In the course of the 14th century, it became a courtly fashion to extend the vocabulary, and by the 15th century, the tendency had reached exaggerated and even satirical proportions. Other synonyms for \"terms of venery\" include \"company nouns\", \"gatherations\", and \"agminals\".\n\"The Treatise\", written by Walter of Bibbesworth in the mid-1200s, is the earliest source for collective nouns of animals in any European vernacular (and also the earliest source for animal noises). The \"Venerie\" of Twiti (early 14th century) distinguished three types of droppings of animals, and three different terms for herds of animals. Gaston Phoebus (14th century) had five terms for droppings of animals, which were extended to seven in the \"Master of the Game\" (early 15th century). The focus on collective terms for groups of animals emerged in the later 15th century. Thus, a list of collective nouns in Egerton MS 1995, dated to under the heading of \"termis of venery &amp;c.\", extends to 70 items, and the list in the \"Book of Saint Albans\" (1486) runs to 164 items, many of which, even though introduced by \"the compaynys of beestys and fowlys\", relate not to venery, but to human groups and professions and are humorous, such as \"a Doctryne of doctoris\", \"a Sentence of Juges\", \"a Fightyng of beggers\", \"an uncredibilite of Cocoldis\", \"a Melody of harpers\", \"a Gagle of women\", \"a Disworship of Scottis\", etc.\nThe \"Book of Saint Albans\" became very popular during the 16th century and was reprinted frequently. Gervase Markham edited and commented on the list in his \"The Gentleman's Academie\", in 1595. The book's popularity had the effect of perpetuating many of these terms as part of the Standard English lexicon even if they were originally meant to be humorous and have long ceased to have any practical application.\nEven in their original context of medieval venery, the terms were of the nature of kennings, intended as a mark of erudition of the gentlemen able to use them correctly rather than for practical communication. The popularity of the terms in the modern period has resulted in the addition of numerous lighthearted, humorous, or facetious collective nouns."}
{"id": "7158", "revid": "30342611", "url": "https://en.wikipedia.org/wiki?curid=7158", "title": "Carat (mass)", "text": "The carat (ct) is a unit of mass equal to , which is used for measuring gemstones and pearls.\nThe current definition, sometimes known as the metric carat, was adopted in 1907 at the Fourth General Conference on Weights and Measures, and soon afterwards in many countries around the world. The carat is divisible into 100 \"points\" of 2\u00a0mg. Other subdivisions, and slightly different mass values, have been used in the past in different locations.\nIn terms of diamonds, a paragon is a flawless stone of at least 100 carats (20\u00a0g).\nThe ANSI X.12 EDI standard abbreviation for the carat is CD.\nEtymology.\nFirst attested in English in the mid-15th century, the word \"carat\" comes from Italian \"carato\", which comes from Arabic (\"q\u012br\u0101\u1e6d\"; \u0642\u064a\u0631\u0627\u0637), in turn borrowed from Greek \"ker\u00e1tion\" \u03ba\u03b5\u03c1\u03ac\u03c4\u03b9\u03bf\u03bd 'carob seed', a diminutive of \"keras\" 'horn'. It was a unit of weight, equal to 1/1728 (1/12) of a pound (see Mina (unit)).\nHistory.\nCarob seeds have been used throughout history to measure jewelry, because it was believed that there was little variance in their mass distribution. However, this was a factual inaccuracy, as their mass varies about as much as seeds of other species.\nIn the past, each country had its own carat. It was often used for weighing gold. Beginning in the 1570s, it was used to measure weights of diamonds.\nStandardization.\nAn 'international carat' of 205 milligrams was proposed in 1871 by the Syndical Chamber of Jewellers, etc., in Paris, and accepted in 1877 by the Syndical Chamber of Diamond Merchants in Paris. A metric carat of 200 milligrams is exactly one-fifth of a gram and had often been suggested in various countries, and was finally proposed by the International Committee of Weights and Measures, and unanimously accepted at the fourth sexennial General Conference of the Metric Convention held in Paris in October 1907. It was soon made compulsory by law in France, but uptake of the new carat was slower in England, where its use was allowed by the Weights and Measures (Metric System) Act of 1897.\nHistorical definitions.\nUK Board of Trade.\nIn the United Kingdom the original Board of Trade carat was exactly grains (~3.170\u00a0grains = ~205\u00a0mg); in 1888, the Board of Trade carat was changed to exactly grains (~3.168\u00a0grains = ~205\u00a0mg). Despite it being a non-metric unit, a number of metric countries have used this unit for its limited range of application.\nThe Board of Trade carat was divisible into four \"diamond grains\", but measurements were typically made in multiples of carat.\nRefiners' carats.\nThere were also two varieties of \"refiners' carats\" once used in the United Kingdom\u2014the pound carat and the ounce carat. The pound troy was divisible into 24 \"pound carats\" of 240 grains troy each; the pound carat was divisible into four \"pound grains\" of 60 grains troy each; and the pound grain was divisible into four \"pound quarters\" of 15 grains troy each. Likewise, the ounce troy was divisible into 24 \"ounce carats\" of 20 grains troy each; the ounce carat was divisible into four \"ounce grains\" of 5 grains troy each; and the ounce grain was divisible into four \"ounce quarters\" of grains troy each.\nGreco-Roman.\nThe \"solidus\" was also a Roman weight unit. There is literary evidence that the weight of 72\u00a0coins of the type called \"solidus\" was exactly 1 Roman pound, and that the weight of 1\u00a0\"solidus\" was 24\u00a0\"siliquae\". The weight of a Roman pound is generally believed to have been 327.45\u00a0g or possibly up to 5\u00a0g less. Therefore, the metric equivalent of 1 \"siliqua\" was approximately 189\u00a0mg. The Greeks had a similar unit of the same value.\nGold fineness in carats comes from carats and grains of gold in a solidus of coin. The conversion rates 1\u00a0solidus = 24\u00a0carats, 1\u00a0carat = 4\u00a0grains still stand. Woolhouse's \"Measures, Weights and Moneys of All Nations\" gives gold fineness in carats of 4 grains, and silver in troy pounds of 12\u00a0troy ounces of 20\u00a0pennyweight each."}
{"id": "7160", "revid": "13501746", "url": "https://en.wikipedia.org/wiki?curid=7160", "title": "European Conference of Postal and Telecommunications Administrations", "text": "The European Conference of Postal and Telecommunications Administrations (CEPT) was established on June 26, 1959, by nineteen European states in Montreux, Switzerland, as a coordinating body for European state telecommunications and postal organizations. The acronym comes from the French version of its name, .\nCEPT was responsible for the creation of the European Telecommunications Standards Institute (ETSI) in 1988.\nOrganization.\nCEPT is organised into three main components:\nMember countries.\n\"As of March 2022: 46 countries.\" \nAlbania, Andorra, Austria, Azerbaijan, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Georgia, Germany, Greece, Hungary, Iceland, Ireland, Italy, Latvia, Liechtenstein, Lithuania, Luxembourg, Malta, Moldova, Monaco, Montenegro, Netherlands, North Macedonia, Norway, Poland, Portugal, Romania, San Marino, Serbia, Slovak Republic, Slovenia, Spain, Sweden, Switzerland, Turkey, Ukraine, United Kingdom, Vatican City. The Russian Federation and Belarus memberships were suspended indefinitely on March 17, 2022."}
{"id": "7161", "revid": "1234701", "url": "https://en.wikipedia.org/wiki?curid=7161", "title": "Chain termination method", "text": ""}
{"id": "7162", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=7162", "title": "Tramlink", "text": "Tramlink, previously Croydon Tramlink and currently branded as London Trams, is a light rail tram system serving Croydon and surrounding areas in South London, England. It is the first operational tram system serving the London region since 1952. Tramlink is presently managed by London Trams, a public body part of Transport for London (TfL), and has been operated by FirstGroup since 2017. It is one of two light rail networks in Greater London, the other being the Docklands Light Railway. Tramlink is the fourth-busiest light rail network in the UK behind the Docklands Light Railway, Manchester Metrolink and Tyne and Wear Metro.\nStudies for the delivery of a modern-day tram system in Croydon begun in the 1960s and detailed planning was performed in the 1980s. Approval of the scheme was received in 1990 and, following a competitive tender process, construction and initial operation of the tramway was undertaken by \"Tramtrack Croydon\" (TC) via a 99-year Private Finance Initiative (PFI) contract. The official opening of Tramlink took place on 10 May 2000; by the end of the year three routes were operational. The network consists of 39 stops along of track, on a mixture of street track shared with other traffic, dedicated track in public roads, and off-street track consisting of new rights-of-way, former railway lines, and one right-of-way where the Tramlink track runs parallel to a third rail-electrified Network Rail line. The network's lines coincide in central Croydon, with eastern termini at Beckenham Junction, Elmers End and New Addington, and a western terminus at Wimbledon, where there is an interchange for London Underground.\nSince its original opening, the tram network has been expanded and additional rolling stock has been purchased. During 2008, TfL took over Tramlink operations, ending the PFI and making the company a subsidiary of TfL. Additional rolling stock was introduced during the early 2010s. Furthermore, numerous extensions to the network have been discussed, the most recent of which is the Sutton Link, an extension to connect Sutton to Colliers Wood. Sutton Link was paused in 2020 until funding can be secured. In the 2020s, TfL began work to order new trams for the system.\nHistory.\nInception.\nIn the first half of the 20th century, Croydon had many tramlines. However, these were all closed, the first was the Addiscombe \u2013 East Croydon station route through George Street to Cherry Orchard Road in 1927 and the last to close was the Purley - Embankment and Croydon (Coombe Road) - Thornton Heath routes closed in April 1951. However, in the Spring of 1950, the Highways Committee were presented by the Mayor with the concept of running trams between East Croydon station and the new estate being constructed at New Addington. This was based on the fact that the Feltham cars used in Croydon were going to Leeds to serve their new estates on reserved tracks. During 1962, a private study with assistance from BR engineers, showed how easy it was to convert the West Croydon - Wimbledon train service to tram operation and successfully prevent conflict between trams and trains.\nThese two concepts became joined in joint LRTL/TLRS concept of New Addington to Wimbledon every 15 minutes via East and West Croydon and Mitcham plus New Addington to Tattenham Corner every 15 minutes via East and West Croydon, Sutton and Epsom Downs. A branch into Forestdale to give an overlap service from Sutton was also included. During the 1970s, several BR directors and up-and-coming managers were aware of the advantages. Chris Green, upon becoming managing director, Network South East, published his plans in 1987 expanding the concept to take in the Tattenham Corner and Caterham branches and provide a service from Croydon to Lewisham via Addiscombe and Hayes. Following on from the opening of the DLR a small group working under Tony Ridley, then managing director, London Transport, investigated the potential for further light rail in London. The report 'Light Rail for London', written by engineer David Catling and Transport Planner Jon Willis, looked at a number of possible schemes including conversion of the East London Line. However a light rail network focussed on Croydon, with the conversion of existing heavy rail routes, was the most promising. The London Borough of Croydon wanted to improve access to the town centre without further road building and also improve access to the LCC built New Addington estate. Furthermore, road traffic in Croydon expanded considerably during the 1980s and planners were keen to apply public transit to fulfil the recorded growth in demand in the area. The project was developed by a small team in LT, headed by Scott McIntosh and in Croydon by Jill Lucas.\nThe scheme was accepted in principle in February 1990 by Croydon Council who worked with what was then London Regional Transport (LRT) to propose Tramlink to Parliament. The Croydon Tramlink Act 1994 resulted, which gave LRT the power to build and run Tramlink.\nConstruction.\nBoth the delivery and operation of the tramway was accomplished via a competitive tender process. During November 1995, it was announced that four consortia were shortlisted to build, operate and maintain Tramlink:\nIn May 1996, Tramtrack Croydon (TC) was awarded a 99-year Private Finance Initiative (PFI) contract to design, build, operate and maintain Tramlink. The equity partners in TC were Amey (50%), Royal Bank of Scotland (20%), 3i (20%) and Sir Robert McAlpine with Bombardier Transportation contracted to build and maintain the trams and FirstGroup operate the service. TC retained the revenue generated by Tramlink and LRT had to pay compensation to TC for any changes to the fares and ticketing policy introduced later. The concession agreement with TC was signed in November 1996, allowing construction to begin.\nConstruction work started in January 1997, with an expected opening in November 1999. The first tram was delivered in October 1998 to the new depot at Therapia Lane and testing on the sections of the Wimbledon line began shortly afterwards. Part of its track is the original route of the Surrey Iron Railway that opened in 1803.\nOpening.\nThe official opening of Tramlink took place on 10 May 2000 when route 3 from Croydon to New Addington opened to the public. Route 2 from Croydon to Beckenham Junction followed on 23 May 2000, and route 1 from Elmers End to Wimbledon opened a week later on 30 May 2000. It was the first modern tram project in London, with low-floor trams and low platforms allowing accessibility for all.\nBuyout by Transport for London.\nIn March 2008, TfL announced that it had reached agreement to buy TC for \u00a398million. The purchase was finalised on 28 June 2008. The background to this purchase relates to the requirement that TfL (who took over from London Regional Transport in 2000) compensates TC for the consequences of any changes to the fares and ticketing policy introduced since 1996. In 2007, that payment was \u00a34million, with an annual increase in rate. Despite this change, FirstGroup continues to operate the service.\nDuring October 2008, TfL introduced a new livery, using the blue, white and green of the routes on TfL maps, to distinguish the trams from buses operating in the area. The colour of the cars was changed to green, and the brand name was changed from Croydon Tramlink to simply Tramlink. The rebranding work was completed in early 2009.\nAdditional stop and trams.\nCentrale tram stop, in Tamworth Road on the one-way central loop, opened on 10 December 2005, increasing journey times slightly. As turnround times were already quite tight, this raised the issue of buying an extra tram to maintain punctuality. Partly for this reason, but also to take into account the planned restructuring of services, (subsequently introduced in July 2006), TfL issued tenders for a new tram. However, nothing resulted from this.\nIn January 2011, TfL opened a tender for the supply of ten new or second-hand trams from the end of summer 2011, for use between Therapia Lane and Elmers End. On 18 August 2011, TfL announced that Stadler Rail had won a \u00a316.3million contract to supply six Variobahn trams similar to those used by Bybanen in Bergen, Norway. They entered service in 2012. \nIn August 2013, TfL ordered an additional four Variobahn trams for delivery in 2015, for use on the Wimbledon to Croydon link, an order later increased to six. This brought the total Variobahn fleet up to ten in 2015, and 12 in 2016 when the final two trams were delivered.\nCurrent network.\nStops.\nThere are 39 stops, with 38 opened in the initial phase, and Centrale tram stop added on 10 December 2005. Most stops are long. The tram stops have low platforms, above rail level, virtually level with the doors. This level access from platform to tram allows wheelchairs, prams, pushchairs and the elderly to board easily with no steps. In street sections, the stop is integrated with the pavement. All platforms are all wider than . Tramlink uses some former main-line stations on the Wimbledon\u2013West Croydon and Elmers End\u2013Coombe Lane stretches of line. The railway platforms have been demolished and rebuilt to Tramlink specifications, except at Elmers End and Wimbledon where the track level was raised to meet the higher main-line platforms to enable cross-platform interchange.\nStops are unstaffed and had automated ticket machines that are no longer in use due to TfL making trams cashless. In general, access between the platforms involves crossing the tracks by pedestrian level crossing. Stops also feature CCTV, a Passenger Help Point, a Passenger Information Display (PID), litter bins, a noticeboard and lamp-posts, and most also have seats and a shelter. The PIDs display the destinations and expected arrival times of the next two trams. They can also display any message the controllers want to display, such as information on delays or even safety instructions for vandals to stop putting rubbish or other objects onto the track.\nRoutes.\nTramlink has been shown on the principal tube map since 1 June 2016, having previously appeared only on the \"London Connections\" map.\nWhen Tramlink first opened it had three routes: Line 1 (yellow) from Wimbledon to Elmers End, Line 2 (red) from Croydon to Beckenham Junction, and Line 3 (green) from Croydon to New Addington. On 23 July 2006, the network was restructured, with Route 1 from Elmers End to Croydon, Route 2 from Beckenham Junction to Croydon and Route 3 from New Addington to Wimbledon. On 25 June 2012, Route 4 from Therapia Lane to Elmers End was introduced. On 4 April 2016, Route 4 was extended from Therapia Lane to Wimbledon.\nOn 25 February 2018, the network and timetables were restructured again for more even and reliable services. As part of this change, trams would no longer display route numbers on their dot matrix destination screens. This resulted in three routes:\nAdditionally, the first two trams from New Addington will run to Wimbledon. Overall, this would result in a decrease in 2tph leaving Elmers End, resulting in a 25% decrease in capacity here, and 14% in the Addiscombe area. However, this would also regulate waiting times in this area and on the Wimbledon branch to every five minutes, from every two\u2013seven minutes.\nFormer lines reused.\nTramlink makes use of a number of National Rail lines, running parallel to franchised services, or in some cases, runs on previously abandoned railway corridors. Between Birkbeck and Beckenham Junction, Tramlink uses the Crystal Palace line, running on a single track alongside the track carrying Southern rail services. The National Rail track had been singled some years earlier.\nFrom Elmers End to Woodside, Tramlink follows the former Addiscombe Line. At Woodside, the old station buildings stand disused, and the original platforms have been replaced by accessible low platforms. Tramlink then follows the former Woodside and South Croydon Railway (W&amp;SCR) to reach the current Addiscombe tram stop, adjacent to the site of the demolished Bingham Road railway station. It continues along the former railway route to near Sandilands, where Tramlink curves sharply towards Sandilands tram stop. Another route from Sandilands tram stop curves sharply on to the W&amp;SCR before passing through Park Hill (or Sandilands) tunnels and to the site of Coombe Road station after which it curves away across Lloyd Park.\nBetween Wimbledon station and Wandle Park, Tramlink follows the former West Croydon to Wimbledon Line, which was first opened in 1855 and closed on 31 May 1997 to allow for conversion into Tramlink. Within this section, from near Phipps Bridge to near Reeves Corner, Tramlink follows the Surrey Iron Railway, giving Tramlink a claim to one of the world's oldest railway alignments. Beyond Wandle Park, a Victorian footbridge beside Waddon New Road was dismantled to make way for the flyover over the West Croydon to Sutton railway line. The footbridge has been re-erected at Corfe Castle station on the Swanage Railway (although some evidence suggests that this was a similar footbridge removed from the site of Merton Park railway station).\nFeeder buses.\nBus routes T31, T32 and T33 used to connect with Tramlink at the New Addington, Fieldway and Addington Village stops. T31 and T32 no longer run, and T33 has been renumbered as 433.\nOnboard announcements.\nThe onboard announcements are by BBC News reader (and tram enthusiast) Nicholas Owen. The announcement pattern is as follows: e.g. \"This tram is for Wimbledon; the next stop will be Merton Park\".\nRolling stock.\nCurrent fleet.\nTramlink currently uses 35 trams. In summary:\nBombardier CR4000.\nThe original fleet comprised 24 articulated low floor Bombardier Flexity Swift CR4000 trams built in Vienna numbered beginning at 2530, continuing from the highest-numbered tram 2529 on London's former tram network, which closed in 1952. The original livery was red and white. One (2550) was painted in FirstGroup white, blue and pink livery. During 2006, the CR4000 fleet was refreshed, with the bus-style destination roller blinds being replaced with a digital dot-matrix display. Between 2008 and 2009 the fleet was repainted externally in the new green livery and the interiors were refurbished with new flooring, seat covers retrimmed in a new moquette and stanchions repainted from yellow to green. One (2551) has been permanently withdrawn having been significantly damaged in the 2016 Croydon tram derailment on 9 November 2016.\nIn 2007, tram 2535 was named after Steven Parascandolo, a well known tram enthusiast.\nCroydon Variobahn.\nIn January 2011, Tramtrack Croydon invited tenders for the supply of then new or second-hand trams, and on 18 August 2011, TfL announced that Stadler Rail had won a \u00a316.3million contract to supply six Variobahn trams similar to those used by Bybanen in Bergen, Norway. They entered service during 2012. In August 2013, TfL ordered an additional four Variobahn trams for delivery in 2015, an order that was later increased to six. This brought the total Variobahn fleet up to ten in 2015, and 12 in 2016 when the final two trams were delivered.\nAncillary vehicles.\nEngineers' vehicles used in Tramlink construction were hired for that purpose.\nIn November 2006, Tramlink purchased five second-hand engineering vehicles from Deutsche Bahn. These were two engineers' trams (numbered 058 and 059 in Tramlink service), and three 4-wheel wagons (numbered 060, 061, and 062). Service tram 058 and trailer 061 were both sold to the National Tramway Museum in 2010.\nFuture fleet.\nIn the 2020s, TfL began work to replace the CR4000 tram fleet, which are approaching their end of its life and becoming increasingly unreliable. In June 2023, one-fifth of the CR4000 fleet was temporarily withdrawn due to issues with their wheels.\nIn January 2024, Tramtrack Croydon invited tenders for a base order of 24 new trams with an option for 16 more and a 30-year technical support contract, costed at \u00a3385million. In September 2024, TfL announced that four manufacturers (Alstom, Construcciones y Auxiliar de Ferrocarriles, Hitachi Rail and Stadler Rail Valencia) had been invited to place bids. The new fleet is intended to replace the CR4000 trams, which are reaching the end of their design life.\nFares and ticketing.\nTfL Bus &amp; Tram Passes are valid on Tramlink, as are Travelcards that include any of zones 3, 4, 5 and 6.\nPay-as-you-go Oyster card fares are the same as on London Buses, although special fares may apply when using Tramlink feeder buses.\nWhen using Oyster cards, passengers must touch in on the platform before boarding the tram. Special arrangements apply at Wimbledon station, where the Tramlink stop is within the National Rail and London Underground station. Tramlink passengers must therefore touch in at the station entry barriers then again at the Tramlink platform to inform the system that no mainline/LUL rail journey has been made.\nContactless payment cards can also be used to pay for fares in the same manner as Oyster cards. Ticket machines were withdrawn on 16 July 2018.\nCorporate affairs.\nOwnership and structure.\nThe service was created as a result of the Croydon Tramlink Act 1994 that received Royal Assent on 21 July 1994, a Private Bill jointly promoted by London Regional Transport (the predecessor of Transport for London (TfL)) and Croydon London Borough Council. Following a competitive tender, a consortium company Tramtrack Croydon Limited (incorporated in 1995) was awarded a 99-year concession to build and run the system. On 17 March 2008, it was announced that TfL would take over Tramlink in exchange for \u00a398million. Since 28 June 2008, the company has been a subsidiary of TfL.\nTramlink is currently operated by Tram Operations Ltd (TOL), a subsidiary of FirstGroup, who have a contract to operate the service until 2030. TOL provides the drivers and management to operate the service; the infrastructure and trams are owned and maintained by a TfL subsidiary.\nBusiness trends.\nThe key available trends in recent years for Tramlink are (years ending 31 March):\nActivities in the financial year 2020/21 were severely reduced by the impact of the coronavirus pandemic.\nPassenger numbers.\nDetailed passenger journeys since Tramlink commenced operations in May 2000 were:\nProposals for extensions.\nNumerous extensions to the network have been discussed or proposed over the years, involving varying degrees of support and investigative effort.\nDuring 2002, as part of The Mayor's Transport Strategy for London, a number of proposed extensions were identified, including to Sutton from Wimbledon or Mitcham; to Crystal Palace; to Colliers Wood/Tooting; and along the A23. The Strategy said that \"extensions to the network could, in principle, be developed at relatively modest cost where there is potential demand...\" and sought initial views on the viability of a number of extensions by summer 2002.\nIn 2006, in a TfL consultation on an extension to Crystal Palace, three options were presented: on-street, off-street and a mixture of the two. After the consultation, the off-street option was favoured, to include Crystal Palace Station and Crystal Palace Parade. TfL stated in 2008 that due to lack of funding the plans for this extension would not be taken forward. They were revived shortly after Boris Johnson's re-election as Mayor in May 2012, but six months later they were cancelled again.\nDuring November 2014, a 15-year plan, Trams 2030, called for upgrades to increase capacity on the network in line with an expected increase in ridership to 60million passengers by 2031 (although the passenger numbers at the time (2013/14: 31.2 million) have not been exceeded since (as at 2019)).\nThe upgrades were to improve reliability, support regeneration in the Croydon metropolitan centre, and future-proof the network for Crossrail 2, a potential Bakerloo line extension, and extensions to the tram network itself to a wide variety of destinations. The plans involve dual-tracking across the network and introducing diverting loops on either side of Croydon, allowing for a higher frequency of trams on all four branches without increasing congestion in central Croydon. The \u00a3737million investment was to be funded by the Croydon Growth Zone, TfL Business Plan, housing levies, and the respective boroughs, and by the affected developers.\nAll the various developments, if implemented, could theoretically require an increase in the fleet from 30 to up to 80 trams (depending on whether longer trams or coupled trams are used). As such, an increase in depot and stabling capacity would also be required; enlargement of the current Therapia Lane site, as well as sites near the Elmers End and Harrington Road tram stops, were shortlisted.\nSutton Link.\nDuring July 2013, then Mayor Boris Johnson had affirmed that there was a reasonable business case for Tramlink to cover the Wimbledon \u2013 Sutton corridor, which might also include a loop via St Helier Hospital and an extension to The Royal Marsden Hospital. In 2014, a proposed \u00a3320M scheme for a new line to connect Wimbledon to Sutton via Morden was made and brought to consultation jointly by the London Boroughs of Merton and Sutton. Although \u00a3100M from TfL was initially secured in the draft 2016/17 budget, this was subsequently reallocated.\nIn 2018, TfL opened a consultation on proposals for a connection to Sutton, with three route options: from South Wimbledon, from Colliers Wood (both having an option of a bus rapid transit route or a tram line) or from Wimbledon (only as a tram line). During February 2020, following the consultation, TfL announced their preference for a north\u2013south tramway between Colliers Wood and Sutton town centre, with a projected cost of \u00a3425M, on the condition of securing additional funding. Work on the project stopped in July 2020, as Transport for London could not find sufficient funding for it to continue.\nIn February 2020, TfL announced the preferred route of the extension, expressing their support for \"Route Option 2 (Colliers Wood \u2013 Sutton) operated as a tram service ... assuming we are successful in securing funding to deliver the project\".,\nOn 24 July 2020, the project was temporarily put on hold due to the COVID-19 pandemic. TfL said they were pausing development work on the scheme \"as the transport case is poor and there remains a significant funding gap\". Andy Byford, London's Transport Commissioner, said that this involves making 'difficult choices' about which projects can be funded.\nDuring 2023, Sutton's council leader Ruth Dombey advocated for the project and urged TfL and the mayor's office to provide fair and adequate funding, especially in light of the ULEZ charge. However, London Mayor Sadiq Khan dismissed the project as inadequate and pointed out the \u00a3440M funding shortfall. London Mayor Sadiq Khan faced criticism from Sutton MP Paul Scully on 21 April 2023, for the delayed Sutton tram extension project and implementing the Ultra Low Emission Zone charge without sufficient public transport alternatives, while defending the delay citing a \u00a3440M funding gap. In December 2023, TfL stated that further progress will depend on funding agreements with other stakeholders such as local councils, the Department for Transport, as well as Government, and that the Sutton Link is currently the only extension being considered. Rival proposals included new bus routes."}
{"id": "7163", "revid": "1271539522", "url": "https://en.wikipedia.org/wiki?curid=7163", "title": "Catenary", "text": "In physics and geometry, a catenary ( , ) is the curve that an idealized hanging chain or cable assumes under its own weight when supported only at its ends in a uniform gravitational field.\nThe catenary curve has a U-like shape, superficially similar in appearance to a parabola, which it is not.\nThe curve appears in the design of certain types of arches and as a cross section of the catenoid\u2014the shape assumed by a soap film bounded by two parallel circular rings.\nThe catenary is also called the alysoid, chainette, or, particularly in the materials sciences, an example of a funicular. Rope statics describes catenaries in a classic statics problem involving a hanging rope.\nMathematically, the catenary curve is the graph of the hyperbolic cosine function. The surface of revolution of the catenary curve, the catenoid, is a minimal surface, specifically a minimal surface of revolution. A hanging chain will assume a shape of least potential energy which is a catenary. Galileo Galilei in 1638 discussed the catenary in the book \"Two New Sciences\" recognizing that it was different from a parabola. The mathematical properties of the catenary curve were studied by Robert Hooke in the 1670s, and its equation was derived by Leibniz, Huygens and Johann Bernoulli in 1691.\nCatenaries and related curves are used in architecture and engineering (e.g., in the design of bridges and arches so that forces do not result in bending moments). In the offshore oil and gas industry, \"catenary\" refers to a steel catenary riser, a pipeline suspended between a production platform and the seabed that adopts an approximate catenary shape. In the rail industry it refers to the overhead wiring that transfers power to trains. (This often supports a contact wire, in which case it does not follow a true catenary curve.)\nIn optics and electromagnetics, the hyperbolic cosine and sine functions are basic solutions to Maxwell's equations. The symmetric modes consisting of two evanescent waves would form a catenary shape.\nHistory.\nThe word \"catenary\" is derived from the Latin word \"cat\u0113na\", which means \"chain\". The English word \"catenary\" is usually attributed to Thomas Jefferson,\nwho wrote in a letter to Thomas Paine on the construction of an arch for a bridge:\nIt is often said that Galileo thought the curve of a hanging chain was parabolic. However, in his \"Two New Sciences\" (1638), Galileo wrote that a hanging cord is only an approximate parabola, correctly observing that this approximation improves in accuracy as the curvature gets smaller and is almost exact when the elevation is less than 45\u00b0. The fact that the curve followed by a chain is not a parabola was proven by Joachim Jungius (1587\u20131657); this result was published posthumously in 1669.\nThe application of the catenary to the construction of arches is attributed to Robert Hooke, whose \"true mathematical and mechanical form\" in the context of the rebuilding of St Paul's Cathedral alluded to a catenary. Some much older arches approximate catenaries, an example of which is the Arch of Taq-i Kisra in Ctesiphon.\nIn 1671, Hooke announced to the Royal Society that he had solved the problem of the optimal shape of an arch, and in 1675 published an encrypted solution as a Latin anagram in an appendix to his \"Description of Helioscopes,\" where he wrote that he had found \"a true mathematical and mechanical form of all manner of Arches for Building.\" He did not publish the solution to this anagram in his lifetime, but in 1705 his executor provided it as \"ut pendet continuum flexile, sic stabit contiguum rigidum inversum\", meaning \"As hangs a flexible cable so, inverted, stand the touching pieces of an arch.\"\nIn 1691, Gottfried Leibniz, Christiaan Huygens, and Johann Bernoulli derived the equation in response to a challenge by Jakob Bernoulli; their solutions were published in the \"Acta Eruditorum\" for June 1691. David Gregory wrote a treatise on the catenary in 1697 in which he provided an incorrect derivation of the correct differential equation.\nLeonhard Euler proved in 1744 that the catenary is the curve which, when rotated about the -axis, gives the surface of minimum surface area (the catenoid) for the given bounding circles. Nicolas Fuss gave equations describing the equilibrium of a chain under any force in 1796.\nInverted catenary arch.\nCatenary arches are often used in the construction of kilns. To create the desired curve, the shape of a hanging chain of the desired dimensions is transferred to a form which is then used as a guide for the placement of bricks or other building material.\nThe Gateway Arch in St. Louis, Missouri, United States is sometimes said to be an (inverted) catenary, but this is incorrect. It is close to a more general curve called a flattened catenary, with equation , which is a catenary if . While a catenary is the ideal shape for a freestanding arch of constant thickness, the Gateway Arch is narrower near the top. According to the U.S. National Historic Landmark nomination for the arch, it is a \"weighted catenary\" instead. Its shape corresponds to the shape that a weighted chain, having lighter links in the middle, would form. \nCatenary bridges.\nIn free-hanging chains, the force exerted is uniform with respect to length of the chain, and so the chain follows the catenary curve. The same is true of a simple suspension bridge or \"catenary bridge,\" where the roadway follows the cable.\nA stressed ribbon bridge is a more sophisticated structure with the same catenary shape.\nHowever, in a suspension bridge with a suspended roadway, the chains or cables support the weight of the bridge, and so do not hang freely. In most cases the roadway is flat, so when the weight of the cable is negligible compared with the weight being supported, the force exerted is uniform with respect to horizontal distance, and the result is a parabola, as discussed below (although the term \"catenary\" is often still used, in an informal sense). If the cable is heavy then the resulting curve is between a catenary and a parabola.\nAnchoring of marine objects.\nThe catenary produced by gravity provides an advantage to heavy anchor rodes. An anchor rode (or anchor line) usually consists of chain or cable or both. Anchor rodes are used by ships, oil rigs, docks, floating wind turbines, and other marine equipment which must be anchored to the seabed.\nWhen the rope is slack, the catenary curve presents a lower angle of pull on the anchor or mooring device than would be the case if it were nearly straight. This enhances the performance of the anchor and raises the level of force it will resist before dragging. To maintain the catenary shape in the presence of wind, a heavy chain is needed, so that only larger ships in deeper water can rely on this effect. Smaller boats also rely on catenary to maintain maximum holding power.\nCable ferries and chain boats present a special case of marine vehicles moving although moored by the two catenaries each of one or more cables (wire ropes or chains) passing through the vehicle and moved along by motorized sheaves. The catenaries can be evaluated graphically.\nMathematical description.\nEquation.\nThe equation of a catenary in Cartesian coordinates has the form\nformula_1\nwhere is the hyperbolic cosine function, and where is the distance of the lowest point above the x axis. All catenary curves are similar to each other, since changing the parameter is equivalent to a uniform scaling of the curve.\nThe Whewell equation for the catenary is\nformula_2\nwhere formula_3 is the tangential angle and the arc length.\nDifferentiating gives\nformula_4\nand eliminating formula_3 gives the Ces\u00e0ro equation\nformula_6\nwhere formula_7 is the curvature.\nThe radius of curvature is then\nformula_8\nwhich is the length of the normal between the curve and the -axis.\nRelation to other curves.\nWhen a parabola is rolled along a straight line, the roulette curve traced by its focus is a catenary. The envelope of the directrix of the parabola is also a catenary. The involute from the vertex, that is the roulette traced by a point starting at the vertex when a line is rolled on a catenary, is the tractrix.\nAnother roulette, formed by rolling a line on a catenary, is another line. This implies that square wheels can roll perfectly smoothly on a road made of a series of bumps in the shape of an inverted catenary curve. The wheels can be any regular polygon except a triangle, but the catenary must have parameters corresponding to the shape and dimensions of the wheels.\nGeometrical properties.\nOver any horizontal interval, the ratio of the area under the catenary to its length equals , independent of the interval selected. The catenary is the only plane curve other than a horizontal line with this property. Also, the geometric centroid of the area under a stretch of catenary is the midpoint of the perpendicular segment connecting the centroid of the curve itself and the -axis.\nScience.\nA moving charge in a uniform electric field travels along a catenary (which tends to a parabola if the charge velocity is much less than the speed of light ).\nThe surface of revolution with fixed radii at either end that has minimum surface area is a catenary\nformula_9\nrevolved about the -axis.\nAnalysis.\nModel of chains and arches.\nIn the mathematical model the chain (or cord, cable, rope, string, etc.) is idealized by assuming that it is so thin that it can be regarded as a curve and that it is so flexible any force of tension exerted by the chain is parallel to the chain. The analysis of the curve for an optimal arch is similar except that the forces of tension become forces of compression and everything is inverted.\nAn underlying principle is that the chain may be considered a rigid body once it has attained equilibrium. Equations which define the shape of the curve and the tension of the chain at each point may be derived by a careful inspection of the various forces acting on a segment using the fact that these forces must be in balance if the chain is in static equilibrium.\nLet the path followed by the chain be given parametrically by where represents arc length and is the position vector. This is the natural parameterization and has the property that\nformula_10\nwhere is a unit tangent vector.\nA differential equation for the curve may be derived as follows. Let be the lowest point on the chain, called the vertex of the catenary. The slope of the curve is zero at since it is a minimum point. Assume is to the right of since the other case is implied by symmetry. The forces acting on the section of the chain from to are the tension of the chain at , the tension of the chain at , and the weight of the chain. The tension at is tangent to the curve at and is therefore horizontal without any vertical component and it pulls the section to the left so it may be written where is the magnitude of the force. The tension at is parallel to the curve at and pulls the section to the right. The tension at can be split into two components so it may be written , where is the magnitude of the force and is the angle between the curve at and the -axis (see tangential angle). Finally, the weight of the chain is represented by where is the weight per unit length and is the length of the segment of chain between and .\nThe chain is in equilibrium so the sum of three forces is , therefore\nformula_11\nand\nformula_12\nand dividing these gives\nformula_13\nIt is convenient to write\nformula_14\nwhich is the length of chain whose weight is equal in magnitude to the tension at . Then\nformula_15\nis an equation defining the curve.\nThe horizontal component of the tension, is constant and the vertical component of the tension, is proportional to the length of chain between and the vertex.\nDerivation of equations for the curve.\nThe differential equation formula_16, given above, can be solved\nto produce equations for the curve.\nWe will solve the equation using the boundary condition that\nthe vertex is positioned at formula_17 and formula_18.\nFirst, invoke the formula for\narc length\nto get\nformula_19\nthen separate variables\nto obtain\nformula_20\nA reasonably straightforward approach to integrate this is to use\nhyperbolic substitution,\nwhich gives\nformula_21\n(where formula_22 is a constant of integration),\nand hence\nformula_23\nBut formula_24, so\nformula_25\nwhich integrates as\nformula_26\n(with formula_27 being the constant of integration satisfying the boundary condition).\nSince the primary interest here is simply the shape of the curve,\nthe placement of the coordinate axes are arbitrary;\nso make the convenient choice of formula_28\nto simplify the result to\nformula_29\nFor completeness, the formula_30 relation can be derived by\nsolving each of the formula_31 and formula_32 relations for formula_33, giving:\nformula_34\nso\nformula_35\nwhich can be rewritten as\nformula_36\nAlternative derivation.\nThe differential equation can be solved using a different approach. From\nformula_37\nit follows that\nformula_38\nand\nformula_39\nIntegrating gives,\nformula_40\nand\nformula_41\nAs before, the and -axes can be shifted so and can be taken to be 0. Then\nformula_42\nand taking the reciprocal of both sides\nformula_43\nAdding and subtracting the last two equations then gives the solution\nformula_44\nand\nformula_45\nDetermining parameters.\nIn general the parameter is the position of the axis. The equation can be determined in this case as follows:\nRelabel if necessary so that is to the left of and let be the horizontal and be the vertical distance from to . Translate the axes so that the vertex of the catenary lies on the -axis and its height is adjusted so the catenary satisfies the standard equation of the curve\nformula_46\nand let the coordinates of and be and respectively. The curve passes through these points, so the difference of height is\nformula_47\nand the length of the curve from to is\nformula_48\nWhen is expanded using these expressions the result is\nformula_49\nso\nformula_50\nThis is a transcendental equation in and must be solved numerically. Since formula_51 is strictly monotonic on formula_52, there is at most one solution with and so there is at most one position of equilibrium.\nHowever, if both ends of the curve ( and ) are at the same level (), it can be shown that\nformula_53\nwhere L is the total length of the curve between and and is the sag (vertical distance between , and the vertex of the curve).\nIt can also be shown that\nformula_54\nand\nformula_55\nwhere H is the horizontal distance between and which are located at the same level ().\nThe horizontal traction force at and is , where is the weight per unit length of the chain or cable.\nTension relations.\nThere is a simple relationship between the tension in the cable at a point and its - and/or - coordinate. Begin by combining the squares of the vector components of the tension:\nformula_56\nwhich (recalling that formula_57) can be rewritten as\nformula_58\nBut, as shown above,\nformula_59 (assuming that formula_60), so we get the simple relations\nformula_61\nVariational formulation.\nConsider a chain of length formula_62 suspended from two points of equal height and at distance formula_63. The curve has to minimize its potential energy\nformula_64\n(where is the weight per unit length) and is subject to the constraint \nformula_65\nThe modified Lagrangian is therefore \nformula_66\nwhere formula_67 is the Lagrange multiplier to be determined. As the independent variable formula_68 does not appear in the Lagrangian, we can use the Beltrami identity \nformula_69\nwhere formula_70 is an integration constant, in order to obtain a first integral\nformula_71\nThis is an ordinary first order differential equation that can be solved by the method of separation of variables. Its solution is the usual hyperbolic cosine where the parameters are obtained from the constraints.\nGeneralizations with vertical force.\nNonuniform chains.\nIf the density of the chain is variable then the analysis above can be adapted to produce equations for the curve given the density, or given the curve to find the density.\nLet denote the weight per unit length of the chain, then the weight of the chain has magnitude\nformula_72\nwhere the limits of integration are and . Balancing forces as in the uniform chain produces\nformula_11\nand\nformula_74\nand therefore\nformula_75\nDifferentiation then gives\nformula_76\nIn terms of and the radius of curvature this becomes\nformula_77\nSuspension bridge curve.\nA similar analysis can be done to find the curve followed by the cable supporting a suspension bridge with a horizontal roadway. If the weight of the roadway per unit length is and the weight of the cable and the wire supporting the bridge is negligible in comparison, then the weight on the cable (see the figure in Catenary#Model of chains and arches) from to is where is the horizontal distance between and . Proceeding as before gives the differential equation\nformula_78\nThis is solved by simple integration to get\nformula_79\nand so the cable follows a parabola. If the weight of the cable and supporting wires is not negligible then the analysis is more complex.\nCatenary of equal strength.\nIn a catenary of equal strength, the cable is strengthened according to the magnitude of the tension at each point, so its resistance to breaking is constant along its length. Assuming that the strength of the cable is proportional to its density per unit length, the weight, , per unit length of the chain can be written , where is constant, and the analysis for nonuniform chains can be applied.\nIn this case the equations for tension are\nformula_80\nCombining gives\nformula_81\nand by differentiation\nformula_82\nwhere is the radius of curvature.\nThe solution to this is\nformula_83\nIn this case, the curve has vertical asymptotes and this limits the span to . Other relations are\nformula_84\nThe curve was studied 1826 by Davies Gilbert and, apparently independently, by Gaspard-Gustave Coriolis in 1836.\nRecently, it was shown that this type of catenary could act as a building block of electromagnetic metasurface and was known as \"catenary of equal phase gradient\".\nElastic catenary.\nIn an elastic catenary, the chain is replaced by a spring which can stretch in response to tension. The spring is assumed to stretch in accordance with Hooke's Law. Specifically, if is the natural length of a section of spring, then the length of the spring with tension applied has length\nformula_85\nwhere is a constant equal to , where is the stiffness of the spring. In the catenary the value of is variable, but ratio remains valid at a local level, so\nformula_86\nThe curve followed by an elastic spring can now be derived following a similar method as for the inelastic spring.\nThe equations for tension of the spring are\nformula_87\nand\nformula_88\nfrom which\nformula_89\nwhere is the natural length of the segment from to and is the weight per unit length of the spring with no tension. Write\nformula_90\nso\nformula_91\nThen \nformula_92\nfrom which\nformula_93\nIntegrating gives the parametric equations\nformula_94\nAgain, the and -axes can be shifted so and can be taken to be 0. So\nformula_95\nare parametric equations for the curve. At the rigid limit where is large, the shape of the curve reduces to that of a non-elastic chain.\nOther generalizations.\nChain under a general force.\nWith no assumptions being made regarding the force acting on the chain, the following analysis can be made.\nFirst, let be the force of tension as a function of . The chain is flexible so it can only exert a force parallel to itself. Since tension is defined as the force that the chain exerts on itself, must be parallel to the chain. In other words,\nformula_96\nwhere is the magnitude of and is the unit tangent vector.\nSecond, let be the external force per unit length acting on a small segment of a chain as a function of . The forces acting on the segment of the chain between and are the force of tension at one end of the segment, the nearly opposite force at the other end, and the external force acting on the segment which is approximately . These forces must balance so\nformula_97\nDivide by and take the limit as to obtain\nformula_98\nThese equations can be used as the starting point in the analysis of a flexible chain acting under any external force. In the case of the standard catenary, where the chain has weight per unit length."}
{"id": "7164", "revid": "42195518", "url": "https://en.wikipedia.org/wiki?curid=7164", "title": "Color temperature", "text": "Color temperature is a parameter describing the color of a visible light source by comparing it to the color of light emitted by an idealized opaque, non-reflective body. The temperature of the ideal emitter that matches the color most closely is defined as the color temperature of the original visible light source. The color temperature scale describes only the \"color\" of light emitted by a light source, which may actually be at a different (and often much lower) temperature.\nColor temperature has applications in lighting, photography, videography, publishing, manufacturing, astrophysics, and other fields. In practice, color temperature is most meaningful for light sources that correspond somewhat closely to the color of some black body, i.e., light in a range going from red to orange to yellow to white to bluish white. Although the concept of correlated color temperature extends the definition to any visible light, the color temperature of a green or a purple light rarely is useful information. Color temperature is conventionally expressed in kelvins, using the symbol K, a unit for absolute temperature.\nColor temperatures over 5000\u00a0K are called \"cool colors\" (bluish), while lower color temperatures (2700\u20133000\u00a0K) are called \"warm colors\" (yellowish). \"Warm\" in this context is with respect to a traditional categorization of colors, not a reference to black body temperature. The hue-heat hypothesis states that low color temperatures will feel warmer while higher color temperatures will feel cooler. The spectral peak of warm-colored light is closer to infrared, and most natural warm-colored light sources emit significant infrared radiation. The fact that \"warm\" lighting in this sense actually has a \"cooler\" color temperature often leads to confusion.\nCategorizing different lighting.\nThe color temperature of the electromagnetic radiation emitted from an ideal black body is defined as its surface temperature in kelvins, or alternatively in micro reciprocal degrees (mired). This permits the definition of a standard by which light sources are compared.\nTo the extent that a hot surface emits thermal radiation but is not an ideal black-body radiator, the color temperature of the light is not the actual temperature of the surface. An incandescent lamp's light is thermal radiation, and the bulb approximates an ideal black-body radiator, so its color temperature is essentially the temperature of the filament. Thus a relatively low temperature emits a dull red and a high temperature emits the almost white of the traditional incandescent light bulb. Metal workers are able to judge the temperature of hot metals by their color, from dark red to orange-white and then white (see red heat).\nMany other light sources, such as fluorescent lamps, or light emitting diodes (LEDs) emit light primarily by processes other than thermal radiation. This means that the emitted radiation does not follow the form of a black-body spectrum. These sources are assigned what is known as a correlated color temperature (CCT). CCT is the color temperature of a black-body radiator which to human color perception most closely matches the light from the lamp. Because such an approximation is not required for incandescent light, the CCT for an incandescent light is simply its unadjusted temperature, derived from comparison to a black-body radiator.\nThe Sun.\nThe Sun closely approximates a black-body radiator. The effective temperature, defined by the total radiative power per square unit, is 5772\u00a0K. The color temperature of sunlight above the atmosphere is about 5900\u00a0K.\nThe Sun may appear red, orange, yellow, or white from Earth, depending on its position in the sky. The changing color of the Sun over the course of the day is mainly a result of the scattering of sunlight and is not due to changes in black-body radiation. Rayleigh scattering of sunlight by Earth's atmosphere causes the blue color of the sky, which tends to scatter blue light more than red light.\nSome daylight in the early morning and late afternoon (the golden hours) has a lower (\"warmer\") color temperature due to increased scattering of shorter-wavelength sunlight by atmospheric particulates \u2013 an optical phenomenon called the Tyndall effect.\nDaylight has a spectrum similar to that of a black body with a correlated color temperature of 6500\u00a0K (D65 viewing standard) or 5500\u00a0K (daylight-balanced photographic film standard).\nFor colors based on black-body theory, blue occurs at higher temperatures, whereas red occurs at lower temperatures. This is the opposite of the cultural associations attributed to colors, in which \"red\" is \"hot\", and \"blue\" is \"cold\".\nApplications.\nLighting.\nFor lighting building interiors, it is often important to take into account the color temperature of illumination. A warmer (i.e., a lower color temperature) light is often used in public areas to promote relaxation, while a cooler (higher color temperature) light is used to enhance concentration, for example in schools and offices.\nCCT dimming for LED technology is regarded as a difficult task, since binning, age and temperature drift effects of LEDs change the actual color value output. Here feedback loop systems are used, for example with color sensors, to actively monitor and control the color output of multiple color mixing LEDs.\nAquaculture.\nIn fishkeeping, color temperature has different functions and foci in the various branches.\nDigital photography.\nIn digital photography, the term color temperature sometimes refers to remapping of color values to simulate variations in ambient color temperature. Most digital cameras and raw image software provide presets simulating specific ambient values (e.g., sunny, cloudy, tungsten, etc.) while others allow explicit entry of white balance values in kelvins. These settings vary color values along the blue\u2013yellow axis, while some software includes additional controls (sometimes labeled \"tint\") adding the magenta\u2013green axis, and are to some extent arbitrary and a matter of artistic interpretation.\nPhotographic film.\nPhotographic emulsion film does not respond to lighting color identically to the human retina or visual perception. An object that appears to the observer to be white may turn out to be very blue or orange in a photograph. The color balance may need to be corrected during printing to achieve a neutral color print. The extent of this correction is limited since color film normally has three layers sensitive to different colors and when used under the \"wrong\" light source, every layer may not respond proportionally, giving odd color casts in the shadows, although the mid-tones may have been correctly white-balanced under the enlarger. Light sources with discontinuous spectra, such as fluorescent tubes, cannot be fully corrected in printing either, since one of the layers may barely have recorded an image at all.\nPhotographic film is made for specific light sources (most commonly daylight film and tungsten film), and, used properly, will create a neutral color print. Matching the sensitivity of the film to the color temperature of the light source is one way to balance color. If tungsten film is used indoors with incandescent lamps, the yellowish-orange light of the tungsten incandescent lamps will appear as white (3200\u00a0K) in the photograph. Color negative film is almost always daylight-balanced, since it is assumed that color can be adjusted in printing (with limitations, see above). Color transparency film, being the final artefact in the process, has to be matched to the light source or filters must be used to correct color.\nFilters on a camera lens, or color gels over the light source(s) may be used to correct color balance. When shooting with a bluish light (high color temperature) source such as on an overcast day, in the shade, in window light, or if using tungsten film with white or blue light, a yellowish-orange filter will correct this. For shooting with daylight film (calibrated to 5600\u00a0K) under warmer (low color temperature) light sources such as sunsets, candlelight or tungsten lighting, a bluish (e.g. #80A) filter may be used. More-subtle filters are needed to correct for the difference between, say 3200\u00a0K and 3400\u00a0K tungsten lamps or to correct for the slightly blue cast of some flash tubes, which may be 6000\u00a0K.\nIf there is more than one light source with varied color temperatures, one way to balance the color is to use daylight film and place color-correcting gel filters over each light source.\nPhotographers sometimes use color temperature meters. These are usually designed to read only two regions along the visible spectrum (red and blue); more expensive ones read three regions (red, green, and blue). However, they are ineffective with sources such as fluorescent or discharge lamps, whose light varies in color and may be harder to correct for. Because this light is often greenish, a magenta filter may correct it. More sophisticated colorimetry tools can be used if such meters are lacking.\nDesktop publishing.\nIn the desktop publishing industry, it is important to know a monitor's color temperature. Color matching software, such as Apple's ColorSync Utility for MacOS, measures a monitor's color temperature and then adjusts its settings accordingly. This enables on-screen color to more closely match printed color. Common monitor color temperatures, along with matching standard illuminants in parentheses, are as follows:\nD50 is scientific shorthand for a standard illuminant: the daylight spectrum at a correlated color temperature of 5000\u00a0K. Similar definitions exist for D55, D65 and D75. Designations such as \"D50\" are used to help classify color temperatures of light tables and viewing booths. When viewing a color slide at a light table, it is important that the light be balanced properly so that the colors are not shifted towards the red or blue.\nDigital cameras, web graphics, DVDs, etc., are normally designed for a 6500\u00a0K color temperature. The sRGB standard commonly used for images on the Internet stipulates a 6500\u00a0K display white point.\nMicrosoft Windows prior to Windows 11 are use sRGB as default display color space, and use 6500 K as default display color temperature; this can be override by the GPU driver; ambient light sensors found on many new laptops can also adjust the display color temperature automatically. Windows 11 22H2 have supports for Auto Color Management (ACM) which further optimized for OLED monitors by reading EDID data.\nTV, video, and digital still cameras.\nThe NTSC and PAL TV norms call for a compliant TV screen to display an electrically black and white signal (minimal color saturation) at a color temperature of 6500\u00a0K. On many consumer-grade televisions, there is a very noticeable deviation from this requirement. However, higher-end consumer-grade televisions can have their color temperatures adjusted to 6500\u00a0K by using a preprogrammed setting or a custom calibration. Current versions of ATSC explicitly call for the color temperature data to be included in the data stream, but old versions of ATSC allowed this data to be omitted. In this case, current versions of ATSC cite default colorimetry standards depending on the format. Both of the cited standards specify a 6500\u00a0K color temperature.\nMost video and digital still cameras can adjust for color temperature by zooming into a white or neutral colored object and setting the manual \"white balance\" (telling the camera that \"this object is white\"); the camera then shows true white as white and adjusts all the other colors accordingly. White-balancing is necessary especially when indoors under fluorescent lighting and when moving the camera from one lighting situation to another. Most cameras also have an automatic white balance function that attempts to determine the color of the light and correct accordingly. While these settings were once unreliable, they are much improved in today's digital cameras and produce an accurate white balance in a wide variety of lighting situations.\nHowever, in NTSC-J and NTSC-C standards, 9300 K color temperature is recommended. TVs and projectors sold in Japan, South Korea, China, Hong Kong, Taiwan and Philippines are usually adopt 9300 K as default settings. But for compatibility reasons, computer monitors sold in these country/region are usually adopt 6500 K as default settings; these color temperature settings are usually tuneable in OSD menu.\nArtistic application via control of color temperature.\nVideo camera operators can white-balance objects that are not white, downplaying the color of the object used for white-balancing. For instance, they can bring more warmth into a picture by white-balancing off something that is light blue, such as faded blue denim; in this way white-balancing can replace a filter or lighting gel when those are not available.\nCinematographers do not \"white balance\" in the same way as video camera operators; they use techniques such as filters, choice of film stock, pre-flashing, and, after shooting, color grading, both by exposure at the labs and also digitally. Cinematographers also work closely with set designers and lighting crews to achieve the desired color effects.\nFor artists, most pigments and papers have a cool or warm cast, as the human eye can detect even a minute amount of saturation. Gray mixed with yellow, orange, or red is a \"warm gray\". Green, blue, or purple create \"cool grays\". This sense of temperature is the reverse of that of real temperature; bluer is described as \"cooler\" even though it corresponds to a higher-temperature black body.\nLighting designers sometimes select filters by color temperature, commonly to match light that is theoretically white. Since fixtures using discharge type lamps produce a light of a considerably higher color temperature than do tungsten lamps, using the two in conjunction could potentially produce a stark contrast, so sometimes fixtures with HID lamps, commonly producing light of 6000\u20137000\u00a0K, are fitted with 3200\u00a0K filters to emulate tungsten light. Fixtures with color mixing features or with multiple colors (if including 3200\u00a0K), are also capable of producing tungsten-like light. Color temperature may also be a factor when selecting lamps, since each is likely to have a different color temperature.\nColor rendering index.\nThe CIE color rendering index (CRI) is a method to determine how well a light source's illumination of eight sample patches compares to the illumination provided by a reference source. Cited together, the CRI and CCT give a numerical estimate of what reference (ideal) light source best approximates a particular artificial light, and what the difference is.\nSpectral power distribution.\nLight sources and illuminants may be characterized by their spectral power distribution (SPD). The relative SPD curves provided by many manufacturers may have been produced using 10\u00a0nm increments or more on their spectroradiometer. The result is what would seem to be a smoother (\"fuller spectrum\") power distribution than the lamp actually has. Owing to their spiky distribution, much finer increments are advisable for taking measurements of fluorescent lights, and this requires more expensive equipment.\nColor temperature in astronomy.\nIn astronomy, the color temperature is defined by the local slope of the SPD at a given wavelength, or, in practice, a wavelength range. Given, for example, the color magnitudes \"B\" and \"V\" which are calibrated to be equal for an A0V star (e.g. Vega), the stellar color temperature formula_1 is given by the temperature for which the color index formula_2 of a black-body radiator fits the stellar one. Besides the formula_2, other color indices can be used as well. The color temperature (as well as the correlated color temperature defined above) may differ largely from the effective temperature given by the radiative flux of the stellar surface. For example, the color temperature of an A0V star is about 15000\u00a0K compared to an effective temperature of about 9500\u00a0K.\nFor most applications in astronomy (e.g., to place a star on the HR diagram or to determine the temperature of a model flux fitting an observed spectrum) the effective temperature is the quantity of interest. Various color-effective temperature relations exist in the literature. There relations also have smaller dependencies on other stellar parameters, such as the stellar metallicity and surface gravity"}
{"id": "7165", "revid": "46742609", "url": "https://en.wikipedia.org/wiki?curid=7165", "title": "Cartoon", "text": "A cartoon is a type of visual art that is typically drawn, frequently animated, in an unrealistic or semi-realistic style. The specific meaning has evolved, but the modern usage usually refers to either: an image or series of images intended for satire, caricature, or humor; or a motion picture that relies on a sequence of illustrations for its animation. Someone who creates cartoons in the first sense is called a \"cartoonist\", and in the second sense they are usually called an \"animator\".\nThe concept originated in the Middle Ages, and first described a preparatory drawing for a piece of art, such as a painting, fresco, tapestry, or stained glass window. In the 19th century, beginning in \"Punch\" magazine in 1843, cartoon came to refer \u2013 ironically at first \u2013 to humorous artworks in magazines and newspapers. Then it also was used for political cartoons and comic strips. When the medium developed, in the early 20th century, it began to refer to animated films that resembled print cartoons.\nFine art.\nIn fine art, a cartoon (from and \u2014words describing strong, heavy paper or pasteboard and cognates for carton) is a full-size drawing made on sturdy paper as a design or \"modello\" for a painting, stained glass, or tapestry. Cartoons were typically used in the production of frescoes, to accurately link the component parts of the composition when painted on damp plaster over a series of days (\"giornate\"). In media such as stained tapestry or stained glass, the cartoon was handed over by the artist to the skilled craftsmen who produced the final work. \nSuch cartoons often have pinpricks along the outlines of the design so that a bag of soot patted or \"pounced\" over a cartoon, held against the wall, would leave black dots on the plaster (\"pouncing\"). Cartoons by painters, such as the Raphael Cartoons in London, Francisco Goya's tapestry cartoons, and examples by Leonardo da Vinci, are highly prized in their own right. Tapestry cartoons, usually colored, could be placed behind the loom, where the weaver would replicate the design. As tapestries are worked from behind, a mirror could be placed behind the loom to allow the weaver to see their work; in such cases the cartoon was placed behind the weaver.\nMass media.\nIn print media, a cartoon is a drawing or series of drawings, usually humorous in intent. This usage dates from 1843, when \"Punch\" magazine applied the term to satirical drawings in its pages, particularly sketches by John Leech. The first of these parodied the preparatory cartoons for grand historical frescoes in the then-new Palace of Westminster in London. \nSir John Tenniel\u2014illustrator of \"Alice's Adventures in Wonderland\u2014\"joined \"Punch\" in 1850, and over 50 years contributed over two thousand cartoons.\nCartoons can be divided into gag cartoons, which include editorial cartoons, and comic strips.\nModern single-panel gag cartoons, found in magazines, generally consist of a single drawing with a typeset caption positioned beneath, or, less often, a speech balloon. Newspaper syndicates have also distributed single-panel gag cartoons by Mel Calman, Bill Holman, Gary Larson, George Lichty, Fred Neher and others. Many consider \"New Yorker\" cartoonist Peter Arno the father of the modern gag cartoon (as did Arno himself). The roster of magazine gag cartoonists includes Charles Addams, Charles Barsotti, and Chon Day.\nBill Hoest, Jerry Marcus, and Virgil Partch began as magazine gag cartoonists and moved to syndicated comic strips. Richard Thompson illustrated numerous feature articles in \"The Washington Post\" before creating his \"Cul de Sac\" comic strip. The sports section of newspapers usually featured cartoons, sometimes including syndicated features such as Chester \"Chet\" Brown's \"All in Sport\".\n\"Editorial cartoons\" are found almost exclusively in news publications and news websites. Although they also employ humor, they are more serious in tone, commonly using irony or satire. The art usually acts as a visual metaphor to illustrate a point of view on current social or political topics. Editorial cartoons often include speech balloons and sometimes use multiple panels. Editorial cartoonists of note include Herblock, David Low, Jeff MacNelly, Mike Peters, and Gerald Scarfe.\n\"Comic strips\", also known as \"cartoon strips\" in the United Kingdom, are found daily in newspapers worldwide, and are usually a short series of cartoon illustrations in sequence. In the United States, they are not commonly called \"cartoons\" themselves, but rather \"comics\" or \"funnies\". Nonetheless, the creators of comic strips\u2014as well as comic books and graphic novels\u2014are usually referred to as \"cartoonists\". Although humor is the most prevalent subject matter, adventure and drama are also represented in this medium. Some noteworthy cartoonists of humorous comic strips are Scott Adams, Charles Schulz, E. C. Segar, Mort Walker and Bill Watterson.\nPolitical.\nPolitical cartoons are like illustrated editorials that serve visual commentaries on political events. They offer subtle criticism which are cleverly quoted with humour and satire to the extent that the criticized does not get embittered.\nThe pictorial satire of William Hogarth is regarded as a precursor to the development of political cartoons in 18th century England. George Townshend produced some of the first overtly political cartoons and caricatures in the 1750s. The medium began to develop in the latter part of the 18th century under the direction of its great exponents, James Gillray and Thomas Rowlandson, both from London. Gillray explored the use of the medium for lampooning and caricature, and has been referred to as the father of the political cartoon. By calling the king, prime ministers and generals to account for their behaviour, many of Gillray's satires were directed against George III, depicting him as a pretentious buffoon, while the bulk of his work was dedicated to ridiculing the ambitions of revolutionary France and Napoleon. George Cruikshank became the leading cartoonist in the period following Gillray, from 1815 until the 1840s. His career was renowned for his social caricatures of English life for popular publications.\nBy the mid 19th century, major political newspapers in many other countries featured cartoons commenting on the politics of the day. Thomas Nast, in New York City, showed how realistic German drawing techniques could redefine American cartooning. His 160 cartoons relentlessly pursued the criminal characteristic of the Tweed machine in New York City, and helped bring it down. Indeed, Tweed was arrested in Spain when police identified him from Nast's cartoons. In Britain, Sir John Tenniel was the toast of London. In France under the July Monarchy, Honor\u00e9 Daumier took up the new genre of political and social caricature, most famously lampooning the rotund King Louis Philippe.\nPolitical cartoons can be humorous or satirical, sometimes with piercing effect. The target of the humor may complain, but can seldom fight back. Lawsuits have been very rare; the first successful lawsuit against a cartoonist in over a century in Britain came in 1921, when J. H. Thomas, the leader of the National Union of Railwaymen (NUR), initiated libel proceedings against the magazine of the British Communist Party. Thomas claimed defamation in the form of cartoons and words depicting the events of \"Black Friday\", when he allegedly betrayed the locked-out Miners' Federation. To Thomas, the framing of his image by the far left threatened to grievously degrade his character in the popular imagination. Soviet-inspired communism was a new element in European politics, and cartoonists unrestrained by tradition tested the boundaries of libel law. Thomas won the lawsuit and restored his reputation.\nScientific.\nCartoons such as \"xkcd\" have also found their place in the world of science, mathematics, and technology. For example, the cartoon \"Wonderlab\" looked at daily life in the chemistry lab. In the U.S., one well-known cartoonist for these fields is Sidney Harris. Many of Gary Larson's cartoons have a scientific flavor.\nComic books.\nThe first comic-strip cartoons were of a humorous tone. Notable early humor comics include the Swiss comic-strip book \"Mr. Vieux Bois\" (1837), the British strip \"Ally Sloper\" (first appearing in 1867) and the American strip \"Yellow Kid\" (first appearing in 1895).\nIn the United States in the 1930s, books with cartoons were magazine-format \"American comic books\" with original material, or occasionally reprints of newspaper comic strips.\nIn Britain in the 1930s, adventure comic magazines became quite popular, especially those published by DC Thomson; the publisher sent observers around the country to talk to boys and learn what they wanted to read about. The story line in magazines, comic books and cinema that most appealed to boys was the glamorous heroism of British soldiers fighting wars that were exciting and just. DC Thomson issued the first \"The Dandy Comic\" in December 1937. It had a revolutionary design that broke away from the usual children's comics that were published broadsheet in size and not very colourful. Thomson capitalized on its success with a similar product \"The Beano\" in 1938.\nOn some occasions, new gag cartoons have been created for book publication.\nAnimation.\nBecause of the stylistic similarities between comic strips and early animated films, \"cartoon\" came to refer to animation, and the word \"cartoon\" is currently used in reference to both animated cartoons and gag cartoons. While \"animation\" designates any style of illustrated images seen in rapid succession to give the impression of movement, the word \"cartoon\" is most often used as a descriptor for television programs and short films aimed at children, possibly featuring anthropomorphized animals, superheroes, the adventures of child protagonists or related themes."}
{"id": "7167", "revid": "1270067533", "url": "https://en.wikipedia.org/wiki?curid=7167", "title": "Chief Minister of the Northern Territory", "text": "The chief minister of the Northern Territory is the head of government of the Northern Territory. The office is the equivalent of a state premier. \nWhen the Northern Territory Legislative Assembly was created in 1974, the head of government was officially known as majority leader. This title was used in the first parliament (1974\u20131977) and the first eighteen months of the second. When the Northern Territory acquired limited self-government in 1978, the title of the head of government became chief minister with greatly expanded powers, though still somewhat less than those of a state premier.\nThe chief minister is formally appointed by the administrator, who in normal circumstances will appoint the head of whichever party holds the majority of seats in the unicameral Legislative Assembly. In times of constitutional crisis, the administrator can appoint someone else as chief minister, though this has never occurred.\nSince 28 August 2024, following the 2024 Northern Territory general election, the chief minister is Lia Finocchiaro of the Country Liberal Party. She is the fourth female chief minister of the Northern Territory.\nHistory.\nThe Country Liberal Party won the first Northern Territory election on 19 October 1974 and elected Goff Letts majority leader. He headed an Executive that carried out most of the functions of a ministry at the state level. At the 1977 election Letts lost his seat and party leadership. He was succeeded on 13 August 1977 by Paul Everingham (CLP) as Majority Leader. When the Territory attained self-government on 1 July 1978, Everingham became chief minister and his Executive became a Ministry.\nDespite the Majority Leader's title, the Majority Leader's opposite number was not known as Minority Leader but instead the Leader of the Opposition.\nIn 2001, Clare Martin became the first Labor and female chief minister of the Northern Territory. Until 2004 the conduct of elections and drawing of electoral boundaries was performed by the Northern Territory Electoral Office, a unit of the Department of the chief minister. In March 2004 the independent Northern Territory Electoral Commission was established.\nIn 2013, Mills was replaced as chief minister and CLP leader by Adam Giles at the 2013 CLP leadership ballot on 13 March to become the first indigenous Australian to lead a state or territory government in Australia.\nFollowing the 2016 election landslide outcome, Labor's Michael Gunner became chief minister; he was the first Chief Minister who was born in the Northern Territory. On 10 May 2022, Gunner announced his intention to resign. On 13 May 2022, Natasha Fyles was elected to the position by the Labor caucus. On 19 December 2023, Fyles resigned following controversy over undeclared shares in mining company South32. On 21 December 2023, Eva Lawler replaced Fyles by a unanimous decision of the Labor caucus.\nList of chief ministers of the Northern Territory.\nFrom the foundation of the Northern Territory Legislative Assembly in 1974 until the granting of self-government in 1978, the head of government was known as the majority leader:\nFrom 1978, the position was known as the chief minister:"}
{"id": "7170", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7170", "title": "Chinese exclusion", "text": ""}
{"id": "7172", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=7172", "title": "Chemotherapy", "text": "Chemotherapy (often abbreviated chemo, sometimes CTX and CTx) is the type of cancer treatment that uses one or more anti-cancer drugs (chemotherapeutic agents or alkylating agents) in a standard regimen. Chemotherapy may be given with a curative intent (which almost always involves combinations of drugs), or it may aim only to prolong life or to reduce symptoms (palliative chemotherapy). Chemotherapy is one of the major categories of the medical discipline specifically devoted to pharmacotherapy for cancer, which is called \"medical oncology\".\nThe term \"chemotherapy\" now means the non-specific use of intracellular poisons to inhibit mitosis (cell division) or to induce DNA damage (so that DNA repair can augment chemotherapy). This meaning excludes the more-selective agents that block extracellular signals (signal transduction). Therapies with specific molecular or genetic targets, which inhibit growth-promoting signals from classic endocrine hormones (primarily estrogens for breast cancer and androgens for prostate cancer), are now called \"hormonal therapies\". Other inhibitions of growth-signals, such as those associated with receptor tyrosine kinases, are \"targeted therapy\".\nThe use of drugs (whether chemotherapy, hormonal therapy, or targeted therapy) is \"systemic therapy\" for cancer: they are introduced into the blood stream (the system) and therefore can treat cancer anywhere in the body. Systemic therapy is often used with other, \"local therapy\" (treatments that work only where they are applied), such as radiation, surgery, and hyperthermia.\nTraditional chemotherapeutic agents are cytotoxic by means of interfering with cell division (mitosis) but cancer cells vary widely in their susceptibility to these agents. To a large extent, chemotherapy can be thought of as a way to damage or stress cells, which may then lead to cell death if apoptosis is initiated. Many of the side effects of chemotherapy can be traced to damage to normal cells that divide rapidly and are thus sensitive to anti-mitotic drugs: cells in the bone marrow, digestive tract and hair follicles. This results in the most common side-effects of chemotherapy: myelosuppression (decreased production of blood cells, hence that also immunosuppression), mucositis (inflammation of the lining of the digestive tract), and alopecia (hair loss). Because of the effect on immune cells (especially lymphocytes), chemotherapy drugs often find use in a host of diseases that result from harmful overactivity of the immune system against self (so-called autoimmunity). These include rheumatoid arthritis, systemic lupus erythematosus, multiple sclerosis, vasculitis and many others.\nTreatment strategies.\nThere are a number of strategies in the administration of chemotherapeutic drugs used today. Chemotherapy may be given with a curative intent or it may aim to prolong life or to palliate symptoms.\nAll chemotherapy regimens require that the recipient be capable of undergoing the treatment. Performance status is often used as a measure to determine whether a person can receive chemotherapy, or whether dose reduction is required. Because only a fraction of the cells in a tumor die with each treatment (fractional kill), repeated doses must be administered to continue to reduce the size of the tumor. Current chemotherapy regimens apply drug treatment in cycles, with the frequency and duration of treatments limited by toxicity.\nEffectiveness.\nThe effectiveness of chemotherapy depends on the type of cancer and the stage. The overall effectiveness ranges from being curative for some cancers, such as some leukemias, to being ineffective, such as in some brain tumors, to being needless in others, like most non-melanoma skin cancers.\nDosage.\nDosage of chemotherapy can be difficult: If the dose is too low, it will be ineffective against the tumor, whereas, at excessive doses, the toxicity (side-effects) will be intolerable to the person receiving it. The standard method of determining chemotherapy dosage is based on calculated body surface area (BSA). The BSA is usually calculated with a mathematical formula or a nomogram, using the recipient's weight and height, rather than by direct measurement of body area. This formula was originally derived in a 1916 study and attempted to translate medicinal doses established with laboratory animals to equivalent doses for humans. The study only included nine human subjects. When chemotherapy was introduced in the 1950s, the BSA formula was adopted as the official standard for chemotherapy dosing for lack of a better option.\nThe validity of this method in calculating uniform doses has been questioned because the formula only takes into account the individual's weight and height. Drug absorption and clearance are influenced by multiple factors, including age, sex, metabolism, disease state, organ function, drug-to-drug interactions, genetics, and obesity, which have major impacts on the actual concentration of the drug in the person's bloodstream. As a result, there is high variability in the systemic chemotherapy drug concentration in people dosed by BSA, and this variability has been demonstrated to be more than ten-fold for many drugs. In other words, if two people receive the same dose of a given drug based on BSA, the concentration of that drug in the bloodstream of one person may be 10 times higher or lower compared to that of the other person. This variability is typical with many chemotherapy drugs dosed by BSA, and, as shown below, was demonstrated in a study of 14 common chemotherapy drugs.\nThe result of this pharmacokinetic variability among people is that many people do not receive the right dose to achieve optimal treatment effectiveness with minimized toxic side effects. Some people are overdosed while others are underdosed. For example, in a randomized clinical trial, investigators found 85% of metastatic colorectal cancer patients treated with 5-fluorouracil (5-FU) did not receive the optimal therapeutic dose when dosed by the BSA standard\u201468% were underdosed and 17% were overdosed.\nThere has been controversy over the use of BSA to calculate chemotherapy doses for people who are obese. Because of their higher BSA, clinicians often arbitrarily reduce the dose prescribed by the BSA formula for fear of overdosing. In many cases, this can result in sub-optimal treatment.\nSeveral clinical studies have demonstrated that when chemotherapy dosing is individualized to achieve optimal systemic drug exposure, treatment outcomes are improved and toxic side effects are reduced. In the 5-FU clinical study cited above, people whose dose was adjusted to achieve a pre-determined target exposure realized an 84% improvement in treatment response rate and a six-month improvement in overall survival (OS) compared with those dosed by BSA.\nIn the same study, investigators compared the incidence of common 5-FU-associated grade 3/4 toxicities between the dose-adjusted people and people dosed per BSA. The incidence of debilitating grades of diarrhea was reduced from 18% in the BSA-dosed group to 4% in the dose-adjusted group and serious hematologic side effects were eliminated. Because of the reduced toxicity, dose-adjusted patients were able to be treated for longer periods of time. BSA-dosed people were treated for a total of 680 months while people in the dose-adjusted group were treated for a total of 791 months. Completing the course of treatment is an important factor in achieving better treatment outcomes.\nSimilar results were found in a study involving people with colorectal cancer who have been treated with the popular FOLFOX regimen. The incidence of serious diarrhea was reduced from 12% in the BSA-dosed group of patients to 1.7% in the dose-adjusted group, and the incidence of severe mucositis was reduced from 15% to 0.8%.\nThe FOLFOX study also demonstrated an improvement in treatment outcomes. Positive response increased from 46% in the BSA-dosed group to 70% in the dose-adjusted group. Median progression free survival (PFS) and overall survival (OS) both improved by six months in the dose adjusted group.\nOne approach that can help clinicians individualize chemotherapy dosing is to measure the drug levels in blood plasma over time and adjust dose according to a formula or algorithm to achieve optimal exposure. With an established target exposure for optimized treatment effectiveness with minimized toxicities, dosing can be personalized to achieve target exposure and optimal results for each person. Such an algorithm was used in the clinical trials cited above and resulted in significantly improved treatment outcomes.\nOncologists are already individualizing dosing of some cancer drugs based on exposure. Carboplatin and busulfan dosing rely upon results from blood tests to calculate the optimal dose for each person. Simple blood tests are also available for dose optimization of methotrexate, 5-FU, paclitaxel, and docetaxel.\nThe serum albumin level immediately prior to chemotherapy administration is an independent prognostic predictor of survival in various cancer types.\nTypes.\nAlkylating agents.\nAlkylating agents are the oldest group of chemotherapeutics in use today. Originally derived from mustard gas used in World War I, there are now many types of alkylating agents in use. They are so named because of their ability to alkylate many molecules, including proteins, RNA and DNA. This ability to bind covalently to DNA via their alkyl group is the primary cause for their anti-cancer effects. DNA is made of two strands and the molecules may either bind twice to one strand of DNA (intrastrand crosslink) or may bind once to both strands (interstrand crosslink). If the cell tries to replicate crosslinked DNA during cell division, or tries to repair it, the DNA strands can break. This leads to a form of programmed cell death called apoptosis. Alkylating agents will work at any point in the cell cycle and thus are known as cell cycle-independent drugs. For this reason, the effect on the cell is dose dependent; the fraction of cells that die is directly proportional to the dose of drug.\nThe subtypes of alkylating agents are the nitrogen mustards, nitrosoureas, tetrazines, aziridines, cisplatins and derivatives, and non-classical alkylating agents. Nitrogen mustards include mechlorethamine, cyclophosphamide, melphalan, chlorambucil, ifosfamide and busulfan. Nitrosoureas include N-Nitroso-N-methylurea (MNU), carmustine (BCNU), lomustine (CCNU) and semustine (MeCCNU), fotemustine and streptozotocin. Tetrazines include dacarbazine, mitozolomide and temozolomide. Aziridines include thiotepa, mytomycin and diaziquone (AZQ). Cisplatin and derivatives include cisplatin, carboplatin and oxaliplatin. They impair cell function by forming covalent bonds with the amino, carboxyl, sulfhydryl, and phosphate groups in biologically important molecules. Non-classical alkylating agents include procarbazine and hexamethylmelamine.\nAntimetabolites.\nAnti-metabolites are a group of molecules that impede DNA and RNA synthesis. Many of them have a similar structure to the building blocks of DNA and RNA. The building blocks are nucleotides; a molecule comprising a nucleobase, a sugar and a phosphate group. The nucleobases are divided into purines (guanine and adenine) and pyrimidines (cytosine, thymine and uracil). Anti-metabolites resemble either nucleobases or nucleosides (a nucleotide without the phosphate group), but have altered chemical groups. These drugs exert their effect by either blocking the enzymes required for DNA synthesis or becoming incorporated into DNA or RNA. By inhibiting the enzymes involved in DNA synthesis, they prevent mitosis because the DNA cannot duplicate itself. Also, after misincorporation of the molecules into DNA, DNA damage can occur and programmed cell death (apoptosis) is induced. Unlike alkylating agents, anti-metabolites are cell cycle dependent. This means that they only work during a specific part of the cell cycle, in this case S-phase (the DNA synthesis phase). For this reason, at a certain dose, the effect plateaus and proportionally no more cell death occurs with increased doses. Subtypes of the anti-metabolites are the anti-folates, fluoropyrimidines, deoxynucleoside analogues and thiopurines.\nThe anti-folates include methotrexate and pemetrexed. Methotrexate inhibits dihydrofolate reductase (DHFR), an enzyme that regenerates tetrahydrofolate from dihydrofolate. When the enzyme is inhibited by methotrexate, the cellular levels of folate coenzymes diminish. These are required for thymidylate and purine production, which are both essential for DNA synthesis and cell division. Pemetrexed is another anti-metabolite that affects purine and pyrimidine production, and therefore also inhibits DNA synthesis. It primarily inhibits the enzyme thymidylate synthase, but also has effects on DHFR, aminoimidazole carboxamide ribonucleotide formyltransferase and glycinamide ribonucleotide formyltransferase. The fluoropyrimidines include fluorouracil and capecitabine. Fluorouracil is a nucleobase analogue that is metabolised in cells to form at least two active products; 5-fluourouridine monophosphate (FUMP) and 5-fluoro-2'-deoxyuridine 5'-phosphate (fdUMP). FUMP becomes incorporated into RNA and fdUMP inhibits the enzyme thymidylate synthase; both of which lead to cell death. Capecitabine is a prodrug of 5-fluorouracil that is broken down in cells to produce the active drug. The deoxynucleoside analogues include cytarabine, gemcitabine, decitabine, azacitidine, fludarabine, nelarabine, cladribine, clofarabine, and pentostatin. The thiopurines include thioguanine and mercaptopurine.\nAnti-microtubule agents.\nAnti-microtubule agents are plant-derived chemicals that block cell division by preventing microtubule function. Microtubules are an important cellular structure composed of two proteins, \u03b1-tubulin and \u03b2-tubulin. They are hollow, rod-shaped structures that are required for cell division, among other cellular functions. Microtubules are dynamic structures, which means that they are permanently in a state of assembly and disassembly. \"Vinca\" alkaloids and taxanes are the two main groups of anti-microtubule agents, and although both of these groups of drugs cause microtubule dysfunction, their mechanisms of action are completely opposite: \"Vinca\" alkaloids prevent the assembly of microtubules, whereas taxanes prevent their disassembly. By doing so, they can induce mitotic catastrophe in the cancer cells. Following this, cell cycle arrest occurs, which induces programmed cell death (apoptosis). These drugs can also affect blood vessel growth, an essential process that tumours utilise in order to grow and metastasise.\n\"Vinca\" alkaloids are derived from the Madagascar periwinkle, \"Catharanthus roseus\", formerly known as \"Vinca rosea\". They bind to specific sites on tubulin, inhibiting the assembly of tubulin into microtubules. The original \"vinca\" alkaloids are natural products that include vincristine and vinblastine. Following the success of these drugs, semi-synthetic \"vinca\" alkaloids were produced: vinorelbine (used in the treatment of non-small-cell lung cancer), vindesine, and vinflunine. These drugs are cell cycle-specific. They bind to the tubulin molecules in S-phase and prevent proper microtubule formation required for M-phase.\nTaxanes are natural and semi-synthetic drugs. The first drug of their class, paclitaxel, was originally extracted from \"Taxus brevifolia\", the Pacific yew. Now this drug and another in this class, docetaxel, are produced semi-synthetically from a chemical found in the bark of another yew tree, \"Taxus baccata\".\nPodophyllotoxin is an antineoplastic lignan obtained primarily from the American mayapple (\"Podophyllum peltatum\") and Himalayan mayapple (\"Sinopodophyllum hexandrum\"). It has anti-microtubule activity, and its mechanism is similar to that of \"vinca\" alkaloids in that they bind to tubulin, inhibiting microtubule formation. Podophyllotoxin is used to produce two other drugs with different mechanisms of action: etoposide and teniposide.\nTopoisomerase inhibitors.\nTopoisomerase inhibitors are drugs that affect the activity of two enzymes: topoisomerase I and topoisomerase II. When the DNA double-strand helix is unwound, during DNA replication or transcription, for example, the adjacent unopened DNA winds tighter (supercoils), like opening the middle of a twisted rope. The stress caused by this effect is in part aided by the topoisomerase enzymes. They produce single- or double-strand breaks into DNA, reducing the tension in the DNA strand. This allows the normal unwinding of DNA to occur during replication or transcription. Inhibition of topoisomerase I or II interferes with both of these processes.\nTwo topoisomerase I inhibitors, irinotecan and topotecan, are semi-synthetically derived from camptothecin, which is obtained from the Chinese ornamental tree \"Camptotheca acuminata\". Drugs that target topoisomerase II can be divided into two groups. The topoisomerase II poisons cause increased levels enzymes bound to DNA. This prevents DNA replication and transcription, causes DNA strand breaks, and leads to programmed cell death (apoptosis). These agents include etoposide, doxorubicin, mitoxantrone and teniposide. The second group, catalytic inhibitors, are drugs that block the activity of topoisomerase II, and therefore prevent DNA synthesis and translation because the DNA cannot unwind properly. This group includes novobiocin, merbarone, and aclarubicin, which also have other significant mechanisms of action.\nCytotoxic antibiotics.\nThe cytotoxic antibiotics are a varied group of drugs that have various mechanisms of action. The common theme that they share in their chemotherapy indication is that they interrupt cell division. The most important subgroup is the anthracyclines and the bleomycins; other prominent examples include mitomycin C and actinomycin.\nAmong the anthracyclines, doxorubicin and daunorubicin were the first, and were obtained from the bacterium \"Streptomyces peucetius\". Derivatives of these compounds include epirubicin and idarubicin. Other clinically used drugs in the anthracycline group are pirarubicin, aclarubicin, and mitoxantrone. The mechanisms of anthracyclines include DNA intercalation (molecules insert between the two strands of DNA), generation of highly reactive free radicals that damage intercellular molecules and topoisomerase inhibition.\nActinomycin is a complex molecule that intercalates DNA and prevents RNA synthesis.\nBleomycin, a glycopeptide isolated from \"Streptomyces verticillus\", also intercalates DNA, but produces free radicals that damage DNA. This occurs when bleomycin binds to a metal ion, becomes chemically reduced and reacts with oxygen.\nMitomycin is a cytotoxic antibiotic with the ability to alkylate DNA.\nDelivery.\nMost chemotherapy is delivered intravenously, although a number of agents can be administered orally (e.g., melphalan, busulfan, capecitabine). According to a recent (2016) systematic review, oral therapies present additional challenges for patients and care teams to maintain and support adherence to treatment plans.\nThere are many intravenous methods of drug delivery, known as vascular access devices. These include the winged infusion device, peripheral venous catheter, midline catheter, peripherally inserted central catheter (PICC), central venous catheter and implantable port. The devices have different applications regarding duration of chemotherapy treatment, method of delivery and types of chemotherapeutic agent.\nDepending on the person, the cancer, the stage of cancer, the type of chemotherapy, and the dosage, intravenous chemotherapy may be given on either an inpatient or an outpatient basis. For continuous, frequent or prolonged intravenous chemotherapy administration, various systems may be surgically inserted into the vasculature to maintain access. Commonly used systems are the Hickman line, the Port-a-Cath, and the PICC line. These have a lower infection risk, are much less prone to phlebitis or extravasation, and eliminate the need for repeated insertion of peripheral cannulae.\nIsolated limb perfusion (often used in melanoma), or isolated infusion of chemotherapy into the liver or the lung have been used to treat some tumors. The main purpose of these approaches is to deliver a very high dose of chemotherapy to tumor sites without causing overwhelming systemic damage. These approaches can help control solitary or limited metastases, but they are by definition not systemic, and, therefore, do not treat distributed metastases or micrometastases.\nTopical chemotherapies, such as 5-fluorouracil, are used to treat some cases of non-melanoma skin cancer.\nIf the cancer has central nervous system involvement, or with meningeal disease, intrathecal chemotherapy may be administered.\nAdverse effects.\nChemotherapeutic techniques have a range of side effects that depend on the type of medications used. The most common medications affect mainly the fast-dividing cells of the body, such as blood cells and the cells lining the mouth, stomach, and intestines. Chemotherapy-related iatrogenic toxicities can occur acutely after administration, within hours or days, or chronically, from weeks to years.\nImmunosuppression and myelosuppression.\nVirtually all chemotherapeutic regimens can cause depression of the immune system, often by paralysing the bone marrow and leading to a decrease of white blood cells, red blood cells, and platelets.\nAnemia and thrombocytopenia may require blood transfusion. Neutropenia (a decrease of the neutrophil granulocyte count below 0.5 x 109/litre) can be improved with synthetic G-CSF (granulocyte-colony-stimulating factor, e.g., filgrastim, lenograstim, efbemalenograstim alfa).\nIn very severe myelosuppression, which occurs in some regimens, almost all the bone marrow stem cells (cells that produce white and red blood cells) are destroyed, meaning \"allogenic\" or \"autologous\" bone marrow cell transplants are necessary. (In autologous BMTs, cells are removed from the person before the treatment, multiplied and then re-injected afterward; in \"allogenic\" BMTs, the source is a donor.) However, some people still develop diseases because of this interference with bone marrow.\nAlthough people receiving chemotherapy are encouraged to wash their hands, avoid sick people, and take other infection-reducing steps, about 85% of infections are due to naturally occurring microorganisms in the person's own gastrointestinal tract (including oral cavity) and skin. This may manifest as systemic infections, such as sepsis, or as localized outbreaks, such as Herpes simplex, shingles, or other members of the Herpesviridea. The risk of illness and death can be reduced by taking common antibiotics such as quinolones or trimethoprim/sulfamethoxazole before any fever or sign of infection appears. Quinolones show effective prophylaxis mainly with hematological cancer. However, in general, for every five people who are immunosuppressed following chemotherapy who take an antibiotic, one fever can be prevented; for every 34 who take an antibiotic, one death can be prevented. Sometimes, chemotherapy treatments are postponed because the immune system is suppressed to a critically low level.\nIn Japan, the government has approved the use of some medicinal mushrooms like \"Trametes versicolor\", to counteract depression of the immune system in people undergoing chemotherapy.\nTrilaciclib is an inhibitor of cyclin-dependent kinase 4/6 approved for the prevention of myelosuppression caused by chemotherapy. The drug is given before chemotherapy to protect bone marrow function.\nNeutropenic enterocolitis.\nDue to immune system suppression, neutropenic enterocolitis (typhlitis) is a \"life-threatening gastrointestinal complication of chemotherapy.\" Typhlitis is an intestinal infection which may manifest itself through symptoms including nausea, vomiting, diarrhea, a distended abdomen, fever, chills, or abdominal pain and tenderness.\nTyphlitis is a medical emergency. It has a very poor prognosis and is often fatal unless promptly recognized and aggressively treated. Successful treatment hinges on early diagnosis provided by a high index of suspicion and the use of CT scanning, nonoperative treatment for uncomplicated cases, and sometimes elective right hemicolectomy to prevent recurrence.\nGastrointestinal distress.\nNausea, vomiting, anorexia, diarrhea, abdominal cramps, and constipation are common side-effects of chemotherapeutic medications that kill fast-dividing cells. Malnutrition and dehydration can result when the recipient does not eat or drink enough, or when the person vomits frequently, because of gastrointestinal damage. This can result in rapid weight loss, or occasionally in weight gain, if the person eats too much in an effort to allay nausea or heartburn. Weight gain can also be caused by some steroid medications. These side-effects can frequently be reduced or eliminated with antiemetic drugs. Low-certainty evidence also suggests that probiotics may have a preventative and treatment effect of diarrhoea related to chemotherapy alone and with radiotherapy. However, a high index of suspicion is appropriate, since diarrhoea and bloating are also symptoms of typhlitis, a very serious and potentially life-threatening medical emergency that requires immediate treatment.\nAnemia.\nAnemia can be a combined outcome caused by myelosuppressive chemotherapy, and possible cancer-related causes such as bleeding, blood cell destruction (hemolysis), hereditary disease, kidney dysfunction, nutritional deficiencies or anemia of chronic disease. Treatments to mitigate anemia include hormones to boost blood production (erythropoietin), iron supplements, and blood transfusions. Myelosuppressive therapy can cause a tendency to bleed easily, leading to anemia. Medications that kill rapidly dividing cells or blood cells can reduce the number of platelets in the blood, which can result in bruises and bleeding. Extremely low platelet counts may be temporarily boosted through platelet transfusions and new drugs to increase platelet counts during chemotherapy are being developed. Sometimes, chemotherapy treatments are postponed to allow platelet counts to recover.\nFatigue may be a consequence of the cancer or its treatment, and can last for months to years after treatment. One physiological cause of fatigue is anemia, which can be caused by chemotherapy, surgery, radiotherapy, primary and metastatic disease or nutritional depletion. Aerobic exercise has been found to be beneficial in reducing fatigue in people with solid tumours.\nNausea and vomiting.\nNausea and vomiting are two of the most feared cancer treatment-related side-effects for people with cancer and their families. In 1983, Coates et al. found that people receiving chemotherapy ranked nausea and vomiting as the first and second most severe side-effects, respectively. Up to 20% of people receiving highly emetogenic agents in this era postponed, or even refused potentially curative treatments. Chemotherapy-induced nausea and vomiting (CINV) are common with many treatments and some forms of cancer. Since the 1990s, several novel classes of antiemetics have been developed and commercialized, becoming a nearly universal standard in chemotherapy regimens, and helping to successfully manage these symptoms in many people. Effective mediation of these unpleasant and sometimes debilitating symptoms results in increased quality of life for the recipient and more efficient treatment cycles, as patients are less likely to avoid or refuse treatment.\nHair loss.\nHair loss (alopecia) can be caused by chemotherapy that kills rapidly dividing cells; other medications may cause hair to thin. These are most often temporary effects: hair usually starts to regrow a few weeks after the last treatment, but sometimes with a change in color, texture, thickness or style. Sometimes hair has a tendency to curl after regrowth, resulting in \"chemo curls.\" Severe hair loss occurs most often with drugs such as doxorubicin, daunorubicin, paclitaxel, docetaxel, cyclophosphamide, ifosfamide and etoposide. Permanent thinning or hair loss can result from some standard chemotherapy regimens.\nChemotherapy induced hair loss occurs by a non-androgenic mechanism, and can manifest as alopecia totalis, telogen effluvium, or less often alopecia areata. It is usually associated with systemic treatment due to the high mitotic rate of hair follicles, and more reversible than androgenic hair loss, although permanent cases can occur. Chemotherapy induces hair loss in women more often than men.\nScalp cooling offers a means of preventing both permanent and temporary hair loss; however, concerns about this method have been raised.\nSecondary neoplasm.\nDevelopment of secondary neoplasia after successful chemotherapy or radiotherapy treatment can occur. The most common secondary neoplasm is secondary acute myeloid leukemia, which develops primarily after treatment with alkylating agents or topoisomerase inhibitors. Survivors of childhood cancer are more than 13 times as likely to get a secondary neoplasm during the 30 years after treatment than the general population. Not all of this increase can be attributed to chemotherapy.\nInfertility.\nSome types of chemotherapy are gonadotoxic and may cause infertility. Chemotherapies with high risk include procarbazine and other alkylating drugs such as cyclophosphamide, ifosfamide, busulfan, melphalan, chlorambucil, and chlormethine. Drugs with medium risk include doxorubicin and platinum analogs such as cisplatin and carboplatin. On the other hand, therapies with low risk of gonadotoxicity include plant derivatives such as vincristine and vinblastine, antibiotics such as bleomycin and dactinomycin, and antimetabolites such as methotrexate, mercaptopurine, and 5-fluorouracil.\nFemale infertility by chemotherapy appears to be secondary to premature ovarian failure by loss of primordial follicles. This loss is not necessarily a direct effect of the chemotherapeutic agents, but could be due to an increased rate of growth initiation to replace damaged developing follicles.\nPeople may choose between several methods of fertility preservation prior to chemotherapy, including cryopreservation of semen, ovarian tissue, oocytes, or embryos. As more than half of cancer patients are elderly, this adverse effect is only relevant for a minority of patients. A study in France between 1999 and 2011 came to the result that embryo freezing before administration of gonadotoxic agents to females caused a delay of treatment in 34% of cases, and a live birth in 27% of surviving cases who wanted to become pregnant, with the follow-up time varying between 1 and 13 years.\nPotential protective or attenuating agents include GnRH analogs, where several studies have shown a protective effect \"in vivo\" in humans, but some studies show no such effect. Sphingosine-1-phosphate (S1P) has shown similar effect, but its mechanism of inhibiting the sphingomyelin apoptotic pathway may also interfere with the apoptosis action of chemotherapy drugs.\nIn chemotherapy as a conditioning regimen in hematopoietic stem cell transplantation, a study of people conditioned with cyclophosphamide alone for severe aplastic anemia came to the result that ovarian recovery occurred in all women younger than 26 years at time of transplantation, but only in five of 16 women older than 26 years.\nTeratogenicity.\nChemotherapy is teratogenic during pregnancy, especially during the first trimester, to the extent that abortion usually is recommended if pregnancy in this period is found during chemotherapy. Second- and third-trimester exposure does not usually increase the teratogenic risk and adverse effects on cognitive development, but it may increase the risk of various complications of pregnancy and fetal myelosuppression.\nFemale patients of reproductive potential should use effective contraception during chemotherapy and for a few months after the last dose (e.g. 6 month for doxorubicin).\nIn males previously having undergone chemotherapy or radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy. The use of assisted reproductive technologies and micromanipulation techniques might increase this risk. In females previously having undergone chemotherapy, miscarriage and congenital malformations are not increased in subsequent conceptions. However, when in vitro fertilization and embryo cryopreservation is practised between or shortly after treatment, possible genetic risks to the growing oocytes exist, and hence it has been recommended that the babies be screened.\nPeripheral neuropathy.\nBetween 30 and 40 percent of people undergoing chemotherapy experience chemotherapy-induced peripheral neuropathy (CIPN), a progressive, enduring, and often irreversible condition, causing pain, tingling, numbness and sensitivity to cold, beginning in the hands and feet and sometimes progressing to the arms and legs. Chemotherapy drugs associated with CIPN include thalidomide, epothilones, \"vinca\" alkaloids, taxanes, proteasome inhibitors, and the platinum-based drugs. Whether CIPN arises, and to what degree, is determined by the choice of drug, duration of use, the total amount consumed and whether the person already has peripheral neuropathy. Though the symptoms are mainly sensory, in some cases motor nerves and the autonomic nervous system are affected. CIPN often follows the first chemotherapy dose and increases in severity as treatment continues, but this progression usually levels off at completion of treatment. The platinum-based drugs are the exception; with these drugs, sensation may continue to deteriorate for several months after the end of treatment. Some CIPN appears to be irreversible. Pain can often be managed with drug or other treatment but the numbness is usually resistant to treatment.\nCognitive impairment.\nSome people receiving chemotherapy report fatigue or non-specific neurocognitive problems, such as an inability to concentrate; this is sometimes called post-chemotherapy cognitive impairment, referred to as \"chemo brain\" in popular and social media.\nTumor lysis syndrome.\nIn particularly large tumors and cancers with high white cell counts, such as lymphomas, teratomas, and some leukemias, some people develop tumor lysis syndrome. The rapid breakdown of cancer cells causes the release of chemicals from the inside of the cells. Following this, high levels of uric acid, potassium and phosphate are found in the blood. High levels of phosphate induce secondary hypoparathyroidism, resulting in low levels of calcium in the blood. This causes kidney damage and the high levels of potassium can cause cardiac arrhythmia. Although prophylaxis is available and is often initiated in people with large tumors, this is a dangerous side-effect that can lead to death if left untreated.\nOrgan damage.\nCardiotoxicity (heart damage) is especially prominent with the use of anthracycline drugs (doxorubicin, epirubicin, idarubicin, and liposomal doxorubicin). The cause of this is most likely due to the production of free radicals in the cell and subsequent DNA damage. Other chemotherapeutic agents that cause cardiotoxicity, but at a lower incidence, are cyclophosphamide, docetaxel and clofarabine.\nHepatotoxicity (liver damage) can be caused by many cytotoxic drugs. The susceptibility of an individual to liver damage can be altered by other factors such as the cancer itself, viral hepatitis, immunosuppression and nutritional deficiency. The liver damage can consist of damage to liver cells, hepatic sinusoidal syndrome (obstruction of the veins in the liver), cholestasis (where bile does not flow from the liver to the intestine) and liver fibrosis.\nNephrotoxicity (kidney damage) can be caused by tumor lysis syndrome and also due direct effects of drug clearance by the kidneys. Different drugs will affect different parts of the kidney and the toxicity may be asymptomatic (only seen on blood or urine tests) or may cause acute kidney injury.\nOtotoxicity (damage to the inner ear) is a common side effect of platinum based drugs that can produce symptoms such as dizziness and vertigo. Children treated with platinum analogues have been found to be at risk for developing hearing loss. \nOther side-effects.\nLess common side-effects include red skin (erythema), dry skin, damaged fingernails, a dry mouth (xerostomia), water retention, and sexual impotence. Some medications can trigger allergic or pseudoallergic reactions.\nSpecific chemotherapeutic agents are associated with organ-specific toxicities, including cardiovascular disease (e.g., doxorubicin), interstitial lung disease (e.g., bleomycin) and occasionally secondary neoplasm (e.g., MOPP therapy for Hodgkin's disease).\nHand-foot syndrome is another side effect to cytotoxic chemotherapy.\nNutritional problems are also frequently seen in cancer patients at diagnosis and through chemotherapy treatment. Research suggests that in children and young people undergoing cancer treatment, parenteral nutrition may help with this leading to weight gain and increased calorie and protein intake, when compared to enteral nutrition.\nLimitations.\nChemotherapy does not always work, and even when it is useful, it may not completely destroy the cancer. People frequently fail to understand its limitations. In one study of people who had been newly diagnosed with incurable, stage 4 cancer, more than two-thirds of people with lung cancer and more than four-fifths of people with colorectal cancer still believed that chemotherapy was likely to cure their cancer.\nThe blood\u2013brain barrier poses an obstacle to delivery of chemotherapy to the brain. This is because the brain has an extensive system in place to protect it from harmful chemicals. Drug transporters can pump out drugs from the brain and brain's blood vessel cells into the cerebrospinal fluid and blood circulation. These transporters pump out most chemotherapy drugs, which reduces their efficacy for treatment of brain tumors. Only small lipophilic alkylating agents such as lomustine or temozolomide are able to cross this blood\u2013brain barrier.\nBlood vessels in tumors are very different from those seen in normal tissues. As a tumor grows, tumor cells furthest away from the blood vessels become low in oxygen (hypoxic). To counteract this they then signal for new blood vessels to grow. The newly formed tumor vasculature is poorly formed and does not deliver an adequate blood supply to all areas of the tumor. This leads to issues with drug delivery because many drugs will be delivered to the tumor by the circulatory system.\nResistance.\nResistance is a major cause of treatment failure in chemotherapeutic drugs. There are a few possible causes of resistance in cancer, one of which is the presence of small pumps on the surface of cancer cells that actively move chemotherapy from inside the cell to the outside. Cancer cells produce high amounts of these pumps, known as p-glycoprotein, in order to protect themselves from chemotherapeutics. Research on p-glycoprotein and other such chemotherapy efflux pumps is currently ongoing. Medications to inhibit the function of p-glycoprotein are undergoing investigation, but due to toxicities and interactions with anti-cancer drugs their development has been difficult. Another mechanism of resistance is gene amplification, a process in which multiple copies of a gene are produced by cancer cells. This overcomes the effect of drugs that reduce the expression of genes involved in replication. With more copies of the gene, the drug can not prevent all expression of the gene and therefore the cell can restore its proliferative ability. Cancer cells can also cause defects in the cellular pathways of apoptosis (programmed cell death). As most chemotherapy drugs kill cancer cells in this manner, defective apoptosis allows survival of these cells, making them resistant. Many chemotherapy drugs also cause DNA damage, which can be repaired by enzymes in the cell that carry out DNA repair. Upregulation of these genes can overcome the DNA damage and prevent the induction of apoptosis. Mutations in genes that produce drug target proteins, such as tubulin, can occur which prevent the drugs from binding to the protein, leading to resistance to these types of drugs. Drugs used in chemotherapy can induce cell stress, which can kill a cancer cell; however, under certain conditions, cells stress can induce changes in gene expression that enables resistance to several types of drugs. In lung cancer, the transcription factor NF\u03baB is thought to play a role in resistance to chemotherapy, via inflammatory pathways.\nCytotoxics and targeted therapies.\nTargeted therapies are a relatively new class of cancer drugs that can overcome many of the issues seen with the use of cytotoxics. They are divided into two groups: small molecule and antibodies. The massive toxicity seen with the use of cytotoxics is due to the lack of cell specificity of the drugs. They will kill any rapidly dividing cell, tumor or normal. Targeted therapies are designed to affect cellular proteins or processes that are utilised by the cancer cells. This allows a high dose to cancer tissues with a relatively low dose to other tissues. Although the side effects are often less severe than that seen of cytotoxic chemotherapeutics, life-threatening effects can occur. Initially, the targeted therapeutics were supposed to be solely selective for one protein. Now it is clear that there is often a range of protein targets that the drug can bind. An example target for targeted therapy is the BCR-ABL1 protein produced from the Philadelphia chromosome, a genetic lesion found commonly in chronic myelogenous leukemia and in some patients with acute lymphoblastic leukemia. This fusion protein has enzyme activity that can be inhibited by imatinib, a small molecule drug.\nMechanism of action.\nCancer is the uncontrolled growth of cells coupled with malignant behaviour: invasion and metastasis (among other features). It is caused by the interaction between genetic susceptibility and environmental factors. These factors lead to accumulations of genetic mutations in oncogenes (genes that control the growth rate of cells) and tumor suppressor genes (genes that help to prevent cancer), which gives cancer cells their malignant characteristics, such as uncontrolled growth.\nIn the broad sense, most chemotherapeutic drugs work by impairing mitosis (cell division), effectively targeting fast-dividing cells. As these drugs cause damage to cells, they are termed \"cytotoxic\". They prevent mitosis by various mechanisms including damaging DNA and inhibition of the cellular machinery involved in cell division. One theory as to why these drugs kill cancer cells is that they induce a programmed form of cell death known as apoptosis.\nAs chemotherapy affects cell division, tumors with high growth rates (such as acute myelogenous leukemia and the aggressive lymphomas, including Hodgkin's disease) are more sensitive to chemotherapy, as a larger proportion of the targeted cells are undergoing cell division at any time. Malignancies with slower growth rates, such as indolent lymphomas, tend to respond to chemotherapy much more modestly. Heterogeneic tumours may also display varying sensitivities to chemotherapy agents, depending on the subclonal populations within the tumor.\nCells from the immune system also make crucial contributions to the antitumor effects of chemotherapy. For example, the chemotherapeutic drugs oxaliplatin and cyclophosphamide can cause tumor cells to die in a way that is detectable by the immune system (called immunogenic cell death), which mobilizes immune cells with antitumor functions. Chemotherapeutic drugs that cause cancer immunogenic tumor cell death can make unresponsive tumors sensitive to immune checkpoint therapy.\nOther uses.\nSome chemotherapy drugs are used in diseases other than cancer, such as in autoimmune disorders, and noncancerous plasma cell dyscrasia. In some cases they are often used at lower doses, which means that the side effects are minimized, while in other cases doses similar to ones used to treat cancer are used. Methotrexate is used in the treatment of rheumatoid arthritis (RA), psoriasis, ankylosing spondylitis and multiple sclerosis. The anti-inflammatory response seen in RA is thought to be due to increases in adenosine, which causes immunosuppression; effects on immuno-regulatory cyclooxygenase-2 enzyme pathways; reduction in pro-inflammatory cytokines; and anti-proliferative properties. Although methotrexate is used to treat both multiple sclerosis and ankylosing spondylitis, its efficacy in these diseases is still uncertain. Cyclophosphamide is sometimes used to treat lupus nephritis, a common symptom of systemic lupus erythematosus. Dexamethasone along with either bortezomib or melphalan is commonly used as a treatment for AL amyloidosis. Recently, bortezomid in combination with cyclophosphamide and dexamethasone has also shown promise as a treatment for AL amyloidosis. Other drugs used to treat myeloma such as lenalidomide have shown promise in treating AL amyloidosis.\nChemotherapy drugs are also used in conditioning regimens prior to bone marrow transplant (hematopoietic stem cell transplant). Conditioning regimens are used to suppress the recipient's immune system in order to allow a transplant to engraft. Cyclophosphamide is a common cytotoxic drug used in this manner and is often used in conjunction with total body irradiation. Chemotherapeutic drugs may be used at high doses to permanently remove the recipient's bone marrow cells (myeloablative conditioning) or at lower doses that will prevent permanent bone marrow loss (non-myeloablative and reduced intensity conditioning). When used in non-cancer setting, the treatment is still called \"chemotherapy\", and is often done in the same treatment centers used for people with cancer.\nOccupational exposure and safe handling.\nIn the 1970s, antineoplastic (chemotherapy) drugs were identified as hazardous, and the American Society of Health-System Pharmacists (ASHP) has since then introduced the concept of hazardous drugs after publishing a recommendation in 1983 regarding handling hazardous drugs. The adaptation of federal regulations came when the U.S. Occupational Safety and Health Administration (OSHA) first released its guidelines in 1986 and then updated them in 1996, 1999, and, most recently, 2006.\nThe National Institute for Occupational Safety and Health (NIOSH) has been conducting an assessment in the workplace since then regarding these drugs. Occupational exposure to antineoplastic drugs has been linked to multiple health effects, including infertility and possible carcinogenic effects. A few cases have been reported by the NIOSH alert report, such as one in which a female pharmacist was diagnosed with papillary transitional cell carcinoma. Twelve years before the pharmacist was diagnosed with the condition, she had worked for 20 months in a hospital where she was responsible for preparing multiple antineoplastic drugs. The pharmacist did not have any other risk factor for cancer, and therefore, her cancer was attributed to the exposure to the antineoplastic drugs, although a cause-and-effect relationship has not been established in the literature. Another case happened when a malfunction in biosafety cabinetry is believed to have exposed nursing personnel to antineoplastic drugs. Investigations revealed evidence of genotoxic biomarkers two and nine months after that exposure.\nRoutes of exposure.\nAntineoplastic drugs are usually given through intravenous, intramuscular, intrathecal, or subcutaneous administration. In most cases, before the medication is administered to the patient, it needs to be prepared and handled by several workers. Any worker who is involved in handling, preparing, or administering the drugs, or with cleaning objects that have come into contact with antineoplastic drugs, is potentially exposed to hazardous drugs. Health care workers are exposed to drugs in different circumstances, such as when pharmacists and pharmacy technicians prepare and handle antineoplastic drugs and when nurses and physicians administer the drugs to patients. Additionally, those who are responsible for disposing antineoplastic drugs in health care facilities are also at risk of exposure.\nDermal exposure is thought to be the main route of exposure due to the fact that significant amounts of the antineoplastic agents have been found in the gloves worn by healthcare workers who prepare, handle, and administer the agents. Another noteworthy route of exposure is inhalation of the drugs' vapors. Multiple studies have investigated inhalation as a route of exposure, and although air sampling has not shown any dangerous levels, it is still a potential route of exposure. Ingestion by hand to mouth is a route of exposure that is less likely compared to others because of the enforced hygienic standard in the health institutions. However, it is still a potential route, especially in the workplace, outside of a health institute. One can also be exposed to these hazardous drugs through injection by needle sticks. Research conducted in this area has established that occupational exposure occurs by examining evidence in multiple urine samples from health care workers.\nHazards.\nHazardous drugs expose health care workers to serious health risks. Many studies show that antineoplastic drugs could have many side effects on the reproductive system, such as fetal loss, congenital malformation, and infertility. Health care workers who are exposed to antineoplastic drugs on many occasions have adverse reproductive outcomes such as spontaneous abortions, stillbirths, and congenital malformations. Moreover, studies have shown that exposure to these drugs leads to menstrual cycle irregularities. Antineoplastic drugs may also increase the risk of learning disabilities among children of health care workers who are exposed to these hazardous substances.\nMoreover, these drugs have carcinogenic effects. In the past five decades, multiple studies have shown the carcinogenic effects of exposure to antineoplastic drugs. Similarly, there have been research studies that linked alkylating agents with humans developing leukemias. Studies have reported elevated risk of breast cancer, nonmelanoma skin cancer, and cancer of the rectum among nurses who are exposed to these drugs. Other investigations revealed that there is a potential genotoxic effect from anti-neoplastic drugs to workers in health care settings.\nSafe handling in health care settings.\nAs of 2018, there were no occupational exposure limits set for antineoplastic drugs, i.e., OSHA or the American Conference of Governmental Industrial Hygienists (ACGIH) have not set workplace safety guidelines.\nPreparation.\nNIOSH recommends using a ventilated cabinet that is designed to decrease worker exposure. Additionally, it recommends training of all staff, the use of cabinets, implementing an initial evaluation of the technique of the safety program, and wearing protective gloves and gowns when opening drug packaging, handling vials, or labeling. When wearing personal protective equipment, one should inspect gloves for physical defects before use and always wear double gloves and protective gowns. Health care workers are also required to wash their hands with water and soap before and after working with antineoplastic drugs, change gloves every 30 minutes or whenever punctured, and discard them immediately in a chemotherapy waste container.\nThe gowns used should be disposable gowns made of polyethylene-coated polypropylene. When wearing gowns, individuals should make sure that the gowns are closed and have long sleeves. When preparation is done, the final product should be completely sealed in a plastic bag.\nThe health care worker should also wipe all waste containers inside the ventilated cabinet before removing them from the cabinet. Finally, workers should remove all protective wear and put them in a bag for their disposal inside the ventilated cabinet.\nAdministration.\nDrugs should only be administered using protective medical devices such as needle lists and closed systems and techniques such as priming of IV tubing by pharmacy personnel inside a ventilated cabinet. Workers should always wear personal protective equipment such as double gloves, goggles, and protective gowns when opening the outer bag and assembling the delivery system to deliver the drug to the patient, and when disposing of all material used in the administration of the drugs.\nHospital workers should never remove tubing from an IV bag that contains an antineoplastic drug, and when disconnecting the tubing in the system, they should make sure the tubing has been thoroughly flushed. After removing the IV bag, the workers should place it together with other disposable items directly in the yellow chemotherapy waste container with the lid closed. Protective equipment should be removed and put into a disposable chemotherapy waste container. After this has been done, one should double bag the chemotherapy waste before or after removing one's inner gloves. Moreover, one must always wash one's hands with soap and water before leaving the drug administration site.\nEmployee training.\nAll employees whose jobs in health care facilities expose them to hazardous drugs must receive training. Training should include shipping and receiving personnel, housekeepers, pharmacists, assistants, and all individuals involved in the transportation and storage of antineoplastic drugs. These individuals should receive information and training to inform them of the hazards of the drugs present in their areas of work. They should be informed and trained on operations and procedures in their work areas where they can encounter hazards, different methods used to detect the presence of hazardous drugs and how the hazards are released, and the physical and health hazards of the drugs, including their reproductive and carcinogenic hazard potential. Additionally, they should be informed and trained on the measures they should take to avoid and protect themselves from these hazards. This information ought to be provided when health care workers come into contact with the drugs, that is, perform the initial assignment in a work area with hazardous drugs. Moreover, training should also be provided when new hazards emerge as well as when new drugs, procedures, or equipment are introduced.\nHousekeeping and waste disposal.\nWhen performing cleaning and decontaminating the work area where antineoplastic drugs are used, one should make sure that there is sufficient ventilation to prevent the buildup of airborne drug concentrations. When cleaning the work surface, hospital workers should use deactivation and cleaning agents before and after each activity as well as at the end of their shifts. Cleaning should always be done using double protective gloves and disposable gowns. After employees finish up cleaning, they should dispose of the items used in the activity in a yellow chemotherapy waste container while still wearing protective gloves. After removing the gloves, they should thoroughly wash their hands with soap and water. Anything that comes into contact or has a trace of the antineoplastic drugs, such as needles, empty vials, syringes, gowns, and gloves, should be put in the chemotherapy waste container.\nSpill control.\nA written policy needs to be in place in case of a spill of antineoplastic products. The policy should address the possibility of various sizes of spills as well as the procedure and personal protective equipment required for each size. A trained worker should handle a large spill and always dispose of all cleanup materials in the chemical waste container according to EPA regulations, not in a yellow chemotherapy waste container.\nOccupational monitoring.\nA medical surveillance program must be established. In case of exposure, occupational health professionals need to ask for a detailed history and do a thorough physical exam. They should test the urine of the potentially exposed worker by doing a urine dipstick or microscopic examination, mainly looking for blood, as several antineoplastic drugs are known to cause bladder damage.\nUrinary mutagenicity is a marker of exposure to antineoplastic drugs that was first used by Falck and colleagues in 1979 and uses bacterial mutagenicity assays. Apart from being nonspecific, the test can be influenced by extraneous factors such as dietary intake and smoking and is, therefore, used sparingly. However, the test played a significant role in changing the use of horizontal flow cabinets to vertical flow biological safety cabinets during the preparation of antineoplastic drugs because the former exposed health care workers to high levels of drugs. This changed the handling of drugs and effectively reduced workers' exposure to antineoplastic drugs.\nBiomarkers of exposure to antineoplastic drugs commonly include urinary platinum, methotrexate, urinary cyclophosphamide and ifosfamide, and urinary metabolite of 5-fluorouracil. In addition to this, there are other drugs used to measure the drugs directly in the urine, although they are rarely used. A measurement of these drugs directly in one's urine is a sign of high exposure levels and that an uptake of the drugs is happening either through inhalation or dermally. \u00a0\nAvailable agents.\nThere is an extensive list of antineoplastic agents. Several classification schemes have been used to subdivide the medicines used for cancer into several different types.\nHistory.\nThe first use of small-molecule drugs to treat cancer was in the early 20th century, although the specific chemicals first used were not originally intended for that purpose. Mustard gas was used as a chemical warfare agent during World War I and was discovered to be a potent suppressor of hematopoiesis (blood production). A similar family of compounds known as nitrogen mustards were studied further during World War II at the Yale School of Medicine. It was reasoned that an agent that damaged the rapidly growing white blood cells might have a similar effect on cancer. Therefore, in December 1942, several people with advanced lymphomas (cancers of the lymphatic system and lymph nodes) were given the drug by vein, rather than by breathing the irritating gas. Their improvement, although temporary, was remarkable. Concurrently, during a military operation in World War II, following a German air raid on the Italian harbour of Bari, several hundred people were accidentally exposed to mustard gas, which had been transported there by the Allied forces to prepare for possible retaliation in the event of German use of chemical warfare. The survivors were later found to have very low white blood cell counts. After WWII was over and the reports declassified, the experiences converged and led researchers to look for other substances that might have similar effects against cancer. The first chemotherapy drug to be developed from this line of research was mustine. Since then, many other drugs have been developed to treat cancer, and drug development has exploded into a multibillion-dollar industry, although the principles and limitations of chemotherapy discovered by the early researchers still apply.\nThe term \"chemotherapy\".\nThe word \"chemotherapy\" without a modifier usually refers to cancer treatment, but its historical meaning was broader. The term was coined in the early 1900s by Paul Ehrlich as meaning any use of chemicals to treat any disease (\"chemo-\" + \"-therapy\"), such as the use of antibiotics (\"antibacterial chemotherapy\"). Ehrlich was not optimistic that effective chemotherapy drugs would be found for the treatment of cancer. The first modern chemotherapeutic agent was arsphenamine, an arsenic compound discovered in 1907 and used to treat syphilis. This was later followed by sulfonamides (sulfa drugs) and penicillin. In today's usage, the sense \"any treatment of disease with drugs\" is often expressed with the word \"pharmacotherapy\".\nResearch.\nTargeted delivery vehicles.\nSpecially targeted delivery vehicles aim to increase effective levels of chemotherapy for tumor cells while reducing effective levels for other cells. This should result in an increased tumor kill or reduced toxicity or both.\nAntibody-drug conjugates.\nAntibody-drug conjugates (ADCs) comprise an antibody, drug and a linker between them. The antibody will be targeted at a preferentially expressed protein in the tumour cells (known as a tumor antigen) or on cells that the tumor can utilise, such as blood vessel endothelial cells. They bind to the tumor antigen and are internalised, where the linker releases the drug into the cell. These specially targeted delivery vehicles vary in their stability, selectivity, and choice of target, but, in essence, they all aim to increase the maximum effective dose that can be delivered to the tumor cells. Reduced systemic toxicity means that they can also be used in people who are sicker and that they can carry new chemotherapeutic agents that would have been far too toxic to deliver via traditional systemic approaches.\nThe first approved drug of this type was gemtuzumab ozogamicin (Mylotarg), released by Wyeth (now Pfizer). The drug was approved to treat acute myeloid leukemia. Two other drugs, trastuzumab emtansine and brentuximab vedotin, are both in late clinical trials, and the latter has been granted accelerated approval for the treatment of refractory Hodgkin's lymphoma and systemic anaplastic large cell lymphoma.\nNanoparticles.\nNanoparticles are 1\u20131000 nanometer (nm) sized particles that can promote tumor selectivity and aid in delivering low-solubility drugs. Nanoparticles can be targeted passively or actively. Passive targeting exploits the difference between tumor blood vessels and normal blood vessels. Blood vessels in tumors are \"leaky\" because they have gaps from 200 to 2000\u00a0nm, which allow nanoparticles to escape into the tumor. Active targeting uses biological molecules (antibodies, proteins, DNA and receptor ligands) to preferentially target the nanoparticles to the tumor cells. There are many types of nanoparticle delivery systems, such as silica, polymers, liposomes and magnetic particles. Nanoparticles made of magnetic material can also be used to concentrate agents at tumor sites using an externally applied magnetic field. They have emerged as a useful vehicle in magnetic drug delivery for poorly soluble agents such as paclitaxel.\nElectrochemotherapy.\nElectrochemotherapy is the combined treatment in which injection of a chemotherapeutic drug is followed by application of high-voltage electric pulses locally to the tumor. The treatment enables the chemotherapeutic drugs, which otherwise cannot or hardly go through the membrane of cells (such as bleomycin and cisplatin), to enter the cancer cells. Hence, greater effectiveness of antitumor treatment is achieved.\nClinical electrochemotherapy has been successfully used for treatment of cutaneous and subcutaneous tumors irrespective of their histological origin. The method has been reported as safe, simple and highly effective in all reports on clinical use of electrochemotherapy. According to the ESOPE project (European Standard Operating Procedures of Electrochemotherapy), the Standard Operating Procedures (SOP) for electrochemotherapy were prepared, based on the experience of the leading European cancer centres on electrochemotherapy. Recently, new electrochemotherapy modalities have been developed for treatment of internal tumors using surgical procedures, endoscopic routes or percutaneous approaches to gain access to the treatment area.\nHyperthermia therapy.\nHyperthermia therapy is heat treatment for cancer that can be a powerful tool when used in combination with chemotherapy (thermochemotherapy) or radiation for the control of a variety of cancers. The heat can be applied locally to the tumor site, which will dilate blood vessels to the tumor, allowing more chemotherapeutic medication to enter the tumor. Additionally, the tumor cell membrane will become more porous, further allowing more of the chemotherapeutic medicine to enter the tumor cell.\nHyperthermia has also been shown to help prevent or reverse \"chemo-resistance.\" Chemotherapy resistance sometimes develops over time as the tumors adapt and can overcome the toxicity of the chemo medication. \"Overcoming chemoresistance has been extensively studied within the past, especially using CDDP-resistant cells. In regard to the potential benefit that drug-resistant cells can be recruited for effective therapy by combining chemotherapy with hyperthermia, it was important to show that chemoresistance against several anticancer drugs (e.g. mitomycin C, anthracyclines, BCNU, melphalan) including CDDP could be reversed at least partially by the addition of heat.\nOther animals.\nChemotherapy is used in veterinary medicine similar to how it is used in human medicine."}
{"id": "7174", "revid": "47077949", "url": "https://en.wikipedia.org/wiki?curid=7174", "title": "Chinese historiography", "text": "Chinese historiography is the study of the techniques and sources used by historians to develop the recorded history of China.\nOverview of Chinese history.\nThe recording of events in Chinese history dates back to the Shang dynasty ( 1600\u20131046 BC). Many written examples survive of ceremonial inscriptions, divinations and records of family names, which were carved or painted onto tortoise shell or bones. The uniformly religious context of Shang written records makes avoidance of preservation bias important when interpreting Shang history. The first conscious attempt to record history in China may have been the inscription on the Zhou dynasty bronze Shi Qiang \"pan\". This and thousands of other Chinese bronze inscriptions form our primary sources for the period in which they were interred in elite burials.\nThe oldest surviving history texts of China were compiled in the \"Book of Documents (Shujing)\". The \"Spring and Autumn Annals (Chunqiu)\", the official chronicle of the State of Lu, cover the period from 722 to 481 BC and are among the earliest surviving Chinese historical texts to be arranged as annals. The compilations of both of these works are traditionally ascribed to Confucius. The \"Zuo zhuan\", attributed to Zuo Qiuming in the 5th century BC, is the earliest Chinese work of narrative history and covers the period from 722 to 468 BC. The anonymous \"Zhan Guo Ce\" was a renowned ancient Chinese historical work composed of sporadic materials on the Warring States period between the 3rd and 1st centuries BC.\nThe first systematic Chinese historical text, the \"Records of the Grand Historian\" (\"Shiji\"), was written by Sima Qian (145 or 135\u201386BC) based on work by his father, Sima Tan, during the Han dynasty. It covers the period from the time of the Yellow Emperor until the author's own lifetime. Two instances of systematic book-burning and a palace fire in the preceding centuries narrowed the sources available for this work. Because of this highly praised and frequently copied work, Sima Qian is often regarded as the father of Chinese historiography. The \"Twenty-Four Histories\", the official histories of the dynasties considered legitimate by imperial Chinese historians, all copied Sima Qian's format. Typically, rulers initiating a new dynasty would employ scholars to compile a final history from the records of the previous one, using a broad variety of sources.\nAround the turn of the millennium, father\u2013son imperial librarians Liu Xiang and Liu Xin edited and catalogued a large number of early texts, including each individual text listed by name above. Much transmitted literature surviving today is known to be ultimately the version they edited down from a larger volume of material available at the time. In 190, the imperial capital was again destroyed by arson, causing the loss of significant amounts of historical material.\nThe \"Shitong\" was the first Chinese work about historiography. It was compiled by Liu Zhiji between 708 and 710 AD. The book describes the general pattern of the official dynastic histories with regard to the structure, method, arrangement, sequence, caption, and commentary, dating back to the Warring States period.\nThe \"Zizhi Tongjian\" was a pioneering reference work of Chinese historiography. Emperor Yingzong of Song ordered Sima Guang and other scholars to begin compiling this universal history of China in 1065, and they presented it to his successor Shenzong in 1084. It contains 294 volumes and about three million characters, and it narrates the history of China from 403 BC to the beginning of the Song dynasty in 959. This style broke the nearly thousand-year tradition of Sima Qian, which employed annals for imperial reigns but biographies or treatises for other topics. The more consistent style of the \"Zizhi Tongjian\" was not followed by later official histories. In the mid 13th century, Ouyang Xiu was heavily influenced by the work of Xue Juzheng. This led to the creation of the \"New History of the Five Dynasties\", which covered five dynasties in over 70 chapters.\nToward the end of the Qing dynasty in the early 20th century, scholars looked to Japan and the West for models. In the late 1890s, although deeply learned in the traditional forms, Liang Qichao began to publish extensive and influential studies and polemics that converted young readers to a new type of historiography that Liang regarded as more scientific. Liu Yizheng published several specialized history works including \"History of Chinese Culture\". This next generation became professional historians, training and teaching in universities. They included Chang Chi-yun, Gu Jiegang, Fu Sinian, and Tsiang Tingfu, who were PhDs from Columbia University; and Chen Yinke, who conducted his investigations into medieval Chinese history in both Europe and the United States. Other historians, such as Qian Mu, who was trained largely through independent study, were more conservative but remained innovative in their response to world trends. In the 1920s, wide-ranging scholars, such as Guo Moruo, adapted Marxism in order to portray China as a nation among nations, rather than having an exotic and isolated history. The ensuing years saw historians such as Wu Han master both Western theories, including Marxism, and Chinese learning.\nKey organizing concepts.\nDynastic cycle.\nLike the three ages of the Greek poet Hesiod, the oldest Chinese historiography viewed mankind as living in a fallen age of depravity, cut off from the virtues of the past, as Confucius and his disciples revered the sage kings Emperor Yao and Emperor Shun.\nUnlike Hesiod's system, however, the Duke of Zhou's idea of the Mandate of Heaven as a rationale for dethroning the supposedly divine Zi clan led subsequent historians to see man's fall as a cyclical pattern. In this view, a new dynasty is founded by a morally upright founder, but his successors cannot help but become increasingly corrupt and dissolute. This immorality removes the dynasty's divine favor and is manifested by natural disasters (particularly floods), rebellions, and foreign invasions. Eventually, the dynasty becomes weak enough to be replaced by a new one, whose founder is able to rectify many of society's problems and begin the cycle anew. Over time, many people felt a full correction was not possible, and that the golden age of Yao and Shun could not be attained.\nThis teleological theory implies that there can be only one rightful sovereign under heaven at a time. Thus, despite the fact that Chinese history has had many lengthy and contentious periods of disunity, a great effort was made by official historians to establish a legitimate precursor whose fall allowed a new dynasty to acquire its mandate. Similarly, regardless of the particular merits of individual emperors, founders would be portrayed in more laudatory terms, and the last ruler of a dynasty would always be castigated as depraved and unworthy \u2013 even when that was not the case. Such a narrative was employed after the fall of the empire by those compiling the history of the Qing, and by those who justified the attempted restorations of the imperial system by Yuan Shikai and Zhang Xun.\nMulti-ethnic history.\nTraditional Chinese historiography includes states ruled by other peoples (Mongols, Manchus, Tibetans etc.) in the dynastic history of China proper, ignoring their own historical traditions and considering them parts of China. Two historiographic traditions: of unity in East Asia as a historical norm for this region, and of dynasties successively reigning on the Son of Heaven's throne allowed Chinese elites describing historical process in China in simplified categories providing the basis for the concept of modern \"unitary China\" within the borders of the former Qing Empire, which was also ruled by Chinese emperors. However, deeper analysis reveals that, in fact, there was not a succession of dynasties ruled the same unitary China, but there were different states in certain regions of East Asia, some of which have been termed by later historiographers as the Empire ruled by the Son of the Heaven.\nAs early as the 1930s, the American scholar Owen Lattimore argued that China was the product of the interaction of farming and pastoral societies, rather than simply the expansion of the Han people. Lattimore did not accept the more extreme Sino-Babylonian theories that the essential elements of early Chinese technology and religion had come from Western Asia, but he was among the scholars to argue against the assumption they had all been indigenous.\nBoth the Republic of China and the People's Republic of China hold the view that Chinese history should include all the ethnic groups of the lands held by the Qing dynasty during its territorial peak, with these ethnicities forming part of the \"Zhonghua minzu\" (Chinese nation). This view is in contrast with Han chauvinism promoted by the Qing-era Tongmenghui. This expanded view encompasses internal and external tributary lands, as well as conquest dynasties in the history of a China seen as a coherent multi-ethnic nation since time immemorial, incorporating and accepting the contributions and cultures of non-Han ethnicities.\nThe acceptance of this view by ethnic minorities sometimes depends on their views on present-day issues. The 14th Dalai Lama, long insistent on Tibet's history being separate from that of China, conceded in 2005 that Tibet \"is a part of\" China's \"5,000-year history\" as part of a new proposal for Tibetan autonomy. Korean nationalists have virulently reacted against China's application to UNESCO for recognition of the Goguryeo tombs in Chinese territory. The absolute independence of Goguryeo is a central aspect of Korean identity, because, according to Korean legend, Goguryeo was independent of China and Japan, compared to subordinate states such as the Joseon dynasty and the Korean Empire. The legacy of Genghis Khan has been contested between China, Mongolia, and Russia, all three states having significant numbers of ethnic Mongols within their borders and holding territory that was conquered by the Khan.\nThe Jin dynasty tradition of a new dynasty composing the official history for its preceding dynasty/dynasties has been seen to foster an ethnically inclusive interpretation of Chinese history. The compilation of official histories usually involved monumental intellectual labor. The Yuan and Qing dynasties, ruled by the Mongols and Manchus, faithfully carried out this practice, composing the official Chinese-language histories of the Han-ruled Song and Ming dynasties, respectively.\nRecent Western scholars have reacted against the ethnically inclusive narrative in traditional and Chinese Communist Party (CCP)-sponsored history, by writing revisionist histories of China such as the New Qing History that feature, according to James A. Millward, \"a degree of 'partisanship' for the indigenous underdogs of frontier history\". Scholarly interest in writing about Chinese minorities from non-Chinese perspectives is growing. So too is the rejection of a unified cultural narrative in early China. Historians engaging with archaeological progress find increasingly demonstrated a rich amalgam of diverse cultures in regions the received literature positions as homogeneous.\nMarxism.\nMost Chinese history that is published in the People's Republic of China is based on a Marxist interpretation of history. These theories were first applied in the 1920s by Chinese scholars such as Guo Moruo, and became orthodoxy in academic study after 1949. The Marxist view of history is that history is governed by universal laws and that according to these laws, a society moves through a series of stages, with the transition between stages being driven by class struggle. These stages are:\nThe official historical view within the People's Republic of China associates each of these stages with a particular era in Chinese history.\nBecause of the strength of the CCP and the importance of the Marxist interpretation of history in legitimizing its rule, it was for many years difficult for historians within the PRC to actively argue in favor of non-Marxist and anti-Marxist interpretations of history. However, this political restriction is less confining than it may first appear in that the Marxist historical framework is surprisingly flexible, and it is a rather simple matter to modify an alternative historical theory to use language that at least does not challenge the Marxist interpretation of history.\nPartly because of the interest of Mao Zedong, historians in the 1950s took a special interest in the role of peasant rebellions in Chinese history and compiled documentary histories to examine them.\nThere are several problems associated with imposing Marx's European-based framework on Chinese history. First, slavery existed throughout China's history but never as the primary form of labor. While the Zhou and earlier dynasties may be labeled as feudal, later dynasties were much more centralized than how Marx analyzed their European counterparts as being. To account for the discrepancy, Chinese Marxists invented the term \"bureaucratic feudalism\". The placement of the Tang as the beginning of the bureaucratic phase rests largely on the replacement of patronage networks with the imperial examination. Some world-systems analysts, such as Janet Abu-Lughod, claim that analysis of Kondratiev waves shows that capitalism first arose in Song dynasty China, although widespread trade was subsequently disrupted and then curtailed.\nThe Japanese scholar Tanigawa Michio, writing in the 1970s and 1980s, set out to revise the generally Marxist views of China prevalent in post-war Japan. Tanigawa writes that historians in Japan fell into two schools. One held that China followed the set European pattern which Marxists thought to be universal; that is, from ancient slavery to medieval feudalism to modern capitalism; while another group argued that \"Chinese society was extraordinarily saturated with stagnancy, as compared to the West\" and assumed that China existed in a \"qualitatively different historical world from Western society\". That is, there is an argument between those who see \"unilinear, monistic world history\" and those who conceive of a \"two-tracked or multi-tracked world history\". Tanigawa reviewed the applications of these theories in Japanese writings about Chinese history and then tested them by analyzing the Six Dynasties 220\u2013589 CE period, which Marxist historians saw as feudal. His conclusion was that China did not have feudalism in the sense that Marxists use, that Chinese military governments did not lead to a European-style military aristocracy. The period established social and political patterns which shaped China's history from that point on.\nThere was a gradual relaxation of Marxist interpretation after the death of Mao Zedong in 1976, which was accelerated after the Tian'anmen Square protest and other revolutions in 1989, which damaged Marxism's ideological legitimacy in the eyes of Chinese academics.\nModernization.\nThis view of Chinese history sees Chinese society as a traditional society needing to become modern, usually with the implicit assumption of Western society as the model. Such a view was common amongst European and American historians during the 19th and early 20th centuries, but is now criticized for being a Eurocentric viewpoint, since such a view permits an implicit justification for breaking the society from its static past and bringing it into the modern world under European direction.\nBy the mid-20th century, it was increasingly clear to historians that the notion of \"changeless China\" was untenable. A new concept, popularized by John Fairbank, was the notion of \"change within tradition\", which argued that China did change in the pre-modern period but that this change existed within certain cultural traditions. This notion has also been subject to the criticism that to say \"China has not changed fundamentally\" is tautological, since it requires that one look for things that have not changed and then arbitrarily define those as fundamental.\nNonetheless, studies seeing China's interaction with Europe as the driving force behind its recent history are still common. Such studies may consider the First Opium War as the starting point for China's modern period. Examples include the works of H.B. Morse, who wrote chronicles of China's international relations such as \"Trade and Relations of the Chinese Empire\". The Chinese convention is to use the word \"jindai\" (\"modern\") to refer to a timeframe for modernity which begins with the Opium wars and continues through the May Fourth period.\nIn the 1950s, several of Fairbank's students argued that Confucianism was incompatible with modernity. Joseph Levenson and Mary C. Wright, and Albert Feuerwerker argued in effect that traditional Chinese values were a barrier to modernity and would have to be abandoned before China could make progress. Wright concluded, \"The failure of the T'ung-chih [\"Tongzhi\"] Restoration demonstrated with a rare clarity that even in the most favorable circumstances there is no way in which an effective modern state can be grafted onto a Confucian society. Yet in the decades that followed, the political ideas that had been tested and, for all their grandeur, found wanting, were never given a decent burial.\"\nIn a different view of modernization, the Japanese historian Naito Torajiro argued that China reached modernity during its mid-Imperial period, centuries before Europe. He believed that the reform of the civil service into a meritocratic system and the disappearance of the ancient Chinese nobility from the bureaucracy constituted a modern society. The problem associated with this approach is the subjective meaning of modernity. The Chinese nobility had been in decline since the Qin dynasty, and while the exams were largely meritocratic, performance required time and resources that meant examinees were still typically from the gentry. Moreover, expertise in the Confucian classics did not guarantee competent bureaucrats when it came to managing public works or preparing a budget. Confucian hostility to commerce placed merchants at the bottom of the four occupations, itself an archaism maintained by devotion to classic texts. The social goal continued to be to invest in land and enter the gentry, ideas more like those of the physiocrats than those of Adam Smith.\nHydraulic despotism.\nWith ideas derived from Marx and Max Weber, Karl August Wittfogel argued that bureaucracy arose to manage irrigation systems. Despotism was needed to force the people into building canals, dikes, and waterways to increase agriculture. Yu the Great, one of China's legendary founders, is known for his control of the floods of the Yellow River. The hydraulic empire produces wealth from its stability; while dynasties may change, the structure remains intact until destroyed by modern powers. In Europe abundant rainfall meant less dependence on irrigation. In the Orient natural conditions were such that the bulk of the land could not be cultivated without large-scale irrigation works. As only a centralized administration could organize the building and maintenance of large-scale systems of irrigation, the need for such systems made bureaucratic despotism inevitable in Oriental lands.\nWhen Wittfogel published his \"\", critics pointed out that water management was given the high status China accorded to officials concerned with taxes, rituals, or fighting off bandits. The theory also has a strong orientalist bent, regarding all Asian states as generally the same while finding reasons for European polities not fitting the pattern.\nWhile Wittfogel's theories were not popular among Marxist historians in China, the economist Chi Ch'ao-ting used them in his influential 1936 book, \"Key Economic Areas in Chinese History, as Revealed in the Development of Public Works for Water-Control\". The book identified key areas of grain production which, when controlled by a strong political power, permitted that power to dominate the rest of the country and enforce periods of stability.\nConvergence.\nConvergence theory, including Hu Shih and Ray Huang's involution theory, holds that the past 150 years have been a period in which Chinese and Western civilization have been in the process of converging into a world civilization. Such a view is heavily influenced by modernization theory but, in China's case, it is also strongly influenced by indigenous sources such as the notion of \"Shijie Datong\" or \"Great Unity\". It has tended to be less popular among more recent historians, as postmodern Western historians discount overarching narratives, and nationalist Chinese historians feel similar about narratives failing to account for some special or unique characteristics of Chinese culture.\nAnti-imperialism.\nClosely related are colonial and anti-imperialist narratives. These often merge or are part of Marxist critiques from within China or the former Soviet Union, or are postmodern critiques such as Edward Said's \"Orientalism\", which fault traditional scholarship for trying to fit West, South, and East Asia's histories into European categories unsuited to them. With regard to China particularly, T.F. Tsiang and John Fairbank used newly opened archives in the 1930s to write modern history from a Chinese point of view. Fairbank and Teng Ssu-yu then edited the influential volume \"China's Response to the West\" (1953). This approach was attacked for ascribing the change in China to outside forces. In the 1980s, Paul Cohen, a student of Fairbank's, issued a call for a more \"China-Centered history of China\".\nRepublican.\nThe schools of thought on the 1911 Revolution have evolved from the early years of the Republic. The Marxist view saw the events of 1911 as a bourgeois revolution. In the 1920s, the Nationalist Party issued a theory of three political stages based on Sun Yatsen's writings:\nThe most obvious criticism is the near-identical nature of \"political tutelage\" and of a \"constitutional democracy\" consisting only of the one-party rule until the 1990s. Against this, Chen Shui-bian proposed his own four-stage theory.\nPostmodernism.\nPostmodern interpretations of Chinese history tend to reject narrative history and instead focus on a small subset of Chinese history, particularly the daily lives of ordinary people in particular locations or settings.\nLong-term political economy.\nZooming out from the dynastic cycle but maintaining focus on power dynamics, the following general periodization, based on the most powerful groups and the ways that power is used, has been proposed for Chinese history:\nRecent trends.\nFrom the beginning of CCP rule in 1949 until the 1980s, Chinese historical scholarship focused largely on the officially sanctioned Marxist theory of class struggle. From the time of Deng Xiaoping (1978\u20131992) on, there has been a drift towards a Marxist-inspired Chinese nationalist perspective, and consideration of China's contemporary international status has become of paramount importance in historical studies. The current focus tends to be on specifics of civilization in ancient China, and the general paradigm of how China has responded to the dual challenges of interactions with the outside world and modernization in the post-1700 era. Long abandoned as a research focus among most Western scholars due to postmodernism's influence, this remains the primary interest for most historians inside China.\nThe late 20th century and early 21st century have seen numerous studies of Chinese history that challenge traditional paradigms. The field is rapidly evolving, with much new scholarship, often based on the realization that there is much about Chinese history that is unknown or controversial. For example, an active topic concerns whether the typical Chinese peasant in 1900 was seeing his life improve. In addition to the realization that there are major gaps in our knowledge of Chinese history is the equal realization that there are tremendous quantities of primary source material that have not yet been analyzed. Scholars are using previously overlooked documentary evidence, such as masses of government and family archives, and economic records such as census tax rolls, price records, and land surveys. In addition, artifacts such as vernacular novels, how-to manuals, and children's books are analyzed for clues about day-to-day life.\nRecent Western scholarship of China has been heavily influenced by postmodernism, and has questioned modernist narratives of China's backwardness and lack of development. The desire to challenge the preconception that 19th-century China was weak, for instance, has led to a scholarly interest in Qing expansion into Central Asia. Postmodern scholarship largely rejects grand narratives altogether, preferring to publish empirical studies on the socioeconomics, and political or cultural dynamics, of smaller communities within China.\nAs of at least 2023, there has been a surge of historical writing about key leaders of the Nationalist period. A significant amount of new writing includes texts written for a general (as opposed to only academic) audience. There has been an increasingly nuanced portrayal of Chiang Kai-shek, particularly in more favorably evaluating his leadership during the Second Sino-Japanese War and highlighting his position as one of the Big Four allied leaders. Recently released archival sources on the Nationalist era, including the Chiang Kai-shek diaries at Stanford University's Hoover Institution, have contributed to a surge in academic publishing on the period.\nNationalism.\nIn China, historical scholarship remains largely nationalist and modernist or even traditionalist. The legacies of the modernist school (such as Lo Hsiang-lin) and the traditionalist school (such as Qian Mu (Chien Mu)) remain strong in Chinese circles. The more modernist works focus on imperial systems in China and employ the scientific method to analyze epochs of Chinese dynasties from geographical, genealogical, and cultural artifacts. For example, using Carbon-14 dating and geographical records to correlate climates with cycles of calm and calamity in Chinese history. The traditionalist school of scholarship resorts to official imperial records and colloquial historical works, and analyzes the rise and fall of dynasties using Confucian philosophy, albeit modified by an institutional administration perspective.\nAfter 1911, writers, historians and scholars in China and abroad generally deprecated the late imperial system and its failures. However, in the 21st century, a highly favorable revisionism has emerged in the popular culture, in both the media and social media. \nFlorian Schneider argues that nationalism in China in the early twenty-first century is largely a product of the digital revolution and that a large fraction of the population participates as readers and commentators who relate ideas to their friends over the internet."}
{"id": "7175", "revid": "48823678", "url": "https://en.wikipedia.org/wiki?curid=7175", "title": "Chinese Communist Party", "text": "The Chinese Communist Party (CCP), officially the Communist Party of China (CPC), is the founding and sole ruling party of the People's Republic of China (PRC). Under the leadership of Mao Zedong, the CCP emerged victorious in the Chinese Civil War against the Kuomintang. In 1949, Mao proclaimed the establishment of the People's Republic of China. Since then, the CCP has governed China and has had sole control over the People's Liberation Army (PLA). Successive leaders of the CCP have added their own theories to the party's constitution, which outlines the party's ideology, collectively referred to as socialism with Chinese characteristics. , the CCP has more than 99 million members, making it the second largest political party by membership in the world after India's Bharatiya Janata Party.\nIn 1921, Chen Duxiu and Li Dazhao led the founding of the CCP with the help of the Far Eastern Bureau of the Russian Communist Party (Bolsheviks) and Far Eastern Bureau of the Communist International. For the first six years, the CCP aligned itself with the Kuomintang (KMT) as the organized left wing of the larger nationalist movement. However, when the right wing of the KMT, led by Chiang Kai-shek, turned on the CCP and massacred tens of thousands of the party's members, the two parties split and began a prolonged civil war. During the next ten years of guerrilla warfare, Mao Zedong rose to become the most influential figure in the CCP, and the party established a strong base among the rural peasantry with its land reform policies. Support for the CCP continued to grow throughout the Second Sino-Japanese War, and after the Japanese surrender in 1945, the CCP emerged triumphant in the communist revolution against the Nationalist government. After the KMT's retreat to Taiwan, the CCP established the People's Republic of China on 1 October 1949.\nMao Zedong continued to be the most influential member of the CCP until his death in 1976, although he periodically withdrew from public leadership as his health deteriorated. Under Mao, the party completed its land reform program, launched a series of five-year plans, and eventually split with the Soviet Union. Although Mao attempted to purge the party of capitalist and reactionary elements during the Cultural Revolution, after his death, these policies were only briefly continued by the Gang of Four before a less radical faction seized control. During the 1980s, Deng Xiaoping directed the CCP away from Maoist orthodoxy and towards a policy of economic liberalization. The official explanation for these reforms was that China was still in the primary stage of socialism, a developmental stage similar to the capitalist mode of production. Since the collapse of the Eastern Bloc and the dissolution of the Soviet Union in 1991, the CCP has focused on maintaining its relations with the ruling parties of the remaining socialist states and continues to participate in the International Meeting of Communist and Workers' Parties each year. The CCP has also established relations with several non-communist parties, including dominant nationalist parties of many developing countries in Africa, Asia and Latin America, as well as social democratic parties in Europe.\nThe Chinese Communist Party is organized based on democratic centralism, a principle that entails open policy discussion on the condition of unity among party members in upholding the agreed-upon decision. The highest body of the CCP is the National Congress, convened every fifth year. When the National Congress is not in session, the Central Committee is the highest body, but since that body usually only meets once a year, most duties and responsibilities are vested in the Politburo and its Standing Committee. Members of the latter are seen as the top leadership of the party and the state. Today the party's leader holds the offices of general secretary (responsible for civilian party duties), Chairman of the Central Military Commission (CMC) (responsible for military affairs), and State President (a largely ceremonial position). Because of these posts, the party leader is seen as the country's paramount leader. The current leader is Xi Jinping, who was elected at the 1st Plenary Session of the 18th Central Committee held on 15 November 2012 and has been reelected twice, on 25 October 2017 by the 19th Central Committee and on 10 October 2022 by the 20th Central Committee.\nHistory.\nFounding and early history.\nThe October Revolution and Marxist theory inspired the founding of the CCP. Chen Duxiu and Li Dazhao were among the first to publicly support Leninism and world revolution. Both regarded the October Revolution in Russia as groundbreaking, believing it to herald a new era for oppressed countries everywhere.\nSome historical analysis views the May Fourth Movement as the beginning of the revolutionary struggle that led to the founding of the People's Republic of China. Following the movement, trends towards social transformation increased. Writing in 1939, Mao Zedong stated that the Movement had shown that the bourgeois revolution against imperialism and China had developed to a new stage, but that the proletariat would lead the revolution's completion. The May Fourth Movement led to the establishment of radical intellectuals who went on to mobilize peasants and workers into the CCP and gain the organizational strength that would solidify the success of the Chinese Communist Revolution. Chen and Li were among the most influential promoters of Marxism in China during the May Fourth period. The CCP itself embraces the May Fourth Movement and views itself as part of the movement's legacy.\nStudy circles were, according to Cai Hesen, \"the rudiments [of our party]\". Several study circles were established during the New Culture Movement, but by 1920 many grew sceptical about their ability to bring about reforms. China's intellectual movements were fragmented in the early 1920s. The May Fourth Movement and the New Culture Movement had identified issues of broad concern to Chinese progressives, including anti-imperialism, support for nationalism, support for democracy, promotion of feminism, and rejection of traditional values. Proposed solutions among Chinese progressives differed significantly, however.\nThe CCP was founded on 1 July 1921 with the help of the Far Eastern Bureau of the Russian Communist Party (Bolsheviks) and Far Eastern Secretariat of the Communist International, according to the party's official account of its history. However, party documents suggest that the party's actual founding date was 23 July 1921, the first day of the 1st National Congress of the CCP. The founding National Congress of the CCP was held 23\u201331 July 1921. With only 50 members in the beginning of 1921, among them Chen Duxiu, Li Dazhao and Mao Zedong, the CCP organization and authorities grew tremendously. While it was originally held in a house in the Shanghai French Concession, French police interrupted the meeting on 30 July and the congress was moved to a tourist boat on South Lake in Jiaxing, Zhejiang province. A dozen delegates attended the congress, with neither Li nor Chen being able to attend, the latter sending a personal representative in his stead. The resolutions of the congress called for the establishment of a communist party as a branch of the Communist International (Comintern) and elected Chen as its leader. Chen then served as the first general secretary of the CCP and was referred to as \"China's Lenin\".\nThe Soviets hoped to foster pro-Soviet forces in East Asia to fight against anti-communist countries, particularly Japan. They attempted to contact the warlord Wu Peifu but failed. The Soviets then contacted the Kuomintang (KMT), which was leading the Guangzhou government parallel to the Beiyang government. On 6 October 1923, the Comintern sent Mikhail Borodin to Guangzhou, and the Soviets established friendly relations with the KMT. The Central Committee of the CCP, Soviet leader Joseph Stalin, and the Comintern all hoped that the CCP would eventually control the KMT and called their opponents \"rightists\". KMT leader Sun Yat-sen eased the conflict between the communists and their opponents. CCP membership grew tremendously after the 4th congress in 1925, from 900 to 2,428. The CCP still treats Sun Yat-sen as one of the founders of their movement and claim descent from him as he is viewed as a proto-communist and the economic element of Sun's ideology was socialism. Sun stated, \"Our Principle of Livelihood is a form of communism\".\nThe communists dominated the left wing of the KMT and struggled for power with the party's right-wing factions. When Sun Yat-sen died in March 1925, he was succeeded by a rightist, Chiang Kai-shek, who initiated moves to marginalize the position of the communists. Chiang, Sun's former assistant, was not actively anti-communist at that time, even though he hated the theory of class struggle and the CCP's seizure of power. The communists proposed removing Chiang's power. When Chiang gradually gained the support of Western countries, the conflict between him and the communists became more and more intense. Chiang asked the Kuomintang to join the Comintern to rule out the secret expansion of communists within the KMT, while Chen Duxiu hoped that the communists would completely withdraw from the KMT.\nIn April 1927, both Chiang and the CCP were preparing for conflict. Fresh from the success of the Northern Expedition to overthrow the warlords, Chiang Kai-shek turned on the communists, who by now numbered in the tens of thousands across China. Ignoring the orders of the Wuhan-based KMT government, he marched on Shanghai, a city controlled by communist militias. Although the communists welcomed Chiang's arrival, he turned on them, massacring 5,000 with the aid of the Green Gang. Chiang's army then marched on Wuhan but was prevented from taking the city by CCP General Ye Ting and his troops. Chiang's allies also attacked communists; for example, in Beijing, Li Dazhao and 19 other leading communists were executed by Zhang Zuolin. Angered by these events, the peasant movement supported by the CCP became more violent. Ye Dehui, a famous scholar, was killed by communists in Changsha, and in revenge, KMT general He Jian and his troops gunned down hundreds of peasant militiamen. That May, tens of thousands of communists and their sympathizers were killed by KMT troops, with the CCP losing approximately of its members.\nChinese Civil War and Second Sino-Japanese War.\nThe CCP continued supporting the Wuhan KMT government, but on 15 July 1927 the Wuhan government expelled all communists from the KMT. The CCP reacted by founding the Workers' and Peasants' Red Army of China, better known as the \"Red Army\", to battle the KMT. A battalion led by General Zhu De was ordered to take the city of Nanchang on 1 August 1927 in what became known as the Nanchang uprising.\nInitially successful, Zhu and his troops were forced to retreat after five days, marching south to Shantou, and from there being driven into the wilderness of Fujian. Mao Zedong was appointed commander-in-chief of the Red Army, and led four regiments against Changsha in the Autumn Harvest Uprising, hoping to spark peasant uprisings across Hunan. His plan was to attack the KMT-held city from three directions on 9 September, but the Fourth Regiment deserted to the KMT cause, attacking the Third Regiment. Mao's army made it to Changsha but could not take it; by 15 September, he accepted defeat, with 1,000 survivors marching east to the Jinggang Mountains of Jiangxi.\nThe near destruction of the CCP's urban organizational apparatus led to institutional changes within the party. The party adopted democratic centralism, a way to organize revolutionary parties, and established a politburo to function as the standing committee of the central committee. The result was increased centralization of power within the party. At every level of the party this was duplicated, with standing committees now in effective control. After being expelled from the party, Chen Duxiu went on to lead China's Trotskyist movement. Li Lisan was able to assume \"de facto\" control of the party organization by 1929\u20131930.\nThe 1929 Gutian Congress was important in establishing the principle of party control over the military, which continues to be a core principle of the party's ideology.\nLi's leadership was a failure, leaving the CCP on the brink of destruction. The Comintern became involved, and by late 1930, his powers had been taken away. By 1935, Mao had become a member of Politburo Standing Committee of the CCP and the party's informal military leader, with Zhou Enlai and Zhang Wentian, the formal head of the party, serving as his informal deputies. The conflict with the KMT led to the reorganization of the Red Army, with power now centralized in the leadership through the creation of CCP political departments charged with supervising the army.\nThe Xi'an Incident of December 1936 paused the conflict between the CCP and the KMT. Under pressure from Marshal Zhang Xueliang and the CCP, Chiang Kai-shek finally agreed to a Second United Front focused on repelling the Japanese invaders. While the front formally existed until 1945, all collaboration between the two parties had effectively ended by 1940. Despite their formal alliance, the CCP used the opportunity to expand and carve out independent bases of operations to prepare for the coming war with the KMT. In 1939, the KMT began to restrict CCP expansion within China. This led to frequent clashes between CCP and KMT forces which subsided rapidly on the realization on both sides that civil war amidst a foreign invasion was not an option. By 1943, the CCP was again actively expanding its territory at the expense of the KMT.\nMao Zedong became the Chairman of the CCP in 1945. After the Japanese surrender in 1945, the war between the CCP and the KMT began again in earnest. The 1945\u20131949 period had four stages; the first was from August 1945 (when the Japanese surrendered) to June 1946 (when the peace talks between the CCP and the KMT ended). By 1945, the KMT had three times more soldiers under its command than the CCP and initially appeared to be prevailing. With the cooperation of the US and Japan, the KMT was able to retake major parts of the country. However, KMT rule over the reconquered territories proved unpopular because of its endemic political corruption.\nNotwithstanding its numerical superiority, the KMT failed to reconquer the rural territories which made up the CCP's stronghold. Around the same time, the CCP launched an invasion of Manchuria, where they were assisted by the Soviet Union. The second stage, lasting from July 1946 to June 1947, saw the KMT extend its control over major cities such as Yan'an, the CCP headquarters, for much of the war. The KMT's successes were hollow; the CCP had tactically withdrawn from the cities, and instead undermined KMT rule there by instigating protests among students and intellectuals. The KMT responded to these demonstrations with heavy-handed repression. In the meantime, the KMT was struggling with factional infighting and Chiang Kai-shek's autocratic control over the party, which weakened its ability to respond to attacks.\nThe third stage, lasting from July 1947 to August 1948, saw a limited counteroffensive by the CCP. The objective was clearing \"Central China, strengthening North China, and recovering Northeast China.\" This operation, coupled with military desertions from the KMT, resulted in the KMT losing 2 million of its 3 million troops by the spring of 1948, and saw a significant decline in support for KMT rule. The CCP was consequently able to cut off KMT garrisons in Manchuria and retake several territories.\nThe last stage, lasting from September 1948 to December 1949, saw the communists go on the offensive and the collapse of KMT rule in mainland China as a whole. Mao's proclamation of the founding of the People's Republic of China on 1 October 1949 marked the end of the second phase of the Chinese Civil War (or the Chinese Communist Revolution, as it is called by the CCP).\nProclamation of the PRC and the 1950s.\nMao proclaimed the founding of the People's Republic of China (PRC) before a massive crowd at Tiananmen Square on 1 October 1949. The CCP headed the Central People's Government. From this time through the 1980s, top leaders of the CCP (such as Mao Zedong, Lin Biao, Zhou Enlai and Deng Xiaoping) were largely the same military leaders prior to the PRC's founding. As a result, informal personal ties between political and military leaders dominated civil-military relations.\nStalin proposed a one-party constitution when Liu Shaoqi visited the Soviet Union in 1952. The constitution of the PRC in 1954 subsequently abolished the previous coalition government and established the CCP's one-party system. In 1957, the CCP launched the Anti-Rightist Campaign against political dissidents and prominent figures from minor parties, which resulted in the political persecution of at least 550,000 people. The campaign significantly damaged the limited pluralistic nature in the socialist republic and solidified the country's status as a \"de facto\" one-party state.\nThe Anti-Rightist Campaign led to the catastrophic results of the Second Five Year Plan from 1958 to 1962, known as the Great Leap Forward. In an effort to transform the country from an agrarian economy into an industrialized one, the CCP collectivized farmland, formed people's communes, and diverted labour to factories. General mismanagement and exaggerations of harvests by CCP officials led to the Great Chinese Famine, which resulted in an estimated 15 to 45 million deaths, making it the largest famine in recorded history.\nSino-Soviet split and Cultural Revolution.\nDuring the 1960s and 1970s, the CCP experienced a significant ideological separation from the Communist Party of the Soviet Union which was going through a period of \"de-Stalinization\" under Nikita Khrushchev. By that time, Mao had begun saying that the \"continued revolution under the dictatorship of the proletariat\" stipulated that class enemies continued to exist even though the socialist revolution seemed to be complete, leading to the Cultural Revolution in which millions were persecuted and killed. During the Cultural Revolution, party leaders such as Liu Shaoqi, Deng Xiaoping, Peng Dehuai, and He Long were purged or exiled, and the Gang of Four, led by Mao's wife Jiang Qing, emerged to fill in the power vacuum left behind.\nReforms under Deng Xiaoping.\nFollowing Mao's death in 1976, a power struggle between CCP chairman Hua Guofeng and vice-chairman Deng Xiaoping erupted. Deng won the struggle, and became China's paramount leader in 1978. Deng, alongside Hu Yaobang and Zhao Ziyang, spearheaded the \"reform and opening-up\" policies, and introduced the ideological concept of socialism with Chinese characteristics, opening China to the world's markets. In reversing some of Mao's \"leftist\" policies, Deng argued that a socialist state could use the market economy without itself being capitalist. While asserting the political power of the CCP, the change in policy generated significant economic growth. This was justified on the basis that \"Practice is the Sole Criterion for the Truth\", a principle reinforced through a 1978 article that aimed to combat dogmatism and criticized the \"Two Whatevers\" policy. The new ideology, however, was contested on both sides of the spectrum, by Maoists to the left of the CCP's leadership, as well as by those supporting political liberalization. In 1981, the Party adopted a historical resolution, which assessed the historical legacy of the Mao Zedong era and the future priorities of the CCP. With other social factors, the conflicts culminated in the 1989 Tiananmen Square protests and massacre. The protests having been crushed and the reformist party general secretary Zhao Ziyang under house arrest, Deng's economic policies resumed and by the early 1990s the concept of a socialist market economy had been introduced. In 1997, Deng's beliefs (officially called \"Deng Xiaoping Theory\") were embedded into the CCP's constitution.\nFurther reforms under Jiang Zemin and Hu Jintao.\nCCP general secretary Jiang Zemin succeeded Deng as paramount leader in the 1990s and continued most of his policies. In the 1990s, the CCP transformed from a veteran revolutionary leadership that was both leading militarily and politically, to a political elite increasingly renewed according to institutionalized norms in the civil bureaucracy. Leadership was largely selected based on rules and norms on promotion and retirement, educational background, and managerial and technical expertise. There is a largely separate group of professionalized military officers, serving under top CCP leadership largely through formal relationships within institutional channels.\nThe CCP ratified Jiang's Three Represents concept for the 2003 revision of the party's constitution, as a \"guiding ideology\" to encourage the party to represent \"advanced productive forces, the progressive course of China's culture, and the fundamental interests of the people.\" The theory legitimized the entry of private business owners and bourgeois elements into the party. Hu Jintao, Jiang Zemin's successor as general secretary, took office in 2002. Unlike Mao, Deng and Jiang Zemin, Hu laid emphasis on collective leadership and opposed one-man dominance of the political system. The insistence on focusing on economic growth led to a wide range of serious social problems. To address these, Hu introduced two main ideological concepts: the \"Scientific Outlook on Development\" and \"Harmonious Society\". Hu resigned from his post as CCP general secretary and Chairman of the CMC at the 18th National Congress held in 2012, and was succeeded in both posts by Xi Jinping.\nLeadership of Xi Jinping.\nSince taking power, Xi has initiated a wide-reaching anti-corruption campaign, while centralizing powers in the office of CCP general secretary at the expense of the collective leadership of prior decades. Commentators have described the campaign as a defining part of Xi's leadership as well as \"the principal reason why he has been able to consolidate his power so quickly and effectively.\" Xi's leadership has also overseen an increase in the Party's role in China. Xi has added his ideology, named after himself, into the CCP constitution in 2017. Xi's term as general secretary was renewed in 2022.\nSince 2014, the CCP has led efforts in Xinjiang that involve the detention of more than 1 million Uyghurs and other ethnic minorities in internment camps, as well as other repressive measures. This has been described as a genocide by some academics and some governments. On the other hand, a greater number of countries signed a letter penned to the Human Rights Council supporting the policies as an effort to combat terrorism in the region.\nCelebrations of the 100th anniversary of the CCP's founding, one of the Two Centenaries, took place on 1 July 2021. In the sixth plenary session of the 19th Central Committee in November 2021, CCP adopted a resolution on the Party's history, which for the first time credited Xi as being the \"main innovator\" of Xi Jinping Thought while also declaring Xi's leadership as being \"the key to the great rejuvenation of the Chinese nation\". In comparison with the other historical resolutions, Xi's one did not herald a major change in how the CCP evaluated its history.\nOn 6 July 2021, Xi chaired the Communist Party of China and World Political Parties Summit, which involved representatives from 500 political parties across 160 countries. Xi urged the participants to oppose \"technology blockades,\" and \"developmental decoupling\" in order to work towards \"building a community with a shared future for mankind.\"\nIdeology.\nFormal ideology.\nThe core ideology of the party has evolved with each distinct generation of Chinese leadership. As both the CCP and the People's Liberation Army promote their members according to seniority, it is possible to discern distinct generations of Chinese leadership. In official discourse, each group of leadership is identified with a distinct extension of the ideology of the party. Historians have studied various periods in the development of the government of the People's Republic of China by reference to these \"generations\".\nMarxism\u2013Leninism was the first official ideology of the CCP. According to the CCP, \"Marxism\u2013Leninism reveals the universal laws governing the development of history of human society.\" To the CCP, Marxism\u2013Leninism provides a \"vision of the contradictions in capitalist society and of the inevitability of a future socialist and communist societies\". According to the \"People's Daily\", Mao Zedong Thought \"is Marxism\u2013Leninism applied and developed in China\". Mao Zedong Thought was conceived not only by Mao Zedong, but by leading party officials, according to Xinhua News Agency.\nDeng Xiaoping Theory was added to the party constitution at the 14th National Congress in 1992. The concepts of \"socialism with Chinese characteristics\" and \"the primary stage of socialism\" were credited to the theory. Deng Xiaoping Theory can be defined as a belief that state socialism and state planning is not by definition communist, and that market mechanisms are class neutral. In addition, the party needs to react to the changing situation dynamically; to know if a certain policy is obsolete or not, the party had to \"seek truth from facts\" and follow the slogan \"practice is the sole criterion for the truth\". At the 14th National Congress, Jiang reiterated Deng's mantra that it was unnecessary to ask if something was socialist or capitalist, since the important factor was whether it worked.\nThe \"Three Represents\", Jiang Zemin's contribution to the party's ideology, was adopted by the party at the 16th National Congress. The Three Represents defines the role of the CCP, and stresses that the Party must always represent the requirements for developing China's advanced productive forces, the orientation of China's advanced culture and the fundamental interests of the overwhelming majority of the Chinese people.\" Certain segments within the CCP criticized the Three Represents as being un-Marxist and a betrayal of basic Marxist values. Supporters viewed it as a further development of socialism with Chinese characteristics. Jiang disagreed, and had concluded that attaining the communist mode of production, as formulated by earlier communists, was more complex than had been realized, and that it was useless to try to force a change in the mode of production, as it had to develop naturally, by following the \"economic laws of history.\" The theory is most notable for allowing capitalists, officially referred to as the \"new social strata\", to join the party on the grounds that they engaged in \"honest labor and work\" and through their labour contributed \"to build[ing] socialism with Chinese characteristics.\"\nIn 2003, the 3rd Plenary Session of the 16th Central Committee conceived and formulated the ideology of the Scientific Outlook on Development (SOD). It is considered to be Hu Jintao's contribution to the official ideological discourse. The SOD incorporates scientific socialism, sustainable development, social welfare, a humanistic society, increased democracy, and, ultimately, the creation of a Socialist Harmonious Society. According to official statements by the CCP, the concept integrates \"Marxism with the reality of contemporary China and with the underlying features of our times, and it fully embodies the Marxist worldview on and methodology for development.\"\nXi Jinping Thought on Socialism with Chinese Characteristics for a New Era, commonly known as Xi Jinping Thought, was added to the party constitution in the 19th National Congress in 2017. The theory's main elements are summarized in the ten affirmations, the fourteen commitments, and the thirteen areas of achievements.\nThe party combines elements of both socialist patriotism and Chinese nationalism.\nEconomics.\nDeng did not believe that the fundamental difference between the capitalist mode of production and the socialist mode of production was central planning versus free markets. He said, \"A planned economy is not the definition of socialism, because there is planning under capitalism; the market economy happens under socialism, too. Planning and market forces are both ways of controlling economic activity\". Jiang Zemin supported Deng's thinking, and stated in a party gathering that it did not matter if a certain mechanism was capitalist or socialist, because the only thing that mattered was whether it worked. It was at this gathering that Jiang Zemin introduced the term socialist market economy, which replaced Chen Yun's \"planned socialist market economy\". In his report to the 14th National Congress Jiang Zemin told the delegates that the socialist state would \"let market forces play a basic role in resource allocation.\" At the 15th National Congress, the party line was changed to \"make market forces further play their role in resource allocation\"; this line continued until the 3rd Plenary Session of the 18th Central Committee, when it was amended to \"let market forces play a \"decisive\" role in resource allocation.\" Despite this, the 3rd Plenary Session of the 18th Central Committee upheld the creed \"Maintain the dominance of the public sector and strengthen the economic vitality of the state-owned economy.\"\nThe CCP views the world as organized into two opposing camps; socialist and capitalist. They insist that socialism, on the basis of historical materialism, will eventually triumph over capitalism. In recent years, when the party has been asked to explain the capitalist globalization occurring, the party has returned to the writings of Karl Marx. Despite admitting that globalization developed through the capitalist system, the party's leaders and theorists argue that globalization is not intrinsically capitalist. The reason being that if globalization was purely capitalist, it would exclude an alternative socialist form of modernity. Globalization, as with the market economy, therefore does not have one specific class character (neither socialist nor capitalist) according to the party. The insistence that globalization is not fixed in nature comes from Deng's insistence that China can pursue socialist modernization by incorporating elements of capitalism. Because of this there is considerable optimism within the CCP that despite the current capitalist dominance of globalization, globalization can be turned into a vehicle supporting socialism.\nAnalysis and criticism.\nWhile foreign analysts generally agree that the CCP has rejected orthodox Marxism\u2013Leninism and Mao Zedong Thought (or at least basic thoughts within orthodox thinking), the CCP itself disagrees. Critics of the CCP argue that Jiang Zemin ended the party's formal commitment to Marxism\u2013Leninism with the introduction of the ideological theory, the Three Represents. However, party theorist Leng Rong disagrees, claiming that \"President Jiang rid the Party of the ideological obstacles to different kinds of ownership... He did not give up Marxism or socialism. He strengthened the Party by providing a modern understanding of Marxism and socialism\u2014which is why we talk about a 'socialist market economy' with Chinese characteristics.\" The attainment of true \"communism\" is still described as the CCP's and China's \"ultimate goal\". While the CCP claims that China is in the primary stage of socialism, party theorists argue that the current development stage \"looks a lot like capitalism\". Alternatively, certain party theorists argue that \"capitalism is the early or first stage of communism.\" Some have dismissed the concept of a primary stage of socialism as intellectual cynicism. For example, Robert Lawrence Kuhn, a former foreign adviser to the Chinese government, stated: \"When I first heard this rationale, I thought it more comic than clever\u2014a wry caricature of hack propagandists leaked by intellectual cynics. But the 100-year horizon comes from serious political theorists.\"\nAmerican political scientist and sinologist David Shambaugh argues that before the \"Practice Is the Sole Criterion for the Truth\" campaign, the relationship between ideology and decision making was a deductive one, meaning that policy-making was derived from ideological knowledge. However, under Deng's leadership this relationship was turned upside down, with decision making justifying ideology. Chinese policy-makers have described the Soviet Union's state ideology as \"rigid, unimaginative, ossified, and disconnected from reality\", believing that this was one of the reasons for the dissolution of the Soviet Union. Therefore, Shambaugh argues, Chinese policy-makers believe that their party ideology must be dynamic to safeguard the party's rule.\nBritish sinologist Kerry Brown argues that the CCP does not have an ideology, and that the party organization is pragmatic and interested only in what works. The party itself argues against this assertion. Hu Jintao stated in 2012 that the Western world is \"threatening to divide us\" and that \"the international culture of the West is strong while we are weak ... Ideological and cultural fields are our main targets\". As such, the CCP puts a great deal of effort into the party schools and into crafting its ideological message.\nGovernance.\nCollective leadership.\nCollective leadership, the idea that decisions will be taken through consensus, has been the ideal in the CCP. The concept has its origins back to Lenin and the Russian Bolshevik Party. At the level of the central party leadership this means that, for instance, all members of the Politburo Standing Committee are of equal standing (each member having only one vote). A member of the Politburo Standing Committee often represents a sector; during Mao's reign, he controlled the People's Liberation Army, Kang Sheng, the security apparatus, and Zhou Enlai, the State Council and the Ministry of Foreign Affairs. This counts as informal power. Despite this, in a paradoxical relation, members of a body are ranked hierarchically (despite the fact that members are in theory equal to one another). Informally, the collective leadership is headed by a \"leadership core\"; that is, the paramount leader, the person who holds the offices of CCP general secretary, CMC chairman and PRC president. Before Jiang Zemin's tenure as paramount leader, the party core and collective leadership were indistinguishable. In practice, the core was not responsible to the collective leadership. However, by the time of Jiang, the party had begun propagating a responsibility system, referring to it in official pronouncements as the \"core of the collective leadership\". Academics have noted a decline in collective leadership under Xi Jinping.\nDemocratic centralism.\nThe CCP's organizational principle is democratic centralism, a principle that entails open discussion of policy on the condition of unity among party members in upholding the agreed-upon decision. It is based on two principles: democracy (synonymous in official discourse with \"socialist democracy\" and \"inner-party democracy\") and centralism. This has been the guiding organizational principle of the party since the 5th National Congress, held in 1927. In the words of the party constitution, \"The Party is an integral body organized under its program and constitution and on the basis of democratic centralism\". Mao once quipped that democratic centralism was \"at once democratic and centralized, with the two seeming opposites of democracy and centralization united in a definite form.\" Mao claimed that the superiority of democratic centralism lay in its internal contradictions, between democracy and centralism, and freedom and discipline. Currently, the CCP is claiming that \"democracy is the lifeline of the Party, the lifeline of socialism\". But for democracy to be implemented, and functioning properly, there needs to be centralization. Democracy in any form, the CCP claims, needs centralism, since without centralism there will be no order.\n\"Shuanggui\".\n\"Shuanggui\" is an intra-party disciplinary process conducted by the Central Commission for Discipline Inspection (CCDI), which conducts \"shuanggui\" on members accused of \"disciplinary violations\", a charge which generally refers to political corruption. The process, which literally translates to \"double regulation\", aims to extract confessions from members accused of violating party rules. According to the Dui Hua Foundation, tactics such as cigarette burns, beatings and simulated drowning are among those used to extract confessions. Other reported techniques include the use of induced hallucinations, with one subject of this method reporting that \"In the end I was so exhausted, I agreed to all the accusations against me even though they were false.\"\nUnited front.\nThe CCP employs a political strategy that it terms \"united front work\" that involves groups and key individuals that are influenced or controlled by the CCP and used to advance its interests. United front work is managed primarily but not exclusively by the United Front Work Department (UFWD). The united front has historically been a popular front that has included eight legally-permitted political parties alongside other people's organizations which have nominal representation in the National People's Congress and the Chinese People's Political Consultative Conference (CPPCC). However, the CPPCC is a body without real power. While consultation does take place, it is supervised and directed by the CCP. Under Xi Jinping, the united front and its targets of influence have expanded in size and scope.\nOrganization.\nCentral organization.\nThe National Congress is the party's highest body, and, since the 9th National Congress in 1969, has been convened every five years (prior to the 9th Congress they were convened on an irregular basis). According to the party's constitution, a congress may not be postponed except \"under extraordinary circumstances.\" The party constitution gives the National Congress six responsibilities:\nIn practice, the delegates rarely discuss issues at length at the National Congresses. Most substantive discussion takes place before the congress, in the preparation period, among a group of top party leaders. In between National Congresses, the Central Committee is the highest decision-making institution. The CCDI is responsible for supervising party's internal anti-corruption and ethics system. In between congresses the CCDI is under the authority of the Central Committee.\nThe Central Committee, as the party's highest decision-making institution between national congresses, elects several bodies to carry out its work. The first plenary session of a newly elected central committee elects the general secretary of the Central Committee, the party's leader; the Central Military Commission (CMC); the Politburo; the Politburo Standing Committee (PSC). The first plenum also endorses the composition of the Secretariat and the leadership of the CCDI. According to the party constitution, the general secretary must be a member of the Politburo Standing Committee (PSC), and is responsible for convening meetings of the PSC and the Politburo, while also presiding over the work of the Secretariat. The Politburo \"exercises the functions and powers of the Central Committee when a plenum is not in session\". The PSC is the party's highest decision-making institution when the Politburo, the Central Committee and the National Congress are not in session. It convenes at least once a week. It was established at the 8th National Congress, in 1958, to take over the policy-making role formerly assumed by the Secretariat. The Secretariat is the top implementation body of the Central Committee, and can make decisions within the policy framework established by the Politburo; it is also responsible for supervising the work of organizations that report directly into the Central Committee, for example departments, commissions, publications, and so on. The CMC is the highest decision-making institution on military affairs within the party, and controls the operations of the People's Liberation Army. The general secretary has, since Jiang Zemin, also served as Chairman of the CMC. Unlike the collective leadership ideal of other party organs, the CMC chairman acts as commander-in-chief with full authority to appoint or dismiss top military officers at will.\nA first plenum of the Central Committee also elects heads of departments, bureaus, central leading groups and other institutions to pursue its work during a term (a \"term\" being the period elapsing between national congresses, usually five years). The General Office is the party's \"nerve centre\", in charge of day-to-day administrative work, including communications, protocol, and setting agendas for meetings. The CCP currently has six main central departments: the Organization Department, responsible for overseeing provincial appointments and vetting cadres for future appointments, the Publicity Department (formerly \"Propaganda Department\"), which oversees the media and formulates the party line to the media, the United Front Work Department, which oversees the country's eight minor parties, people's organizations, and influence groups inside and outside of the country, the International Department, functioning as the party's \"foreign affairs ministry\" with other parties, the Social Work Department, which handles work related to civic groups, chambers of commerce and industry groups and mixed-ownership and non-public enterprises, and the Central Political and Legal Affairs Commission, which oversees the country's legal enforcement authorities. The CC also has direct control over the Central Policy Research Office, which is responsible for researching issues of significant interest to the party leadership, the Central Party School, which provides political training and ideological indoctrination in communist thought for high-ranking and rising cadres, the Institute of Party History and Literature, which sets priorities for scholarly research in state-run universities and the Central Party School and studies and translates the classical works of Marxism. The party's newspaper, the \"People's Daily\", is under the direct control of the Central Committee and is published with the objectives \"to tell good stories about China and the (Party)\" and to promote its party leader. The theoretical magazines \"Qiushi\" and \"Study Times\" are published by the Central Party School. The China Media Group, which oversees China Central Television (CCTV), China National Radio (CNR) and China Radio International (CRI), is under the direct control of the Publicity Department. The various offices of the \"Central Leading Groups\", such as the Hong Kong and Macau Work Office, the Taiwan Affairs Office, and the Central Finance Office, also report to the central committee during a plenary session. Additionally, CCP has sole control over the People's Liberation Army (PLA) through its Central Military Commission.\nLower-level organizations.\nAfter seizing political power, the CCP extended the dual party-state command system to all government institutions, social organizations, and economic entities. The State Council and the Supreme Court each has a party group, established since November 1949. Party committees permeate in every state administrative organ as well as the People's Consultation Conferences and mass organizations at all levels. According to scholar Rush Doshi, \"[t]he Party sits above the state, runs parallel to the state, and is enmeshed in every level of the state.\" Modelled after the Soviet Nomenklatura system, the party committee's organization department at each level has the power to recruit, train, monitor, appoint, and relocate these officials.\nParty committees exist at the level of provinces, cities, counties, and neighbourhoods. These committees play a key role in directing local policy by selecting local leaders and assigning critical tasks. The Party secretary at each level is more senior than that of the leader of the government, with the CCP standing committee being the main source of power. Party committee members in each level are selected by the leadership in the level above, with provincial leaders selected by the central Organizational Department, and not removable by the local party secretary. Neighborhood committees are generally composed of older volunteers.\nCCP committees exist inside of companies, both private and state-owned. A business that has more than three party members is legally required to establish a committee or branch. , more than half of China's private firms have such organizations. These branches provide places for new member socialization and host morale boosting events for existing members. They also provide mechanisms that help private firm interface with government bodies and learn about policies which relate to their fields. On average, the profitability of private firms with a CCP branch is 12.6 per cent higher than the profitability of private firms.\nWithin state-owned enterprises, these branches are governing bodies that make important decisions and inculcate CCP ideology in employees. Party committees or branches within companies also provide various benefits to employees. These may include bonuses, interest-free loans, mentorship programs, and free medical and other services for those in need. Enterprises that have party branches generally provide more expansive benefits for employees in the areas of retirement, medical care, unemployment, injury, and birth and fertility. Increasingly, the CCP is requiring private companies to revise their charters to include the role of the party.\nFunding.\nThe funding of all CCP organizations mainly comes from state fiscal revenue. Data for the proportion of total CCP organizations' expenditures in total China fiscal revenue is unavailable.\nMembers.\nThe CCP reached 99.19\u00a0million members at the end of 2023, a net increase of 1.1\u00a0million over the previous year. It is the second largest political party in the world after India's Bharatiya Janata Party.\nTo join the CCP, an applicant must go through an approval process. Adults can file applications for membership with their local party branch. A prescreening process, akin to a background check, follows. Next, established party members at the local branch vet applicants' behaviour and political attitudes and may make a formal inquiry to a party branch near the applicants' parents residence to vet family loyalty to communism and the party. In 2014, only 2 million applications were accepted out of some 22\u00a0million applicants. Admitted members then spend a year as a probationary member. Probationary members are typically accepted into the party. Members must pay dues regardless of location and, in 2019, the CCP Central Committee issued a rule requiring members abroad to contact CCP cells at home at least once every six months.\nIn contrast to the past, when emphasis was placed on the applicants' ideological criteria, the current CCP stresses technical and educational qualifications. To become a probationary member, the applicant must take an admission oath before the party flag. The relevant CCP organization is responsible for observing and educating probationary members. Probationary members have duties similar to those of full members, with the exception that they may not vote in party elections nor stand for election. Many join the CCP through the Communist Youth League. Under Jiang Zemin, private entrepreneurs were allowed to become party members.\nMembership demographics.\n, individuals who identify as farmers, herdsmen and fishermen make up 26 million members; members identifying as workers totalled 6.6 million. Another group, the \"Managing, professional and technical staff in enterprises and public institutions\", made up 16.2 million, 11.5 million identified as working in administrative staff and 7.6 million described themselves as party cadres. The CCP systematically recruits white-collar workers over other social groups. By 2023, CCP membership had become more educated, younger, and less blue-collar than previously, with 56.2% of party members having a college degree or above. , around 30 to 35 per cent of Chinese entrepreneurs are or have been a party member. At the end of 2023, the CCP stated that it has approximately 7.59\u00a0million ethnic minority members or 7.7% of the party.\nStatus of women.\n, 30.19 million women are CCP members, representing 30.4% of the party. Women in China have low participation rates as political leaders. Women's disadvantage is most evident in their severe underrepresentation in the more powerful political positions. At the top level of decision making, no woman has ever been among the members of the Politburo Standing Committee, while the broader Politburo currently does not have any female members. Just 3 of 27 government ministers are women, and importantly, since 1997, China has fallen to 53rd place from 16th in the world in terms of female representation in the National People's Congress, according to the Inter-Parliamentary Union. CCP leaders such as Zhao Ziyang have vigorously opposed the participation of women in the political process. Within the party women face a glass ceiling.\nBenefits of membership.\nA 2019 Binghamton University study found that CCP members gain a 20% wage premium in the market over non-members. A subsequent academic study found that the economic benefit of CCP membership is strongest on those in lower wealth brackets. CCP households also tend to accumulate wealth faster than non-CCP households. \nCertain CCP cadres have access to a special supply system for foodstuffs called \"tegong\". CCP leadership cadres have access to a dedicated healthcare system managed by the CCP General Office.\nCommunist Youth League.\nThe Communist Youth League (CYL) is the CCP's youth wing, and the largest mass organization for youth in China. To join, an applicant has to be between the ages of 14 and 28. It controls and supervises Young Pioneers, a youth organization for children below the age of 14. The organizational structure of CYL is an exact copy of the CCP's; the highest body is the National Congress, followed by the Central Committee, Politburo, and the Politburo Standing Committee. However, the Central Committee (and all central organs) of the CYL work under the guidance of the CCP central leadership. 2021 estimates put the number of CYL members at over 81 million.\nSymbols.\nAt the beginning of its history, the CCP did not have a single official standard for the flag, but instead allowed individual party committees to copy the flag of the Communist Party of the Soviet Union. The Central Politburo decreed the establishment of a sole official flag on 28 April 1942: \"The flag of the CPC has the length-to-width proportion of 3:2 with a hammer and sickle in the upper-left corner, and with no five-pointed star. The Political Bureau authorizes the General Office to custom-make a number of standard flags and distribute them to all major organs\".\nAccording to \"People's Daily\", \"The red color symbolizes revolution; the hammer-and-sickle are tools of workers and peasants, meaning that the Communist Party of China represents the interests of the masses and the people; the yellow color signifies brightness.\"\nParty-to-party relations.\nThe International Department of the Chinese Communist Party is responsible for dialogue with global political parties.\nCommunist parties.\nThe CCP continues to have relations with non-ruling communist and workers' parties and attends international communist conferences, most notably the International Meeting of Communist and Workers' Parties. While the CCP retains contact with major parties such as the Communist Party of Portugal, the Communist Party of France, the Communist Party of the Russian Federation, the Communist Party of Bohemia and Moravia, the Communist Party of Brazil, the Communist Party of Greece, the Communist Party of Nepal and the Communist Party of Spain, the party also retains relations with minor communist and workers' parties, such as the Communist Party of Australia, the Workers Party of Bangladesh, the Communist Party of Bangladesh (Marxist\u2013Leninist) (Barua), the Communist Party of Sri Lanka, the Workers' Party of Belgium, the Hungarian Workers' Party, the Dominican Workers' Party, the Nepal Workers Peasants Party, and the Party for the Transformation of Honduras, for instance. It has prickly relations with the Japanese Communist Party. In recent years, noting the self-reform of the European social democratic movement in the 1980s and 1990s, the CCP \"has noted the increased marginalization of West European communist parties.\"\nRuling parties of socialist states.\nThe CCP has retained close relations with the ruling parties of socialist states still espousing communism: Cuba, Laos, North Korea, and Vietnam. It spends a fair amount of time analysing the situation in the remaining socialist states, trying to reach conclusions as to why these states survived when so many did not, following the collapse of the Eastern European socialist states in 1989 and the dissolution of the Soviet Union in 1991. In general, the analyses of the remaining socialist states and their chances of survival have been positive, and the CCP believes that the socialist movement will be revitalized sometime in the future.\nThe ruling party which the CCP is most interested in is the Communist Party of Vietnam (CPV). In general the CPV is considered a model example of socialist development in the post-Soviet era. Chinese analysts on Vietnam believe that the introduction of the \u0110\u1ed5i M\u1edbi reform policy at the 6th CPV National Congress is the key reason for Vietnam's current success.\nWhile the CCP is probably the organization with most access to North Korea, writing about North Korea is tightly circumscribed. The few reports accessible to the general public are those about North Korean economic reforms. While Chinese analysts of North Korea tend to speak positively of North Korea in public, in official discussions they show much disdain for North Korea's economic system, the cult of personality which pervades society, the Kim family, the idea of hereditary succession in a socialist state, the security state, the use of scarce resources on the Korean People's Army and the general impoverishment of the North Korean people. Circa 2008, there are those analysts who compare the current situation of North Korea with that of China during the Cultural Revolution. Over the years, the CCP has tried to persuade the Workers' Party of Korea (or WPK, North Korea's ruling party) to introduce economic reforms by showing them key economic infrastructure in China. For instance, in 2006 the CCP invited then-WPK general secretary Kim Jong Il to Guangdong to showcase the success economic reforms had brought China. In general, the CCP considers the WPK and North Korea to be negative examples of a ruling communist party and socialist state.\nThere is a considerable degree of interest in Cuba within the CCP. Fidel Castro, the former First Secretary of the Communist Party of Cuba (PCC), is greatly admired, and books have been written focusing on the successes of the Cuban Revolution. Communication between the CCP and the PCC has increased since the 1990s. At the 4th Plenary Session of the 16th Central Committee, which discussed the possibility of the CCP learning from other ruling parties, praise was heaped on the PCC. When Wu Guanzheng, a Central Politburo member, met with Fidel Castro in 2007, he gave him a personal letter written by Hu Jintao: \"Facts have shown that China and Cuba are trustworthy good friends, good comrades, and good brothers who treat each other with sincerity. The two countries' friendship has withstood the test of a changeable international situation, and the friendship has been further strengthened and consolidated.\"\nNon-communist parties.\nSince the decline and fall of communism in Eastern Europe, the CCP has begun establishing party-to-party relations with non-communist parties. These relations are sought so that the CCP can learn from them. For instance, the CCP has been eager to understand how the People's Action Party of Singapore (PAP) maintains its total domination over Singaporean politics through its \"low-key presence, but total control.\" According to the CCP's own analysis of Singapore, the PAP's dominance can be explained by its \"well-developed social network, which controls constituencies effectively by extending its tentacles deeply into society through branches of government and party-controlled groups.\" While the CCP accepts that Singapore is a liberal democracy, they view it as a guided democracy led by the PAP. Other differences are, according to the CCP, \"that it is not a political party based on the working class\u2014instead it is a political party of the elite... It is also a political party of the parliamentary system, not a revolutionary party.\" Other parties which the CCP studies and maintains strong party-to-party relations with are the United Malays National Organization, which has ruled Malaysia (1957\u20132018, 2020\u20132022), and the Liberal Democratic Party in Japan, which dominated Japanese politics since 1955.\nSince Jiang Zemin's time, the CCP has made friendly overtures to its erstwhile foe, the Kuomintang. The CCP emphasizes strong party-to-party relations with the KMT so as to strengthen the probability of the reunification of Taiwan with mainland China. However, several studies have been written on the KMT's loss of power in 2000 after having ruled Taiwan since 1949 (the KMT officially ruled mainland China from 1928 to 1949). In general, one-party states or dominant-party states are of special interest to the party and party-to-party relations are formed so that the CCP can study them. The longevity of the Syrian Regional Branch of the Arab Socialist Ba'ath Party is attributed to the personalization of power in the al-Assad family, the strong presidential system, the inheritance of power, which passed from Hafez al-Assad to his son Bashar al-Assad, and the role given to the Syrian military in politics.\nCirca 2008, the CCP has been especially interested in Latin America, as shown by the increasing number of delegates sent to and received from these countries. Of special fascination for the CCP is the 71-year-long rule of the Institutional Revolutionary Party (PRI) in Mexico. While the CCP attributed the PRI's long reign in power to the strong presidential system, tapping into the machismo culture of the country, its nationalist posture, its close identification with the rural populace and the implementation of nationalization alongside the marketization of the economy, the CCP concluded that the PRI failed because of the lack of inner-party democracy, its pursuit of social democracy, its rigid party structures that could not be reformed, its political corruption, the pressure of globalization, and American interference in Mexican politics. While the CCP was slow to recognize the pink tide in Latin America, it has strengthened party-to-party relations with several socialist and anti-American political parties over the years. The CCP has occasionally expressed some irritation over Hugo Ch\u00e1vez's anti-capitalist and anti-American rhetoric. Despite this, the CCP reached an agreement in 2013 with the United Socialist Party of Venezuela (PSUV), which was founded by Ch\u00e1vez, for the CCP to educate PSUV cadres in political and social fields. By 2008, the CCP claimed to have established relations with 99 political parties in 29 Latin American countries.\nSocial democratic movements in Europe have been of great interest to the CCP since the early 1980s. With the exception of a short period in which the CCP forged party-to-party relations with far-right parties during the 1970s in an effort to halt \"Soviet expansionism\", the CCP's relations with European social democratic parties were its first serious efforts to establish cordial party-to-party relations with non-communist parties. The CCP credits the European social democrats with creating a \"capitalism with a human face\". Before the 1980s, the CCP had a highly negative and dismissive view of social democracy, a view dating back to the Second International and the Marxist\u2013Leninist view on the social democratic movement. By the 1980s, that view had changed and the CCP concluded that it could actually learn something from the social democratic movement. CCP delegates were sent all over Europe to observe. By the 1980s, most European social democratic parties were facing electoral decline and in a period of self-reform. The CCP followed this with great interest, laying most weight on reform efforts within the British Labour Party and the Social Democratic Party of Germany. The CCP concluded that both parties were re-elected because they modernized, replacing traditional state socialist tenets with new ones supporting privatization, shedding the belief in big government, conceiving a new view of the welfare state, changing their negative views of the market and moving from their traditional support base of trade unions to entrepreneurs, the young and students."}
{"id": "7176", "revid": "32903182", "url": "https://en.wikipedia.org/wiki?curid=7176", "title": "Cryogenics", "text": "In physics, cryogenics is the production and behaviour of materials at very low temperatures.\nThe 13th International Institute of Refrigeration's (IIR) International Congress of Refrigeration (held in Washington DC in 1971) endorsed a universal definition of \"cryogenics\" and \"cryogenic\" by accepting a threshold of to distinguish these terms from conventional refrigeration. This is a logical dividing line, since the normal boiling points of the so-called permanent gases (such as helium, hydrogen, neon, nitrogen, oxygen, and normal air) lie below 120\u00a0K, while the Freon refrigerants, hydrocarbons, and other common refrigerants have boiling points above 120\u00a0K.\nDiscovery of superconducting materials with critical temperatures significantly above the boiling point of nitrogen has provided new interest in reliable, low-cost methods of producing high-temperature cryogenic refrigeration. The term \"high temperature cryogenic\" describes temperatures ranging from above the boiling point of liquid nitrogen, , up to . The discovery of superconductive properties is first attributed to Heike Kamerlingh Onnes on July 10, 1908. The discovery came after the ability to reach a temperature of 2\u00a0K. These first superconductive properties were observed in mercury at a temperature of 4.2\u00a0K.\nCryogenicists use the Kelvin or Rankine temperature scale, both of which measure from absolute zero, rather than more usual scales such as Celsius which measures from the freezing point of water at sea level or Fahrenheit which measures from the freezing point of a particular brine solution at sea level.\nEtymology.\nThe word \"cryogenics\" stems from Greek \"\u03ba\u03c1\u03cd\u03bf\u03c2 (cryos)\" \u2013 \"cold\" + \"\u03b3\u03b5\u03bd\u03ae\u03c2 (genis)\" \u2013 \"generating\".\nCryogenic fluids.\nCryogenic fluids with their boiling point in Kelvin and degree Celsius.\nIndustrial applications.\nLiquefied gases, such as liquid nitrogen and liquid helium, are used in many cryogenic applications. Liquid nitrogen is the most commonly used element in cryogenics and is legally purchasable around the world. Liquid helium is also commonly used and allows for the lowest attainable temperatures to be reached.\nThese liquids may be stored in Dewar flasks, which are double-walled containers with a high vacuum between the walls to reduce heat transfer into the liquid. Typical laboratory Dewar flasks are spherical, made of glass and protected in a metal outer container. Dewar flasks for extremely cold liquids such as liquid helium have another double-walled container filled with liquid nitrogen. Dewar flasks are named after their inventor, James Dewar, the man who first liquefied hydrogen. Thermos bottles are smaller vacuum flasks fitted in a protective casing.\nCryogenic barcode labels are used to mark Dewar flasks containing these liquids, and will not frost over down to \u2212195 degrees Celsius.\nCryogenic transfer pumps are the pumps used on LNG piers to transfer liquefied natural gas from LNG carriers to LNG storage tanks, as are cryogenic valves.\nCryogenic processing.\nThe field of cryogenics advanced during World War II when scientists found that metals frozen to low temperatures showed more resistance to wear. Based on this theory of cryogenic hardening, the commercial cryogenic processing industry was founded in 1966 by Bill and Ed Busch. With a background in the heat treating industry, the Busch brothers founded a company in Detroit called CryoTech in 1966. Busch originally experimented with the possibility of increasing the life of metal tools to anywhere between 200% and 400% of the original life expectancy using cryogenic tempering instead of heat treating. This evolved in the late 1990s into the treatment of other parts.\nCryogens, such as liquid nitrogen, are further used for specialty chilling and freezing applications. Some chemical reactions, like those used to produce the active ingredients for the popular statin drugs, must occur at low temperatures of approximately . Special cryogenic chemical reactors are used to remove reaction heat and provide a low temperature environment. The freezing of foods and biotechnology products, like vaccines, requires nitrogen in blast freezing or immersion freezing systems. Certain soft or elastic materials become hard and brittle at very low temperatures, which makes cryogenic milling (cryomilling) an option for some materials that cannot easily be milled at higher temperatures.\nCryogenic processing is not a substitute for heat treatment, but rather an extension of the heating\u2013quenching\u2013tempering cycle. Normally, when an item is quenched, the final temperature is ambient. The only reason for this is that most heat treaters do not have cooling equipment. There is nothing metallurgically significant about ambient temperature. The cryogenic process continues this action from ambient temperature down to .\nIn most instances the cryogenic cycle is followed by a heat tempering procedure. As all alloys do not have the same chemical constituents, the tempering procedure varies according to the material's chemical composition, thermal history and/or a tool's particular service application.\nThe entire process takes 3\u20134 days.\nFuels.\nAnother use of cryogenics is cryogenic fuels for rockets with liquid hydrogen as the most widely used example. Liquid oxygen (LOX) is even more widely used but as an oxidizer, not a fuel. NASA's workhorse Space Shuttle used cryogenic hydrogen/oxygen propellant as its primary means of getting into orbit. LOX is also widely used with RP-1 kerosene, a non-cryogenic hydrocarbon, such as in the rockets built for the Soviet space program by Sergei Korolev.\nRussian aircraft manufacturer Tupolev developed a version of its popular design Tu-154 with a cryogenic fuel system, known as the Tu-155. The plane uses a fuel referred to as liquefied natural gas or LNG, and made its first flight in 1989.\nOther applications.\nSome applications of cryogenics:\nProduction.\nCryogenic cooling of devices and material is usually achieved via the use of liquid nitrogen, liquid helium, or a mechanical cryocooler (which uses high-pressure helium lines). Gifford-McMahon cryocoolers, pulse tube cryocoolers and Stirling cryocoolers are in wide use with selection based on required base temperature and cooling capacity. The most recent development in cryogenics is the use of magnets as regenerators as well as refrigerators. These devices work on the principle known as the magnetocaloric effect.\nDetectors.\nThere are various cryogenic detectors which are used to detect particles.\nFor cryogenic temperature measurement down to 30 K, Pt100 sensors, a resistance temperature detector (RTD), are used. For temperatures lower than 30 K, it is necessary to use a silicon diode for accuracy."}
{"id": "7179", "revid": "1273184189", "url": "https://en.wikipedia.org/wiki?curid=7179", "title": "Cary Elwes", "text": "Ivan Simon Cary Elwes (; born 26 October 1962) is an English actor. He starred as Westley in \"The Princess Bride\" (1987), and also had lead roles in films such as ' (1993) and the \"Saw\" series. The accolades he has received include nominations for a Screen Actors Guild Award and two Satellite Awards. Elwes' other performances in films include \"Glory\" (1989), \"Days of Thunder\" (1990), \"Hot Shots!\" (1991), \"Bram Stoker's Dracula\" (1992), \"Twister\" (1996), \"Kiss the Girls\" (1997), \"Liar Liar\" (1997), \"Shadow of the Vampire\" (2000), \"The Cat's Meow\" (2001), \"Ella Enchanted\" (2004), \"Pope John Paul II\" (2005), \"No Strings Attached\" (2011), \"Burning at Both Ends\" (2022), \"BlackBerry,\" and ' (both 2023).\nElwes has appeared on television in a number of series including \"The X-Files\", \"Seinfeld\", \"From the Earth to the Moon\", \"Psych\", and \"Life in Pieces\". In 2019, he appeared in the Netflix drama series \"Stranger Things,\" the Amazon Prime comedy series \"The Marvelous Mrs. Maisel,\" and in 2024, he appeared in the Paramount+ comedy series \"Knuckles\". Elwes has written a memoir of his time working on \"The Princess Bride\" called \"As You Wish\", which was published in 2014.\nEarly life and education.\nElwes was born on 26 October 1962 in Westminster, London. He is the youngest of three sons of portrait painter Dominick Elwes and Tessa Kennedy, an interior designer and socialite. Cary is the brother of artist Damian Elwes and film producers Cassian Elwes and Milica Kastner. Cary's stepfather, Elliott Kastner, was an American film producer and the first American to set up independent film production in the United Kingdom. Cary's paternal grandfather was the portrait painter Simon Elwes, whose own father was the diplomat and tenor Gervase Elwes (1866\u20131921).\nOne of Cary Elwes' relatives is John Elwes, a British miser who was the inspiration for Ebenezer Scrooge in \"A Christmas Carol\" (1843), having been referenced by Charles Dickens himself in chapter six of his last completed novel, \"Our Mutual Friend\". Elwes himself played five roles in the 2009 film adaptation of \"A Christmas Carol\". Through his maternal grandfather, Elwes is also related to Sir Alexander William \"Blackie\" Kennedy, one of the first photographers to document the archaeological site of Petra following the collapse of the Ottoman Empire.\nElwes was brought up as a Catholic and was an altar boy at Westminster Cathedral. His paternal relatives include such clerics as Dudley Charles Cary-Elwes (1868\u20131932), the Bishop of Northampton, and Abbot Columba Cary-Elwes (Ampleforth Abbey, Saint Louis Abbey). He discussed this in an interview while he was filming the 2005 CBS television film \"Pope John Paul II\", in which he played the young priest Karol Wojty\u0142a.\nElwes's parents divorced when he was four years old. In 1975, when Elwes was 13, his father died by suicide. He was educated at Harrow School, and the London Academy of Music and Dramatic Art. In 1981, he moved to the United States to study acting at Sarah Lawrence College in Bronxville, New York. While living there, Elwes studied acting at both the Actors Studio and the Lee Strasberg Theatre and Film Institute under the tutelage of Al Pacino's mentor, Charlie Laughton (not to be confused with English actor Charles Laughton). As a teenager, he also worked as a production assistant on the films \"Absolution\", \"Octopussy\", and \"Superman\", where he was assigned to Marlon Brando. When Elwes introduced himself to the actor, Brando insisted on calling him \"Rocky\" after Rocky Marciano.\nCareer.\n1984\u20131999.\nElwes made his acting debut in 1984 in Marek Kanievska's film \"Another Country\", which was loosely based on the English boarding school exploits of British spies Burgess, Philby and MacLean. He played James Harcourt, a gay student. He then played Guilford Dudley in the British historical drama film \"Lady Jane\", opposite Helena Bonham Carter. He was cast as stable-boy-turned-swashbuckler Westley in Rob Reiner's fantasy-comedy \"The Princess Bride\" (1987), which was based on the novel of the same name by William Goldman. It was a modest box office success, but received critical acclaim. As a result of years of reviews, it earned a score of 97% on the review aggregation website Rotten Tomatoes. Since being released on home video and television, the film has become a cult classic.\nElwes continued to work steadily, varying between dramatic roles, such as in the Oscar-winning \"Glory\" (1989) and comedic roles, as in \"Hot Shots!\" (1991). He played a rival driver to Tom Cruise in \"Days of Thunder\" (1990). In 1993, he starred as Robin Hood in Mel Brooks's comedy \"\". Elwes then appeared in supporting roles in such films as Francis Ford Coppola's adaptation of \"Bram Stoker's Dracula\" (1992), \"The Crush\" (1993), \"The Jungle Book\" (1994), \"Twister\" (1996), \"Liar Liar\" (1997), and \"Kiss the Girls\". In 1999, he portrayed famed theatre and film producer John Houseman for Tim Robbins in his ensemble film based on Orson Welles's musical, \"Cradle Will Rock\". Following that, he travelled to Luxembourg to work with John Malkovich and Willem Dafoe in \"Shadow of the Vampire\".\nElwes made his first television appearance in 1996 as David Lookner on \"Seinfeld\". Two years later he played astronaut Michael Collins in the Golden Globe Award-winning HBO miniseries \"From the Earth To the Moon\". The following year Elwes was nominated for a Golden Satellite Award for Best Performance by an Actor in a Mini-Series or Motion Picture Made for Television for his portrayal of Colonel James Burton in \"The Pentagon Wars\" directed by Richard Benjamin. In 1999, he guest starred as Dr. John York in an episode of the television series \"The Outer Limits\".\n2000\u20132009.\nIn 2001, he co-starred in Peter Bogdanovich's ensemble film \"The Cat's Meow\" portraying film mogul Thomas Ince, who died mysteriously while vacationing with William Randolph Hearst on his yacht. Shortly afterward Elwes received another Golden Satellite Award nomination for his work on the ensemble NBC Television film \"Uprising\" opposite Jon Voight directed by Jon Avnet. Elwes had a recurring role in the final season (from 2001 to 2002) of Chris Carter's hit series \"The X-Files\" as FBI Assistant Director Brad Follmer. In 2003 Elwes portrayed Kerry Max Cook in the off-Broadway play \"The Exonerated\" in New York, directed by Bob Balaban (18\u201323 March 2003).\nIn 2004, Elwes starred in the horror\u2013thriller \"Saw\" which, at a budget of a little over $1 million, grossed over $100 million worldwide. The same year he appeared in \"Ella Enchanted\", this time as the villain, not the hero. Also in 2004, he portrayed serial killer Ted Bundy in the A&amp;E Network film \"The Riverman\", which became one of the highest rated original films in the network's history and garnered a prestigious BANFF Rockie Award nomination. The following year, Elwes played the young Karol Wojty\u0142a in the CBS television film \"Pope John Paul II\". The TV film was highly successful not only in North America but also in Europe, where it broke box office records in the late Pope's native Poland and became the first film ever to break $1 million in three days. He made an uncredited appearance as Sam Green, the man who introduced Andy Warhol to Edie Sedgwick, in the 2006 film \"Factory Girl\". In 2007, he appeared in Garry Marshall's \"Georgia Rule\" opposite Jane Fonda.\nIn 2007, he made a guest appearance on the ' episode \"\" as a Mafia lawyer. In 2009, he played the role of Pierre Despereaux, an international art thief, in the fourth-season premiere of \"Psych\". Also in 2009 Elwes joined the cast of Robert Zemeckis's motion capture adaptation of Charles Dickens' \"A Christmas Carol\" portraying five roles. That same year he was chosen by Steven Spielberg to appear in his motion capture adaptation of Belgian artist Herg\u00e9's popular comic strip '.\nElwes's voice-over work includes the narrator in James Patterson's audiobook \"The Jester\", as well as characters in film and television animations such as \"Quest for Camelot\", \"Pinky and The Brain\", \"Batman Beyond\", and the English versions of the Studio Ghibli films, \"Porco Rosso\", \"Whisper of the Heart\" and \"The Cat Returns\". For the 2004 video game \"The Bard's Tale\", he served as screenwriter, improviser, and voice actor of the main character The Bard. In 2009, Elwes reunited with Jason Alexander for the Indian film, \"Delhi Safari\". The following year Elwes portrayed the part of Gremlin Gus in Disney's video game, '. In 2014, he appeared in ' as the voice of scientists Edmond Halley and Robert Hooke.\n2010\u2013present.\nIn 2010, he returned to the \"Saw\" franchise in \"Saw 3D\" (2010), the seventh film in the series, as Dr. Lawrence Gordon. In 2010, he returned to \"Psych\", reprising his role in the second half of the fifth season, again in the show's sixth season, and again in the show's eighth season premiere. In 2014, Elwes played Hugh Ashmeade, Director of the CIA, in the second season of the BYUtv series \"Granite Flats\". In 2011, he was selected by Ivan Reitman to star alongside Natalie Portman in \"No Strings Attached\". That same year, Elwes and Garry Marshall teamed up again in the ensemble romantic comedy \"New Year's Eve\" opposite Robert de Niro and Halle Berry.\nIn 2012, Elwes starred in the independent drama \"The Citizen\". and the following year Elwes joined Selena Gomez for the comedy ensemble, \"Behaving Badly\" directed by Tim Garrick. In 2015, he completed \"Sugar Mountain\" directed by Richard Gray; the drama \"We Don't Belong Here\", opposite Anton Yelchin and Catherine Keener directed by Peer Pedersen, and \"Being Charlie\" which reunited Elwes with director Rob Reiner after 28 years and premiered at the Toronto International Film Festival. In 2016, Elwes starred opposite Penelope Cruz in Fernando Trueba's Spanish-language period pic \"The Queen of Spain\", a sequel to Trueba's 1998 drama \"The Girl of Your Dreams\". This also re-united Elwes with his \"Princess Bride\" co-star, Mandy Patinkin.\nIn October 2014 Touchstone (Simon &amp; Schuster) published Elwes's memoir of the making of \"The Princess Bride\", entitled \"As You Wish: Inconceivable Tales from the Making of The Princess Bride\", which he co-wrote with Joe Layden. The book featured never-before-told stories, exclusive behind-the-scenes photographs, and interviews with co-stars Robin Wright, Wallace Shawn, Billy Crystal, Christopher Guest, Fred Savage and Mandy Patinkin, as well as screenwriter William Goldman, producer Norman Lear, and director Rob Reiner. The book debuted on \"The New York Times\" Best Seller list.\nIn 2014, Elwes co-wrote the screenplay for a film entitled \"Elvis &amp; Nixon\", about the pair's famous meeting at the White House in 1970. The film starred Michael Shannon and Kevin Spacey; it was bought by Amazon as their first theatrical feature and released on 22 April 2016. In May 2015, Elwes was cast as Arthur Davenport, a shrewd and eccentric world-class collector of illegal art and antiquities in Crackle's first streaming network series drama, \"The Art of More\", which explored the cutthroat world of premium auction houses. The series debuted on 19 November and was picked up for a second season.\nIn April 2018 Elwes portrayed Larry Kline, mayor of Hawkins, for the third season of the Netflix series \"Stranger Things\", which premiered in July 2019. He was nominated along with the cast for the Screen Actors Guild Award for Outstanding Performance by an Ensemble in a Drama Series. In May 2019, he joined the third season of the Amazon series \"The Marvelous Mrs. Maisel\" as Gavin Hawk.\nPersonal life.\nElwes met photographer Lisa Marie Kurbikoff in 1991 at a chili cook-off in Malibu, California; they were engaged in 1997. They married in 2000 and have one daughter. Elwes and his family lost their home in the Palisades Fire during the January 2025 Southern California wildfires, but evacuated safely.\nIn March 2021, Elwes posted on his social media accounts that his younger half-sister Milica had died after battling Stage 4 cancer for more than a year.\nElwes is known for his feud with Republican Texas Senator and \"Princess Bride\" fan Ted Cruz. According to the \"Hollywood Reporter\", Elwes initiated the 2020 fundraiser that re-united many \"Princess Bride\" cast members to support Joe Biden in the battleground state of Wisconsin. The \"Princess Bride\" Reunion raised more than $4 million for Wisconsin Democrats.\n\"Saw\" lawsuit.\nIn August 2005, Elwes filed a lawsuit against Evolution Entertainment, his management firm and producer of \"Saw\". Elwes said he was promised a minimum of 1% of the producers' net profits and did not receive the full amount. The case was settled out of court. Elwes returned to the series in 2010 reprising his role in \"Saw 3D\"."}
{"id": "7180", "revid": "16495000", "url": "https://en.wikipedia.org/wiki?curid=7180", "title": "Chris Sarandon", "text": "Christopher Sarandon (; born July 24, 1942) is an American actor. He is well known for playing Jerry Dandrige in \"Fright Night\" (1985), Prince Humperdinck in \"The Princess Bride\" (1987), Detective Mike Norris in \"Child's Play\" (1988), and Jack Skellington\u2019s speaking voice in \"The Nightmare Before Christmas\" (1993). He was nominated for the Academy Award for Best Supporting Actor for his performance as Leon Shermer in \"Dog Day Afternoon\" (1975).\nEarly life.\nChris Sarandon was born and raised in Beckley, West Virginia, the son of Greek-American restaurateurs Chris and Cliffie (n\u00e9e Cardullias) Sarandon. His father, whose surname was originally \"Sarondonethes\", was born to Greek parents in Istanbul, Turkey.\nSarandon graduated from Woodrow Wilson High School in Beckley. He earned a degree in speech at West Virginia University, and earned his master's degree in theater from Catholic University of America (CUA) in Washington, D.C.\nCareer.\nAfter graduation, he toured with numerous improvisational companies and became much involved with regional theatre, making his professional debut in the play \"The Rose Tattoo\" during 1965. In the summer of 1968 he and his then-wife, Susan Sarandon, worked as actors at the Wayside Theatre in Middletown, Virginia. Later that year Sarandon moved to New York City, where he obtained his first television role as Dr. Tom Halverson for the series \"The Guiding Light\" (1973\u20131974). He appeared in the primetime television movies \"The Satan Murders\" (1974) and \"Thursday's Game\" before obtaining the role in \"Dog Day Afternoon\" (1975), a performance which earned him nominations for Best New Male Star of the Year at the Golden Globes and the Academy Award for Best Supporting Actor.\nSarandon appeared in the Broadway play \"The Rothschilds\" and \"The Two Gentlemen of Verona\", as well making regular appearances at numerous Shakespeare and George Bernard Shaw festivals in the United States and Canada. He also had a series of television roles, some of which (such as \"A Tale of Two Cities\" in 1980) corresponded to his affinity for the classics. He also had roles in the thriller movie \"Lipstick\" (1976) and as a demon in the movie \"The Sentinel\" (1977).\nTo avoid being typecast in villainous roles, Sarandon accepted various roles of other types during the years to come, portraying the title role of Christ in the made-for-television movie \"The Day Christ Died\" (1980). He received accolades for his portrayal of Sydney Carton in a TV-movie version of \"A Tale of Two Cities\" (1980), co-starred with Dennis Hopper in the 1983 movie \"The Osterman Weekend\", which was based on the Robert Ludlum novel of the same name, and co-starred with Goldie Hawn in the movie \"Protocol\" (1984). These were followed by another mainstream success as the vampire-next-door in the horror movie \"Fright Night\" (1985). He starred in the 1986 TV movie \"Liberty\", which addressed the making of New York City's Statue of Liberty.\nOne of his most endearing roles onscreen is that of Prince Humperdinck in Rob Reiner's 1987 movie \"The Princess Bride\", though he also has had supporting parts in many other successful films, including his lead turn in the original horror classic \"Child's Play\" (1988). In 1992, he played Joseph Curwen/Charles Dexter Ward in \"The Resurrected\". He also played Jack Skellington, the main character of Tim Burton's animated Disney movie \"The Nightmare Before Christmas\" (1993), and has since reprised the role in other productions, including the Disney/Square video games \"Kingdom Hearts\" and \"Kingdom Hearts II\" and the Capcom sequel to the original movie, \"\". Sarandon also reprised his role as Jack Skellington for several Disneyland Halloween events and attractions including; \"Halloween Screams\", the \"Frightfully Fun Parade,\" and the Haunted Mansion Holiday, a three-month overlay of the Haunted Mansion, where Jack and his friends take control of a mansion in an attempt to introduce Christmas, much as his character did in the movie.\nSarandon appeared in TV again with a recurring role as Dr. Burke on NBC's long-running medical drama \"ER\".\nIn 1991 he performed on Broadway in the short-lived musical \"Nick &amp; Nora\" (based on the movie \"The Thin Man\") with Joanna Gleason, the daughter of Monty Hall. Sarandon married Gleason in 1994. They have appeared together in a number of movies, including \"Edie &amp; Pen\" (1996), \"American Perfekt\" (1997), and \"Let the Devil Wear Black\" (1999). During the 2000s he made guest appearances in several TV series, notably as the Necromancer demon, Armand, in \"Charmed\", and as superior court judge Barry Krumble for six episodes of \"Judging Amy\".\nIn 2006 he played Signor Naccarelli in the six-time Tony award-winning Broadway musical play \"The Light in the Piazza\" at Lincoln Center. Most recently he appeared in \"Cyrano de Bergerac\" as Antoine de Guiche, with Kevin Kline, Jennifer Garner, and Daniel Sunjata.\nIn 2016 he performed in the Off-Broadway production of the Dave Malloy musical \"Preludes\" as Anton Chekhov, Tchaikovsky, Alexander Glazunov, Leo Tolstoy, Tsar Nicholas II, and The Master.\nHe is on the advisory board for the Greenbrier Valley Theatre in Lewisburg, West Virginia.\nPersonal life.\nSarandon has been married three times: he married actress Susan Sarandon in 1967. The two met while attending Catholic University of America together in Washington, D.C. The marriage lasted for twelve years; the pair divorced in 1979. After his divorce from Susan, he married his second wife, fashion model Lisa Ann Cooper, in 1980. The couple had two daughters and one son: Stephanie (born 1982), Alexis (born 1984), and Michael (born 1988). After nine years, the marriage ended in divorce in 1989. \nIn 1994, he married his third wife, actress and singer Joanna Gleason. The couple met while performing in Broadway's short-lived 1991 musical \"Nick &amp; Nora\"; they returned to the stage together in 1998's \"Thorn and Bloom\". They also collaborated in several films together, such as \"Road Ends\", \"Edie &amp; Pen\", \"Let the Devil Wear Black\", and \"American Perfekt\".\nSarandon is a member of the Greek Orthodox Church."}
{"id": "7182", "revid": "48868964", "url": "https://en.wikipedia.org/wiki?curid=7182", "title": "Christopher Guest", "text": "Christopher Haden-Guest, 5th Baron Haden-Guest (born 5 February 1948), known professionally as Christopher Guest, is a British-American actor, comedian, screenwriter and director. Guest has written, directed, and starred in his series of comedy films shot in mockumentary style. He wrote and acted in the rock satire \"This Is Spinal Tap\" (1984), and later directed a string of satirical mockumentary films such as \"Waiting for Guffman\" (1996), \"Best in Show\" (2000), \"A Mighty Wind\" (2003), \"For Your Consideration\" (2006), and \"Mascots\" (2016).\nGuest holds a hereditary British peerage as the 5th Baron Haden-Guest, but has publicly expressed a desire to see the House of Lords reformed as a democratically elected chamber. Though he was initially active in the Lords, his career there was cut short by the House of Lords Act 1999, which removed the right of most hereditary peers to a seat in the parliament. When using his title, he is normally styled as Lord Haden-Guest. Guest is married to the actress Jamie Lee Curtis, who is styled as The Right Honourable the Lady Haden-Guest; however, she opts not to use her title.\nEarly life.\nGuest was born in New York City, the son of Peter Haden-Guest, a British United Nations diplomat who later became the 4th Baron Haden-Guest, and his second wife, the former Jean Pauline Hindes, an American former vice president of casting at CBS. Guest's paternal grandfather, Leslie, Baron Haden-Guest, was a Labour Party politician, who was a convert to Judaism. Guest's paternal grandmother, a descendant of the Dutch Jewish Goldsmid family, was the daughter of Colonel Albert Goldsmid, a British officer who founded the Jewish Lads' and Girls' Brigade and the Maccabaeans. Guest's maternal grandparents were Jewish emigrants from Russia. Both of Guest's parents had become atheists, and Guest himself had no religious upbringing. In 1938, his uncle, David Guest, a lecturer and Communist Party member, was killed in the Spanish Civil War, fighting in the International Brigades.\nGuest spent parts of his childhood in his father's native United Kingdom. He attended the High School of Music &amp; Art (New York City), studying classical music (clarinet) at the Stockbridge School in the village of Interlaken in Stockbridge, Massachusetts. He later took up the mandolin, became interested in country music, and played guitar with Arlo Guthrie, a fellow student at Stockbridge School. Guest later began performing with bluegrass bands until he took up rock and roll. Guest went to Bard College for a year and then studied acting at New York University's Graduate Acting Program at the Tisch School of the Arts, graduating in 1971.\nCareer.\n1970s.\nGuest began his career in theatre during the early 1970s with one of his earliest professional performances being the role of Norman in Michael Weller's \"Moonchildren\" for the play's American premiere at the Arena Stage in Washington, DC, in November 1971. Guest continued with the production when it moved to Broadway in 1972. The following year, he began making contributions to \"The National Lampoon Radio Hour\" for a variety of National Lampoon audio recordings. He both performed comic characters (Flash Bazbo\u2014Space Explorer, Mr. Rogers, music critic Roger de Swans, and sleazy record company rep Ron Fields) and wrote, arranged, and performed numerous musical parodies (of Bob Dylan, James Taylor, and others). He was featured alongside Chevy Chase and John Belushi in the off-Broadway revue \"National Lampoon's Lemmings\". Two of his earliest film roles were small parts as uniformed police officers in the 1972 film \"The Hot Rock\" and 1974's \"Death Wish\".\nGuest played a small role in the 1977 \"All in the Family\" episode \"\", where in a flashback sequence Mike and Gloria recall their first blind date, set up by Michael's college buddy Jim (Guest), who dated Gloria's girlfriend Debbie (Priscilla Lopez).\nGuest also had a small but important role in \"it Happened One Christmas\", the 1977 gender-reversed TV remake of the Frank Capra classic \"it's a Wonderful Life,\" starring Marlo Thomas as Mary Bailey (the Jimmy Stewart role), with Cloris Leachman as Mary's guardian angel and Orson Welles as the villainous Mr. Potter. Guest played Mary's brother Harry, who returned from the Army in the final scene, speaking one of the last lines of the film: \"A toast! To my big sister Mary, the richest person in town!\"\n1980s.\nGuest's biggest role of the first two decades of his career is likely that of Nigel Tufnel in the 1984 Rob Reiner film \"This Is Spinal Tap\". Guest made his first appearance as Tufnel on the 1978 sketch comedy program \"The TV Show\".\nAlong with Martin Short, Billy Crystal, and Harry Shearer, Guest was hired as a one-year-only cast member for the 1984\u20131985 season on NBC's \"Saturday Night Live\". Recurring characters on SNL played by Guest include Frankie, of Willie and Frankie (coworkers who recount in detail physically painful situations in which they have found themselves, remarking laconically \"I hate when that happens\"); Herb Minkman, a novelty toymaker with his brother Al (played by Crystal); Rajeev Vindaloo, an eccentric foreign man in the same vein as Andy Kaufman's Latka character from \"Taxi\"; and Se\u00f1or Cosa, a Spanish ventriloquist often seen on the recurring spoof of \"The Joe Franklin Show\". He also experimented behind the camera with pre-filmed sketches, notably directing a documentary-style short starring Shearer and Short as synchronized swimmers. In another short film from SNL, Guest and Crystal appear in blackface as retired Negro league baseball players, \"The Rooster and the King\".\nHe appeared as Count Rugen (the \"six-fingered man\") in \"The Princess Bride\". He had a cameo role as the first customer, a pedestrian, in the 1986 musical remake of \"The Little Shop of Horrors\". As a co-writer and director, Guest made the Hollywood satire \"The Big Picture\".\nUpon his father succeeding to the family peerage in 1987, he was known as \"the Hon. Christopher Haden-Guest\". This was his official style and name until he inherited the barony in 1996.\n1990\u2013present.\nThe experience of making \"This is Spinal Tap\" directly informed the second phase of his career. Starting in 1996, Guest began writing, directing, and acting in his own series of substantially improvised films. Many of them are considered definitive examples of what came to be known as \"mockumentaries\"\u2014not a term Guest appreciates.\nTogether, Guest, his frequent writing partner Eugene Levy, and a small band of actors have formed a loose repertory group, which appears in several films. These include Catherine O'Hara, Michael McKean, Parker Posey, Bob Balaban, Jane Lynch, John Michael Higgins, Harry Shearer, Jennifer Coolidge, Ed Begley Jr., Jim Piddock and Fred Willard. Guest and Levy write backgrounds for each of the characters and notecards for each specific scene, outlining the plot, and then leave it up to the actors to improvise the dialogue, which is supposed to result in a much more natural conversation than scripted dialogue would. Typically, everyone who appears in these movies receives the same fee and the same portion of profits. Among the films performed in this manner, which have been written and directed by Guest, include \"Waiting for Guffman\" (1996), about a community theatre group, \"Best in Show\" (2000), about the dog show circuit, \"A Mighty Wind\" (2003), about folk singers, \"For Your Consideration\" (2006), about the hype surrounding Oscar season, and \"Mascots\" (2016), about a sports team mascot competition.\nGuest had a guest voice-over role in the animated comedy series \"SpongeBob SquarePants\" as SpongeBob's cousin, Stanley.\nGuest again collaborated with Reiner in \"A Few Good Men\" (1992), appearing as Dr. Stone. In the 2000s, Guest appeared in the 2005 biographical musical \"Mrs Henderson Presents\" and in the 2009 comedy \"The Invention of Lying\".\nHe is also currently a member of the musical group The Beyman Bros, which he formed with childhood friend David Nichtern and Spinal Tap's current keyboardist C. J. Vanston. Their debut album \"Memories of Summer as a Child\" was released on January 20, 2009.\nIn 2010, the United States Census Bureau paid $2.5\u00a0million to have a television commercial directed by Guest shown during television coverage of Super Bowl XLIV.\nGuest holds an honorary doctorate from and is a member of the board of trustees for Berklee College of Music in Boston.\nIn 2013, Guest was the co-writer and producer of the HBO series \"Family Tree,\" in collaboration with Jim Piddock, a lighthearted story in the style he made famous in \"This is Spinal Tap\", in which the main character, Tom Chadwick, inherits a box of curios from his great aunt, spurring interest in his ancestry.\nOn August 11, 2015, Netflix announced that \"Mascots\", a film directed by Guest and co-written with Jim Piddock, about the competition for the World Mascot Association championship's Gold Fluffy Award, would debut in 2016.\nGuest replayed his role as Count Tyrone Rugen in the \"Princess Bride\" Reunion on September 13, 2020.\nFamily.\nGuest became the 5th Baron Haden-Guest, of Great Saling, in the County of Essex, when his father died in 1996. He succeeded upon the ineligibility of his older half-brother, Anthony Haden-Guest, who was born before his parents married. According to a 2004 article in \"The Guardian\", Guest attended the House of Lords regularly until the House of Lords Act 1999 barred most hereditary peers from their seats. In the article Guest remarked:\nGuest married actress Jamie Lee Curtis in 1984 at the home of their mutual friend Rob Reiner. They have two daughters, through adoption. Guest was played by Seth Green in the film \"A Futile and Stupid Gesture.\"\nFilmography.\nRecurring cast members.\nGuest has worked multiple times with certain actors, notably with frequent writing partner Eugene Levy, who has appeared in five of his projects. Other repeat collaborators of Guest include Fred Willard (7 projects); Michael McKean, Bob Balaban, and Ed Begley Jr. (6 projects each); Paul Benedict, Parker Posey, Jim Piddock, Michael Hitchcock and Harry Shearer (5 projects each); Catherine O'Hara, Larry Miller, John Michael Higgins, Jane Lynch, and Jennifer Coolidge (4 projects each); Fran Drescher and Rob Reiner (3 projects each)"}
{"id": "7183", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=7183", "title": "Carol Kane", "text": "Carolyn Laurie Kane (born June 18, 1952) is an American actress. She gained recognition for her role in \"Hester Street\" (1975), for which she received an Academy Award nomination for Best Actress. She became known in the 1970s and 1980s in films such as \"Dog Day Afternoon\" (1975), \"Annie Hall\" (1977), \"When a Stranger Calls\" (1979), \"The Princess Bride\" (1987), \"Scrooged\" (1988) and \"Flashback\" (1990).\nKane appeared on the television series \"Taxi\" in the early 1980s, as Simka Gravas, the wife of Latka, the character played by Andy Kaufman, winning two Emmy Awards for her work. She has played the character of Madame Morrible in the musical \"Wicked\", both in touring productions and on Broadway from 2005 to 2014. From 2015 to 2020, she was a main cast member on the Netflix series \"Unbreakable Kimmy Schmidt\", in which she played Lillian Kaushtupper. She currently plays the recurring role of Pelia in \"\" (2023present).\nEarly life.\nKane was born on June 18, 1952, in Cleveland, the daughter of Joy, a jazz singer, teacher, dancer, and pianist, and architect Michael Kane. Her family is Jewish, and her grandparents emigrated from Russia, Austria, and Poland. Due to her father's occupation, Kane moved frequently as a child; she briefly lived in Paris at age eight, where she began learning to speak French. Additionally, she resided in Haiti at age 10. Her parents divorced when she was 12 years old.\nShe attended the Cherry Lawn School, a boarding school in Darien, Connecticut, until 1965. She studied theater at HB Studio and also went to the Professional Children's School in New York City. She became a member of both the Screen Actors Guild and the Actors' Equity Association at age 14. Kane made her professional theater debut in a 1966 production of \"The Prime of Miss Jean Brodie\" starring Tammy Grimes, her first job as a member of Actors' Equity.\nCareer.\n1971\u20131979: Career beginnings and early recognition.\nKane's on-screen career began while she was still a teenager, when she appeared in minor roles in films such as \"Desperate Characters\" and Mike Nichols's \"Carnal Knowledge\" in 1971, the latter of which led her to befriend lead actor Jack Nicholson. In 1972, she was cast in her first leading role in the Canadian production \"Wedding in White\", where she played a teenage rape victim who is forced into marriage by her father. She also appeared as a sex worker in Hal Ashby's 1973 film \"The Last Detail\", where she collaborated with Nicholson yet again.\nIn 1975, Kane was cast in Joan Micklin Silver's feature-length debut \"Hester Street\", in which she played a Russian-Jewish immigrant who struggles with her husband to assimilate in late 19th-century New York. For her performance in the film, Kane garnered her sole Academy Award nomination for Best Actress at the 48th Academy Awards, and it remains her favorite of all her roles. Additionally, 1975 saw her appear as a bank teller in Sidney Lumet's crime drama \"Dog Day Afternoon\", which received numerous Academy Award nominations in other categories that same year. This also marked her first on-screen collaboration with Al Pacino, whom she had known prior to the film thanks to their shared background in theater.\nDespite this recognition, however, Kane has recounted waiting for approximately a year before being cast in her next role, which she has attributed to the trend of actors being typecast after receiving awards attention. Her return to the screen would come with Gene Wilder's 1977 comedy \"The World's Greatest Lover,\" which she has credited for identifying the comedic talents that would become her staple in later years. During the same year, she was cast in Woody Allen's romantic comedy \"Annie Hall\", where she played Allison Portchnik, the first wife of Allen's character Alvy Singer. She also appeared in Ken Russell's film \"Valentino\", which, like \"The World's Greatest Lover\", takes inspiration from the silent film era, as it is a biographical drama loosely inspired by the life of Rudolph Valentino.After this, Kane appeared in the horror films \"The Mafu Cage\" (1978) and \"When a Stranger Calls\" (1979); ironically, Kane herself is largely averse to horror, and she admits to being unable to watch the latter. In 1979, she also appeared in a cameo role in \"The Muppet Movie\".\n1980\u20131990: \"Taxi\" and transition into comedy.\nFrom 1980 to 1983, Kane portrayed Simka Dahblitz-Gravas, the wife of Andy Kaufman's character Latka Gravas, on the American television series \"Taxi\". She has theorized that she was cast in \"Taxi\" in part due to her work in \"Hester Street\", where a significant portion of her dialogue was spoken in Yiddish, since Simka speaks a fictional language with a vaguely Eastern European accent.\nKane has attributed the on-screen rapport she shared with Kaufman to their different work ethics: where she was trained in the theater and enjoyed rehearsal time, Kaufman was rooted more in stand-up comedy and did not care for rehearsals, a contrast that she believes enhanced their believability as a married couple. However, she maintains that she and Kaufman had a loving relationship on set, and she has spoken fondly of him in retrospective interviews. Kane received two Emmy Awards for her work on \"Taxi\". Her role on the series has largely been credited as the beginning of her pivot towards more comedic roles, as she began to regularly appear in sitcoms and comedy films after the series ended.\nIn 1984, Kane appeared in episode 12, season 3 of \"Cheers\" as Amanda, an acquaintance of Diane Chambers from her time spent in a mental institution. She was also a regular on the 1986 series \"All Is Forgiven.\"\nIn 1987, Kane appeared in \"Ishtar\", Elaine May's notorious box-office flop turned cult classic, playing the frustrated girlfriend of Dustin Hoffman's character. That year also saw her make one of her most recognizable film appearances in Rob Reiner's fantasy romance \"The Princess Bride\", where she played Valerie, the wife of Miracle Max (Billy Crystal). In 1988, Kane appeared in the Cinemax Comedy Experiment \"Rap Master Ronnie: A Report Card\" alongside Jon Cryer and the Smothers Brothers. During the same year, she was also featured in the Bill Murray vehicle \"Scrooged\", where she portrayed a contemporary version of the Ghost of Christmas Present, depicted in the film as a fairy. For this performance, \"Variety\" called her \"unquestionably [the] pic's comic highlight\". Additionally, she played a potential love interest for Steve Martin's character in the 1990 film \"My Blue Heaven\".\n1990\u20132004: Television and film regularity.\nKane became a regular on the NBC series \"American Dreamer\", which ran from 1990 to 1991\".\" In 1993, she appeared in \"Addams Family Values\" where she replaced Judith Malina as Grandmama Addams; this role saw her reunite with her \"Taxi\" castmate Christopher Lloyd. She also guest starred on a 1994 episode of \"Seinfeld\", as well as a 1996 episode of \"Ellen.\" In 1996, she was given a supporting role in the short-lived sitcom \"Pearl\". From there, she continued to appear in a number of film roles throughout the 1990s and early 2000s, including \"The Pallbearer\" (1996), \"Office Killer\" (1997), \"Jawbreaker\" (1999), and \"My First Mister\" (2001). In 1998, she voiced Mother Duck in the American version of the animated television film \"The First Snow of Winter\".\nIn 1999, she made a cameo in the Andy Kaufman biopic \"Man on the Moon\" as herself playing the \"Taxi\" character.\n2005\u20132014: \"Wicked\" and career expansion.\nKane is also known for her portrayal of the evil headmistress Madame Morrible in the Broadway musical \"Wicked\", whom she played in various productions from 2005 to 2014. Kane made her \"Wicked\" debut on the 1st National Tour, playing the role from March 9 through December 19, 2005. She then reprised the role in the Broadway production from January 10 through November 12, 2006. She again played the role for the Los Angeles production which began performances on February 7, 2007. She left the production on December 30, 2007, and later returned on August 26, 2008, until the production closed on January 11, 2009.\nIn January 2009, she guest starred in the television series \"Two and a Half Men\" as the mother of Alan Harper's receptionist.\nShe then transferred with the Los Angeles company of \"Wicked\" to reprise her role once again, this time in the San Francisco production, which began performances January 27, 2009. She ended her limited engagement on March 22, 2009.\nIn March 2010, Kane appeared in the ABC series \"Ugly Betty\" as Justin Suarez's acting teacher.\nKane starred in the off-Broadway play \"Love, Loss, and What I Wore\" in February 2010. She made her West End debut in January 2011 in a major revival of Lillian Hellman's drama \"The Children's Hour\" at London's Comedy Theatre, where she starred alongside Keira Knightley, Elisabeth Moss and Ellen Burstyn. In May 2012, Kane appeared on Broadway as Betty Chumley in a revival of the play \"Harvey\".\nKane returned to the Broadway company of \"Wicked\" from July 1, 2013, through February 22, 2014, a period that included the show's 10th anniversary.\nIn 2014, she was cast in a recurring role on the television series \"Gotham\" as Gertrude Kapelput, the mother of Oswald Cobblepot, also known as Penguin.\n2015\u2013present: \"Unbreakable Kimmy Schmidt\" and legacy roles.\nIn 2015, Kane was cast in the recurring role of Lillian Kaushtupper, the landlord to the title character of the Netflix series \"Unbreakable Kimmy Schmidt\". Kane joined the cast due in part to her admiration of showrunner Tina Fey, with whom she had previously wanted to collaborate on the NBC series \"30 Rock\". She was promoted to a series regular for the show's second season. \"Unbreakable Kimmy Schmidt\" ran for four seasons, making it one of Kane's longest television roles to date. She reprised the role in the \"interactive\" television special \"\".\nIn 2018, Kane was cast in Jacques Audiard's Western film \"The Sisters Brothers\". In 2019, she appeared in Jim Jarmusch's horror comedy \"The Dead Don't Die,\" marking another collaboration with Bill Murray. That same year, she was featured in the recurring role of Bianca Nova in season one of the HBO series \"Los Espookys\", where she reunited with her \"Unbreakable Kimmy Schmidt\" castmate Fred Armisen.\nIn 2020, Kane was featured in the ensemble cast of the Amazon series \"Hunters,\" which also includes her longtime acquaintance Al Pacino. Additionally, during the same year, she participated in two cast reunion fundraisers, one with the cast of \"Taxi\" for the Actors Fund, the other with the cast of \"The Princess Bride\" for the Democratic Party of Wisconsin.\nIt was announced on \"Star Trek\" Day 2022 that Kane would join the cast of \"\" for season two as Chief Engineer Pelia. Prior to her casting, Kane had never seen an episode of the original \"Star Trek\" series, though she has said the show's writers thought this oversight improved her performance.\nIn 2023, Kane was announced as one of the leads in Nathan Silver's comedy film \"Between the Temples\". In December 2024, she was announced as part of the cast of Darren Aronofsky's upcoming film Caught Stealing in a role where she only speaks in Yiddish. \nPersonal life.\nKane was in a relationship with actor Woody Harrelson from 1986 to 1988. The two have remained friends since their break-up, and Harrelson was seen attending Kane's 60th birthday party in 2012.\nShe has never been married, nor has she had any children. Regarding the latter decision, she has said, \"I never felt that I would be calm and stable enough to be the kind of mother I'd like to be. I don't think everyone randomly is mother material.\"\nKane is often noted for her high, breathy, slow voice, though her vocal timbre has grown raspier with age. Kane, who has often altered her voice to suit various roles, has confessed to disliking it, telling \"People\" magazine in 2020 that she wishes her voice was \"deep and beautiful and sexy\"."}
{"id": "7184", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=7184", "title": "C*-algebra", "text": "In mathematics, specifically in functional analysis, a C\u2217-algebra (pronounced \"C-star\") is a Banach algebra together with an involution satisfying the properties of the adjoint. A particular case is that of a complex algebra \"A\" of continuous linear operators on a complex Hilbert space with two additional properties:\nAnother important class of non-Hilbert C*-algebras includes the algebra formula_1 of complex-valued continuous functions on \"X\" that vanish at infinity, where \"X\" is a locally compact Hausdorff space.\nC*-algebras were first considered primarily for their use in quantum mechanics to model algebras of physical observables. This line of research began with Werner Heisenberg's matrix mechanics and in a more mathematically developed form with Pascual Jordan around 1933. Subsequently, John von Neumann attempted to establish a general framework for these algebras, which culminated in a series of papers on rings of operators. These papers considered a special class of C*-algebras that are now known as von Neumann algebras.\nAround 1943, the work of Israel Gelfand and Mark Naimark yielded an abstract characterisation of C*-algebras making no reference to operators on a Hilbert space.\nC*-algebras are now an important tool in the theory of unitary representations of locally compact groups, and are also used in algebraic formulations of quantum mechanics. Another active area of research is the program to obtain classification, or to determine the extent of which classification is possible, for separable simple nuclear C*-algebras.\nAbstract characterization.\nWe begin with the abstract characterization of C*-algebras given in the 1943 paper by Gelfand and Naimark.\nA C*-algebra, \"A\", is a Banach algebra over the field of complex numbers, together with a map formula_2 for formula_3 with the following properties:\nRemark. The first four identities say that \"A\" is a *-algebra. The last identity is called the C* identity and is equivalent to:\nformula_10\nwhich is sometimes called the B*-identity. For history behind the names C*- and B*-algebras, see the section below.\nThe C*-identity is a very strong requirement. For instance, together with the spectral radius formula, it implies that the C*-norm is uniquely determined by the algebraic structure:\nA bounded linear map, \"\u03c0\" : \"A\" \u2192 \"B\", between C*-algebras \"A\" and \"B\" is called a *-homomorphism if\nIn the case of C*-algebras, any *-homomorphism \"\u03c0\" between C*-algebras is contractive, i.e. bounded with norm \u2264 1. Furthermore, an injective *-homomorphism between C*-algebras is isometric. These are consequences of the C*-identity.\nA bijective *-homomorphism \"\u03c0\" is called a C*-isomorphism, in which case \"A\" and \"B\" are said to be isomorphic.\nSome history: B*-algebras and C*-algebras.\nThe term B*-algebra was introduced by C. E. Rickart in 1946 to describe Banach *-algebras that satisfy the condition:\nThis condition automatically implies that the *-involution is isometric, that is, formula_15. Hence, formula_16, and therefore, a B*-algebra is also a C*-algebra. Conversely, the C*-condition implies the B*-condition. This is nontrivial, and can be proved without using the condition formula_17. For these reasons, the term B*-algebra is rarely used in current terminology, and has been replaced by the term 'C*-algebra'.\nThe term C*-algebra was introduced by I. E. Segal in 1947 to describe norm-closed subalgebras of \"B\"(\"H\"), namely, the space of bounded operators on some Hilbert space \"H\". 'C' stood for 'closed'. In his paper Segal defines a C*-algebra as a \"uniformly closed, self-adjoint algebra of bounded operators on a Hilbert space\".\nStructure of C*-algebras.\nC*-algebras have a large number of properties that are technically convenient. Some of these properties can be established by using the continuous functional calculus or by reduction to commutative C*-algebras. In the latter case, we can use the fact that the structure of these is completely determined by the Gelfand isomorphism.\nSelf-adjoint elements.\nSelf-adjoint elements are those of the form formula_18. The set of elements of a C*-algebra \"A\" of the form formula_19 forms a closed convex cone. This cone is identical to the elements of the form formula_20. Elements of this cone are called \"non-negative\" (or sometimes \"positive\", even though this terminology conflicts with its use for elements of formula_21)\nThe set of self-adjoint elements of a C*-algebra \"A\" naturally has the structure of a partially ordered vector space; the ordering is usually denoted formula_22. In this ordering, a self-adjoint element formula_23 satisfies formula_24 if and only if the spectrum of formula_25 is non-negative, if and only if formula_26 for some formula_27. Two self-adjoint elements formula_28 and formula_29 of \"A\" satisfy formula_30 if formula_31.\nThis partially ordered subspace allows the definition of a positive linear functional on a C*-algebra, which in turn is used to define the states of a C*-algebra, which in turn can be used to construct the spectrum of a C*-algebra using the GNS construction.\nQuotients and approximate identities.\nAny C*-algebra \"A\" has an approximate identity. In fact, there is a directed family {\"e\"\u03bb}\u03bb\u2208I of self-adjoint elements of \"A\" such that\nUsing approximate identities, one can show that the algebraic quotient of a C*-algebra by a closed proper two-sided ideal, with the natural norm, is a C*-algebra.\nSimilarly, a closed two-sided ideal of a C*-algebra is itself a C*-algebra.\nExamples.\nFinite-dimensional C*-algebras.\nThe algebra M(\"n\", C) of \"n\" \u00d7 \"n\" matrices over C becomes a C*-algebra if we consider matrices as operators on the Euclidean space, C\"n\", and use the operator norm ||\u00b7|| on matrices. The involution is given by the conjugate transpose. More generally, one can consider finite direct sums of matrix algebras. In fact, all C*-algebras that are finite dimensional as vector spaces are of this form, up to isomorphism. The self-adjoint requirement means finite-dimensional C*-algebras are semisimple, from which fact one can deduce the following theorem of Artin\u2013Wedderburn type:\nTheorem. A finite-dimensional C*-algebra, \"A\", is canonically isomorphic to a finite direct sum\nwhere min \"A\" is the set of minimal nonzero self-adjoint central projections of \"A\".\nEach C*-algebra, \"Ae\", is isomorphic (in a noncanonical way) to the full matrix algebra M(dim(\"e\"), C). The finite family indexed on min \"A\" given by {dim(\"e\")}\"e\" is called the \"dimension vector\" of \"A\". This vector uniquely determines the isomorphism class of a finite-dimensional C*-algebra. In the language of K-theory, this vector is the positive cone of the \"K\"0 group of \"A\".\nA \u2020-algebra (or, more explicitly, a \"\u2020-closed algebra\") is the name occasionally used in physics for a finite-dimensional C*-algebra. The dagger, \u2020, is used in the name because physicists typically use the symbol to denote a Hermitian adjoint, and are often not worried about the subtleties associated with an infinite number of dimensions. (Mathematicians usually use the asterisk, *, to denote the Hermitian adjoint.) \u2020-algebras feature prominently in quantum mechanics, and especially quantum information science.\nAn immediate generalization of finite dimensional C*-algebras are the approximately finite dimensional C*-algebras.\nC*-algebras of operators.\nThe prototypical example of a C*-algebra is the algebra \"B(H)\" of bounded (equivalently continuous) linear operators defined on a complex Hilbert space \"H\"; here \"x*\" denotes the adjoint operator of the operator \"x\" : \"H\" \u2192 \"H\". In fact, every C*-algebra, \"A\", is *-isomorphic to a norm-closed adjoint closed subalgebra of \"B\"(\"H\") for a suitable Hilbert space, \"H\"; this is the content of the Gelfand\u2013Naimark theorem.\nC*-algebras of compact operators.\nLet \"H\" be a separable infinite-dimensional Hilbert space. The algebra \"K\"(\"H\") of compact operators on \"H\" is a norm closed subalgebra of \"B\"(\"H\"). It is also closed under involution; hence it is a C*-algebra.\nConcrete C*-algebras of compact operators admit a characterization similar to Wedderburn's theorem for finite dimensional C*-algebras:\nTheorem. If \"A\" is a C*-subalgebra of \"K\"(\"H\"), then there exists Hilbert spaces {\"Hi\"}\"i\"\u2208\"I\" such that\nwhere the (C*-)direct sum consists of elements (\"Ti\") of the Cartesian product \u03a0 \"K\"(\"Hi\") with ||\"Ti\"|| \u2192 0.\nThough \"K\"(\"H\") does not have an identity element, a sequential approximate identity for \"K\"(\"H\") can be developed. To be specific, \"H\" is isomorphic to the space of square summable sequences \"l\"2; we may assume that \"H\" = \"l\"2. For each natural number \"n\" let \"Hn\" be the subspace of sequences of \"l\"2 which vanish for indices \"k\" \u2265 \"n\" and let \"en\" be the orthogonal projection onto \"Hn\". The sequence {\"en\"}\"n\" is an approximate identity for \"K\"(\"H\").\n\"K\"(\"H\") is a two-sided closed ideal of \"B\"(\"H\"). For separable Hilbert spaces, it is the unique ideal. The quotient of \"B\"(\"H\") by \"K\"(\"H\") is the Calkin algebra.\nCommutative C*-algebras.\nLet \"X\" be a locally compact Hausdorff space. The space formula_1 of complex-valued continuous functions on \"X\" that \"vanish at infinity\" (defined in the article on local compactness) forms a commutative C*-algebra formula_1 under pointwise multiplication and addition. The involution is pointwise conjugation. formula_1 has a multiplicative unit element if and only if formula_39 is compact. As does any C*-algebra, formula_1 has an approximate identity. In the case of formula_1 this is immediate: consider the directed set of compact subsets of formula_39, and for each compact formula_43 let formula_44 be a function of compact support which is identically 1 on formula_43. Such functions exist by the Tietze extension theorem, which applies to locally compact Hausdorff spaces. Any such sequence of functions formula_46 is an approximate identity.\nThe Gelfand representation states that every commutative C*-algebra is *-isomorphic to the algebra formula_1, where formula_39 is the space of characters equipped with the weak* topology. Furthermore, if formula_1 is isomorphic to formula_50 as C*-algebras, it follows that formula_39 and formula_52 are homeomorphic. This characterization is one of the motivations for the noncommutative topology and noncommutative geometry programs.\nC*-enveloping algebra.\nGiven a Banach *-algebra \"A\" with an approximate identity, there is a unique (up to C*-isomorphism) C*-algebra E(\"A\") and *-morphism \u03c0 from \"A\" into E(\"A\") that is universal, that is, every other continuous *-morphism factors uniquely through \u03c0. The algebra E(\"A\") is called the C*-enveloping algebra of the Banach *-algebra \"A\".\nOf particular importance is the C*-algebra of a locally compact group \"G\". This is defined as the enveloping C*-algebra of the group algebra of \"G\". The C*-algebra of \"G\" provides context for general harmonic analysis of \"G\" in the case \"G\" is non-abelian. In particular, the dual of a locally compact group is defined to be the primitive ideal space of the group C*-algebra. See spectrum of a C*-algebra.\nVon Neumann algebras.\nVon Neumann algebras, known as W* algebras before the 1960s, are a special kind of C*-algebra. They are required to be closed in the weak operator topology, which is weaker than the norm topology.\nThe Sherman\u2013Takeda theorem implies that any C*-algebra has a universal enveloping W*-algebra, such that any homomorphism to a W*-algebra factors through it.\nType for C*-algebras.\nA C*-algebra \"A\" is of type I if and only if for all non-degenerate representations \u03c0 of \"A\" the von Neumann algebra \u03c0(\"A\") (that is, the bicommutant of \u03c0(\"A\")) is a type I von Neumann algebra. In fact it is sufficient to consider only factor representations, i.e. representations \u03c0 for which \u03c0(\"A\") is a factor.\nA locally compact group is said to be of type I if and only if its group C*-algebra is type I.\nHowever, if a C*-algebra has non-type I representations, then by results of James Glimm it also has representations of type II and type III. Thus for C*-algebras and locally compact groups, it is only meaningful to speak of type I and non type I properties.\nC*-algebras and quantum field theory.\nIn quantum mechanics, one typically describes a physical system with a C*-algebra \"A\" with unit element; the self-adjoint elements of \"A\" (elements \"x\" with \"x*\" = \"x\") are thought of as the \"observables\", the measurable quantities, of the system. A \"state\" of the system is defined as a positive functional on \"A\" (a C-linear map \u03c6 : \"A\" \u2192 C with \u03c6(\"u*u\") \u2265 0 for all \"u\" \u2208 \"A\") such that \u03c6(1) = 1. The expected value of the observable \"x\", if the system is in state \u03c6, is then \u03c6(\"x\").\nThis C*-algebra approach is used in the Haag\u2013Kastler axiomatization of local quantum field theory, where every open set of Minkowski spacetime is associated with a C*-algebra."}
{"id": "7185", "revid": "38005489", "url": "https://en.wikipedia.org/wiki?curid=7185", "title": "London Borough of Croydon", "text": "The London Borough of Croydon () is a borough in south London, part of Outer London. It covers an area of . It is the southernmost borough of London. At its centre is the town of Croydon from which the borough takes its name; while other urban centres include Coulsdon, Purley, South Norwood, Norbury, New Addington, Selsdon and Thornton Heath. Croydon is mentioned in Domesday Book, and from a small market town has expanded into one of the most populous areas on the fringe of London. The borough is now one of London's leading business, financial and cultural centres. Its influence in entertainment and the arts contribute to its status as a major metropolitan centre. Its population is 390,719, making it the most populous London borough and sixteenth largest English district.\nThe borough was formed in 1965 from the merger of the County Borough of Croydon with Coulsdon and Purley Urban District, both of which had been within Surrey. The local authority, Croydon London Borough Council, is now part of London Councils, the local government association for Greater London. The economic strength of Croydon dates back mainly to Croydon Airport which was a major factor in the development of Croydon as a business centre. Once London's main airport for all international flights to and from the capital, it was closed on 30 September 1959 due to the lack of expansion space needed for an airport to serve the growing city. It is now a Grade II listed building and tourist attraction. Croydon Council and its predecessor Croydon Corporation unsuccessfully applied for city status in 1954, 2000, 2002 and 2012. The area is currently going through a large regeneration project called Croydon Vision 2020 which is predicted to attract more businesses and tourists to the area as well as backing Croydon's bid to become \"London's Third City\" (after the City of London and Westminster). Croydon is mostly urban, though there are large suburban and rural uplands towards the south of the borough. Since 2003, Croydon has been certified as a Fairtrade borough by the Fairtrade Foundation. It was the first London borough to have Fairtrade status which is awarded on certain criteria.\nThe area is one of the hearts of culture in London and the South East of England. Institutions such as the major arts and entertainment centre Fairfield Halls add to the vibrancy of the borough. However, its famous fringe theatre, the Warehouse Theatre, went into administration in 2012 when the council withdrew funding, and the building itself was demolished in 2013. The Croydon Clocktower was opened by Queen Elizabeth II in 1994 as an arts venue featuring a library, the independent David Lean Cinema (closed by the council in 2011 after sixteen years of operating, but now partially reopened on a part-time and volunteer basis) and museum. From 2000 to 2010, Croydon staged an annual summer festival celebrating the area's black and Indian cultural diversity, with audiences reaching over 50,000 people.\nPremier League football club Crystal Palace F.C. play at Selhurst Park in Selhurst, a stadium they have been based in since 1924. Other landmarks in the borough include what remains of Croydon Palace, an important residence of the Archbishops of Canterbury since around the ninth century CE, and known as 'The Old Palace' during its time as a school. It served as the Manor House of the manor of Croydon since it had been held as a manor by the Archbishops since the Anglo-Saxon period. Its local successor is Addington Palace, an eighteenth-century mansion which became the official second residence of six Archbishops of Canterbury, Shirley Windmill, one of the few surviving large windmills in Greater London built in the 1850s, and the BRIT School, a creative arts institute run by the BRIT Trust which has produced artists such as Adele, Amy Winehouse and Leona Lewis.\nHistory.\nThe name Croydon comes from Crogdene or Croindone, named by the Saxons in the 8th century when they settled here, although the area had been inhabited since prehistoric times. It is thought to derive from the Anglo-Saxon \"croeas deanas\", meaning \"the valley of the crocuses\", indicating that, like Saffron Walden in Essex, it was a centre for the collection of saffron.\nAt the time of the Norman invasion Croydon had a church, a mill and around 365 inhabitants as recorded in the Domesday Book. The Archbishop of Canterbury, Archbishop Lanfranc lived at Croydon Palace which still stands. Visitors included Thomas Becket (another Archbishop), and royal figures such as Henry VIII of England and Elizabeth I. The royal charter for Surrey Street Market dates back to 1276.\nCroydon carried on as a market town, producing charcoal, tanned leather, and beer. Croydon was served by the Surrey Iron Railway, the first public railway (horse-drawn) in the world, in 1803, and by the London to Brighton rail link in the mid-19th century, helping it to become the largest town in what was then Surrey.\nIn the 20th century, Croydon became known for industries such as metal working, car manufacture, and its aerodrome, Croydon Airport. Starting out during World War I as an airfield for protection against Zeppelins, an adjacent airfield was combined, and the new aerodrome opened on 29 March 1920. It became the largest in London and was the main terminal for international air freight into the capital. It developed into one of the airports of the world during the 1920s and 1930s and welcomed the world's aviators in its heyday. British Airways Ltd used the airport for a short period after redirecting from Northolt Aerodrome, and Croydon was the operating base for Imperial Airways. It was partly due to the airport that Croydon suffered heavy bomb damage during World War II. As aviation technology progressed, however, and aircraft became larger and more numerous, it was recognized in 1952 that the airport would be too small to cope with the ever-increasing volume of air traffic. The last scheduled flight departed on 30 September 1959. It was superseded as the main airport by both London Heathrow and London Gatwick Airport (see below). The air terminal, now known as Airport House, has been restored and has a hotel and museum in it.\nIn the late 1950s and through the 1960s the council commercialized the centre of Croydon with the massive development of office blocks and the Whitgift Centre which was formerly the biggest in-town shopping centre in Europe. The centre was officially opened in October 1970 by the Duchess of Kent. The original Whitgift School there had moved to Haling Park, South Croydon in the 1930s; the replacement school on the site, Whitgift Middle School, now the Trinity School of John Whitgift, moved to Shirley Park in the 1960s, when the buildings were demolished.\nCroydon, in common with many other areas, was hit by extensive rioting in August 2011. Reeves, an historic furniture store established in 1867, that gave its name to a junction and tram stop in the town centre, was destroyed by arson.\nCroydon is currently the subject of a series of \u00a33.5bn of development projects, called Croydon Vision 2020. This aims to change the urban planning of central Croydon . It aims to make Croydon \"London's Third City\" and the hub of retail, business, culture, and living in south London and South East England. The plan was showcased in a series of events called Croydon Expo.\nAdministrative history.\nThe area of the modern borough broadly corresponds to the four ancient parishes of Croydon, Addington, Coulsdon and Sanderstead.\nThe parish of Croydon was governed by improvement commissioners from 1829 until 1849 when it was made a local board district. Croydon was incorporated as a municipal borough in 1883. When elected county councils were established in 1889, Croydon was considered large enough to provide its own county-level services. It was therefore made a county borough, independent from the new Surrey County Council, whilst remaining part of Surrey for judicial and lieutenancy purposes. The borough was enlarged in 1928 to absorb the neighbouring parish of Addington.\nCoulsdon and Sanderstead were governed as rural parishes within the Croydon Rural District until 1915 when the Coulsdon and Purley Urban District was created covering the two parishes. Purley itself was not a civil parish, being in the parish of Coulsdon, but its name was included in the urban district's name on account of it being one of the main built-up settlements in the district. There were subsequent adjustments to the boundaries with neighbouring areas, notably including in 1933 when the urban district absorbed the parish of Farleigh, after which there were three urban parishes in the district, being Cousldon, Farleigh and Sanderstead.\nThe London Borough of Croydon was created on 1 April 1965 under the London Government Act 1963, covering the combined area of the former Coulsdon and Purley Urban District and the County Borough of Croydon, both of which were abolished at the same time. The area was transferred from Surrey to Greater London to become one of the 32 London boroughs. The Farleigh area was removed from the borough in 1969 and transferred back to Surrey, becoming part of the parish of Chelsham and Farleigh.\nThe borough council has unsuccessfully applied for city status on several occasions: in 1965, 1977, 1992, 2000, 2002, and 2012. At present, the London Borough of Croydon is the second most populous local government district of England without city status. Croydon's applications were refused as it was felt not to have an identity separate from the rest of Greater London. In 1965 it was described as \"...now just part of the London conurbation and almost indistinguishable from many of the other Greater London boroughs\" and in 2000 as having \"no particular identity of its own\".\nGovernance.\nThe local authority is Croydon Council, which meets at Croydon Town Hall on Katherine Street in the centre of Croydon, and has its main offices at the adjoining Bernard Weatherill House. Since 2022 the council has been led by the directly elected Mayor of Croydon.\nGreater London representation.\nSince 2000, for elections to the London Assembly, the borough forms part of the Croydon and Sutton constituency.\nWestminster representation.\nThe borough is covered by three parliamentary constituencies: these are Croydon North, Croydon Central and Croydon South. Sarah Jones (politician) won the Croydon Central seat for Labour in 2017. Croydon North has a Labour MP, Steve Reed (politician), and Croydon South has a Conservative MP, Chris Philp.\nGovernment buildings.\nCroydon Town Hall on Katharine Street in central Croydon houses the committee rooms, the mayor's and other councillors' offices, electoral services and the arts and heritage services.\nThe present Town Hall is Croydon's third. The first town hall is thought to have been built in either 1566 or 1609. The second was built in 1808 to serve the growing town but was demolished after the present town hall was erected in 1895.\nThe 1808 building cost \u00a38,000, which was regarded as an enormous sum for those days and was perhaps as controversial as the administrative building Bernard Weatherill House opened for occupation in 2013 and reputed to have cost \u00a3220,000,000. The early 19th century building was known initially as \"Courthouse\" as, like its predecessor and successor, the local court met there. The building stood on the western side of the High Street near to the junction with Surrey Street, the location of the town's market. The building became inadequate for the growing local administrative responsibilities and stood at a narrow point of a High Street in need of widening.\nThe present town hall was designed by local architect Charles Henman and was officially opened by the Prince and Princess of Wales on 19 May 1896. It was constructed in red brick, sourced from Wrotham in Kent, with Portland stone dressings and green Westmoreland slates for the roof. It also housed the court and most central council employees.\nThe Borough's incorporation in 1883 and a desire to improve central Croydon with improvements to traffic flows and the removal of social deprivation in Middle Row prompted the move to a new configuration of town hall provision. The second closure of the Central Railway Station provided the corporation with the opportunity to buy the station land from the London, Brighton and South Coast Railway Company for \u00a311,500 to provide the site for the new town hall. Indeed, the council hoped to be able to sell on some of the land purchased with enough for municipal needs and still \"leave a considerable margin of land which might be disposed of\". The purchase of the failed railway station came despite local leaders having successfully urged the re-opening of the poorly patronised railway station. The railway station re-opening had failed to be a success so freeing up the land for alternative use.\nParts, including the former court rooms, have been converted into the Museum of Croydon and exhibition galleries. The original public library was converted into the David Lean Cinema, part of the Croydon Clocktower. The Braithwaite Hall is used for events and performances. The town hall was renovated in the mid-1990s and the imposing central staircase, long closed to the public and kept for councillors only, was re-opened in 1994. The civic complex, meanwhile, was added to, with buildings across Mint Walk and the 19-floor Taberner House to house the rapidly expanding corporation's employees.\nRuskin House is the headquarters of Croydon's Labour, Trade Union and Co-operative movements and is itself a co-operative with shareholders from organisations across the three movements. In the 19th century, Croydon was a bustling commercial centre of London. It was said that, at the turn of the 20th century, approximately \u00a310,000 was spent in Croydon's taverns and inns every week. For the early labour movement, then, it was natural to meet in the town's public houses, in this environment. However, the temperance movement was equally strong, and Georgina King Lewis, a keen member of the Croydon United Temperance Council, took it upon herself to establish a dry centre for the labour movement. The first Ruskin House was highly successful, and there has been two more since. The current house was officially opened in 1967 by the then Labour Prime Minister, Harold Wilson. Today, Ruskin House continues to serve as the headquarters of the Trade Union, Labour and Co-operative movements in Croydon, hosting a range of meetings and being the base for several labour movement groups. Office tenants include the headquarters of the Communist Party of Britain and Croydon Labour Party. Geraint Davies, the MP for Croydon Central, had offices in the building, until he was defeated by Andrew Pelling and is now the Labour representative standing for Swansea West in Wales.\nTaberner House was built between 1964 and 1967, designed by architect H. Thornley, with Allan Holt and Hugh Lea as borough engineers. Although the council had needed extra space since the 1920s, it was only with the imminent creation of the London Borough of Croydon that action was taken. The building, being demolished in 2014, was in classic 1960s style, praised at the time but subsequently much derided. It has its elegant upper slab block narrowing towards both ends, a formal device which has been compared to the famous Pirelli Tower in Milan. It was named after Ernest Taberner OBE, Town Clerk from 1937 to 1963. Until September 2013, Taberner House housed most of the council's central employees and was the main location for the public to access information and services, particularly with respect to housing.\nIn September 2013, Council staff moved into Bernard Weatherill House in Fell Road, (named after the former Speaker of the House and Member of Parliament for Croydon North-East). Staff from the Met Police, NHS, Jobcentre Plus, Croydon Credit Union, Citizens Advice Bureau as well as 75 services from the council all moved to the new building.\nGeography and climate.\nThe borough is in the far south of London, with the M25 orbital motorway stretching to the south of it, between Croydon and Tandridge. To the north and east, the borough mainly borders the London Borough of Bromley, and in the north west the boroughs of Lambeth and Southwark. The boroughs of Sutton and Merton are located directly to the west. It is at the head of the River Wandle, just to the north of a significant gap in the North Downs. It lies south of Central London, and the earliest settlement may have been a Roman staging post on the London-Portslade road, although conclusive evidence has not yet been found. The main town centre houses a great variety of well-known stores on North End and two shopping centres. It was pedestrianised in 1989 to attract people back to the town centre. Another shopping centre called Park Place was due to open in 2012 but has since been scrapped.\nTownscape description.\nThe CR postcode area covers most of the south and centre of the borough while the SE and SW postcodes cover the northern parts, including Crystal Palace, Upper Norwood, South Norwood, Selhurst (part), Thornton Heath (part), Norbury and Pollards Hill (part).\nDistricts in the London Borough of Croydon include Addington, a village to the east of Croydon which until 2000 was poorly linked to the rest of the borough as it was without any railway or light rail stations, with only a few patchy bus services. Addiscombe is a district just northeast of the centre of Croydon, and is popular with commuters to central London as it is close to the busy East Croydon station. Ashburton, to the northeast of Croydon, is mostly home to residential houses and flats, being named after Ashburton House, one of the three big houses in the Addiscombe area. Broad Green is a small district, centred on a large green with many homes and local shops in West Croydon. Coombe is an area, just east of Croydon, which has barely been urbanised and has retained its collection of large houses fairly intact. Coulsdon, south west of Central Croydon, which has retained a good mix of traditional high street shops as well as a large number of restaurants for its size. Croydon is the principal area of the borough, Crystal Palace is an area north of Croydon, which is shared with the London Boroughs of Lambeth, Southwark, Lewisham and Bromley. Fairfield, just northeast of Croydon, holds the Fairfield Halls and the village of Forestdale, to the east of Croydon's main area, commenced work in the late 1960s and completed in the mid-70s to create a larger town on what was previously open ground. Hamsey Green is a place on the plateau of the North Downs, south of Croydon. Kenley, again south of the centre, lie within the London Green Belt and features a landscape dominated by green space. New Addington, to the east, is a large local council estate surrounded by open countryside and golf courses. Norbury, to the northwest, is a suburb with a large ethnic population. Norwood New Town is a part of the Norwood triangle, to the north of Croydon. Monks Orchard is a small district made up of large houses and open space in the northeast of the borough. Pollards Hill is a residential district with houses on roads, which are lined with pollarded lime trees, stretching to Norbury. Purley, to the south, is a main town whose name derives from \"pirlea\", which means 'Peartree lea'. Sanderstead, to the south, is a village mainly on high ground at the edge of suburban development in Greater London. Selhurst is a town, to the north of Croydon, which holds the nationally known school, The BRIT School. Selsdon is a suburb which was developed during the inter-war period in the 1920s and 1930s, and is remarkable for its many Art Deco houses, to the southeast of Croydon Centre. Shirley, is to the east of Croydon, and holds Shirley Windmill. South Croydon, to the south of Croydon, is a locality which holds local landmarks such as The Swan and Sugarloaf public house and independent Whitgift School part of the Whitgift Foundation. South Norwood, to the north, is in common with West Norwood and Upper Norwood, named after a contraction of Great North Wood and has a population of around 14,590. Thornton Heath is a town, to the northwest of Croydon, which holds Croydon's principal hospital Mayday. Upper Norwood is north of Croydon, on a mainly elevated area of the borough. Waddon is a residential area, mainly based on the Purley Way retail area, to the west of the borough. Woodside is located to the northeast of the borough, with streets based on Woodside Green, a small sized area of green land. And finally Whyteleafe is a town, right to the edge of Croydon with some areas in the Surrey district of Tandridge.\nCroydon is a gateway to the south from central London, with some major roads running through it. Purley Way, part of the A23, was built to by-pass Croydon town centre. It is one of the busiest roads in the borough, and is the site of several major retail developments including one of only 18 IKEA stores in the country, built on the site of the former power station. The A23 continues southward as Brighton Road, which is the main route running towards the south from Croydon to Purley. The centre of Croydon is very congested, and the urban planning has since become out of date and quite inadequate, due to the expansion of Croydon's main shopping area and office blocks. Wellesley Road is a north\u2013south dual carriageway that cuts through the centre of the town, and makes it hard to walk between the town centre's two railway stations. Croydon Vision 2020 includes a plan for a more pedestrian-friendly replacement. It has also been named as one of the worst roads for cyclists in the area. Construction of the Croydon Underpass beneath the junction of George Street and Wellesley Road/Park Lane started in the early 1960s, mainly to alleviate traffic congestion on Park Lane, above the underpass. The Croydon Flyover is also near the underpass, and next to Taberner House. It mainly leads traffic on to Duppas Hill, towards Purley Way with links to Sutton and Kingston upon Thames. The major junction on the flyover is for Old Town, which is also a large three-lane road.\nTopography and climate.\nCroydon covers an area of 86.52\u00a0km2. Croydon's physical features consist of many hills and rivers that are spread out across the borough and into the North Downs, Surrey and the rest of south London. Addington Hills is a major hilly area to the south of London and is recognised as a significant obstacle to the growth of London from its origins as a port on the north side of the river, to a large circular city. The Great North Wood is a former natural oak forest that covered the Sydenham Ridge and the southern reaches of the River Effra and its tributaries.\nThe most notable tree, called Vicar's Oak, marked the boundary of four ancient parishes; Lambeth, Camberwell, Croydon and Bromley. John Aubrey referred to this \"ancient remarkable tree\" in the past tense as early as 1718, but according to JB Wilson, the Vicar's Oak survived until 1825. The River Wandle, a chalk stream, is also a major tributary of the River Thames, where it stretches to Wandsworth and Putney for from its main source in Waddon.\nCroydon has a temperate climate in common with most areas of Great Britain: its K\u00f6ppen climate classification is \"Cfb\". Its mean annual temperature of 9.6\u00a0\u00b0C is similar to that experienced throughout the Weald, and slightly cooler than nearby areas such as the Sussex coast and central London. Rainfall is considerably below England's average (1971\u20132000) level of 838\u00a0mm, and every month is drier overall than the England average.\nThe nearest weather station is at Gatwick Airport.\nArchitecture.\nThe skyline of Croydon has significantly changed over the past 50 years. High rise buildings, mainly office blocks, now dominate the skyline. The most notable of these buildings include Croydon Council's headquarters Taberner House, which has been compared to the famous Pirelli Tower of Milan, and the Nestl\u00e9 Tower, the former UK headquarters of Nestl\u00e9.\nIn recent years, the development of tall buildings, such as the approved Croydon Vocational Tower and Wellesley Square, has been encouraged in the London Plan, and will lead to the erection of new skyscrapers in the coming years as part of London's high-rise boom.\nNo. 1 Croydon, formerly the NLA Tower, Britain's 88th tallest tower, close to East Croydon station, is an example of 1970s architecture. The tower was originally nicknamed the \"Threepenny bit building\", as it resembles a stack of pre-decimalisation Threepence coins, which were 12-sided. It is now most commonly called The Octagon, being 8-sided.\nLunar House is another high-rise building. Like other government office buildings on Wellesley Road, such as Apollo House, the name of the building was inspired by the US Moon landings (In the Croydon suburb of New Addington there is a public house, built during the same period, called \"The Man on the Moon\"). Lunar House houses the Home Office building for Visas and Immigration. Apollo House houses The Border Patrol Agency.\nA new generation of buildings are being considered by the council as part of Croydon Vision 2020, so that the borough doesn't lose its title of having the \"largest office space in the south east\", excluding central London. Projects such as Wellesley Square, which will be a mix of residential and retail with an eye-catching colour design and 100 George Street a proposed modern office block are incorporated in this vision.\nNotable events that have happened to Croydon's skyline include the Millennium project to create the largest single urban lighting project ever. It was created for the buildings of Croydon to illuminate them for the third millennium. The project provided new lighting for the buildings, and provided an opportunity to project images and words onto them, mixing art and poetry with coloured light, and also displaying public information after dark. Apart from increasing night time activity in Croydon and thereby reducing the fear of crime, it helped to promote the sustainable use of older buildings by displaying them in a more positive way.\nLandmarks.\nThere are a large number of attractions and places of interest all across the borough of Croydon, ranging from historic sites in the north and south to modern towers in the centre.\nCroydon Airport was once London's main airport, but closed on 30 September 1959 due to the expansion of London and because it didn't have room to grow; so Heathrow International Airport took over as London's main airport. It has now been mostly converted to offices, although some important elements of the airport remain. It is a tourist attraction.\nThe Croydon Clocktower arts venue was opened by Elizabeth II in 1994. It includes the Braithwaite Hall (the former reference library \u2013 named after the Rev. Braithwaite who donated it to the town) for live events, David Lean Cinema (built in memory of David Lean), the Museum of Croydon and Croydon Central Library. The Museum of Croydon (formerly known as Croydon Lifetimes Museum) highlights Croydon in the past and the present and currently features high-profile exhibitions including the Riesco Collection, The Art of Dr Seuss and the Whatever the Weather gallery. Shirley Windmill is a working windmill and one of the few surviving large windmills in Surrey, built in 1854. It is Grade II listed and received a \u00a3218,100 grant from the Heritage Lottery Fund. Addington Palace is an 18th-century mansion in Addington which was originally built as Addington Place in the 16th century. The palace became the official second residence of six archbishops, five of whom are buried in St Mary's Church and churchyard nearby.\nNorth End is the main pedestrianised shopping road in Croydon, having Centrale to one side and the Whitgift Centre to the other. The Warehouse Theatre is a popular theatre for mostly young performers and is due to get a face-lift on the Croydon Gateway site.\nThe Nestl\u00e9 Tower was the UK headquarters of Nestl\u00e9 and is one of the tallest towers in England, which is due to be re-fitted during the Park Place development. The Fairfield Halls is a well known concert hall and exhibition centre, opened in 1962. It is frequently used for BBC recordings and was formerly the home of ITV's World of Sport. It includes the Ashcroft Theatre and the Arnhem Gallery.\nCroydon Palace was the summer residence of the Archbishop of Canterbury for over 500 years and included regular visitors such as Henry III and Queen Elizabeth I. It is thought to have been built around 960. Croydon Cemetery is a large cemetery and crematorium west of Croydon and is most famous for the gravestone of Derek Bentley, who was wrongly hanged in 1953. Mitcham Common is an area of common land partly shared with the boroughs of Sutton and Merton. Almost 500,000 years ago, Mitcham Common formed part of the river bed of the River Thames.\nThe BRIT School is a performing Arts &amp; Technology school, owned by the BRIT Trust (known for the BRIT Awards Music Ceremony). Famous former students include Kellie Shirley, Amy Winehouse, Leona Lewis, Adele, Kate Nash, Dane Bowers, Katie Melua and Lyndon David-Hall. Grants is an entertainment venue in the centre of Croydon which includes a Vue cinema.\nSurrey Street Market has roots in the 13th century, or earlier, and was chartered by the Archbishop of Canterbury in 1276. The market is regularly used as a location for TV, film and advertising. Croydon Minster, formerly the parish church, was established in the Anglo-Saxon period, and parts of the surviving building (notably the tower) date from the 14th and 15th centuries. However, the church was largely destroyed by fire in 1867, so the present structure is a rebuild of 1867\u201369 to the designs of George Gilbert Scott. It is the burial place of six archbishops, and contains monuments to Archbishops Sheldon and Whitgift.\nDemography.\nPopulation change.\nThe table shows population change since 1801, including the percentage change since previous census. Although the London Borough of Croydon has existed only since 1965, earlier figures have been generated by combining data from the towns, villages, and civil parishes that would later be absorbed into the authority.\nEthnicity.\nAccording to the 2011 census, Croydon had a population of 363,378, making Croydon the most populated borough in Greater London. The estimated population in 2017 was around 384,800. 186,900 were males, with 197,900 females. The density was 4,448 inhabitants per km2. 248,200 residents of Croydon were between the age of 16 and 64.\nIn 2011, white was the majority ethnicity with 55.1%. Black was the second-largest ethnicity with 20.2%; 16.4% were Asian and 8.3% stated to be something other.\nThe most common householder type were owner occupied with only a small percentage rented. Many new housing schemes and developments are currently taking place in Croydon, such as The Exchange and Bridge House, IYLO, Wellesley Square (now known as Saffron Square) and Altitude 25. In 2006, The Metropolitan Police recorded a 10% fall in the number of crimes committed in Croydon, better than the rate which crime in London as a whole is falling. Croydon has had the highest fall in the number of cases of violence against the person in south London, and is one of the top 10 safest local authorities in London. According to \"Your Croydon\" (a local community magazine) this is due to a stronger partnership struck between Croydon Council and the police. In 2007, overall crime figures across the borough saw decrease of 5%, with the number of incidents decreasing from 32,506 in 2006 to 30,862 in 2007. However, in the year ending April 2012, The Metropolitan Police recorded the highest rates for murder and rape throughout London in Croydon, accounting for almost 10% of all murders, and 7% of all rapes. Croydon has five police stations. Croydon police station is on Park Lane in the centre of the town near the Fairfield Halls; South Norwood police station is a newly refurbished building just off the High Street; Norbury police station is on London Road; Kenley station is on Godstone Road; and New Addington police station is on Addington Village road.\nReligion.\nThe predominant religion of the borough is Christianity. According to the 2021 United Kingdom census, the borough has over 190,880 Christians, mainly Protestants. This is the largest religious following in the borough followed by Islam with 40,717 Muslims resident.\n101,119 Croydon residents stated that they are atheist or non-religious in the 2021 Census.\nCroydon Minster is the most notable of the borough's 35 churches. This church was founded in Saxon times, since there is a record of \"a priest of Croydon\" in 960, although the first record of a church building is in the Domesday Book (1086). In its final medieval form, the church was mainly a Perpendicular-style structure, but this was severely damaged by fire in 1867, following which only the tower, south porch and outer walls remained. Under the direction of Sir George Gilbert Scott the church was rebuilt, incorporating the remains and essentially following the design of the medieval building, and was reconsecrated in 1870. It still contains several important monuments and fittings saved from the old church.\nThe Area Bishop of Croydon is a position as a suffragan Bishop in the Anglican Diocese of Southwark. The present bishop is the Right Reverend Rosemarie Mallett.\nEconomy.\nThe main employment sectors of the Borough is retail and enterprise which is mainly based in Central Croydon. Major employers are well-known companies, who hold stores or offices in the town. The Purley Way shopping district is a major employer of people. IKEA Croydon, when it was built in 1992, brought many non-skilled jobs to Croydon. The store, which is a total size of 23,000 m2, took over the former site of Croydon Power station, which had led to the unemployment of many skilled workers. In May 2006, the expansion of the IKEA made it the fifth biggest employer in Croydon.\nCroydon town centre is also a major retail centre, and home to many high street and department stores as well as designer boutiques. The main town centre shopping areas are on the North End precinct, in the Whitgift Centre, Centrale and St George's Walk. Croydon's main market is Surrey Street Market, which has a royal charter dating back to 1276. Shopping areas outside the town centre include the Valley Park retail complex, Croydon Colonnades, Croydon Fiveways, and the Waddon Goods Park.\nIn research from 2010 on retail footprint, Croydon came out as 29th in Britain in terms of retail expenditure at \u00a3770 million, sixth in the Greater London area. The 2010 results were a decline from the 2005 figures, when Croydon was 21st in Britain and second in London, with \u00a3909 million in expenditures.\nIn 2007, Croydon leapt up the annual business growth league table, with a 14% rise in new firms trading in the borough after 125 new companies started up, increasing the number from 900 to 1,025, enabling the town, which has also won the Enterprising Britain Award and \"the most enterprising borough in London\" award, to jump from 31 to 14 in the table.\nTramlink created many jobs when it opened in 2000, not only drivers but engineers as well. Many of the people involved came from Croydon, which was the original hub of the system. Retail stores inside both Centrale and the Whitgift Centre as well as on North End employee people regularly and create many jobs, especially at Christmas. As well as the new building of Park Place, which will create yet more jobs, so will the regeneration of Croydon, called Croydon Vision 2020, highlighted in the Croydon Expo which includes the Croydon Gateway, Wellesley Square, Central One plus much more.\nCroydon is a major office area in the south east of England, being the largest outside of central London. Many powerful companies based in Europe and worldwide have European or British headquarters in the town. American International Group (AIG) have offices in No. 1 Croydon, formerly the NLA Tower, shared with Liberata, Pegasus and the Institute of Public Finance. AIG is the sixth-largest company in the world according to the 2007 Forbes Global 2000 list. The Swiss company Nestl\u00e9 has its UK headquarters in the Nestl\u00e9 Tower, on the site of the formerly proposed Park Place shopping centre. Real Digital International has developed a purpose built factory on Purley Way equipped with \"the most sophisticated production equipment and technical solutions\". ntl:Telewest, now Virgin Media, have offices at Communications House, from the Telewest side when it was known as Croydon Cable.\nThe Home Office UK Visas and Immigration department has its headquarters in Lunar House in Central Croydon. In 1981, Superdrug opened a distribution centre and office complex at Beddington Lane. The head office of international engineering and management consultant Mott MacDonald is located in Mott MacDonald House on Sydenham Road, one of four offices they occupy in the town centre. BT has large offices in Prospect East in Central Croydon. The Royal Bank of Scotland also has large offices in Purley, south of Croydon. Direct Line also has an office opposite Taberner House. Other companies with offices in Croydon include Lloyds TSB, Merrill Lynch and Balfour Beatty. Ann Summers used to have its headquarters in the borough but has moved to the Wapses Lodge Roundabout in Tandridge.\nThe Council declared bankruptcy via a section 114 notice in December 2020.\nTransport.\nRail.\nEast Croydon and West Croydon are the main stations in the borough.\nSouth Croydon railway station is also a railway station in Croydon, but it is lesser known.\nEast Croydon is served by Govia Thameslink Railway, operating under the Southern and Thameslink brands. Services travel via the Brighton Main Line north to London Victoria, London Bridge, London St Pancras, Luton Airport, Bedford, Cambridge and Peterborough and south to Gatwick Airport, Ore, Brighton, Littlehampton, Bognor Regis, Southampton and Portsmouth. East Croydon is the largest and busiest station in Croydon and the third busiest in London, excluding Travelcard Zone 1.\nEast Croydon was served by long distance Arriva CrossCountry services to Birmingham and the North of England until they were withdrawn in December 2008.\nWest Croydon is served by London Overground and Southern services north to Highbury &amp; Islington, London Bridge and London Victoria, and south to Sutton and Epsom Downs.\nSouth Croydon is mainly served by Network Rail services operated by Southern for suburban lines to and from London Bridge, London Victoria and the eastern part of Surrey.\nCroydon is one of only five London Boroughs not to have at least one London Underground station within its boundaries, with the closest tube station being Morden.\nBus.\nA sizeable bus infrastructure which is part of the London Buses network operates from a hub at West Croydon bus station. The original bus station opened in May 1985, closing in October 2014. A new bus station opened in October 2016.\nAddington Village Interchange is a regional bus terminal in Addington Village which has an interchange between Tramlink and bus services in the remote area. Services are operated under contract by Arriva London, London Central, Metrobus, Quality Line, Selkent and Transport UK London Bus.\nTram.\nThe Tramlink light rail system opened in 2000, serving the borough and surrounding areas. Its network consists of three lines, from Elmers End to West Croydon, from Beckenham to West Croydon, and from New Addington to Wimbledon, with all three lines running via the Croydon loop on which it is centred. It is also the only tram system in London but there is another light rail system, the Docklands Light Railway. It serves Mitcham, Woodside, Addiscombe and the Purley Way retail and industrial area amongst others.\nRoad.\nCroydon is linked into the national motorway network via the M23 and M25 orbital motorway. The M25 skirts the south of the borough, linking Croydon with other parts of London and the surrounding counties; the M23 branches from the M25 close to Coulsdon, linking the town with the south coast, Crawley, Reigate, and Gatwick Airport. The A23 connects the borough with the motorways. The A23 is the major trunk road through Croydon, linking it with central London, East Sussex, Horsham, and Littlehaven. The old London to Brighton road, passes through the west of the borough on Purley Way, bypassing the commercial centre of Croydon which it once did.\nThe A22 and A23 are the major trunk roads through Croydon. These both run north\u2013south, connecting to each other in Purley. The A22 connects Croydon, its starting point, to East Grinstead, Tunbridge Wells, Uckfield, and Eastbourne. Other major roads generally radiate spoke-like from the town centre. The A23 road, cuts right through Croydon, and it starts from London and links to Brighton and Gatwick Airport .Wellesley Road is an urban dual carriageway which cuts through the middle of the central business district. It was constructed in the 1960s as part of a planned ring road for Croydon and includes an underpass, which allows traffic to avoid going into the town centre.\nAir.\nThe closest international airport to Croydon is Gatwick Airport, which is located from the town centre. Gatwick Airport opened in August 1930 as an aerodrome and is a major international operational base for British Airways, EasyJet and Virgin Atlantic. It currently handles around 35 million passengers a year, making it London's second largest airport, and the second busiest airport in the United Kingdom after Heathrow. Heathrow, London City and Luton airports all lie within a two hours' drive of Croydon. Gatwick and Luton Airports are connected to Croydon by frequent direct trains, while Heathrow is accessible by the route SL7 bus.\nCycling.\nAlthough hilly, Croydon is compact and has few major trunk roads running through it. It is on one of the Connect2 schemes which are part of the National Cycle Network route running around Croydon. The North Downs, an area of outstanding natural beauty popular with both on- and off-road cyclists, is so close to Croydon that part of the park lies within the borough boundary, and there are routes into the park almost from the civic centre.\nTravel to work.\nIn March 2011, the main forms of transport that residents used to travel to work were: driving a car or van, 20.2% of all residents aged 16\u201374; train, 59.5%; bus, minibus or coach, 7.5%; on foot, 5.1%; underground, metro, light rail, tram, 4.3%; work mainly at or from home, 2.9%; passenger in a car or van, 1.5%.\nPublic services.\nHome Office policing in Croydon is provided by the Metropolitan Police. The force's Croydon arm have their head offices for policing on Park Lane next to the Fairfield Halls and Croydon College in central Croydon. Public transport is co-ordinated by Transport for London. Statutory emergency fire and rescue service is provided by the London Fire Brigade, which has five stations in Croydon.\nHealth services.\nNHS South West London Clinical Commissioning Group (A merger of the previous NHS Croydon CCG and others in South West London) is the body responsible for public health and for planning and funding health services in the borough. Croydon has 227 GPs in 64 practices, 156 dentists in 51 practices, 166 pharmacists and 70 optometrists in 28 practices.\nCroydon University Hospital, formerly known as Mayday Hospital, built on a site in Thornton Heath at the west of Croydon's boundaries with Merton, is a large NHS hospital administered by Croydon Health Services NHS Trust. Former names of the hospital include the Croydon Union Infirmary from 1885 to 1923 and the Mayday Road Hospital from 1923 to around 1930. It is a District General Hospital with a 24-hour accident and emergency department. NHS Direct has a regional centre based at the hospital. The NHS Trust also provides services at Purley War Memorial Hospital, in Purley. Croydon General Hospital was on London Road but services transferred to Mayday, as the size of this hospital was insufficient to cope with the growing population of the borough. Sickle Cell and Thalassaemia Centre and the Emergency Minor Treatment Centre are other smaller hospitals operated by the Mayday in the borough. Cane Hill was a psychiatric hospital in Coulsdon.\nWaste management.\nWaste management is co-ordinated by the local authority. Unlike other waste disposal authorities in Greater London, Croydon's rubbish is collected independently and isn't part of a waste authority unit. Locally produced inert waste for disposal is sent to landfill in the south of Croydon. There have recently been calls by the ODPM to bring waste management powers to the Greater London Authority, giving it a waste function. The Mayor of London has made repeated attempts to bring the different waste authorities together, to form a single waste authority in London. This has faced significant opposition from existing authorities. However, it has had significant support from all other sectors and the surrounding regions managing most of London's waste. Croydon has the joint best recycling rate in London, at 36%, but the refuse collectors have been criticised for their rushed performance lacking quality. Croydon's distribution network operator for electricity is EDF Energy Networks; there are no power stations in the borough. Thames Water manages Croydon's drinking and waste water; water supplies being sourced from several local reservoirs, including Beckton and King George VI. Before 1971, Croydon Corporation was responsible for water treatment in the borough.\nLondon Fire Brigade.\nThe borough of Croydon is 86.52\u00a0kmsq, populating approximately 340,000 people. There are five fire stations within the borough; Addington (two pumping appliances), Croydon (two pumping appliances, incident response unit, fire rescue unit and a USAR appliance), Norbury (two pumping appliances), Purley (one pumping appliance) and Woodside (one pumping appliance). Purley has the largest station ground, but dealt with the fewest incidents during 2006/07.\nThe fire stations, as part of the Community Fire Safety scheme, visited 49 schools in 2006/2007.\nEducation.\nThe borough compared with the other London boroughs has the highest number of schools in it, with 26% of its population under 20 years old. They include primary schools (95), secondary schools (21) and four further education establishments. Croydon College has its main building in Central Croydon, it is a high rise building. John Ruskin College is one of the other colleges in the borough, located in Addington and Coulsdon College in Coulsdon. South Norwood has been the home of Spurgeon's College, a world-famous Baptist theological college, since 1923; Spurgeon's is located on South Norwood Hill and currently has some 1000 students. The London Borough of Croydon is the local education authority for the borough.\nOverall, Croydon was ranked 77th out of all the local education authorities in the UK, up from 92nd in 2007. In 2007, the Croydon LEA was ranked 81st out of 149 in the country \u2013 and 21st in Greater London \u2013 based on the percentage of pupils attaining at least 5 A*\u2013C grades at GCSE including maths and English (37.8% compared with the national average of 46.7%). The most successful public sector schools in 2010 were Harris City Academy Crystal Palace and Coloma Convent Girls' School. The percentage of pupils achieving 5 A*-C GCSEs including maths and English was above the national average in 2010.\nLibraries.\nThe borough of Croydon has 14 libraries, a joint library and a mobile library. Many of the libraries were built a long time ago and therefore have become outdated, so the council started updating a few including Ashburton Library which moved from its former spot into the state-of-the-art Ashburton Learning Village complex which is on the former site of the old 'A Block' of Ashburton Community School which is now situated inside the centre. The library is now on one floor. This format was planned to be rolled out across all of the council's libraries but what was seen as costing too much.\nSouth Norwood Library, New Addington Library, Shirley Library, Selsdon Library, Sanderstead Library, Broad Green, Purley Library, Coulsdon Library and Bradmore Green Library are examples of older council libraries. The main library is Croydon Central Library which holds many references, newspaper archives and a tourist information point (one of three in southeast London). Upper Norwood Library is a joint library with the London Borough of Lambeth. This means that both councils fund the library and its resources, but even though Lambeth have nearly doubled their funding for the library in the past several years Croydon has kept it the same, doubting the future of the library.\nSport and leisure.\nThe borough has been criticised in the past for not having enough leisure facilities, maintaining the position of Croydon as a three star borough. Thornton Heath's ageing sports centre has been demolished and replaced by a newer more modern leisure centre. South Norwood Leisure Centre was closed down in 2006 so that it could be demolished and re-designed from scratch like Thornton Heath, at an estimated cost of around \u00a310 million.\nIn May 2006 the Conservative Party took control of Croydon Council and decided a refurbishment would be more economical than rebuilding, this decision caused some controversy.\nSport Croydon, is the commercial arm for leisure in the borough. Fusion currently provides leisure services for the council, a contract previously held by Parkwood Leisure.\nFootball teams include Crystal Palace F.C., which play at Selhurst Park, and in the Premier League. AFC Croydon Athletic, whose nickname is The Rams, is a football club who play at Croydon Sports Arena along with Croydon F.C., both in the Combined Counties League and Holmesdale, who were founded in South Norwood but currently playing on Oakley Road in Bromley, currently in the Southern Counties East Football League.\nNon-football teams that play in Croydon are Streatham-Croydon RFC, a rugby union club in Thornton Heath who play at Frant Road, as well as South London Storm Rugby League Club, based at Streatham's ground, who compete in the Rugby League Conference. The London Olympians are an American Football team that play in Division 1 South in the British American Football League. The Croydon Pirates are one of the most successful teams in the British Baseball Federation, though their ground is actually just located outside the borough in Sutton.\nThere are a number of field hockey clubs based in and around Croydon that are part of the South East Hockey and the London Hockey league structures.\n Current hockey clubs in and around the area are Addiscombe, Croydon Trinity Whitgiftian, Kenley, Purley, Purley Walcountians and Sanderstead.\nCroydon Amphibians SC plays in the Division 2 British Water Polo League. The team won the National League Division 2 in 2008.\nCroydon has over 120 parks and open spaces, ranging from the Selsdon Wood Nature Reserve to many recreation grounds and sports fields scattered throughout the Borough. This provides many places for rambling. The Wandle Trail links central London to Croydon and then The Vanguard Way links East Croydon to the South Coast and bysecting The London Loop, the North Downs Way and the Pilgrims' Way.\nCulture.\nCroydon has cut funding to the Warehouse Theatre.\nIn 2005, Croydon Council drew up a \"Public Art Strategy\", with a vision intended to be accessible and to enhance people's enjoyment of their surroundings. The public art strategy delivered a new event called \"Croydon's Summer Festival\" hosted in Lloyd Park. The festival consists of two days of events. The first is called \"Croydon's World Party\" which is a free one-day event with three stages featuring world, jazz and dance music from the UK and internationally. The final days event is the \"Croydon Mela\", a day of music with a mix of traditional Asian culture and east-meets-western club beats across four stages as well as dozens of food stalls and a funfair. It has attracted crowds of over 50,000 people. The strategy also created a creative industries hub in Old Town, ensured that public art is included in developments such as College Green and Ruskin Square and investigated the possibility of gallery space in the Cultural Quarter.\nFairfield Halls, Arnhem Gallery and the Ashcroft Theatre show productions that are held throughout the year such as drama, ballet, opera and pantomimes and can be converted to show films. It also contains the Arnhem Gallery civic hall and an art gallery. Other cultural activities, including shopping and exhibitions, are Surrey Street Market which is mainly a meat and vegetables market near the main shopping environment of Croydon. The market has a Royal Charter dating back to 1276. Airport House is a newly refurbished conference and exhibition centre inside part of Croydon Airport. The Whitgift Centre is the current main shopping centre in the borough. Centrale is a new shopping centre that houses many more familiar names, as well as Croydon's House of Fraser.\nMedia.\nThere are three local newspapers which operate within the borough. The Croydon Advertiser began life in 1869, and was in 2005 the third-best selling paid-for weekly newspaper in London. The Advertiser is Croydon's major paid-for weekly paper and is on sale every Friday in five geographical editions: Croydon; Sutton &amp; Epsom; Coulsdon &amp; Purley; New Addington; and Caterham. The paper converted from a broadsheet to a compact (tabloid) format on 31 March 2006. It was bought by Northcliffe Media which is part of the Daily Mail and General Trust group on 6 July 2007. The Croydon Post is a free newspaper available across the borough and is operated by the Advertiser group. The circulation of the newspaper was in 2008 more than the main title published by the Advertiser Group.\nThe Croydon Guardian is another local weekly paper, which is paid for at newsagents but free at Croydon Council libraries and via deliveries. It is one of the best circulated local newspapers in London and once had the highest circulation in Croydon with around one thousand more copies distributed than The Post.\nThe borough is served by the London regional versions of BBC and ITV coverage, from either the Crystal Palace or Croydon transmitters.\nCroydon Television is owned by Croydon broadcasting corporation. Broadcasting from studios in Croydon, the CBC is fully independent. It does not receive any government or local council grants or funding and is supported by donations, sponsorship and by commercial advertising.\nCapital Radio and Gold serve the borough. Local BBC radio is provided by BBC London 94.9. Other stations include Kiss 100, Absolute Radio and Magic 105.4 FM from Bauer Radio and Capital Xtra, Heart 106.2 and Smooth Radio from Global Radio. In 2012, Croydon Radio, an online and FM radio station, and the first official FM radio station for the London Borough of Croydon, began serving the area. The borough is also home to its own local TV station, \"Croydon TV\".\nTwinning.\nThe London Borough of Croydon is twinned with the municipality of Arnhem which is located in the east of the Netherlands. The city of Arnhem is one of the 20 largest cities in the Netherlands. They have been twinned since 1946 after both towns had suffered extensive bomb damage during the recently ended war. There is also a Guyanese link supported by the council.\nInvestment in the tobacco industry.\nIn September 2009 it was revealed that Croydon Council had around \u00a320m of its pension fund for employees invested in shares in Imperial Tobacco and British American Tobacco. Members of the opposition Labour group on the council, who had banned such shareholdings when in control, described this as \"dealing in death\" and inconsistent with the council's tobacco control strategy.\nFreedom of the Borough.\nThe following people and military units have received the Freedom of the Borough of Croydon."}
{"id": "7187", "revid": "39553948", "url": "https://en.wikipedia.org/wiki?curid=7187", "title": "Chick Publications", "text": ""}
{"id": "7188", "revid": "27509064", "url": "https://en.wikipedia.org/wiki?curid=7188", "title": "Carme (moon)", "text": "Carme is a retrograde irregular satellite of Jupiter. It was discovered by Seth Barnes Nicholson at Mount Wilson Observatory in California in July 1938. It is named after the mythological Carme, mother by Zeus of Britomartis, a Cretan goddess.\nHistory.\nCarme did not receive its present name until 1975; before then, it was simply known as . It was sometimes called \"Pan\" between 1955 and 1975 (Pan is now the name of a satellite of Saturn).\nIt gives its name to the Carme group, made up of irregular retrograde moons orbiting Jupiter at a distance ranging between 23 and 24 Gm and at an inclination of about 165\u00b0. Its orbital elements are as of 17 December 2020. They are continuously changing due to solar and planetary perturbations.\nProperties.\nWith a diameter of , it is the largest member of the Carme group and the fourth largest irregular moon of Jupiter. It is light red in color (B\u2212V=0.76, V\u2212R=0.47), similar to D-type asteroids and consistent with Taygete, but not Kalyke."}
{"id": "7189", "revid": "300", "url": "https://en.wikipedia.org/wiki?curid=7189", "title": "Commedia del arte", "text": ""}
{"id": "7193", "revid": "36029", "url": "https://en.wikipedia.org/wiki?curid=7193", "title": "Commutator", "text": "In mathematics, the commutator gives an indication of the extent to which a certain binary operation fails to be commutative. There are different definitions used in group theory and ring theory.\nGroup theory.\nThe commutator of two elements, and , of a group , is the element\nThis element is equal to the group's identity if and only if and commute (that is, if and only if ).\nThe set of all commutators of a group is not in general closed under the group operation, but the subgroup of \"G\" generated by all commutators is closed and is called the \"derived group\" or the \"commutator subgroup\" of \"G\". Commutators are used to define nilpotent and solvable groups and the largest abelian quotient group.\nThe definition of the commutator above is used throughout this article, but many group theorists define the commutator as\nUsing the first definition, this can be expressed as .\nIdentities (group theory).\nCommutator identities are an important tool in group theory. The expression denotes the conjugate of by , defined as .\nRelation (3) is called anticommutativity, while (4) is the Jacobi identity.\nAdditional identities.\nIf is a fixed element of a ring \"R\", identity (1) can be interpreted as a [[product rule|Leibniz rule]] for the map formula_19 given by formula_20. In other words, the map ad\"A\" defines a [[derivation (abstract algebra)|derivation]] on the ring \"R\". Identities (2), (3) represent Leibniz rules for more than two factors, and are valid for any derivation. Identities (4)\u2013(6) can also be interpreted as Leibniz rules. Identities (7), (8) express Z-[[Bilinear map|bilinearity]].\nFrom identity (9), one finds that the commutator of integer powers of ring elements is:\nSome of the above identities can be extended to the anticommutator using the above \u00b1 subscript notation.\nFor example:\nExponential identities.\nConsider a ring or algebra in which the [[exponential function|exponential]] formula_28 can be meaningfully defined, such as a [[Banach algebra]] or a ring of [[formal power series]].\nIn such a ring, [[Hadamard's lemma]] applied to nested commutators gives: formula_29 (For the last expression, see \"Adjoint derivation\" below.) This formula underlies the [[Baker\u2013Campbell\u2013Hausdorff formula#An important lemma|Baker\u2013Campbell\u2013Hausdorff expansion]] of log(exp(\"A\") exp(\"B\")).\nA similar expansion expresses the group commutator of expressions formula_30 (analogous to elements of a [[Lie group]]) in terms of a series of nested commutators (Lie brackets),\nformula_31\nGraded rings and algebras.\nWhen dealing with [[graded algebra]]s, the commutator is usually replaced by the graded commutator, defined in homogeneous components as \nAdjoint derivation.\nEspecially if one deals with multiple commutators in a ring \"R\", another notation turns out to be useful. For an element formula_33, we define the [[adjoint representation of a Lie algebra|adjoint]] mapping formula_34 by:\nThis mapping is a [[Derivation (differential algebra)|derivation]] on the ring \"R\": \nBy the [[Jacobi identity]], it is also a derivation over the commutation operation: \nComposing such mappings, we get for example formula_38 and formula_39 We may consider formula_40 itself as a mapping, formula_41, where formula_42 is the ring of mappings from \"R\" to itself with composition as the multiplication operation. Then formula_40 is a [[Lie algebra]] homomorphism, preserving the commutator:\nBy contrast, it is not always a ring homomorphism: usually formula_45.\nGeneral Leibniz rule.\nThe [[general Leibniz rule]], expanding repeated derivatives of a product, can be written abstractly using the adjoint representation:\nReplacing formula_47 by the differentiation operator formula_48, and formula_49 by the multiplication operator formula_50, we get formula_51, and applying both sides to a function \"g\", the identity becomes the usual Leibniz rule for the \"n\"th derivative formula_52.\nExternal links.\n[[Category:Abstract algebra]]\n[[Category:Group theory]]\n[[Category:Binary operations]]\n[[Category:Mathematical identities]]"}
{"id": "7196", "revid": "29463730", "url": "https://en.wikipedia.org/wiki?curid=7196", "title": "Cairn", "text": "A cairn is a human-made pile (or stack) of stones raised for a purpose, usually as a marker or as a burial mound. The word \"cairn\" comes from the (plural ).\nCairns have been and are used for a broad variety of purposes. In prehistory, they were raised as markers, as memorials and as burial monuments (some of which contained chambers). \nIn the modern era, cairns are often raised as landmarks, especially to mark the summits of mountains. Cairns are also used as trail markers. They vary in size from small stone markers to entire artificial hills, and in complexity from loose conical rock piles to elaborate megalithic structures. Cairns may be painted or otherwise decorated, whether for increased visibility or for religious reasons.\nA variant is the inuksuk (plural inuksuit), used by the Inuit and other peoples of the Arctic region of North America.\nHistory.\nEurope.\nThe building of cairns for various purposes goes back into prehistory in Eurasia, ranging in size from small rock sculptures to substantial human-made hills of stone (some built on top of larger, natural hills). The latter are often relatively massive Bronze Age or earlier structures which, like kistvaens and dolmens, frequently contain burials; they are comparable to tumuli (kurgans), but of stone construction instead of earthworks. \"Cairn\" originally could more broadly refer to various types of hills and natural stone piles, but today is used exclusively of artificial ones.\nIreland and Britain.\nThe word \"cairn\" derives from Scots (with the same meaning), in turn from Scottish Gaelic , which is essentially the same as the corresponding words in other native Celtic languages of Britain, Ireland and Brittany, including Welsh (and ), Breton , Irish , and Cornish or . Cornwall () itself may actually be named after the cairns that dot its landscape, such as Cornwall's highest point, Brown Willy Summit Cairn, a 5\u00a0m (16\u00a0ft) high and 24\u00a0m (79\u00a0ft) diameter mound atop Brown Willy hill in Bodmin Moor, an area with many ancient cairns. Burial cairns and other megaliths are the subject of a variety of legends and folklore throughout Britain and Ireland. In Scotland, it is traditional to carry a stone up from the bottom of a hill to place on a cairn at its top. In such a fashion, cairns would grow ever larger. An old Scottish Gaelic blessing is , \"I'll put a stone on your cairn\". In Highland folklore it is recounted that before Highland clans fought in a battle, each man would place a stone in a pile. Those who survived the battle returned and removed a stone from the pile. The stones that remained were built into a cairn to honour the dead. Cairns in the region were also put to vital practical use. For example, D\u00fan Aonghasa, an all-stone Iron Age Irish hill fort on Inishmore in the Aran Islands, is still surrounded by small cairns and strategically placed jutting rocks, used collectively as an alternative to defensive earthworks because of the karst landscape's lack of soil. In February 2020, ancient cairns dated back to 4,500 year-old used to bury the leaders or chieftains of neolithic tribes people were revealed in the Cwmcelyn in Blaenau Gwent by the Aberystruth Archaeological Society.\nScandinavia and Iceland.\nIn Scandinavia, cairns have been used for centuries as trail and sea marks, among other purposes, the most notable being the Three-Country Cairn. In Iceland, cairns were often used as markers along the numerous single-file roads or paths that crisscrossed the island; many of these ancient cairns are still standing, although the paths have disappeared. In Norse Greenland, cairns were used as a hunting implement, a game-driving \"lane\", used to direct reindeer towards a game jump.\nGreece and the Balkans.\nIn the mythology of ancient Greece, cairns were associated with Hermes, the god of overland travel. According to one legend, Hermes was put on trial by Hera for slaying her favorite servant, the monster Argus. All of the other gods acted as a jury, and as a way of declaring their verdict they were given pebbles, and told to throw them at whichever person they deemed to be in the right, Hermes or Hera. Hermes argued so skillfully that he ended up buried under a heap of pebbles, and this was the first cairn.\nIn Croatia, in areas of ancient Dalmatia, such as Herzegovina and the Krajina, they are known as \"gromila\".\nPortugal.\nIn Portugal, a cairn is called a . In a legend the are enchanted soldiers, and if one stone is taken from the pile and put under a pillow, in the morning a soldier will appear for a brief moment, then will change back to a stone and magically return to the pile. The cairns that mark the place where someone died or cover the graves alongside the roads where in the past people were buried are called . The same name given to the stones was given to the dead whose identity was unknown.\nNorth and northeast Africa.\nCairns (\"taalo\") are a common feature at El Ayo, Haylan, Qa'ableh, Qombo'ul, Heis, Salweyn and Gelweita, among other places. Somaliland in general is home to a lot of such historical settlements and archaeological sites wherein are found numerous ancient ruins and buildings, many of obscure origins. However, many of these old structures have yet to be properly explored, a process which would help shed further light on local history and facilitate their preservation for posterity.\nSince Neolithic times, the climate of North Africa has become drier. A reminder of the desertification of the area is provided by megalithic remains, which occur in a great variety of forms and in vast numbers in presently arid and uninhabitable wastelands: cairns (\"kerkour\"), dolmens and circles like Stonehenge, underground cells excavated in rock, barrows topped with huge slabs, and step pyramid-like mounds.\nMiddle East.\nThe Biblical place name Gilead (mentioned in the Old Testament books of Genesis, Numbers, Judges and elsewhere) means literally 'a heap of testimony (or evidence)' as does its Aramaic translation \" \". In modern Hebrew, \"gal-'ed\" () is the actual word for \"cairn\". In the cairn of Gilead was set up as a border demarcation between Jacob and his father-in-law Laban at their last meeting.\nAsia and the Pacific.\nStarting in the Bronze Age, burial cists were sometimes interred into cairns, which would be situated in conspicuous positions, often on the skyline above the village of the deceased. Though most often found in the British Isles, evidence of Bronze Age cists have been found in Mongolia. The stones may have been thought to deter grave robbers and scavengers. Another explanation is that they were to stop the dead from rising. There remains a Jewish tradition of placing small stones on a person's grave as a token of respect, known as visitation stones, though this is generally to relate the longevity of stone to the eternal nature of the soul and is not usually done in a cairn fashion. Stupas in India and Tibet probably started out in a similar fashion, although they now generally contain the ashes of a Buddhist saint or lama.\nA traditional and often decorated, heap-formed cairn called an \"ovoo\" is made in Mongolia. It primarily serves religious purposes, and finds use in both Tengriist and Buddhist ceremonies. Ovoos were also often used as landmarks and meeting points in traditional nomadic Mongolian culture. Traditional ceremonies still take place at ovoos today, and in a survey conducted, 75 participants out of 144 participants stated that they believe in ovoo ceremonies. However, mining and other industrial operations today threaten the ovoos\nIn Hawaii, cairns, called by the Hawaiian word , are still being built today. Though in other cultures, the cairns were typically used as trail markers and sometimes funerary sites, the ancient Hawaiians also used them as altars or security towers. The Hawaiian people are still building these cairns today, using them as the focal points for ceremonies honoring their ancestors and spirituality.\nIn South Korea, cairns are quite prevalent, often found along roadsides and trails, up on mountain peaks, and adjacent to Buddhist temples. Hikers frequently add stones to existing cairns trying to get just one more on top of the pile, to bring good luck. This tradition has its roots in the worship of San-shin, or Mountain Spirit, so often still revered in Korean culture.\nThe Americas.\nThroughout what today are the continental United States and Canada, some Indigenous peoples of the Americas have built structures similar to cairns. In some cases, these are general trail markers, and in other cases they mark game-driving \"lanes\", such as those leading to buffalo jumps.\nReligious Practices (North America).\nPeoples from some of the Indigenous cultures of arctic North America (i.e. northern Canada, Alaska and Greenland) have built carefully constructed stone sculptures called and, which serve as landmarks and directional markers. The oldest of these structures are very old and pre-date contact with Europeans. They are iconic of the region (an even features on the flag of the Canadian far-northeastern territory, Nunavut).\nCairns have been used throughout what is now Latin America, since pre-Columbian times, to mark trails. Even today, in the Andes of South America, the Quechuan peoples build cairns as part of their spiritual and religious traditions.\nModern cairns.\nCairns can be used to mark hiking trails, especially in mountain regions at or above the tree line. Examples can be seen in the lava fields of Volcanoes National Park to mark several hikes. Placed at regular intervals, a series of cairns can be used to indicate a path across stony or barren terrain, even across glaciers. In Acadia National Park, in Maine, the trails are marked by a special type of cairn instituted in the 1890s by Waldron Bates and dubbed Bates cairns.\nSea cairns.\nCoastal cairns called sea marks are also common in the northern latitudes, especially in the island-strewn waters of Scandinavia and eastern Canada. They are placed along shores and on islands and islets. Usually painted white for improved offshore visibility, they serve as navigation aids. In Sweden, they are called , in Finland , in Norway , and are indicated in navigation charts and maintained as part of the nautical marking system.\nCriticisms.\nCairns can be seen as a physical invitation to interact with the environment around you, but there are many voices that oppose the construction of cairns. Concerns have been raised over the construction of needless cairns. Cairns have been noted to hold cultural significance to indigenous people,\u00a0the construction of inauthentic cairns by visitors can be seen as an appropriation of indigenous traditions. The concerns arise primely over how the intent of visitors creating cairns disrespects traditional practices and attempts at land preservation.\nThe Hawaiian Volcano Observatory asks visitors to say \u201cno\u201d to rock piles after a surge in the creation of cairns by visitors. The construction of these rock formations comes at the cost of important geological features that visitors pry rocks off of. The practice is viewed as an act of graffiti on the landscape of the park.\nThe US National Park Service has a set of rules regarding public interaction with cairns found within the boundaries of the park. Falling within the rules set by the Leave No Trace rule, the Park Service has three rules:\nThis guideline is made with the intent of preventing needless cairns created by visitors and preventing the destruction of important trail-marking cairns."}
{"id": "7198", "revid": "45181859", "url": "https://en.wikipedia.org/wiki?curid=7198", "title": "Characteristic subgroup", "text": "In mathematics, particularly in the area of abstract algebra known as group theory, a characteristic subgroup is a subgroup that is mapped to itself by every automorphism of the parent group. Because every conjugation map is an inner automorphism, every characteristic subgroup is normal; though the converse is not guaranteed. Examples of characteristic subgroups include the commutator subgroup and the center of a group.\nDefinition.\nA subgroup of a group is called a characteristic subgroup if for every automorphism of , one has ; then write .\nIt would be equivalent to require the stronger condition = for every automorphism of , because implies the reverse inclusion .\nBasic properties.\nGiven , every automorphism of induces an automorphism of the quotient group , which yields a homomorphism .\nIf has a unique subgroup of a given index, then is characteristic in .\nRelated concepts.\nNormal subgroup.\nA subgroup of that is invariant under all inner automorphisms is called normal; also, an invariant subgroup.\nSince and a characteristic subgroup is invariant under all automorphisms, every characteristic subgroup is normal. However, not every normal subgroup is characteristic. Here are several examples:\nStrictly characteristic subgroup.\nA ', or a ', is one which is invariant under surjective endomorphisms. For finite groups, surjectivity of an endomorphism implies injectivity, so a surjective endomorphism is an automorphism; thus being \"strictly characteristic\" is equivalent to \"characteristic\". This is not the case anymore for infinite groups.\nFully characteristic subgroup.\nFor an even stronger constraint, a \"fully characteristic subgroup\" (also, \"fully invariant subgroup\") of a group \"G\", is a subgroup \"H\" \u2264 \"G\" that is invariant under every endomorphism of (and not just every automorphism):\nEvery group has itself (the improper subgroup) and the trivial subgroup as two of its fully characteristic subgroups. The commutator subgroup of a group is always a fully characteristic subgroup.\nEvery endomorphism of induces an endomorphism of , which yields a map .\nVerbal subgroup.\nAn even stronger constraint is verbal subgroup, which is the image of a fully invariant subgroup of a free group under a homomorphism. More generally, any verbal subgroup is always fully characteristic. For any reduced free group, and, in particular, for any free group, the converse also holds: every fully characteristic subgroup is verbal.\nTransitivity.\nThe property of being characteristic or fully characteristic is transitive; if is a (fully) characteristic subgroup of , and is a (fully) characteristic subgroup of , then is a (fully) characteristic subgroup of .\nMoreover, while normality is not transitive, it is true that every characteristic subgroup of a normal subgroup is normal.\nSimilarly, while being strictly characteristic (distinguished) is not transitive, it is true that every fully characteristic subgroup of a strictly characteristic subgroup is strictly characteristic.\nHowever, unlike normality, if and is a subgroup of containing , then in general is not necessarily characteristic in .\nContainments.\nEvery subgroup that is fully characteristic is certainly strictly characteristic and characteristic; but a characteristic or even strictly characteristic subgroup need not be fully characteristic.\nThe center of a group is always a strictly characteristic subgroup, but it is not always fully characteristic. For example, the finite group of order 12, , has a homomorphism taking to , which takes the center, formula_2, into a subgroup of , which meets the center only in the identity.\nThe relationship amongst these subgroup properties can be expressed as:\nExamples.\nFinite example.\nConsider the group (the group of order 12 that is the direct product of the symmetric group of order 6 and a cyclic group of order 2). The center of is isomorphic to its second factor formula_3. Note that the first factor, , contains subgroups isomorphic to formula_3, for instance ; let formula_5 be the morphism mapping formula_3 onto the indicated subgroup. Then the composition of the projection of onto its second factor formula_3, followed by , followed by the inclusion of into as its first factor, provides an endomorphism of under which the image of the center, formula_3, is not contained in the center, so here the center is not a fully characteristic subgroup of .\nCyclic groups.\nEvery subgroup of a cyclic group is characteristic.\nSubgroup functors.\nThe derived subgroup (or commutator subgroup) of a group is a verbal subgroup. The torsion subgroup of an abelian group is a fully invariant subgroup.\nTopological groups.\nThe identity component of a topological group is always a characteristic subgroup."}
{"id": "7199", "revid": "49048463", "url": "https://en.wikipedia.org/wiki?curid=7199", "title": "List of cat breeds", "text": "Domestic cats have been diversified by humans into breeds and domestic and wild hybrids. Many such breeds recognized by various cat registries. Additionally, there are new and experimental breeds, landraces being established as standardized breeds, distinct domestic populations not being actively developed and lapsed (extinct) breeds.\nAs of 2023, The International Cat Association (TICA) recognizes 73 standardized breeds, the Cat Fanciers' Association (CFA) recognizes 45, the F\u00e9d\u00e9ration Internationale F\u00e9line (FIFe) recognizes 50, the Governing Council of the Cat Fancy (GCCF) recognizes 45, and the World Cat Federation (WCF) recognizes 69.\nInconsistency in a breed's classification and naming among registries means that an individual animal may be considered different breeds by different registries (though not necessarily eligible for registry in them all, depending on its exact ancestry). For example, TICA's Himalayan is considered a colorpoint variety of the Persian by the CFA, while the Javanese (or Colorpoint Longhair) is a color variation of the Balinese in both the TICA and the CFA; both breeds are merged (along with the Colorpoint Shorthair) into a single \"mega-breed\", the Colourpoint, by the World Cat Federation (WCF), who have repurposed the name \"Javanese\" for the Oriental Longhair. Also, \"Colo[u]rpoint Longhair\" refers to different breeds in other registries. There are many examples of nomenclatural overlap and differences of this sort. Furthermore, many geographical and cultural names for cat breeds are fanciful selections made by Western breeders to be exotic sounding and bear no relationship to the actual origin of the breeds; the Balinese, Javanese, and Himalayan are all examples of this trend.\nThe domestic short-haired and domestic long-haired cat types are not breeds, but terms used (with various spellings) in the cat fancy to describe \"mongrel\" or \"bicolor\" cats by coat length, ones that do not belong to a particular breed. Some registries such as the Cat Fanciers' Association allow for domestic short hairs and domestic long hairs to be registered for the purpose of outcrossing. They should not be confused with standardized breeds with similar names, such as the British Shorthair and Oriental Longhair."}
{"id": "7200", "revid": "27556023", "url": "https://en.wikipedia.org/wiki?curid=7200", "title": "Class action", "text": "A class action, also known as a class action lawsuit, class suit, or representative action, is a type of lawsuit where one of the parties is a group of people who are represented collectively by a member or members of that group. The class action originated in the United States and is still predominantly an American phenomenon, but Canada, as well as several European countries with civil law, have made changes in recent years to allow consumer organizations to bring claims on behalf of consumers.\nDescription.\nIn a typical class action, a plaintiff sues a defendant or a number of defendants on behalf of a group, or class, of absent parties. This differs from a traditional lawsuit, in which the plaintiffs sue one or more defendants, and all of the parties are present in court. For example, a group in a class action lawsuit could be any person who ever bought a specific dangerous product; in a traditional lawsuit, the plaintiff is a single individual person or business that bought the dangerous product.\nAlthough standards differ between states and countries, class actions are most common where the allegations usually involve at least 40 people who the same defendant has injured in the same way. Instead of each individual person bringing their own lawsuits separately, the class action allows all the claims of all class members\u2014whether they know they have been damaged or not\u2014to be resolved in a single proceeding through the efforts of the representative plaintiff(s) and appointed class counsel.\nHistory.\nEngland and the United Kingdom.\nThe antecedent of the class action was what modern observers call \"group litigation,\" which appears to have been quite common in medieval England from about 1200 onward. These lawsuits involved groups of people either suing or being sued in actions at common law. These groups were usually based on existing societal structures like villages, towns, parishes, and guilds. Unlike modern courts, the medieval English courts did not question the right of the actual plaintiffs to sue on behalf of a group or a few representatives to defend an entire group.\nFrom 1400 to 1700, group litigation gradually switched from being the norm in England to the exception. The development of the concept of the corporation led to the wealthy supporters of the corporate form becoming suspicious of all unincorporated legal entities, which in turn led to the modern concept of the unincorporated or voluntary association. The tumultuous history of the Wars of the Roses and then the Star Chamber resulted in periods during which the common law courts were frequently paralyzed, and out of the confusion the Court of Chancery emerged with exclusive jurisdiction over group litigation.\nBy 1850, Parliament had enacted several statutes on a case-by-case basis to deal with issues regularly faced by certain types of organizations, like joint-stock companies, and with the impetus for most types of group litigation removed, it went into a steep decline in English jurisprudence from which it never recovered. It was further weakened by the fact that equity pleading, in general, was falling into disfavor, which culminated in the Judicature Acts of 1874 and 1875. Group litigation was essentially dead in the United Kingdom after 1850.\nUnited States.\nClass actions survived in the United States thanks to the influence of Supreme Court Associate Justice Joseph Story, who imported it into US law through summary discussions in his two equity treatises as well as his opinion in \"West v. Randall\" (1820). However, Story did not necessarily endorse class actions, because he \"could not conceive of a modern function or a coherent theory for representative litigation.\"\nThe oldest predecessor to the class-action rule in the United States was in the Federal Equity Rules, specifically Equity Rule 48, promulgated in 1842.\nWhere the parties on either side are very numerous, and cannot, without manifest inconvenience and oppressive delays in the suit, be all brought before it, the court in its discretion may dispense with making all of them parties, and may proceed in the suit, having sufficient parties before it to represent all the adverse interests of the plaintiffs and the defendants in the suit properly before it. But in such cases, the decree shall be without prejudice to the rights and claims of all the absent parties.\nThis allowed for representative suits in situations where there were too many individual parties (which now forms the first requirement for class-action litigation \u2013 numerosity). However, this rule did not allow such suits to bind similarly situated absent parties, which rendered the rule ineffective. Within ten years, the Supreme Court interpreted Rule 48 in such a way so that it could apply to absent parties under certain circumstances, but only by ignoring the plain meaning of the rule. In the rules published in 1912, Equity Rule 48 was replaced with Equity Rule 38 as part of a major restructuring of the Equity Rules, and when federal courts merged their legal and equitable procedural systems in 1938, Equity Rule 38 became Rule 23 of the Federal Rules of Civil Procedure.\nModern developments.\nA major revision of the FRCP in 1966 radically transformed Rule 23, made the opt-out class action the standard option, and gave birth to the modern class action. Entire treatises have been written since to summarize the huge mass of law that sprang up from the 1966 revision of Rule 23. Just as medieval group litigation bound all members of the group regardless of whether they all actually appeared in court, the modern class action binds \"all\" members of the class, except for those who choose to opt out (if the rules permit them to do so).\nThe Advisory Committee that drafted the new Rule 23 in the mid-1960s was influenced by two major developments. First was the suggestion of Harry Kalven Jr. and Maurice Rosenfield in 1941 that class-action litigation by individual shareholders on behalf of all shareholders of a company could effectively supplement direct government regulation of securities markets and other similar markets. The second development was the rise of the civil rights movement, environmentalism and consumerism. The groups behind these movements, as well as many others in the 1960s, 1970s and 1980s, all turned to class actions as a means for achieving their goals. For example, a 1978 environmental law treatise reprinted the \"entire\" text of Rule 23 and mentioned \"class actions\" 14 times in its index.\nBusinesses targeted by class actions for inflicting massive aggregate harm have sought ways to avoid class actions altogether. In the 1990s, the US Supreme Court issued several decisions that strengthened the \"federal policy favoring arbitration\". In response, lawyers have added provisions to consumer contracts of adhesion called \"collective action waivers\", which prohibit those signing the contracts from bringing class-action suits. In 2011, the US Supreme Court ruled in a 5\u20134 decision in \"AT&amp;T Mobility v. Concepcion\" that the Federal Arbitration Act of 1925 preempts state laws that prohibit contracts from disallowing class-action lawsuits, which will make it more difficult for consumers to file class-action lawsuits. The dissent pointed to a saving clause in the federal act which allowed states to determine how a contract or its clauses may be revoked.\nIn two major 21st-century cases, the Supreme Court ruled 5\u20134 against certification of class actions due to differences in each individual members' circumstances: first in \"Wal-Mart v. Dukes\" (2011) and later in \"Comcast Corp. v. Behrend\" (2013).\nCompanies may insert the phrase \"may elect to resolve any claim by individual arbitration\" into their consumer and employment contracts to use arbitration and prevent class-action lawsuits.\nRejecting arguments that they violated employees' rights to collective bargaining, and that modestly-valued consumer claims would be more efficiently litigated within the parameters of one lawsuit, the U.S. Supreme Court, in \"Epic Systems Corp. v. Lewis\" (2018), enabled the use of class action waivers. Citing its deference to freedom to contract principles, the Epic Systems opinion opened the door dramatically to the use of these waivers as a condition of employment, consumer purchases and the like. Some commentators in opposition to the ruling see it as a \"death knell\" to many employment and consumer class actions, and have increasingly pushed for legislation to circumvent it in hopes of reviving otherwise-underrepresented parties' ability to litigate on a group basis. Supporters (mostly pro-business) of the high court's ruling argue its holding is consistent with private contract principles. Many of those supporters had long-since argued that class action procedures were generally inconsistent with due process mandates and unnecessarily promoted litigation of otherwise small claims\u2014thus heralding the ruling's anti-litigation effect.\nIn 2017, the US Supreme Court issued its opinion in Bristol-Meyer Squibb Co. v. Superior Court of California, 137 S. Ct. 1773 (2017), holding that over five hundred plaintiffs from other states cannot bring a consolidated mass action against the pharmaceutical giant in the State of California. This opinion may arguably render nationwide mass action and class action impossible in any single state besides the defendant's home state.\nIn 2020, the 11th Circuit Court of Appeals found incentive awards are impermissible. Incentive awards are a relatively modest payment made to class representatives as part of a class settlement. The ruling was a response to an objector who claimed Rule 23 required that the fee petition be filed \"before\" the time frame for class member objections to be filed; and payments to the class representative violates doctrine from two US Supreme Court cases from the 1800s.\nStatistics.\nAs of 2010, there was no publicly maintained list of nonsecurities class-action settlements, although a securities class-action database exists in the Stanford Law School Securities Class Action Clearinghouse and several for-profit companies maintain lists of the securities settlements. One study of federal settlements required the researcher to manually search databases of lawsuits for the relevant records, although state class actions were not included due to the difficulty in gathering the information. Another source of data is US Bureau of Justice Statistics \"Civil Justice Survey of State Courts\", which offers statistics for the year 2005.\nAdvantages.\nProponents of class actions state that they offer a number of advantages because they aggregate many individualized claims into one representational lawsuit.\nFirst, aggregation can increase the efficiency of the legal process, and lower the costs of litigation. In cases with common questions of law and fact, aggregation of claims into a class action may avoid the necessity of repeating \"days of the same witnesses, exhibits and issues from trial to trial\". \"Jenkins v. Raymark Indus. Inc.\", 782 F.2d 468, 473 (5th Cir. 1986) (granting certification of a class action involving asbestos).\nSecond, a class action may overcome \"the problem that small recoveries do not provide the incentive for any individual to bring a solo action prosecuting his or her rights\". \"Amchem Prods., Inc. v. Windsor\", 521 U.S. 591, 617 (1997) (quoting \"Mace v. Van Ru Credit Corp.\", 109 F.3d 388, 344 (7th Cir. 1997)). \"A class action solves this problem by aggregating the relatively paltry potential recoveries into something worth someone's (usually an attorney's) labor.\" \"Amchem Prods., Inc.\", 521 U.S. at 617 (quoting \"Mace\", 109 F.3d at 344). In other words, a class action ensures that a defendant who engages in widespread harmbut does so minimally against each individual plaintiffmust compensate those individuals for their injuries. For example, thousands of shareholders of a public company may have losses too small to justify separate lawsuits, but a class action can be brought efficiently on behalf of all shareholders. Perhaps even more important than compensation is that class treatment of claims may be the only way to impose the costs of wrongdoing on the wrongdoer, thus deterring future wrongdoing.\nThird, class-action cases may be brought to purposely change behavior of a class of which the defendant is a member. \"Landeros v. Flood\" (1976) was a landmark case decided by the California Supreme Court that aimed at purposefully changing the behavior of doctors, encouraging them to report suspected child abuse. Otherwise, they would face the threat of civil action for damages in tort proximately flowing from the failure to report the suspected injuries. Previously, many physicians had remained reluctant to report cases of apparent child abuse, despite existing law that required it.\nFourth, in \"limited fund\" cases, a class action ensures that all plaintiffs receive relief and that early-filing plaintiffs do not raid the fund (i.e., the defendant) of all its assets before other plaintiffs may be compensated. See \"Ortiz v. Fibreboard Corp.\", 527 U.S. 815 (1999). A class action in such a situation centralizes all claims into one venue where a court can equitably divide the assets amongst all the plaintiffs if they win the case.\nFinally, a class action avoids the situation where different court rulings could create \"incompatible standards\" of conduct for the defendant to follow. See Fed. R. Civ. P. 23(b)(1)(A). For example, a court might certify a case for class treatment where a number of individual bond-holders sue to determine whether they may convert their bonds to common stock. Refusing to litigate the case in one trial could result in different outcomes and inconsistent standards of conduct for the defendant corporation. Thus, courts will generally allow a class action in such a situation. See, e.g., \"Van Gemert v. Boeing Co.\", 259 F. Supp. 125 (S.D.N.Y. 1966).\nWhether a class action is superior to individual litigation depends on the case and is determined by the judge's ruling on a motion for class certification. The Advisory Committee Note to Rule 23, for example, states that mass torts are ordinarily \"not appropriate\" for class treatment. Class treatment may not improve the efficiency of a mass tort because the claims frequently involve individualized issues of law and fact that will have to be re-tried on an individual basis. See \"Castano v. Am. Tobacco Co.\", 84 F.3d 734 (5th Cir. 1996) (rejecting nationwide class action against tobacco companies). Mass torts also involve high individual damage awards; thus, the absence of class treatment will not impede the ability of individual claimants to seek justice. Other cases, however, may be more conducive to class treatment.\nThe preamble to the Class Action Fairness Act of 2005, passed by the United States Congress, found:\nClass-action lawsuits are an important and valuable part of the legal system when they permit the fair and efficient resolution of legitimate claims of numerous parties by allowing the claims to be aggregated into a single action against a defendant that has allegedly caused harm. \nCriticisms.\nThere are several criticisms of class actions. The preamble to the Class Action Fairness Act stated that some abusive class actions have harmed class members possessing legitimate claims and defendants acting responsibly; have adversely affected interstate commerce; and have undermined public respect for the country's judicial system.\nClass members often receive little or no benefit from class actions. Examples cited for this include large fees for the attorneys, while leaving class members with coupons or other awards of little or no value; unjustified awards are made to certain plaintiffs at the expense of other class members; and confusing notices are published that prevent class members from being able to fully understand and effectively exercise their rights.\nFor example, in the United States, class lawsuits sometimes bind all class members with a low settlement. These \"coupon settlements\" (which usually allow the plaintiffs to receive a small benefit such as a small check or a coupon for future services or products with the defendant company) are a way for a defendant to forestall major liability by precluding many people from litigating their claims separately, to recover reasonable compensation for the damages. However, existing law requires judicial approval of all class-action settlements, and in most cases, class members are given a chance to opt out of class settlement, though class members, despite opt-out notices, may be unaware of their right to opt-out because they did not receive the notice, did not read it or did not understand it.\nThe Class Action Fairness Act of 2005 addresses these concerns. An independent expert may scrutinize coupon settlements before judicial approval in order to ensure that the settlement will be of value to the class members (28 U.S.C.A. 1712(d)). Further, if the action provides for settlement in coupons, \"the portion of any attorney's fee award to class counsel that is attributable to the award of the coupons shall be based on the value to class members of the coupons that are redeemed\". 28 U.S.C.A. 1712(a).\nA common critique is that class actions are a form of judicially sanctioned extortion. The extortion thesis was first articulated by law professor Milton Handler, who published a famous law review article in 1971 calling the class action a form of \"legalized blackmail\". It has garnered the support of a significant minority of the justices of the U.S. Supreme Court, along with prominent judges like Henry Friendly and Richard Posner. However, empirical studies have generally found the extortion thesis to be \"overstated\".\nEthics.\nClass action cases present significant ethical challenges. Defendants can hold reverse auctions and any of several parties can engage in collusive settlement discussions. Subclasses may have interests that diverge greatly from the class but may be treated the same. Proposed settlements could offer some groups (such as former customers) much greater benefits than others. In one paper presented at an ABA conference on class actions in 2007, authors commented that \"competing cases can also provide opportunities for collusive settlement discussions and reverse auctions by defendants anxious to resolve their new exposure at the most economic cost\".\nAdvertising or otherwise soliciting to find lead plaintiffs may also be unethical, as the plaintiff may not genuinely be aggrieved.\nDefendant class action.\nAlthough normally plaintiffs are the class, defendant class actions are also possible. For example, in 2005, the Roman Catholic Archdiocese of Portland in Oregon was sued as part of the Catholic priest sex-abuse scandal. All parishioners of the Archdiocese's churches were cited as a defendant class. This was done to include their assets (local churches) in any settlement. Where both the plaintiffs and the defendants have been organized into court-approved classes, the action is called a bilateral class action.\nIn the United States, only a few hundred defendant class actions have been filed (mostly in securities cases and constitutional challenges), and circuit courts are split as to whether injunctive relief is available against defendant classes at all.\nMass actions.\nIn a class action, the plaintiff seeks court approval to litigate on behalf of a group of similarly situated persons. Not every plaintiff looks for or could obtain such approval. As a procedural alternative, plaintiff's counsel may attempt to sign up every similarly situated person that counsel can find as a client. Plaintiff's counsel can then join the claims of all of these persons in one complaint, a so-called \"mass action\", hoping to have the same efficiencies and economic leverage as if a class had been certified.\nBecause mass actions operate outside the detailed procedures laid out for class actions, they can pose special difficulties for both plaintiffs, defendants, and the court. For example, settlement of class actions follows a predictable path of negotiation with class counsel and representatives, court scrutiny, and notice. There may not be a way to uniformly settle all of the many claims brought via a mass action. Some states permit plaintiff's counsel to settle for all the mass action plaintiffs according to a majority vote, for example. Other states, such as New Jersey, require each plaintiff to approve the settlement of that plaintiff's own individual claims.\nClass action legislation.\nArgentina.\nClass actions were recognized in \"Halabi\" leading case (Supreme Court, 2009).\nAustralia and New Zealand.\nClass actions became part of the Australian legal landscape only when the Federal Parliament amended the Federal Court of Australia Act in 1992 to introduce \"representative proceedings\", the equivalent of the American \"class actions\".\nLikewise, class actions appeared slowly in the New Zealand legal system. However, a group can bring litigation through the action of a representative under the High Court Rules which provide that one or a multitude of persons may sue on behalf of, or for the benefit of, all persons \"with the same interest in the subject matter of a proceeding\". The presence and expansion of litigation funders have been playing a significant role in the emergence of class actions in New Zealand. For example, the \"Fair Play on Fees\" proceedings in relation to penalty fees charged by banks were funded by Litigation Lending Services (LLS), a company specializing in the funding and management of litigation in Australia and New Zealand. It was the biggest class-action suit in New Zealand history.\nAustria.\nThe Austrian Code of Civil Procedure (\u00a0\u2013 ZPO) does not provide for a special proceeding for complex class-action litigation. However, Austrian consumer organizations ( (VKI) and the Federal Chamber of Labour / ) have brought claims on behalf of hundreds or even thousands of consumers. In these cases, the individual consumers assigned their claims to one entity, who has then brought an ordinary (two-party) lawsuit over the assigned claims. The monetary benefits were redistributed among the class. This technique, labeled as \"class action Austrian style\", allows for a significant reduction of overall costs. The Austrian Supreme Court, in a judgment, confirmed the legal admissibility of these lawsuits under the condition that all claims are essentially based on the same grounds.\nThe Austrian Parliament unanimously requested the Austrian Federal Minister for Justice to examine the possibility of new legislation providing for a cost-effective and appropriate way to deal with mass claims. Together with the Austrian Ministry for Social Security, Generations and Consumer Protection, the Justice Ministry opened the discussion with a conference held in Vienna in June 2005. With the aid of a group of experts from many fields, the Justice Ministry began drafting the new law in September 2005. With the individual positions varying greatly, a political consensus could not be reached.\nCanada.\nProvincial laws in Canada allow class actions. All provinces permit plaintiff classes and some permit defendant classes. Quebec was the first province to enact class proceedings legislation, in 1978. Ontario was next, with the Class Proceedings Act, 1992. As of 2008, 9 of 10 provinces had enacted comprehensive class actions legislation. In Prince Edward Island, where no comprehensive legislation exists, following the decision of the Supreme Court of Canada in \"Western Canadian Shopping Centres Inc. v. Dutton\", [2001] 2 S.C.R. 534, class actions may be advanced under a local rule of court. The Federal Court of Canada permits class actions under Part V.1 of the Federal Courts Rules.\nLegislation in Saskatchewan, Manitoba, Ontario, and Nova Scotia expressly or by judicial opinion has been read to allow for what are informally known as national \"opt-out\" class actions, whereby residents of other provinces may be included in the class definition and potentially be bound by the court's judgment on common issues unless they opt out in a prescribed manner and time. Court rulings have determined that this permits a court in one province to include residents of other provinces in the class action on an \"opt-out\" basis.\nJudicial opinions have indicated that provincial legislative national opt-out powers should not be exercised to interfere with the ability of another province to certify a parallel class action for residents of other provinces. The first court to certify will generally exclude residents of provinces whose courts have certified a parallel class action. However, in the Vioxx litigation, two provincial courts certified overlapping class actions whereby Canadian residents were class members in two class actions in two provinces. Both decisions are under appeal.\nOther legislation may provide for representative actions on behalf of a large number of plaintiffs, independent of class action procedures. For instance, under Ontario's Condominium Act, a condominium's governing corporation may launch a lawsuit on behalf of the owners for damage to the condominium's common elements, even though the corporation does not own the common elements.\nThe largest class action suit in Canada was settled in 2005 after Nora Bernard initiated efforts that led to an estimated 79,000 survivors of Canada's residential school system suing the Canadian government. The settlement amounted to upwards of $5 billion.\nChile.\nChile approved class actions in 2004. The Chilean model is technically an opt-out issue class action, followed by a compensatory stage which can be collective or individual. This means that the class action is designed to declare the defendant generally liable with effects if and only if the defendant is found liable, and the declaratory judgment can be used then to pursue damages in the same procedure or in individual ones in different jurisdictions. If the latter is the case, the liability cannot be discussed, but only the damages. There under the Chilean procedural rules, one particular case works as an opt-out class action for damages. This is the case when defendants can identify and compensate consumers directly, i.e. because it is their banking institution. In such cases, the judge can skip the compensatory stage and order redress directly. Since 2005 more than 100 cases have been filed, mostly by [SERNAC], the Chilean consumer protection agency. Salient cases have been \"Condecus v. BancoEstado\" and \"SERNAC v. La Polar\".\nFrance.\nUnder French law, an association can represent the collective interests of consumers; however, each claimant must be individually named in the lawsuit. On January 4, 2005, President Chirac urged changes that would provide greater consumer protection. A draft bill was proposed in April 2006 but did not pass.\nFollowing the change of majority in France in 2012, the new government proposed introducing class actions into French law. The project of of May 2013 aimed to limit the class action to consumer and competition disputes. The law was passed on March 1, 2014.\nGermany.\nClass actions are generally not permitted in Germany, as German law does not recognize the concept of a targeted class being affected by certain actions. This requires each plaintiff to individually prove that they were affected by an action, and present their individual damages, and prove the causality between both parties.\nJoint litigation () is a legal act that may permit plaintiffs that are in the same legal community with respect to the dispute, or are entitled by the same factual or legal reason. These are not typically regarded as class action suits, as each individual plaintiff is entitled to compensation for their individual, incurred damages and not as a result of being a member of a class.\nThe combination of court cases () is another method that permits a judge to combine multiple separate court cases into a single trial with a single verdict. According to \u00a7 147 ZPO, this is only permissible if all cases are regarding the same factual and legal event and basis.\nMediation procedure.\nA genuine extension of the legal effect of a court decision beyond the parties involved in the proceedings is offered under corporate law. This procedure applies to the review of stock payoffs under the Stock Corporation Act (). Pursuant to Sec. 13 Paragraph 2 of the Mediation Procedure Act (), the court decision concerning the dismissal or direction of a binding arrangement of an adequate compensation is effective for and against all shareholders, including those who have already agreed to a previous settlement in this matter.\nInvestor model case proceedings.\nThe Capital Investor Model Case Act () is an attempt to enable model cases to be brought by a large number of potentially affected parties in the event of disputes, limited to the investment market. In contrast to US class actions, each affected party must file a lawsuit in its own name in order to participate in the model proceedings.\nModel Declaratory Action.\nEffective on November 1, 2018, the Code of Civil Procedure () introduced the Model Declaratory Action (\u00a7 606 ZPO) that created the ability to bundle similar claims by many affected parties efficiently into one proceeding.\nRegistered Consumer Protection Associations can file \u2013 if they represent at least 10 individuals \u2013 for a (general) judicial finding whether the factual and legal requirements for of claims or legal relationships are met or not. These individuals have to register in order to inhibit their claims. Since these Adjudications are more of a general nature, each individual must assert their claims in their own court proceedings. The competent court is bound by the Model Declaratory Action decision.\nAssociate Action.\nGerman law also recognizes the associative action (), which is comparable to the class action and is predominantly used in environmental law. In civil law, the associative action is represented by a foreign body in the matter of asserting and enforcing individual claims and the claimant can no longer control the proceedings.\nClass action with relation to the United States.\nClass actions can be brought by Germans in the US for events in Germany if the facts of the case relate to the US. For example, in the case of the Eschede train disaster, the lawsuit was allowed because several aggrieved parties came from the US and had purchased rail tickets there.\nIndia.\nDecisions of the Indian Supreme Court in the 1980s loosened strict \"locus standi\" requirements to permit the filing of suits on behalf of rights of deprived sections of society by public-minded individuals or bodies. Although not strictly \"class action litigation\" as it is understood in American law, public interest litigation arose out of the wide powers of judicial review granted to the Supreme Court of India and the various High Courts under and Article 226 of the Constitution of India. The sort of remedies sought from courts in public interest litigation go beyond mere award of damages to all affected groups, and have sometimes (controversially) gone on to include Court monitoring of the implementation of legislation and even the framing of guidelines in the absence of Parliamentary legislation.\nHowever, this innovative jurisprudence did not help the victims of the Bhopal gas tragedy, who were unable to fully prosecute a class-action litigation (as understood in the American sense) against Union Carbide due to procedural rules that would make such litigation impossible to conclude and unwieldy to carry out. Instead, the Government of India exercised its right of \"parens patriae\" to appropriate all the claims of the victims and proceeded to litigate on their behalf, first in the New York courts and later, in the Indian courts. Ultimately, the matter was settled between the Union of India and Union Carbide (in a settlement overseen by the Supreme Court of India) for a sum of as a complete settlement of all claims of all victims for all time to come.\nPublic interest litigation has now broadened in scope to cover larger and larger groups of citizens who may be affected by government inaction. Examples of this trend include the conversion of all public transport in the city of Delhi from diesel engines to compressed natural gas engines on the basis of the orders of the Delhi High Court; the monitoring of forest use by the High Courts and the Supreme Court to ensure that there is no unjustified loss of forest cover; and the directions mandating the disclosure of assets of electoral candidates for the Houses of Parliament and State Assembly.\nThe Supreme Court has observed that the PIL has tended to become a means to gain publicity or obtain relief contrary to constitutionally valid legislation and policy. Observers point out that many High Courts and certain Supreme Court judges are reluctant to entertain PILs filed by non-governmental organizations and activists, citing concerns of separation of powers and parliamentary sovereignty.\nIreland.\nIn Irish law, there is no such thing as a \"class action\" per se. Third-party litigation funding (champerty) is prohibited under Irish law. Instead, there is the 'representative action' () or 'test case' (). A representative action is \"where one claimant or defendant, with the same interest as a group of claimants or defendants in an action, institutes or defends proceedings on behalf of that group of claimants or defendants.\"\nSome test cases in Ireland have included:\nItaly.\nItaly has class action legislation. Consumer associations can file claims on behalf of groups of consumers to obtain judicial orders against corporations that cause injury or damage to consumers. These types of claims are increasing, and Italian courts have allowed them against banks that continue to apply compound interest on retail clients' current account overdrafts. Class action is regulated by art. 140 bis of the Italian consumers' code and has been in force since 1 July 2009. On May 19, 2021, the reform of the Italian legal framework on class actions finally entered into force. The new rules, designed by Law n. 31 and published on April 18, 2019, (Law n. 31/2019), were initially intended to become effective on April 19, 2020, but were delayed twice. The new rules on class actions are now included in the Italian Civil Procedure Code (ICPC). Overall, the new class action appears to be a viable instrument which, through a system of economic incentives, could overcome the rational apathy of small-claims holders and ensure redress.\nNetherlands.\nDutch law allows associations () and foundations () to bring a so-called collective action on behalf of other persons, provided they can represent the interests of such persons according to their by-laws () (section 3:305a Dutch Civil Code). All types of actions are permitted. This includes a claim for monetary damages, provided the event occurred after 15 November 2016 (pursuant to new legislation which entered into force 1 January 2020). Most class actions over the past decade have been in the field of securities fraud and financial services. The acting association or foundation may come to a collective settlement with the defendant. The settlement may also include\u00a0\u2013 and usually primarily consists of\u00a0\u2013 monetary compensation of damages. Such settlement can be declared binding for all injured parties by the Amsterdam Court of Appeal (section 7:907 Dutch Civil Code). The injured parties have an opt-out right during the opt-out period set by the Court, usually 3 to 6 months. Settlements involving injured parties from outside the Netherlands can also be declared binding by the Court. Since US courts are reluctant to take up class actions brought on behalf of injured parties not residing in the US who have suffered damages due to acts or omissions committed outside the US, combinations of US class actions and Dutch collective actions may come to a settlement that covers plaintiffs worldwide. An example of this is the Royal Dutch Shell Oil Reserves Settlement that was declared binding upon both US and non-US plaintiffs.\nPoland.\n or class action has been allowed under Polish law since July 19, 2010. A minimum of 10 persons, suing based on the same law, is required.\nRussia.\nCollective litigation has been allowed under Russian law since 2002. Basic criteria are, like in the US, numerosity, commonality, and typicality.\nSpain.\nSpanish law allows nominated consumer associations to take action to protect the interests of consumers. A number of groups already have the power to bring collective or class actions: certain consumer associations, bodies legally constituted to defend the \"collective interest\" and groups of injured parties.\nRecent changes to Spanish civil procedure rules include the introduction of a quasi-class action right for certain consumer associations to claim damages on behalf of unidentified classes of consumers. The rules require consumer associations to represent an adequate number of affected parties who have suffered the same harm. Also, any judgment made by the Spanish court will list the individual beneficiaries or, if that is not possible, conditions that need to be fulfilled for a party to benefit from a judgment.\nSwitzerland.\nSwiss law does not allow for any form of class action. When the government proposed a new federal code of civil procedure in 2006, replacing the cantonal codes of civil procedure, it rejected the introduction of class actions, arguing that\nUnited Kingdom.\nEngland and Wales.\nThe Civil Procedure Rules of the courts of England and Wales came into force in 1999 and have provided for group litigation orders in limited circumstances (under Part 19.21\u201326, supplemented by Practice Direction 19B). HM Courts and Tribunals Service maintains a public list of group litigation orders, and there have been 124 orders granted.\nA sectoral mechanism was adopted by the Consumer Rights Act 2015, taking effect on October 1, 2015. Under the provisions therein, opt-in or opt-out collective procedures may be certified for breaches of competition law. This is currently the closest mechanism to a class action in England and Wales.\nScotland.\nA similar approach exists in Scotland to bring group proceedings under Part 4 of the Civil Litigation (Expenses and Group Proceedings) (Scotland) Act 2018. The Scottish Courts and Tribunals Service maintain a public list of group proceedings cases.\nUnited States.\nIn the United States, the class representative, also called a lead plaintiff, named plaintiff, or representative plaintiff, is the named party in a class-action lawsuit. Although the class representative is named as a party to the litigation, the court must approve the class representative when it certifies the lawsuit as a class action.\nThe class representative must be able to represent the interests of all the members of the class, by being typical of the class members and not having conflicts with them. He or she is responsible for hiring the attorney, filing the lawsuit, consulting on the case, and agreeing to any settlement. In exchange, the class representative may be entitled to compensation (at the court's discretion) out of the recovery amount.\nStanding.\nIn securities class actions that allege violations of Section 11 of the Securities Act of 1933, \"officers and directors are liable together with the corporation for material misrepresentations in the registration statement.\"\nTo have standing to sue under Section 11 of the 1933 Act in a class action, a plaintiff must be able to prove that he can trace his shares to the registration statement in question, as to which there is alleged a material misstatement or omission. In the absence of an ability to actually trace his shares, such as when securities issued at multiple times are held by the depository trust company in a fungible bulk and physical tracing of particular shares may be impossible, the plaintiff may be barred from pursuing his claim for lack of standing.\nFederal courts.\nIn federal courts, class actions are governed by Federal Rules of Civil Procedure Rule and 28 U.S.C.A. \u00a7\u00a01332(d). Cases in federal courts are only allowed to proceed as class actions if the court has jurisdiction to hear the case, and if the case meets the criteria set out in Rule 23. In the vast majority of federal class actions, the class is acting as the plaintiff. However, Rule 23 also provides for defendant class actions.\nTypically, federal courts are thought to be more favorable for defendants, and state courts more favorable for plaintiffs. Many class actions are filed initially in state court. The defendant will frequently try to remove the case to federal court. The Class Action Fairness Act of 2005 increases defendants' ability to remove state cases to federal court by giving federal courts original jurisdiction for all class actions with damages exceeding $5,000,000 exclusive of interest and costs. The Class Action Fairness Act contains carve-outs for, among other things, shareholder class actions covered by the Private Securities Litigation Reform Act of 1995 and those concerning internal corporate governance issues (the latter typically being brought as shareholder derivative actions in the state courts of Delaware, the state of incorporation of most large corporations).\nJurisdiction.\nClass actions may be brought in federal court if the claim arises under federal law or if the claim falls under 28 U.S.C. \u00a7 1332(d). Under \u00a7\u00a01332(d)(2) the federal district courts have original jurisdiction over any civil action where the amount in controversy exceeds $5,000,000 and\nNationwide plaintiff classes are possible, but such suits must have a commonality of issues across state lines. This may be difficult if the civil law in the various states lack significant commonalities. Large class actions brought in federal court frequently are consolidated for pre-trial purposes through the device of multidistrict litigation (MDL). It is also possible to bring class actions under state law, and in some cases the court may extend its jurisdiction to all the members of the class, including out of state (or even internationally) as the key element is the jurisdiction that the court has over the defendant.\nClass certification under Rule 23.\nFor the case to proceed as a class action and bind absent class members, the court must certify the class under Rule 23 on a motion from the party wishing to proceed on a class basis. For a class to be certified, the moving party must meet all of the criteria listed under Rule 23(a), and at least one of the criteria listed under Rule 23(b).\nThe 23(a) criteria are referred to as numerosity, commonality, typicality, and adequacy. Numerosity refers to the number of people in the class. To be certified, the class has to have enough members that simply adding each of them as a named party to the lawsuit would be impractical. There is no bright-line rule to determine numerosity, but classes with hundreds of members are generally deemed to be sufficiently numerous. To satisfy commonality, there must be a common question of law and fact such that \"determination of its truth or falsity will resolve an issue that is central to the validity of each one of the claims in one stroke\". The typicality requirement ensures that the claims or defenses of the named plaintiff are typical of those of everyone else in the class. Finally, adequacy requirement states that the named plaintiff must fairly and adequately represent the interests of the absent class members.\nRule 23(b)(3) allows class certification if \"questions of law or fact common to class members \"predominate\" over any questions affecting only individual members, and that a class action is \"superior\" to other available methods for fairly and efficiently adjudicating the controversy.\"\nNotice and settlement.\nDue process requires in most cases that notice describing the class action be sent, published, or broadcast to class members. As part of this notice procedure, there may have to be several notices, first a notice allowing class members to opt out of the class, i.e. if individuals wish to proceed with their own litigation they are entitled to do so, only to the extent that they give timely notice to the class counsel or the court that they are opting out. Second, if there is a settlement proposal, the court will usually direct the class counsel to send a settlement notice to all the members of the certified class, informing them of the details of the proposed settlement.\nState courts.\nSince 1938, many states have adopted rules similar to the FRCP. However, some states, like California, have civil procedure systems, which deviate significantly from the federal rules; the California Codes provide for four separate types of class actions. As a result, there are two separate treatises devoted solely to the complex topic of California class actions. , only Virginia and Massachusetts do not provide for any class actions. Others, such as New York, limit the types of claims that may be brought as class actions.&lt;ref name=\"LII / Legal Information Institute 1966 g798\"&gt;&lt;/ref&gt;"}
{"id": "7201", "revid": "38001712", "url": "https://en.wikipedia.org/wiki?curid=7201", "title": "Contempt of court", "text": "Contempt of court, often referred to simply as \"contempt\", is the crime of being disobedient to or disrespectful toward a court of law and its officers in the form of behavior that opposes or defies the authority, justice, and dignity of the court. A similar attitude toward a legislative body is termed contempt of Parliament or contempt of Congress. The verb for \"to commit contempt\" is contemn (as in \"to contemn a court order\") and a person guilty of this is a contemnor or contemner.\nThere are broadly two categories of contempt: being disrespectful to legal authorities in the courtroom, or willfully failing to obey a court order. Contempt proceedings are especially used to enforce equitable remedies, such as injunctions. In some jurisdictions, the refusal to respond to subpoena, to testify, to fulfill the obligations of a juror, or to provide certain information can constitute contempt of the court.\nWhen a court decides that an action constitutes contempt of court, it can issue an order in the context of a court trial or hearing that declares a person or organization to have disobeyed or been disrespectful of the court's authority, called \"found\" or \"held\" in contempt. That is the judge's strongest power to impose sanctions for acts that disrupt the court's normal process.\nA finding of being in contempt of court may result from a failure to obey a lawful order of a court, showing disrespect for the judge, disruption of the proceedings through poor behavior, or publication of material or non-disclosure of material, which in doing so is deemed likely to jeopardize a fair trial. A judge may impose sanctions such as a fine, jail or social service for someone found guilty of contempt of court, which makes contempt of court a process crime. Judges in common law systems usually have more extensive power to declare someone in contempt than judges in civil law systems.\nIn use today.\nContempt of court is essentially seen as a form of disturbance that may impede the functioning of the court. The judge may impose fines and/or jail time upon any person committing contempt of court. The person is usually let out upon an agreement to fulfill the wishes of the court. Civil contempt can involve acts of omission. The judge will make use of warnings in most situations that may lead to a person being charged with contempt if the warnings are ignored. It is relatively rare that a person is charged for contempt without first receiving at least one warning from the judge. Constructive contempt, also called \"consequential contempt\", is when a person fails to fulfill the will of the court as it applies to outside obligations of the person. In most cases, constructive contempt is considered to be in the realm of civil contempt due to its passive nature.\nIndirect contempt is something that is associated with civil and constructive contempt and involves a failure to follow court orders. Criminal contempt includes anything that could be considered a disturbance, such as repeatedly talking out of turn, bringing forth previously banned evidence, or harassment of any other party in the courtroom, including committing an assault against the defendant in a criminal case. There have been instances during murder trials that grieving family members of murder victims have attacked the defendants in courtrooms in plain view of judges, bailiffs, and jurors, leading to said family members to be charged with contempt. Direct contempt is an unacceptable act in the presence of the judge (\"in facie curiae\"), and generally begins with a warning; it may be accompanied by the immediate imposition of a punishment.\nAustralia.\nIn Australia, a judge may impose a fine or jail for contempt of court.\nBelgium.\nA Belgian correctional or civil judge may immediately try the person for insulting the court.\nCanada.\nCommon law offense.\nIn Canada, contempt of court is an exception to the general principle that all criminal offences are set out in the federal Criminal Code. Contempt of court is the only remaining common law offence in Canada.\nContempt of court includes the following behaviors:\nCanadian Federal courts.\n\"This section applies only to the Federal Court of Appeal and Federal Court.\"\nUnder Federal Court Rules, Rules 466, and Rule 467 a person who is accused of Contempt needs to be first served with a contempt order and then appear in court to answer the charges. Convictions can only be made when proof beyond a reasonable doubt is achieved.\nIf it is a matter of urgency or the contempt was done in front of a judge, that person can be punished immediately. Punishment can range from the person being imprisoned for a period of less than five years or until the person complies with the order or fine.\nTax Court of Canada.\nUnder Tax Court of Canada Rules of \"Tax Court of Canada Act\", a person who is found to be in contempt may be imprisoned for a period of less than two years or fined. Similar procedures for serving an order first is also used at the Tax Court.\nProvincial courts.\nDifferent procedures exist for different provincial courts. For example, in British Columbia, a justice of the peace can only issue a summons to an offender for contempt, which will be dealt with by a judge, even if the offence was done in the face of the justice.\nHong Kong.\nJudges from the Hong Kong Court of Final Appeal, High Court of Hong Kong, District Court along with members from the various tribunals and Coroner's Court all have the power to impose immediate punishments for contempt in the face of the court, derived from legislation or through common law:\nThe use of insulting or threatening language in the magistrates' courts or against a magistrate is in breach of section 99 of the Magistrates Ordinance (Cap 227) which states the magistrate can 'summarily sentence the offender to a fine at level 3 and to imprisonment for 6 months.'\nIn addition, certain appeal boards are given the statutory authority for contempt by them (e.g., Residential Care Home, Hotel and Guesthouse Accommodation, Air Pollution Control, etc.). For contempt in front of these boards, the chairperson will certify the act of contempt to the Court of First Instance who will then proceed with a hearing and determine the punishment.\nEngland and Wales.\nIn England and Wales (a common law jurisdiction), the law on contempt is partly set out in case law (common law), and partly codified by the Contempt of Court Act 1981. Contempt may be classified as \"criminal\" or \"civil\". The maximum penalty for criminal contempt under the 1981 Act is committal to prison for two years.\nDisorderly, contemptuous or insolent behaviour toward the judge or magistrates while holding the court, tending to interrupt the due course of a trial or other judicial proceeding, may be prosecuted as \"direct\" contempt. The term \"direct\" means that the court itself cites the person in contempt by describing the behaviour observed on the record. Direct contempt is distinctly different from indirect contempt, wherein another individual may file papers alleging contempt against a person who has willfully violated a lawful court order.\nThere are limits to the powers of contempt created by rulings of European Court of Human Rights. Reporting on contempt of court, the Law Commission commented that \"punishment of an advocate for what he or she says in court, whether a criticism of the judge or a prosecutor, amounts to an interference with his or her rights under article 10 of the ECHR\" and that such limits must be \"prescribed by law\" and be \"necessary in a democratic society\", citing Nikula v Finland.\nCriminal contempt.\nThe Crown Court is a superior court according to the Senior Courts Act 1981, and Crown Courts have the power to punish contempt. The Divisional Court as part of the High Court has ruled that this power can apply in these three circumstances:\nWhere it is necessary to act quickly, a judge may act to impose committal (to prison) for contempt.\nWhere it is not necessary to be so urgent, or where indirect contempt has taken place the Attorney General can intervene and the Crown Prosecution Service will institute criminal proceedings on his behalf before a Divisional Court of the King's Bench Division of the High Court of Justice of England and Wales. In January 2012, for example, a juror who had researched information on the internet was jailed for contempt of court. Theodora Dallas, initially searching for the meaning of the term \"grievous bodily harm\", added search criteria which localised her search and brought to light another charge against the defendant. Because she then shared this information with the other jurors, the judge stated that she had compromised the defendant's right to a fair trial and the prosecution was abandoned.\nMagistrates' courts also have powers under the 1981 Act to order to detain any person who \"insults the court\" or otherwise disrupts its proceedings until the end of the sitting. Upon contempt being admitted or proved the (invariably) District Judge (sitting as a magistrate) may order committal to prison for a maximum of one month, impose a fine of up to \u00a32,500, or both.\nIt will be contempt to bring an audio recording device or picture-taking device of any sort into an English court without the consent of the court.\nIt will not be contempt according to section 10 of the Act for a journalist to refuse to disclose his sources, unless the court has considered the evidence available and determined that the information is \"necessary in the interests of justice or national security or for the prevention of disorder or crime\".\nStrict liability contempt.\nUnder the Contempt of Court Act it is criminal contempt to publish anything which creates a real risk that the course of justice in proceedings may be seriously impaired. It only applies where proceedings are active, and the Attorney General has issued guidance as to when he believes this to be the case, and there is also statutory guidance. The clause prevents the newspapers and media from publishing material that is too extreme or sensationalist about a criminal case until the trial or linked trials are over and the juries have given their verdicts.\nSection 2 of the Act defines and limits the previous common law definition of contempt (which was previously based upon a presumption that any conduct could be treated as contempt, regardless of intent), to only instances where there can be proved an intent to cause a substantial risk of serious prejudice to the administration of justice (i.e./e.g., the conduct of a trial).\nCivil contempt.\nIn civil proceedings there are two main ways in which contempt is committed:\nIndia.\nIn India, contempt of court is of two types:\nUnited States.\nIn United States jurisprudence, acts of contempt are generally divided into direct or indirect, and civil or criminal. Direct contempt occurs in the presence of a judge; civil contempt is \"coercive and remedial\" as opposed to punitive. In the United States, relevant statutes include and Federal Rule of Criminal Procedure 42.\nContempt of court in a civil suit is generally not considered to be a criminal offense, with the party benefiting from the order also holding responsibility for the enforcement of the order. However, some cases of civil contempt have been perceived as intending to harm the reputation of the plaintiff, or to a lesser degree, the judge or the court.\nSanctions for contempt may be criminal or civil. If a person is to be punished criminally, then the contempt must be proven beyond a reasonable doubt, but once the charge is proven, then punishment (such as a fine or, in more serious cases, imprisonment) is imposed unconditionally. The civil sanction for contempt (which is typically incarceration in the custody of the sheriff or similar court officer) is limited in its imposition for so long as the disobedience to the court's order continues: once the party complies with the court's order, the sanction is lifted. The imposed party is said to \"hold the keys\" to their own cell, thus conventional due process is not required. In federal and most state courts, the burden of proof for civil contempt is clear and convincing evidence, a lower standard than in criminal cases.\nIn civil contempt cases there is no principle of proportionality. In \"Chadwick v. Janecka\" (3d Cir. 2002), a U.S. court of appeals held that H. Beatty Chadwick could be held indefinitely for his failure to produce $2.5 million as a state court ordered in a civil trial. Chadwick had been imprisoned for nine years at that time and continued to be held in prison until 2009, when a state court set him free after 14 years, making his imprisonment the longest on a contempt charge to date.\nCivil contempt is only appropriate when the imposed party has the power to comply with the underlying order. Controversial contempt rulings have periodically arisen from cases involving asset protection trusts, where the court has ordered a settlor of an asset protection trust to repatriate assets so that the assets may be made available to a creditor. A court cannot maintain an order of contempt where the imposed party does not have the ability to comply with the underlying order. This claim when made by the imposed party is known as the \"impossibility defense\".\nContempt of court is considered a prerogative of the court, and \"the requirement of a jury does not apply to 'contempts committed in disobedience of any lawful writ, process, order, rule, decree, or command entered in any suit or action brought or prosecuted in the name of, or on behalf of, the United States. This stance is not universally agreed with by other areas of the legal world, and there have been many calls to have contempt cases to be tried by jury, rather than by judge, as a potential conflict of interest rising from a judge both accusing and sentencing the defendant. At least one Supreme Court justice has made calls for jury trials to replace judge trials on contempt cases.\nThe United States Marshals Service is the agency component that first holds all federal prisoners. It uses the Prisoner Population Management System /Prisoner Tracking System. The only types of records that are disclosed as being in the system are those of \"federal prisoners who are in custody pending criminal proceedings.\" The records of \"alleged civil contempors\" are not listed in the Federal Register as being in the system leading to a potential claim for damages under The Privacy Act, .\nNews media in the United States.\nIn the United States, because of the broad protections granted by the First Amendment, with extremely limited exceptions, unless the media outlet is a party to the case, a media outlet cannot be found in contempt of court for reporting about a case because a court cannot order the media in general not to report on a case or forbid it from reporting facts discovered publicly. Newspapers cannot be closed because of their content.\nCriticism.\nThere have been criticisms over the practice of trying contempt from the bench. In particular, Supreme Court Justice Hugo Black wrote in a dissent, \"It is high time, in my judgment, to wipe out root and branch the judge-invented and judge-maintained notion that judges can try criminal contempt cases without a jury.\""}
{"id": "7202", "revid": "47559963", "url": "https://en.wikipedia.org/wiki?curid=7202", "title": "Corroborating evidence", "text": "Corroborating evidence, also referred to as corroboration, is a type of evidence in lawful command.\nTypes and uses.\nCorroborating evidence tends to support a proposition that is already supported by some initial evidence, therefore confirming the proposition. For example, W, a witness, testifies that she saw X drive his automobile into a green car. Meanwhile, Y, another witness, \"corroborates\" the proposition by testifying that when he examined X's car, later that day, he noticed green paint on its fender. There can also be corroborating evidence related to a certain source, such as what makes an author think a certain way due to the evidence that was supplied by witnesses or objects.\nAnother type of corroborating evidence comes from using the Baconian method, i.e., the method of agreement, method of difference, and method of concomitant variations.\nThese methods are followed in experimental design. They were codified by Francis Bacon, and developed further by John Stuart Mill and consist of controlling several variables, in turn, to establish which variables are causally connected. These principles are widely used intuitively in various kinds of proofs, demonstrations, and investigations, in addition to being fundamental to experimental design.\nIn law, corroboration refers to the requirement in some jurisdictions, such as in Scots law, that any evidence adduced be backed up by at least one other source (see Corroboration in Scots law).\nAn example of corroboration.\nDefendant says, \"It was like what he/she (a witness) said but...\". This is Corroborative evidence from the defendant that the evidence the witness gave is true and correct.\nCorroboration is not needed in certain instances. For example, there are certain statutory exceptions. In the Education (Scotland) Act, it is only necessary to produce a register as proof of lack of attendance. No further evidence is needed.\nEngland and Wales.\nPerjury\nSee section 13 of the Perjury Act 1911.\nSpeeding offences\nSee section 89(2) of the Road Traffic Regulation Act 1984.\nSexual offences\nSee section 32 of the Criminal Justice and Public Order Act 1994.\nConfessions by mentally handicapped persons\nSee section 77 of the Police and Criminal Evidence Act 1984.\nEvidence of children\nSee section 34 of the Criminal Justice Act 1988.\nEvidence of accomplices\nSee section 32 of the Criminal Justice and Public Order Act 1994."}
{"id": "7203", "revid": "42019239", "url": "https://en.wikipedia.org/wiki?curid=7203", "title": "Cross-examination", "text": "In law, cross-examination is the interrogation of a witness by one's opponent. It is preceded by direct examination (known as examination-in-chief in Ireland, the United Kingdom, Australia, Canada, South Africa, India and Pakistan) and may be followed by a redirect (known as re-examination in the aforementioned countries). A redirect examination, performed by the attorney or pro se individual who performed the direct examination, clarifies the witness' testimony provided during cross-examination including any subject matter raised during cross-examination but not discussed during direct examination. Recross examination addresses the witness' testimony discussed in redirect by the opponent. Depending on the judge's discretion, opponents are allowed multiple opportunities to redirect and recross examine witnesses (this may vary by jurisdiction).\nVariations by jurisdiction.\nIn the United States federal courts, a cross-examining attorney is generally limited by Rule 611 of the Federal Rules of Evidence to the \"subject matter of the direct examination and matters affecting the witness's credibility\". The rule also permits the trial court, in its discretion, to \"allow inquiry into additional matters as if on direct examination\". Many state courts do permit a lawyer to cross-examine a witness on matters not raised during direct examination, though California restricts cross-examination to \"any matter within the scope of the direct examination\". Similarly, courts in England, South Africa, Australia, and Canada allow a cross-examiner to exceed the scope of direct examination.\nSince a witness called by the opposing party is presumed to be hostile, leading questions are allowed on cross-examination. A witness called by a direct examiner, on the other hand, may only be treated as hostile by that examiner after being permitted to do so by the judge, at the request of that examiner and as a result of the witness being openly antagonistic and/or prejudiced against the party that called them.\nAffecting the outcome of jury trials.\nCross-examination is a key component of a trial and the topic is given substantial attention during courses on trial advocacy. The opinions of a jury or judge are often changed if cross-examination casts doubt on the witness. On the other hand, a credible witness may reinforce the substance of their original statements and enhance the judge's or jury's belief. Though the closing argument is often considered the deciding moment of a trial, effective cross-examination wins trials.\nAttorneys anticipate hostile witnesses' responses during pretrial planning, and often attempt to shape the witnesses' perception of the questions to draw out information helpful to the attorney's case. Typically during an attorney's closing argument, they will repeat any admissions made by witnesses that favor their case. In the United States, cross-examination is seen as a core part of the entire adversarial system of justice, in that it \"is the principal means by which the believability of a witness and the truth of his testimony are tested.\" Another key component affecting a trial outcome is jury selection, in which attorneys will attempt to include jurors from whom they feel they can get a favorable response or at the least an unbiased fair decision. So while there are many factors affecting the outcome of a trial, the cross-examination of a witness will often influence an open-minded unbiased jury searching for the certainty of facts upon which to base their decision."}
{"id": "7206", "revid": "1152308", "url": "https://en.wikipedia.org/wiki?curid=7206", "title": "Christiania", "text": "Christiania may refer to:"}
{"id": "7207", "revid": "20260395", "url": "https://en.wikipedia.org/wiki?curid=7207", "title": "Charles d'Abancourt", "text": "Charles Xavier Joseph de Franque Ville d'Abancourt (4 July 17589 September 1792) was a French statesman, minister to Louis XVI.\nBiography.\nD'Abancourt was born in Douai, and was the nephew of Charles Alexandre de Calonne. He was Louis XVI's last minister of war (July 1792), and organised the defence of the Tuileries Palace during the 10 August attack. Commanded by the Legislative Assembly to send away the Swiss Guards, he refused, and was arrested for treason to the nation and sent to Orl\u00e9ans to be tried.\nAt the end of August the Assembly ordered Abancourt and the other prisoners at Orl\u00e9ans to be transferred to Paris with an escort commanded by Claude Fournier, nicknamed \"l'Americain\". At Versailles they learned of the massacres at Paris. Abancourt and his fellow-prisoners were murdered in cold blood during the 9 September massacres (9 September 1792) at Versailles. Fournier was unjustly charged with complicity in the crime."}
{"id": "7210", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7210", "title": "Cubic feet", "text": ""}
{"id": "7211", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=7211", "title": "Curtiss P-40 Warhawk", "text": "The Curtiss P-40 Warhawk is an American single-engined, single-seat, all-metal fighter-bomber that first flew in 1938. The P-40 design was a modification of the previous Curtiss P-36 Hawk which reduced development time and enabled a rapid entry into production and operational service. The Warhawk was used by most Allied powers during World War II, and remained in frontline service until the end of the war. It was the third most-produced American fighter of World War II, after the North American P-51 Mustang and Republic P-47 Thunderbolt; by November 1944, when production of the P-40 ceased, 13,738 had been built, all at Curtiss-Wright Corporation's main production facilities in Buffalo, New York.\nP-40 Warhawk was the name the United States Army Air Corps gave the plane, and after June 1941, the USAAF\nadopted the name for all models, making it the official name in the U.S. for all P-40s. The British Commonwealth and Soviet air forces used the name Tomahawk for models equivalent to the original P-40, P-40B, and P-40C, and the name Kittyhawk for models equivalent to the P-40D and all later variants. P-40s first saw combat with the British Commonwealth squadrons of the Desert Air Force in the Middle East and North African campaigns, during June 1941. No. 112 Squadron Royal Air Force, was among the first to operate Tomahawks in North Africa and the unit was the first Allied military aviation unit to feature the \"shark mouth\" logo, copying similar markings on some Luftwaffe Messerschmitt Bf 110 twin-engine fighters. \nThe lack of a two-speed supercharger for the P-40's Allison V-1710 engine's made it inferior to Luftwaffe fighters such as the Messerschmitt Bf 109 or the Focke-Wulf Fw 190 in high-altitude combat and it was rarely used in operations in Northwest Europe. However, between 1941 and 1944, the P-40 played a critical role with Allied air forces in three major theaters: North Africa, the Southwest Pacific, and China. It also had a significant role in the Middle East, Southeast Asia, Eastern Europe, Alaska and Italy. The P-40's performance at high altitudes was not as important in those theaters, where it served as an air superiority fighter, bomber escort and fighter-bomber.\nAlthough it gained a postwar reputation as a mediocre design, suitable only for close air support, more recent research including scrutiny of the records of Allied squadrons indicates that this was not the case; the P-40 performed surprisingly well as an air superiority fighter, at times suffering severe losses, but also inflicting a very heavy toll on enemy aircraft. Based on war-time victory claims, over 200 Allied fighter pilots \u2013 from the UK, Australia, New Zealand, Canada, South Africa, the US and the Soviet Union \u2013 became aces flying the P-40. These included at least 20 double aces, mostly over North Africa, China, Burma and India, the South West Pacific and Eastern Europe. The P-40 offered the additional advantages of low cost and durability, which kept it in production as a ground-attack aircraft long after it was obsolescent as a fighter.\nDesign and development.\nOrigins.\nOn 14 October 1938, Curtiss test pilot Edward Elliott flew the prototype XP-40 on its first flight in Buffalo. The XP-40 was the 10th production Curtiss P-36 Hawk, with its Pratt &amp; Whitney R-1830 Twin Wasp 14-cylinder air-cooled radial engine replaced at the direction of Chief Engineer Don R. Berlin by a liquid-cooled, supercharged Allison V-1710 V-12 engine. The first prototype placed the glycol coolant radiator in an underbelly position on the fighter, just aft of the wing's trailing edge. USAAC Fighter Projects Officer Lieutenant Benjamin S. Kelsey flew this prototype some 300 miles in 57 minutes, approximately . Hiding his disappointment, he told reporters that future versions would likely go faster. Kelsey was interested in the Allison engine because it was sturdy and dependable, and it had a smooth, predictable power curve. The V-12 engine offered as much power as a radial engine but had a smaller frontal area and allowed a more streamlined cowl than an aircraft with a radial engine, promising a theoretical 5% increase in top speed.\nCurtiss engineers worked to improve the XP-40's speed by moving the radiator forward in steps. Seeing little gain, Kelsey ordered the aircraft to be evaluated in a NACA wind tunnel to identify solutions for better aerodynamic qualities. From 28 March to 11 April 1939, the prototype was studied by NACA. Based on the data obtained, Curtiss moved the glycol coolant radiator forward to the chin; its new air scoop also accommodated the oil cooler air intake. Other improvements to the landing gear doors and the exhaust manifold combined to give performance that was satisfactory to the USAAC. Without beneficial tail winds, Kelsey flew the XP-40 from Wright Field back to Curtiss's plant in Buffalo at an average speed of . Further tests in December 1939 proved the fighter could reach .\nAn unusual production feature was a special truck rig to speed delivery at the main Curtiss plant in Buffalo, New York. The rig moved the newly built P-40s in two main components, the main wing and the fuselage, the eight miles from the plant to the airport where the two units were mated for flight and delivery.\nPerformance characteristics.\nThe P-40 was conceived as a pursuit aircraft and was agile at low and medium altitudes but suffered from a lack of power at higher altitudes. At medium and high speeds it was one of the tightest-turning early monoplane designs of the war, and it could out-turn most opponents it faced in North Africa and the Russian Front. In the Pacific Theater it was out-turned at lower speeds by the lightweight fighters Mitsubishi A6M Zero and Nakajima Ki-43 Hayabusa (known to Allies as \"Oscar\"). The American Volunteer Group Commander Claire Chennault advised against prolonged dog-fighting with the Japanese fighters due to speed reduction favoring the Japanese.\nAllison's V-1710 engines produced at sea level and . This was not powerful compared with contemporary fighters, and the early P-40 variants' top speeds were only average. The single-stage, single-speed supercharger meant that the P-40 was a poor high-altitude fighter. Later versions, with Allisons or more powerful 1,400\u00a0hp Packard Merlin engines were more capable. Climb performance was fair to poor, depending on the subtype. Dive acceleration was good and dive speed was excellent. The highest-scoring P-40 ace, Clive Caldwell (RAAF), who claimed 22 of his 28\u00bd kills in the type, said that the P-40 had \"almost no vices\", although \"it was a little difficult to control in terminal velocity\". The P-40 had one of the fastest maximum dive speeds of any fighter of the early war period, and good high-speed handling.\nThe P-40 tolerated harsh conditions and a variety of climates. Its semi-modular design was easy to maintain in the field. It lacked innovations such as boosted ailerons or automatic leading edge slats, but its strong structure included a five-spar wing, which enabled P-40s to pull high-G turns and survive some midair collisions. Intentional ramming attacks against enemy aircraft were occasionally recorded as victories by the Desert Air Force and Soviet Air Forces. Caldwell said P-40s \"would take a tremendous amount of punishment, violent aerobatics as well as enemy action\". Operational range was good by early war standards and was almost double that of the Supermarine Spitfire or Messerschmitt Bf 109, although inferior to the Mitsubishi A6M Zero, Nakajima Ki-43 and Lockheed P-38 Lightning.\nCaldwell found the P-40C Tomahawk's armament of two Browning AN/M2 \"light-barrel\" dorsal nose-mount synchronized machine guns and two Browning machine guns in each wing to be inadequate. This was improved with the P-40D (Kittyhawk I) which abandoned the synchronized gun mounts and instead had two guns in each wing, although Caldwell still preferred the earlier Tomahawk in other respects. The D had armor around the engine and the cockpit, which enabled it to withstand considerable damage. This allowed Allied pilots in Asia and the Pacific to attack Japanese fighters head on, rather than try to out-turn and out-climb their opponents. Late-model P-40s were well armored. Visibility was adequate, although hampered by a complex windscreen frame, and completely blocked to the rear in early models by a raised turtledeck. Poor ground visibility and relatively narrow landing gear track caused many losses on the ground.\nCurtiss tested a follow-on design, the Curtiss XP-46, but it offered little improvement over newer P-40 models and was cancelled.\nOperational history.\nIn April 1939, the U.S. Army Air Corps, having witnessed the new, sleek, high-speed, in-line-engined fighters of the European air forces, placed the largest fighter order it had ever made for 524 P-40s.\nFrench Air Force.\nAn early order came from the French \"Arm\u00e9e de l'Air\", which was already operating P-36s. The \"Arm\u00e9e de l'Air\" ordered 100 (later the order was increased to 230) as the Hawk 81A-1 but the French were defeated before the aircraft had left the factory and the aircraft were diverted to British and Commonwealth service (as the Tomahawk I), in some cases complete with metric flight instruments.\nIn late 1942, as French forces in North Africa split from the Vichy government to side with the Allies, U.S. forces transferred P-40Fs from 33rd FG to \"GC II/5\", a squadron that was historically associated with the Lafayette Escadrille. GC II/5 used its P-40Fs and Ls in combat in Tunisia and later for patrol duty off the Mediterranean coast until mid-1944, when they were replaced by Republic P-47D Thunderbolts.\nBritish Commonwealth.\nDeployment.\nIn all, 18 Royal Air Force (RAF) squadrons, four Royal Canadian Air Force (RCAF), three South African Air Force (SAAF) and two Royal Australian Air Force (RAAF) squadrons serving with RAF formations, used P-40s. The first units to convert were Hawker Hurricane squadrons of the Desert Air Force (DAF), in early 1941. The first Tomahawks delivered came without armor, bulletproof windscreens or self-sealing fuel tanks, which were installed in subsequent shipments. Pilots used to British fighters sometimes found it difficult to adapt to the P-40's rear-folding landing gear, which was more prone to collapse than the lateral-folding landing gear of the Hurricane or Supermarine Spitfire. In contrast to the \"three-point landing\" commonly employed with British types, P-40 pilots were obliged to use a \"wheels landing\": a longer, low angle approach that touched down on the main wheels first.\nTesting showed the aircraft did not have the performance needed for use in Northwest Europe at high-altitude, due to the service ceiling limitation. Spitfires used in the theater operated at heights around , while the P-40's Allison engine, with its single-stage, low altitude rated supercharger, worked best at or lower. When the Tomahawk was used by Allied units based in the UK from February 1941, this limitation relegated the Tomahawk to low-level reconnaissance with RAF Army Cooperation Command\nand only No. 403 Squadron RCAF was used in the fighter role for a mere 29 sorties, before being replaced by Spitfires. Air Ministry deemed the P-40 unsuitable for the theater. UK P-40 squadrons \nThe Tomahawk was superseded in North Africa by the more powerful Kittyhawk (\"D\"-mark onwards) types from early 1942, though some Tomahawks remained in service until 1943. Kittyhawks included many improvements and were the DAF's air superiority fighter for the critical first few months of 1942, until \"tropicalised\" Supermarine Spitfires were available. DAF units received nearly 330 Packard V-1650 Merlin-powered P-40Fs, called Kittyhawk IIs, most of which went to the USAAF and the majority of the 700 \"lightweight\" L models, also powered by the Packard Merlin, in which the armament was reduced to four .50\u00a0in (12.7\u00a0mm) Brownings (Kittyhawk IIA). The DAF also received some 21 of the later P-40K and the majority of the 600 P-40Ms built; these were known as Kittyhawk IIIs. The \"lightweight\" P-40Ns (Kittyhawk IV) arrived from early 1943 and were used mostly as fighter-bombers. From July 1942 until mid-1943, elements of the U.S. 57th Fighter Group (57th FG) were attached to DAF P-40 units. The British government also donated 23 P-40s to the Soviet Union.\nCombat performance.\nTomahawks and Kittyhawks bore the brunt of \"Luftwaffe\" and \"Regia Aeronautica\" fighter attacks during the North African campaign. The P-40s were considered superior to the Hurricane, which they replaced as the primary fighter of the Desert Air Force. \nThe P-40 initially proved quite effective against Axis aircraft and contributed to a slight shift of advantage in the Allies' favor. The gradual replacement of Hurricanes by the Tomahawks and Kittyhawks led to the \"Luftwaffe\" accelerating retirement of the Bf 109E and introducing the newer Bf 109F; these were to be flown by the veteran pilots of elite \"Luftwaffe\" units, such as \"Jagdgeschwader\" 27 (JG27), in North Africa. The P-40 was generally considered roughly equal or slightly superior to the Bf 109 at low altitude but inferior at high altitude, particularly against the Bf 109F. Most air combat in North Africa took place well below , negating much of the Bf 109's superiority. The P-40 usually had an advantage over the Bf 109 in turning, dive speed and structural strength, was roughly equal in firepower but was slightly inferior in speed and outclassed in rate of climb and operational ceiling.\nThe P-40 was generally superior to early Italian fighter types, such as the Fiat G.50 Freccia and the Macchi C.200. Its performance against the Macchi C.202 \"Folgore\" elicited varying opinions. Some observers consider the Macchi C.202 superior. Caldwell, who scored victories against them in his P-40, felt that the \"Folgore\" was superior to the P-40 and the Bf 109 except that its armament of only two or four machine guns was inadequate. Other observers considered the two equally matched or favored the \"Folgore\" in aerobatic performance, such as turning radius. The aviation historian Walter J. Boyne wrote that over Africa, the P-40 and the \"Folgore\" were \"equivalent\". Against its lack of high-altitude performance, the P-40 was considered to be a stable gun platform and its rugged construction meant that it was able to operate from rough front line airstrips with a good rate of serviceability.\nThe earliest victory claims by P-40 pilots include Vichy French aircraft, during the 1941 Syria-Lebanon campaign, against Dewoitine D.520s, a type often considered to be the best French fighter of the war. The P-40 was deadly against Axis bombers in the theater, as well as against the Bf 110 twin-engine fighter. In June 1941, Caldwell, of 250 Squadron in Egypt, flying as flying Officer (F/O) Jack Hamlyn's wingman, recorded in his log book that he was involved in the first air combat victory for the P-40. This was a CANT Z.1007 bomber on 6 June. The claim was not officially recognized, as the crash of the CANT was not witnessed. The first official victory occurred on 8 June, when Hamlyn and Flight Sergeant (Flt Sgt) Tom Paxton destroyed a CANT Z.1007 from \"211a Squadriglia\" of the \"Regia Aeronautica\", over Alexandria. Several days later, the Tomahawk was in action over Syria with No. 3 Squadron RAAF, which claimed 19 aerial victories over Vichy French aircraft during June and July 1941, for the loss of one P-40 (and one lost to ground fire).\nSome DAF units initially failed to use the P-40's strengths or used outdated defensive tactics such as the Lufbery circle. The superior climb rate of the Bf 109 enabled fast, swooping attacks, neutralizing the advantages offered by conventional defensive tactics. Various new formations were tried by Tomahawk units from 1941 to 1942, including \"fluid pairs\" (similar to the German \"rotte\"); the Thach Weave (one or two \"weavers\") at the back of a squadron in formation and whole squadrons bobbing and weaving in loose formations. Werner Schr\u00f6er, who was credited with destroying 114 Allied aircraft in only 197 combat missions, referred to the latter formation as \"bunches of grapes\", because he found them so easy to pick off. The leading German \"expert\" in North Africa, Hans-Joachim Marseille, claimed as many as 101 P-40s during his career.\nFrom 26 May 1942, Kittyhawk units operated primarily as fighter-bomber units, giving rise to the nickname \"Kittybomber\". As a result of this change in role and because DAF P-40 squadrons were frequently used in bomber escort and close air support missions, they suffered relatively high losses; many Desert Air Force P-40 pilots were caught flying low and slow by marauding Bf 109s.\nCaldwell believed that Operational Training Units did not properly prepare pilots for air combat in the P-40 and as a commander, stressed the importance of training novice pilots properly.\nCompetent pilots who took advantage of the P-40's strengths were effective against the best of the \"Luftwaffe\" and \"Regia Aeronautica\". In August 1941, Caldwell was attacked by two Bf 109s, one of them piloted by German ace Werner Schr\u00f6er. Although Caldwell was wounded three times and his Tomahawk was hit by more than 100 bullets and five 20 mm cannon shells, Caldwell shot down Schr\u00f6er's wingman and returned to base. Some sources also claim that in December 1941, Caldwell killed a prominent German \"Experte\", Erbo von Kageneck (69 kills), while flying a P-40. Caldwell's victories in North Africa included 10 Bf 109s and two Macchi C.202s. Billy Drake of 112 Squadron was the leading British P-40 ace with 13 victories. James \"Stocky\" Edwards (RCAF), who achieved 12 kills in the P-40 in North Africa, shot down German ace Otto Schulz (51 kills) while flying a Kittyhawk with No. 260 Squadron RAF. Caldwell, Drake, Edwards and Nicky Barr were among at least a dozen pilots who achieved ace status twice over while flying the P-40. A total of 46 British Commonwealth pilots became aces in P-40s, including seven double aces.\nChinese Air Force.\nFlying Tigers (American Volunteer Group).\nThe Flying Tigers, known officially as the 1st American Volunteer Group (AVG), were a unit of the Chinese Air Force, recruited from amongst U.S. Navy, Marine Corps and Army aviators and ground crew.\nAVG leader Claire Chennault received crated Model Bs which his airmen assembled in Burma at the end of 1941, adding self-sealing fuel tanks and a second pair of wing guns, such that the aircraft became a hybrid of B and C models. These were not well-liked by their pilots: they lacked drop tanks for extra range, and there were no bomb racks on the wings. Chennault considered the liquid-cooled engine vulnerable in combat because a single bullet through the coolant system would cause the engine to overheat in minutes. The Tomahawks also had no radios, so the AVG improvised by installing a fragile radio transceiver, the RCA-7-H, which had been built for a Piper Cub. Because the plane had a single-stage low-altitude supercharger, its effective ceiling was about . The most critical problem was the lack of spare parts; the only source was from damaged aircraft. The planes were viewed as cast-offs that no one else wanted, dangerous and difficult to fly. But the pilots did appreciate some of the planes' features. There were two heavy sheets of steel behind the pilot's head and back that offered solid protection, and overall the planes were ruggedly constructed.\nCompared to opposing Japanese fighters, the P-40B's strengths were that it was sturdy, well armed, faster in a dive and possessed an excellent rate of roll. While the P-40s could not match the maneuverability of the Japanese Army air arm's Nakajima Ki-27s and Ki-43s, nor the much more famous Zero naval fighter in slow, turning dogfights, at higher speeds the P-40s were more than a match. Chennault trained his pilots to use the P-40's particular performance advantages. The P-40 had a higher dive speed than any Japanese fighter aircraft of the early war years, for example, and could exploit so-called \"boom-and-zoom\" tactics. The AVG was highly successful, and its feats were widely publicized by an active cadre of international journalists to boost sagging public morale at home. According to its official records, in just months, the Flying Tigers destroyed 297 enemy aircraft for the loss of just four of its own in air-to-air combat.\nIn the spring of 1942, the AVG received a small number of Model E's. Each came equipped with a radio, six .50-caliber machine guns, and auxiliary bomb racks that could hold 35-lb fragmentation bombs. Chennault's armorer added bomb racks for 570-lb Russian bombs, which the Chinese had in abundance. These planes were used in the battle of the Salween River Gorge in late May 1942, which kept the Japanese from entering China from Burma and threatening Kunming. Spare parts, however, remained in short supply. \"Scores of new planes...were now in India, and there they stayed\u2014in case the Japanese decided to invade... the AVG was lucky to get a few tires and spark plugs with which to carry on its daily war.\"\n4th Air Group.\nChina received 27 P-40E models in early 1943. These were assigned to squadrons of the 4th Air Group.\nUnited States Army Air Forces.\nA total of 15 USAAF pursuit/fighter groups (FG), along with other pursuit/fighter squadrons and a few tactical reconnaissance (TR) units, operated the P-40 during 1941\u201345. As was also the case with the Bell P-39 Airacobra, many USAAF officers considered the P-40 exceptional but it was gradually replaced by the Lockheed P-38 Lightning, the Republic P-47 Thunderbolt and the North American P-51 Mustang. The bulk of the fighter operations by the USAAF in 1942\u201343 were borne by the P-40 and the P-39. In the Pacific, these two fighters, along with the U.S. Navy Grumman F4F Wildcat, contributed more than any other U.S. types to breaking Japanese air power during this critical period.\nPacific theaters.\nThe P-40 was the main USAAF fighter aircraft in the South West Pacific and Pacific Ocean theaters during 1941\u201342. At Pearl Harbor and in the Philippines, USAAF P-40 squadrons suffered crippling losses on the ground and in the air to Japanese fighters such as the A6M Zero and Ki-43 Hayabusa respectively. During the attack on Pearl Harbor, most of the USAAF fighters were P-40Bs, the majority of which were destroyed. However, a few P-40s managed to get in the air and shoot down several Japanese aircraft, most notably by George Welch and Kenneth Taylor.\nIn the Dutch East Indies campaign, the 17th Pursuit Squadron (Provisional), formed from USAAF pilots evacuated from the Philippines, claimed 49 Japanese aircraft destroyed, for the loss of 17 P-40s The seaplane tender USS \"Langley\" was sunk by Japanese airplanes while delivering P-40s to Tjilatjap, Java. In the Solomon Islands and New Guinea Campaigns and the air defence of Australia, improved tactics and training allowed the USAAF to better use the strengths of the P-40. Due to aircraft fatigue, scarcity of spare parts and replacement problems, the US Fifth Air Force and Royal Australian Air Force created a joint P-40 management and replacement pool on 30 July 1942 and many P-40s went back and forth between the air forces.\nThe 49th Fighter Group was in action in the Pacific from the beginning of the war. Robert M. DeHaven scored 10 kills (of 14 overall) in the P-40 with the 49th FG. He compared the P-40 favorably with the P-38:\nThe 8th, 15th, 18th, 24th, 49th, 343rd and 347th PGs/FGs, flew P-40s in the Pacific theaters between 1941 and 1945, with most units converting to P-38s from 1943 to 1944. In 1945, the 71st Reconnaissance Group employed them as armed forward air controllers during ground operations in the Philippines, until it received delivery of P-51s. They claimed 655 aerial victories.\nContrary to conventional wisdom, with sufficient altitude, the P-40 could turn with the A6M and other Japanese fighters, using a combination of a nose-down vertical turn with a bank turn, a technique known as a low yo-yo. Robert DeHaven describes how this tactic was used in the 49th Fighter group:\nChina Burma India Theater.\nUSAAF and Chinese P-40 pilots performed well in this theater against many Japanese types such as the Ki-43, Nakajima Ki-44 \"Tojo\" and the Zero. The P-40 remained in use in the China Burma India Theater (CBI) until 1944 and was reportedly preferred over the P-51 Mustang by some US pilots flying in China. The American Volunteer Group (Flying Tigers) was integrated into the USAAF as the 23rd Fighter Group in June 1942. The unit continued to fly newer model P-40s until 1944, achieving a high kill-to-loss ratio.\nIn the Battle of the Salween River Gorge of May 1942 the AVG used the P-40E model equipped with wing racks that could carry six 35-pound fragmentation bombs and Chennault's armorer developed belly racks to carry Russian 570-pound bombs, which the Chinese had in large quantity.\nUnits arriving in the CBI after the AVG in the 10th and 14th Air Forces continued to perform well with the P-40, claiming 973 kills in the theater, or 64.8 percent of all enemy aircraft shot down. Aviation historian Carl Molesworth stated that \"...the P-40 simply dominated the skies over Burma and China. They were able to establish air superiority over free China, northern Burma and the Assam valley of India in 1942, and they never relinquished it.\" The 3rd, 5th, 23rd, 51st and 80th FGs, along with the 10th TRS, operated the P-40 in the CBI. CBI P-40 pilots used the aircraft very effectively as a fighter-bomber. The 80th Fighter Group in particular used its so-called \"B-40\" (P-40s carrying 1,000-pound high-explosive bombs) to destroy bridges and kill bridge repair crews, sometimes demolishing their target with one bomb. At least 40 U.S. pilots reached ace status while flying the P-40 in the CBI.\nEurope and Mediterranean theaters.\nOn 14 August 1942, the first confirmed victory by a USAAF unit over a German aircraft in World War II was initiated by a P-40C pilot. 2nd Lt Joseph D. Shaffer, of the 33rd Fighter Squadron, intercepted a Focke-Wulf Fw 200C-3 maritime patrol aircraft that overflew his base at Reykjav\u00edk, Iceland. Shaffer damaged the Fw 200, which was finished off by a P-38F. Warhawks were used extensively in the Mediterranean and Middle East theatre of World War II by USAAF units, including the 33rd, 57th, 58th, 79th, 324th and 325th Fighter Groups. While the P-40 suffered heavy losses in the MTO, many USAAF P-40 units achieved high kill-to-loss ratios against Axis aircraft; the 324th FG scored better than a 2:1 ratio in the MTO. In all, 23 U.S. pilots became aces in the MTO on the P-40, most of them during the first half of 1943.\nP-40 pilots from the 57th FG were the first USAAF fliers to see action in the MTO, while attached to Desert Air Force Kittyhawk squadrons, from July 1942. The 57th was also the main unit involved in the \"Palm Sunday Massacre\", on 18 April 1943. Decoded Ultra signals revealed a plan for a large formation of Junkers Ju 52 transports to cross the Mediterranean, escorted by German and Italian fighters. Between 1630 and 1830 hours, all wings of the group were engaged in an intensive effort against the enemy air transports. Of the four Kittyhawk wings, three had left the patrol area before a convoy of a 100+ enemy transports were sighted by 57th FG, which tallied 74 aircraft destroyed. The group was last in the area, and intercepted the Ju 52s escorted by large numbers of Bf 109s, Bf 110s and Macchi C.202s. The group claimed 58 Ju 52s, 14 Bf 109s and two Bf 110s destroyed, with several probables and damaged. Between 20 and 40 of the Axis aircraft landed on the beaches around Cap Bon to avoid being shot down; six Allied fighters were lost, five of them P-40s.\nOn 22 April, in Operation Flax, a similar force of P-40s attacked a formation of 14 Messerschmitt Me 323 \"Gigant\" (\"Giant\") six-engine transports, covered by seven Bf 109s from II./JG 27. All the transports were shot down, for a loss of three P-40s. The 57th FG was equipped with the Curtiss fighter until early 1944, during which time they were credited with at least 140 air-to-air kills. On 23 February 1943, during Operation Torch, the pilots of the 58th FG flew 75 P-40Ls off the aircraft carrier to the newly captured Vichy French airfield, Cazas, near Casablanca, in French Morocco. The aircraft supplied the 33rd FG and the pilots were reassigned.\nThe 325th FG (known as the \"Checkertail Clan\") flew P-40s in the MTO and was credited with at least 133 air-to-air kills from April\u2013October 1943, of which 95 were Bf 109s and 26 were Macchi C.202s, for the loss of 17 P-40s in combat. The 325th FG historian Carol Cathcart wrote:\nCathcart wrote that Lt. Robert Sederberg assisted a comrade being attacked by five Bf 109s, destroyed at least one German aircraft, and may have shot down as many as five. Sederberg was shot down and became a prisoner of war.\nA famous African-American unit, the 99th FS, better known as the \"Tuskegee Airmen\" or \"Redtails\", flew P-40s in stateside training and for their initial eight months in the MTO. On 9 June 1943, they became the first African-American fighter pilots to engage enemy aircraft, over Pantelleria, Italy. A single Focke-Wulf Fw 190 was reported damaged by Lieutenant Willie Ashley Jr. On 2 July the squadron claimed its first verified kill; a Fw 190 destroyed by Captain Charles Hall. The 99th continued to score with P-40s until February 1944, when they were assigned P-39s and P-51 Mustangs.\nThe much-lightened P-40L was most heavily used in the MTO, primarily by U.S. pilots. Many US pilots stripped down their P-40s even further to improve performance, often removing two or more of the wing guns to improve the roll rate.\nRoyal Australian Air Force.\nThe Kittyhawk was the main fighter used by the RAAF in World War II, in greater numbers than the Spitfire. Two RAAF squadrons serving with the Desert Air Force, No. 3 and No. 450 Squadrons, were the first Australian units to be assigned P-40s. Other RAAF pilots served with RAF or SAAF P-40 squadrons in the theater.\nMany RAAF pilots achieved high scores in the P-40. At least five reached \"double ace\" status: Clive Caldwell, Nicky Barr, John Waddy, Bob Whittle (11 kills each) and Bobby Gibbes (10 kills) in the Middle East, North African and/or New Guinea campaigns. In all, 18 RAAF pilots became aces while flying P-40s.\nNicky Barr, like many Australian pilots, considered the P-40 a reliable mount: \"The Kittyhawk became, to me, a friend. It was quite capable of getting you out of trouble more often than not. It was a real warhorse.\"\nAt the same time as the heaviest fighting in North Africa, the Pacific War was also in its early stages, and RAAF units in Australia were completely lacking in suitable fighter aircraft. Spitfire production was being absorbed by the war in Europe; P-38s were trialled, but were difficult to obtain; Mustangs had not yet reached squadrons anywhere, and Australia's tiny and inexperienced aircraft industry was geared towards larger aircraft. USAAF P-40s and their pilots originally intended for the U.S. Far East Air Force in the Philippines, but diverted to Australia as a result of Japanese naval activity were the first suitable fighter aircraft to arrive in substantial numbers. By mid-1942, the RAAF was able to obtain some USAAF replacement shipments.\nRAAF Kittyhawks played a crucial role in the South West Pacific theater. They fought on the front line as fighters during the critical early years of the Pacific War, and the durability and bomb-carrying abilities (1,000\u00a0lb/454\u00a0kg) of the P-40 also made it ideal for the ground attack role. During the Battle of Port Moresby RAAF 75 destroyed or damaged some 33 Japanese aircraft of various types, with another 30 probables. General Henry H. Arnold said of No 75 squadron: \"Victory in the entire air war against Japan can be traced back to the actions which took place from that dusty strip at Port Moresby in early 1942.\" For example, 75, and 76 Squadrons played a critical role during the Battle of Milne Bay, fending off Japanese aircraft and providing effective close air support for the Australian infantry, negating the initial Japanese advantage in light tanks and sea power. The Kittyhawks fired \"nearly 200,000 rounds of half-inch ammunition\" during the course of the battle.\nThe RAAF units that most used Kittyhawks in the South West Pacific were 75, 76, 77, 78, 80, 82, 84 and 86 Squadrons. These squadrons saw action mostly in the New Guinea and Borneo campaigns.\nLate in 1945, RAAF fighter squadrons in the South West Pacific began converting to P-51Ds. However, Kittyhawks were in use with the RAAF until the end of the war, in Borneo. In all, the RAAF acquired 841 Kittyhawks (not counting the British-ordered examples used in North Africa), including 163 P-40E, 42 P-40K, 90 P-40 M and 553 P-40N models. In addition, the RAAF ordered 67 Kittyhawks for use by No. 120 (Netherlands East Indies) Squadron (a joint Australian-Dutch unit in the South West Pacific). The P-40 was retired by the RAAF in 1947.\nRoyal Canadian Air Force.\nA total of 13 Royal Canadian Air Force units operated the P-40 in the North West European or Alaskan theaters.\nIn mid-May 1940, Canadian and US officers watched comparative tests of a XP-40 and a Spitfire, at RCAF Uplands, Ottawa. While the Spitfire was considered to have performed better, it was not available for use in Canada and the P-40 was ordered to meet home air defense requirements. In all, eight Home War Establishment Squadrons were equipped with the Kittyhawk: 72 Kittyhawk I, 12 Kittyhawk Ia, 15 Kittyhawk III and 35 Kittyhawk IV aircraft, for a total of 134 aircraft. These aircraft were mostly diverted from RAF Lend-Lease orders for service in Canada. The P-40 Kittyhawks were obtained in lieu of 144 P-39 Airacobras originally allocated to Canada but reassigned to the RAF.\nHowever, before any home units received the P-40, three RCAF Article XV squadrons operated Tomahawk aircraft from bases in the United Kingdom. No. 403 Squadron RCAF, a fighter unit, used the Tomahawk Mk II briefly before converting to Spitfires. Two Army Co-operation (close air support) squadrons: 400 and 414 Sqns trained with Tomahawks, before converting to Mustang Mk. I aircraft and a fighter/reconnaissance role. Of these, only No. 400 Squadron used Tomahawks operationally, conducting a number of armed sweeps over France in the late 1941. RCAF pilots also flew Tomahawks or Kittyhawks with other British Commonwealth units based in North Africa, the Mediterranean, South East Asia and (in at least one case) the South West Pacific.\nIn 1942, the Imperial Japanese Navy occupied two islands, Attu and Kiska, in the Aleutians, off Alaska. RCAF home defense P-40 squadrons saw combat over the Aleutians, assisting the USAAF. The RCAF initially sent 111 Squadron, flying the Kittyhawk I, to the US base on Adak island. During the drawn-out campaign, 12 Canadian Kittyhawks operated on a rotational basis from a new, more advanced base on Amchitka, southeast of Kiska. 14 and 111 Sqns took \"turn-about\" at the base. During a major attack on Japanese positions at Kiska on 25 September 1942, Squadron Leader Ken Boomer shot down a Nakajima A6M2-N (\"Rufe\") seaplane. The RCAF also purchased 12 P-40Ks directly from the USAAF while in the Aleutians. After the Japanese threat diminished, these two RCAF squadrons returned to Canada and eventually transferred to England without their Kittyhawks.\nIn January 1943, a further Article XV unit, 430 Squadron was formed at RAF Hartford Bridge, England and trained on obsolete Tomahawk IIA. The squadron converted to the Mustang I before commencing operations in mid-1943.\nIn early 1945 pilots from No. 133 Squadron RCAF, operating the P-40N out of RCAF Patricia Bay, (Victoria, British Columbia), intercepted and destroyed two Japanese balloon-bombs, which were designed to cause wildfires on the North American mainland. On 21 February, Pilot Officer E. E. Maxwell shot down a balloon, which landed on Sumas Mountain in Washington State. On 10 March, Pilot Officer J. 0. Patten destroyed a balloon near Saltspring Island, British Columbia. The last interception took place on 20 April 1945 when Pilot Officer P.V. Brodeur from 135 Squadron out of Abbotsford, British Columbia shot down a balloon over Vedder Mountain.\nThe RCAF units that operated P-40s were, in order of conversion: \nRoyal New Zealand Air Force.\nSome Royal New Zealand Air Force (RNZAF) pilots and New Zealanders in other air forces flew British P-40s while serving with DAF squadrons in North Africa and Italy, including the ace Jerry Westenra.\nA total of 301 P-40s were allocated to the RNZAF under Lend-Lease, for use in the Pacific Theater, although four of these were lost in transit. The aircraft equipped 14 Squadron, 15 Squadron, 16 Squadron, 17 Squadron, 18 Squadron, 19 Squadron and 20 Squadron.\nRNZAF P-40 squadrons were successful in air combat against the Japanese between 1942 and 1944. Their pilots claimed 100 aerial victories in P-40s, whilst losing 20 aircraft in combat Geoff Fisken, the highest scoring British Commonwealth ace in the Pacific, flew P-40s with 15 Squadron, although half of his victories were claimed with the Brewster Buffalo.\nThe overwhelming majority of RNZAF P-40 victories were scored against Japanese fighters, mostly Zeroes. Other victories included Aichi D3A \"Val\" dive bombers. The only confirmed twin engine claim, a Ki-21 \"Sally\" (misidentified as a G4M \"Betty\") fell to Fisken in July 1943.\nFrom late 1943 and 1944, RNZAF P-40s were increasingly used against ground targets, including the innovative use of naval depth charges as improvised high-capacity bombs. The last front line RNZAF P-40s were replaced by Vought F4U Corsairs in 1944. The P-40s were relegated to use as advanced pilot trainers.\nThe remaining RNZAF P-40s, excluding the 20 shot down and 154 written off, were mostly scrapped at Rukuhia in 1948.\nSoviet Union.\nThe Soviet Air Forces and Soviet Naval Aviation also referred to P-40s as \"Tomahawks\" and \"Kittyhawks\". In fact, the Curtiss P-40 Tomahawk / Kittyhawk was the first Allied fighter supplied to the USSR under the Lend-Lease agreement.\nThe USSR received 247 P-40B/Cs (equivalent to the Tomahawk IIA/B in RAF service) and 2,178 P-40E, -K, -L, and -N models between 1941 and 1944. The Tomahawks were shipped from Great Britain and directly from the US, many of them arriving incomplete, lacking machine guns and even the lower half of the engine cowling. In late September 1941, the first 48 P-40s were assembled and checked in the USSR. Test flights showed some manufacturing defects: generator and oil pump gears and generator shafts failed repeatedly, which led to emergency landings. The test report indicated that the Tomahawk was inferior to Soviet \"M-105P-powered production fighters in speed and rate of climb. However, it had good short field performance, horizontal maneuverability, range, and endurance.\" Nevertheless, Tomahawks and Kittyhawks were used against the Germans. The 126th Fighter Aviation Regiment (IAP), fighting on the Western and Kalinin Fronts, were the first unit to receive the P-40. The regiment entered action on 12 October 1941. By 15 November 1941, the regiment had shot down 17 German aircraft. However, Lt (SG) Smirnov noted that the P-40 armament was sufficient for strafing enemy lines but rather ineffective in aerial combat. Another pilot, Stephan Ridny (a Hero of the Soviet Union), remarked that he had to shoot half the ammunition at 50\u2013100 meters (165\u2013340\u00a0ft) to shoot down an enemy aircraft.\nIn January 1942, some 198 aircraft sorties were flown (334 flying hours) and 11 aerial engagements were conducted, in which five Bf 109s, one Ju 88, and one He 111 were downed. These statistics reveal a surprising fact: it turns out that the Tomahawk was fully capable of successful air combat with a Bf 109. The reports of pilots about the circumstances of the engagements confirm this fact. On 18 January 1942, Lieutenants S. V. Levin and I. P. Levsha (in pair) fought an engagement with seven Bf 109s and shot down two of them without loss. On 22 January, a flight of three aircraft led by Lieutenant E. E. Lozov engaged 13 enemy aircraft and shot down two Bf 109Es, again without loss. Altogether, in January, two Tomahawks were lost; one downed by German anti-aircraft artillery and one lost to Messerschmitts.\nThe Soviets stripped down their P-40s significantly for combat, in many cases removing the wing guns altogether in P-40B/C types, for example. Soviet Air Force reports state that they liked the range and fuel capacity of the P-40, which were superior to most of the Soviet fighters, though they still preferred the P-39. Soviet pilot Nikolai G. Golodnikov recalled: \"The cockpit was vast and high. At first it felt unpleasant to sit waist-high in glass, as the edge of the fuselage was almost at waist level. But the bullet-proof glass and armored seat were strong and visibility was good. The radio was also good. It was powerful, reliable, but only on HF (high frequency). The American radios did not have hand microphones but throat microphones. These were good throat mikes: small, light and comfortable.\" The biggest complaint of some Soviet airmen was its poor climb rate and problems with maintenance, especially with burning out the engines. VVS pilots usually flew the P-40 at War Emergency Power settings while in combat, which brought acceleration and speed performance closer to that of their German rivals, but could burn out engines in a matter of weeks. Tires and batteries also failed. The fluid in the engine's radiators often froze, cracking their cores, which made the Allison engine unsuitable for operations during harsh winter conditions. During the winter of 1941, the 126th Fighter Aviation Regiment suffered from cracked radiators on 38 occasions. Often, entire regiments were reduced to a single flyable aircraft because no replacement parts were available. They also had difficulty with the more demanding requirements for fuel and oil quality of the Allison engines. A fair number of burned-out P-40s were re-engined with Soviet Klimov M-105 engines, but these performed relatively poorly and were relegated to rear area use.\nThe P-40 saw the most front line use in Soviet hands in 1942 and early 1943. Deliveries over the Alaska-Siberia ALSIB ferry route began in October 1942. It was used in the northern sectors and played a significant role in the defense of Leningrad. The most numerically important types were P-40B/C, P-40E and P-40K/M. By the time the better P-40F and N types became available, production of superior Soviet fighters had increased sufficiently so that the P-40 was replaced in most Soviet Air Force units by the Lavochkin La-5 and various later Yakovlev types. In spring 1943, Lt D.I. Koval of the 45th IAP gained ace status on the North Caucasian front, shooting down six German aircraft flying a P-40. Some Soviet P-40 squadrons had good combat records. Some Soviet pilots became aces on the P-40, though not as many as on the P-39 Airacobra, the most numerous Lend-Lease fighter used by the Soviet Union. However, Soviet commanders thought the Kittyhawk significantly outclassed the Hurricane, although it was \"not in the same league as the Yak-1\".\nJapan.\nThe Japanese Army captured some P-40s and later operated a number in Burma. The Japanese appear to have had as many as 10 flyable P-40Es. For a brief period in 1943, a few of them were used operationally by 2 \"Hiko Chutai\", 50 \"Hiko Sentai\" (2nd Air Squadron, 50th Air Regiment) in the defense of Rangoon. Testimony of this is given by Yasuhiko Kuroe, a member of the 64 \"Hiko Sentai\". In his memoirs, he says one Japanese-operated P-40 was shot down in error by a friendly Mitsubishi Ki-21 \"Sally\" over Rangoon.\nOther nations.\nThe P-40 was used by over two dozen countries during and after the war. The P-40 was used by Brazil, Egypt, Finland and Turkey. The last P-40s in military service, used by the Brazilian Air Force (FAB), were retired in 1954.\nIn the air war over Finland, several Soviet P-40s were shot down or had to crash-land due to other reasons. The Finns, short of good aircraft, collected these and managed to repair one P-40M, P-40M-10-CU 43\u20135925, \"white 23\", which received Finnish Air Force serial number KH-51 (KH denoting \"Kittyhawk\", as the British designation of this type was Kittyhawk III). This aircraft was attached to an operational squadron HLeLv 32 of the Finnish Air Force, but lack of spares kept it on the ground, with the exception of a few evaluation flights.\nSeveral P-40Ns were used by the Royal Netherlands East Indies Army Air Force with No. 120 (Netherlands East Indies) Squadron RAAF against the Japanese before being used during the fighting in Indonesia until February 1949.\nVariants and development stages.\nThis new liquid-cooled engine fighter had a radiator mounted under the rear fuselage\nbut the prototype XP-40 was later modified and the radiator was moved forward under the engine.\nSurviving aircraft.\nOf the 13,738 P-40s built, only 28 remain airworthy, with three of them being converted to dual-controls/dual-seat configuration. Approximately 13 aircraft are on static display and another 36 airframes are under restoration for either display or flight."}
{"id": "7212", "revid": "7704042", "url": "https://en.wikipedia.org/wiki?curid=7212", "title": "Creed", "text": "A creed, also known as a confession of faith, a symbol, or a statement of faith, is a statement of the shared beliefs of a community (often a religious community) which summarize its core tenets.\nMany Christian denominations use three creeds: the Niceno-Constantinopolitan Creed, the Apostles' Creed and the Athanasian Creed. Some Christian denominations do not use any of those creeds. \nThe term \"creed\" is sometimes extended to comparable concepts in non-Christian theologies. The Islamic concept of \"\u02bfaq\u012bdah\" (literally \"bond, tie\") is often rendered as \"creed\".\nHistory.\nThe earliest known creed in Christianity, \"Jesus is Lord\", originated in the writings of Paul the Apostle. One of the most significant and widely used Christian creeds is the Nicene Creed, first formulated in AD 325 at the First Council of Nicaea to affirm the deity of Christ and revised at the First Council of Constantinople in AD 381 to affirm the trinity as a whole. The creed was further affirmed in 431 by the Chalcedonian Definition, which clarified the doctrine of Christ. Affirmation of this creed, which describes the Trinity, is often taken as a fundamental test of orthodoxy by many Christian denominations, and was historically purposed against Arianism. The Apostles' Creed, another early creed which concisely details the trinity, virgin birth, crucifixion, and resurrection, is most popular within western Christianity, and is widely used in Christian church services.\nIn Islamic theology, the term most closely corresponding to \"creed\" is \"\u02bfaq\u012bdah\" ().\nTerminology.\nThe word \"creed\" is particularly used for a concise statement which is recited as part of liturgy. The term is anglicized from Latin \"credo\" \"I believe\", the incipit of the Latin texts of the Apostles' Creed and the Nicene Creed. A creed is sometimes referred to as a \"symbol\" in a specialized meaning of that word (which was first introduced to Late Middle English in this sense), after Latin \"symbolum\" \"creed\" (as in \"Symbolum Apostolorum\" = the \"Apostles' Creed\", a shorter version of the traditional Nicene Creed), after Greek \"symbolon\" \"token, watchword\".\nSome longer statements of faith in the Protestant tradition are instead called \"confessions of faith\", or simply \"confession\" (as in e.g. Helvetic Confession). Within Evangelical Protestantism, the terms \"doctrinal statement\" or \"doctrinal basis\" tend to be preferred. Doctrinal statements may include positions on lectionary and translations of the Bible, particularly in fundamentalist churches of the King James Only movement.\nChristianity.\nThe first confession of faith established within Christianity was the Nicene Creed by the Early Church in 325. It was established to summarize the foundations of the Christian faith and to protect believers from false doctrines. Various Christian denominations from Protestantism and Evangelical Christianity have published confession of faith as a basis for fellowship among churches of the same denomination.\nMany Christian denominations did not try to be too exhaustive in their confessions of faith and thus allow different opinions on some secondary topics. In addition, some churches are open to revising their confession of faith when necessary. Moreover, Baptist \"confessions of faith\" have often had a clause such as this from the First London Baptist Confession (Revised edition, 1646):\nExcommunication.\nExcommunication is a practice of the Bible to exclude members who do not respect the Church's confession of faith and do not want to repent. It is practiced by most Christian denominations and is intended to protect against the consequences of heretics' teachings and apostasy.\nChristians without creeds.\nSome Christian denominations do not profess a creed. This stance is often referred to as \"non-creedalism\".\nAnabaptism, with its origins in the 16th century Radical Reformation, spawned a number of sects and denominations that espouse \"No creed, but the Bible/New Testament\". This was a common reason for Anabaptist persecution from Catholic and Protestant believers. Anabaptist groups that exist today include the Amish, Hutterites, Mennonites, Schwarzenau Brethren (Church of the Brethren), River Brethren, Bruderhof, and the Apostolic Christian Church. \nThe Religious Society of Friends, the group known as the Quakers, was founded in the 17th century and is similarly non-creedal. They believe that such formal structures, \u201cbe they written words, steeple-houses or a clerical hierarchy,\u201d cannot take the place of communal relationships and a shared connection with God.\nSimilar reservations about the use of creeds can be found in the Restoration Movement and its descendants, the Christian Church (Disciples of Christ), the Churches of Christ, and the Christian churches and churches of Christ. Restorationists profess \"no creed but Christ\".\nThe Seventh-day Adventist Church also shares this sentiment.\nJehovah's Witnesses contrast \"memorizing or repeating creeds\" with acting to \"do what Jesus said\".\nChristian creeds.\nSeveral creeds originated in Christianity.\nChristian confessions of faith.\nProtestant denominations are usually associated with confessions of faith, which are similar to creeds but usually longer.\nControversies.\nIn the Swiss Reformed Churches, there was a quarrel about the Apostles' Creed in the mid-19th century. As a result, most cantonal reformed churches stopped prescribing any particular creed.\nIn 2005, Bishop John Shelby Spong, retired Episcopal Bishop of Newark, has written that dogmas and creeds were merely \"a stage in our development\" and \"part of our religious childhood.\" In his book, \"Sins of the Scripture\", Spong wrote that \"Jesus seemed to understand that no one can finally fit the holy God into his or her creeds or doctrines. That is idolatry.\"\nSimilar concepts in other religions.\nThe Church of Jesus Christ of Latter-day Saints.\nWithin the sects of the Latter Day Saint movement, the \"Articles of Faith\" are contained in a list which was composed by Joseph Smith as part of an 1842 letter which he sent to \"Long\" John Wentworth, editor of the \"Chicago Democrat\". It is canonized along with the King James Version of the Bible, the \"Book of Mormon\", the \"Doctrine &amp; Covenants\" and the \"Pearl of Great Price\", as a part of the standard works of the Church of Jesus Christ of Latter-day Saints.\nIslamic \"aq\u012bdah\".\nIn Islamic theology, the term most closely corresponding to \"creed\" is \"\u02bfaq\u012bdah\" (). The first such creed was written as \"a short answer to the pressing heresies of the time\" is known as \"Al-Fiqh Al-Akbar\" and ascribed to Ab\u016b \u1e24an\u012bfa. Two well known creeds were the \"Fiqh Akbar II\" \"representative\" of the al-Ash'ari, and \"Fiqh Akbar III\", \"representative\" of the Ash-Shafi'i.\n\"Iman\" () in Islamic theology denotes a believer's religious faith. Its most simple definition is the belief in the six articles of faith, known as \"ark\u0101n al-\u012bm\u0101n\".\nJewish \"Shema Yisrael\".\nRabbi Milton Steinberg wrote that \"By its nature Judaism is averse to formal creeds which of necessity limit and restrain thought\" and asserted in his book \"Basic Judaism\" (1947) that \"Judaism has never arrived at a creed.\" The 1976 Centenary Platform of the Central Conference of American Rabbis, an organization of Reform rabbis, agrees that \"Judaism emphasizes action rather than creed as the primary expression of a religious life.\"\nStill, the opening lines of the prayer Shema Yisrael can be read as a creedal statement of strict monotheism: \"Hear O Israel, the Lord is our God, the Lord is One\" (; transliterated \"Shema Yisrael Adonai Eloheinu Adonai Echad\").\nA notable statement of Jewish principles of faith was drawn up by Maimonides as his 13 Principles of Faith.\nReligions without creeds.\nFollowing a debate that lasted more than twenty years, the National Conference of the American Unitarian Association passed a resolution in 1894 that established the denomination as non-creedal. The Unitarians later merged with the Universalist Church of America to form the Unitarian Universalist Association (UUA). Instead of a creed, the UUA abides by a set of principles, such as \u201ca free and responsible search for truth and meaning\u201d. It cites diverse sources of inspiration, including Christianity, Judaism, Humanism, and Earth-centered traditions."}
{"id": "7213", "revid": "2308770", "url": "https://en.wikipedia.org/wiki?curid=7213", "title": "Claudius Aelianus", "text": "Claudius Aelianus (, Greek transliteration \"Kl\u00e1udios Ailian\u00f3s\"; ), commonly Aelian (), born at Praeneste, was a Roman author and teacher of rhetoric who flourished under Septimius Severus and probably outlived Elagabalus, who died in 222. He spoke Greek so fluently that he was called \"honey-tongued\" ( ); Roman-born, he preferred Greek authors, and wrote in a slightly archaizing Greek himself.\nHis two chief works are valuable for the numerous quotations from the works of earlier authors, which are otherwise lost, and for the surprising lore, which offers unexpected glimpses into the Greco-Roman world-view. \"De Natura Animalium\" is also the only Greco-Roman work to mention Gilgamesh.\n\"De Natura Animalium\".\n\"On the Nature of Animals\" (alternatively \"On the Characteristics of Animals\"; , \"\"; usually cited by its Latin title \"De Natura Animalium\") is a collection, in seventeen books, of brief stories of natural history. Some are included for the moral lessons they convey; others because they are astonishing.\nThe Loeb Classical Library introduction characterizes the book as \"an appealing collection of facts and fables about the animal kingdom that invites the reader to ponder contrasts between human and animal behavior\".\nAelian's anecdotes on animals rarely depend on direct observation: they are almost entirely taken from written sources, not only Pliny the Elder, Theopompus, and Lycus of Rhegium, but also other authors and works now lost, to whom he is thus a valuable witness. He is more attentive to marine life than might be expected, though, and this seems to reflect first-hand personal interest; he often quotes \"fishermen\". At times he strikes the modern reader as thoroughly credulous, but at others he specifically states that he is merely reporting what is told by others, and even that he does not believe them. Aelian's work is one of the sources of medieval natural history and of the bestiaries of the Middle Ages.\nThe surviving portions of the text are badly mangled and garbled and replete with later interpolations. Conrad Gessner (or Gesner), the Swiss scientist and natural historian of the Renaissance, made a Latin translation of Aelian's work, to give it a wider European audience. An English translation by A. F. Scholfield has been published in the Loeb Classical Library, 3 vols. (1958-59).\n\"Varia Historia\".\n\"Various History\" (, \"\")\u2014for the most part preserved only in an abridged form\u2014is Aelian's other well-known work, a miscellany of anecdotes and biographical sketches, lists, pithy maxims, and descriptions of natural wonders and strange local customs, in 14\u00a0books, with many surprises for the cultural historian and the mythographer, anecdotes about the famous Greek philosophers, poets, historians, and playwrights and myths instructively retold. The emphasis is on \"various\" moralizing tales about heroes and rulers, athletes and wise men; reports about food and drink, different styles in dress or lovers, local habits in giving gifts or entertainments, or in religious beliefs and death customs; and comments on Greek painting. Aelian gives accounts of, among other things, fly fishing using lures of red wool and feathers, lacquerwork, and serpent worship. Essentially, the \"Various History\" is a classical \"magazine\" in the original sense of that word. He is not perfectly trustworthy in details, and his writing was heavily influenced by Stoic opinions, perhaps so that his readers will not feel guilty, but Jane Ellen Harrison found survivals of archaic rites mentioned by Aelian very illuminating in her \"Prolegomena to the Study of Greek Religion\" (1903, 1922).\n\"Varia Historia\" was first printed in 1545. The standard modern text is that of Mervin R. Dilts (1974).\nTwo English translations of the \"Various History,\" by Fleming (1576) and Stanley (1665) made Aelian's miscellany available to English readers, but after 1665 no English translation appeared, until three English translations appeared almost simultaneously: James G. DeVoto, \"Claudius Aelianus: \u03a0\u03bf\u03b9\u03ba\u03af\u03bb\u03b7\u03c2 \u1f39\u03c3\u03c4\u03bf\u03c1\u03af\u03b1\u03c2 (\"Varia Historia\")\" Chicago, 1995; Diane Ostrom Johnson, \"An English Translation of Claudius Aelianus' \"Varia Historia\"\", 1997; and N. G. Wilson, \"Aelian: Historical Miscellany\" in the Loeb Classical Library.\nOther works.\nConsiderable fragments of two other works, \"On Providence\" and \"Divine Manifestations\", are preserved in the early medieval encyclopedia, the \"Suda.\" Twenty \"letters from a farmer\" after the manner of Alciphron are also attributed to him. The letters are invented compositions to a fictitious correspondent, which are a device for vignettes of agricultural and rural life, set in Attica, though mellifluous Aelian once boasted that he had never been outside Italy, never been aboard a ship (which is at variance, though, with his own statement, \"de Natura Animalium\" XI.40, that he had seen the bull Serapis with his own eyes). Thus conclusions about actual agriculture in the \"Letters\" are as likely to evoke Latium as Attica. The fragments have been edited in 1998 by D. Domingo-Foraste, but are not available in English. The \"Letters\" are available in the Loeb Classical Library, translated by Allen Rogers Benner and Francis H. Fobes (1949)."}
{"id": "7214", "revid": "3492060", "url": "https://en.wikipedia.org/wiki?curid=7214", "title": "Callisto (mythology)", "text": "In Greek mythology, Callisto (; ) was a nymph, or the daughter of King Lycaon; the myth varies in such details. She was believed to be one of the followers of Artemis (Diana for the Romans) who attracted Zeus. Many versions of Callisto's story survive. According to some writers, Zeus transformed himself into the figure of Artemis to pursue Callisto, and she slept with him believing Zeus to be Artemis. She became pregnant and when this was eventually discovered, she was expelled from Artemis's group, after which a furious Hera, the wife of Zeus, transformed her into a bear, although in some versions, Artemis is the one to give her an ursine form. Later, just as she was about to be killed by her son when he was hunting, she was set among the stars as Ursa Major (\"the Great Bear\") by Zeus. She was the bear-mother of the Arcadians, through her son Arcas by Zeus.\nIn other accounts, the birth mother of Arcas was called Megisto, daughter of Ceteus, son of Lycaon, or else Themisto, daughter of Inachus.\nThe fourth Galilean moon of Jupiter and a main belt asteroid are named after Callisto.\nMythology.\nAs a follower of Artemis, Callisto, who Hesiod said was the daughter of Lycaon, king of Arcadia, took a vow to remain a virgin, as did all the nymphs of Artemis.\nAccording to Hesiod, she was seduced by Zeus, and of the consequences that followed:\n[Callisto] chose to occupy herself with wild-beasts in the mountains together with Artemis, and, when she was seduced by Zeus, continued some time undetected by the goddess, but afterwards, when she was already with child, was seen by her bathing and so discovered. Upon this, the goddess was enraged and changed her into a beast. Thus she became a bear and gave birth to a son called Arcas. But while she was in the mountains, she was hunted by some goat-herds and given up with her babe to Lycaon. Some while after, she thought fit to go into the forbidden precinct of Zeus, not knowing the law, and being pursued by her own son and the Arcadians, was about to be killed because of the said law; but Zeus delivered her because of her connection with him and put her among the stars, giving her the name Bear because of the misfortune which had befallen her.\nEratosthenes also mentions a variation in which the virginal companion of Artemis that was seduced by Zeus and eventually transformed into the constellation Ursa Minor was named Phoenice instead.\nAccording to Ovid, it was Jupiter who took the form of Diana so that he might evade his wife Juno's detection, forcing himself upon Callisto while she was separated from Diana and the other nymphs. Callisto recognized that something was wrong the moment Jupiter started giving her \"non-virginal kisses\", but by that point it was too late, and even though she fought him off, he overpowered her. The real Diana arrived in the scene soon after and called Callisto to her, only for the girl to run away in fear she was Jupiter, until she noticed the nymphs accompanying the goddess. Callisto's subsequent pregnancy was discovered several months later while she was bathing with Diana and her fellow nymphs. Diana became enraged when she saw that Callisto was pregnant and expelled her from the group. Callisto later gave birth to Arcas. Juno then took the opportunity to avenge her wounded pride and transformed the nymph into a bear. Sixteen years later Callisto, still a bear, encountered her son Arcas hunting in the forest. Just as Arcas was about to kill his own mother with his javelin, Jupiter averted the tragedy by placing mother and son amongst the stars as Ursa Major and Minor, respectively. Juno, enraged that her attempt at revenge had been frustrated, appealed to Tethys that the two might never meet her waters, thus providing a poetic explanation for the constellations' circumpolar positions in ancient times.\nAccording to Hyginus, the origin of the transformation of Zeus, with its lesbian overtones, was from a rendition of the tale in a comedy in a lost work by the Attic comedian Amphis where Zeus embraced Callisto as Artemis and she, after being questioned by Artemis for her pregnancy, blamed the goddess, thinking she had impregnated her; Artemis then changed her into a bear. She was caught by some Aetolians and brought to Lycaon, her father. Still a bear, she rushed with her son Arcas into a temple of Zeus as the Arcadians followed to kill them; Zeus turned mother and son into constellations. Hyginus also records a version where Hera changed Callisto for sleeping with Zeus, and Artemis later slew her while hunting, not recognizing her. In another of the versions Hyginus records, it was Zeus who turned Callisto into a bear, to conceal her from Juno, who had noticed what her husband was doing. Juno then pointed Callisto to Diana, who proceeded to shoot her with her arrows.\nAccording to the mythographer Apollodorus, Zeus forced himself on Callisto when he disguised himself as Artemis or Apollo, in order to lure the sworn maiden into his embrace. Apollodorus is the only author to mention Apollo, but implies that it is not a rarity. Callisto was then turned into a bear by Zeus trying to hide her from Hera, but Hera asked Artemis to shoot the animal, and Artemis complied. Zeus then took the child, named it Arcas, and gave it to Maia to bring up in Arcadia; and Callisto he turned into a star and called it the Bear. Alternatively, Artemis killed Callisto for not protecting her virginity. Nonnus also writes that a \"female paramour entered a woman's bed.\"\nEither Artemis \"slew Kallisto with a shot of her silver bow,\" according to Homer, in order to please Juno (Hera) as Pausanias and Pseudo-Apollodorus write or later Arcas, the eponym of Arcadia, nearly killed his bear-mother, when she had wandered into the forbidden precinct of Zeus. In every case, Zeus placed them both in the sky as the constellations Ursa Major, called \"Arktos\" (), the Bear, by Greeks, and Ursa Minor.\nAccording to John Tzetzes, Charon of Lampsacus wrote that Callisto's son Arcas had been fathered not by Zeus but rather by Apollo.\nAs a constellation, Ursa Major (who was also known as Helice, from an alternative origin story of the constellation) told Demeter, when the goddess asked the stars whether they knew anything about her daughter Persephone's abduction, to ask Helios the sun god, for he knew the deeds of the day well, while the night was blameless.\nOrigin of the myth.\nThe name \"Kalliste\" (), \"most beautiful\", may be recognized as an epithet of the goddess herself, though none of the inscriptions at Athens that record priests of \"Artemis Kalliste\" (), date before the third century BCE. Artemis Kalliste was worshiped in Athens in a shrine which lay outside the Dipylon gate, by the side of the road to the Academy. W. S. Ferguson suggested that Artemis Soteira and Artemis Kalliste were joined in a common cult administered by a single priest. The bearlike character of Artemis herself was a feature of the Brauronia. It has been suggested that the myths of Artemis' nymphs breaking their vows were originally about Artemis herself, before her characterization shifted to that of a sworn virgin who fiercely defends her chastity.\nThe myth in \"Catasterismi\" may be derived from the fact that a set of constellations appear close together in the sky, in and near the Zodiac sign of Libra, namely Ursa Minor, Ursa Major, Bo\u00f6tes, and Virgo. The constellation Bo\u00f6tes, was explicitly identified in the Hesiodic \"Astronomia\" () as Arcas, the \"Bear-warden\" (\"Arktophylax\"; ): He is Arkas the son of Kallisto and Zeus, and he lived in the country about Lykaion. After Zeus had seduced Kallisto, Lykaon, pretending not to know of the matter, entertained Zeus, as Hesiod says, and set before him on the table the babe [Arkas] which he had cut up.\nThe stars of Ursa Major were all circumpolar in Athens of 400 BCE, and all but the stars in the Great Bear's left foot were circumpolar in Ovid's Rome, in the first century CE. Now, however, due to the precession of the equinoxes, the feet of the Great Bear constellation do sink below the horizon from Rome and especially from Athens; however, Ursa Minor (Arcas) does remain completely above the horizon, even from latitudes as far south as Honolulu and Hong Kong.\nAccording to Julien d'Huy, who used phylogenetic and statistical tools, the story could be a recent transformation of a Palaeolithic myth.\nIn art.\nCallisto's story was sometimes depicted in classical art, where the moment of transformation into a bear was the most popular. From the Renaissance on a series of major history paintings as well as many smaller cabinet paintings and book illustrations, usually called \"Diana and Callisto\", depicted the traumatic moment of discovery of the pregnancy, as the goddess and her nymphs bathed in a pool, following Ovid's account. The subject's attraction was undoubtedly mainly the opportunity it offered for a group of several females to be shown largely nude.\nTitian's \"Diana and Callisto\" (1556\u20131559), was the greatest (though not the first) of these, quickly disseminated by a print by Cornelius Cort. Here, as in most subsequent depictions, Diana points angrily, as Callisto is held by two nymphs, who may be pulling off what little clothing remains on her. Other versions include one by Rubens, and \"Diana Bathing with her Nymphs with Actaeon and Callisto\" by Rembrandt, which unusually combines the moment with the arrival of Actaeon. The basic composition is rather unusually consistent. Carlo Ridolfi said there was a version by Giorgione, who died in 1510, though his many attributions to Giorgione of paintings that are now lost are treated with suspicion by scholars. Other, less dramatic, treatments before Titian established his composition are by Palma Vecchio and Dosso Dossi. Annibale Carracci's \"The Loves of the Gods\" includes an image of Juno urging Diana to shoot Callisto in ursine form.\nAlthough Ovid places the discovery in the ninth month of Callisto's pregnancy, in paintings she is generally shown with a rather modest bump for late pregnancy. With the \"Visitation\" in religious art, this was the leading recurring subject in history painting that required showing pregnancy in art, which Early Modern painters still approached with some caution. In any case, the narrative required that the rest of the group had not previously noticed the pregnancy.\nCallisto being seduced by Zeus/Jupiter in disguise was also a popular subject, usually called \"Jupiter and Callisto\"; it was the clearest common subject with lesbian lovers from classical mythology. The two lovers are usually shown happily embracing in a bower. The violence described by Ovid as following Callisto's realization of what is going on is rarely shown. In versions before about 1700 Callisto may show some doubt about what is going on, as in the versions by Rubens. It was especially popular in the 18th century, when depictions were increasingly erotic; Fran\u00e7ois Boucher painted several versions.\nDuring the Nazi occupation of France, resistance poet Robert Desnos wrote a collection of poems entitled \"Calixto suivi de contr\u00e9e,\" where he used the myth of Callisto as a symbol for beauty imprisoned beneath ugliness: a metaphor for France under the German occupation.\nAeschylus' tragedy \"Callisto\" is lost. However, Callisto rejoined the dramatic tradition in the Baroque period when Francesco Cavalli composed La Calisto in 1651."}
{"id": "7216", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7216", "title": "Centromer", "text": ""}
{"id": "7218", "revid": "42102409", "url": "https://en.wikipedia.org/wiki?curid=7218", "title": "Cookie", "text": "A cookie (American English) or biscuit (British English) is a baked snack or dessert that is typically small, flat, and sweet. It usually contains flour, sugar, egg, and some type of oil, fat, or butter. It may include other ingredients such as raisins, oats, chocolate chips, or nuts.\nMost English-speaking countries call crunchy cookies \"biscuits\", except for the United States and Canada, where \"biscuit\" refers to a type of quick bread. Chewier biscuits are sometimes called \"cookies,\u201d even in the Commonwealth. Some cookies may also be named by their shape, such as date squares or bars.\nBiscuit or cookie variants include sandwich biscuits, such as custard creams, Jammie Dodgers, Bourbons, and Oreos, with marshmallows or jam filling and sometimes dipped in chocolate or another sweet coating. Cookies are often served with beverages such as milk, coffee, or tea and sometimes dunked, an approach which releases more flavour from confections by dissolving the sugars, while also softening their texture. Factory-made cookies are sold in grocery stores, convenience stores, and vending machines. Fresh-baked cookies are sold at bakeries and coffeehouses.\nTerminology.\nIn many English-speaking countries outside North America, including the United Kingdom, the most common word for a crisp cookie is \"biscuit\". The term \"cookie\" is normally used to describe chewier ones. However, in many regions both terms are used. The container used to store cookies may be called a cookie jar.\nIn Scotland, the term \"cookie\" is sometimes used to describe a plain bun.\nCookies that are baked as a solid layer on a sheet pan and then cut, rather than being baked as individual pieces, are called bar cookies in American English or traybakes in British English.\nEtymology.\nThe word \"cookie\" dates from at least 1701 in Scottish usage where the word meant \"plain bun\", rather than thin baked good, and so it is not certain whether it is the same word. From 1808, the word \"cookie\" is attested \"...in the sense of \"small, flat, sweet cake\" in American English. The American use is derived from Dutch \"little cake\", which is a diminutive of \"\" (\"cake\"), which came from the Middle Dutch word \"\" with an informal, dialect variant . According to the Scottish National Dictionary, its Scottish name may derive from the diminutive form (+ suffix \"-ie\") of the word \"cook\", giving the Middle Scots \"cookie\", \"cooky\" or \"cu(c)kie\". There was much trade and cultural contact across the North Sea between the Low Countries and Scotland during the Middle Ages, which can also be seen in the history of curling and, perhaps, golf.\nDescription.\nCookies are most commonly baked until crisp or else for just long enough to ensure a soft interior. Other types of cookies are not baked at all, such as varieties of peanut butter cookies that use solidified chocolate rather than set eggs and wheat gluten as a binder. Cookies are produced in a wide variety of styles, using an array of ingredients including sugars, spices, chocolate, butter, peanut butter, nuts, or dried fruits.\nA general theory of cookies may be formulated in the following way. Despite its descent from cakes and other sweetened breads, the cookie in almost all its forms has abandoned water as a medium for cohesion. Water in cakes serves to make the batter as thin as possible, the better to allow bubbles\u2014responsible for a cake's fluffiness\u2014to form. In the cookie the agent of cohesion has become some form of oil. Oils, whether in the form of butter, vegetable oils, or lard, are much more viscous than water and evaporate freely at a far higher temperature. Thus a cake made with butter or eggs in place of water is much denser after removal from the oven.\nRather than evaporating as water does in a baking cake, oils in cookies remain. These oils saturate the cavities created during baking by bubbles of escaping gases. These gases are primarily composed of steam vaporized from the egg whites and the carbon dioxide released by heating the baking powder. This saturation produces the most texturally attractive feature of the cookie, and indeed all fried foods: crispness saturated with a moisture (namely oil) that does not render soggy the food it has soaked into.\nHistory.\nCookie-like hard wafers have existed for as long as baking has been documented, in part because they survive travel very well, but they were usually not sweet enough to be considered cookies by modern standards.\nCookies appear to have their origins in 7th century AD Persia, shortly after the use of sugar became relatively common in the region. They spread to Europe through the Muslim conquest of Spain. By the 14th century, they were common in all levels of society throughout Europe, from royal cuisine to street vendors. The first documented instance of the figure-shaped gingerbread man was at the court of Elizabeth I of England in the 16th century. She had the gingerbread figures made and presented in the likeness of some of her important guests.\nWith global travel becoming widespread at that time, cookies made a natural travel companion, a modernized equivalent of the travel cakes used throughout history. One of the most popular early cookies, which traveled especially well and became known on every continent by similar names, was the jumble, a relatively hard cookie made largely from nuts, sweetener, and water.\nCookies came to America through the Dutch in New Amsterdam in the late 1620s. The Dutch word \"\" was Anglicized to \"cookie\" or cooky. The earliest reference to cookies in America is in 1703, when \"The Dutch in New York provided...'in 1703...at a funeral 800 cookies...\nThe modern form of cookies, which is based on creaming butter and sugar together, did not appear commonly until the 18th century. The Industrial Revolution in Britain and the consumers it created saw cookies (biscuits) become products for the masses, and firms such as Huntley &amp; Palmers (formed in 1822), McVitie's (formed in 1830) and Carr's (formed in 1831) were all established. The decorative biscuit tin, invented by Huntley &amp; Palmers in 1831, saw British cookies exported around the world. In 1891, Cadbury filed a patent for a chocolate-coated cookie.\nClassification.\nCookies are broadly classified according to how they are formed or made, including at least these categories:\nOther types of cookies are classified for other reasons, such as their ingredients, size, or intended time of serving:\nReception.\nLeah Ettman from Nutrition Action has criticized the high-calorie count and fat content of supersized cookies, which are extra large cookies; she cites the Panera Kitchen Sink Cookie, a supersized chocolate chip cookie, which measures inches in diameter and has 800 calories. For busy people who eat breakfast cookies in the morning, Kate Bratskeir from the \"Huffington Post\" recommends lower-sugar cookies filled with \"heart-healthy nuts and fiber-rich oats\". A book on nutrition by Paul Insel et al. notes that \"low-fat\" or \"diet cookies\" may have the same number of calories as regular cookies, due to added sugar.\nIn popular culture.\nThere are a number of slang usages of the term \"cookie\". The slang use of \"cookie\" to mean a person, \"especially an attractive woman\" is attested to in print since 1920. The catchphrase \"that's the way the cookie crumbles\", which means \"that's just the way things happen\" is attested to in print in 1955. Other slang terms include \"smart cookie\" and \"tough cookie.\" According to \"The Cambridge International Dictionary of Idioms\", a smart cookie is \"someone who is clever and good at dealing with difficult situations.\" The word \"cookie\" has been vulgar slang for \"vagina\" in the US since 1970. The word \"cookies\" is used to refer to the contents of the stomach, often in reference to vomiting (e.g., \"pop your cookies\", a 1960s expression, or \"toss your cookies\", a 1970s expression). The expression \"cookie cutter\", in addition to referring literally to a culinary device used to cut rolled cookie dough into shapes, is also used metaphorically to refer to items or things \"having the same configuration or look as many others\" (e.g., a \"cookie cutter tract house\") or to label something as \"stereotyped or formulaic\" (e.g., an action movie filled with \"generic cookie cutter characters\").\n\"Cookie duster\" is a whimsical expression for a mustache.\nCookie Monster is a Muppet on the children's television show \"Sesame Street.\" He is best known for his voracious appetite for cookies and his famous eating phrases, such as \"Me want cookie!\", \"Me eat cookie!\" (or simply \"COOKIE!\"), and \"Om nom nom nom\" (said through a mouth full of food).\nCookie Clicker is a game where players click a cookie to buy upgrades to make more cookies."}
{"id": "7220", "revid": "1272374372", "url": "https://en.wikipedia.org/wiki?curid=7220", "title": "Common Gateway Interface", "text": "In computing, Common Gateway Interface (CGI) is an interface specification that enables web servers to execute an external program to process HTTP or HTTPS user requests.\nSuch programs are often written in a scripting language and are commonly referred to as \"CGI scripts\", but they may include compiled programs.\nA typical use case occurs when a web user submits a web form on a web page that uses CGI. The form's data is sent to the web server within a HTTP request with a URL denoting a CGI script. The web server then launches the CGI script in a new computer process, passing the form data to it. The CGI script passes its output, usually in the form of HTML, to the Web server, and the server relays it back to the browser as its response to the browser's request.\nDeveloped in the early 1990s, CGI was the earliest common method available that allowed a web page to be interactive. Due to a necessity to run CGI scripts in a separate process every time the request comes in from a client, various alternatives were developed.\nHistory.\nIn 1993, the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list. The other Web server developers adopted it, and it has been a standard for Web servers ever since. A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined. This work resulted in RFC 3875, which specified CGI Version 1.1. Specifically mentioned in the RFC are the following contributors:\nHistorically CGI programs were often written using the C programming language. RFC 3875 \"The Common Gateway Interface (CGI)\" partially defines CGI using C, in saying that environment variables \"are accessed by the C library routine getenv() or variable environ\".\nThe name CGI comes from the early days of the Web, where \"webmasters\" wanted to connect legacy information systems such as databases to their Web servers. The CGI program was executed by the server and provided a common \"gateway\" between the Web server and the legacy information system.\nPurpose.\nTraditionally a Web server has a directory which is designated as a document collection, that is, a set of files that can be sent to Web browsers connected to the server. For example, if a web server has the fully-qualified domain name codice_1, and its document collection is stored at codice_2 in the local file system (its \"document root\"), then the web server will respond to a request for codice_3 by sending to the browser a copy of the file codice_4 (if it exists).\nFor pages constructed on the fly, the server software may defer requests to separate programs and relay the results to the requesting client (usually, a Web browser that displays the page to the end user).\nSuch programs usually require some additional information to be specified with the request, such as query strings or cookies. Conversely, upon returning, the script must provide all the information required by HTTP for a response to the request: the HTTP status of the request, the document content (if available), the document type (e.g. HTML, PDF, or plain text), et cetera.\nInitially, there were no standardized methods for data exchange between a browser, the HTTP server with which it was communicating and the scripts on the server that were expected to process the data and ultimately return a result to the browser. As a result, mutual incompatibilities existed between different HTTP server variants that undermined script portability.\nRecognition of this problem led to the specification of how data exchange was to be carried out, resulting in the development of CGI. Web page-generating programs invoked by server software that adheres to the CGI specification are known as \"CGI scripts\", even though they may actually have been written in a non-scripting language, such as C.\nThe CGI specification was quickly adopted and continues to be supported by all well-known HTTP server packages, such as Apache, Microsoft IIS, and (with an extension) Node.js-based servers.\nAn early use of CGI scripts was to process forms. In the beginning of HTML, HTML forms typically had an \"action\" attribute and a button designated as the \"submit\" button. When the submit button is pushed the URI specified in the \"action\" attribute would be sent to the server with the data from the form sent as a query string. If the \"action\" specifies a CGI script then the CGI script would be executed, the script in turn generating an HTML page.\nDeployment.\nA Web server that supports CGI can be configured to interpret a URL that it serves as a reference to a CGI script. A common convention is to have a codice_5 directory at the base of the directory tree and treat all executable files within this directory (and no other, for security) as CGI scripts. When a Web browser requests a URL that points to a file within the CGI directory (e.g., codice_6), then, instead of simply sending that file (codice_7) to the Web browser, the HTTP server runs the specified script and passes the output of the script to the Web browser. That is, anything that the script sends to standard output is passed to the Web client instead of being shown in the terminal window that started the web server. Another popular convention is to use filename extensions; for instance, if CGI scripts are consistently given the extension codice_8, the Web server can be configured to interpret all such files as CGI scripts. While convenient, and required by many prepackaged scripts, it opens the server to attack if a remote user can upload executable code with the proper extension.\nThe CGI specification defines how additional information passed with the request is passed to the script. The Web server creates a subset of the environment variables passed to it and adds details pertinent to the HTTP environment. For instance, if a slash and additional directory name(s) are appended to the URL immediately after the name of the script (in this example, codice_9), then that path is stored in the codice_10 environment variable before the script is called. If parameters are sent to the script via an HTTP GET request (a question mark appended to the URL, followed by param=value pairs; in the example, codice_11), then those parameters are stored in the codice_12 environment variable before the script is called. Request HTTP message body, such as form parameters sent via an HTTP POST request, are passed to the script's standard input. The script can then read these environment variables or data from standard input and adapt to the Web browser's request.\nUses.\nCGI is often used to process input information from the user and produce the appropriate output. An example of a CGI program is one implementing a wiki. If the user agent requests the name of an entry, the Web server executes the CGI program. The CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result. The Web server receives the output from the CGI program and transmits it to the user agent. Then if the user agent clicks the \"Edit page\" button, the CGI program populates an HTML codice_13 or other editing control with the page's contents. Finally if the user agent clicks the \"Publish page\" button, the CGI program transforms the updated HTML into the source of that entry's page and saves it.\nSecurity.\nCGI programs run, by default, in the security context of the Web server. When first introduced a number of example scripts were provided with the reference distributions of the NCSA, Apache and CERN Web servers to show how shell scripts or C programs could be coded to make use of the new CGI. One such example script was a CGI program called PHF that implemented a simple phone book.\nIn common with a number of other scripts at the time, this script made use of a function: codice_14. The function was supposed to sanitize its argument, which came from user input and then pass the input to the Unix shell, to be run in the security context of the Web server. The script did not correctly sanitize all input and allowed new lines to be passed to the shell, which effectively allowed multiple commands to be run. The results of these commands were then displayed on the Web server. If the security context of the Web server allowed it, malicious commands could be executed by attackers.\nThis was the first widespread example of a new type of Web-based attack called code injection, where unsanitized data from Web users could lead to execution of code on a Web server. Because the example code was installed by default, attacks were widespread and led to a number of security advisories in early 1996.\nAlternatives.\nFor each incoming HTTP request, a Web server creates a new CGI process for handling it and destroys the CGI process after the HTTP request has been handled. Creating and destroying a process can consume more CPU time and memory resources than the actual work of generating the output of the process, especially when the CGI program still needs to be interpreted by a virtual machine. For a high number of HTTP requests, the resulting workload can quickly overwhelm the Web server.\nThe computational overhead involved in CGI process creation and destruction can be reduced by the following techniques:\nThe optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these trade-offs need to be analyzed to determine the best implementation for a given task and time budget. Web frameworks offer an alternative to using CGI scripts to interact with user agents."}
{"id": "7222", "revid": "1269583556", "url": "https://en.wikipedia.org/wiki?curid=7222", "title": "Choctaw", "text": "The Choctaw ( ) are a Native American people originally based in the Southeastern Woodlands, in what is now Mississippi and Alabama. The Choctaw language is a Western Muskogean language. Today, Choctaw people are enrolled in four federally recognized tribes: the Choctaw Nation of Oklahoma, Mississippi Band of Choctaw Indians, Jena Band of Choctaw Indians in Louisiana, and the Yowani Choctaws enrolled under the confederacy of the Caddo Nation. Choctaw descendants are also members of state-recognized tribes.\nEtymology.\nThe Choctaw autonym is Chahta. The proper noun \"Choctaw\" is an anglization of \"Chahta.\" According to Anthropologist John R. Swanton, the Choctaw derived their name from an early leader of the Choctaw people. Swanton's report was taken directly from the Choctaw people as they recounted a story of their early history regarding a journey to seek a new homeland. On this journey, the ancestral group of people divided into two groups over different interpretations of a totem the people were consulting on how they might proceed on their journey. A leader named \"Chahta\" proposes that the totem indicates they should proceed to the north while another leader, Chahta's brother \"Chiksa'\", proposes the indication is to proceed to the east. The people chose which leader they would follow which split them into two groups. Those who followed Chahta became the Chahta or Choctaw people and those who followed Chiksa' became the Chiksa' or Chickasaw people. Henry Halbert, a historian, suggests that the name is derived from the Choctaw phrase \"Hacha hatak\" (river people). This view has little support.\nOrigins.\nOne Choctaw origin story relates how in pre-historic times the Choctaw people lived in areas near or around what is now known as the Yucatan Peninsula. This story explains that when the ancient South American homeland of the Choctaw people became overcrowded many of them sailed across the sea to the land of present-day Alabama and Mississippi in North America. (Caitlin, Letters and Notes, 1841) A Choctaw origin in South America is supported by DNA evidence as well as the similarities between the beliefs, art, and customs of the people of the Southeastern Ceremonial Complex in North America and the Indigenous peoples of South America. The ancestral people of the Choctaw and other indigenous peoples in North America have participated in the evolution of their respective North American cultures for hundreds and even thousands of years. However, the Choctaw people as they are known today are believed to have coalesced during the 16th century. The original peoples involved in this coalescence likely formed in Alabama and were made up of populations such as the Plaquemine culture. Prior to their arrival in the Americas the founding populations of both North and South American indigenous peoples are generally explained to be Beringian populations broken down into several groups which are illuminated within the fields of Archeology, Anthropology, and Genomics. Technological advances have provided breakthroughs in the genetic history of the Indigenous peoples of North and South America. Until modern times the connection between North and South American indigenous people was unknown in modern scholarship.\nLanguage.\nThe Choctaw language is a member of the Muskogean language family. The Choctaw language was well known among the American frontiersmen of the early 19th century. In 1870, a Christian Missionary and fluent Choctaw speaker Cyrus Byington published a Choctaw Dictionary \"Grammar of the Choctaw Language.\" Revised additions include contributions from American historian Henry S. Halbert, who was also a fluent Choctaw speaker, and Anthropologist John R. Swanton.\nChoctaw or Chahta, as it is called in the native language, is closely related to the Chickasaw language. Some linguists consider Choctaw and Chickasaw to be dialects of a singular original language. This idea is supported by Choctaw and Chickasaw origin stories which both state that the Choctaw and Chickasaw people arose out of a singular ancestral people.\nThe Choctaw language is at the heart of Choctaw tribal culture, tradition, and identity. The Choctaw Nation of Oklahoma currently offers courses in the Choctaw language. Choctaw is regularly spoken as part of daily life on the Mississippi Choctaw reservation. Although Choctaw had begun to diminish in the 20th century it remains a living language and in recent years has shown a resurgence among the people of the Choctaw Nation of Oklahoma, the Mississippi Band of Choctaw Indians, the Jena Band of Choctaw Indians, and the Yowani Choctaws.\nOrthography.\nThe written Choctaw language is based upon the English version of the Roman alphabet and was developed in conjunction with the \"civilization program\" of the United States in the early 19th century. Byington's alphabet and a version modified by John Swanton is seen here.\nByington/Swanton (Linguistic).\nThe following table is an example of Choctaw text and its translation:\nTraditional religion.\nThe traditional Choctaw belief system evolved out of the North American Southeastern Ceremonial Complex. The Choctaw believed in a good spirit and an evil spirit. They may have been sun, or \"Hvshtahli\", worshippers. The historian John Swanton wrote,\nThe word \"nanpisa\" (the one who sees) expressed the reverence the Choctaw had for the sun.\nChoctaw prophets were known to have addressed the sun. John Swanton wrote, \"an old Choctaw informed Wright that before the arrival of the missionaries, they had no conception of prayer. He added, \"I have indeed heard it asserted by some, that anciently their hopaii, or prophets, on some occasions were accustomed to address the sun ...\"\nTraditional culture.\nChoctaw culture as it's understood today has its historical roots going back to the 16th century. Prior to this period what is known of the Choctaw culture comes from oral traditions and the obvious participation of the Choctaw people in the wider Southeastern Ceremonial Complex. From at least the 16th century until the present-day a definable Choctaw culture has been expressed through rich traditions of song, dance, dress, beading, pottery, basketry, and stickball. Choctaw people maintain their ancient traditions in their personal and daily lives as well as participating in community events. One example is the mid-summer Choctaw Indian Fair hosted by the Mississippi Band of Choctaw Indians. This event hosts Choctaw people from all over world and includes hospitality and events such as cooking, entertainment, dancing, and stickball. The Choctaw culture is an ancient culture that continues to thrive within the nations and communities of the Choctaw Nation of Oklahoma in Oklahoma, the Mississippi Band of Choctaw Indians in Mississippi, the Jena Band of Choctaw Indians in Louisiana, and the Yowani Choctaws in Mississippi, Texas, Louisiana, and in Oklahoma as part of the Caddo Confederacy.\nTraditional tribal structure.\nThe traditional Choctaw tribal structure prioritized two distinct moieties: \"Imoklashas\" (elders) and \"Inhulalatas\" (youth). Each moiety had several iksas or clans and in rare cases a totemic clan. Identity for the Choctaw people was established first by moiety and second as part of the individuals iksa. The Choctaw people existed in a matrilineal kinship system, with children born into the iksa of their mother and the mother's iksa conferring her children's social status. Another tradition of this maternally oriented system was the role of the maternal uncle as an important figure in the lives of his sister's children. Maternal uncles acted as fathers and caretakers to the children of their sisters. The Choctaw people's adoration of woman and the Mother goddess was also reflected in their religious and spiritual reverence for the sacred mound of Nanih Waiya which is known as the \"Mother Mound.\" Nanih Waiya is a great earthwork platform mound located in central-east Mississippi. This site remains a place of female pilgrimage for prayer, song, and dance to this day.\nEarly American writings record some of the names of the historical Choctaw iksas. Anthropologist John R. Swanton made his contribution through his 1931 book \"Source material for the social and ceremonial life of the Choctaw Indians\". The main iksas holding significant sway over all others at the time of his writings were the \"Okla Falaya\" meaning \"Long People\", the eastern \"Okla Tannap\" meaning \"People on the Other Side\", and the southern \"Okla Hannali\" meaning \"Six Towns People.\" Swanton reported from both personal contact and previous scholarship in his writings. The names of the known iksas when Swanton's aforementioned book was published are as follows:\nAfter the U.S. government had broken several treaties with the Choctaw people, and eventually when the Choctaw were forcibly removed from their traditional lands in Mississippi during the American tragedy of the Trail of Tears, the Choctaw reestablished themselves in Indian Territory according to the three most powerful districts in their lost homeland. The Choctaw named these three districts after the leading chiefs from each of those districts. Moshulatubbee was the name given for the district of the Okla Tannap, Apuckshunubbee was given for Okla Falaya, and Pushmataha was given for Okla Hannali.\nTraditional communal economy.\nEarly Choctaw communities worked communally and shared their harvest. They had trouble understanding why English settlers allowed their poor to suffer from hunger. In Ireland, the generosity of the Choctaw nation during their Great Famine in the mid-nineteenth century is remembered to this day and recently marked by a sculpture, 'Kindred Spirits', in a park at Midleton, Cork.\nTraditional building structures.\nBoth, the Chickasaw and the Choctaw Indians traditionally made three kinds of buildings, per family, consisting of 1) a summer house (made into an oblong square), 2) a corn house (also made into an oblong square), and 3) a winter house, which latter was made circular, and was also known as the 'hot house'.\nTraditional clothing.\nThe colorful dresses worn by today's Choctaw are made by hand. They are based on designs of their ancestors, who adapted 19th-century European-American styles to their needs. Today many Choctaw wear such traditional clothing mainly for special events. Choctaw elders, especially the women, dress in their traditional garb every day. Choctaw dresses are trimmed by full diamond, half diamond or circle, and crosses that represent stickball sticks.\nTraditional games.\nChoctaw stickball, the oldest field sport in North America, was also known as the \"little brother of war\" because of its roughness and substitution for war. When disputes arose between Choctaw communities, stickball provided a civil way to settle issues. The stickball games would involve as few as twenty or as many as 300 players. The goal posts could be from a few hundred feet apart to a few miles. Goal posts were sometimes located within each opposing team's village. A Jesuit priest referenced stickball in 1729, and George Catlin painted the subject. The Mississippi Band of Choctaw Indians continue to practice the sport.\nChunkey was a game using a disk-shaped stone that was about 1\u20132\u00a0inches in length.\nPlayers would throw the disk down a corridor so that it could roll past the players at great speed. As the disk rolled down the corridor, players would throw wooden shafts at it. The object of the game was to strike the disk or prevent your opponents from hitting it.\nOther games included using corn, cane, and moccasins. The corn game used five to seven kernels of corn. One side was blackened and the other side white. Players won points based on each color. One point was awarded for the black side and 5\u20137 points for the white side. There were usually only two players.\nTreaties.\nLand was the most valuable asset, which the Native Americans held in collective stewardship. The United States systematically obtained Choctaw land for conventional European-American settlement through treaties, legislation, and threats of warfare. Although the Choctaw made treaties with Great Britain, France, Spain, and the Confederate States of America; the nation signed only nine treaties with the United States. Some treaties which the US made with other nations, such as the Treaty of San Lorenzo, indirectly affected the Choctaw.\nReservations.\nReservations can be found in Louisiana (Jena Band of Choctaw Indians), Mississippi (Mississippi Band of Choctaw Indians), and Oklahoma (Choctaw Nation of Oklahoma). The Oklahoma reservation is defined by treaty. Other population centers can be found throughout the United States.\nGeneral History.\nThe Choctaw coalesced as a people in the 16th century and had developed at least three distinct political and geographical divisions prior to European contact: the western Okla Falaya (\"Long People\"), the eastern Okla Tannap (\"People on the Other Side\"), and the southern Okla Hannali (\"Six Towns People\"). Eventually, these different groups would create distinct, independent alliances with nearby European powers.\nThe Choctaw were first noted by Europeans in French written records of the 17th century. Early Spanish explorers of the mid-16th century in the Southeast encountered ancestral Mississippian culture villages and chiefs. Eventually, the Spanish, French, and English would all, through their various explorers, governments, and peoples, discover the Choctaw as a complex society with firmly established tribal governments, alliances, religious practice, and culture.\nEarly contact between the Choctaw and Europeans include the French, based on the Gulf Coast and in Louisiana; the English of the Southeast; and Spain in Florida and Louisiana during the colonial era. These interactions introduced Choctaw communities to new and extensive social interactions and trade with Europeans, including more formal interactions with the governments of Spain, France, and England. These relationships with Europeans were influential in shaping the modern Choctaw people. After the United States was formed and its settlers began to move into the Southeast, the Choctaw were among the Five Civilized Tribes, who adopted many of their ways. Many Choctaw transitioned to yeoman farming methods and incorporated European Americans and African Americans (as tribal members, prisoners, and slaves) into their society.\nMost Choctaw allied with the Americans during the American Revolution, War of 1812, and the Red Stick War, most notably at the Battle of New Orleans. European Americans considered the Choctaw to be one of the \"Five Civilized Tribes\" of the Southeast. The Choctaw and the United States agreed to a total of nine treaties. By the last three, the US gained vast land cessions in the Southeast. As part of Indian Removal, despite not having waged war against the United States, the majority of Choctaw were forcibly relocated to Indian Territory from 1831 to 1833. The Choctaw government in Indian Territory maintained the tri-union tradition of their homeland by having three governmental districts. Each district had its own chief, who together with the town chiefs, sat on the Choctaw National Council.\nThose Choctaw who chose to stay in the state of Mississippi were considered state and U.S. citizens; they were one of the first major non-European ethnic groups to be granted citizenship. Article 14 in the 1830 treaty with the Choctaw stated Choctaws may wish to become citizens of the United States under the 14th Article of the Treaty of Dancing Rabbit Creek on all of the combined lands which were consolidated under Article I from all previous treaties between the United States and the Choctaw.\nDuring the American Civil War, the Choctaw in both Indian Territory and Mississippi mostly sided with the Confederate States of America. Under the late 19th-century Dawes Act and Curtis Acts, the US federal government broke up tribal land holdings and dissolved tribal governments in Indian Territory in order to extinguish Indian land claims before the admission of Oklahoma as a state in 1907. From that period, for several decades the United States Bureau of Indian Affairs appointed chiefs of the Choctaw and other tribes in the former Indian Territory.\nDuring World War I, Choctaw soldiers served in the US military as some of the first Native American codetalkers, using the Choctaw language. Since the Indian Reorganization Act of 1934, the Choctaw people in three areas have reconstituted their governments and gained federal recognition. The largest are the Choctaw Nation of Oklahoma, followed by the Mississippi Band of Choctaw Indians, and the Jena Band of Choctaw Indians, respectively.\nSince the 20th century, the Mississippi Band of Choctaw Indians were federally recognized in 1945, the Choctaw Nation of Oklahoma in 1971, and the Jena Band of Choctaw Indians in 1995. The Choctaw Apache Tribe of Ebarb (House Concurrent Resolution 2), Clifton Choctaw Band (House Concurrent Resolution 3), and Louisiana Band of Choctaw (Senate Concurrent Resolution 3), all based in Louisiana, were state-recognized in 1978. The MOWA Band of Choctaw Indians was state-recognized by the Alabama legislature in 1979, and again in 1984 during the establishment of the Alabama Indian Commission through Alabama Code 41-9-708.\nPopulation history.\nThe highest of early estimates - possibly representing the population peak - is that of Le Page du Pratz who estimated the Choctaw at 25,000 warriors (and therefore around 125,000 people) in year 1718. Other estimates from that time period were usually lower, but it is possible that they represented only a part of the tribe. Similar figures were given by St. Denis who estimated the Choctaw at 18,000 warriors (or 90,000 people) in 1714 and by W. Bull who estimated them at 16,000 warriors (or 80,000 people) in 1738. According to B. R. Carroll the Choctaw were reckoned by the French to be the most numerous nation of Indians in America and consisted of many thousand men. John R. Swanton enumerated a total of 102 Choctaw villages and towns in his book. Robert Rogers estimated the Choctaw at 10,000 warriors in 1775 (indicating a total population of 50,000). According to Gilbert Imlay they mustered 6,000 warriors around the year 1800 (implying a total population of 30,000). Jedidiah Morse estimated the Choctaw at 25,000 people in about year 1820. A census taken in 1830, shortly before the removal, reported a total population of 19,554. A report by the Commissioner of Indian Affairs dated 25 November 1841 indicates that by then 15,177 Choctaws had already moved to Oklahoma (Indian Territory). Few thousand more emigratted to the west in subsequent years. The Indian Office in 1856 reported the number of the Choctaws as 22,707. Emmanuel Domenech estimated the Choctaw at up to 25,000 people in about 1860. Enumeration published in 1886 counted 18,000 Choctaws in Oklahoma as of year 1884. The census of 1910 counted 15,917 Choctaws. Around years 1916\u20131919 there were in Oklahoma 17,488 Choctaws by blood, 1,651 by intermarriage and 6,029 freedmen, and in addition to that there were also at that time 3,099 Mississippi Choctaws and around 200 Choctaws living elsewhere.\nIn the 20th and 21st centuries Choctaw population has rebounded, in 2020 they numbered 254,154 (including 90,973 in Oklahoma)."}
{"id": "7223", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7223", "title": "Choctaws", "text": ""}
{"id": "7224", "revid": "47713794", "url": "https://en.wikipedia.org/wiki?curid=7224", "title": "Calypso", "text": "Calypso, Calipso, Kalypso, Kalipso, may refer to:"}
{"id": "7225", "revid": "14660971", "url": "https://en.wikipedia.org/wiki?curid=7225", "title": "Chemical affinity", "text": "In chemical physics and physical chemistry, chemical affinity is the electronic property by which dissimilar chemical species are capable of forming chemical compounds. Chemical affinity can also refer to the tendency of an atom or compound to combine by chemical reaction with atoms or compounds of unlike composition.\nHistory.\nEarly theories.\nThe idea of \"affinity\" is extremely old. Many attempts have been made at identifying its origins. The majority of such attempts, however, except in a general manner, end in futility since \"affinities\" lie at the basis of all magic, thereby pre-dating science. Physical chemistry, however, was one of the first branches of science to study and formulate a \"theory of affinity\". The name \"affinitas\" was first used in the sense of chemical relation by German philosopher Albertus Magnus near the year 1250. Later, those as Robert Boyle, John Mayow, Johann Glauber, Isaac Newton, and Georg Stahl put forward ideas on elective affinity in attempts to explain how heat is evolved during combustion reactions.\nThe term \"affinity\" has been used figuratively since c.\u00a01600 in discussions of structural relationships in chemistry, philology, etc., and reference to \"natural attraction\" is from 1616. \"Chemical affinity\", historically, has referred to the \"force\" that causes chemical reactions. as well as, more generally, and earlier, the \u2033tendency to combine\u2033 of any pair of substances. The broad definition, used generally throughout history, is that chemical affinity is that whereby substances enter into or resist decomposition.\nThe modern term chemical affinity is a somewhat modified variation of its eighteenth-century precursor \"elective affinity\" or elective attractions, a term that was used by the 18th century chemistry lecturer William Cullen. Whether Cullen coined the phrase is not clear, but his usage seems to predate most others, although it rapidly became widespread across Europe, and was used in particular by the Swedish chemist Torbern Olof Bergman throughout his book (1775). Affinity theories were used in one way or another by most chemists from around the middle of the 18th century into the 19th century to explain and organise the different combinations into which substances could enter and from which they could be retrieved. Antoine Lavoisier, in his famed 1789 \"Trait\u00e9 \u00c9l\u00e9mentaire de Chimie (Elements of Chemistry)\", refers to Bergman's work and discusses the concept of elective affinities or attractions.\nAccording to chemistry historian Henry Leicester, the influential 1923 textbook \"Thermodynamics and the Free Energy of Chemical Reactions\" by Gilbert N. Lewis and Merle Randall led to the replacement of the term \"affinity\" by the term \"free energy\" in much of the English-speaking world.\nAccording to Prigogine, the term was introduced and developed by Th\u00e9ophile de Donder.\nJohann Wolfgang von Goethe used the concept in his novel \"Elective Affinities\" (1809).\nVisual representations.\nThe affinity concept was very closely linked to the visual representation of substances on a table. The first-ever \"affinity table\", which was based on displacement reactions, was published in 1718 by the French chemist \u00c9tienne Fran\u00e7ois Geoffroy. Geoffroy's name is best known in connection with these tables of \"affinities\" (\"tables des rapports\"), which were first presented to the French Academy of Sciences in 1718 and 1720.\nDuring the 18th century many versions of the table were proposed with leading chemists like Torbern Bergman in Sweden and Joseph Black in Scotland adapting it to accommodate new chemical discoveries. All the tables were essentially lists, prepared by collating observations on the actions of substances one upon another, showing the varying degrees of affinity exhibited by analogous bodies for different reagents.\nCrucially, the table was the central graphic tool used to teach chemistry to students and its visual arrangement was often combined with other kinds diagrams. Joseph Black, for example, used the table in combination with chiastic and circlet diagrams to visualise the core principles of chemical affinity. Affinity tables were used throughout Europe until the early 19th century when they were displaced by affinity concepts introduced by Claude Berthollet.\nModern conceptions.\nIn chemical physics and physical chemistry, chemical affinity is the electronic property by which dissimilar chemical species are capable of forming chemical compounds. Chemical affinity can also refer to the tendency of an atom or compound to combine by chemical reaction with atoms or compounds of unlike composition.\nIn modern terms, we relate affinity to the phenomenon whereby certain atoms or molecules have the tendency to aggregate or bond. For example, in the 1919 book \"Chemistry of Human Life\" physician George W. Carey states that, \"Health depends on a proper amount of iron phosphate Fe3(PO4)2 in the blood, for the molecules of this salt have chemical affinity for oxygen and carry it to all parts of the organism.\" In this antiquated context, chemical affinity is sometimes found synonymous with the term \"magnetic attraction\". Many writings, up until about 1925, also refer to a \"law of chemical affinity\".\nIlya Prigogine summarized the concept of affinity, saying, \"All chemical reactions drive the system to a state of equilibrium in which the \"affinities\" of the reactions vanish.\"\nThermodynamics.\nThe present IUPAC definition is that affinity \"A\" is the negative partial derivative of Gibbs free energy \"G\" with respect to extent of reaction \"\u03be\" at constant pressure and temperature. That is,\nIt follows that affinity is positive for spontaneous reactions.\nIn 1923, the Belgian mathematician and physicist Th\u00e9ophile de Donder derived a relation between affinity and the Gibbs free energy of a chemical reaction. Through a series of derivations, de Donder showed that if we consider a mixture of chemical species with the possibility of chemical reaction, it can be proven that the following relation holds:\nWith the writings of Th\u00e9ophile de Donder as precedent, Ilya Prigogine and Defay in \"Chemical Thermodynamics\" (1954) defined chemical affinity as the rate of change of the uncompensated heat of reaction \"Q\"' as the reaction progress variable or reaction extent \"\u03be\" grows infinitesimally:"}
{"id": "7227", "revid": "47110710", "url": "https://en.wikipedia.org/wiki?curid=7227", "title": "Comet Hale\u2013Bopp", "text": "Comet Hale\u2013Bopp (formally designated C/1995\u00a0O1) is a long-period comet that was one of the most widely observed of the 20th century and one of the brightest seen for many decades.\nAlan Hale and Thomas Bopp discovered Comet Hale\u2013Bopp separately on July 23, 1995, before it became visible to the naked eye. It is difficult to predict the maximum brightness of new comets with any degree of certainty, but Hale\u2013Bopp exceeded most predictions when it passed perihelion on April 1, 1997, reaching about magnitude \u22121.8. It was visible to the naked eye for a record 18\u00a0months, due to its massive nucleus size. This is twice as long as the Great Comet of 1811, the previous record holder. Accordingly, Hale\u2013Bopp was dubbed the Great Comet of 1997.\nDiscovery.\nThe comet was discovered independently on July 23, 1995, by two observers, Alan Hale and Thomas Bopp, both in the United States.\nHale had spent many hundreds of hours searching for comets without success, and was tracking known comets from his driveway in New Mexico when he chanced upon Hale\u2013Bopp just after midnight. The comet had an apparent magnitude of 10.5 and lay near the globular cluster M70 in the constellation of Sagittarius. Hale first established that there was no other deep-sky object near M70, and then consulted a directory of known comets, finding that none were known to be in this area of the sky. Once he had established that the object was moving relative to the background stars, he emailed the Central Bureau for Astronomical Telegrams, the clearing house for astronomical discoveries.\nBopp did not own a telescope. He was out with friends near Stanfield, Arizona, observing star clusters and galaxies when he chanced across the comet while at the eyepiece of his friend's telescope. He realized he might have spotted something new when, like Hale, he checked his star maps to determine if any other deep-sky objects were known to be near M70, and found none. He alerted the Central Bureau for Astronomical Telegrams through a Western Union telegram. Brian G. Marsden, who had run the bureau since 1968, laughed, \"Nobody sends telegrams anymore. I mean, by the time that telegram got here, Alan Hale had already e-mailed us three times with updated coordinates.\"\nThe following morning, it was confirmed that this was a new comet, and it was given the designation C/1995\u00a0O1. The discovery was announced in International Astronomical Union circular 6187.\nEarly observation.\nHale\u2013Bopp's orbital position was calculated as 7.2\u00a0astronomical units (au) from the Sun, placing it between Jupiter and Saturn and by far the greatest distance from Earth at which a comet had been discovered by amateurs. Most comets at this distance are extremely faint, and show no discernible activity, but Hale\u2013Bopp already had an observable coma. A precovery image taken at the UK Schmidt Telescope in 1993 was found to show the then-unnoticed comet some 13\u00a0au from the Sun, a distance at which most comets are essentially unobservable. (Halley's Comet was more than 100 times fainter at the same distance from the Sun.) Analysis indicated later that its comet nucleus was 60\u00b120\u00a0kilometres in diameter, approximately six times the size of Halley's Comet.\nIts great distance and surprising activity indicated that comet Hale\u2013Bopp might become very bright when it reached perihelion in 1997. However, comet scientists were wary \u2013 comets can be extremely unpredictable, and many have large outbursts at great distances only to diminish in brightness later. Comet Kohoutek in 1973 had been touted as a \"comet of the century\" and turned out to be unspectacular.\nPerihelion.\nHale\u2013Bopp became visible to the naked eye in May 1996, and although its rate of brightening slowed considerably during the latter half of that year, scientists were still cautiously optimistic that it would become very bright. It was too closely aligned with the Sun to be observable during December 1996, but when it reappeared in January 1997 it was already bright enough to be seen by anyone who looked for it, even from large cities with light-polluted skies.\nThe Internet was a growing phenomenon at the time, and numerous websites that tracked the comet's progress and provided daily images from around the world became extremely popular. The Internet played a large role in encouraging the unprecedented public interest in comet Hale\u2013Bopp.\nAs the comet approached the Sun, it continued to brighten, shining at 2nd\u00a0magnitude in February, and showing a growing pair of tails, the blue gas tail pointing straight away from the Sun and the yellowish dust tail curving away along its orbit. On March 9, a solar eclipse in China, Mongolia and eastern Siberia allowed observers there to see the comet in the daytime. Hale\u2013Bopp had its closest approach to Earth on March 22, 1997, at a distance of 1.315\u00a0au.\nAs it passed perihelion on April 1, 1997, the comet developed into a spectacular sight. It shone brighter than any star in the sky except Sirius, and its dust tail stretched 40\u201345 degrees across the sky. The comet was visible well before the sky got fully dark each night, and while many great comets are very close to the Sun as they pass perihelion, comet Hale\u2013Bopp was visible all night to Northern Hemisphere observers.\nAfter perihelion.\nAfter its perihelion passage, the comet moved into the southern celestial hemisphere. The comet was much less impressive to southern hemisphere observers than it had been in the northern hemisphere, but southerners could see the comet gradually fade from view during the second half of 1997. The last naked-eye observations were reported in December 1997, which meant that the comet had remained visible without aid for 569\u00a0days, or about months. The previous record had been set by the Great Comet of 1811, which was visible to the naked eye for about 9 months.\nThe comet continued to fade as it receded, but was still tracked by astronomers. In October 2007, 10 years after the perihelion and at a distance of 25.7 au from the Sun, the comet was still active as indicated by the detection of the CO-driven coma. Herschel Space Observatory images taken in 2010 suggest comet Hale\u2013Bopp is covered in a fresh frost layer. Hale\u2013Bopp was again detected in December 2010 when it was 30.7\u00a0au away from the Sun, and in 2012, at 33.2\u00a0au from the Sun. The James Webb Space Telescope observed Hale\u2013Bopp in 2022, when it was 46.2\u00a0au from the Sun.\nOrbital changes.\nThe comet likely made its previous perihelion approximately 4,200 years ago, roughly the year 2215\u00a0BC. The estimated closest approach to Earth was 1.4\u00a0au, and it may have been observed in ancient Egypt during the 6th dynasty reign of the Pharaoh Pepi II (Reign: 2247 \u2013 c. 2216 BC). Pepi's pyramid at Saqqara contains a text referring to an \"nhh-star\" as a companion of the pharaoh in the heavens, where \"\" is the hieroglyph for long hair.\nHale\u2013Bopp may have had a near collision with Jupiter in 2215\u00a0BC, which probably caused a dramatic change in its orbit, and 2215\u00a0BC may have been its first passage through the inner Solar System from the Oort cloud. The comet's current orbit is almost perpendicular to the plane of the ecliptic, so further close approaches to planets will be rare. However, in April 1996 the comet passed within 0.77\u00a0au of Jupiter, close enough for its orbit to be measurably affected by the planet's gravity. The comet's orbit was shortened considerably to a period of roughly 2,399\u00a0years, and it will next return to the inner Solar System around the year 4385. Its greatest distance from the Sun (aphelion) will be about 354\u00a0au, reduced from about 525\u00a0au.\nThe estimated probability of Hale\u2013Bopp's striking Earth in future passages through the inner Solar System is remote, about 2.5\u00d710\u22129 per orbit. However, given that the comet nucleus is around 60\u00a0km in diameter, the consequences of such an impact would be apocalyptic. Weissman conservatively estimates the diameter at 35\u00a0km; an estimated density of 0.6 g/cm3 then gives a cometary mass of 1.3\u00d71019 g. At a probable impact velocity of 52.5\u00a0km/s, impact energy can be calculated as 1.9\u00d71032 ergs, or 4.4\u00d7109 megatons, about 44 times the estimated energy of the K-T impact event.\nOver many orbits, the cumulative effect of gravitational perturbations on comets with high orbital inclinations and small perihelion distances is generally to reduce the perihelion distance to very small values. Hale\u2013Bopp has about a 15%\u00a0chance of eventually becoming a sungrazing comet through this process. If such is the case, it could undergo huge mass loss, or break up into smaller pieces like the Kreutz sungrazers. It would also be extremely bright, due to a combination of closeness to the Sun and nuclei size, potentially exceeding Halley's Comet in 837 AD.\nScientific results.\nDue to the massive size of its nucleus, Comet Hale\u2013Bopp was observed intensively by astronomers during its perihelion passage, and several important advances in cometary science resulted from these observations. The dust production rate of the comet was very high (up to 2.0 kg/s), which may have made the inner coma optically thick. Based on the properties of the dust grainshigh temperature, high albedo and strong 10\u00a0\u03bcm silicate emission featurethe astronomers concluded the dust grains are smaller than observed in any other comet.\nHale\u2013Bopp showed the highest ever linear polarization detected for any comet. Such polarization is the result of solar radiation getting scattered by the dust particles in the coma of the comet and depends on the nature of the grains. It further confirms that the dust grains in the coma of comet Hale\u2013Bopp were smaller than inferred in any other comet.\nSodium tail.\nOne of the most remarkable discoveries was that the comet had a third type of tail. In addition to the well-known gas and dust tails, Hale\u2013Bopp also exhibited a faint sodium tail, only visible with powerful instruments with dedicated filters. Sodium emission had been previously observed in other comets, but had not been shown to come from a tail. Hale\u2013Bopp's sodium tail consisted of neutral atoms (not ions), and extended to some 50\u00a0million kilometres in length.\nThe source of the sodium appeared to be the inner coma, although not necessarily the nucleus. There are several possible mechanisms for generating a source of sodium atoms, including collisions between dust grains surrounding the nucleus, and \"sputtering\" of sodium from dust grains by ultraviolet light. It is not yet established which mechanism is primarily responsible for creating Hale\u2013Bopp's sodium tail, and the narrow and diffuse components of the tail may have different origins.\nWhile the comet's dust tail roughly followed the path of the comet's orbit and the gas tail pointed almost directly away from the Sun, the sodium tail appeared to lie between the two. This implies that the sodium atoms are driven away from the comet's head by radiation pressure.\nDeuterium abundance.\nThe abundance of deuterium in comet Hale\u2013Bopp in the form of heavy water was found to be about twice that of Earth's oceans. If Hale\u2013Bopp's deuterium abundance is typical of all comets, this implies that although cometary impacts are thought to be the source of a significant amount of the water on Earth, they cannot be the only source.\nDeuterium was also detected in many other hydrogen compounds in the comet. The ratio of deuterium to normal hydrogen was found to vary from compound to compound, which astronomers believe suggests that cometary ices were formed in interstellar clouds, rather than in the solar nebula. Theoretical modelling of ice formation in interstellar clouds suggests that comet Hale\u2013Bopp formed at temperatures of around 25\u201345\u00a0kelvin.\nOrganics.\nSpectroscopic observations of Hale\u2013Bopp revealed the presence of many organic chemicals, several of which had never been detected in comets before. These complex molecules may exist within the cometary nucleus, or might be synthesised by reactions in the comet.\nDetection of argon.\nHale\u2013Bopp was the first comet where the noble gas argon was detected. Noble gases are chemically inert and vary from low to high volatility. Since different noble elements have different sublimation temperatures, and don't interact with other elements, they can be used for probing the temperature histories of the cometary ices. Krypton has a sublimation temperature of 16\u201320\u00a0K and was found to be depleted more than 25 times relative to the solar abundance, while argon with its higher sublimation temperature was enriched relative to the solar abundance. Together these observations indicate that the interior of Hale\u2013Bopp has always been colder than 35\u201340\u00a0K, but has at some point been warmer than 20\u00a0K. Unless the solar nebula was much colder and richer in argon than generally believed, this suggests that the comet formed beyond Neptune in the Kuiper belt region and then migrated outward to the Oort cloud.\nRotation.\nComet Hale\u2013Bopp's activity and outgassing were not spread uniformly over its nucleus, but instead came from several specific jets. Observations of the material streaming away from these jets allowed astronomers to measure the rotation period of the comet, which was found to be about 11 hours 46 minutes.\nBinary nucleus question.\nIn 1997 a paper was published that hypothesised the existence of a binary nucleus to fully explain the observed pattern of comet Hale\u2013Bopp's dust emission observed in October 1995. The paper was based on theoretical analysis, and did not claim an observational detection of the proposed satellite nucleus, but estimated that it would have a diameter of about 30\u00a0km, with the main nucleus being about 70\u00a0km across, and would orbit in about three days at a distance of about 180\u00a0km. This analysis was confirmed by observations in 1996 using Wide-Field Planetary Camera 2 of the Hubble Space Telescope which had taken images of the comet that revealed the satellite.\nAlthough observations using adaptive optics in late 1997 and early 1998 showed a double peak in the brightness of the nucleus, controversy still exists over whether such observations can only be explained by a binary nucleus. The discovery of the satellite was not confirmed by other observations. Also, while comets have been observed to break up before, no case had been found of a stable binary nucleus until the subsequent discovery of .\nUFO claims.\nIn November 1996, amateur astronomer Chuck Shramek of Houston, Texas took a CCD image of the comet which showed a fuzzy, slightly elongated object nearby. His computer sky-viewing program did not identify the star, so Shramek called the Art Bell radio program \"Coast to Coast AM\" to announce that he had discovered a \"Saturn-like object\" following Hale\u2013Bopp. UFO enthusiasts, such as remote viewing proponent and Emory University political science professor Courtney Brown, soon concluded that there was an alien spacecraft following the comet.\nSeveral astronomers, including Alan Hale, stated that the object was simply the 8.5-magnitude star SAO141894. They noted that the star did not appear on Shramek's computer program because the user preferences were set incorrectly. Art Bell claimed to have obtained an image of the object from an anonymous astrophysicist who was about to confirm its discovery. However, astronomers Olivier Hainaut and David Tholen of the University of Hawaii stated that the alleged photo was an altered copy of one of their own comet images.\nThirty-nine members of the Heaven's Gate cult died in a mass suicide, in March 1997 with the intention of teleporting to a spaceship which they believed was flying behind the comet.\nNancy Lieder, who claims to receive messages from aliens through an implant in her brain, stated that Hale\u2013Bopp was a fiction designed to distract the population from the coming arrival of \"Nibiru\" or \"Planet X\", a giant planet whose close passage would disrupt the Earth's rotation, causing global cataclysm. Her original date for the apocalypse was May 2003, which passed without incident, but various conspiracy websites continued to predict the coming of Nibiru, most of whom tied it to the 2012 phenomenon. Lieder and others' claims of the planet Nibiru have been repeatedly debunked by scientists.\nLegacy.\nIts lengthy period of visibility and extensive coverage in the media meant that Hale\u2013Bopp was probably the most-observed comet in history, making a far greater impact on the general public than the return of Halley's Comet in 1986, and certainly seen by a greater number of people than witnessed any of Halley's previous appearances. For instance, 69% of Americans had seen Hale\u2013Bopp by April 9, 1997.\nHale\u2013Bopp was a record-breaking cometthe farthest comet from the Sun discovered by amateurs, with the largest well-measured cometary nucleus known after 95P/Chiron, and it was visible to the naked eye for twice as long as the previous record-holder. It was also brighter than magnitude\u00a00 for eight\u00a0weeks, longer than any other recorded comet.\nCarolyn Shoemaker and her husband Gene, co-discoverers of comet Shoemaker\u2013Levy 9, were involved in a car crash after photographing the comet. Gene died in the crash and his ashes were sent to the Moon aboard NASA's \"Lunar Prospector\" mission along with an image of Hale\u2013Bopp, \"the last comet that the Shoemakers observed together\".\nComposer Dmitry Kayukin created the music album \u201cComet 97\u201d based on his memories of observing Comet Hale\u2013Bopp."}
{"id": "7229", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7229", "title": "C-star algebra", "text": ""}
{"id": "7230", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=7230", "title": "Conspiracy", "text": "A conspiracy, also known as a plot, ploy, or scheme, is a secret plan or agreement between people (called conspirers or conspirators) for an unlawful or harmful purpose, such as murder, treason, or corruption, especially with a political motivation, while keeping their agreement secret from the public or from other people affected by it. In a political sense, conspiracy refers to a group of people united in the goal of subverting established political power structures. This can take the form of usurping or altering them, or even continually illegally profiteering from certain activities in a way that weakens the establishment with help from various political authorities. Depending on the circumstances, a conspiracy may also be a crime or a civil wrong. The term generally connotes, or implies, wrongdoing or illegality on the part of the conspirators, as it is commonly believed that people would not need to conspire to engage in activities that were lawful and ethical, or to which no one would object.\nThere are some coordinated activities that people engage in with secrecy that are not generally thought of as conspiracies. For example, intelligence agencies such as the American CIA and the British MI6 necessarily make plans in secret to spy on suspected enemies of their respective countries and the general populace of its home countries, but this kind of activity is generally not considered to be a conspiracy so long as their goal is to fulfill their official functions, and not something like improperly enriching themselves. Similarly, the coaches of competing sports teams routinely meet behind closed doors to plan game strategies and specific plays designed to defeat their opponents, but this activity is not considered a conspiracy because this is considered a legitimate part of the sport. Furthermore, a conspiracy must be engaged in knowingly. The continuation of social traditions that work to the advantage of certain groups and to the disadvantage of certain other groups, though possibly unethical, is not a conspiracy if participants in the practice are not carrying it forward for the purpose of perpetuating this advantage.\nOn the other hand, if the intent of carrying out a conspiracy exists, then there is a conspiracy even if the details are never agreed to aloud by the participants. CIA covert operations, for instance, are by their very nature hard to prove definitively, but research into the agency's work, as well as revelations by former CIA employees, has suggested several cases where the agency tried to influence events. During the Cold War, the United States tried to covertly change other nations' governments 66 times, succeeding in 26 cases.\nA \"conspiracy theory\" is a belief that a conspiracy has actually been decisive in producing a political event of which the theorists strongly disapprove. Conspiracy theories tend to be internally consistent and correlate with each other; they are generally designed to resist falsification either by evidence against them or a lack of evidence for them. Political scientist Michael Barkun has described conspiracy theories as relying on the view that the universe is governed by design, and embody three principles: nothing happens by accident, nothing is as it seems, and everything is connected. Another common feature is that conspiracy theories evolve to incorporate whatever evidence exists against them, so that they become, as Barkun writes, a closed system that is unfalsifiable, and therefore \"a matter of faith rather than proof.\"\nEtymology.\n\"Conspiracy\" comes from the Latin word \"conspiratio\". While \"conspiratio\" can mean \"plot\" or \"conspiracy\", it can also be translated as \"unity\" and \"agreement\", in the context of a group an example of this \"Kirri and Adele commenced the conspiracy at the secret thursday gin meeting\". \"Conspiratio\" comes from \"conspiro\" which, while still meaning \"conspiracy\" in the modern sense, also means \"I sing in unison\", as \"con\"- means \"with\" or \"together\", and \"spiro\" means \"I breathe\", literally meaning \"I breathe together with others\"."}
{"id": "7231", "revid": "2428506", "url": "https://en.wikipedia.org/wiki?curid=7231", "title": "Cytoprotectant", "text": ""}
{"id": "7232", "revid": "23755636", "url": "https://en.wikipedia.org/wiki?curid=7232", "title": "Cholistan Desert", "text": "The Cholistan Desert (; Saraiki: ), also locally known as Rohi (), is a desert in the southern part of Punjab, Pakistan that forms part of the Greater Thar Desert, which extends to Sindh province and the Indian state of Rajasthan. It is one of two large deserts in Punjab, the other being the Thal Desert. The name is derived from the Turkic word \"chol\", meaning \"sands,\" and \"istan\", a Persian suffix meaning \"land of.\"\nCholistan was a center for caravan trade, leading to the construction of numerous forts in the medieval period to protect trade routes - of which the Derawar Fort is the best-preserved example.\nGeography.\nCholistan covers an area of in the Bahawalpur, Bahawalnagar, and Rahim Yar Khan districts of southern Punjab. The nearest major city is Bahawalpur city, from the edge of the desert. The desert stretches about 480 kilometres in length, with a width varying between 32 and 192 kilometres. It is located between 27\u00b042\u038400\u0384\u0384 to 29\u00b0 45\u038400\u0384\u0384 north, and 69\u00b057' 30'\u2032 to 72\u00b0 52' 30'\u2032 east. 81% of the desert is sandy, while 19% is characterized by alluvial flats and small sandy dunes. The entire region is subject to desertification due to poor vegetation cover resulting in wind erosion.\nClimate.\nCholistan's climate is characterized as an arid and semi-arid Tropical desert, with very low annual humidity. The mean temperature in Cholistan is , with the hottest month being July with a mean temperature of . Summer temperatures can surpass , and sometimes rise over during periods of drought. Winter temperatures occasionally dip to . Average rainfall in Cholistan is up to 180mm, with July and August being the wettest months, although droughts are common. Water is collected seasonally in a system of natural pools called \"Toba,\" or manmade pools called \"Kund\". Subsoil water is found at a depth of 30\u201340 meters, but is typically brackish, and unsuitable for most plant growth.\n2022 Cholistan water crisis.\nIn May 2022, in the desert areas of Cholistan in Pakistan many cattle died due to extreme heat and water shortage. Shepherds, including cattle, have started migrating from water-scarce areas. Toba Salem Sar and Toba Nawa Kahu were the worst affected areas where 50 sheep died due to lack of water while more data is being collected from the affected areas.\nGeology.\nCholistan was formed during the Pleistocene period. Geologically, Cholistan is divided into the Greater Cholistan and Lesser Cholistan, which are roughly divided by the dry bed of the ancient Hakra River. Greater Cholistan is a mostly sandy area in the south and west part of the desert up to the border with India, and covers an area of . Sand dunes in this area reach over 100 meters in height. Soil in the region is also highly saline. Lesser Cholistan is an arid and slightly less sandy region approximately in area which extends north and east from the old Hakra river bed, historically up to the banks of the Sutlej River.\nSoil quality is generally poor with little organic matter in the Greater Cholistan, and compacted alluvial clays in the Lesser Cholistan. A canal system built during the British era led to irrigation of the northern part of Lesser Cholistan.\nHistory.\nThough now an arid region, Cholistan once had a large river flowing through it that was formed by the waters of the Sutlej and Yamuna Rivers. The dry bed of the Hakra River runs through the area, along which many settlements of the Indus Valley civilization/Harappan culture have been discovered, including the large urban site of Ganweriwal. The river system supported settlements in the region between 4000 BCE and 600 BCE when the river changed course. The river carried significant amounts of water, and flowed until at least where Derawar Fort is now located.\nOver 400 Harappan sites had been listed in Cholistan in the 1970s, with a further 37 added in the 1990s. The high density of settlements in Cholistan suggest it may have been one of the most productive regions of the Indus Valley Civilization. In the post-Harappan period, Cholistan was part of the Cemetery H culture which grew as a surviving regional variant of the Harappan culture, which was then followed by the Painted Grey Ware culture.\nThe region became a center for caravan trade, leading to the construction of a dense network of forts in the medieval period - of which the Derawar Fort is the best-preserved example. Other large forts in Cholistan include Meergarh, Jaangarh, Marotgarh, Maujgarh, Dingarh, Khangarh, Khairgarh, Bijnotgarh and Islamgarh - with the suffix \"garh\" denoting \"fort.\" These forts are part of the Tentative List of UNESCO World Heritage Sites, and run roughly parallel to the Indus and Sutlej Rivers 40 miles to the south. Smaller forts in the area include Bara, Bhagla, Duheinwala, Falji, Kandera, Liara, Murid, Machki, Nawankot, and Phulra forts.\nEconomy.\nLivestock.\nThe backbone of Cholistan economy is animal rearing. Few other livelihood opportunities aside from livestock farming are available in the region. Agricultural farming away from the irrigated regions in Lower Cholistan is difficult due to the lack of steady water supply.\nCamels in particular are prized in Cholistan for their meat and milk, use as transportation, and for entertainment such as racing and camel dancing. Two types of camels are found in Cholistan: \"Marrecha,\" or \"Mahra,\" is used for transportation or racing/dancing. \"Berella\" is used for milk production, and can produce 10\u201315 liters of milk per day per animal.\nLivestock holds much importance for meeting the area's major needs for cottage industry as well as providing milk, meat and fat. Because of the nomadic way of life, the main wealth of the people are their cattle that are bred for sale, milked or shorn for their wool. Moreover, isolated as they were, they had to depend upon themselves for all their needs like food, clothing, and items of daily use. So all their crafts initially stemmed from necessity but later on they started exporting their goods to the other places as well. The estimated number of livestock in the desert areas is 1.6 million.\nCotton and wool products.\nCholistan produces a very superior type of carpet wool compared to that produced in other parts of Pakistan. From this wool they knit beautiful carpets, rugs, and other woolen items. This includes blankets, which is also a local necessity for the desert as it is not always dust and heat, but winter nights here are very cold too, usually below the freezing point. Khes and pattu are also manufactured with wool or cotton. Khes is a form of blanket with a field of black white and pattu has a white ground base. Cholistan is now selling the wool for it brings maximum profit.\nTextiles.\nIt may be mentioned that cotton textiles have always been a hallmark craft of the Indus Valley civilization. Various kinds of khaddar-cloth are made for local consumption, and fine khaddar bedclothes and coarse lungies are woven here. A beautiful cloth called Sufi is also woven of silk and cotton, or with cotton wrap and silk wool. Gargas are made with numerous patterns and color, having complicated embroidery, mirror, and patchwork. Ajrak is another specialty of Cholistan. It is a special and delicate printing technique on both sides of the cloth in indigo blue and red patterns covering the base cloth. Cotton turbans and shawls are also made here. Chunri is another form of dopattas, having innumerable colors and patterns like dots, squares, and circles on it.\nPeople.\nAs per the 1998 Census of Pakistan, a total of 128,019 people, with a 2015 estimate of 229,071, with 70% living in Lesser Cholistan. The average household size is 6.65.\nLocal crafts.\nAs mentioned above, the Indus Valley has always been occupied by the wandering nomadic tribes who are fond of isolated areas, as such areas allow them to lead life free of foreign intrusion, enabling them to establish their own individual and unique cultures. Cholistan till the era of Mughal rule had also been isolated from outside influence. During the rule of Mughal Emperor Akbar, it became a proper productive unit. The entire area was ruled by a host of kings who securely guarded their frontiers. The rulers were the great patrons of art, and the various crafts underwent a simultaneous and parallel development, influencing each other. Masons, stone carvers, artisans, artists, and designers started rebuilding the old cities and new sites, and with that flourished new courts, paintings, weaving, and pottery. The fields of architecture, sculpture, terra cotta, and pottery developed greatly in this phase.\nCamel products.\nCamels are highly valued by the desert dwellers. Camels are not only useful for transportation and loading purposes, but its skin and wool are also quite valuable. Camel wool is spun and woven into beautiful woolen blankets known as falsies and into stylish and durable rugs. The camel's leather is also utilized in making caps, goblets, and expensive lampshades.\nLeather work.\nLeather work is another important local cottage industry due to the large number of livestock here. Other than the products mentioned above, Khusa (shoes) is a specialty of this area. Cholistani khusas are very famous for the quality of workmanship, variety, and richness of designs especially when stitched and embroidered with golden or brightly colored threads.\nJewelry.\nThe people of Cholistan are fond of jewelry, especially gold jewelry. The chief ornaments made and worn by them are \"Nath\" (nosegay), \"Katmala\" (necklace) \"Kangan\" (bracelet), and \"Pazeb\" (anklets). Gold and silver bangles are also a product of Cholistan. The locals similarly work in enamel, producing enamel buttons, earrings, bangles, and rings.\nEcology.\nFlora.\nSubsoil water in Cholistan is typically brackish, and unsuitable for most plant growth. Native trees, shrubs, and grasses are drought tolerant. There are 131 plant species in Cholistan from 89 genera and 24 families. \nMost common of them are below;\nA man-made forest called \"Dingarh\" was developed by the Pakistan Council of Research in Water Resources (PCRWR) on more than 100 ha. Dunes were fixed and stabilized by mechanical and vegetative means, and the area is now covered with trees with orchards of \"zizyphus,\" date palms, and grassland grown with collected rainwater and saline groundwater.\nFauna.\nThe wildlife of Cholistan desert mostly consists of migratory birds, especially the Houbara bustard who migrates to this part during winter. This species of bird is most famous in the hunting season, even though they are endangered in Pakistan (vulnerable globally), according to the IUCN Red List. Their population has decreased from 4,746 in 2001 to just a few dozens in recent times. \nIn December 2016, a Qatari prince had his hunting license rejected due to the species being endangered. Another prince, Dr. Fahad was fined with Rs. 80,000 ($760) and all of the birds he caught were set free for hunting without permit and license.\nA few endangered species in this desert are the Chinkara Antelope, Great Indian Bustard, and Blue Bull, etc. Their population of Chinkara has decreased from 3,000 in 2007 to just a little above 1,000 in 2010 due to non-permit hunting of the species by influential political families.\nTerracotta.\nThe Indus civilization was one of the earliest centres of pottery, and thus the pottery of Cholistan has a long history. Local soil is very fine and suitable for making pottery. The fineness of the earth can be observed on the Kacha houses which are actually plastered with mud but look like they have been white washed. The chief Cholistani ceramic articles are their surahies, piyalas, and glasses, remarkable for their lightness and fine finishing.\nIn earlier times, only the art of pottery and terracotta developed, but from the seventh century onwards, a large number of temples and images were also built on account of the intensified religious passions and the accumulation of wealth in cities."}
{"id": "7233", "revid": "39166520", "url": "https://en.wikipedia.org/wiki?curid=7233", "title": "Causant\u00edn mac Cin\u00e1eda", "text": "Causant\u00edn mac Cin\u00e1eda (Modern Gaelic: ; 836-877) was a king of the Picts. He is often known as Constantine I in reference to his place in modern lists of Scottish monarchs, but contemporary sources described only as a Pictish king. A son of (\"Kenneth MacAlpin\"), he succeeded his uncle as Pictish king following the latter's death on 13 April 862. It is likely that the reign of Causant\u00edn witnessed increased activity by Vikings, based in Ireland, Northumbria and northern Britain. He died fighting one such invasion.\nSources.\nVery few records of ninth century events in northern Britain survive. The main local source from the period is the \"Chronicle of the Kings of Alba\", a list of kings from Cin\u00e1ed mac Ailp\u00edn (died 858) to Cin\u00e1ed mac Ma\u00edl Coluim (died 995). The list survives in the Poppleton Manuscript, a thirteenth century compilation. Originally simply a list of kings with reign lengths, the other details contained in the Poppleton Manuscript version were added from the tenth century onwards. In addition to this, later king lists survive. The earliest genealogical records of the descendants of Cin\u00e1ed mac Ailp\u00edn may date from the end of the tenth century, but their value lies more in their context, and the information they provide about the interests of those for whom they were compiled, than in the unreliable claims they contain. The Pictish king-lists originally ended with this Causant\u00edn, who was reckoned the seventieth and last king of the Picts.\nFor narrative history, the principal sources are the \"Anglo-Saxon Chronicle\" and the Irish annals. While Scandinavian sagas describe events in 9th century Britain, their value as sources of historical narrative, rather than documents of social history, is disputed. If the sources for north-eastern Britain, the lands of the kingdom of Northumbria and the former Pictland, are limited and late, those for the areas on the Irish Sea and Atlantic coasts \u2014 the modern regions of north-west England and all of northern and western Scotland \u2014 are non-existent, and archaeology and toponymy are of primary importance.\nLanguages and names.\nWriting a century before Causant\u00edn was born, Bede recorded five languages in Britain. Latin, the common language of the church; Old English, the language of the Angles and Saxons; Irish, spoken on the western coasts of Britain and in Ireland; Brythonic, ancestor of the \nWelsh language, spoken in large parts of western Britain; and Pictish, spoken in northern Britain. By the ninth century a sixth language, Old Norse, had arrived with the Vikings.\nAmla\u00edb and \u00cdmar.\nViking activity in northern Britain appears to have reached a peak during Causant\u00edn's reign. Viking armies were led by a small group of men who may have been kinsmen. Among those noted by the Irish annals, the \"Chronicle of the Kings of Alba\" and the \"Anglo-Saxon Chronicle\" are \u00cdvarr \u2014 \u00cdmar in Irish sources \u2014 who was active from East Anglia to Ireland, Halfd\u00e1n \u2014 Albdann in Irish, Healfdene in Old English \u2014 and Amla\u00edb or \u00d3l\u00e1fr. As well as these leaders, various others related to them appear in the surviving record.\nViking activity in Britain increased in 865 when the Great Heathen Army, probably a part of the forces which had been active in Francia, landed in East Anglia. The following year, having obtained tribute from the East Anglian King Edmund, the Great Army moved north, seizing York, chief city of the Northumbrians. The Great Army defeated an attack on York by the two rivals for the Northumbrian throne, Osberht and \u00c6lla, who had put aside their differences in the face of a common enemy. Both would-be kings were killed in the failed assault, probably on 21 March 867. Following this, the leaders of the Great Army are said to have installed one Ecgberht as king of the Northumbrians. Their next target was Mercia where King Burgred, aided by his brother-in-law King \u00c6thelred of Wessex, drove them off.\nWhile the kingdoms of East Anglia, Mercia and Northumbria were under attack, other Viking armies were active in the far north. Amla\u00edb and Auisle (\u00c1sl or Au\u00f0g\u00edsl), said to be his brother, brought an army to Fortriu and obtained tribute and hostages in 866. Historians disagree as to whether the army returned to Ireland in 866, 867 or even in 869. Late sources of uncertain reliability state that Auisle was killed by Amla\u00edb in 867 in a dispute over Amla\u00edb's wife, the daughter of Cin\u00e1ed. It is unclear whether, if accurate, this woman should be identified as a daughter of Cin\u00e1ed mac Ailp\u00edn, and thus Causant\u00edn's sister, or as a daughter of Cin\u00e1ed mac Conaing, king of Brega. While Amla\u00edb and Auisle were in north Britain, the \"Annals of Ulster\" record that \u00c1ed Findliath, High King of Ireland, took advantage of their absence to destroy the longphorts along the northern coasts of Ireland. \u00c1ed Findliath was married to Causant\u00edn's sister M\u00e1el Muire. She later married \u00c1ed's successor Flann Sinna. Her death is recorded in 913.\nIn 870, Amla\u00edb and \u00cdvarr attacked Dumbarton Rock, where the River Leven meets the River Clyde, the chief place of the kingdom of Alt Clut, south-western neighbour of Pictland. The siege lasted four months before the fortress fell to the Vikings who returned to Ireland with many prisoners, \"Angles, Britons and Picts\", in 871. Archaeological evidence suggests that Dumbarton Rock was largely abandoned and that Govan replaced it as the chief place of the kingdom of Strathclyde, as Alt Clut was later known. King Artgal of Alt Clut did not long survive these events, being killed \"at the instigation\" of Causant\u00edn son of Cin\u00e1ed two years later. Artgal's son and successor Run was married to a sister of Causant\u00edn.\nAmla\u00edb disappears from Irish annals after his return to Ireland in 871. According to the \"Chronicle of the Kings of Alba\", he was killed by Causant\u00edn either in 871 or 872 when he returned to Pictland to collect further tribute. His ally \u00cdvarr died in 873.\nLast days of the Pictish kingdom.\nIn 875, the \"Chronicle\" and the \"Annals of Ulster\" again report a Viking army in Pictland; the \"Annals of Ulster\" say that \"a great slaughter of the Picts resulted\". No name is given to the battle in which the slaughter occurred, yet the Chronicle notes a battle fought between Danes and Scots near Dollar but notes a subsequent \"annihilation\" at Atholl. In 877, shortly after building a new church for the Culdees at St Andrews, Causant\u00edn was captured and executed (or perhaps killed in battle) after defending against Viking raiders. Although there is agreement on the time and general manner of his death, it is not clear where this happened. Some believe he was beheaded on a Fife beach, following a battle at Fife Ness, near Crail. William Forbes Skene reads the \"Chronicle\" as placing Causant\u00edn's death at Inverdovat (by Newport-on-Tay), which appears to match the Prophecy of Berch\u00e1n. The account in the \"Chronicle of Melrose\" names the place as the \"Black Cave\" and John of Fordun calls it the \"Black Den\". Causant\u00edn was buried on Iona.\nAftermath.\nCausant\u00edn's son Domnall and his descendants represented the main line of the kings of Alba and later Scotland."}
{"id": "7234", "revid": "8524693", "url": "https://en.wikipedia.org/wiki?curid=7234", "title": "Constantine II (emperor)", "text": "Constantine II (; 316\u2013340) was Roman emperor from 337 to 340. The son of the emperor Constantine I, he was proclaimed \"caesar\" by his father shortly after his birth. He was associated with military victories over the Sarmatians, Alamanni and Goths during his career, for which he was granted a number of victory titles. He held the consulship four times \u2013 in 320, 321, 324, and 329.\nConstantine I had arranged for his sons to share power with their cousins Dalmatius and Hannibalianus, but this was not accepted by Constantine II and his brothers. As a result, Constantine II's brother Constantius II ordered the killings of numerous male relatives following Constantine I's death, including Dalmatius and Hannibalianus, thus eliminating any possible opponents to the succession of Constantine I's sons. Constantine II then ascended to the throne alongside his two younger brothers, ruling Gaul, Hispania, and Britain. However, his belief in his rights of primogeniture and attempts to exert them over his youngest brother Constans caused conflict, which ended with his death in a failed invasion of Italy in 340. Constans subsequently took control of Constantine's territories, with the latter being subjected to \"damnatio memoriae\".\nLife.\nBorn in Arles in 316, Constantine II was the second son of the Roman emperor Constantine I, and the eldest with his wife Fausta, the daughter of the emperor Maximian.\nCaesar.\nOn 1 March 317, he was made \"caesar\" at Serdica. After accompanying his father on his campaign against the Sarmatians in 323, he was commemorated on coinage produced to recognize the ensuing victory. Constantine II usually resided with his father until 328, when his own court was installed at Trier. An inscription dated to 328\u2013330 records the title of , indicating that his generals won a victory over the Alamanni. His military career continued when Constantine I made him field commander during the 332 winter campaign against the Goths. As a result of his leadership, the military operation concluded with 100,000 Goths reportedly slain and the surrender of the ruler Ariaric. Festival games were initiated in Rome to celebrate the \"caesar\"'s role in the successful military campaigns, in a public advertisement of his capability to rule. He was married prior to 336, although his wife's identity remains unknown.\nWhile Constantine I had intended for his sons to rule together with their cousins Dalmatius and Hannibalianus, soon after his death in May 337 the army murdered several of their male relatives, including Dalmatius and Hannibalianus, on the orders of Constantine II's younger brother Constantius II. Although Constantine himself appears to not have been directly involved, Burgess observed from numismatic evidence that he and his brothers \"not only seem not to have fully accepted the legitimacy of Dalmatius and viewed him as an interloper, but also appear to have communicated with one another on this point and agreed on a common response.\"\nIn what seemed to be an attempt to distance themselves from the massacre, the three brothers proceeded to print coins of Theodora, whom their murdered relatives had been descended from. Most of the coins were generated at Constantine II's capital, Trier, indicating that he was the one responsible for designing and producing the coinage at the start, as well as convincing his brothers to do the same. Woods considered it to suggest that he was more sympathetic to Theodora's memory than his brothers, possibly because his wife may have been a granddaughter of Theodora.\nIn June 337, before he was named emperor, Constantine had already begun attempting to assert his seniority. He issued an order allowing the exiled bishop Athanasius to return to Alexandria, which was under the control of Constantius II, claiming to be carrying out the unfulfilled intentions of his father. While Constantine's motives remain unclear, suggested explanations include him truly believing in the bishop's innocence, him wanting to get rid of a religious nuisance, or him wanting to cause trouble for Constantius, who would oust Athanasius from Alexandria only two years later.\nAugustus.\nThe three brothers were not named as \"Augusti\" until 9 September 337, when they gathered together in Pannonia and divided the Roman territories among themselves. Constantine received Gaul, Britannia and Hispania. Unlike his younger brothers, he gained little from Dalmatius's removal.\nConstantine was evidently left unsatisfied with the results of their meeting, seemingly believing that his age granted him some sort of seniority in the imperial college and, by extension, control over the dominion of his youngest brother Constans, who was still a teenager in 337. Even after campaigning successfully against the Alamanni in 338, Constantine continued to maintain his position. The Theodosian Code recorded his legislative intervention in Constans's territory through issuing an edict to the proconsul of Africa in 339.\nIn April 340, Constantine launched an invasion into Italy to claim territory from Constans. Constans, at that time in Naissus, sent a number of troops to confront him, and Constantine was killed in an ambush near Aquileia. Constans then took control of his brother's realm, whose inhabitants seem to have been largely unaffected by their change in ruler. \nAfter his death, Constantine was subjected to \"damnatio memoriae\". Constans issued legislation repealing Constantine's acts shortly after his death, where the deceased emperor was branded as \"the public enemy and our own enemy.\" Years later, when Libanius delivered a panegyric for both Constans and Constantius, Constantine was completely omitted from the narrative, as if he had never existed."}
{"id": "7235", "revid": "236591", "url": "https://en.wikipedia.org/wiki?curid=7235", "title": "Constantine II of Scotland", "text": "Causant\u00edn mac \u00c1eda (Modern Gaelic: , anglicised Constantine II; born no later than 879; died 952) was an early King of Scotland, known then by the Gaelic name \"Alba\". The Kingdom of Alba, a name which first appears in Constantine's lifetime, was situated in what is now Northern Scotland.\nThe core of the kingdom was formed by the lands around the River Tay. Its southern limit was the River Forth, northwards it extended towards the Moray Firth and perhaps to Caithness, while its western limits are uncertain. Constantine's grandfather Kenneth I (Cin\u00e1ed mac Ailp\u00edn, died 858) was the first of the family recorded as a king, but as king of the Picts. This change of title, from king of the Picts to king of Alba, is part of a broader transformation of Pictland and the origins of the Kingdom of Alba are traced to Constantine's lifetime.\nHis reign, like those of his predecessors, was dominated by the actions of Viking rulers in the British Isles, particularly the U\u00ed \u00cdmair ('Grandsons/Descenants of \u00cdmar', or Ivar the Boneless). During Constantine's reign, the rulers of the southern kingdoms of Wessex and Mercia, later the Kingdom of England, extended their authority northwards into the disputed kingdoms of Northumbria. At first, the southern rulers allied with him against the Vikings, but in 934, \u00c6thelstan, unprovoked, invaded Scotland both by sea and land with a huge retinue that included four Welsh kings. He ravaged southern Alba, but there is no record of any battles. He had withdrawn by September. Three years later, in 937, probably in retaliation for the invasion of Alba, King Constantine allied with Olaf Guthfrithson, King of Dublin, and Owain ap Dyfnwal, King of Strathclyde, but they were defeated at the battle of Brunanburh. In 943, Constantine abdicated the throne and retired to the C\u00e9li D\u00e9 (Culdee) monastery of St Andrews where he died in 952. He was succeeded by his predecessor's son Malcolm I (M\u00e1el Coluim mac Domnaill).\nConstantine's reign of 43 years, exceeded in Scotland only by that of King William the Lion before the Union of the Crowns in 1603, is believed to have played a defining part in the Gaelicisation of Pictland, in which his patronage of the Irish C\u00e9li D\u00e9 monastic reformers was a significant factor. During his reign, the words \"Scots\" and \"Scotland\" () are first used to mean part of what is now Scotland. The earliest evidence for the ecclesiastical and administrative institutions which would last until the Davidian Revolution also appears at this time.\nSources.\nCompared to neighbouring Ireland and Anglo-Saxon England, few records of 9th and 10th century events in Scotland survive. The main local source from the period is the \"Chronicle of the Kings of Alba\", a list of kings from Kenneth MacAlpin (died 858) to Kenneth II (Cin\u00e1ed mac Ma\u00edl Coluim, died 995). The list survives in the Poppleton manuscript, a 13th-century compilation. Originally simply a list of kings with reign lengths, the other details contained in the Poppleton manuscript version were added in the 10th and 12th centuries. In addition to this, later king lists survive. The earliest genealogical records of the descendants of Kenneth MacAlpin may date from the end of the 10th century, but their value lies more in their context, and the information they provide about the interests of those for whom they were compiled, than in the unreliable claims they contain.\nFor narrative history, the principal sources are the \"Anglo-Saxon Chronicle\" and the Irish annals. The evidence from charters created in the Kingdom of England provides occasional insight into events in Scotland. While Scandinavian sagas describe events in 10th century Britain, their value as sources of historical narrative, rather than documents of social history, is disputed. Mainland European sources rarely concern themselves with affairs in any part of the British Isles, and even less commonly with events in Scotland, but the life of Saint Cathr\u00f3e of Metz, a work of hagiography written in Germany at the end of the 10th century, provides plausible details of the saint's early life in north Britain.\nWhile the sources for north-eastern Britain, the lands of the kingdom of Northumbria and the former Pictland, are limited and late, those for the areas on the Irish Sea and Atlantic coasts \u2014 the modern regions of north-west England and all of northern and western Scotland \u2014 are non-existent, and archaeology and toponymy are of primary importance.\nPictland from Constant\u00edn mac Fergusa to Constantine I.\nThe dominant kingdom in eastern Scotland before the Viking Age was the northern Pictish kingdom of Fortriu on the shores of the Moray Firth. By the 9th century, the Gaels of D\u00e1l Riata (Dalriada) were subject to the kings of Fortriu of the family of Causant\u00edn mac Fergusa (Constantine son of Fergus). Constant\u00edn's family dominated Fortriu after 789 and perhaps, if Constant\u00edn was a kinsman of \u00d3engus I of the Picts (\u00d3engus son of Fergus), from around 730. The dominance of Fortriu came to an end in 839 with a defeat by Viking armies reported by the \"Annals of Ulster\" in which King Uen of Fortriu and his brother Bran, Constant\u00edn's nephews, together with the king of D\u00e1l Riata, \u00c1ed mac Boanta, \"and others almost innumerable\" were killed. These deaths led to a period of instability lasting a decade as several families attempted to establish their dominance in Pictland. By around 848 Kenneth MacAlpin had emerged as the winner.\nLater national myth made Kenneth MacAlpin the creator of the Kingdom of Scotland, the founding of which was dated from 843, the year in which he was said to have destroyed the Picts and inaugurated a new era. The historical record for 9th century Scotland is meagre, but the Irish annals and the 10th century \"Chronicle of the Kings of Alba\" agree that Kenneth was a Pictish king, and call him \"king of the Picts\" at his death. The same style is used of Kenneth's brother Donald I (Domnall mac Ailp\u00edn) and sons Constantine I (Constant\u00edn mac Cin\u00e1eda) and \u00c1ed (\u00c1ed mac Cin\u00e1eda).\nThe kingdom ruled by Kenneth's descendants \u2014 older works used the name House of Alpin to describe them but descent from Kenneth was the defining factor, Irish sources referring to \"Clann Cin\u00e1eda meic Ailp\u00edn\" (\"the Clan of Kenneth MacAlpin\") \u2014 lay to the south of the previously dominant kingdom of Fortriu, centred in the lands around the River Tay. The extent of Kenneth's nameless kingdom is uncertain, but it certainly extended from the Firth of Forth in the south to the Mounth in the north. Whether it extended beyond the mountainous spine of north Britain \u2014 Druim Alban \u2014 is unclear. The core of the kingdom was similar to the old counties of Mearns, Forfarshire, Forfar, Perth, Fife, and Kinross. Among the chief ecclesiastical centres named in the records are Dunkeld, probably the seat of the bishop of the kingdom, and \"Cell R\u00edgmonaid\" (modern St Andrews).\nKenneth's son Constantine died in 876, probably killed fighting against a Viking army that had come north from Northumbria in 874. According to the king lists, he was counted as the 70th and last king of the Picts in later times.\nBritain and Ireland at the end of the 9th century.\nIn 899 Alfred the Great, king of Wessex, died leaving his son Edward the Elder as ruler of England south of the River Thames and his daughter \u00c6thelfl\u00e6d and son-in-law \u00c6thelred ruling the western, English part of Mercia. The situation in the Danish kingdoms of eastern England is less clear. King Eohric was probably ruling in East Anglia, but no dates can reliably be assigned to the successors of Guthfrith of York in Northumbria. It is known that Guthfrith was succeeded by Siefredus and Cnut, although whether these men ruled jointly or one after the other is uncertain. Northumbria may have been divided by this time between the Viking kings in York and the local rulers, perhaps represented by Eadulf, based at Bamburgh who controlled the lands from the River Tyne or River Tees to the Forth in the north.\nIn Ireland, Flann Sinna, married to Constantine's aunt M\u00e1el Muire, was dominant. The years around 900 represented a period of weakness among the Vikings and Norse\u2013Gaels of Dublin. They are reported to have been divided between two rival leaders. In 894 one group left Dublin, perhaps settling on the Irish Sea coast of Britain between the River Mersey and the Firth of Clyde. The remaining Dubliners were expelled in 902 by Flann Sinna's son-in-law Cerball mac Muirec\u00e1in, and soon afterwards appeared in western and northern Britain.\nTo the southwest of Constantine's lands lay the Kingdom of Strathclyde. This extended north into The Lennox, east to the River Forth, and south into the Southern Uplands. In 900 it was probably ruled by King Dyfnwal.\nThe situation of the Gaelic kingdoms of D\u00e1l Riata in western Scotland is uncertain. No kings are known by name after \u00c1ed mac Boanta. The Frankish \"Annales Bertiniani\" may record the conquest of the Inner Hebrides, the seaward part of D\u00e1l Riata, by Northmen in 849. In addition to these, the arrival of new groups of Vikings from northern and western Europe was still commonplace. Whether there were Viking or Norse-Gael kingdoms in the Western Isles or the Northern Isles at this time is debated.\nEarly life.\n\u00c1ed, Constantine's father, succeeded Constantine's uncle and namesake Constantine I in 876 but was killed in 878. \u00c1ed's short reign is glossed as being of no importance by most king lists. Although the date of his birth is nowhere recorded, Constantine II cannot have been born any later than the year after his father's death, \"i.e.\", 879. His name may suggest that he was born a few years earlier, during the reign of his uncle Constantine I.\nAfter \u00c1ed's death, there is a two-decade gap until the death of Donald II (Domnall mac Constant\u00edn) in 900 during which nothing is reported in the Irish annals. The entry for the reign between \u00c1ed and Donald II is corrupt in the \"Chronicle of the Kings of Alba\", and in this case, the \"Chronicle\" is at variance with every other king list. According to the \"Chronicle\", \u00c1ed was followed by Eochaid, a grandson of Kenneth MacAlpin, who is somehow connected with Giric, but all other lists say that Giric ruled after \u00c1ed and make great claims for him. Giric is not known to have been a kinsman of Kenneth's, although it has been suggested that he was related to him by marriage. The major changes in Pictland which began at about this time have been associated by Alex Woolf and Archie Duncan with Giric's reign.\nWoolf suggests that Constantine and his younger brother Donald may have passed Giric's reign in exile in Ireland where their aunt M\u00e1el Muire was wife of two successive High Kings of Ireland, \u00c1ed Findliath and Flann Sinna. Giric died in 889. If he had been in exile, Constantine may have returned to Pictland where his cousin Donald II became king. Donald's reputation is suggested by the epithet \"dasachtach\", a word used of violent madmen and mad bulls, attached to him in the 11th-century writings of Flann Mainistrech, echoed by his description in \"The Prophecy of Berch\u00e1n\" as \"the rough one who will think relics and psalms of little worth\". Wars with the Viking kings in Britain and Ireland continued during Donald's reign and he was probably killed fighting yet more Vikings at Dunnottar in the Mearns in 900. Constantine succeeded him as king.\nVikings and bishops.\nThe earliest event recorded in the \"Chronicle of the Kings of Alba\" in Constantine's reign is an attack by Vikings and the plundering of Dunkeld \"and all Albania\" in his third year. This is the first use of the word Albania, the Latin form of the Old Irish \"Alba\", in the \"Chronicle\" which until then describes the lands ruled by the descendants of Cin\u00e1ed as Pictavia.\nThese Norsemen could have been some of those who were driven out of Dublin in 902 or were the same group who had defeated Domnall in 900. The \"Chronicle\" states that the Northmen were killed in \"Srath Erenn\", which is confirmed by the \"Annals of Ulster\" which records the death of \u00cdmar grandson of \u00cdmar and many others at the hands of the men of Fortriu in 904. This \u00cdmar was the first of the U\u00ed \u00cdmair, the grandsons of \u00cdmar, to be reported; three more grandsons of \u00cdmar appear later in Constant\u00edn's reign. The \"Fragmentary Annals of Ireland\" contain an account of the battle, and this attributes the defeat of the Norsemen to the intercession of Saint Columba following fasting and prayer. An entry in the \"Chronicon Scotorum\" under the year 904 may possibly contain a corrupted reference to this battle.\nThe next event reported by the \"Chronicle of the Kings of Alba\" is dated to 906. This records that:\n The meaning of this entry, and its significance, have been the subject of debate.\nThe phrase \"pariter cum Scottis\" in the Latin text of the \"Chronicle\" has been translated in several ways. William Forbes Skene and Alan Orr Anderson proposed that it should be read as \"in conformity with the customs of the Gaels\", relating it to the claims in the king lists that Giric liberated the church from secular oppression and adopted Irish customs. It has been read as \"together with the Gaels\", suggesting either public participation or the presence of Gaels from the western coasts as well as the people of the east coast. Finally, it is suggested that it was the ceremony that followed \"the custom of the Gaels\" and not the agreements.\nThe idea that this gathering agreed to uphold Irish laws governing the church has suggested that it was an important step in the gaelicisation of the lands east of Druim Alban. Others have proposed that the ceremony in some way endorsed Constantine's kingship, prefiguring later royal inaugurations at Scone. Alternatively, if Bishop Cellach was appointed by Giric, it may be that the gathering was intended to heal a rift between king and church.\nReturn of the U\u00ed \u00cdmair.\nFollowing the events at Scone, there is little of substance reported for a decade. A story in the \"Fragmentary Annals of Ireland\", perhaps referring to events sometime after 911, claims that \u00c6thelfl\u00e6d, who ruled in Mercia, allied with the Irish and northern rulers against the Norsemen on the Irish sea coasts of Northumbria. The \"Annals of Ulster\" record the defeat of an Irish fleet from the kingdom of Ulaid by Vikings \"on the coast of England\" at about this time.\nIn this period the \"Chronicle of the Kings of Alba\" reports the death of Cormac mac Cuilenn\u00e1in, king of Munster, in the eighth year of Constantine's reign. This is followed by an undated entry which was formerly read as \"In his time Domnall [i.e. Dyfnwal], king of the [Strathclyde] Britons died, and Domnall son of \u00c1ed was elected\". This was thought to record the election of a brother of Constantine named Domnall to the kingship of the Britons of Strathclyde and was seen as early evidence of the domination of Strathclyde by the kings of Alba. The entry in question is now read as \"... Dyfnwal ... and Domnall son \u00c1ed king of Ailech died\", this Domnall being a son of \u00c1ed Findliath who died on 21 March 915. Finally, the deaths of Flann Sinna and Niall Gl\u00fandub are recorded.\nThere are more reports of Viking fleets in the Irish Sea from 914 onwards. By 916 fleets under Sihtric C\u00e1ech and Ragnall, said to be grandsons of \u00cdmar (that is, they belonged to the same U\u00ed \u00cdmair kindred as the \u00cdmar who was killed in 904), were very active in Ireland. Sihtric inflicted a heavy defeat on the armies of Leinster and retook Dublin in 917. The following year Ragnall appears to have returned across the Irish Sea intent on establishing himself as king at York. The only precisely dated event in the summer of 918 is the death of \u00c6thelfl\u00e6d of Mercia on 12 June 918 at Tamworth, Staffordshire. \u00c6thelfl\u00e6d had been negotiating with the Northumbrians to obtain their submission, but her death put an end to this and her successor, her brother Edward the Elder, was occupied with securing control of Mercia.\nThe northern part of Northumbria, and perhaps the whole kingdom, had probably been ruled by Ealdred son of Eadulf since 913. Faced with Ragnall's invasion, Ealdred came north seeking assistance from Constantine. The two advanced south to face Ragnall, and this led to a battle somewhere on the banks of the River Tyne, probably at Corbridge where Dere Street crosses the river. The Battle of Corbridge appears to have been indecisive; the \"Chronicle of the Kings of Alba\" is alone in giving Constantine the victory.\nThe report of the battle in the \"Annals of Ulster\" says that none of the kings or mormaers among the men of Alba were killed. This is the first surviving use of the word mormaer; other than the knowledge that Constantine's kingdom had its own bishop or bishops and royal villas, this is the only hint to the institutions of the kingdom.\nAfter Corbridge, Ragnall enjoyed only a short respite. In the south, Alfred's son Edward had rapidly secured control of Mercia and had a burh constructed at Bakewell in the Peak District from which his armies could easily strike north. An army from Dublin led by Ragnall's kinsman Sihtric struck at north-western Mercia in 919, but in 920 or 921 Edward met with Ragnall and other kings. The \"Anglo-Saxon Chronicle\" states that these kings \"chose Edward as father and lord\". Among the other kings present were Constantine, Ealdred son of Eadwulf, and the king of Strathclyde, Owain ap Dyfnwal. Here, again, a new term appears in the record, the \"Anglo-Saxon Chronicle\" for the first time using the word \"scottas\", from which Scots derives, to describe the inhabitants of Constantine's kingdom in its report of these events.\nEdward died in 924. His realms appear to have been divided with the West Saxons recognising \u00c6lfweard while the Mercians chose \u00c6thelstan who had been raised at \u00c6thelfl\u00e6d's court. \u00c6lfweard died within weeks of his father and \u00c6thelstan was inaugurated as king of all of Edward's lands in 925.\n\u00c6thelstan.\nBy 926 Sihtric had evidently acknowledged \u00c6thelstan as overlord, adopting Christianity and marrying a sister of \u00c6thelstan at Tamworth. Within the year he appears to have forsaken his new faith and repudiated his wife, but before \u00c6thelstan could respond, Sihtric died suddenly in 927. His kinsman, perhaps brother, Gofraid, who had remained as his deputy in Dublin, came from Ireland to take power in York but failed. \u00c6thelstan moved quickly, seizing much of Northumbria. In less than a decade, the kingdom of the English had become by far the greatest power in Britain and Ireland, perhaps stretching as far north as the Firth of Forth.\nJohn of Worcester's chronicle suggests that \u00c6thelstan faced opposition from Constantine, Owain, and the Welsh kings. William of Malmesbury writes that Gofraid, together with Sihtric's young son Olaf Cuaran fled north and received refuge from Constantine, which led to war with \u00c6thelstan. A meeting at Eamont Bridge on 12 July 927 was sealed by an agreement that Constantine, Owain, Hywel Dda, and Ealdred would \"renounce all idolatry\": that is, they would not ally with the Viking kings. William states that \u00c6thelstan stood godfather to a son of Constantine, probably Indulf (Ildulb mac Constant\u00edn), during the conference.\n\u00c6thelstan followed up his advances in the north by securing the recognition of the Welsh kings. For the next seven years, the record of events in the north is blank. \u00c6thelstan's court was attended by the Welsh kings, but not by Constantine or Owain. This absence of record means that \u00c6thelstan's reasons for marching north against Constantine in 934 are unclear.\n\u00c6thelstan's invasion is reported in brief by the \"Anglo-Saxon Chronicle\", and later chroniclers such as John of Worcester, William of Malmesbury, Henry of Huntingdon and Symeon of Durham add detail to that bald account. \u00c6thelstan's army began gathering at Winchester by 28 May 934 and travelled north to Nottingham by 7 June. He was accompanied by many leaders, including the Welsh kings Hywel Dda, Idwal Foel and Morgan ab Owain. From Mercia, the army continued to Chester-le-Street, before resuming the march accompanied by a fleet of ships. Owain was defeated and Symeon states that the army went as far north as Dunnottar and Fortriu, while the fleet is said to have raided Caithness, by which a much larger area, including Sutherland, is probably intended. It is unlikely that Constantine's personal authority extended so far north, so the attacks were probably directed at his allies, comprising simple looting expeditions.\nThe \"Annals of Clonmacnoise\" state that \"the Scottish men compelled [\u00c6thelstan] to return without any great victory\", while Henry of Huntingdon claims that the English faced no opposition. A negotiated settlement might have ended matters: according to John of Worcester, a son of Constantine was given as a hostage to \u00c6thelstan and Constantine himself accompanied the English king on his return south. He witnessed a charter with \u00c6thelstan at Buckingham on 13 September 934 in which he is described as \"subregulus\", \"i.e.\", a king acknowledging \u00c6thelstan's overlordship, the only place there is any record of such a description. However, there is no record of Constantine having ever submitted to \u00c6thelstan's overlordship or that he considered himself such. The following year, Constantine was again in England at \u00c6thelstan's court, this time at Cirencester where he appears as a witness, as the first of several kings, followed by Owain and Hywel Dda, who subscribed to the diploma. At Christmas of 935, Owain was once more at \u00c6thelstan's court along with the Welsh kings, but Constantine was not. His return to England less than two years later would be in very different circumstances.\nBrunanburh and after.\nFollowing his departure from \u00c6thelstan's court after 935, there is no further report of Constantine until 937. In that year, together with Owain and Olaf Guthfrithson of Dublin, Constantine invaded England. The resulting battle of Brunanburh \u2014 \"D\u00fan Brunde\" \u2014 is reported in the \"Annals of Ulster\" as follows:\n The battle was remembered in England a generation later as \"the Great Battle\". When reporting the battle, the \"Anglo-Saxon Chronicle\" abandons its usual terse style in favour of a heroic poem vaunting the great victory. In this, the \"hoary\" Constantine, by now around 60 years of age, is said to have lost a son in the battle, a claim which the \"Chronicle of the Kings of Alba\" confirms. The \"Annals of Clonmacnoise\" give his name as Cellach. For all its fame, the site of the battle is uncertain and several sites have been advanced, with Bromborough on the Wirral the most favoured location.\nBrunanburh, for all that it had been a famous and bloody battle, settled nothing. On 27 October 939 \u00c6thelstan, the \"pillar of the dignity of the western world\" in the words of the \"Annals of Ulster\", died at Malmesbury. He was succeeded by his brother Edmund, then aged 18. \u00c6thelstan's realm, seemingly made safe by the victory of Brunanburh, collapsed in little more than a year from his death when Amla\u00edb returned from Ireland and seized Northumbria and the Mercian Danelaw. Edmund spent the remainder of Constant\u00edn's reign rebuilding his kingdom.\nFor Constantine's last years as king, there is only the meagre record of the \"Chronicle of the Kings of Alba\". The death of \u00c6thelstan is reported, as are two others. The first of these, in 938, is that of Dubacan, mormaer of Angus or son of the mormaer. Unlike the report of 918, on this occasion, the title mormaer is attached to a geographical area, but it is unknown whether the Angus of 938 was in any way similar to the later mormaerdom or earldom. The second death entered with that of \u00c6thelstan, is that of Eochaid mac Ailp\u00edn, who might, from his name, have been a kinsman of Constant\u00edn.\nAbdication and posterity.\nBy the early 940s, Constantine was an old man in his late sixties or seventies. The kingdom of Alba was too new to be said to have a customary rule of succession, but Pictish and Irish precedents favoured an adult successor descended from Kenneth MacAlpin. Constantine's surviving son Indulf, probably baptised in 927, would have been too young to be a serious candidate for the kingship in the early 940s, and the obvious heir was Constantine's nephew, Malcolm I. As Malcolm was born no later than 901, by the 940s he was no longer a young man and may have been impatient. Willingly or not \u2014 the 11th century \"The Prophecy of Berch\u00e1n\", a verse history in the form of a supposed prophecy, states that it was not a voluntary decision \u2014 Constantine abdicated in 943 and entered a monastery, leaving the kingdom to Malcolm.\nAlthough his retirement might have been involuntary, the \"Life\" of Cathr\u00f3e of Metz and \"The Prophecy of Berch\u00e1n\" portray Constantine as a devout king. The monastery to which Constantine retired, and where he is said to have been abbot, was probably that of St Andrews. This had been refounded in his reign and given to the reforming C\u00e9li D\u00e9 (Culdee) movement. The C\u00e9li D\u00e9 were subsequently to be entrusted with many monasteries throughout the kingdom of Alba until replaced in the 12th century by new orders imported from France.\nSeven years later the \"Chronicle of the Kings of Alba\" says:\n Woolf suggests that the association of Constantine with the raid is a late addition, one derived from a now-lost saga or poem.\nConstantine's death in 952 is recorded by the Irish annals, who enter it among ecclesiastics. His son Indulf would become king on Malcolm's death. The last of Constantine's certain descendants to be king in Alba was a great-grandson, Constantine III (Constant\u00edn mac Cuil\u00e9in). Another son had died at Brunanburh and according to John of Worcester, Amla\u00edb mac Gofraid was married to a daughter of Constantine. It is possible that Constantine had other children, but like the name of his wife, or wives, this has not been recorded.\nThe form of kingdom which appeared in Constantine's reign continued in much the same way until the Davidian Revolution in the 12th century. As with his ecclesiastical reforms, his political legacy was the creation of a new form of Scottish kingship that lasted for two centuries after his death.\nFamily.\nThe name of Constantine's wife is not known, however, they are known to have had at least 3 children:"}
{"id": "7236", "revid": "38596647", "url": "https://en.wikipedia.org/wiki?curid=7236", "title": "Constantine the Great", "text": "Constantine I (Latin: Flavius Valerius Constantinus; 27 February 22 May 337), also known as Constantine the Great, was a Roman emperor from AD\u00a0306 to 337 and the first Roman emperor to convert to Christianity. He played a pivotal role in elevating the status of Christianity in Rome, decriminalizing Christian practice and ceasing Christian persecution. This was a turning point the Christianization of the Roman Empire. He founded the city of Constantinople and made it the capital of the Empire, which it remained for over a millennium.\nBorn in \"Naissus\", in Dardania within Moesia Superior (now Ni\u0161, Serbia), Constantine was the son of Flavius Constantius, a Roman army officer of Illyrian origin who would become one of the four emperors of the Tetrarchy. His mother, Helena, was a woman of low birth, probably from Asia Minor in modern Turkey. Later canonised as a saint, she is credited for the conversion of her son in some traditions, though others believe that Constantine converted her. Constantine served with distinction under the Roman emperors Diocletian and Galerius. He began his career by campaigning in the eastern provinces (against the Persians) before being recalled in the west (in AD\u00a0305) to fight alongside his father in the province of Britannia. After his father's death in 306, Constantine was proclaimed as \"augustus\" (emperor) by his army at Eboracum (York, England). He eventually emerged victorious in the civil wars against emperors Maxentius and Licinius to become the sole ruler of the Roman Empire by 324.\nUpon his ascension, Constantine enacted numerous reforms to strengthen the empire. He restructured the government, separating civil and military authorities. To combat inflation, he introduced the solidus, a new gold coin that became the standard for Byzantine and European currencies for more than a thousand years. The Roman army was reorganised to consist of mobile units (), often around the Emperor, to serve on campaigns against external enemies or Roman rebels, and frontier-garrison troops () which were capable of countering barbarian raids, but less and less capable, over time, of countering full-scale barbarian invasions. Constantine pursued successful campaigns against the tribes on the Roman frontiers\u2014such as the Franks, the Alemanni, the Goths, and the Sarmatians\u2014and resettled territories abandoned by his predecessors during the Crisis of the Third Century with citizens of Roman culture.\nAlthough Constantine lived much of his life as a pagan and later as a catechumen, he began to favour Christianity beginning in 312, finally becoming a Christian and being baptised by Eusebius of Nicomedia, an Arian bishop, although the Catholic Church and the Coptic Orthodox Church maintain that he was baptised by Pope Sylvester I. He played an influential role in the proclamation of the Edict of Milan in 313, which declared tolerance for Christianity in the Roman Empire. He convoked the First Council of Nicaea in 325 which produced the statement of Christian belief known as the Nicene Creed. The Church of the Holy Sepulchre was built on his orders at the claimed site of Jesus' tomb in Jerusalem and was deemed the holiest place in all of Christendom. The papal claim to temporal power in the High Middle Ages was based on the fabricated Donation of Constantine. He has historically been referred to as the \"First Christian Emperor\", but while he did favour the Christian Church, some modern scholars debate his beliefs and even his comprehension of Christianity. Nevertheless, he is venerated as a saint in Eastern Christianity, and he did much to push Christianity towards the mainstream of Roman culture.\nThe age of Constantine marked a distinct epoch in the history of the Roman Empire and a pivotal moment in the transition from classical antiquity to the Middle Ages. He built a new imperial residence in the city of Byzantium and renamed it New Rome, later adopting the name Constantinople after himself, where it was located in modern Istanbul. It subsequently became the capital of the empire for more than a thousand years, the later Eastern Roman Empire often being referred to in English as the \"Byzantine Empire\", a term never used by the Empire, invented by German historian Hieronymus Wolf. His more immediate political legacy was that he replaced Diocletian's Tetrarchy with the \"de facto\" principle of dynastic succession by leaving the empire to his sons and other members of the Constantinian dynasty. His reputation flourished during the lifetime of his children and for centuries after his reign. The medieval church held him up as a paragon of virtue, while secular rulers invoked him as a prototype, a point of reference, and the symbol of imperial legitimacy and identity. At the beginning of the Renaissance, there were more critical appraisals of his reign with the rediscovery of anti-Constantinian sources. Trends in modern and recent scholarship have attempted to balance the extremes of previous scholarship.\nSources.\nConstantine was a ruler of major importance and has always been a controversial figure. The fluctuations in his reputation reflect the nature of the ancient sources for his reign. These are abundant and detailed, but they have been strongly influenced by the official propaganda of the period and are often one-sided; no contemporaneous histories or biographies dealing with his life and rule have survived. The nearest replacement is Eusebius's \"Vita Constantini\"\u2014a mixture of eulogy and hagiography written between 335 and \u2014that extols Constantine's moral and religious virtues. The \"Vita\" creates a contentiously positive image of Constantine, and modern historians have frequently challenged its reliability. The fullest secular life of Constantine is the anonymous \"Origo Constantini\", a work of uncertain date which focuses on military and political events to the neglect of cultural and religious matters.\nLactantius' \"De mortibus persecutorum\", a political Christian pamphlet on the reigns of Diocletian and the Tetrarchy, provides valuable but tendentious detail on Constantine's predecessors and early life. The ecclesiastical histories of Socrates, Sozomen, and Theodoret describe the ecclesiastic disputes of Constantine's later reign. Written during the reign of Theodosius II (r. 402\u2013450), a century after Constantine's reign, these ecclesiastical historians obscure the events and theologies of the Constantinian period through misdirection, misrepresentation, and deliberate obscurity. The contemporary writings of the orthodox Christian Athanasius and the ecclesiastical history of the Arian Philostorgius also survive, though their biases are no less firm.\nThe epitomes of Aurelius Victor (\"De Caesaribus\"), Eutropius (\"Breviarium\"), Festus (\"Breviarium\"), and the anonymous author of the \"Epitome de Caesaribus\" offer compressed secular political and military histories of the period. Although not Christian, the epitomes paint a favourable image of Constantine but omit reference to Constantine's religious policies. The \"Panegyrici Latini\", a collection of panegyrics from the late 3rd and early 4th centuries, provides valuable information on the politics and ideology of the tetrarchic period and the early life of Constantine. Contemporary architecture\u2014such as the Arch of Constantine in Rome and palaces in Gamzigrad and C\u00f3rdoba\u2014epigraphic remains, and the coinage of the era complement the literary sources.\nEarly life.\nConstantine was born on 27 February, in the city of \"Naissus\", a time where the unity of the Empire was threatened by the breakaway wars of the Palmyrene Empire. The city\u2014which is modern day Ni\u0161 in Serbia\u2014was located in Dardania within Moesia Superior. His father was Flavius Constantius an Illyrian who was born in the same region, and a native of the province of Moesia. His original full name, as well as that of his father, is not known. His \"praenomen\" is variously given as Lucius, Marcus and Gaius. Whatever the case, \"praenomina\" had already disappeared from most public records by this time. He also adopted the name \"Valerius\", the \"nomen\" of emperor Diocletian, following his father's ascension as caesar.\nConstantine probably spent little time with his father who was an officer in the Roman army, part of Emperor Aurelian's imperial bodyguard. Being described as a tolerant and politically skilled man, Constantius advanced through the ranks, earning the governorship of Dalmatia from Emperor Diocletian, another of Aurelian's companions from Illyricum, in 284 or 285. Constantine's mother was Helena, a woman of low social standing, possibly from Helenopolis of Bithynia. It is uncertain whether she was legally married to Constantius or merely his concubine. His main language was Latin, and during his public speeches he needed Greek translators.\nIn April 286, Diocletian declared Maximian, another colleague from Illyricum, his co-emperor. Each emperor would have his own court, his own military and administrative faculties, and each would rule with a separate praetorian prefect as chief lieutenant. Maximian ruled in the West, from his capitals at \"Mediolanum\" (Milan, Italy) or \"Augusta Treverorum\" (Trier, Germany), while Diocletian ruled in the East, from \"Nicomedia\" (\u0130zmit, Turkey). The division was merely pragmatic: the empire was called \"indivisible\" in official panegyric, and both emperors could move freely throughout the empire. In 288, Maximian appointed Constantius to serve as his praetorian prefect in Gaul. Constantius left Helena to marry Maximian's stepdaughter Theodora in 288 or 289.\nDiocletian divided the empire again in 293, appointing two caesars to rule over further subdivisions of East and West. Each would be subordinate to his respective \"augustus\" but would act with supreme authority in his assigned lands. This system would later be called the Tetrarchy. Diocletian's first appointee for the office of Caesar was Constantius; his second was Galerius, a native of \"Felix Romuliana\". According to Lactantius, Galerius was a brutal, animalistic man. Although he shared the paganism of Rome's aristocracy, he seemed to them an alien figure, a semi-barbarian. On 1 March, Constantius was promoted to the office of Caesar, and dispatched to Gaul to fight the rebels Carausius and Allectus. In spite of meritocratic overtones, the Tetrarchy retained vestiges of hereditary privilege, and Constantine became the prime candidate for future appointment as Caesar as soon as his father took the position. Constantine went to the court of Diocletian, where he lived as his father's heir presumptive.\nIn the East.\nConstantine received a formal education at Diocletian's court, where he learned Latin literature, Greek, and philosophy. The cultural environment in \"Nicomedia\" was open, fluid, and socially mobile; in it, Constantine could mix with intellectuals both pagan and Christian. He may have attended the lectures of Lactantius, a Christian scholar of Latin in the city. Because Diocletian did not completely trust Constantius\u2014none of the Tetrarchs fully trusted their colleagues\u2014Constantine was held as something of a hostage, a tool to ensure Constantius' best behavior. Constantine was nonetheless a prominent member of the court: he fought for Diocletian and Galerius in Asia and served in a variety of tribunates; he campaigned against barbarians on the Danube in 296 and fought the Persians under Diocletian in Syria in 297, as well as under Galerius in Mesopotamia in 298\u2013299. By late 305, he had become a tribune of the first order, a \"tribunus ordinis primi\".\nConstantine had returned to \"Nicomedia\" from the eastern front by the spring of 303, in time to witness the beginnings of Diocletian's \"Great Persecution\", the most severe persecution of Christians in Roman history. In late 302, Diocletian and Galerius sent a messenger to the oracle of Apollo at Didyma with an inquiry about Christians. Constantine could recall his presence at the palace when the messenger returned and Diocletian accepted the imperial court's demands for universal persecution. On 23 February 303, Diocletian ordered the destruction of \"Nicomedia\"'s new church, condemned its scriptures to the flames, and had its treasures seized. In the months that followed, churches and scriptures were destroyed, Christians were deprived of official ranks, and priests were imprisoned. It is unlikely that Constantine played any role in the persecution. In his later writings, he attempted to present himself as an opponent of Diocletian's \"sanguinary edicts\" against the \"Worshippers of God\", but nothing indicates that he opposed it effectively at the time. Although no contemporary Christian challenged Constantine for his inaction during the persecutions, it remained a political liability throughout his life.\nOn 1 May 305, Diocletian, as a result of a debilitating sickness taken in the winter of 304\u2013305, announced his resignation. In a parallel ceremony in Milan, Maximian did the same. Lactantius states that Galerius manipulated the weakened Diocletian into resigning and forced him to accept Galerius' allies in the imperial succession. According to Lactantius, the crowd listening to Diocletian's resignation speech believed, until the last moment, that Diocletian would choose Constantine and Maxentius (Maximian's son) as his successors. It was not to be: Constantius and Galerius were promoted to \"augusti\", while Severus and Maximinus, Galerius' nephew, were appointed their caesars respectively. Constantine and Maxentius were ignored.\nSome of the ancient sources detail plots that Galerius made on Constantine's life in the months following Diocletian's abdication. They assert that Galerius assigned Constantine to lead an advance unit in a cavalry charge through a swamp on the middle Danube, made him enter into single combat with a lion, and attempted to kill him in hunts and wars. Constantine always emerged victorious: the lion emerged from the contest in a poorer condition than Constantine; Constantine returned to \"Nicomedia\" from the Danube with a Sarmatian captive to drop at Galerius' feet. It is uncertain how much these tales can be trusted.\nIn the West.\nConstantine recognised the implicit danger in remaining at Galerius' court, where he was held as a virtual hostage. His career depended on being rescued by his father in the West. Constantius was quick to intervene. In the late spring or early summer of 305, Constantius requested leave for his son to help him campaign in Britain. After a long evening of drinking, Galerius granted the request. Constantine's later propaganda describes how he fled the court in the night, before Galerius could change his mind. He rode from post-house to post-house at high speed, hamstringing every horse in his wake. By the time Galerius awoke the following morning, Constantine had fled too far to be caught. Constantine joined his father in Gaul, at \"Bononia\" (Boulogne) before the summer of 305.\nFrom Bononia, they crossed the English Channel to Britain and made their way to \"Eboracum\" (York), capital of the province of Britannia Secunda and home to a large military base. Constantine was able to spend a year in northern Britain at his father's side, campaigning against the Picts beyond Hadrian's Wall in the summer and autumn. Constantius' campaign, like that of Septimius Severus before it, probably advanced far into the north without achieving great success. Constantius had become severely sick over the course of his reign and died on 25 July 306 in \"Eboracum\". Before dying, he declared his support for raising Constantine to the rank of full Augustus. The Alamannic king Chrocus, a barbarian taken into service under Constantius, then proclaimed Constantine as augustus. The troops loyal to Constantius' memory followed him in acclamation. Gaul and Britain quickly accepted his rule; Hispania, which had been in his father's domain for less than a year, rejected it.\nConstantine sent Galerius an official notice of Constantius' death and his own acclamation. Along with the notice, he included a portrait of himself in the robes of an Augustus. The portrait was wreathed in bay. He requested recognition as heir to his father's throne and passed off responsibility for his unlawful ascension on his army, claiming they had \"forced it upon him\". Galerius was put into a fury by the message; he almost set the portrait and messenger on fire. His advisers calmed him and argued that outright denial of Constantine's claims would mean certain war. Galerius was compelled to compromise: he granted Constantine the title \"caesar\" rather than \"augustus\" (the latter office went to Severus instead). Wishing to make it clear that he alone gave Constantine legitimacy, Galerius personally sent Constantine the emperor's traditional purple robes. Constantine accepted the decision, knowing that it would remove doubts as to his legitimacy.\nReign.\nConstantine's share of the empire consisted of Britain, Gaul, and Spain, and he commanded one of the largest Roman armies which was stationed along the important Rhine frontier. He remained in Britain after his promotion to emperor, driving back the tribes of the Picts and securing his control in the northwestern dioceses. He completed the reconstruction of military bases begun under his father's rule, and he ordered the repair of the region's roadways. He then left for \"Augusta Treverorum\" (Trier) in Gaul, the Tetrarchic capital of the northwestern Roman Empire. The Franks learned of Constantine's acclamation and invaded Gaul across the lower Rhine over the winter of 306\u2013307. He drove them back beyond the Rhine and captured kings Ascaric and Merogais; the kings and their soldiers were fed to the beasts of Trier Amphitheater in the \"adventus\" (arrival) celebrations which followed.\nConstantine began a major expansion of Trier. He strengthened the circuit wall around the city with military towers and fortified gates, and he began building a palace complex in the northeastern part of the city. To the south of his palace, he ordered the construction of a large formal audience hall and a massive imperial bathhouse. He sponsored many building projects throughout Gaul during his tenure as emperor of the West, especially in Augustodunum (Autun) and Arelate (Arles). According to Lactantius, Constantine followed a tolerant policy towards Christianity, although he was not yet a Christian. He probably judged it a more sensible policy than open persecution and a way to distinguish himself from the \"great persecutor\" Galerius. He decreed a formal end to persecution and returned to Christians all that they had lost under the first of the persecuting edicts.\nConstantine was largely untried and had a hint of illegitimacy about him; he relied on his father's reputation in his early propaganda, which gave as much coverage to his father's deeds as to his. His military skill and building projects, however, soon gave the panegyrist the opportunity to comment favourably on the similarities between father and son, and Eusebius remarked that Constantine was a \"renewal, as it were, in his own person, of his father's life and reign\". Constantinian coinage, sculpture, and oratory also show a tendency for disdain towards the \"barbarians\" beyond the frontiers. He minted a coin issue after his victory over the Alemanni which depicts weeping and begging Alemannic tribesmen, \"the Alemanni conquered\" beneath the phrase \"Romans' rejoicing\". There was little sympathy for these enemies; as his panegyrist declared, \"It is a stupid clemency that spares the conquered foe.\"\nMaxentius' rebellion.\nFollowing Galerius' recognition of Constantine as caesar, Constantine's portrait was brought to Rome, as was customary. Maxentius mocked the portrait's subject as the son of a harlot and lamented his own powerlessness. Maxentius, envious of Constantine's authority, seized the title of emperor on 28 October 306. Galerius refused to recognize him but failed to unseat him. Severus was sent against Maxentius in April 307, but during the campaign, Severus' armies, previously under command of Maxentius' father Maximian, defected, and Severus was seized and imprisoned. Maximian, brought out of retirement by his son's rebellion, left for Gaul to confer with Constantine. He offered to marry his daughter Fausta to Constantine and elevate him to augustan rank. In return, Constantine would reaffirm the old family alliance between Maximian and Constantius and offer support to Maxentius' cause in Italy. Constantine accepted and married Fausta in Trier in summer 307. Constantine gave Maxentius his meagre support, offering Maxentius political recognition.\nConstantine remained aloof from the Italian conflict, however. Over the spring and summer of 307, he had left Gaul for Britain to avoid any involvement in the Italian turmoil; now, instead of giving Maxentius military aid, he sent his troops against Germanic tribes along the Rhine. In 308, he raided the territory of the Bructeri and made a bridge across the Rhine at Colonia Agrippinensium (Cologne). In 310, he marched to the northern Rhine and fought the Franks. When not campaigning, he toured his lands advertising his benevolence and supporting the economy and the arts. His refusal to participate in the war increased his popularity among his people and strengthened his power base in the West. Maximian returned to Rome in the winter of 307\u2013308 but soon fell out with his son. In early 308, after a failed attempt to usurp Maxentius' title, Maximian returned to Constantine's court.\nOn 11 November 308, Galerius called a general council at the military city of Carnuntum (Petronell-Carnuntum, Austria) to resolve the instability in the western provinces. In attendance were Diocletian, briefly returned from retirement, Galerius, and Maximian. Maximian was forced to abdicate again and Constantine was again demoted to caesar. Licinius, one of Galerius' old military companions, was appointed augustus in the western regions. The new system did not last long: Constantine refused to accept the demotion and continued to style himself as augustus on his coinage, even as other members of the Tetrarchy referred to him as a caesar on theirs. Maximinus was frustrated that he had been passed over for promotion while the newcomer Licinius had been raised to the office of augustus and demanded that Galerius promote him. Galerius offered to call both Maximinus and Constantine \"sons of the augusti\", but neither accepted the new title. By the spring of 310, Galerius was referring to both men as augusti.\nMaximian's rebellion.\nIn 310, a dispossessed Maximian rebelled against Constantine while Constantine was away campaigning against the Franks. Maximian had been sent south to Arles with a contingent of Constantine's army, in preparation for any attacks by Maxentius in southern Gaul. He announced that Constantine was dead and took up the imperial purple. In spite of a large donative pledge to any who would support him as emperor, most of Constantine's army remained loyal to their emperor, and Maximian was soon compelled to leave. When Constantine heard of the rebellion, he abandoned his campaign against the Franks and marched his army up the Rhine. At Cabillunum (Chalon-sur-Sa\u00f4ne), he moved his troops onto waiting boats to row down the slow waters of the Sa\u00f4ne to the quicker waters of the Rhone. He disembarked at Lugdunum (Lyon). Maximian fled to Massilia (Marseille), a town better able to withstand a long siege than Arles. It made little difference, however, as loyal citizens opened the rear gates to Constantine. Maximian was captured and reproved for his crimes. Constantine granted some clemency but strongly encouraged his suicide. In July 310, Maximian hanged himself.\nIn spite of the earlier rupture in their relations, Maxentius was eager to present himself as his father's devoted son after his death. He began minting coins with his father's deified image, proclaiming his desire to avenge Maximian's death. Constantine initially presented the suicide as an unfortunate family tragedy. By 311, however, he was spreading another version. According to this, after Constantine had pardoned him, Maximian planned to murder Constantine in his sleep. Fausta learned of the plot and warned Constantine, who put a eunuch in his own place in bed. Maximian was apprehended when he killed the eunuch and was offered suicide, which he accepted. Along with using propaganda, Constantine instituted a \"damnatio memoriae\" on Maximian, destroying all inscriptions referring to him and eliminating any public work bearing his image.\nThe death of Maximian required a shift in Constantine's public image. He could no longer rely on his connection to the elder Emperor Maximian and needed a new source of legitimacy. In a speech delivered in Gaul on 25 July 310, the anonymous orator reveals a previously unknown dynastic connection to Claudius II, a 3rd-century emperor famed for defeating the Goths and restoring order to the empire. Breaking away from tetrarchic models, the speech emphasizes Constantine's ancestral prerogative to rule, rather than principles of imperial equality. The new ideology expressed in the speech made Galerius and Maximian irrelevant to Constantine's right to rule. Indeed, the orator emphasizes ancestry to the exclusion of all other factors: \"No chance agreement of men, nor some unexpected consequence of favour, made you emperor,\" the orator declares to Constantine.\nThe oration also moves away from the religious ideology of the Tetrarchy, with its focus on twin dynasties of Jupiter and Hercules. Instead, the orator proclaims that Constantine experienced a divine vision of Apollo and Victory granting him laurel wreaths of health and a long reign. In the likeness of Apollo, Constantine recognised himself as the saving figure to whom would be granted \"rule of the whole world\", as the poet Virgil had once foretold. The oration's religious shift is paralleled by a similar shift in Constantine's coinage. In his early reign, the coinage of Constantine advertised Mars as his patron. From 310 on, Mars was replaced by Sol Invictus, a god conventionally identified with Apollo. There is little reason to believe that either the dynastic connection or the divine vision are anything other than fiction, but their proclamation strengthened Constantine's claims to legitimacy and increased his popularity among the citizens of Gaul.\nCivil wars.\nWar against Maxentius.\nBy the middle of 310, Galerius had become too ill to involve himself in imperial politics. His final act survives: a letter to provincials posted in \"Nicomedia\" on 30 April 311, proclaiming an end to the persecutions, and the resumption of religious toleration.\nEusebius maintains \"divine providence [...] took action against the perpetrator of these crimes\" and gives a graphic account of Galerius' demise:\n\"Without warning suppurative inflammation broke out round the middle of his genitals, then a deep-seated fistula ulcer; these ate their way incurably into his innermost bowels. From them came a teeming indescribable mass of worms, and a sickening smell was given off, for the whole of his hulking body, thanks to over eating, had been transformed even before his illness into a huge lump of flabby fat, which then decomposed and presented those who came near it with a revolting and horrifying sight.\"\nGalerius died soon after the edict's proclamation, destroying what little remained of the Tetrarchy. Maximinus mobilised against Licinius and seized Asia Minor. A hasty peace was signed on a boat in the middle of the Bosphorus. While Constantine toured Britain and Gaul, Maxentius prepared for war. He fortified northern Italy and strengthened his support in the Christian community by allowing it to elect Eusebius as bishop of Rome.\nMaxentius' rule was nevertheless insecure. His early support dissolved in the wake of heightened tax rates and depressed trade; riots broke out in Rome and Carthage; and Domitius Alexander was able to briefly usurp his authority in Africa. By 312, he was a man barely tolerated, not one actively supported, even among Christian Italians. In the summer of 311, Maxentius mobilised against Constantine while Licinius was occupied with affairs in the East. He declared war on Constantine, vowing to avenge his father's \"murder\". To prevent Maxentius from forming an alliance against him with Licinius, Constantine forged his own alliance with Licinius over the winter of 311\u2013312 and offered him his sister Constantia in marriage. Maximinus considered Constantine's arrangement with Licinius an affront to his authority. In response, he sent ambassadors to Rome, offering political recognition to Maxentius in exchange for a military support, which Maxentius accepted. According to Eusebius, inter-regional travel became impossible, and there was military buildup everywhere. There was \"not a place where people were not expecting the onset of hostilities every day\".\nConstantine's advisers and generals cautioned against preemptive attack on Maxentius; even his soothsayers recommended against it, stating that the sacrifices had produced unfavourable omens. Constantine, with a spirit that left a deep impression on his followers, inspiring some to believe that he had some form of supernatural guidance, ignored all these cautions. Early in the spring of 312, Constantine crossed the Cottian Alps with a quarter of his army, a force numbering about 40,000. The first town his army encountered was \"Segusium\" (Susa, Italy), a heavily fortified town that shut its gates to him. Constantine ordered his men to set fire to its gates and scale its walls. He took the town quickly. Constantine ordered his troops not to loot the town and advanced into northern Italy.\nAt the approach to the west of the important city of Augusta Taurinorum (Turin, Italy), Constantine met a large force of heavily armed Maxentian cavalry. In the ensuing Battle of Turin Constantine's army encircled Maxentius' cavalry, flanked them with his own cavalry, and dismounted them with blows from his soldiers' iron-tipped clubs. Constantine's armies emerged victorious. Turin refused to give refuge to Maxentius' retreating forces, opening its gates to Constantine instead. Other cities of the north Italian plain sent Constantine embassies of congratulation for his victory. He moved on to Milan, where he was met with open gates and jubilant rejoicing. Constantine rested his army in Milan until mid-summer 312, when he moved on to Brixia (Brescia).\nBrescia's army was easily dispersed, and Constantine quickly advanced to Verona where a large Maxentian force was camped. Ruricius Pompeianus, general of the Veronese forces and Maxentius' praetorian prefect, was in a strong defensive position since the town was surrounded on three sides by the Adige. Constantine sent a small force north of the town in an attempt to cross the river unnoticed. Ruricius sent a large detachment to counter Constantine's expeditionary force but was defeated. Constantine's forces successfully surrounded the town and laid siege. Ruricius gave Constantine the slip and returned with a larger force to oppose Constantine. Constantine refused to let up on the siege and sent only a small force to oppose him. In the desperately fought encounter that followed, Ruricius was killed and his army destroyed. Verona surrendered soon afterwards, followed by Aquileia, Mutina (Modena), and Ravenna. The road to Rome was now wide open to Constantine.\nMaxentius prepared for the same type of war he had waged against Severus and Galerius: he occupied Rome and prepared for a siege. He still controlled Rome's Praetorian Guard, was well-stocked with African grain, and was surrounded on all sides by the seemingly impregnable Aurelian Walls. He ordered all bridges across the Tiber cut, reportedly on the counsel of the gods, and left the rest of central Italy undefended; Constantine secured that region's support without challenge. Constantine progressed slowly along the \"Via Flaminia\", allowing the weakness of Maxentius to draw his regime further into turmoil. Maxentius' support continued to weaken: at chariot races on 27 October, the crowd openly taunted Maxentius, shouting that Constantine was invincible. Maxentius, no longer certain that he would emerge from a siege victorious, built a temporary boat bridge across the Tiber in preparation for a field battle against Constantine. On 28 October 312, the sixth anniversary of his reign, he approached the keepers of the Sibylline Books for guidance. The keepers prophesied that, on that very day, \"the enemy of the Romans\" would die. Maxentius advanced north to meet Constantine in battle.\nConstantine adopts the Greek letters Chi Rho for Christ's initials.\nMaxentius' forces were still twice the size of Constantine's, and he organised them in long lines facing the battle plain with their backs to the river. Constantine's army arrived on the field bearing unfamiliar symbols on their standards and their shields. According to Lactantius \"Constantine was directed in a dream to cause the heavenly sign to be delineated on the shields of his soldiers, and so to proceed to battle. He did as he had been commanded, and he marked on their shields the letter \u03a7, with a perpendicular line drawn through it and turned round thus at the top, being the cipher of Christ. Having this sign (\u2627), his troops stood to arms.\" Eusebius describes a vision that Constantine had while marching at midday in which \"he saw with his own eyes the trophy of a cross of light in the heavens, above the sun, and bearing the inscription, \"In Hoc Signo Vinces\"\" (\"In this sign thou shalt conquer\"). In Eusebius's account, Constantine had a dream the following night in which Christ appeared with the same heavenly sign and told him to make an army standard in the form of the \"labarum\". Eusebius is vague about when and where these events took place, but it enters his narrative before the war begins against Maxentius. He describes the sign as Chi (\u03a7) traversed by Rho (\u03a1) to form \u2627, representing the first two letters of the Greek word (Christos). A medallion was issued at Ticinum in 315 which shows Constantine wearing a helmet emblazoned with the \"Chi Rho\", and coins issued at Siscia in 317/318 repeat the image. The figure was otherwise rare and is uncommon in imperial iconography and propaganda before the 320s. It was not completely unknown, however, being an abbreviation of the Greek word chr\u0113ston (good), having previously appeared on the coins of Ptolemy III Euergetes in the 3rd century BC. Following Constantine, centuries of Christians invoked the miraculous or the supernatural when justifying or describing their warfare.\nConstantine deployed his own forces along the whole length of Maxentius' line. He ordered his cavalry to charge, and they broke Maxentius' cavalry. He then sent his infantry against Maxentius' infantry, pushing many into the Tiber where they were slaughtered and drowned. The battle was brief, and Maxentius' troops were broken before the first charge. His horse guards and praetorians initially held their position, but they broke under the force of a Constantinian cavalry charge; they also broke ranks and fled to the river. Maxentius rode with them and attempted to cross the bridge of boats (Ponte Milvio), but he was pushed into the Tiber and drowned by the mass of his fleeing soldiers.\nIn Rome.\nConstantine entered Rome on 29 October 312 and staged a grand \"adventus\" in the city which was met with jubilation. Maxentius' body was fished out of the Tiber and decapitated, and his head was paraded through the streets for all to see. After the ceremonies, the disembodied head was sent to Carthage, and Carthage offered no further resistance. Unlike his predecessors, Constantine neglected to make the trip to the Capitoline Hill and perform customary sacrifices at the Temple of Jupiter. However, he did visit the Senatorial Curia Julia, and he promised to restore its ancestral privileges and give it a secure role in his reformed government; there would be no revenge against Maxentius' supporters. In response, the Senate decreed him \"title of the first name\", which meant that his name would be listed first in all official documents, and they acclaimed him as \"the greatest augustus\". He issued decrees returning property that was lost under Maxentius, recalling political exiles, and releasing Maxentius' imprisoned opponents.\nAn extensive propaganda campaign followed, during which Maxentius' image was purged from all public places. He was written up as a \"tyrant\" and set against an idealised image of Constantine the \"liberator\". Eusebius is the best representative of this strand of Constantinian propaganda. Maxentius' rescripts were declared invalid, and the honours that he had granted to leaders of the Senate were also invalidated. Constantine also attempted to remove Maxentius' influence on Rome's urban landscape. All structures built by him were rededicated to Constantine, including the Temple of Romulus and the Basilica of Maxentius. At the focal point of the basilica, a stone statue was erected of Constantine holding the Christian \"labarum\" in its hand. Its inscription bore the message which the statue illustrated: \"By this sign, Constantine had freed Rome from the yoke of the tyrant.\"\nConstantine also sought to upstage Maxentius' achievements. For example, the Circus Maximus was redeveloped so that its seating capacity was 25 times larger than that of Maxentius' racing complex on the Via Appia. Maxentius' strongest military supporters were neutralised when he disbanded the Praetorian Guard and Imperial Horse Guard. The tombstones of the Imperial Horse Guard were ground up and used in a basilica on the Via Labicana, and their former base was redeveloped into the Lateran Basilica on 9 November 312\u2014barely two weeks after Constantine captured the city. The Legio II Parthica was removed from Albano Laziale, and the remainder of Maxentius' armies were sent to do frontier duty on the Rhine.\nWars against Licinius.\nIn the following years, Constantine gradually consolidated his military superiority over his rivals in the crumbling Tetrarchy. In 313, he met Licinius in Milan to secure their alliance by the marriage of Licinius and Constantine's half-sister Constantia. During this meeting, the emperors agreed on the so-called Edict of Milan, officially granting full tolerance to Christianity and all religions in the empire. The document had special benefits for Christians, legalizing their religion and granting them restoration for all property seized during Diocletian's persecution. It repudiates past methods of religious coercion and used only general terms to refer to the divine sphere\u2014\"Divinity\" and \"Supreme Divinity\", \"summa divinitas\". The conference was cut short, however, when news reached Licinius that his rival Maximinus had crossed the Bosporus and invaded European territory. Licinius departed and eventually defeated Maximinus, gaining control over the entire eastern half of the Roman Empire. Relations between the two remaining emperors deteriorated, as Constantine suffered an assassination attempt at the hands of a character that Licinius wanted elevated to the rank of Caesar; Licinius, for his part, had Constantine's statues in Emona destroyed. In either 314 or 316, the two augusti fought against one another at the Battle of Cibalae, with Constantine being victorious. They clashed again at the Battle of Mardia in 317 and agreed to a settlement in which Constantine's sons Crispus and Constantine II, and Licinius' son Licinius Junior were made \"caesars\". After this arrangement, Constantine ruled the dioceses of Pannonia and Macedonia and took residence at Sirmium, whence he could wage war on the Goths and Sarmatians in 322, and on the Goths in 323, defeating and killing their leader Rausimod.\nIn 320, Licinius allegedly reneged on the religious freedom promised by the Edict of Milan and began to oppress Christians anew, generally without bloodshed, but resorting to confiscations and sacking of Christian office-holders. Although this characterization of Licinius as anti-Christian is somewhat doubtful, the fact is that he seems to have been far less open in his support of Christianity than Constantine. Therefore, Licinius was prone to see the Church as a force more loyal to Constantine than to the Imperial system in general, as the explanation offered by the Church historian Sozomen.\nThis dubious arrangement eventually became a challenge to Constantine in the West, climaxing in the great civil war of 324. Constantine's Christian eulogists present the war as a battle between Christianity and paganism; Licinius, aided by Gothic mercenaries, represented the past and ancient paganism, while Constantine and his Franks marched under the standard of the \"labarum\". Outnumbered but fired by their zeal, Constantine's army emerged victorious in the Battle of Adrianople. Licinius fled across the Bosphorus and appointed Martinian, his \"magister officiorum\", as nominal augustus in the West, but Constantine next won the Battle of the Hellespont and finally the Battle of Chrysopolis on 18 September 324. Licinius and Martinian surrendered to Constantine at \"Nicomedia\" on the promise their lives would be spared: they were sent to live as private citizens in Thessalonica and Cappadocia respectively, but in 325 Constantine accused Licinius of plotting against him and had them both arrested and hanged; Licinius' son (the son of Constantine's half-sister) was killed in 326. Thus Constantine became the sole emperor of the Roman Empire.\nLater rule.\nFoundation of Constantinople.\nDiocletian had chosen \"Nicomedia\" in the East as his capital during the Tetrarchy\u2014not far from Byzantium, well situated to defend Thrace, Asia, and Egypt, all of which had required his military attention. Constantine had recognised the shift of the empire from the remote and depopulated West to the richer cities of the East, and the military strategic importance of protecting the Danube from barbarian excursions and Asia from a hostile Persia in choosing his new capital as well as being able to monitor shipping traffic between the Black Sea and the Mediterranean. Licinius' defeat came to represent the defeat of a rival centre of pagan and Greek-speaking political activity in the East, as opposed to the Christian and Latin-speaking Rome, and it was proposed that a new Eastern capital should represent the integration of the East into the Roman Empire as a whole, as a centre of learning, prosperity, and cultural preservation for the whole of the Eastern Roman Empire. Among the various locations proposed for this alternative capital, Constantine appears to have toyed earlier with Serdica (present-day Sofia), as he was reported saying that \"Serdica is my Rome\". Sirmium and Thessalonica were also considered. Eventually, however, Constantine decided to work on the Greek city of Byzantium, which offered the advantage of having already been extensively rebuilt on Roman patterns of urbanism during the preceding century by Septimius Severus and Caracalla, who had already acknowledged its strategic importance. The city was thus founded in 324, dedicated on 11 May 330 and renamed \"Constantinopolis\" (\"Constantine's City\" or Constantinople in English). Special commemorative coins were issued in 330 to honor the event. The new city was protected by the relics of the True Cross, the Rod of Moses and other holy relics, though a cameo now at the Hermitage Museum also represented Constantine crowned by the tyche of the new city. The figures of old gods were either replaced or assimilated into a framework of Christian symbolism. Constantine built the new Church of the Holy Apostles on the site of a temple to Aphrodite. Generations later there was the story that a divine vision led Constantine to this spot, and an angel no one else could see led him on a circuit of the new walls. The capital would often be compared to the 'old' Rome as \"Nova Roma Constantinopolitana\", the \"New Rome of Constantinople\".\nReligious policy.\nConstantine was the first emperor to stop the persecution of Christians and to legalize Christianity, along with all other religions/cults in the Roman Empire. In February 313, he met with Licinius in Milan and developed the Edict of Milan, which stated that Christians should be allowed to follow their faith without oppression. This removed penalties for professing Christianity, under which many had been martyred previously, and it returned confiscated Church property. The edict protected all religions from persecution, not only Christianity, allowing anyone to worship any deity that they chose. A similar edict had been issued in 311 by Galerius, senior emperor of the Tetrarchy, which granted Christians the right to practise their religion but did not restore any property to them. The Edict of Milan included several clauses which stated that all confiscated churches would be returned, as well as other provisions for previously persecuted Christians. Some scholars think that Helena adopted Christianity as an adult, and according to Eusebius she was converted by Constantine, but other historians debate whether Constantine adopted his mother Helena's Christianity in his youth or whether he adopted it gradually over the course of his life.\nConstantine possibly retained the title of \"pontifex maximus\" which emperors bore as heads of the ancient Roman religion until Gratian renounced the title. According to Christian writers, Constantine was over 40 when he finally declared himself a Christian, making it clear that he owed his successes to the protection of the Christian High God alone. Despite these declarations of being a Christian, he waited to be baptised on his deathbed, believing that the baptism would release him of any sins he committed in the course of carrying out his policies while emperor. He supported the Church financially, built basilicas, granted privileges to clergy (such as exemption from certain taxes), promoted Christians to high office, and returned property confiscated during the long period of persecution. His most famous building projects include the Church of the Holy Sepulchre and Old St. Peter's Basilica. In constructing the Old St. Peter's Basilica, Constantine went to great lengths to erect the basilica on top of St. Peter's resting place, so much so that it even affected the design of the basilica, including the challenge of erecting it on the hill where St. Peter rested, making its complete construction time over 30 years from the date Constantine ordered it to be built.\nConstantine might not have patronised Christianity alone. A triumphal arch was built in 315 to celebrate his victory in the Battle of the Milvian Bridge which was decorated with images of the goddess Victoria, and sacrifices were made to pagan gods at its dedication, including Apollo, Diana, and Hercules. Absent from the arch are any depictions of Christian symbolism. However, the arch was commissioned by the Senate, so the absence of Christian symbols may reflect the role of the Curia at the time as a pagan redoubt.\nIn 321, he legislated that the \"venerable Sunday\" should be a day of rest for all citizens. In 323, he issued a decree banning Christians from participating in state sacrifices. After the pagan gods had disappeared from his coinage, Christian symbols appeared as Constantine's attributes, the chi rho between his hands or on his labarum, as well on the coinage. The reign of Constantine established a precedent for the emperor to have great influence and authority in the early Christian councils, most notably the dispute over Arianism. Constantine disliked the risks to societal stability that religious disputes and controversies brought with them, preferring to establish an orthodoxy. His influence over the Church councils was to enforce doctrine, root out heresy, and uphold ecclesiastical unity; the Church's role was to determine proper worship, doctrines, and dogma.\nNorth African bishops struggled with Christian bishops who had been ordained by Donatus in opposition to Caecilian from 313 to 316. The African bishops could not come to terms, and the Donatists asked Constantine to act as a judge in the dispute. Three regional Church councils and another trial before Constantine all ruled against Donatus and the Donatism movement in North Africa. In 317, Constantine issued an edict to confiscate Donatist church property and to send Donatist clergy into exile. More significantly, in 325 he summoned the First Council of Nicaea, most known for its dealing with Arianism and for instituting the Nicene Creed. He enforced the council's prohibition against celebrating the Lord's Supper on the day before the Jewish Passover, which marked a definite break of Christianity from the Judaic tradition. From then on, the solar Julian calendar was given precedence over the lunisolar Hebrew calendar among the Christian churches of the Roman Empire.\nConstantine made some new laws regarding the Jews; some of them were unfavourable towards Jews, although they were not harsher than those of his predecessors. It was made illegal for Jews to seek converts or to attack other Jews who had converted to Christianity. They were forbidden to own Christian slaves or to circumcise their slaves. On the other hand, Jewish clergy were given the same exemptions as Christian clergy.\nAdministrative reforms.\nBeginning in the mid-3rd century, the emperors began to favour members of the equestrian order over senators, who had a monopoly on the most important offices of the state. Senators were stripped of the command of legions and most provincial governorships, as it was felt that they lacked the specialised military upbringing needed in an age of acute defense needs; such posts were given to equestrians by Diocletian and his colleagues, following a practice enforced piecemeal by their predecessors. The emperors, however, still needed the talents and the help of the very rich, who were relied on to maintain social order and cohesion by means of a web of powerful influence and contacts at all levels. Exclusion of the old senatorial aristocracy threatened this arrangement.\nIn 326, Constantine reversed this pro-equestrian trend, raising many administrative positions to senatorial rank and thus opening these offices to the old aristocracy; at the same time, he elevated the rank of existing equestrian office-holders to senator, degrading the equestrian order in the process (at least as a bureaucratic rank). The title of \"perfectissimus\" was granted only to mid- or low-level officials by the end of the 4th century.\nBy the new Constantinian arrangement, one could become a senator by being elected praetor or by fulfilling a function of senatorial rank. From then on, holding actual power and social status were melded together into a joint imperial hierarchy. Constantine gained the support of the old nobility with this, as the Senate was allowed to elect praetors and quaestors in place of the usual practice of the emperors directly creating magistrates (\"adlectio\"). An inscription in honor of city prefect Ceionius Rufus Albinus states that Constantine had restored the Senate \"the \"auctoritas\" it had lost at Caesar's time\".\nThe Senate as a body remained devoid of any significant power; nevertheless, the senators had been marginalised as potential holders of imperial functions during the 3rd century but could dispute such positions alongside more upstart bureaucrats. Some modern historians see in those administrative reforms an attempt by Constantine at reintegrating the senatorial order into the imperial administrative elite to counter the possibility of alienating pagan senators from a Christianised imperial rule; however, such an interpretation remains conjectural, given the fact that we do not have the precise numbers about pre-Constantine conversions to Christianity in the old senatorial milieu. Some historians suggest that early conversions among the old aristocracy were more numerous than previously supposed.\nConstantine's reforms had to do only with the civilian administration. The military chiefs had risen from the ranks since the Crisis of the Third Century but remained outside the Senate, in which they were included only by Constantine's children.\nMonetary reforms.\nIn the 3rd century, the production of fiat money to pay for public expenses resulted in runaway inflation, and Diocletian tried unsuccessfully to re-establish trustworthy minting of silver coins, as well as silver-bronze \"billon\" coins (the term \"billon\" meaning an alloy of precious and base metals that is mostly base metal). Silver currency was overvalued in terms of its actual metal content and therefore could only circulate at much discounted rates. Constantine stopped minting the Diocletianic \"pure\" silver \"argenteus\" soon after 305, while the \"billon\" currency continued to be used until the 360s. From the early 300s on, Constantine forsook any attempts at restoring the silver currency, preferring instead to concentrate on minting large quantities of the gold solidus, 72 of which made a pound of gold. New and highly debased silver pieces continued to be issued during his later reign and after his death, in a continuous process of retariffing, until this \"billon\" minting ceased in 367, and the silver piece was continued by various denominations of bronze coins, the most important being the \"centenionalis\".\nThese bronze pieces continued to be devalued, assuring the possibility of keeping fiduciary minting alongside a gold standard. The author of \"De Rebus Bellicis\" held that the rift widened between classes because of this monetary policy; the rich benefited from the stability in purchasing power of the gold piece, while the poor had to cope with ever-degrading bronze pieces. Later emperors such as Julian the Apostate insisted on trustworthy mintings of the bronze currency.\nConstantine's monetary policies were closely associated with his religious policies; increased minting was associated with the confiscation of all gold, silver, and bronze statues from pagan temples between 331 and 336 which were declared to be imperial property. Two imperial commissioners for each province had the task of getting the statues and melting them for immediate minting, with the exception of a number of bronze statues that were used as public monuments in Constantinople.\nExecutions of Crispus and Fausta.\nConstantine had his eldest son Crispus seized and put to death by \"cold poison\" at Pola (Pula, Croatia) sometime between 15 May and 17 June 326. In July, he had his wife Empress Fausta (stepmother of Crispus) killed in an overheated bath. Their names were wiped from the face of many inscriptions, references to their lives were eradicated from the literary record, and their memory was condemned. Eusebius, for example, edited out any praise of Crispus from later copies of \"Historia Ecclesiastica\", and his \"Vita Constantini\" contains no mention of Fausta or Crispus. Few ancient sources are willing to discuss possible motives for the events, and the few that do are of later provenance and are generally unreliable. At the time of the executions, it was commonly believed that Empress Fausta was either in an illicit relationship with Crispus or was spreading rumors to that effect. A popular myth arose, modified to allude to the Hippolytus\u2013Phaedra legend, with the suggestion that Constantine killed Crispus and Fausta for their immoralities; the largely fictional \"Passion of Artemius\" explicitly makes this connection. The myth rests on slim evidence as an interpretation of the executions; only late and unreliable sources allude to the relationship between Crispus and Fausta, and there is no evidence for the modern suggestion that Constantine's \"godly\" edicts of 326 and the irregularities of Crispus are somehow connected.\nAlthough Constantine created his apparent heirs \"caesars\", following a pattern established by Diocletian, he gave his creations a hereditary character, alien to the tetrarchic system: Constantine's caesars were to be kept in the hope of ascending to empire and entirely subordinated to their augustus, as long as he was alive. Adrian Goldsworthy speculates an alternative explanation for the execution of Crispus was Constantine's desire to keep a firm grip on his prospective heirs, this\u2014and Fausta's desire for having her sons inheriting instead of their half-brother\u2014being reason enough for killing Crispus; the subsequent execution of Fausta, however, was probably meant as a reminder to her children that Constantine would not hesitate in \"killing his own relatives when he felt this was necessary\".\nLater campaigns.\nConstantine considered Constantinople his capital and permanent residence. He lived there for a good portion of his later life. In 328, construction was completed on Constantine's Bridge at \"Sucidava\", (today Celei in Romania) in hopes of reconquering Dacia, a province that had been abandoned under Aurelian. In the late winter of 332, Constantine campaigned with the Sarmatians against the Goths. The weather and lack of food reportedly cost the Goths dearly before they submitted to Rome. In 334, after Sarmatian commoners had overthrown their leaders, Constantine led a campaign against the tribe. He won a victory in the war and extended his control over the region, as remains of camps and fortifications in the region indicate. Constantine resettled some Sarmatian exiles as farmers in Illyrian and Roman districts and conscripted the rest into the army. Constantine reconquered the South of Dacia and the new frontier in Dacia was along the wall and ditch called Brazda lui Novac line supported by new \"castra\". Constantine took the title \"Dacicus maximus\" in 336.\nIn the last years of his life, Constantine made plans for a campaign against Persia. In a letter written to the king of Persia, Shapur, Constantine had asserted his patronage over Persia's Christian subjects and urged Shapur to treat them well. The letter is undatable. In response to border raids, Constantine sent Constantius to guard the eastern frontier in 335. In 336, Prince Narseh invaded Armenia (a Christian kingdom since 301) and installed a Persian client on the throne. Constantine then resolved to campaign against Persia. He treated the war as a Christian crusade, calling for bishops to accompany the army and commissioning a tent in the shape of a church to follow him everywhere. Constantine planned to be baptised in the Jordan River before crossing into Persia. Persian diplomats came to Constantinople over the winter of 336\u2013337, seeking peace, but Constantine turned them away. The campaign was called off, however, when Constantine became sick in the spring of 337.\nIllness and death.\nFrom his recent illness, Constantine knew death would soon come. Within the Church of the Holy Apostles, Constantine had secretly prepared a final resting-place for himself. It came sooner than he had expected. Soon after the Feast of Easter 337, Constantine fell seriously ill. He left Constantinople for the hot baths near his mother's city of Helenopolis (Alt\u0131nova), on the southern shores of the Gulf of Nicomedia (present-day Gulf of \u0130zmit). There, in a church his mother built in honor of Lucian the Martyr, he prayed, and there he realised that he was dying. Seeking purification, he became a catechumen and attempted a return to Constantinople, making it only as far as a suburb of \"Nicomedia\". He summoned the bishops and told them of his hope to be baptised in the River Jordan, where Christ was written to have been baptised. He requested the baptism right away, promising to live a more Christian life should he live through his illness. The bishops, Eusebius records, \"performed the sacred ceremonies according to custom\". He chose the Arian bishop Eusebius of Nicomedia, bishop of the city where he lay dying, as his baptizer. In postponing his baptism, he followed one custom at the time which postponed baptism until after infancy. It has been thought that Constantine put off baptism as long as he did so as to be absolved from as much of his sin as possible. Constantine died soon after at a suburban villa called Achyron, on the last day of the fifty-day festival of Pentecost directly following Pascha (or Easter), on 22 May 337.\nAlthough Constantine's death follows the conclusion of the Persian campaign in Eusebius's account, most other sources report his death as occurring in its middle. Emperor Julian (a nephew of Constantine), writing in the mid-350s, observes that the Sassanians escaped punishment for their ill-deeds, because Constantine died \"in the middle of his preparations for war\". Similar accounts are given in the \"Origo Constantini\", an anonymous document composed while Constantine was still living, which has Constantine dying in \"Nicomedia\"; the \"Historiae abbreviatae\" of Sextus Aurelius Victor, written in 361, which has Constantine dying at an estate near \"Nicomedia\" called Achyrona while marching against the Persians; and the \"Breviarium\" of Eutropius, a handbook compiled in 369 for the Emperor Valens, which has Constantine dying in a nameless state villa in \"Nicomedia\". From these and other accounts, some have concluded that Eusebius's \"Vita\" was edited to defend Constantine's reputation against what Eusebius saw as a less congenial version of the campaign.\nFollowing his death, his body was transferred to Constantinople and buried in the Church of the Holy Apostles, in a porphyry sarcophagus that was described in the 10th century by Constantine VII Porphyrogenitus in the \"De Ceremoniis\". His body survived the plundering of the city during the Fourth Crusade in 1204 but was destroyed at some point afterwards. Constantine was succeeded by his three sons born of Fausta, Constantine II, Constantius II and Constans. His sons, along with his nephew Dalmatius, had already received one division of the empire each to administer as caesars; Constantine may have intended his successors to resume a structure akin to Diocletian's Tetrarchy. A number of relatives were killed by followers of Constantius, notably Constantine's nephews Dalmatius (who held the rank of caesar) and Hannibalianus, presumably to eliminate possible contenders to an already complicated succession. He also had two daughters, Constantina and Helena, wife of Emperor Julian.\nAssessment and legacy.\nConstantine reunited the empire under one emperor, and he won major victories over the Franks and Alamanni in 306\u2013308, the Franks again in 313\u2013314, the Goths in 332, and the Sarmatians in 334. By 336, he had reoccupied most of the long-lost province of Dacia which Aurelian had been forced to abandon in 271. At the time of his death, he was planning a great expedition to end raids on the eastern provinces from the Persian Empire.\nIn the cultural sphere, Constantine revived the clean-shaven face fashion of earlier emperors, originally introduced among the Romans by Scipio Africanus (236\u2013183 BC) and changed into the wearing of the beard by Hadrian (r. 117\u2013138). With the exception of Julian the Apostate (r. 360\u2013363), this new Roman imperial fashion lasted until the reign of Phocas (r. 602\u2013610) in the 7th century.\nThe Holy Roman Empire reckoned Constantine among the venerable figures of its tradition. In the later Byzantine state, it became a great honor for an emperor to be hailed as a \"new Constantine\"; ten emperors carried the name, including the last emperor of the Eastern Roman Empire. Charlemagne used monumental Constantinian forms in his court to suggest that he was Constantine's successor and equal. Charlemagne, Henry VIII, Philip II of Spain, Godfrey of Bouillon, House of Capet, House of Habsburg, House of Stuart, Macedonian dynasty and Phokas family claimed descent from Constantine. Geoffrey of Monmouth embroidered a tale that the legendary king of Britain, King Arthur, was also a descendant of Constantine. Constantine acquired a mythic role as a hero and warrior against heathens. His reception as a saint seems to have spread within the Byzantine empire during wars against the Sasanian Persians and the Muslims in the late 6th and 7th century. The motif of the Romanesque equestrian, the mounted figure in the posture of a triumphant Roman emperor, became a visual metaphor in statuary in praise of local benefactors. The name \"Constantine\" enjoyed renewed popularity in western France in the 11th and 12th centuries. During the Fascist period in Italy in the , parallels between Constantine and Mussolini became especially popular after the signing of the Lateran Pacts by the Italian State and the Catholic Church in 1929. Mussolini's perceived role in bringing about the historic agreement was sometimes even explicitly compared to Constantine's Edict of Milan. For example, the archbishop of Milan, Cardinal Ildefonso Schuster, claimed that, after sixteen centuries, a second March on Rome had occurred and a second 'religious pact' had been established, linking Mussolini to the \"spiriti magni\" of both Constantine and Augustus.\nThe Ni\u0161 Constantine the Great Airport is named in honor of him. A large cross was planned to be built on a hill overlooking Ni\u0161, but the project was cancelled. In 2012, a memorial was erected in Ni\u0161 in his honor. The \"Commemoration of the Edict of Milan\" was held in Ni\u0161 in 2013. The Orthodox Church considers Constantine a saint (\u0386\u03b3\u03b9\u03bf\u03c2 \u039a\u03c9\u03bd\u03c3\u03c4\u03b1\u03bd\u03c4\u03af\u03bd\u03bf\u03c2, Saint Constantine), having a feast day on 21 May, and calls him \"isapostolos\" (\u03b9\u03c3\u03b1\u03c0\u03cc\u03c3\u03c4\u03bf\u03bb\u03bf\u03c2 \u039a\u03c9\u03bd\u03c3\u03c4\u03b1\u03bd\u03c4\u03af\u03bd\u03bf\u03c2)\u2014an equal of the Apostles.\nConstantine is sometimes associated with the religiopolitical ideology known as Caesaropapism, which epitomizes the unity of church and state. However, his association with this ideology has been debated.\nHistoriography.\nDuring Constantine's lifetime, Praxagoras of Athens and Libanius, pagan authors, showered Constantine with praise, presenting him as a paragon of virtue. His nephew and son-in-law Julian the Apostate, however, wrote the satire \"Symposium, or the Saturnalia\" in 361, after the last of his sons died; it denigrated Constantine, calling him inferior to the great pagan emperors, and given over to luxury and greed. Following Julian, Eunapius began \u2013 and Zosimus continued \u2013 a historiographic tradition that blamed Constantine for weakening the empire through his indulgence to the Christians.\nDuring the Middle Ages, European and Near-East Byzantine writers presented Constantine as an ideal ruler, the standard against which any king or emperor could be measured. The Renaissance rediscovery of anti-Constantinian sources prompted a re-evaluation of his career. German humanist Johannes Leunclavius discovered Zosimus' writings and published a Latin translation in 1576. In its preface, he argues that Zosimus' picture of Constantine offered a more balanced view than that of Eusebius and the Church historians. Cardinal Caesar Baronius criticised Zosimus, favouring Eusebius' account of the Constantinian era. Baronius' \"Life of Constantine\" (1588) presents Constantine as the model of a Christian prince. Edward Gibbon aimed to unite the two extremes of Constantinian scholarship in his work \"The History of the Decline and Fall of the Roman Empire\" (1776\u20131789) by contrasting the portraits presented by Eusebius and Zosimus. He presents a noble war hero who transforms into an Oriental despot in his old age, \"degenerating into a cruel and dissolute monarch\".\nModern interpretations of Constantine's rule begin with Jacob Burckhardt's \"The Age of Constantine the Great\" (1853, rev. 1880). Burckhardt's Constantine is a scheming secularist, a politician who manipulates all parties in a quest to secure his own power. Henri Gr\u00e9goire followed Burckhardt's evaluation of Constantine in the 1930s, suggesting that Constantine developed an interest in Christianity only after witnessing its political usefulness. Gr\u00e9goire was skeptical of the authenticity of Eusebius's ', and postulated a pseudo-Eusebius to assume responsibility for the vision and conversion narratives of that work. Otto Seeck's ' (1920\u20131923) and Andr\u00e9 Piganiol's ' (1932) go against this historiographic tradition. Seeck presents Constantine as a sincere war hero whose ambiguities were the product of his own na\u00efve inconsistency. Piganiol's Constantine is a philosophical monotheist, a child of his era's religious syncretism. Related histories by Arnold Hugh Martin Jones (\"Constantine and the Conversion of Europe\", 1949) and Ramsay MacMullen (\"Constantine\", 1969) give portraits of a less visionary and more impulsive Constantine.\nThese later accounts were more willing to present Constantine as a genuine convert to Christianity. Norman H. Baynes began a historiographic tradition with \"Constantine the Great and the Christian Church\" (1929) which presents Constantine as a committed Christian, reinforced by Andreas Alf\u00f6ldi's \"The Conversion of Constantine and Pagan Rome\" (1948), and Timothy Barnes's \"Constantine and Eusebius\" (1981) is the culmination of this trend. Barnes' Constantine experienced a radical conversion which drove him on a personal crusade to convert his empire. Charles Matson Odahl's \"Constantine and the Christian Empire\" (2004) takes much the same tack. In spite of Barnes' work, arguments continue over the strength and depth of Constantine's religious conversion. Certain themes in this school reached new extremes in T. G. Elliott's \"The Christianity of Constantine the Great\" (1996), which presented Constantine as a committed Christian from early childhood. Paul Veyne's 2007 work \"\" holds a similar view which does not speculate on the origin of Constantine's Christian motivation, but presents him as a religious revolutionary who fervently believed that he was meant \"to play a providential role in the millenary economy of the salvation of humanity\". Peter Heather argues that it is most plausible that Constantine had been a Christian considerably before 312\u00a0\u2013 possibly even for his entire life\u00a0\u2013 with the public timeline of events instead reflecting his \"coming out\" as Christian in stages as doing so became politically viable. As a parallel illustrating the cogency of this interpretation, Heather gestures to the later conversion of Constantine's nephew Julian from Christianity to Hellenism, after which he practiced in secret for a decade.\nDonation of Constantine.\nLatin Christians considered it inappropriate that Constantine was baptised only on his death bed by an unorthodox bishop, and a legend emerged by the early 4th century that Pope Sylvester I had cured the pagan emperor from leprosy. According to this legend, Constantine was baptised and began the construction of a church in the Lateran Basilica. The \"Donation of Constantine\" appeared in the 8th century, most likely during the pontificate of Pope Stephen II, in which the freshly converted Constantine gives \"the city of Rome and all the provinces, districts, and cities of Italy and the Western regions\" to Sylvester and his successors. In the High Middle Ages, this document was used and accepted as the basis for the pope's temporal power, though it was denounced as a forgery by Emperor Otto III and lamented as the root of papal worldliness by Dante Alighieri. Philologist and Catholic priest Lorenzo Valla proved in 1440 that the document was indeed a forgery.\nGeoffrey of Monmouth's \"Historia\".\nDuring the medieval period, Britons regarded Constantine as a king of their own people, particularly associating him with Caernarfon in Gwynedd. While some of this is owed to his fame and his proclamation as emperor in Britain, there was also confusion of his family with Magnus Maximus's supposed wife Elen and her son, another Constantine . In the 12th century Henry of Huntingdon included a passage in his \"Historia Anglorum\" that the Emperor Constantine's mother was a Briton, making her the daughter of King Cole of Colchester. Geoffrey of Monmouth expanded this story in his highly fictionalised , an account of the supposed Kings of Britain from their Trojan origins to the Anglo-Saxon invasion. According to Geoffrey, Cole was King of the Britons when Constantius, here a senator, came to Britain. Afraid of the Romans, Cole submits to Roman law so long as he retains his kingship. However, he dies only a month later, and Constantius takes the throne himself, marrying Cole's daughter Helena. They have their son Constantine, who succeeds his father as King of Britain before becoming Roman emperor.\nHistorically, this series of events is extremely improbable. Constantius had already left Helena by the time he left for Britain. Additionally, no earlier source mentions that Helena was born in Britain, let alone that she was a princess. Henry's source for the story is unknown, though it may have been a lost hagiography of Helena.\nFamily tree.\nEmperors are shown with a rounded-corner border with their dates as Augusti, names with a thicker border appear in both sections\n1: Constantine's parents and half-siblings\n2: Constantine's children"}
{"id": "7237", "revid": "45807063", "url": "https://en.wikipedia.org/wiki?curid=7237", "title": "Common Language Infrastructure", "text": "The Common Language Infrastructure (CLI) is an open specification and technical standard originally developed by Microsoft and standardized by ISO/IEC (ISO/IEC 23271) and Ecma International (ECMA 335) that describes executable code and a runtime environment that allows multiple high-level languages to be used on different computer platforms without being rewritten for specific architectures. This implies it is platform agnostic. The .NET Framework, .NET and Mono are implementations of the CLI.\nThe metadata format is also used to specify the API definitions exposed by the Windows Runtime.\nOverview.\nAmong other things, the CLI specification describes the following five aspects:\nStandardization and licensing.\nIn August 2000, Microsoft, Hewlett-Packard, Intel, and others worked to standardize CLI. By December 2001, it was ratified by the Ecma, with ISO/IEC standardization following in April 2003.\nMicrosoft and its partners hold patents for CLI. Ecma and ISO/IEC require that all patents essential to implementation be made available under \"reasonable and non-discriminatory (RAND) terms.\" It is common for RAND licensing to require some royalty payment, which could be a cause for concern with Mono. , neither Microsoft nor its partners have identified any patents essential to CLI implementations subject to RAND terms.\n, Microsoft added C# and CLI to the list of specifications that the Microsoft Community Promise applies to, so anyone can safely implement specified editions of the standards without fearing a patent lawsuit from Microsoft. To implement the CLI standard requires conformance to one of the supported and defined profiles of the standard, the minimum of which is the kernel profile. The kernel profile is actually a very small set of types to support in comparison to the well known core library of default .NET installations. However, the conformance clause of the CLI allows for extending the supported profile by adding new methods and types to classes, as well as deriving from new namespaces. But it does not allow for adding new members to interfaces. This means that the features of the CLI can be used and extended, as long as the conforming profile implementation does not change the behavior of a program intended to run on that profile, while allowing for unspecified behavior from programs written specifically for that implementation.\nIn 2012, Ecma and ISO/IEC published the new edition of the CLI standard."}
{"id": "7239", "revid": "46582359", "url": "https://en.wikipedia.org/wiki?curid=7239", "title": "Cricket World Cup", "text": "The ICC Men's Cricket World Cup is the quadrennial international championship of One Day International cricket. The event is organised by the sport's governing body, the International Cricket Council, every four years, with preliminary qualification rounds leading up to a finals tournament. The tournament is one of the world's most viewed sporting events and considered the \"flagship event of the international cricket calendar\" by the ICC. It is widely considered the pinnacle championship of the sport of cricket.\nThe first Cricket World Cup was organised in England in June 1975, with the first ODI cricket match having been played only four years earlier. However, a separate Women's Cricket World Cup had been held two years before the first men's tournament, and a tournament involving multiple international teams had been held as early as 1912, when a triangular tournament of Test matches was played between Australia, England and South Africa. The first three World Cups were held in England. From the 1987 tournament onwards, hosting has been shared between countries under an unofficial rotation system, with fourteen ICC members having hosted at least one match in the tournament.\nThe current format involves a qualification phase, which takes place over the preceding three years, to determine which teams qualify for the tournament phase. In the tournament phase, 10 teams, including the automatically qualifying host nation, compete for the title at venues within the host nation over about a month. In the 2027 edition, the format will be changed to accommodate an expanded 14-team final competition.\nA total of twenty teams have competed in the 13 editions of the tournament, with ten teams competing in the recent 2023 tournament. Australia has won the tournament six times, India and West Indies twice each, while Pakistan, Sri Lanka and England have won it once each. The best performance by a non-full-member team came when Kenya made the semi-finals of the 2003 tournament.\nAustralia are the current champions after winning the 2023 World Cup in India. The subsequent 2027 World Cup will be held jointly in South Africa, Zimbabwe and Namibia.\nHistory.\nThe first international cricket match was played between Canada and the United States, on 24 and 25 September 1844. However, the first credited Test match was played in 1877 between Australia and England, and the two teams competed regularly for The Ashes in subsequent years. South Africa was admitted to Test status in 1889. Representative cricket teams were selected to tour each other, resulting in bilateral competition. Cricket was also included as an Olympic sport at the 1900 Paris Games, where Great Britain defeated France to win the gold medal. This was the only appearance of cricket at the Summer Olympics.\nThe first multilateral competition at international level was the 1912 Triangular Tournament, a Test cricket tournament played in England between all three Test-playing nations at the time: England, Australia and South Africa. The event was not a success: the summer was exceptionally wet, making play difficult on damp uncovered pitches, and crowd attendances were poor, attributed to a \"surfeit of cricket\". Since then, international Test cricket has generally been organised as bilateral series: a multilateral Test tournament was not organised again until the triangular Asian Test Championship in 1999.\nThe number of nations playing Test cricket increased gradually over time, with the addition of West Indies in 1928, New Zealand in 1930, India in 1932, and Pakistan in 1952. However, international cricket continued to be played as bilateral Test matches over three, four or five days.\nIn the early 1960s, English county cricket teams began playing a shortened version of cricket which only lasted for one day. Starting in 1962 with a four-team knockout competition known as the Midlands Knock-Out Cup, and continuing with the inaugural Gillette Cup in 1963, one-day cricket grew in popularity in England. A national Sunday League was formed in 1969. The first One-Day International match was played on the fifth day of a rain-aborted Test match between England and Australia at Melbourne in 1971, to fill the time available and as compensation for the frustrated crowd. It was a forty over game with eight balls per over. The success and popularity of the domestic one-day competitions in England and other parts of the world, as well as the early One-Day Internationals, prompted the ICC to consider organizing a Cricket World Cup.\nPrudential World Cups (1975\u20131983).\nThe inaugural Cricket World Cup was hosted in 1975 by England, the only nation able to put forward the resources to stage an event of such magnitude at the time. The first three tournaments were held in England and officially known as the Prudential Cup after the sponsors Prudential plc. The matches consisted of 60 six-ball overs per team, played during daytime in the traditional form, with the players wearing cricket whites and using red cricket balls.\nEight teams participated in the first tournament: Australia, England, India, New Zealand, Pakistan, and the West Indies (the six Test nations at the time), together with Sri Lanka and a composite team from East Africa. One notable omission was South Africa, who were banned from international cricket due to apartheid. The tournament was won by the West Indies, who defeated Australia by 17 runs in the final at Lord's. Roy Fredricks of West Indies was the first batsmen who got hit-wicket in ODI during the 1975 World Cup final.\nThe 1979 World Cup saw the introduction of the ICC Trophy competition to select non-Test playing teams for the World Cup, with Sri Lanka and Canada qualifying. The West Indies won a second consecutive World Cup tournament, defeating the hosts England by 92 runs in the final. At a meeting which followed the World Cup, the International Cricket Conference agreed to make the competition a quadrennial event.\nThe 1983 event was hosted by England for a third consecutive time. By this stage, Sri Lanka had become a Test-playing nation, and Zimbabwe qualified through the ICC Trophy. A fielding circle was introduced, away from the stumps. Four fieldsmen needed to be inside it at all times. The teams faced each other twice, before moving into the knock-outs. India was crowned champions after upsetting the West Indies by 43 runs in the final.\nDifferent champions (1987\u20131996).\nIndia and Pakistan jointly hosted the 1987 tournament, the first time that the competition was held outside England. The games were reduced from 60 to 50 overs per innings, the current standard, because of the shorter daylight hours in the Indian subcontinent compared with England's summer. Australia won the championship by defeating England by 7 runs in the final, the closest margin in the World Cup final until the 2019 edition between England and New Zealand.\nThe 1992 World Cup, held in Australia and New Zealand, introduced many changes to the game, such as coloured clothing, white balls, day/night matches, and a change to the fielding restriction rules. The South African cricket team participated in the event for the first time, following the fall of the apartheid regime and the end of the international sports boycott. Pakistan overcame a dismal start in the tournament to eventually defeat England by 22 runs in the final and emerge as winners.\nThe 1996 championship was held in the Indian subcontinent for a second time, with the inclusion of Sri Lanka as host for some of its group stage matches. In the semi-final, Sri Lanka, heading towards a crushing victory over India at Eden Gardens after the hosts lost eight wickets while scoring 120 runs in pursuit of 252, were awarded victory by default after crowd unrest broke out in protest against the Indian performance. Sri Lanka went on to win their maiden championship by defeating Australia by seven wickets in the final at Lahore.\nAustralian treble (1999\u20132007).\nIn 1999, the event was hosted by England, with some matches also being held in Scotland, Ireland, Wales and the Netherlands. Twelve teams contested the World Cup. Australia qualified for the semi-finals after reaching their target in their Super 6 match against South Africa off the final over of the match. They then proceeded to the final with a tied match in the semi-final also against South Africa where a mix-up between South African batsmen Lance Klusener and Allan Donald saw Donald drop his bat and stranded mid-pitch to be run out. In the final, Australia dismissed Pakistan for 132 and then reached the target in less than 20 overs and with eight wickets in hand.\nSouth Africa, Zimbabwe and Kenya hosted the 2003 World Cup. The number of teams participating in the event increased from twelve to fourteen. Kenya's victories over Sri Lanka and Zimbabwe, among others\u00a0\u2013 and a forfeit by the New Zealand team, which refused to play in Kenya because of security concerns\u00a0\u2013 enabled Kenya to reach the semi-finals, the best result by an associate. In the final, Australia made 359 runs for the loss of two wickets, the largest ever total in a final, defeating India by 125 runs.\nIn 2007, the tournament was hosted by the West Indies and expanded to sixteen teams. Following Pakistan's upset loss to World Cup debutants Ireland in the group stage, Pakistani coach Bob Woolmer was found dead in his hotel room. Jamaican police had initially launched a murder investigation into Woolmer's death but later confirmed that he died of heart failure. Australia defeated Sri Lanka in the final by 53 runs (D/L) in farcical light conditions, and extended their undefeated run in the World Cup to 29 matches and winning three straight championships.\nHosts triumph (2011\u20132019).\nIndia, Sri Lanka and Bangladesh together hosted the 2011 World Cup. Pakistan was stripped of its hosting rights following the terrorist attack on the Sri Lankan cricket team in 2009, with the games originally scheduled for Pakistan redistributed to the other host countries. The number of teams participating in the World Cup was reduced to fourteen. Australia lost their final group stage match against Pakistan on 19 March 2011, ending an unbeaten streak of 35 World Cup matches, which had begun on 23 May 1999. India won their second World Cup title by beating Sri Lanka by 6 wickets in the final at Wankhede Stadium in Mumbai, where the Indian captain M.S. Dhoni along with the spinning all-rounder Yuvraj Singh chased 275 with notable performances from Gautam Gambhir and Virat Kohli, making India the first country to win the World Cup at home. This was also the first time that two Asian countries faced each other in a World Cup Final.\nAustralia and New Zealand jointly hosted the 2015 World Cup. The number of participants remained at fourteen. Ireland was the most successful Associate nation with a total of three wins in the tournament. New Zealand beat South Africa in a thrilling first semi-final to qualify for their maiden World Cup final. Australia defeated New Zealand by seven wickets in the final at Melbourne to lift the World Cup for the fifth time.\nThe 2019 World Cup was hosted by England and Wales. The number of participants was reduced to 10. New Zealand defeated India in the first semi-final, which was pushed over to the reserve day due to rain. England defeated the defending champions, Australia, in the second semi-final. Neither finalist had previously won the World Cup. In the final, the scores were tied at 241 after 50 overs and the match went to a super over, after which the scores were again tied at 15. The World Cup was won by England, whose boundary count was greater than New Zealand's.\nAustralian resurgence (2023).\nThe 2023 Cricket World Cup was hosted by India. Ten teams participated, including The Netherlands, which returned to the tournament after a 12-year absence. The tournament was structured as a single group round-robin, with the top four advancing to the semi-final knockout stage. India were unbeaten in the group stage and defeated New Zealand to advance to the final; Australia lost twice during the group stage before defeating South Africa in the semi-finals. Afghanistan had their most successful World Cup, with four wins during the group stage, including over defending champions England; they also came close to defeating Australia before Glenn Maxwell's double century turned the match around. In the final, Australia defeated the 10-match unbeaten India to clinch their 6th World Cup.\nFormat.\nQualification.\nFrom the first World Cup in 1975 up to the 2019 World Cup, the majority of teams taking part qualified automatically. Until the 2015 World Cup this was mostly through having Full Membership of the ICC, and for the 2019 World Cup this was mostly through ranking position in the ICC ODI Championship.\nSince the second World Cup in 1979 up to the 2019 World Cup, the teams that qualified automatically were joined by a small number of others who qualified for the World Cup through the qualification process. The first qualifying tournament being the ICC Trophy; later the process expanding with pre-qualifying tournaments. For the 2011 World Cup, the ICC World Cricket League replaced the past pre-qualifying processes; and the name \"ICC Trophy\" was changed to \"ICC Men's Cricket World Cup Qualifier\". The World Cricket League was the qualification system provided to allow the Associate and Affiliate members of the ICC more opportunities to qualify. The number of teams qualifying varied throughout the years.\nFrom the 2023 World Cup onwards, only the host nation(s) will qualify automatically. All countries will participate in a series of leagues to determine qualification, with automatic promotion and relegation between divisions from one World Cup cycle to the next.\nTournament.\nThe format of the Cricket World Cup has changed greatly over the course of its history. Each of the first four tournaments was played by eight teams, divided into two groups of four. The competition consisted of two stages, a group stage and a knock-out stage. The four teams in each group played each other in the round-robin group stage, with the top two teams in each group progressing to the semi-finals. The winners of the semi-finals played against each other in the final. With South Africa returning in the fifth tournament in 1992 as a result of the end of the apartheid boycott, nine teams played each other once in the group phase, and the top four teams progressed to the semi-finals. The tournament was further expanded in 1996, with two groups of six teams. The top four teams from each group progressed to quarter-finals and semi-finals.\nA distinct format was used for the 1999 and 2003 World Cups. The teams were split into two pools, with the top three teams in each pool advancing to the \"Super 6\". The \"Super 6\" teams played the three other teams that advanced from the other group. As they advanced, the teams carried their points forward from previous matches against other teams advancing alongside them, giving them an incentive to perform well in the group stages. The top four teams from the \"Super 6\" stage progressed to the semi-finals, with the winners playing in the final.\nThe format used in the 2007 World Cup involved 16 teams allocated into four groups of four. Within each group, the teams played each other in a round-robin format. Teams earned points for wins and half-points for ties. The top two teams from each group moved forward to the \"Super 8\" round. The \"Super 8\" teams played the other six teams that progressed from the different groups. Teams earned points in the same way as the group stage, but carried their points forward from previous matches against the other teams who qualified from the same group to the \"Super 8\" stage. The top four teams from the \"Super 8\" round advanced to the semi-finals, and the winners of the semi-finals played in the final.\nThe format used in the 2011 and 2015 World Cups featured two groups of seven teams, each playing in a round-robin format. The top four teams from each group proceeded to the knock out stage consisting of quarter-finals, semi-finals and ultimately the final.\nIn the 2019 and 2023 editions of the tournament, the number of teams participating dropped to 10. Each team is scheduled to play against each other once in a round robin format, before entering the semifinals, a similar format to the 1992 World Cup. The 2027 and 2031 World Cups will have 14 teams, with the format same as the 2003 edition.\nTrophy.\nThe ICC Cricket World Cup Trophy is presented to the winners of the World Cup. The current trophy was created for the 1999 championships, and was the first permanent prize in the tournament's history. Prior to this, different trophies were made for each World Cup. Before introducing the permanent ICC Cricket World Cup Trophy in 1999, individual trophies were designed and awarded for each edition of the tournament. For example, the Prudential Cup trophies were used for the first three editions (1975, 1979, and 1983) when Prudential plc was the sponsor. Similarly, subsequent tournaments used different designs until the permanent trophy was introduced. This shift to a permanent design was intended to establish a consistent and iconic representation of the World Cup's legacy. The trophy was designed and produced in London by a team of craftsmen from Garrard &amp; Co over a period of two months.\nThe current trophy is made from silver and gilt, and features a golden globe held up by three silver columns. The columns, shaped as stumps and bails, represent the three fundamental aspects of cricket: batting, bowling and fielding, while the globe characterises a cricket ball. The seam is tilted to symbolize the axial tilt of the Earth. It stands high and weighs approximately . The names of the previous winners are engraved on the base of the trophy, with space for a total of twenty inscriptions. The ICC keeps the original trophy. A replica differing only in the inscriptions is permanently awarded to the winning team.\nMedia coverage.\nThe tournament is one of the world's most-viewed sporting events, and successive tournaments have generated increasing media attention as One-Day International cricket has become more established. The 2011 Cricket World Cup was televised in over 200 countries to over 2.2\u00a0billion viewers. Television rights, mainly for the 2011 and 2015 World Cup, were sold for over US$1.1\u00a0billion, and sponsorship rights were sold for a further US$500\u00a0million. The ICC claimed a total of 1.6\u00a0billion viewers for the 2019 World Cup as well as 4.6\u00a0billion views of digital video of the tournament. The most-watched match of the tournament was the group game between India and Pakistan, which was watched by more than 300 million people live.\nSelection of hosts.\nThe International Cricket Council's executive committee votes for the hosts of the tournament after examining the bids made by the nations keen to hold a Cricket World Cup.\nEngland hosted the first three competitions. The ICC decided that England should host the first tournament because it was ready to devote the resources required to organising the inaugural event. India volunteered to host the third Cricket World Cup, but most ICC members preferred England as the longer period of daylight in England in June meant that a match could be completed in one day. The 1987 Cricket World Cup was held in India and Pakistan, the first hosted outside England.\nMany of the tournaments have been jointly hosted by nations from the same geographical region, such as South Asia in 1987, 1996 and 2011, Australasia (in Australia and New Zealand) in 1992 and 2015, Southern Africa in 2003 and West Indies in 2007.\nIn November 2021, ICC published the name of the hosts for ICC events to be played between 2024 and 2031 cycle. The hosts for the 50-over World Cup along with T20 World Cup and Champions Trophy were selected through a competitive bidding process.\nTournament summary.\nTwenty nations have qualified for the Cricket World Cup at least once. Six teams have competed in every tournament, five of which have won the title. The West Indies won the first two tournaments, Australia has won six, India has won two, while Pakistan, Sri Lanka and England have each won once. The West Indies (1975 and 1979) and Australia (1999, 2003 and 2007) are the only teams to have won consecutive titles. Australia has played in eight of the thirteen finals (1975, 1987, 1996, 1999, 2003, 2007, 2015 and 2023). New Zealand has yet to win the World Cup, but has been runners-up two times (2015 and 2019). The best result by a non-Test playing nation is the semi-final appearance by Kenya in the 2003 tournament; while the best result by a non-Test playing team on their debut is the Super 8 (second round) by Ireland in 2007.\nSri Lanka, as a co-host of the 1996 World Cup, was the first host to win the tournament, though the final was held in Pakistan. India won in 2011 as host and was the first team to win a final played in their own country. Australia and England repeated the feat in 2015 and 2019 respectively. Other than this, England in 1979 and India in 2023 made it to the final which was hosted by their country. Other countries which have achieved or equalled their best World Cup results while co-hosting the tournament are Sri Lanka and New Zealand as finalists in 2011 and 2015 respectively, Zimbabwe who reached the Super Six in 2003, and Kenya as semi-finalists in 2003. In 1987, co-hosts India and Pakistan both reached the semi-finals, but were eliminated by England and Australia respectively. Australia in 1992, England in 1999, South Africa in 2003, and Bangladesh in 2011 have been host teams that were eliminated in the first round.\nTeams' performances.\nAn overview of the teams' performances in every World Cup is given below. For each tournament, the number of teams in each finals tournament (in brackets) are shown.\nLegend\nOverview.\nThe table below provides an overview of the performances of teams over past World Cups, as of the end of the 2023 tournament. Teams are ordered by best result then by appearances, then by winning percentage, then by total number of wins, total number of number of games, and then alphabetically:\n\"Note:\""}
{"id": "7241", "revid": "60840", "url": "https://en.wikipedia.org/wiki?curid=7241", "title": "Commonwealth Heads of Government Meeting", "text": " \nThe Commonwealth Heads of Government Meeting (CHOGM; or) is a biennial summit meeting of the governmental leaders from all Commonwealth nations. Despite the name, the head of state may be present in the meeting instead of the head of government, especially among semi-presidential states. Every two years the meeting is held in a different member state and is chaired by that nation's respective prime minister or president, who becomes the Commonwealth Chair-in-Office until the next meeting. Queen Elizabeth II, who was the Head of the Commonwealth, attended every CHOGM beginning with Ottawa in 1973 until Perth in 2011, although her formal participation only began in 1997. She was represented by the Prince of Wales at the 2013 meeting as the 87-year-old monarch was curtailing long-distance travel. The Queen attended the 2015 summit in Malta and the 2018 summit (delayed by one year) in London, but was represented again by the Prince of Wales at the 2022 meeting (delayed by two years) in Rwanda.\nThe first CHOGM was held in 1971 in Singapore and there have been 27 held in total: the most recent was held in Apia, Samoa. They are held once every two years, although this pattern has occasionally been interrupted. They are held around the Commonwealth, rotating by invitation amongst its members.\nIn the past, CHOGMs have attempted to orchestrate common policies on certain contentious issues and current events, with a special focus on issues affecting member nations. CHOGMs have discussed the continuation of apartheid rule in South Africa and how to end it, military coups in Pakistan and Fiji, and allegations of electoral fraud in Zimbabwe. Sometimes the member states agree on a common idea or solution and release a joint statement declaring their opinion. More recently, beginning at the 1997 CHOGM, the meeting has had an official theme, set by the host nation, on which the primary discussions have been focused.\nHistory.\nThe meetings originated with the leaders of the self-governing colonies of the British Empire. The First Colonial Conference in 1887 was followed by periodic meetings, known as Imperial Conferences from 1907, of government leaders of the Empire. The development of the independence of the dominions, and the creation of a number of new dominions, as well as the invitation of Southern Rhodesia (which also attended as a \"sui generis\" colony), changed the nature of the meetings. As the dominion leaders asserted themselves more and more at the meetings, it became clear that the time for 'imperial' conferences was over.\nFrom the ashes of the Second World War, seventeen Commonwealth Prime Ministers' Conferences were held between 1944 and 1969. Of these, sixteen were held in London, reflecting then-prevailing views of the Commonwealth as the continuation of the Empire and the centralisation of power in the British Commonwealth Office (the one meeting outside London, in Lagos, was an extraordinary meeting held in January 1966 to co-ordinate policies towards Rhodesia). Two supplementary meetings were also held during this period: a Commonwealth Statesmen's meeting to discuss peace terms in April 1945, and a Commonwealth Economic Conference in 1952.\nThe 1960s saw an overhaul of the Commonwealth. The swift expansion of the Commonwealth after decolonisation saw the newly independent countries demand the creation of the Commonwealth Secretariat, and the United Kingdom, in response, successfully founding the Commonwealth Foundation. This decentralisation of power demanded a reformulation of the meetings. Instead of the meetings always being held in London, they would rotate across the membership, subject to countries' ability to host the meetings: beginning with Singapore in 1971. They were also renamed the 'Commonwealth Heads of Government Meetings' to reflect the growing diversity of the constitutional structures in the Commonwealth.\nStructure.\nThe core of the CHOGM are the executive sessions, which are the formal gatherings of the heads of government to do business. However, the majority of the important decisions are held not in the main meetings themselves, but at the informal 'retreats': introduced at the second CHOGM, in Ottawa, by Prime Minister of Canada Pierre Trudeau, but reminiscent of the excursions to Chequers or Dorneywood in the days of the Prime Ministers' Conferences. Only the head of the delegation and their spouse and one additional person attend the retreats. The additional person may be of any capacity (personal, political, security, etc.) but only has occasional and intermittent access to the head of the delegation. It is usually at the retreat where, isolated from their advisers, the heads resolve the most intransigent issues: leading to the Gleneagles Agreement in 1977, the Lusaka Declaration in 1979, the Langkawi Declaration in 1989, the Millbrook Programme in 1995, the Aso Rock Declaration in 2003, and the Colombo Declaration on Sustainable, Inclusive and Equitable Development in 2013.\nThe 'fringe' of civil society organisations, including the Commonwealth Family and local groups, adds a cultural dimension to the event, and brings the CHOGM a higher media profile and greater acceptance by the local population. First officially recognised at Limassol in 1993, these events, spanning a longer period than the meeting itself, have, to an extent, preserved the length of the CHOGM: but only in the cultural sphere. Other meetings, such as those of the Commonwealth Ministerial Action Group, Commonwealth Business Council, and respective foreign ministers, have also dealt with business away from the heads of government themselves.\nAs the scope of the CHOGM has expanded beyond the meetings of the heads of governments themselves, the CHOGMs have become progressively shorter, and their business compacted into less time. The 1971 CHOGM lasted for nine days, and the 1977 and 1991 CHOGMs for seven days each. However, Harare's epochal CHOGM was the last to last a week; the 1993 CHOGM lasted for five days, and the contentious 1995 CHOGM for only three-and-a-half. The 2005 and subsequent conferences were held over two to two-and-a-half-days. However, recent CHOGMs have also featured several days of pre-summit Commonwealth Forums on business, women, youth, as well as the Commonwealth People's Forum and meetings of foreign ministers.\nIssues.\nDuring the 1980s, CHOGMs were dominated by calls for the Commonwealth to impose sanctions on South Africa to pressure the country to end apartheid. The division between Britain, during the government of Margaret Thatcher which resisted the call for sanctions and African Commonwealth countries, and the rest of the Commonwealth was intense at times and led to speculation that the organisation might collapse. According to one of Margaret Thatcher's former aides, Mrs. Thatcher, very privately, used to say that CHOGM stood for \"Compulsory Handouts to Greedy Mendicants.\" According to his daughter, Denis Thatcher also referred to CHOGM as standing for 'Coons Holidaying on Government Money'.\nIn 2011, British Prime Minister David Cameron informed the British House of Commons that his proposals to reform the rules governing royal succession, a change which would require the approval of all sixteen Commonwealth realms, was approved at the 28\u201330 October CHOGM in Perth, subsequently referred to as the Perth Agreement.\nRwanda joined the Commonwealth in 2009 despite the Commonwealth Human Rights Initiative's (CHRI) finding that \"the state of governance and human rights in Rwanda does not satisfy Commonwealth standards\", and that it \"does not therefore qualify for admission\". Both the CHRI and Human Rights Watch have found that respect for democracy and human rights in Rwanda has declined since the country joined the Commonwealth. There have been calls for the Commonwealth to stand up for democracy and human rights in Rwanda at the 2022 CHOGM.\nAgenda.\nUnder the Millbrook Commonwealth Action Programme, each CHOGM is responsible for renewing the remit of the Commonwealth Ministerial Action Group, whose responsibility it is to uphold the Harare Declaration on the core political principles of the Commonwealth.\nIncidents.\nA bomb exploded at the Sydney Hilton Hotel, the venue for the February 1978 Commonwealth Heads of Government Regional Meeting. Twelve foreign heads of government were staying in the hotel at the time. Most delegates were evacuated by Royal Australian Air Force helicopters and the meeting was moved to Bowral, protected by 800 soldiers of the Australian Army.\nAs the convocation of heads of governments and permanent Commonwealth staff and experts, CHOGMs are the highest institution of action in the Commonwealth, and rare occasions on which Commonwealth leaders all come together. CHOGMs have been the venues of many of the Commonwealth's most dramatic events. Robert Mugabe announced Zimbabwe's immediate withdrawal from the Commonwealth at the 2003 CHOGM, and Nigeria's execution of Ken Saro-Wiwa and eight others on the first day of the 1995 CHOGM led to that country's suspension.\nIt has also been the trigger of a number of events that have shaken participating countries domestically. The departure of Uganda's President Milton Obote to the 1971 CHOGM allowed Idi Amin to overthrow Obote's government. Similarly, President James Mancham's attendance of the 1977 CHOGM gave Prime Minister France-Albert Ren\u00e9 the opportunity to seize power in the Seychelles.\nList of meetings.\nThe 25th CHOGM was originally scheduled for Vanuatu in 2017 but the country rescinded its offer to host after Cyclone Pam devastated the country's infrastructure in March 2015. The meeting was rescheduled for the United Kingdom in the spring of 2018 which also resulted in the 26th CHOGM, originally scheduled for 2019, to be rescheduled for 22\u201327 June 2020. However, due to the coronavirus pandemic, the 26th CHOGM was again postponed to 2022."}
{"id": "7242", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=7242", "title": "Chinese classics", "text": "The Chinese classics or canonical texts are the works of Chinese literature authored prior to the establishment of the imperial Qin dynasty in 221\u00a0BC. Prominent examples include the Four Books and Five Classics in the Neo-Confucian tradition, themselves an abridgment of the Thirteen Classics. The Chinese classics used a form of written Chinese consciously imitated by later authors, now known as Classical Chinese. A common Chinese word for \"classic\" () literally means 'warp thread', in reference to the techniques by which works of this period were bound into volumes.\nTexts may include \"shi\" (, 'histories') \"zi\" ( 'master texts'), philosophical treatises usually associated with an individual and later systematized into schools of thought but also including works on agriculture, medicine, mathematics, astronomy, divination, art criticism, and other miscellaneous writings) and \"ji\" ( 'literary works') as well as the cultivation of \"jing\", 'essence' in Chinese medicine.\nIn the Ming and Qing dynasties, the Four Books and Five Classics were the subjects of mandatory study by those Confucian scholars who wished to take the imperial examination and needed to pass them in order to become scholar-officials. Any political discussion was full of references to this background, and one could not become part of the literati\u2014or even a military officer in some periods\u2014without having memorized them. Generally, children first memorized the Chinese characters of the \"Three Character Classic\" and \"Hundred Family Surnames\" and they then went on to memorize the other classics. The literate elite therefore shared a common culture and set of values.\nQin dynasty.\nLoss of texts.\nAccording to Sima Qian's \"Records of the Grand Historian\", after Qin Shi Huang, the first emperor of China, unified China in 221\u00a0BC, his chancellor Li Si suggested suppressing intellectual discourse to unify thought and political opinion. This was alleged to have destroyed philosophical treatises of the Hundred Schools of Thought, with the goal of strengthening the official Qin governing philosophy of Legalism. According to the \"Shiji\", three categories of books were viewed by Li Si to be most dangerous politically. These were poetry, history (especially historical records of other states than Qin), and philosophy. The ancient collection of poetry and historical records contained many stories concerning the ancient virtuous rulers. Li Si believed that if the people were to read these works they were likely to invoke the past and become dissatisfied with the present. The reason for opposing various schools of philosophy was that they advocated political ideas often incompatible with the totalitarian regime.\nModern historians doubt the details of the story, which first appeared more than a century later. Regarding the alleged Qin objective of strengthening Legalism, the traditional account is anachronistic in that Legalism was not yet a defined category of thought during the Qin period, and the \"schools of thought\" model is no longer considered to be an accurate portrayal of the intellectual history of pre-imperial China. Michael Nylan observes that despite its mythic significance, the \"burning of books and burying of scholars\" legend does not bear close scrutiny. Nylan suggests that the reason Han dynasty scholars charged the Qin with destroying the Confucian Five Classics was partly to \"slander\" the state they defeated and partly because Han scholars misunderstood the nature of the texts, for it was only after the founding of the Han that Sima Qian labeled the Five Classics as Confucian. Nylan also points out that the Qin court appointed classical scholars who were specialists on the \"Classic of Poetry\" and the \"Book of Documents\", which meant that these texts would have been exempted, and that the \"Book of Rites\" and the \"Zuo Zhuan\" did not contain the glorification of defeated feudal states which the First Emperor gave as his reason for destroying them. Nylan further suggests that the story might be based on the fact that the Qin palace was razed in 207\u00a0BC and many books were undoubtedly lost at that time. Martin Kern adds that Qin and early Han writings frequently cite the Classics, especially the \"Documents\" and the \"Classic of Poetry\", which would not have been possible if they had been burned, as reported.\nWestern Han dynasty.\nFive Classics.\nThe Five Classics () are five pre-Qin texts that became part of the state-sponsored curriculum during the Western Han dynasty, which adopted Confucianism as its official ideology. It was during this period that the texts first began to be considered together as a set collection, and to be called collectively the \"Five Classics\". Several of the texts were already prominent by the Warring States period, but the literature culture at the time did not lend itself to clear boundaries between works, so a high degree of variance between individual witnesses of the same title was common, as well as considerable intertextuality and cognate chapters between different titles. Mencius, the leading Confucian scholar of the time, regarded the \"Spring and Autumn Annals\" as being equally important as the semi-legendary chronicles of earlier periods.\nUp to the Western Han, authors would typically list the Classics in the order Poems-Documents-Rituals-Changes-Spring and Autumn. However, from the Eastern Han the default order instead became Changes-Documents-Poems-Rituals-Spring and Autumn.\nHan imperial library.\nIn 26 BCE, at the command of the emperor, Liu Xiang (77\u20136\u00a0BC) compiled the first catalogue of the imperial library, the \"Abstracts\" (), and is the first known editor of the \"Classic of Mountains and Seas\", which was finished by his son. Liu also edited collections of stories and biographies, the \"Biographies of Exemplary Women\". He has long erroneously been credited with compiling the \"Biographies of the Immortals\", a collection of Taoist hagiographies and hymns. Liu Xiang was also a poet, being credited with the \"Nine Laments\" that appears in the \"Chu Ci\".\nThe works edited and compiled by Liu Xiang include:\nThis work was continued by his son, Liu Xin, who finally completed the task after his father's death. The transmitted corpus of these classical texts all derives from the versions edited down by Liu Xiang and Liu Xin. Michael Nylan has characterised the scope of the Liu pair's editing as having been so vast that it affects our understanding of China's pre-imperial period to the same degree as the Qin unification does.\nSong dynasty.\nFour Books.\nThe Four Books () are texts illustrating the core value and belief systems in Confucianism. They were selected by Zhu Xi (1130\u20131200) during the Song dynasty to serve as general introduction to Confucian thought, and they were, in the Ming and Qing dynasties, made the core of the official curriculum for the civil service examinations.\nThey are:\nMing dynasty.\nThirteen Classics.\nThe official curriculum of the imperial examination system from the Song dynasty onward are the Thirteen Classics. In total, these works total to more than 600,000 characters that must be memorized in order to pass the examination. Moreover, these works are accompanied by extensive commentary and annotation, containing approximately 300 million characters by some estimates.\nList of classics.\nBefore 221 BC.\nIt is often difficult or impossible to precisely date pre-Qin works beyond their being \"pre-Qin\", a period of 1000 years. Information in ancient China was often by oral tradition and passed down from generations before so was rarely written down, so the older the composition of the texts may not be in a chronological order as that which was arranged and presented by their attributed \"authors\".\nThe below list is therefore organized in the order which is found in the \"Siku Quanshu\" (\"Complete Library of the Four Treasuries\"), the encyclopedic collation of the works found in the imperial library of the Qing dynasty under the Qianlong Emperor. The \"Siku Quanshu\" classifies all works into 4 top-level branches: the Confucian Classics and their secondary literature; history; philosophy; and poetry. There are sub-categories within each branch, but due to the small number of pre-Qin works in the Classics, History and Poetry branches, the sub-categories are only reproduced for the Philosophy branch.\nPhilosophy branch.\nThe philosophical typology of individual pre-imperial texts has in every case been applied retroactively, rather than consciously within the text itself. The categorization of works of these genera has been highly contentious, especially in modern times. Many modern scholars reject the continued usefulness of this model as a heuristic for understanding the shape of the intellectual landscape of the time."}
{"id": "7243", "revid": "16528233", "url": "https://en.wikipedia.org/wiki?curid=7243", "title": "Call centre", "text": "A call centre (Commonwealth spelling) or call center (American spelling; see spelling differences) is a managed capability that can be centralised or remote that is used for receiving or transmitting a large volume of enquiries by telephone. An inbound call centre is operated by a company to administer incoming product or service support or information inquiries from consumers. Outbound call centres are usually operated for sales purposes such as telemarketing, for solicitation of charitable or political donations, debt collection, market research, emergency notifications, and urgent/critical needs blood banks. A contact centre is a further extension of call centres telephony based capabilities, administers centralised handling of individual communications, including letters, faxes, live support software, social media, instant message, and email.\nA call center was previously seen as an open workspace for call center agents, with workstations that included a computer and display for each agent and were connected to an inbound/outbound call management system, and one or more supervisor stations. It can be independently operated or networked with additional centers, often linked to a corporate computer network, including mainframes, microcomputer, servers and LANs. It is expected that artificial intelligence-based chatbots will significantly impact call centre jobs and will increase productivity substantially. Many organisations have already adopted AI-based chatbots to improve their customer service experience.\nThe contact center is a central point from which all customer contacts are managed. Through contact centers, valuable information can be routed to the appropriate people or systems, contacts can be tracked, and data may be gathered. It is generally a part of the company's customer relationship management infrastructure. The majority of large companies use contact centers as a means of managing their customer interactions. These centers can be operated by either an in-house department responsible or outsourcing customer interaction to a third-party agency (known as Outsourcing Call Centres).\nHistory.\nAnswering services, as known in the 1960s through the 1980s, earlier and slightly later, involved a business that specifically provided the service. Primarily, by using an off-premises extension (OPX) for each subscribing business, connected at a switchboard at the answering service business, the answering service would answer the otherwise unattended phones of the subscribing businesses with a live operator. The live operator could take messages or relay information, doing so with greater human interactivity than a mechanical answering machine. Although undoubtedly more costly (the human service, the cost of setting up and paying the phone company for the OPX on a monthly basis), it had the advantage of being more ready to respond to the unique needs of after-hours callers. The answering service operators also had the option of calling the client and alerting them to particularly important calls.\nThe origins of call centers date back to the 1960s with the UK-based Birmingham Press and Mail, which installed Private Automated Business Exchanges (PABX) to have rows of agents handling customer contacts. By 1973, call centers had received mainstream attention after Rockwell International patented its Galaxy Automatic Call Distributor (GACD) for a telephone booking system as well as the popularization of telephone headsets as seen on televised NASA Mission Control Center events.\nDuring the late 1970s, call center technology expanded to include telephone sales, airline reservations, and banking systems. The term \"call center\" was first published and recognised by the \"Oxford English Dictionary\" in 1983. The 1980s saw the development of toll-free telephone numbers to increase the efficiency of agents and overall call volume. Call centers increased with the deregulation of long-distance calling and growth in information-dependent industries.\nAs call centres expanded, workers in North America began to join unions such as the Communications Workers of America and the United Steelworkers. In Australia, the National Union of Workers represents unionised workers; their activities form part of the Australian labour movement. In Europe, UNI Global Union of Switzerland is involved in assisting unionisation in the call center industry, and in Germany Vereinte Dienstleistungsgewerkschaft represents call centre workers.\nDuring the 1990s, call centres expanded internationally and developed into two additional subsets of communication: contact centres and outsourced bureau centres. A contact centre is a coordinated system of people, processes, technologies, and strategies that provides access to information, resources, and expertise, through appropriate channels of communication, enabling interactions that create value for the customer and organization. In contrast to in-house management, outsourced bureau contact centres are a model of contact centre that provide services on a \"pay per use\" model. The overheads of the contact centre are shared by many clients, thereby supporting a very cost effective model, especially for low volumes of calls. The modern contact centre includes automated call blending of inbound and outbound calls as well as predictive dialing capabilities, dramatically increasing agents' productivity. New implementations of more complex systems require highly skilled operational and management staff that can use multichannel online and offline tools to improve customer interactions.\nTechnology.\nCall centre technologies often include: speech recognition software which allowed Interactive Voice Response (IVR) systems to handle first levels of customer support, text mining, natural language processing to allow better customer handling, agent training via interactive scripting and automatic mining using best practices from past interactions, support automation and many other technologies to improve agent productivity and customer satisfaction. Automatic lead selection or lead steering is also intended to improve efficiencies, both for inbound and outbound campaigns. This allows inbound calls to be directly routed to the appropriate agent for the task, whilst minimising wait times and long lists of irrelevant options for people calling in.\nFor outbound calls, lead selection allows management to designate what type of leads go to which agent based on factors including skill, socioeconomic factors, past performance, and percentage likelihood of closing a sale per lead.\nThe universal queue standardises the processing of communications across multiple technologies such as fax, phone, and email. The virtual queue provides callers with an alternative to waiting on hold when no agents are available to handle inbound call demand.\nPremises-based technology.\nHistorically call centres have been built on Private branch exchange (PBX) equipment owned, hosted, and maintained by the call centre operator. The PBX can provide functions such as automatic call distribution, interactive voice response, and skills-based routing.\nVirtual call centre.\nIn a virtual call centre model, the call centre operator (business) pays a monthly or annual fee to a vendor that hosts the call centre telephony and data equipment in their own facility, cloud-based. In this model, the operator does not own, operate or host the equipment on which the call centre runs. Agents connect to the vendor's equipment through traditional PSTN telephone lines, or over voice over IP. Calls to and from prospects or contacts originate from or terminate at the vendor's data centre, rather than at the call centre operator's premises. The vendor's telephony equipment (at times data servers) then connects the calls to the call centre operator's agents.\nVirtual call centre technology allows people to work from home or any other location instead of in a traditional, centralised, call centre location, which increasingly allows people 'on the go' or with physical or other disabilities to work from desired locations \u2013 i.e. not leaving their house. The only required equipment is Internet access, a workstation, and a softphone. If the virtual call centre software utilizes webRTC, a softphone is not required to dial. The companies are preferring Virtual Call Centre services due to cost advantage. Companies can start their call centre business immediately without installing the basic infrastructure like Dialer, ACD and IVRS.\nVirtual call centres became increasingly used after the COVID-19 pandemic restricted businesses from operating with large groups of people working in close proximity.\nCloud computing.\nThrough the use of application programming interfaces (APIs), hosted and on-demand call centres that are built on cloud-based software as a service (SaaS) platforms can integrate their functionality with cloud-based applications for customer relationship management (CRM), lead management and more.\nDevelopers use APIs to enhance cloud-based call centre platform functionality\u2014including Computer telephony integration (CTI) APIs which provide basic telephony controls and sophisticated call handling from a separate application, and configuration APIs which enable graphical user interface (GUI) controls of administrative functions.\nOutsourcing.\nOutsourced call centres are often located in developing countries, where wages are significantly lower than in western countries with higher minimum wages. These include the call centre industries in the Philippines, Bangladesh, and India.\nCompanies that regularly utilise outsourced contact centre services include British Sky Broadcasting and Orange in the telecommunications industry, Adidas in the sports and leisure sector, Audi in car manufacturing and charities such as the RSPCA.\nIndustries.\nHealthcare.\nThe healthcare industry has and continues to use outbound call centre programmes for years to help manage billing, collections, and patient communication. The inbound call centre is a new and increasingly popular service for many types of healthcare facilities, including large hospitals. Inbound call centres can be outsourced or managed in-house.\nThese healthcare call centres are designed to help streamline communications, enhance patient retention and satisfaction, reduce expenses and improve operational efficiencies.\nHospitality.\nMany large hospitality companies such as the Hilton Hotels Corporation and Marriott International make use of call centres to manage reservations. These are known in the industry as \"central reservations offices\". Staff members at these call centres take calls from clients wishing to make reservations or other inquiries via a public number, usually a 1-800 number. These centres may operate as many as 24 hours per day, seven days a week, depending on the call volume the chain receives.\nEvaluation.\nMathematical theory.\nQueueing theory is a branch of mathematics in which models of service systems have been developed. A call centre can be seen as a queueing network and results from queueing theory such as the probability an arriving customer needs to wait before starting service useful for provisioning capacity. (Erlang's C formula is such a result for an M/M/c queue and approximations exist for an M/G/k queue.) Statistical analysis of call centre data has suggested arrivals are governed by an inhomogeneous Poisson process and jobs have a log-normal service time distribution. Simulation algorithms are increasingly being used to model call arrival, queueing and service levels.\nCall centre operations have been supported by mathematical models beyond queueing, with operations research, which considers a wide range of optimisation problems seeking to reduce waiting times while keeping server utilisation and therefore efficiency high.\nCriticism.\nCall centres have received criticism for low rates of pay and restrictive working practices for employees, which have been deemed as a dehumanising environment. Other research illustrates how call centre workers develop ways to counter or resist this environment by integrating local cultural sensibilities or embracing a vision of a new life. Most call centres provide electronic reports that outline performance metrics, quarterly highlights and other information about the calls made and received. This has the benefit of helping the company to plan the workload and time of its employees. However, it has also been argued that such close monitoring breaches the human right to privacy.\nComplaints are often logged by callers who find the staff do not have enough skill or authority to resolve problems, as well as appearing apathetic. These concerns are due to a business process that exhibits levels of variability because the experience a customer gets and results a company achieves on a given call are dependent upon the quality of the agent. Call centres are beginning to address this by using agent-assisted automation to standardise the process all agents use. However, more popular alternatives are using personality and skill based approaches. The various challenges encountered by call operators are discussed by several authors.\nMedia portrayals.\nCall centres located in India have been the focus of several documentary films, the 2004 film \"Thomas L. Friedman Reporting: The Other Side of Outsourcing\", the 2005 films \"John and Jane\", \"Nalini by Day, Nancy by Night\", and \"1-800-India: Importing a White-Collar Economy\", and the 2006 film \"Bombay Calling\", among others. An Indian call centre is also the subject of the 2006 film \"Outsourced\" and a key location in the 2008 film, \"Slumdog Millionaire\". The 2014 BBC fly on the wall documentary series \"The Call Centre\" gave an often distorted although humorous view of life in a Welsh call centre.\nAppointment Setting.\nAppointment setting is a specialized function within call centres, where dedicated agents focus on facilitating and scheduling meetings between clients and businesses or sales representatives. This service is particularly prevalent in various industries such as financial services, healthcare, real estate, and B2B sales, where time-sensitive and personalized communications are essential for effective client engagement.\nLead Generation.\nLead generation is a common operation for call centers, encompassing strategies and activities aimed at identifying potential customers or clients for businesses or sales representatives. It involves gathering information and generating interest among individuals or organizations who may have a potential interest in the products or services offered."}
{"id": "7244", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7244", "title": "Corrodo Gini", "text": ""}
{"id": "7245", "revid": "43623777", "url": "https://en.wikipedia.org/wiki?curid=7245", "title": "Caliph", "text": ""}
{"id": "7246", "revid": "47110710", "url": "https://en.wikipedia.org/wiki?curid=7246", "title": "Charles Messier", "text": "Charles Messier (; 26\u00a0June 1730 \u2013 12\u00a0April 1817) was a French astronomer. He published an astronomical catalogue consisting of 110\u00a0nebulae and star clusters, which came to be known as the \"Messier objects\", referred to with the letter M and their number between 1 and 110. Messier's purpose for the catalogue was to help astronomical observers distinguish between permanent and transient visually diffuse objects in the sky.\nBiography.\nMessier was born in Badonviller in the Lorraine region of France, in 1730, the tenth of twelve children of Fran\u00e7oise B. Grandblaise and Nicolas Messier, a Court usher. Six of his brothers and sisters died while young, and his father died in 1741. Charles' interest in astronomy was stimulated by the appearance of the great six-tailed comet in 1744 and by an annular solar eclipse visible from his hometown on 25\u00a0July 1748.\nIn 1751, Messier entered the employ of Joseph Nicolas Delisle, the astronomer of the French Navy, who instructed him to keep careful records of his observations. Messier's first documented observation was that of the Mercury transit of 6\u00a0May 1753, followed by his observations journals at Cluny Hotel and at the French Navy observatories.\nIn 1764, Messier was made a fellow of the Royal Society; in 1769, he was elected a foreign member of the Royal Swedish Academy of Sciences; and on 30\u00a0June 1770, he was elected to the French Academy of Sciences. He was given the nickname \"Ferret of Comets\" by King Louis XV.\nMessier discovered 13 comets:\nHe also co-discovered comet C/1801 N1 (Pons), a discovery shared with several other observers including Pons, M\u00e9chain, and Bouvard. \nNear the end of his life, Messier self-published a booklet connecting the great comet of 1769 to the birth of Napoleon, who was in power at the time of publishing. According to Maik Meyer:\nMessier is buried in P\u00e8re Lachaise Cemetery in the 20th arrondissement of Paris.\nMessier catalogue.\nMessier's occupation as a comet hunter led him to continually come across fixed diffuse objects in the night sky which could be mistaken for comets. He compiled a list of them, in collaboration with his friend and assistant Pierre M\u00e9chain (who may have found at least 20 of the objects), to avoid wasting time sorting them out from the comets they were looking for. The entries are now known to be 39\u00a0galaxies, 4\u00a0planetary nebulae, 7\u00a0other types of nebulae, 26\u00a0open star clusters and 29\u00a0globular star clusters.\nMessier did his observing with a 100\u00a0mm (four-inch) refracting telescope from H\u00f4tel de Cluny (now the Mus\u00e9e national du Moyen \u00c2ge), in downtown Paris, France. The list he compiled only contains objects found in the area of the sky Messier could observe, from the north celestial pole to a declination of about \u221235.7\u00b0\u00a0. They are not organized scientifically by object type, or by location. The first version of Messier's catalogue contained 45\u00a0objects and was published in 1774 in the journal of the French Academy of Sciences in Paris. In addition to his own discoveries, this version included objects previously observed by other astronomers, with only 17 of the 45\u00a0objects being discovered by Messier himself. By 1780 the catalog had increased to 80\u00a0objects.\nThe final version of the catalogue was published in 1781, in the 1784 issue of \"Connaissance des Temps\". The final list of Messier objects had grown to 103. On several occasions between 1921 and 1966, astronomers and historians discovered evidence of another seven objects that were observed either by Messier or by M\u00e9chain, shortly after the final version was published. These seven objects, M\u00a0104 through M\u00a0110, are accepted by astronomers as \"official\" Messier objects.\nThe objects' Messier designations, from M\u00a01 to M\u00a0110, are still used by professional and amateur astronomers today and their relative brightness makes them popular objects in the amateur astronomical community.\nLegacy.\nThe lunar crater Messier and the asteroid 7359 Messier were named in his honour."}
{"id": "7247", "revid": "9023670", "url": "https://en.wikipedia.org/wiki?curid=7247", "title": "Cemetery H culture", "text": "The Cemetery H culture was a Bronze Age culture in the Punjab region in the northern part of the Indian subcontinent, from about 1900 BCE until about 1300 BCE. It is regarded as a regional form of the late phase of the Harappan (Indus Valley) civilisation (alongside the Jhukar culture of Sindh and Rangpur culture of Gujarat), but also as a phase of the Indo-Aryan migrations.\nOrigins.\nThe Cemetery H culture was located in and around the Punjab region in present-day India and Pakistan. It was named after a cemetery found in \"area H\" at Harappa. Remains of the culture have been dated from about 1900 BCE until about 1300 BCE.\nAccording to Mohammad Rafique Mughal, the Cemetery H culture developed out of the northern part of the Indus Valley civilization around 1700 BCE, being part of the Punjab Phase, one of three cultural phases that developed in the Localization Era or \"Late Harappan phase\" of the Indus Valley Tradition. According to Kenoyer, the Cemetery H culture \"may only reflect a change in the focus of settlement organization from that which was the pattern of the earlier Harappan phase and not cultural discontinuity, urban decay, invading aliens, or site abandonment, all of which have been suggested in the past.\" According to Kennedy and Mallory &amp; Adams, the Cemetery H culture also \"shows clear biological affinities\" with the earlier population of Harappa.\nSome traits of the Cemetery H culture have been associated with the Swat culture, which has been regarded as evidence of the Indo-Aryan movement toward the Indian subcontinent. According to Parpola, the Cemetery H culture represents a first wave of Indo-Aryan migration from as early as 1900 BCE, which was followed by a migration to the Punjab \u20131400 BCE. According to Kochhar, the Swat IV co-founded the Harappan Cemetery H phase in Punjab (2000\u20131800 BCE), while the Rigvedic Indo-Aryans of Swat V later absorbed the Cemetery H people and gave rise to the Painted Grey Ware culture (to 1400 BCE).\nTogether with the Gandhara grave culture and the Ochre Coloured Pottery culture, the Cemetery H culture is considered by some scholars as a factor in the formation of the Vedic civilization.\nFeatures.\nThe distinguishing features of this culture include:\nSome of the designs painted on the Cemetery H funerary urns have been interpreted through the lens of Vedic mythology:\nFor instance, peacocks with hollow bodies and a small human form inside, which has been interpreted as the souls of the dead, and a hound that can be seen as the hound of Yama, the god of death. This may indicate the introduction of new religious beliefs during this period, but the archaeological evidence does not support the hypothesis that the Cemetery H people were the destroyers of the Harappan cities.\nArchaeology.\nCremation in India is first attested in the Cemetery H culture, a practice previously described in the Vedas. The Rigveda contains a reference to the emerging practice, in RV 10.15.14, where the forefathers \"both cremated (\"agnidagdh\u00e1-\") and uncremated (\"\u00e1nagnidagdha-\")\" are invoked."}
{"id": "7248", "revid": "33416498", "url": "https://en.wikipedia.org/wiki?curid=7248", "title": "Corrado Gini", "text": "Corrado Gini (23 May 1884 \u2013 13 March 1965) was an Italian statistician, demographer and sociologist who developed the Gini coefficient, a measure of the income inequality in a society. Gini was a proponent of organicism and applied it to nations. Gini was a eugenicist, and prior to and during World War II, he was an advocate of Italian Fascism. Following the war, he founded the Italian Unionist Movement, which advocated for the annexation of Italy by the United States.\nCareer.\nGini was born on May 23, 1884, in Motta di Livenza, near Treviso, into an old landed family. He entered the Faculty of Law at the University of Bologna, where in addition to law he studied mathematics, economics, and biology.\nGini's scientific work ran in two directions: towards the social sciences and towards statistics. His interests ranged well beyond the formal aspects of statistics\u2014to the laws that govern biological and social phenomena.\nHis first published work was \"Il sesso dal punto di vista statistico\" (1908). This work is a thorough review of the natal sex ratio, looking at past theories and at how new hypothesis fit the statistical data. In particular, it presents evidence that the tendency to produce one or the other sex of child is, to some extent, heritable.\nHe published the Gini coefficient in the 1912 paper \"Variability and Mutability\" (). Also called the Gini index and the Gini ratio, it is a measure of statistical dispersion intended to represent the income inequality within a nation or other group.\nIn 1910, he acceded to the Chair of Statistics in the University of Cagliari and then at Padua in 1913.\nHe founded the statistical journal \"Metron\" in 1920, directing it until his death; it only accepted articles with practical applications.\nHe became a professor at the Sapienza University of Rome in 1925. At the University, he founded a lecture course on sociology, maintaining it until his retirement. He also set up the School of Statistics in 1928, and, in 1936, the Faculty of Statistical, Demographic and Actuarial Sciences.\nUnder fascism.\nIn 1926, he was appointed President of the Central Institute of Statistics in Rome. This he organised as a single centre for Italian statistical services. He was a close intimate of Mussolini throughout the 20s. He resigned from his position within the institute in 1932.\nIn 1927 he published a treatise entitled \"The Scientific Basis of Fascism\".\nIn 1929, Gini founded the Italian Committee for the Study of Population Problems (\"Comitato italiano per lo studio dei problemi della popolazione) \" which, two years later, organised the first Population Congress in Rome.\nA eugenicist apart from being a demographer, Gini led an expedition to survey Polish populations, among them the Karaites. Gini was throughout the 20s a supporter of fascism, and expressed his hope that Nazi Germany and Fascist Italy would emerge as victors in WW2. However, he never supported any measure of exclusion of the Jews.\nMilestones during the rest of his career include:\nItalian Unionist Movement.\nOn October 12, 1944, Gini joined with the Calabrian activist Santi Paladino, and fellow-statistician Ugo Damiani to found the Italian Unionist Movement, for which the emblem was the Stars and Stripes, the Italian flag and a world map. According to the three men, the government of the United States should annex all free and democratic nations worldwide, thereby transforming itself into a world government, and allowing Washington, D.C. to maintain Earth in a perpetual condition of peace. The party existed up to 1948 but had little success and its aims were not supported by the United States.\nOrganicism and nations.\nGini was a proponent of organicism and saw nations as organic in nature. Gini shared the view held by Oswald Spengler that populations go through a cycle of birth, growth, and decay. Gini claimed that nations at a primitive level have a high birth rate, but, as they evolve, the upper classes birth rate drops while the lower class birth rate, while higher, will inevitably deplete as their stronger members emigrate, die in war, or enter into the upper classes. If a nation continues on this path without resistance, Gini claimed the nation would enter a final decadent stage where the nation would degenerate as noted by decreasing birth rate, decreasing cultural output, and the lack of imperial conquest. At this point, the decadent nation with its aging population can be overrun by a more youthful and vigorous nation. Gini's organicist theories of nations and natality are believed to have influenced policies of Italian Fascism.\nHonours.\nThe following honorary degrees were conferred upon him:"}
{"id": "7249", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=7249", "title": "Crankshaft", "text": "A crankshaft is a mechanical component used in a piston engine to convert the reciprocating motion into rotational motion. The crankshaft is a rotating shaft containing one or more crankpins, that are driven by the pistons via the connecting rods.\nThe crankpins are also called \"rod bearing journals\", and they rotate within the \"big end\" of the connecting rods.\nMost modern crankshafts are located in the engine block. They are made from steel or cast iron, using either a forging, casting or machining process.\nDesign.\nThe crankshaft is located within the engine block and held in place via main bearings which allow the crankshaft to rotate within the block. The up-down motion of each piston is transferred to the crankshaft via connecting rods. A flywheel is often attached to one end of the crankshaft, in order to smoothen the power delivery and reduce vibration.\nA crankshaft is subjected to enormous stresses, in some cases more than per cylinder. Crankshafts for single-cylinder engines are usually a simpler design than for engines with multiple cylinders.\nBearings.\nThe crankshaft is able to rotate in the engine block due to the 'main bearings'. Since the crankshaft is subject to large horizontal and torsional forces from each cylinder, these main bearings are located at various points along the crankshaft, rather than just one at each end. The number of main bearings is determined based on the overall load factor and the maximum engine speed. Crankshafts in diesel engines often use a main bearing between every cylinder and at both ends of the crankshaft, due to the high forces of combustion present.\nFlexing of the crankshaft was a factor in V8 engines replacing straight-eight engines in the 1950s; the long crankshafts of the latter suffered from an unacceptable amount of flex when engine designers began using higher compression ratios and higher engine speeds (RPM).\nPiston stroke.\nThe distance between the axis of the crankpins and the axis of the crankshaft determines the stroke length of the engine.\nMost modern car engines are classified as \"over square\" or short-stroke, wherein the stroke is less than the diameter of the cylinder bore. A common way to increase the low-RPM torque of an engine is to increase the stroke, sometimes known as \"stroking\" the engine. Historically, the trade-off for a long-stroke engine was a lower rev limit and increased vibration at high RPM, due to the increased piston velocity.\nCross-plane and flat-plane configurations.\nWhen designing an engine, the crankshaft configuration is closely related to the engine's firing order.\nMost production V8 engines (such as the Ford Modular engine and the General Motors LS engine) use a cross-plane crank whereby the crank throws are spaced 90 degrees apart. However, some high-performance V8 engines (such as the Ferrari 488) instead use a flat-plane crank, whereby the throws are spaced 180\u00b0 apart, which essentially results in two inline-four engines sharing a common crankcase. Flat-plane engines are usually able to operate at higher RPM, however they have higher second-order vibrations, so they are better suited to racing car engines.\nEngine balance.\nFor some engines it is necessary to provide counterweights for the reciprocating mass of the piston, conrods and crankshaft, in order to improve the engine balance. These counterweights are typically cast as part of the crankshaft but, occasionally, are bolt-on pieces.\nFlying arms.\nIn some engines, the crankshaft contains direct links between adjacent crankpins, without the usual intermediate main bearing. These links are called \"flying arms\". This arrangement is sometimes used in V6 and V8 engines, in order to maintain an even firing interval while using different V angles, and to reduce the number of main bearings required. The downside of flying arms is that the rigidity of the crankshaft is reduced, which can cause problems at high RPM or high power outputs.\nCounter-rotating crankshafts.\nIn most engines, each connecting rod is attached a single crankshaft, which results in the angle of the connecting rod varying as the piston moves through its stroke. This variation in angle pushes the pistons against the cylinder wall, which causes friction between the piston and cylinder wall. To prevent this, some early engines \u2013 such as the 1900\u20131904 Lanchester Engine Company flat-twin engines \u2013 connected each piston to two crankshafts that are rotating in opposite directions. This arrangement cancels out the lateral forces and reduces the requirement for counterweights. This design is rarely used, however a similar principle applies to balance shafts, which are occasionally used.\nConstruction.\nForged crankshafts.\nCrankshafts can be created from a steel bar using roll forging. Today, manufacturers tend to favour the use of forged crankshafts due to their lighter weight, more compact dimensions and better inherent damping. With forged crankshafts, vanadium micro-alloyed steels are mainly used as these steels can be air-cooled after reaching high strengths without additional heat treatment, except for the surface hardening of the bearing surfaces. The low alloy content also makes the material cheaper than high-alloy steels. Carbon steels also require additional heat treatment to reach the desired properties.\nCast crankshafts.\nAnother construction method is to cast the crankshaft from ductile iron. Cast iron crankshafts are today mostly found in cheaper production engines where the loads are lower.\nMachined crankshafts.\nCrankshafts can also be machined from billet, often a bar of high quality vacuum remelted steel. Though the fiber flow (local inhomogeneities of the material's chemical composition generated during casting) does not follow the shape of the crankshaft (which is undesirable), this is usually not a problem since higher quality steels, which normally are difficult to forge, can be used. Per unit, these crankshafts tend to be expensive due to the large amount of material that must be removed with lathes and milling machines, the high material cost, and the additional heat treatment required. However, since no expensive tooling is needed, this production method allows small production runs without high up-front costs.\nHistory.\nCrankshaft.\nIn 9th century Abbasid Baghdad, automatically operated cranks appear in several of the hydraulic devices described by the Ban\u016b M\u016bs\u0101 brothers in the \"Book of Ingenious Devices\". These automatically operated cranks appear in several devices, two of which contain an action which approximates to that of a crankshaft, five centuries before the earliest known European description of a crankshaft. However, the automatic crank mechanism described by the Ban\u016b M\u016bs\u0101 would not have allowed a full rotation, but only a small modification was required to convert it to a crankshaft.\nIn the Artuqid Sultanate, Arab engineer Ismail al-Jazari (1136\u20131206) described a crank and connecting rod system in a rotating machine for two of his water-raising machines, which include both crank and shaft mechanisms.\nThe Italian physician Guido da Vigevano (), planning for a new Crusade, made illustrations for a paddle boat and war carriages that were propelled by manually turned compound cranks and gear wheels, identified as an early crankshaft prototype by Lynn Townsend White.\nCrankshafts were described by Leonardo da Vinci (1452\u20131519) and a Dutch farmer and windmill owner by the name Cornelis Corneliszoon van Uitgeest in 1592. His wind-powered sawmill used a crankshaft to convert a windmill's circular motion into a back-and-forward motion powering the saw. Corneliszoon was granted a patent for his crankshaft in 1597.\nFrom the 16th century onwards, evidence of cranks and connecting rods integrated into machine design becomes abundant in the technological treatises of the period: Agostino Ramelli's \"The Diverse and Artifactitious Machines\" of 1588 depicts eighteen examples, a number that rises in the \"Theatrum Machinarum Novum\" by Georg Andreas B\u00f6ckler to 45 different machines. Cranks were formerly common on some machines in the early 20th century; for example almost all phonographs before the 1930s were powered by clockwork motors wound with cranks. Reciprocating piston engines use cranks to convert the linear piston motion into rotational motion. Internal combustion engines of early 20th century automobiles were usually started with hand cranks, before electric starters came into general use."}
{"id": "7250", "revid": "713587", "url": "https://en.wikipedia.org/wiki?curid=7250", "title": "CNS", "text": "CNS may refer to:"}
{"id": "7251", "revid": "962462", "url": "https://en.wikipedia.org/wiki?curid=7251", "title": "Central nervous system", "text": "The central nervous system (CNS) is the part of the nervous system consisting primarily of the brain and spinal cord. The CNS is so named because the brain integrates the received information and coordinates and influences the activity of all parts of the bodies of bilaterally symmetric and triploblastic animals\u2014that is, all multicellular animals except sponges and diploblasts. It is a structure composed of nervous tissue positioned along the rostral (nose end) to caudal (tail end) axis of the body and may have an enlarged section at the rostral end which is a brain. Only arthropods, cephalopods and vertebrates have a true brain, though precursor structures exist in onychophorans, gastropods and lancelets.\nThe rest of this article exclusively discusses the vertebrate central nervous system, which is radically distinct from all other animals.\nOverview.\nIn vertebrates, the brain and spinal cord are both enclosed in the meninges. The meninges provide a barrier to chemicals dissolved in the blood, protecting the brain from most neurotoxins commonly found in food. Within the meninges the brain and spinal cord are bathed in cerebral spinal fluid which replaces the body fluid found outside the cells of all bilateral animals.\nIn vertebrates, the CNS is contained within the dorsal body cavity, while the brain is housed in the cranial cavity within the skull. The spinal cord is housed in the spinal canal within the vertebrae. Within the CNS, the interneuronal space is filled with a large amount of supporting non-nervous cells called neuroglia or glia from the Greek for \"glue\".\nIn vertebrates, the CNS also includes the retina and the optic nerve (cranial nerve II), as well as the olfactory nerves and olfactory epithelium. As parts of the CNS, they connect directly to brain neurons without intermediate ganglia. The olfactory epithelium is the only central nervous tissue outside the meninges in direct contact with the environment, which opens up a pathway for therapeutic agents which cannot otherwise cross the meninges barrier.\nStructure.\nThe CNS consists of two major structures: the brain and spinal cord. The brain is encased in the skull, and protected by the cranium. The spinal cord is continuous with the brain and lies caudally to the brain. It is protected by the vertebrae. The spinal cord reaches from the base of the skull, and continues through or starting below the foramen magnum, and terminates roughly level with the first or second lumbar vertebra, occupying the upper sections of the vertebral canal.\nWhite and gray matter.\nMicroscopically, there are differences between the neurons and tissue of the CNS and the peripheral nervous system (PNS). The CNS is composed of white and gray matter. This can also be seen macroscopically on brain tissue. The white matter consists of axons and oligodendrocytes, while the gray matter consists of neurons and unmyelinated fibers. Both tissues include a number of glial cells (although the white matter contains more), which are often referred to as supporting cells of the CNS. Different forms of glial cells have different functions, some acting almost as scaffolding for neuroblasts to climb during neurogenesis such as bergmann glia, while others such as microglia are a specialized form of macrophage, involved in the immune system of the brain as well as the clearance of various metabolites from the brain tissue. Astrocytes may be involved with both clearance of metabolites as well as transport of fuel and various beneficial substances to neurons from the capillaries of the brain. Upon CNS injury astrocytes will proliferate, causing gliosis, a form of neuronal scar tissue, lacking in functional neurons.\nThe brain (cerebrum as well as midbrain and hindbrain) consists of a cortex, composed of neuron-bodies constituting gray matter, while internally there is more white matter that form tracts and commissures. Apart from cortical gray matter there is also subcortical gray matter making up a large number of different nuclei.\nSpinal cord.\nFrom and to the spinal cord are projections of the peripheral nervous system in the form of spinal nerves (sometimes segmental nerves). The nerves connect the spinal cord to skin, joints, muscles etc. and allow for the transmission of efferent motor as well as afferent sensory signals and stimuli. This allows for voluntary and involuntary motions of muscles, as well as the perception of senses.\nAll in all 31 spinal nerves project from the brain stem, some forming plexa as they branch out, such as the brachial plexa, sacral plexa etc. Each spinal nerve will carry both sensory and motor signals, but the nerves synapse at different regions of the spinal cord, either from the periphery to sensory relay neurons that relay the information to the CNS or from the CNS to motor neurons, which relay the information out.\nThe spinal cord relays information up to the brain through spinal tracts through the final common pathway to the thalamus and ultimately to the cortex.\nCranial nerves.\nApart from the spinal cord, there are also peripheral nerves of the PNS that synapse through intermediaries or ganglia directly on the CNS. These 12 nerves exist in the head and neck region and are called cranial nerves. Cranial nerves bring information to the CNS to and from the face, as well as to certain muscles (such as the trapezius muscle, which is innervated by accessory nerves as well as certain cervical spinal nerves).\nTwo pairs of cranial nerves; the olfactory nerves and the optic nerves are often considered structures of the CNS. This is because they do not synapse first on peripheral ganglia, but directly on CNS neurons. The olfactory epithelium is significant in that it consists of CNS tissue expressed in direct contact to the environment, allowing for administration of certain pharmaceuticals and drugs.\nBrain.\nAt the anterior end of the spinal cord lies the brain. The brain makes up the largest portion of the CNS. It is often the main structure referred to when speaking of the nervous system in general. The brain is the major functional unit of the CNS. While the spinal cord has certain processing ability such as that of spinal locomotion and can process reflexes, the brain is the major processing unit of the nervous system.\nBrainstem.\nThe brainstem consists of the medulla, the pons and the midbrain. The medulla can be referred to as an extension of the spinal cord, which both have similar organization and functional properties. The tracts passing from the spinal cord to the brain pass through here.\nRegulatory functions of the medulla nuclei include control of blood pressure and breathing. Other nuclei are involved in balance, taste, hearing, and control of muscles of the face and neck.\nThe next structure rostral to the medulla is the pons, which lies on the ventral anterior side of the brainstem. Nuclei in the pons include pontine nuclei which work with the cerebellum and transmit information between the cerebellum and the cerebral cortex.\nIn the dorsal posterior pons lie nuclei that are involved in the functions of breathing, sleep, and taste.\nThe midbrain, or mesencephalon, is situated above and rostral to the pons. It includes nuclei linking distinct parts of the motor system, including the cerebellum, the basal ganglia and both cerebral hemispheres, among others. Additionally, parts of the visual and auditory systems are located in the midbrain, including control of automatic eye movements.\nThe brainstem at large provides entry and exit to the brain for a number of pathways for motor and autonomic control of the face and neck through cranial nerves, Autonomic control of the organs is mediated by the tenth cranial nerve. A large portion of the brainstem is involved in such autonomic control of the body. Such functions may engage the heart, blood vessels, and pupils, among others.\nThe brainstem also holds the reticular formation, a group of nuclei involved in both arousal and alertness.\nCerebellum.\nThe cerebellum lies behind the pons. The cerebellum is composed of several dividing fissures and lobes. Its function includes the control of posture and the coordination of movements of parts of the body, including the eyes and head, as well as the limbs. Further, it is involved in motion that has been learned and perfected through practice, and it will adapt to new learned movements.\nDespite its previous classification as a motor structure, the cerebellum also displays connections to areas of the cerebral cortex involved in language and cognition. These connections have been shown by the use of medical imaging techniques, such as functional MRI and Positron emission tomography.\nThe body of the cerebellum holds more neurons than any other structure of the brain, including that of the larger cerebrum, but is also more extensively understood than other structures of the brain, as it includes fewer types of different neurons. It handles and processes sensory stimuli, motor information, as well as balance information from the vestibular organ.\nDiencephalon.\nThe two structures of the diencephalon worth noting are the thalamus and the hypothalamus. The thalamus acts as a linkage between incoming pathways from the peripheral nervous system as well as the optical nerve (though it does not receive input from the olfactory nerve) to the cerebral hemispheres. Previously it was considered only a \"relay station\", but it is engaged in the sorting of information that will reach cerebral hemispheres (neocortex).\nApart from its function of sorting information from the periphery, the thalamus also connects the cerebellum and basal ganglia with the cerebrum. In common with the aforementioned reticular system the thalamus is involved in wakefulness and consciousness, such as though the SCN.\nThe hypothalamus engages in functions of a number of primitive emotions or feelings such as hunger, thirst and maternal bonding. This is regulated partly through control of secretion of hormones from the pituitary gland. Additionally the hypothalamus plays a role in motivation and many other behaviors of the individual.\nCerebrum.\nThe cerebrum of cerebral hemispheres make up the largest visual portion of the human brain. Various structures combine to form the cerebral hemispheres, among others: the cortex, basal ganglia, amygdala and hippocampus. The hemispheres together control a large portion of the functions of the human brain such as emotion, memory, perception and motor functions. Apart from this the cerebral hemispheres stand for the cognitive capabilities of the brain.\nConnecting each of the hemispheres is the corpus callosum as well as several additional commissures.\nOne of the most important parts of the cerebral hemispheres is the cortex, made up of gray matter covering the surface of the brain. Functionally, the cerebral cortex is involved in planning and carrying out of everyday tasks.\nThe hippocampus is involved in storage of memories, the amygdala plays a role in perception and communication of emotion, while the basal ganglia play a major role in the coordination of voluntary movement.\nDifference from the peripheral nervous system.\nThe PNS consists of neurons, axons, and Schwann cells. Oligodendrocytes and Schwann cells have similar functions in the CNS and PNS, respectively. Both act to add myelin sheaths to the axons, which acts as a form of insulation allowing for better and faster proliferation of electrical signals along the nerves. Axons in the CNS are often very short, barely a few millimeters, and do not need the same degree of isolation as peripheral nerves. Some peripheral nerves can be over 1 meter in length, such as the nerves to the big toe. To ensure signals move at sufficient speed, myelination is needed.\nThe way in which the Schwann cells and oligodendrocytes myelinate nerves differ. A Schwann cell usually myelinates a single axon, completely surrounding it. Sometimes, they may myelinate many axons, especially when in areas of short axons. Oligodendrocytes usually myelinate several axons. They do this by sending out thin projections of their cell membrane, which envelop and enclose the axon.\nDevelopment.\nDuring early development of the vertebrate embryo, a longitudinal groove on the neural plate gradually deepens and the ridges on either side of the groove (the neural folds) become elevated, and ultimately meet, transforming the groove into a closed tube called the neural tube. The formation of the neural tube is called neurulation. At this stage, the walls of the neural tube contain proliferating neural stem cells in a region called the ventricular zone. The neural stem cells, principally radial glial cells, multiply and generate neurons through the process of neurogenesis, forming the rudiment of the CNS.\nThe neural tube gives rise to both brain and spinal cord. The anterior (or 'rostral') portion of the neural tube initially differentiates into three brain vesicles (pockets): the prosencephalon at the front, the mesencephalon, and, between the mesencephalon and the spinal cord, the rhombencephalon. (By six weeks in the human embryo) the prosencephalon then divides further into the telencephalon and diencephalon; and the rhombencephalon divides into the metencephalon and myelencephalon. The spinal cord is derived from the posterior or 'caudal' portion of the neural tube.\nAs a vertebrate grows, these vesicles differentiate further still. The telencephalon differentiates into, among other things, the striatum, the hippocampus and the neocortex, and its cavity becomes the first and second ventricles (lateral ventricles). Diencephalon elaborations include the subthalamus, hypothalamus, thalamus and epithalamus, and its cavity forms the third ventricle. The tectum, pretectum, cerebral peduncle and other structures develop out of the mesencephalon, and its cavity grows into the mesencephalic duct (cerebral aqueduct). The metencephalon becomes, among other things, the pons and the cerebellum, the myelencephalon forms the medulla oblongata, and their cavities develop into the fourth ventricle.\nEvolution.\nPlanaria.\nPlanarians, members of the phylum Platyhelminthes (flatworms), have the simplest, clearly defined delineation of a nervous system into a CNS and a PNS.\nTheir primitive brains, consisting of two fused anterior ganglia, and longitudinal nerve cords form the CNS. Like vertebrates, have a distinct CNS and PNS. The nerves projecting laterally from the CNS form their PNS.\nA molecular study found that more than 95% of the 116 genes involved in the nervous system of planarians, which includes genes related to the CNS, also exist in humans.\nArthropoda.\nIn arthropods, the ventral nerve cord, the subesophageal ganglia and the supraesophageal ganglia are usually seen as making up the CNS. Arthropoda, unlike vertebrates, have inhibitory motor neurons due to their small size.\nChordata.\nThe CNS of chordates differs from that of other animals in being placed dorsally in the body, above the gut and notochord/spine. The basic pattern of the CNS is highly conserved throughout the different species of vertebrates and during evolution. The major trend that can be observed is towards a progressive telencephalisation: the telencephalon of reptiles is only an appendix to the large olfactory bulb, while in mammals it makes up most of the volume of the CNS. In the human brain, the telencephalon covers most of the diencephalon and the entire mesencephalon. Indeed, the allometric study of brain size among different species shows a striking continuity from rats to whales, and allows us to complete the knowledge about the evolution of the CNS obtained through cranial endocasts.\nMammals.\nMammals \u2013 which appear in the fossil record after the first fishes, amphibians, and reptiles \u2013 are the only vertebrates to possess the evolutionarily recent, outermost part of the cerebral cortex (main part of the telencephalon excluding olfactory bulb) known as the neocortex. This part of the brain is, in mammals, involved in higher thinking and further processing of all senses in the sensory cortices (processing for smell was previously only done by its bulb while those for non-smell senses were only done by the tectum). The neocortex of monotremes (the duck-billed platypus and several species of spiny anteaters) and of marsupials (such as kangaroos, koalas, opossums, wombats, and Tasmanian devils) lack the convolutions \u2013 gyri and sulci \u2013 found in the neocortex of most placental mammals (eutherians).\nWithin placental mammals, the size and complexity of the neocortex increased over time. The area of the neocortex of mice is only about 1/100 that of monkeys, and that of monkeys is only about 1/10 that of humans. In addition, rats lack convolutions in their neocortex (possibly also because rats are small mammals), whereas cats have a moderate degree of convolutions, and humans have quite extensive convolutions. Extreme convolution of the neocortex is found in dolphins, possibly related to their complex echolocation.\nClinical significance.\nDiseases.\nThere are many CNS diseases and conditions, including infections such as encephalitis and poliomyelitis, early-onset neurological disorders including ADHD and autism, seizure disorders such as epilepsy, headache disorders such as migraine, late-onset neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and essential tremor, autoimmune and inflammatory diseases such as multiple sclerosis and acute disseminated encephalomyelitis, genetic disorders such as Krabbe's disease and Huntington's disease, as well as amyotrophic lateral sclerosis and adrenoleukodystrophy. Lastly, cancers of the central nervous system can cause severe illness and, when malignant, can have very high mortality rates. Symptoms depend on the size, growth rate, location and malignancy of tumors and can include alterations in motor control, hearing loss, headaches and changes in cognitive ability and autonomic functioning.\nSpecialty professional organizations recommend that neurological imaging of the brain be done only to answer a specific clinical question and not as routine screening."}
{"id": "7252", "revid": "18174831", "url": "https://en.wikipedia.org/wiki?curid=7252", "title": "Cell cycle", "text": "The cell cycle, or cell-division cycle, is the sequential series of events that take place in a cell that causes it to divide into two daughter cells. These events include the growth of the cell, duplication of its DNA (DNA replication) and some of its organelles, and subsequently the partitioning of its cytoplasm, chromosomes and other components into two daughter cells in a process called cell division.\nIn eukaryotic cells (having a cell nucleus) including animal, plant, fungal, and protist cells, the cell cycle is divided into two main stages: interphase, and the M phase that includes mitosis and cytokinesis. During interphase, the cell grows, accumulating nutrients needed for mitosis, and replicates its DNA and some of its organelles. During the M phase, the replicated chromosomes, organelles, and cytoplasm separate into two new daughter cells. To ensure the proper replication of cellular components and division, there are control mechanisms known as cell cycle checkpoints after each of the key steps of the cycle that determine if the cell can progress to the next phase.\nIn cells without nuclei the prokaryotes, bacteria and archaea, the cell cycle is divided into the B, C, and D periods. The B period extends from the end of cell division to the beginning of DNA replication. DNA replication occurs during the C period. The D period refers to the stage between the end of DNA replication and the splitting of the bacterial cell into two daughter cells.\nIn single-celled organisms, a single cell-division cycle is how the organism reproduces to ensure its survival. In multicellular organisms such as plants and animals, a series of cell-division cycles is how the organism develops from a single-celled fertilized egg into a mature organism, and is also the process by which hair, skin, blood cells, and some internal organs are regenerated and healed (with possible exception of nerves; see nerve damage). After cell division, each of the daughter cells begin the interphase of a new cell cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of the cell division.\nPhases.\nThe eukaryotic cell cycle consists of four distinct phases: G1 phase, S phase (synthesis), G2 phase (collectively known as interphase) and M phase (mitosis and cytokinesis). M phase is itself composed of two tightly coupled processes: mitosis, in which the cell's nucleus divides, and cytokinesis, in which the cell's cytoplasm and cell membrane divides forming two daughter cells. Activation of each phase is dependent on the proper progression and completion of the previous one. Cells that have temporarily or reversibly stopped dividing are said to have entered a state of quiescence known as G0 phase or \"resting phase\".\nG0 phase (quiescence).\nG0 is a resting phase where the cell has left the cycle and has stopped dividing. The cell cycle starts with this phase. Non-proliferative (non-dividing) cells in multicellular eukaryotes generally enter the quiescent G0 state from G1 and may remain quiescent for long periods of time, possibly indefinitely (as is often the case for neurons). This is very common for cells that are fully differentiated. Some cells enter the G0 phase semi-permanently and are considered post-mitotic, e.g., some liver, kidney, and stomach cells. Many cells do not enter G0 and continue to divide throughout an organism's life, e.g., epithelial cells.\nThe word \"post-mitotic\" is sometimes used to refer to both quiescent and senescent cells. Cellular senescence occurs in response to DNA damage and external stress and usually constitutes an arrest in G1. Cellular senescence may make a cell's progeny nonviable; it is often a biochemical alternative to the self-destruction of such a damaged cell by apoptosis.\nInterphase.\nInterphase represents the phase between two successive M phases. Interphase is a series of changes that takes place in a newly formed cell and its nucleus before it becomes capable of division again. It is also called preparatory phase or intermitosis. Typically interphase lasts for at least 91% of the total time required for the cell cycle.\nInterphase proceeds in three stages, G1, S, and G2, followed by the cycle of mitosis and cytokinesis. The cell's nuclear DNA contents are duplicated during S phase.\nG1 phase (First growth phase or Post mitotic gap phase).\nThe first phase within interphase, from the end of the previous M phase until the beginning of DNA synthesis, is called G1 (G indicating \"gap\"). It is also called the growth phase. During this phase, the biosynthetic activities of the cell, which are considerably slowed down during M phase, resume at a high rate. The duration of G1 is highly variable, even among different cells of the same species. In this phase, the cell increases its supply of proteins, increases the number of organelles (such as mitochondria, ribosomes), and grows in size. In G1 phase, a cell has three options. \nThe deciding point is called check point (Restriction point). This check point is called the restriction point or START and is regulated by G1/S cyclins, which cause transition from G1 to S phase. Passage through the G1 check point commits the cell to division.\nS phase (DNA replication).\nThe ensuing S phase starts when DNA synthesis commences; when it is complete, all of the chromosomes have been replicated, i.e., each chromosome consists of two sister chromatids. Thus, during this phase, the amount of DNA in the cell has doubled, though the ploidy and number of chromosomes are unchanged. Rates of RNA transcription and protein synthesis are very low during this phase. An exception to this is histone production, most of which occurs during the S phase.\nG2 phase (growth).\nG2 phase occurs after DNA replication and is a period of protein synthesis and rapid cell growth to prepare the cell for mitosis. During this phase microtubules begin to reorganize to form a spindle (preprophase). Before proceeding to mitotic phase, cells must be checked at the G2 checkpoint for any DNA damage within the chromosomes. The G2 checkpoint is mainly regulated by the tumor protein p53. If the DNA is damaged, p53 will either repair the DNA or trigger the apoptosis of the cell. If p53 is dysfunctional or mutated, cells with damaged DNA may continue through the cell cycle, leading to the development of cancer.\nMitotic phase (chromosome separation).\nThe relatively brief \"M phase\" consists of nuclear division (karyokinesis) and division of cytoplasm (cytokinesis). M phase is complex and highly regulated. The sequence of events is divided into phases, corresponding to the completion of one set of activities and the start of the next. These phases are sequentially known as:\nMitosis is the process by which a eukaryotic cell separates the chromosomes in its cell nucleus into two identical sets in two nuclei. During the process of mitosis the pairs of chromosomes condense and attach to microtubules that pull the sister chromatids to opposite sides of the cell.\nMitosis occurs exclusively in eukaryotic cells, but occurs in different ways in different species. For example, animal cells undergo an \"open\" mitosis, where the nuclear envelope breaks down before the chromosomes separate, while fungi such as \"Aspergillus nidulans\" and \"Saccharomyces cerevisiae\" (yeast) undergo a \"closed\" mitosis, where chromosomes divide within an intact cell nucleus.\nCytokinesis phase (separation of all cell components).\nMitosis is immediately followed by cytokinesis, which divides the nuclei, cytoplasm, organelles and cell membrane into two cells containing roughly equal shares of these cellular components. Cytokinesis occurs differently in plant and animal cells. While the cell membrane forms a groove that gradually deepens to separate the cytoplasm in animal cells, a cell plate is formed to separate it in plant cells. The position of the cell plate is determined by the position of a preprophase band of microtubules and actin filaments. Mitosis and cytokinesis together define the division of the parent cell into two daughter cells, genetically identical to each other and to their parent cell. This accounts for approximately 10% of the cell cycle.\nBecause cytokinesis usually occurs in conjunction with mitosis, \"mitosis\" is often used interchangeably with \"M phase\". However, there are many cells where mitosis and cytokinesis occur separately, forming single cells with multiple nuclei in a process called endoreplication. This occurs most notably among the fungi and slime molds, but is found in various groups. Even in animals, cytokinesis and mitosis may occur independently, for instance during certain stages of fruit fly embryonic development. Errors in mitosis can result in cell death through apoptosis or cause mutations that may lead to cancer.\nRegulation of eukaryotic cell cycle.\nRegulation of the cell cycle involves processes crucial to the survival of a cell, including the detection and repair of genetic damage as well as the prevention of uncontrolled cell division. The molecular events that control the cell cycle are ordered and directional; that is, each process occurs in a sequential fashion and it is impossible to \"reverse\" the cycle.\nRole of cyclins and CDKs.\nTwo key classes of regulatory molecules, cyclins and cyclin-dependent kinases (CDKs), determine a cell's progress through the cell cycle. Leland H. Hartwell, R. Timothy Hunt, and Paul M. Nurse won the 2001 Nobel Prize in Physiology or Medicine for their discovery of these central molecules. Many of the genes encoding cyclins and CDKs are conserved among all eukaryotes, but in general, more complex organisms have more elaborate cell cycle control systems that incorporate more individual components. Many of the relevant genes were first identified by studying yeast, especially \"Saccharomyces cerevisiae\"; genetic nomenclature in yeast dubs many of these genes \"cdc\" (for \"cell division cycle\") followed by an identifying number, e.g. \"cdc25\" or \"cdc20\".\nCyclins form the regulatory subunits and CDKs the catalytic subunits of an activated heterodimer; cyclins have no catalytic activity and CDKs are inactive in the absence of a partner cyclin. When activated by a bound cyclin, CDKs perform a common biochemical reaction called phosphorylation that activates or inactivates target proteins to orchestrate coordinated entry into the next phase of the cell cycle. Different cyclin-CDK combinations determine the downstream proteins targeted. CDKs are constitutively expressed in cells whereas cyclins are synthesised at specific stages of the cell cycle, in response to various molecular signals.\nGeneral mechanism of cyclin-CDK interaction.\nUpon receiving a pro-mitotic extracellular signal, G1 cyclin-CDK complexes become active to prepare the cell for S phase, promoting the expression of transcription factors that in turn promote the expression of S cyclins and of enzymes required for DNA replication. The G1 cyclin-CDK complexes also promote the degradation of molecules that function as S phase inhibitors by targeting them for ubiquitination. Once a protein has been ubiquitinated, it is targeted for proteolytic degradation by the proteasome. Results from a study of E2F transcriptional dynamics at the single-cell level argue that the role of G1 cyclin-CDK activities, in particular cyclin D-CDK4/6, is to tune the timing rather than the commitment of cell cycle entry.\nActive S cyclin-CDK complexes phosphorylate proteins that make up the pre-replication complexes assembled during G1 phase on DNA replication origins. The phosphorylation serves two purposes: to activate each already-assembled pre-replication complex, and to prevent new complexes from forming. This ensures that every portion of the cell's genome will be replicated once and only once. The reason for prevention of gaps in replication is fairly clear, because daughter cells that are missing all or part of crucial genes will die. However, for reasons related to gene copy number effects, possession of extra copies of certain genes is also deleterious to the daughter cells.\nMitotic cyclin-CDK complexes, which are synthesized but inactivated during S and G2 phases, promote the initiation of mitosis by stimulating downstream proteins involved in chromosome condensation and mitotic spindle assembly. A critical complex activated during this process is a ubiquitin ligase known as the anaphase-promoting complex (APC), which promotes degradation of structural proteins associated with the chromosomal kinetochore. APC also targets the mitotic cyclins for degradation, ensuring that telophase and cytokinesis can proceed.\nSpecific action of cyclin-CDK complexes.\nCyclin D is the first cyclin produced in the cells that enter the cell cycle, in response to extracellular signals (e.g. growth factors). Cyclin D levels stay low in resting cells that are not proliferating. Additionally, CDK4/6 and CDK2 are also inactive because CDK4/6 are bound by INK4 family members (e.g., p16), limiting kinase activity. Meanwhile, CDK2 complexes are inhibited by the CIP/KIP proteins such as p21 and p27, When it is time for a cell to enter the cell cycle, which is triggered by a mitogenic stimuli, levels of cyclin D increase. In response to this trigger, cyclin D binds to existing CDK4/6, forming the active cyclin D-CDK4/6 complex. Cyclin D-CDK4/6 complexes in turn mono-phosphorylates the retinoblastoma susceptibility protein (Rb) to pRb. The un-phosphorylated Rb tumour suppressor functions in inducing cell cycle exit and maintaining G0 arrest (senescence).\nIn the last few decades, a model has been widely accepted whereby pRB proteins are inactivated by cyclin D-Cdk4/6-mediated phosphorylation. Rb has 14+ potential phosphorylation sites. Cyclin D-Cdk 4/6 progressively phosphorylates Rb to hyperphosphorylated state, which triggers dissociation of pRB\u2013E2F complexes, thereby inducing G1/S cell cycle gene expression and progression into S phase.\nScientific observations from a study have shown that Rb is present in three types of isoforms: (1) un-phosphorylated Rb in G0 state; (2) mono-phosphorylated Rb, also referred to as \"hypo-phosphorylated' or 'partially' phosphorylated Rb in early G1 state; and (3) inactive hyper-phosphorylated Rb in late G1 state. In early G1 cells, mono-phosphorylated Rb exists as 14 different isoforms, one of each has distinct E2F binding affinity. Rb has been found to associate with hundreds of different proteins and the idea that different mono-phosphorylated Rb isoforms have different protein partners was very appealing. A later report confirmed that mono-phosphorylation controls Rb's association with other proteins and generates functional distinct forms of Rb. All different mono-phosphorylated Rb isoforms inhibit E2F transcriptional program and are able to arrest cells in G1-phase. Different mono-phosphorylated forms of Rb have distinct transcriptional outputs that are extended beyond E2F regulation.\nIn general, the binding of pRb to E2F inhibits the E2F target gene expression of certain G1/S and S transition genes including E-type cyclins. The partial phosphorylation of Rb de-represses the Rb-mediated suppression of E2F target gene expression, begins the expression of cyclin E. The molecular mechanism that causes the cell switched to cyclin E activation is currently not known, but as cyclin E levels rise, the active cyclin E-CDK2 complex is formed, bringing Rb to be inactivated by hyper-phosphorylation. Hyperphosphorylated Rb is completely dissociated from E2F, enabling further expression of a wide range of E2F target genes are required for driving cells to proceed into S phase [1]. It has been identified that cyclin D-Cdk4/6 binds to a C-terminal alpha-helix region of Rb that is only distinguishable to cyclin D rather than other cyclins, cyclin E, A and B. This observation based on the structural analysis of Rb phosphorylation supports that Rb is phosphorylated in a different level through multiple Cyclin-Cdk complexes. This also makes feasible the current model of a simultaneous switch-like inactivation of all mono-phosphorylated Rb isoforms through one type of Rb hyper-phosphorylation mechanism. In addition, mutational analysis of the cyclin D- Cdk 4/6 specific Rb C-terminal helix shows that disruptions of cyclin D-Cdk 4/6 binding to Rb prevents Rb phosphorylation, arrests cells in G1, and bolsters Rb's functions in tumor suppressor. This cyclin-Cdk driven cell cycle transitional mechanism governs a cell committed to the cell cycle that allows cell proliferation. A cancerous cell growth often accompanies with deregulation of Cyclin D-Cdk 4/6 activity.\nThe hyperphosphorylated Rb dissociates from the E2F/DP1/Rb complex (which was bound to the E2F responsive genes, effectively \"blocking\" them from transcription), activating E2F. Activation of E2F results in transcription of various genes like cyclin E, cyclin A, DNA polymerase, thymidine kinase, etc. Cyclin E thus produced binds to CDK2, forming the cyclin E-CDK2 complex, which pushes the cell from G1 to S phase (G1/S, which initiates the G2/M transition). Cyclin B-cdk1 complex activation causes breakdown of nuclear envelope and initiation of prophase, and subsequently, its deactivation causes the cell to exit mitosis. A quantitative study of E2F transcriptional dynamics at the single-cell level by using engineered fluorescent reporter cells provided a quantitative framework for understanding the control logic of cell cycle entry, challenging the canonical textbook model. Genes that regulate the amplitude of E2F accumulation, such as Myc, determine the commitment in cell cycle and S phase entry. G1 cyclin-CDK activities are not the driver of cell cycle entry. Instead, they primarily tune the timing of E2F increase, thereby modulating the pace of cell cycle progression.\nInhibitors.\nEndogenous.\nTwo families of genes, the \"cip/kip\" (\"CDK interacting protein/Kinase inhibitory protein\") family and the INK4a/ARF (\"In\"hibitor of \"K\"inase 4/\"A\"lternative \"R\"eading \"F\"rame) family, prevent the progression of the cell cycle. Because these genes are instrumental in prevention of tumor formation, they are known as tumor suppressors.\nThe \"cip/kip\" family includes the genes p21, p27 and p57. They halt the cell cycle in G1 phase by binding to and inactivating cyclin-CDK complexes. p21 is activated by p53 (which, in turn, is triggered by DNA damage e.g. due to radiation). p27 is activated by Transforming Growth Factor \u03b2 (TGF \u03b2), a growth inhibitor.\nThe INK4a/ARF family includes p16INK4a, which binds to CDK4 and arrests the cell cycle in G1 phase, and p14ARF which prevents p53 degradation.\nSynthetic.\nSynthetic inhibitors of Cdc25 could also be useful for the arrest of cell cycle and therefore be useful as antineoplastic and anticancer agents.\nMany human cancers possess the hyper-activated Cdk 4/6 activities. Given the observations of cyclin D-Cdk 4/6 functions, inhibition of Cdk 4/6 should result in preventing a malignant tumor from proliferating. Consequently, scientists have tried to invent the synthetic Cdk4/6 inhibitor as Cdk4/6 has been characterized to be a therapeutic target for anti-tumor effectiveness. Three Cdk4/6 inhibitors \u2013 palbociclib, ribociclib, and abemaciclib \u2013 currently received FDA approval for clinical use to treat advanced-stage or metastatic, hormone-receptor-positive (HR-positive, HR+), HER2-negative (HER2-) breast cancer. For example, palbociclib is an orally active CDK4/6 inhibitor which has demonstrated improved outcomes for ER-positive/HER2-negative advanced breast cancer. The main side effect is neutropenia which can be managed by dose reduction.\nCdk4/6 targeted therapy will only treat cancer types where Rb is expressed. Cancer cells with loss of Rb have primary resistance to Cdk4/6 inhibitors.\nTranscriptional regulatory network.\nCurrent evidence suggests that a semi-autonomous transcriptional network acts in concert with the CDK-cyclin machinery to regulate the cell cycle. Several gene expression studies in \"Saccharomyces cerevisiae\" have identified 800\u20131200 genes that change expression over the course of the cell cycle. They are transcribed at high levels at specific points in the cell cycle, and remain at lower levels throughout the rest of the cycle. While the set of identified genes differs between studies due to the computational methods and criteria used to identify them, each study indicates that a large portion of yeast genes are temporally regulated.\nMany periodically expressed genes are driven by transcription factors that are also periodically expressed. One screen of single-gene knockouts identified 48 transcription factors (about 20% of all non-essential transcription factors) that show cell cycle progression defects. Genome-wide studies using high throughput technologies have identified the transcription factors that bind to the promoters of yeast genes, and correlating these findings with temporal expression patterns have allowed the identification of transcription factors that drive phase-specific gene expression. The expression profiles of these transcription factors are driven by the transcription factors that peak in the prior phase, and computational models have shown that a CDK-autonomous network of these transcription factors is sufficient to produce steady-state oscillations in gene expression).\nExperimental evidence also suggests that gene expression can oscillate with the period seen in dividing wild-type cells independently of the CDK machinery. Orlando \"et al.\" used microarrays to measure the expression of a set of 1,271 genes that they identified as periodic in both wild type cells and cells lacking all S-phase and mitotic cyclins (\"clb1,2,3,4,5,6\"). Of the 1,271 genes assayed, 882 continued to be expressed in the cyclin-deficient cells at the same time as in the wild type cells, despite the fact that the cyclin-deficient cells arrest at the border between G1 and S phase. However, 833 of the genes assayed changed behavior between the wild type and mutant cells, indicating that these genes are likely directly or indirectly regulated by the CDK-cyclin machinery. Some genes that continued to be expressed on time in the mutant cells were also expressed at different levels in the mutant and wild type cells. These findings suggest that while the transcriptional network may oscillate independently of the CDK-cyclin oscillator, they are coupled in a manner that requires both to ensure the proper timing of cell cycle events. Other work indicates that phosphorylation, a post-translational modification, of cell cycle transcription factors by Cdk1 may alter the localization or activity of the transcription factors in order to tightly control timing of target genes.\nWhile oscillatory transcription plays a key role in the progression of the yeast cell cycle, the CDK-cyclin machinery operates independently in the early embryonic cell cycle. Before the midblastula transition, zygotic transcription does not occur and all needed proteins, such as the B-type cyclins, are translated from maternally loaded mRNA.\nDNA replication and DNA replication origin activity.\nAnalyses of synchronized cultures of \"Saccharomyces cerevisiae\" under conditions that prevent DNA replication initiation without delaying cell cycle progression showed that origin licensing decreases the expression of genes with origins near their 3' ends, revealing that downstream origins can regulate the expression of upstream genes. This confirms previous predictions from mathematical modeling of a global causal coordination between DNA replication origin activity and mRNA expression, and shows that mathematical modeling of DNA microarray data can be used to correctly predict previously unknown biological modes of regulation.\nCheckpoints.\nCell cycle checkpoints are used by the cell to monitor and regulate the progress of the cell cycle. Checkpoints prevent cell cycle progression at specific points, allowing verification of necessary phase processes and repair of DNA damage. The cell cannot proceed to the next phase until checkpoint requirements have been met. Checkpoints typically consist of a network of regulatory proteins that monitor and dictate the progression of the cell through the different stages of the cell cycle.\nIt is estimated that in normal human cells about 1% of single-strand DNA damages are converted to about 50 endogenous DNA double-strand breaks per cell per cell cycle. Although such double-strand breaks are usually repaired with high fidelity, errors in their repair are considered to contribute significantly to the rate of cancer in humans.\nThere are several checkpoints to ensure that damaged or incomplete DNA is not passed on to daughter cells. Three main checkpoints exist: the G1/S checkpoint, the G2/M checkpoint and the metaphase (mitotic) checkpoint. Another checkpoint is the Go checkpoint, in which the cells are checked for maturity. If the cells fail to pass this checkpoint by not being ready yet, they will be discarded from dividing.\nG1/S transition is a rate-limiting step in the cell cycle and is also known as restriction point. This is where the cell checks whether it has enough raw materials to fully replicate its DNA (nucleotide bases, DNA synthase, chromatin, etc.). An unhealthy or malnourished cell will get stuck at this checkpoint.\nThe G2/M checkpoint is where the cell ensures that it has enough cytoplasm and phospholipids for two daughter cells. But sometimes more importantly, it checks to see if it is the right time to replicate. There are some situations where many cells need to all replicate simultaneously (for example, a growing embryo should have a symmetric cell distribution until it reaches the mid-blastula transition). This is done by controlling the G2/M checkpoint.\nThe metaphase checkpoint is a fairly minor checkpoint, in that once a cell is in metaphase, it has committed to undergoing mitosis. However that's not to say it isn't important. In this checkpoint, the cell checks to ensure that the spindle has formed and that all of the chromosomes are aligned at the spindle equator before anaphase begins.\nWhile these are the three \"main\" checkpoints, not all cells have to pass through each of these checkpoints in this order to replicate. Many types of cancer are caused by mutations that allow the cells to speed through the various checkpoints or even skip them altogether. Going from S to M to S phase almost consecutively. Because these cells have lost their checkpoints, any DNA mutations that may have occurred are disregarded and passed on to the daughter cells. This is one reason why cancer cells have a tendency to exponentially acquire mutations. Aside from cancer cells, many fully differentiated cell types no longer replicate so they leave the cell cycle and stay in G0 until their death. Thus removing the need for cellular checkpoints. An alternative model of the cell cycle response to DNA damage has also been proposed, known as the postreplication checkpoint.\nCheckpoint regulation plays an important role in an organism's development. In sexual reproduction, when egg fertilization occurs, when the sperm binds to the egg, it releases signalling factors that notify the egg that it has been fertilized. Among other things, this induces the now fertilized oocyte to return from its previously dormant, G0, state back into the cell cycle and on to mitotic replication and division.\np53 plays an important role in triggering the control mechanisms at both G1/S and G2/M checkpoints. In addition to p53, checkpoint regulators are being heavily researched for their roles in cancer growth and proliferation.\nFluorescence imaging of the cell cycle.\nPioneering work by Atsushi Miyawaki and coworkers developed the fluorescent ubiquitination-based cell cycle indicator (FUCCI), which enables fluorescence imaging of the cell cycle. Originally, a green fluorescent protein, mAG, was fused to hGem(1/110) and an orange fluorescent protein (mKO2) was fused to hCdt1(30/120). Note, these fusions are fragments that contain a nuclear localization signal and ubiquitination sites for degradation, but are not functional proteins. The green fluorescent protein is made during the S, G2, or M phase and degraded during the G0 or G1 phase, while the orange fluorescent protein is made during the G0 or G1 phase and destroyed during the S, G2, or M phase. A far-red and near-infrared FUCCI was developed using a cyanobacteria-derived fluorescent protein (smURFP) and a bacteriophytochrome-derived fluorescent protein (movie found at this link).\nSeveral modifications have been made to the original FUCCI system to improve its usability in several in vitro systems and model organisms. These advancements have increased the sensitivity and accuracy of cell cycle phase detection, enabling more precise assessments of cellular proliferation\nRole in tumor formation.\nA disregulation of the cell cycle components may lead to tumor formation. As mentioned above, when some genes like the cell cycle inhibitors, RB, p53 etc. mutate, they may cause the cell to multiply uncontrollably, forming a tumor. Although the duration of cell cycle in tumor cells is equal to or longer than that of normal cell cycle, the proportion of cells that are in active cell division (versus quiescent cells in G0 phase) in tumors is much higher than that in normal tissue. Thus there is a net increase in cell number as the number of cells that die by apoptosis or senescence remains the same.\nThe cells which are actively undergoing cell cycle are targeted in cancer therapy as the DNA is relatively exposed during cell division and hence susceptible to damage by drugs or radiation. This fact is made use of in cancer treatment; by a process known as debulking, a significant mass of the tumor is removed which pushes a significant number of the remaining tumor cells from G0 to G1 phase (due to increased availability of nutrients, oxygen, growth factors etc.). Radiation or chemotherapy following the debulking procedure kills these cells which have newly entered the cell cycle.\nThe fastest cycling mammalian cells in culture, crypt cells in the intestinal epithelium, have a cycle time as short as 9 to 10 hours. Stem cells in resting mouse skin may have a cycle time of more than 200 hours. Most of this difference is due to the varying length of G1, the most variable phase of the cycle. M and S do not vary much.\nIn general, cells are most radiosensitive in late M and G2 phases and most resistant in late S phase. For cells with a longer cell cycle time and a significantly long G1 phase, there is a second peak of resistance late in G1. The pattern of resistance and sensitivity correlates with the level of sulfhydryl compounds in the cell. Sulfhydryls are natural substances that protect cells from radiation damage and tend to be at their highest levels in S and at their lowest near mitosis.\nHomologous recombination (HR) is an accurate process for repairing DNA double-strand breaks. HR is nearly absent in G1 phase, is most active in S phase, and declines in G2/M. Non-homologous end joining, a less accurate and more mutagenic process for repairing double strand breaks, is active throughout the cell cycle.\nCell cycle evolution.\nEvolution of the genome.\nThe cell cycle must duplicate all cellular constituents and equally partition them into two daughter cells. Many constituents, such as proteins and ribosomes, are produced continuously throughout the cell cycle (except during M-phase). However, the chromosomes and other associated elements like MTOCs, are duplicated just once during the cell cycle. A central component of the cell cycle is its ability to coordinate the continuous and periodic duplications of different cellular elements, which evolved with the formation of the genome.\nThe pre-cellular environment contained functional and self-replicating RNAs. All RNA concentrations depended on the concentrations of other RNAs that might be helping or hindering the gathering of resources. In this environment, growth was simply the continuous production of RNAs. These pre-cellular structures would have had to contend with parasitic RNAs, issues of inheritance, and copy-number control of specific RNAs. \nPartitioning \"genomic\" RNA from \"functional\" RNA helped solve these problems. The fusion of multiple RNAs into a genome gave a template from which functional RNAs were cleaved. Now, parasitic RNAs would have to incorporate themselves into the genome, a much greater barrier, in order to survive. Controlling the copy number of genomic RNA also allowed RNA concentration to be determined through synthesis rates and RNA half-lives, instead of competition. Separating the duplication of genomic RNAs from the generation of functional RNAs allowed for much greater duplication fidelity of genomic RNAs without compromising the production of functional RNAs. Finally, the replacement of genomic RNA with DNA, which is a more stable molecule, allowed for larger genomes. The transition from self-catalysis enzyme synthesis to genome-directed enzyme synthesis was a critical step in cell evolution, and had lasting implications on the cell cycle, which must regulate functional synthesis and genomic duplication in very different ways.\nCyclin-dependent kinase and cyclin evolution.\nCell-cycle progression is controlled by the oscillating concentrations of different cyclins and the resulting molecular interactions from the various cyclin-dependent kinases (CDKs). In yeast, just one CDK (Cdc28 in \"S. cerevisiae\" and Cdc2 in \"S. pombe\") controls the cell cycle. However, in animals, whole families of CDKs have evolved. Cdk1 controls entry to mitosis and Cdk2, Cdk4, and Cdk6 regulate entry into S phase. Despite the evolution of the CDK family in animals, these proteins have related or redundant functions. For example, \"cdk2 cdk4 cdk6\" triple knockout mice cells can still progress through the basic cell cycle. \"cdk1\" knockouts are lethal, which suggests an ancestral CDK1-type kinase ultimately controlling the cell cycle.\n\"Arabidopsis thaliana\" has a Cdk1 homolog called CDKA;1, however \"cdka;1\" \"A. thaliana\" mutants are still viable, running counter to the opisthokont pattern of CDK1-type kinases as essential regulators controlling the cell cycle. Plants also have a unique group of B-type CDKs, whose functions may range from development-specific functions to major players in mitotic regulation.\nG1/S checkpoint evolution.\nThe G1/S checkpoint is the point at which the cell commits to division through the cell cycle. Complex regulatory networks lead to the G1/S transition decision. Across opisthokonts, there are both highly diverged protein sequences as well as strikingly similar network topologies. \nEntry into S-phase in both yeast and animals is controlled by the levels of two opposing regulators. The networks regulating these transcription factors are double-negative feedback loops and positive feedback loops in both yeast and animals. Additional regulation of the regulatory network for the G1/S checkpoint in yeast and animals includes the phosphorylation/de-phosphorylation of CDK-cyclin complexes. The sum of these regulatory networks creates a hysteretic and bistable scheme, despite the specific proteins being highly diverged. For yeast, Whi5 must be suppressed by Cln3 phosphorylation for SBF to be expressed, while in animals Rb must be suppressed by the Cdk4/6-cyclin D complex for E2F to be expressed. Both Rb and Whi5 inhibit transcript through the recruitment of histone deacetylase proteins to promoters. Both proteins additionally have multiple CDK phosphorylation sites through which they are inhibited. However, these proteins share no sequence similarity. \nStudies in \"A. thaliana\" extend our knowledge of the G1/S transition across eukaryotes as a whole. Plants also share a number of conserved network features with opisthokonts, and many plant regulators have direct animal homologs. For example, plants also need to suppress Rb for E2F translation in the network. These conserved elements of the plant and animal cell cycles may be ancestral in eukaryotes. While yeast share a conserved network topology with plants and animals, the highly diverged nature of yeast regulators suggests possible rapid evolution along the yeast lineage."}
{"id": "7253", "revid": "1158002918", "url": "https://en.wikipedia.org/wiki?curid=7253", "title": "Cartesian", "text": "Cartesian means of or relating to the French philosopher Ren\u00e9 Descartes\u2014from his Latinized name \"Cartesius\". It may refer to:"}
{"id": "7255", "revid": "28438", "url": "https://en.wikipedia.org/wiki?curid=7255", "title": "Connection (dance)", "text": "In partner dancing, connection is physical, non-verbal communication between dancers to facilitate synchronized or coordinated dance movements. Some forms of connection involve \"lead/follow\" in which one dancer (the \"lead\") directs the movements of the other dancer (the \"follower\") by means of non-verbal directions conveyed through a physical connection between the dancers. In other forms, connection involves multiple dancers (more than two) without a distinct leader or follower (e.g. contact improvisation). Connection refers to a host of different techniques in many types of partner dancing, especially (but not exclusively) those that feature significant physical contact between the dancers, including the Argentine Tango, Lindy Hop, Balboa, East Coast Swing, West Coast Swing, Salsa, and other ballroom dances.\nOther forms of communication, such as visual cues or spoken cues, sometimes aid in connecting with one's partner, but are often used in specific circumstances (e.g., practicing figures, or figures which are purposely danced without physical connection). Connection can be used to transmit power and energy as well as information and signals; some dance forms (and some dancers) primarily emphasize power or signaling, but most are probably a mixture of both. Philosopher of dance Ilya Vidrin argues that connection between partners involves norm-based communication that include \u201ca physical exchange of information on the basis of ethically-bound conditions\u201d (proximity, orientation, and points of contact) which constrain agency and predictability.\nLead/Follow.\nFollowing and leading in a partner dance is accomplished by maintaining a physical connection called the frame that allows the leader to transmit body movement to the follower, and for the follower to suggest ideas to the leader. A frame is a stable structural combination of both bodies maintained through the dancers' arms and/or legs.\nConnection occurs in both open and closed dance positions (also called \"open frame\" and \"closed frame\").\nIn closed position with body contact, connection is achieved by maintaining the frame. The follower moves to match the leader, maintaining the pressure between the two bodies as well as the position.\nWhen creating frame, tension is the primary means of establishing communication. Changes in tension are made to create rhythmic variations in moves and movements, and are communicated through points of contact. In an open position or a closed position without body contact, the hands and arms alone provide the connection, which may be one of three forms: tension, compression or neutral.\nIn swing dances, tension and compression may be maintained for a significant period of time. In other dances, such as Latin, tension and compression may be used as indications of upcoming movement. However, in both styles, tension and compression do not signal immediate movement: the follow must be careful not to move prior to actual movement by the lead. Until then, the dancers must match pressures without moving their hands. In some styles of Lindy Hop, the tension may become quite high without initiating movement.\nThe general rule for open connections is that moves of the leader's hands back, forth, left or right are originated through moves of the entire body. Accordingly, for the follower, a move of the connected hand is immediately transformed into the corresponding move of the body. Tensing the muscles and locking the arm achieves this effect but is neither comfortable nor correct. Such tension eliminates the subtler communication in the connection, and eliminates free movement up and down, such as is required to initiate many turns.\nInstead of just tensing the arms, connection is achieved by engaging the shoulder, upper body and torso muscles. Movement originates in the body's core. A leader leads by moving himself and maintaining frame and connection. Different forms of dance and different movements within each dance may call for differences in the connection. In some dances the separation distance between the partners remains pretty constant. In others e.g. Modern Jive moving closer together and further apart are fundamental to the dance, requiring flexion and extension of the arms, alternating compression and tension.\nThe connection between two partners has a different feel in every dance and with every partner. Good social dancers adapt to the conventions of the dance and the responses of their partners."}
{"id": "7256", "revid": "30395827", "url": "https://en.wikipedia.org/wiki?curid=7256", "title": "Cardiovascular system", "text": ""}
{"id": "7257", "revid": "6995", "url": "https://en.wikipedia.org/wiki?curid=7257", "title": "Caste", "text": "A caste is a fixed social group into which an individual is born within a particular system of social stratification: a caste system. Within such a system, individuals are expected to marry exclusively within the same caste (endogamy), follow lifestyles often linked to a particular occupation, hold a ritual status observed within a hierarchy, and interact with others based on cultural notions of exclusion, with certain castes considered as either more pure or more polluted than others. The term \"caste\" is also applied to morphological groupings in eusocial insects such as ants, bees, and termites.\nThe paradigmatic ethnographic example of caste is the division of India's Hindu society into rigid social groups. Its roots lie in South Asia's ancient history and it still exists; however, the economic significance of the caste system in India seems to be declining as a result of urbanisation and affirmative action programs. A subject of much scholarship by sociologists and anthropologists, the Hindu caste system is sometimes used as an analogical basis for the study of caste-like social divisions existing outside Hinduism and India. In colonial Spanish America, mixed-race \"castas\" were a category within the Hispanic sector but the social order was otherwise fluid.\nEtymology.\nThe English word \"caste\" () derives from the Spanish and Portuguese , which, according to the John Minsheu's Spanish dictionary (1569), means \"race, lineage, tribe or breed\". The Portuguese and Spanish word \u201ccasta\u201d originated in Gothic \"kasts\" - \u201cgroup of animals\u201d. The word entered the languages of the Iberian Peninsula with the sense \u201ctype of animal,\u201d and soon developed into \u201crace of men\u201d and later \u201cclass, condition of men\u201d. When the Spanish colonised the New World, they used the word to mean a 'clan or lineage'. It was, however, the Portuguese, the first Europeans to reach India by sea in 1498, to first employ in the primary modern sense of the English word 'caste' when they applied it to the thousands of endogamous, hereditary Indian social groups they encountered. The use of the spelling \"caste\", with this latter meaning, is first attested in English in 1613. In the Latin American context, the term \"caste\" is sometimes used to describe the \"casta\" system of racial classification, based on whether a person was of pure European, Indigenous or African descent, or some mix thereof, with the different groups being placed in a racial hierarchy; however, despite the etymological connection between the Latin American \"casta\" system and South Asian caste systems (the former giving its name to the latter), it is controversial to what extent the two phenomena are really comparable.\nIn South Asia.\nIndia.\nModern India's caste system is based on the superimposition of an old four-fold theoretical classification called varna on the social ethnic grouping called j\u0101ti. The Vedic period conceptualised a society as consisting of four types of varnas, or categories: Brahmin, Kshatriya, Vaishya and Shudra, according to the nature of the work of its members. Varna was not an inherited category and the occupation determined the varna. However, a person's Jati is determined at birth and makes them take up that Jati's occupation; members could and did change their occupation based on personal strengths as well as economic, social and political factors. A 2016 study based on the DNA analysis of unrelated Indians determined that endogamous jatis originated during the Gupta Empire.\nFrom 1901 onwards, for the purposes of the Decennial Census, the British colonial authorities arbitrarily and incorrectly forced all J\u0101tis into the four \"Varna\" categories as described in ancient texts. Herbert Hope Risley, the Census Commissioner, noted that \"The principle suggested as a basis was that of classification by social precedence as recognized by native public opinion at the present day, and manifesting itself in the facts that particular castes are supposed to be the modern representatives of one or other of the castes of the theoretical Indian system.\"\n\"Varna\", as mentioned in ancient Hindu texts, describes society as divided into four categories: Brahmins (scholars and yajna priests), Kshatriyas (rulers and warriors), Vaishyas (farmers, merchants and artisans) and Shudras (workmen/service providers). The texts do not mention any hierarchy or a separate, untouchable category in \"Varna\" classifications. Scholars believe that the \"Varnas\" system was never truly operational in society and there is no evidence of it ever being a reality in Indian history. The practical division of the society had always been in terms of \"Jatis\" (birth groups), which are not based on any specific religious principle but could vary from ethnic origins to occupations to geographic areas. The \"J\u0101tis\" have been endogamous social groups without any fixed hierarchy but subject to vague notions of rank articulated over time based on lifestyle and social, political, or economic status. Many of India's major empires and dynasties like the Mauryas, Shalivahanas, Chalukyas, Kakatiyas among many others, were founded by people who would have been classified as Shudras, under the \"Varnas\" system, as interpreted by the British rulers. It is well established that by the 9th century, kings from all the four Varnas, including Brahmins and Vaishyas, had occupied the highest seat in the monarchical system in Hindu India, contrary to the Varna theory. In many instances, as in Bengal, historically the kings and rulers had been called upon, when required, to mediate on the ranks of \"J\u0101tis\", which might number in thousands all over the subcontinent and vary by region. In practice, the \"j\u0101tis\" may or may not fit into the \"Varna\" classes and many prominent \"Jatis\", for example the Jats and Yadavs, straddled two Varnas i.e. Kshatriyas and Vaishyas, and the \"Varna\" status of \"J\u0101tis\" itself was subject to articulation over time.\nStarting with the 1901 Census of India led by colonial administrator Herbert Hope Risley, all the \"j\u0101tis\" were grouped under the theoretical \"varnas\" categories. According to political scientist Lloyd Rudolph, Risley believed that \"varna\", however ancient, could be applied to all the modern castes found in India, and \"[he] meant to identify and place several hundred million Indians within it.\" The terms \"varna\" (conceptual classification based on occupation) and \"j\u0101ti\" (groups) are two distinct concepts: while \"varna\" is a theoretical four-part division, \"j\u0101ti\" (community) refers to the thousands of actual endogamous social groups prevalent across the subcontinent. The classical authors scarcely speak of anything other than the \"varnas\", as it provided a convenient shorthand; but a problem arises when colonial Indologists sometimes confuse the two.\nUpon independence from Britain, the Indian Constitution listed 1,108 Jatis across the country as Scheduled Castes in 1950, for positive discrimination. This constitution would also ban discrimination of the basis of the caste, though its practice in India remained intact. The Untouchable communities are sometimes called \"Scheduled Castes\", \"Dalit\" or \"Harijan\" in contemporary literature. In 2001, Dalits were 16.2% of India's population. Most of the 15 million bonded child workers are from the lowest castes. Independent India has witnessed caste-related violence. In 2005, government recorded approximately 110,000 cases of reported violent acts, including rape and murder, against Dalits.\nThe socio-economic limitations of the caste system are reduced due to urbanisation and affirmative action. Nevertheless, the caste system still exists in endogamy and patrimony, and thrives in the politics of democracy, where caste provides ready made constituencies to politicians. The globalisation and economic opportunities from foreign businesses has influenced the growth of India's middle-class population. Some members of the Chhattisgarh Potter Caste Community (CPCC) are middle-class urban professionals and no longer potters unlike the remaining majority of traditional rural potter members. There is persistence of caste in Indian politics. Caste associations have evolved into caste-based political parties. Political parties and the state perceive caste as an important factor for mobilisation of people and policy development.\nStudies by Bhatt and Beteille have shown changes in status, openness, mobility in the social aspects of Indian society. As a result of modern socio-economic changes in the country, India is experiencing significant changes in the dynamics and the economics of its social sphere. While arranged marriages are still the most common practice in India, the internet has provided a network for younger Indians to take control of their relationships through the use of dating apps. This remains isolated to informal terms, as marriage is not often achieved through the use of these apps. Hypergamy is still a common practice in India and Hindu culture. Men are expected to marry within their caste, or one below, with no social repercussions. If a woman marries into a higher caste, then her children will take the status of their father. If she marries down, her family is reduced to the social status of their son in law. In this case, the women are bearers of the egalitarian principle of the marriage. There would be no benefit in marrying a higher caste if the terms of the marriage did not imply equality. However, men are systematically shielded from the negative implications of the agreement.\nGeographical factors also determine adherence to the caste system. Many Northern villages are more likely to participate in exogamous marriage, due to a lack of eligible suitors within the same caste. Women in North India have been found to be less likely to leave or divorce their husbands since they are of a relatively lower caste system, and have higher restrictions on their freedoms. On the other hand, Pahari women, of the northern mountains, have much more freedom to leave their husbands without stigma. This often leads to better husbandry as his actions are not protected by social expectations.\nChiefly among the factors influencing the rise of exogamy is the rapid urbanisation in India experienced over the last century. It is well known that urban centers tend to be less reliant on agriculture and are more progressive as a whole. As India's cities boomed in population, the job market grew to keep pace. Prosperity and stability were now more easily attained by an individual, and the anxiety to marry quickly and effectively was reduced. Thus, younger, more progressive generations of urban Indians are less likely than ever to participate in the antiquated system of arranged endogamy.\nIndia has also implemented a form of Affirmative Action, locally known as \"reservation groups\". Quota system jobs, as well as placements in publicly funded colleges, hold spots for the 8% of India's minority, and underprivileged groups. As a result, in states such as Tamil Nadu or those in the north-east, where underprivileged populations predominate, over 80% of government jobs are set aside in quotas. In education, colleges lower the marks necessary for the Dalits to enter.\nNepal.\nThe Nepali caste system resembles in some respects the Indian \"j\u0101ti\" system, with numerous \"j\u0101ti\" divisions with a \"varna\" system superimposed. Inscriptions attest the beginnings of a caste system during the Licchavi period. Jayasthiti Malla (1382\u20131395) categorised Newars into 64 castes (Gellner 2001). A similar exercise was made during the reign of Mahindra Malla (1506\u20131575). The Hindu social code was later set up in the Gorkha Kingdom by Ram Shah (1603\u20131636).\nPakistan.\nMcKim Marriott claims a social stratification that is hierarchical, closed, endogamous and hereditary is widely prevalent, particularly in western parts of Pakistan. Frederik Barth in his review of this system of social stratification in Pakistan suggested that these are castes.\nSri Lanka.\nThe caste system in Sri Lanka is a division of society into strata, influenced by the textbook \"j\u0101ti\" system found in India. Ancient Sri Lankan texts such as the Pujavaliya, Sadharmaratnavaliya and Yogaratnakaraya and inscriptional evidence show that the above hierarchy prevailed throughout the feudal period. The repetition of the same caste hierarchy even as recently as the 18th century, in the Kandyan-period Kadayimpoth \u2013 Boundary books as well indicates the continuation of the tradition right up to the end of Sri Lanka's monarchy.\nOutside South Asia.\nSoutheast Asia.\nIndonesia.\nBalinese caste structure has been described as being based either on three categories\u2014the noble triwangsa (thrice born), the middle class of \"dwij\u0101ti\" (twice born), and the lower class of \"ekaj\u0101ti\" (once born), much similar to the traditional Indian BKVS social stratification \u2014 or on four castes\nThe Brahmana caste was further subdivided by Dutch ethnographers into two: Siwa and Buda. The Siwa caste was subdivided into five: Kemenuh, Keniten, Mas, Manuba and Petapan. This classification was to accommodate the observed marriage between higher-caste Brahmana men with lower-caste women. The other castes were similarly further sub-classified by 19th-century and early-20th-century ethnographers based on numerous criteria ranging from profession, endogamy or exogamy or polygamy, and a host of other factors in a manner similar to \"castas\" in Spanish colonies such as Mexico, and caste system studies in British colonies such as India.\nPhilippines.\nIn the Philippines, pre-colonial societies do not have a single social structure. The class structures can be roughly categorised into four types:\nEast Asia.\nChina and Mongolia.\nDuring the period of the Yuan dynasty, ruler Kublai Khan enforced a \"Four Class System\", which was a legal caste system. The order of four classes of people in descending order were:\nTibet.\nThere is significant controversy over the social classes of Tibet, especially with regards to the serfdom in Tibet controversy.\n has put forth the argument that pre-1950s Tibetan society was functionally a caste system, in contrast to previous scholars who defined the Tibetan social class system as similar to European feudal serfdom, as well as non-scholarly western accounts which seek to romanticise a supposedly 'egalitarian' ancient Tibetan society.\nJapan.\nIn Japan's history, social strata based on inherited position rather than personal merit, were rigid and highly formalised in a system called \"mibunsei\" (\u8eab\u5206\u5236). At the top were the Emperor and Court nobles (kuge), together with the Sh\u014dgun and daimy\u014d.\nOlder scholars believed that there were of \"samurai, peasants (\"hyakush\u014d\"), craftsmen, and merchants (\"ch\u014dnin\")\" under the daimyo, with 80% of peasants under the 5% samurai class, followed by craftsmen and merchants. However, various studies have revealed since about 1995 that the classes of peasants, craftsmen, and merchants under the samurai are equal, and the old hierarchy chart has been removed from Japanese history textbooks. In other words, peasants, craftsmen, and merchants are not a social pecking order, but a social classification.\nMarriage between certain classes was generally prohibited. In particular, marriage between daimyo and court nobles was forbidden by the Tokugawa shogunate because it could lead to political maneuvering. For the same reason, marriages between daimyo and high-ranking hatamoto of the samurai class required the approval of the Tokugawa shogunate. It was also forbidden for a member of the samurai class to marry a peasant, craftsman, or merchant, but this was done through a loophole in which a person from a lower class was adopted into the samurai class and then married. Since there was an economic advantage for a poor samurai class person to marry a wealthy merchant or peasant class woman, they would adopt a merchant or peasant class woman into the samurai class as an adopted daughter and then marry her.\nJapan had its own untouchable caste, shunned and ostracised, historically referred to by the insulting term \"eta\", now called \"burakumin\". While modern law has officially abolished the class hierarchy, there are reports of discrimination against the \"buraku\" or \"burakumin\" underclasses. The \"burakumin\" are regarded as \"ostracised\". The \"burakumin\" are one of the main minority groups in Japan, along with the Ainu of Hokkaid\u014d and those of Korean or Chinese descent.\nKorea.\nThe baekjeong () were an \"untouchable\" outcaste of Korea. The meaning today is that of butcher. It originates in the Khitan invasion of Korea in the 11th century. The defeated Khitans who surrendered were settled in isolated communities throughout Goryeo to forestall rebellion. They were valued for their skills in hunting, herding, butchering, and making of leather, common skill sets among nomads. Over time, their ethnic origin was forgotten, and they formed the bottom layer of Korean society.\nIn 1392, with the foundation of the Confucian Joseon dynasty, Korea systemised its own native class system. At the top were the two official classes, the Yangban, which literally means \"two classes\". It was composed of scholars (\"munban\") and warriors (\"muban\"). Scholars had a significant social advantage over the warriors. Below were the \"jung-in\" (: literally \"middle people\"). This was a small class of specialised professions such as medicine, accounting, translators, regional bureaucrats, etc. Below that were the \"sangmin\" (: literally 'commoner'), farmers working their own fields. Korea also had a serf population known as the \"nobi\". The nobi population could fluctuate up to about one third of the population, but on average the nobi made up about 10% of the total population. In 1801, the vast majority of government nobi were emancipated, and by 1858 the nobi population stood at about 1.5% of the total population of Korea. The hereditary nobi system was officially abolished around 1886\u201387 and the rest of the nobi system was abolished with the Gabo Reform of 1894, but traces remained until 1930.\nThe opening of Korea to foreign Christian missionary activity in the late 19th century saw some improvement in the status of the \"baekjeong\". However, everyone was not equal under the Christian congregation, and even so protests erupted when missionaries tried to integrate \"baekjeong\" into worship, with non-\"baekjeong\" finding this attempt insensitive to traditional notions of hierarchical advantage. Around the same time, the \"baekjeong\" began to resist open social discrimination. They focused on social and economic injustices affecting them, hoping to create an egalitarian Korean society. Their efforts included attacking social discrimination by upper class, authorities, and \"commoners\", and the use of degrading language against children in public schools.\nWith the Gabo reform of 1896, the class system of Korea was officially abolished. Following the collapse of the Gabo government, the new cabinet, which became the Gwangmu government after the establishment of the Korean Empire, introduced systematic measures for abolishing the traditional class system. One measure was the new household registration system, reflecting the goals of formal social equality, which was implemented by the loyalists' cabinet. Whereas the old registration system signified household members according to their hierarchical social status, the new system called for an occupation.\nWhile most Koreans by then had surnames and even bongwan, although still substantial number of cheonmin, mostly consisted of serfs and slaves, and untouchables did not. According to the new system, they were then required to fill in the blanks for surname in order to be registered as constituting separate households. Instead of creating their own family name, some cheonmins appropriated their masters' surname, while others simply took the most common surname and its bongwan in the local area. Along with this example, activists within and outside the Korean government had based their visions of a new relationship between the government and people through the concept of citizenship, employing the term \"inmin\" (\"people\") and later, \"kungmin\" (\"citizen\").\nNorth Korea.\nThe Committee for Human Rights in North Korea reported that \"Every North Korean citizen is assigned a heredity-based class and socio-political rank over which the individual exercises no control but which determines all aspects of his or her life.\" Called \"Songbun\", Barbara Demick describes this \"class structure\" as an updating of the hereditary \"caste system\", a combination of Confucianism and Communism. It originated in 1946 and was entrenched by the 1960s, and consisted of 53 categories ranging across three classes: loyal, wavering, and impure. The privileged \"loyal\" class included members of the Korean Workers' Party and Korean People's Army officers' corps, the wavering class included peasants, and the impure class included collaborators with Imperial Japan and landowners. She claims that a bad family background is called \"tainted blood\", and that by law this \"tainted blood\" lasts three generations.\nWest Asia.\nKurdistan.\nYazidis.\nThere are three hereditary groups, often called castes, in Yazidism. Membership in the Yazidi society and a caste is conferred by birth. P\u00eers and Sheikhs are the priestly castes, which are represented by many sacred lineages (). Sheikhs are in charge of both religious and administrative functions and are divided into three endogamous houses, \u015eemsan\u00ee, Adan\u00ee and Qatan\u00ee who are in turn divided into lineages. The P\u00eers are in charge of purely religious functions and traditionally consist of 40 lineages or clans, but approximately 90 appellations of P\u00eer lineages have been found, which may have been a result of new sub-lineages arising and number of clans increasing over time due to division as Yazidis settled in different places and countries. Division could occur in one family, if there were a few brothers in one clan, each of them could become the founder of their own P\u00eer sub-clan (). Mir\u00eeds are the lay caste and are divided into tribes, who are each affiliated to a P\u00eer and a Sheikh priestly lineage assigned to the tribe.\nIran.\nPre-Islamic Sassanid society was immensely complex, with separate systems of social organisation governing numerous different groups within the empire. Historians believe society comprised four social classes, which linguistic analysis indicates may have been referred to collectively as \"pistras\". The classes, from highest to lowest status, were priests (), warriors (), secretaries (), and commoners ().\nYemen.\nIn Yemen there exists a hereditary caste, the African-descended Al-Akhdam who are kept as perennial manual workers. Estimates put their number at over 3.5 million residents who are discriminated, out of a total Yemeni population of around 22 million.\nAfrica.\nVarious sociologists have reported caste systems in Africa. The specifics of the caste systems have varied in ethnically and culturally diverse Africa; however, the following features are common \u2013 it has been a closed system of social stratification, the social status is inherited, the castes are hierarchical, certain castes are shunned while others are merely endogamous and exclusionary. In some cases, concepts of purity and impurity by birth have been prevalent in Africa. In other cases, such as the \"Nupe\" of Nigeria, the \"Beni Amer\" of East Africa, and the \"Tira\" of Sudan, the exclusionary principle has been driven by evolving social factors.\nWest Africa.\nAmong the Igbo of Nigeria \u2013 especially Enugu, Anambra, Imo, Abia, Ebonyi, Edo and Delta states of the country \u2013 scholar Elijah Obinna finds that the Osu caste system has been and continues to be a major social issue. The Osu caste is determined by one's birth into a particular family irrespective of the religion practised by the individual. Once born into Osu caste, this Nigerian person is an outcast, shunned and ostracised, with limited opportunities or acceptance, regardless of his or her ability or merit. Obinna discusses how this caste system-related identity and power is deployed within government, Church and indigenous communities.\nThe \"osu\" class systems of eastern Nigeria and southern Cameroon are derived from indigenous religious beliefs and discriminate against the \"Osus\" people as \"owned by deities\" and outcasts.\nThe Songhai economy was based on a caste system. The most common were metalworkers, fishermen, and carpenters. Lower caste participants consisted of mostly non-farm working immigrants, who at times were provided special privileges and held high positions in society. At the top were noblemen and direct descendants of the original Songhai people, followed by freemen and traders.\nIn a review of social stratification systems in Africa, Richter reports that the term caste has been used by French and American scholars to many groups of West African artisans. These groups have been described as inferior, deprived of all political power, have a specific occupation, are hereditary and sometimes despised by others. Richter illustrates caste system in Ivory Coast, with six sub-caste categories. Unlike other parts of the world, mobility is sometimes possible within sub-castes, but not across caste lines. Farmers and artisans have been, claims Richter, distinct castes. Certain sub-castes are shunned more than others. For example, exogamy is rare for women born into families of woodcarvers.\nSimilarly, the Mand\u00e9 societies in Gambia, Ghana, Guinea, Ivory Coast, Liberia, Senegal and Sierra Leone have social stratification systems that divide society by ethnic ties. The Mande class system regards the \"jonow\" slaves as inferior. Similarly, the Wolof in Senegal is divided into three main groups, the \"geer\" (freeborn/nobles), \"jaam\" (slaves and slave descendants) and the underclass \"neeno\". In various parts of West Africa, Fulani societies also have class divisions. Other castes include \"Griots\", \"Forgerons\", and \"Cordonniers\".\nTamari has described endogamous castes of over fifteen West African peoples, including the Tukulor, Songhay, Dogon, Senufo, Minianka, Moors, Manding, Soninke, Wolof, Serer, Fulani, and Tuareg. Castes appeared among the \"Malinke\" people no later than 14th century, and was present among the \"Wolof\" and \"Soninke\", as well as some \"Songhay\" and \"Fulani\" populations, no later than 16th century. Tamari claims that wars, such as the \"Sosso-Malinke\" war described in the \"Sunjata\" epic, led to the formation of blacksmith and bard castes among the people that ultimately became the Mali empire.\nAs West Africa evolved over time, sub-castes emerged that acquired secondary specialisations or changed occupations. Endogamy was prevalent within a caste or among a limited number of castes, yet castes did not form demographic isolates according to Tamari. Social status according to caste was inherited by off-springs automatically; but this inheritance was paternal. That is, children of higher caste men and lower caste or slave concubines would have the caste status of the father.\nCentral Africa.\nEthel M. Albert in 1960 claimed that the societies in Central Africa were caste-like social stratification systems. Similarly, in 1961, Maquet notes that the society in Rwanda and Burundi can be best described as castes. The Tutsi, noted Maquet, considered themselves as superior, with the more numerous Hutu and the least numerous Twa regarded, by birth, as respectively, second and third in the hierarchy of Rwandese society. These groups were largely endogamous, exclusionary and with limited mobility.\nHorn of Africa.\nIn Ethiopia, there have been a number of studies of castes. Broad studies of castes have been written by Alula Pankhurst, who has published a study of caste groups in SW Ethiopia. and a later volume by Dena Freeman writing with Pankhurst.\nIn a review published in 1977, Todd reports that numerous scholars report a system of social stratification in different parts of Africa that resembles some or all aspects of caste system. Examples of such caste systems, he claims, are to be found in Ethiopia in communities such as the Gurage and Konso. He then presents the Dime of Southwestern Ethiopia, amongst whom there operates a system which Todd claims can be unequivocally labelled as caste system. The Dime have seven castes whose size varies considerably. Each broad caste level is a hierarchical order that is based on notions of purity, non-purity and impurity. It uses the concepts of defilement to limit contacts between caste categories and to preserve the purity of the upper castes. These caste categories have been exclusionary, endogamous and the social identity inherited.\nAmong the Kafa, there were also traditionally groups labelled as castes. \"Based on research done before the Derg regime, these studies generally presume the existence of a social hierarchy similar to the caste system. At the top of this hierarchy were the Kafa, followed by occupational groups including blacksmiths (Qemmo), weavers (Shammano), bards (Shatto), potters, and tanners (Manjo). In this hierarchy, the Manjo were commonly referred to as hunters, given the lowest status equal only to slaves.\"\nThe Borana Oromo of southern Ethiopia in the Horn of Africa also have a class system, wherein the Wata, an acculturated hunter-gatherer group, represent the lowest class. Though the Wata today speak the Oromo language, they have traditions of having previously spoken another language before adopting Oromo.\nThe traditionally nomadic Somali people are divided into clans, wherein the Rahanweyn agro-pastoral clans and the occupational clans such as the Madhiban were traditionally sometimes treated as outcasts. As Gabboye, the Madhiban along with the Yibir and Tumaal (collectively referred to as \"sab\") have since obtained political representation within Somalia, and their general social status has improved with the expansion of urban centers.\nEurope.\nEuropean feudalism with its rigid aristocracy can also be considered as a caste system.\nA formal political expression of the system was the system of three or four estates of the realm.\nBasque region.\nFor centuries, through the modern times, the majority regarded Cagots who lived primarily in the Basque region of France and Spain as an inferior caste, and a group of untouchables. While they had the same skin color and religion as the majority, in the churches they had to use segregated doors, drink from segregated fonts, and receive communion on the end of long wooden spoons. It was a closed social system. The socially isolated Cagots were endogamous, and chances of social mobility non-existent.\nUnited Kingdom.\nIn July 2013, the UK government announced its intention to amend the Equality Act 2010, to \"introduce legislation on caste, including any necessary exceptions to the caste provisions, within the framework of domestic discrimination law\". Section 9(5) of the Equality Act 2010 provides that \"a Minister may by order amend the statutory definition of race to include caste and may provide for exceptions in the Act to apply or not to apply to caste\".\nFrom September 2013 to February 2014, Meena Dhanda led a project on \"Caste in Britain\" for the UK Equality and Human Rights Commission (EHRC).\nAmericas.\nLatin America.\nIn colonial Spanish America (16th-early 19th centuries), there were legal divisions of society, the Republic of Spaniards (), comprising European whites, African slaves (), and mixed-race \"castas\", the offspring of unions between whites, blacks, and indigenous. The Republic of Indians () comprised all the various indigenous peoples, now classified in a single category, , by their colonial rulers. In the social and racial hierarchy, European Spaniards were at the apex, with legal rights and privileges. Lower racial groups (Africans, mixed-race castas, and pure indigenous), had fewer legal rights and lower social status. Unlike the rigid caste system in India, in colonial Spanish America there was some fluidity within the social order.\nUnited States.\nIn the opinion of W. Lloyd Warner, discrimination in the Southern United States in the 1930s against Blacks was similar to Indian castes in such features as residential segregation and marriage restrictions. In her 2020 book \"\", journalist Isabel Wilkerson used caste as an analogy to understand racial discrimination in the United States.\nGerald D. Berreman contrasted the differences between discrimination in the United States and India. In India, there are complex religious features which make up the system, whereas in the United States race and color are the basis for differentiation. The caste systems in India and the United States have higher groups which desire to retain their positions for themselves and thus perpetuate the two systems.\nThe process of creating a homogenized society by social engineering in both India and the Southern US has created other institutions that have made class distinctions among different groups evident. Anthropologist James C. Scott elaborates on how \"global capitalism is perhaps the most powerful force for homogenization, whereas the state may be the defender of local difference and variety in some instances\". The caste system, a relic of feudalistic economic systems, emphasizes differences between socio-economic classes that are obviated by openly free market capitalistic economic systems, which reward individual initiative, enterprise, merit, and thrift, thereby creating a path for social mobility. When the feudalistic slave economy of the southern United States was dismantled, Jim Crow laws and acts of domestic terrorism committed by white supremacists prevented many industrious African Americans from participating in the formal economy and achieving economic success on parity with their white peers, or destroying that economic success in instances where it was achieved, such as Black Wall Street, with only rare but commonly touted exceptions to lasting personal success such as Maggie Walker, Annie Malone, and Madame C.J. Walker. Parts of the United States are sometimes divided by race and class status despite the national narrative of integration.\nA survey on caste discrimination conducted by Equality Labs found 67% of Indian Dalits living in the US reporting that they faced caste-based harassment at the workplace, and 27% reporting verbal or physical assault based on their caste. However, the Carnegie Endowment for International Peace study in 2021 criticizes Equality Labs findings and methodology noting Equality Labs study \"relied on a nonrepresentative snowball sampling method to recruit respondents. Furthermore, respondents who did not disclose a caste identity were dropped from the data set. Therefore, it is likely that the sample does not fully represent the South Asian American population and could skew in favor of those who have strong views about caste. While the existence of caste discrimination in India is incontrovertible, its precise extent and intensity in the United States can be contested\".\nIn 2023, Seattle became the first city in the United States to ban discrimination based on caste.\nRacial casteism.\nRacial casteism is a term used to identify the relationship between caste, race, and colorism. In modern-day India, the caste system has expanded to include groups and identities from diasporic groups as well such as the Africana Siddis and Kaffirs. Siddis make up 40,000 of India's vast population and are perceived as untouchables under the caste framework.This categorization is paired with anti-black ideology in the country, that is often adapted by broader uses of the term caste in western countries, most notably the United States. Like the Siddis, Africana caste Sri Lanka Kaffirs make up a small minority of the population with scholars noting that the exact number is hard to determine due to exclusion and lack of recognition from the government. Siddis and Kaffirs are considered untouchables due to their darker skin color alongside other physical factors that distinguish the group as lower caste.\nThe migration of Africana groups such as the Siddis and Kaffirs to South Asia is widely considered to be a result of the Indian Ocean Slave Trade, initiated by Muslim Arabs. During the trade, enslaved Africans were often brought as court servants, herbalists, midwives, or as bonded labor. The limited awareness of these groups can be attributed to caste-ideology fueled from this trade.\nThe racial understanding of caste has largely been debated by scholars, with some like Dr. B. R. Ambedkar arguing that caste differences between higher caste Aryans and lower cast native-Indians being more due to religious factors. While the term remains contended, it is widely understood that this racial assessment is based on the way lower-caste people are treated. Africana diasporic groups who do not fit the caste system reflected by the scheduled tribe are thus considered inferior for their darker skin and grouped in with the untouchables. Since caste is inherited at birth and is inflexible to change throughout a lifetime, this can lead to a racial caste system where colorism largely influences the mobility one has in their lifetime. Terminology shifted away from race-conscious terms in South Asian antiquity, where Aryans had pre-conceived social hierarchies built off of race, to a caste framework during Buddhism's rise in the third century BCE.\nRacial caste is embedded in the institutions that make up South Asia, particularly its governing bodies. When it comes to the electorate of India, voter preference is often based on race, caste, religion, alongside other attributing physical and political factors. This power imbalance alongside the rigid nature of caste can work against those of darker skin complexion to hold positions of power.\nCaste and higher education.\nThe foundational divisions of caste have historically been seen as a determining factor in one's skills and career prospects. Today, many people perceive higher education as a means of achieving their own professional goals, but there are still methods based on caste assumptions used to keep lower caste out of universities. This leads to their exclusion from the potential to be part of higher-paying jobs that are perceived as more elite. This social expectation and prevention of access to education and opportunity have elongated the struggle for financial and social equity amongst people from scheduled tribes and castes.\nAffirmative Action has been a global phenomenon to develop more spaces in politics, jobs, and education for people from historically disadvantaged backgrounds, which has led to the reservation system being applied to universities. Even with these regulations, caste nevertheless remains a largely determining factor in the university system in India. The guarantee of admittance to a certain proportion of people from oppressed castes is not enough to deal with the implications of divisions in higher education. For example, the reservation percentage can vary by state but is generally around 15% for Scheduled Castes, but 2019-20 data shows most universities miss this mark. Across the board, there is an average of 14.7% of scheduled caste students, meaning many universities are at a far lower rate than legislated. These reservation systems have backlash from upper caste groups, who claim that people are only admitted due to their caste status, as opposed to merit, in a similar argument playing out to affirmative action in the United States.\nReservation policies constitute a first step in providing access to admittance into higher education opportunities but do not overcome the overarching challenge of casteism. Caste-based discrimination and social stigma can still affect the experiences of students from marginalized communities in academic institutions. Universities are a crucial place of integration and moving to offer equitable opportunity beyond just attendance, but implementing protective policies to ensure students can be successful. Attendance at university has already been shown to impact how people view caste and has the potential to shape equity building beyond the current interpersonal and systemic relationship.\nSeveral forms of discrimination manifest in universities:\nSocial Discrimination: Students from marginalized castes face social discrimination, exclusion, and/or isolation on campuses. This affects their general educational experience and mental well-being. Numerous cases of harassment and bullying based on caste lines have been reported, with drastic consequences for the victims, but often none for the perpetrators. This promotes a hostile environment for students and hampers their ability to engage positively in the academic community.\n\"When I was enrolled for an undergraduate course, I was vocal about his Dalit identity and vouched for the rights of Dalits and marginalized sections. Most of my upper-caste mates were against reservation. I was always typecast, stereotyped and even labeled with derogatory nicknames,\" Nishat Kabir, who is studying film at Ambedkar University in New Delhi, told Anadolu Agency.\nCampus Facilities: Discrimination can also be observed in access to living facilities, food services, and other campus amenities. Students from marginalized castes may encounter difficulties in availing of these services without bias, and the living arrangements are often internally segregated.\nAcademic + Faculty Discrimination: Discrimination may extend to the academic sphere, with students facing biased treatment, unfair grading, or limited access to academic resources based on their caste background. Instances of discrimination can involve faculty members, who may hold biases that affect their interactions with students. This comes from the inherent hierarchical nature of caste having built centuries of prejudice against lower caste and indigenous students. This influences academic mentorship, guidance, and opportunities for students from marginalized backgrounds.\nEighty-four percent of the SC/ST students surveyed said examiners had asked them about their caste directly or indirectly during their evaluations. One student said: \"Teachers are fine till they do not know your caste. The moment they come to know, their attitude towards you changes completely.\"\nDue to the challenges experienced on top of the normal pressure of being a student, the discrimination that Dalits and people of OBCs face has led to increased rates of suicide, with numerous examples shown to be tied directly to campus harassment and lack of administrative support.\nThe clarity that comes from people sharing their experiences has led to significant pushback in the 21st century, where students have been centering fights for justice and equity, often based on movements that student activists of the past have used. Allahabad University has seen a spike in student protests and demonstrations against institutional discrimination. Students used tactics of information spreading from pamphlets and court cases, to public civil disobedience through marches and sit-ins to disrupt the flow of university life and lead to broader discussions. The student unrest was not unique to Allahabad University but was strong enough to last over 90 days.\nCaste in sociology and entomology.\nThe initial observational studies of the division of labour in ant colonies attempted to demonstrate that ants specialized in tasks that were best suited to their size when they emerged from the pupae stage into the adult stage. A large proportion of the experimental work was done in species that showed strong variation in size. As the size of an adult was fixed for life, workers of a specific size range came to be called a \"caste\", calling up the traditional caste system in India in which a human's standing in society was decided at birth.\nThe notion of caste encouraged a link between scholarship in entomology and sociology because it served as an example of a division of labour in which the participants seemed to be uncompromisingly adapted to special functions and sometimes even unique environments. To bolster the concept of caste, entomologists and sociologists referred to the complementary social or natural parallel and thereby appeared to generalize the concept and give it an appearance of familiarity. In the late 19th- and early 20th centuries, the perceived similarities between the Indian caste system and caste polymorphism in insects were used to create a correspondence or parallelism for the purpose of explaining or clarifying racial stratification in human societies; the explanations came particularly to be employed in the United States. Ideas from heredity and natural selection influenced some sociologists who believed that some groups were predetermined to belong to a lower social or occupational status. Chiefly through the work of W. Lloyd Warner at the University of Chicago, a group of sociologists sharing similar principles came to evolve around the creed of caste in the 1930s and 1940s.\nThe ecologically oriented sociologist Robert E. Park, although attributing more weight to environmental explanations than the biological nonetheless believed that there were obstacles to the assimilation of blacks into American society and that an \"accommodation stage\" in a biracially organized caste system was required before full assimilation. He did disavow his position in 1937, suggesting that blacks were a minority and not a caste. The Indian sociologist Radhakamal Mukerjee was influenced by Robert E. Park and adopted the concept of \"caste\" to describe race relations in the US. According to anthropologist Diane Rodgers, Mukerjee \"proceeded to suggest that a caste system should be correctly instituted in the (US) South to ease race relations.\" Mukerjee often employed both entomological and sociological data and clues to describe caste systems. He wrote \"while the fundamental industries of man are dispersed throughout the insect world, the same kind of polymorphism appears again and again in different species of social insects which have reacted in the same manner as man, under the influence of the same environment, to ensure the supply and provision of subsistence.\" Comparing the caste system in India to caste polymorphism in insects, he noted, \"where we find the organization of social insects developed to perfection, there also has been seen among human associations a minute and even rigid specialization of functions, along with ant- and bee-like societal integrity and cohesiveness.\" He considered the \"resemblances between insect associations and caste-ridden societies\" to be striking enough to be \"amusing\"."}
{"id": "7258", "revid": "1304678", "url": "https://en.wikipedia.org/wiki?curid=7258", "title": "Creation", "text": "Creation or The Creation may refer to:"}
{"id": "7262", "revid": "41798688", "url": "https://en.wikipedia.org/wiki?curid=7262", "title": "CORAL", "text": "CORAL, short for Computer On-line Real-time Applications Language is a programming language originally developed in 1964 at the Royal Radar Establishment (RRE), Malvern, Worcestershire, in the United Kingdom. The R was originally for \"radar\", not \"real-time\". It was influenced primarily by JOVIAL, and thus ALGOL, but is not a subset of either.\nThe most widely-known version, CORAL 66, was subsequently developed by I. F. Currie and M. Griffiths under the auspices of the \"Inter-Establishment Committee for Computer Applications\" (IECCA). Its official definition, edited by Woodward, Wetherall, and Gorman, was first published in 1970.\nIn 1971, CORAL was selected by the Ministry of Defence as the language for future military applications and to support this, a standardization program was introduced to ensure CORAL compilers met the specifications. This process was later adopted by the US Department of Defense while defining Ada.\nOverview.\nCoral 66 is a general-purpose programming language based on ALGOL 60, with some features from Coral 64, JOVIAL, and Fortran. It includes structured record types (as in Pascal) and supports the packing of data into limited storage (also as in Pascal). Like Edinburgh IMP it allows inline (embedded) assembly language, and also offers good runtime checking and diagnostics. It is designed for real-time computing and embedded system applications, and for use on computers with limited processing power, including those limited to fixed-point arithmetic and those without support for dynamic storage allocation.\nThe language was an inter-service standard for British military programming, and was also widely adopted for civil purposes in the British control and automation industry. It was used to write software for both the Ferranti and General Electric Company (GEC) computers from 1971 onwards. Implementations also exist for the Interdata 8/32, PDP-11, VAX and Alpha platforms and HPE Integrity Servers; for the Honeywell, and for the Computer Technology Limited (CTL, later ITL) Modular-1; and for SPARC running Solaris, and Intel running Linux.\nQueen Elizabeth II sent the first email from a head of state from the Royal Signals and Radar Establishment over the ARPANET on March 26, 1976. The message read \"This message to all ARPANET users announces the availability on ARPANET of the Coral 66 compiler provided by the GEC 4080 computer at the Royal Signals and Radar Establishment, Malvern, England, ... Coral 66 is the standard real-time high level language adopted by the Ministry of Defence.\"\nAs Coral was aimed at a variety of real-time work, rather than general office data processing, there was no standardised equivalent to a stdio library. IECCA recommended a primitive input/output (I/O) package to accompany any compiler (in a document titled \"Input/Output of Character data in Coral 66 Utility Programs\"). Most implementers avoided this by producing Coral interfaces to extant Fortran and, later, C libraries.\nCORAL's most significant contribution to computing may have been enforcing quality control in commercial compilers. To have a CORAL compiler approved by IECCA, and thus allowing a compiler to be marketed as a CORAL 66 compiler, the candidate compiler had to compile and execute a standard suite of 25 test programs and 6 benchmark programs. The process was part of the British Standard (BS) 5905 approval process. This methodology was observed and adapted later by the United States Department of Defense for the certification of Ada compilers.\nSource code for a Coral 66 compiler (written in BCPL) has been recovered and the \"Official Definition of Coral 66\" document by Her Majesty's Stationery Office (HMSO) has been scanned; the Ministry of Defence patent office has issued a licence to the Edinburgh Computer History project to allow them to put both the code and the language reference online for non-commercial use.\nVariants.\nA variant of Coral 66 named PO-CORAL was developed during the late 1970s to early 1980s by the British General Post Office (GPO), together with GEC, STC and Plessey, for use on the System X digital telephone exchange control computers. This was later renamed BT-CORAL when British Telecom was spun off from the Post Office. Unique features of this language were the focus on real-time execution, message processing, limits on statement execution between waiting for input, and a prohibition on recursion to remove the need for a stack."}
{"id": "7263", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7263", "title": "Cardiovascular System", "text": ""}
{"id": "7264", "revid": "352850", "url": "https://en.wikipedia.org/wiki?curid=7264", "title": "Rhyming slang", "text": "Rhyming slang is a form of slang word construction in the English language. It is especially prevalent among Cockneys in England, and was first used in the early 19th century in the East End of London; hence its alternative name, Cockney rhyming slang. In the US, especially the criminal underworld of the West Coast between 1880 and 1920, rhyming slang has sometimes been known as Australian slang.\nThe construction of rhyming slang involves replacing a common word with a phrase of two or more words, the last of which rhymes with the original word; then, in almost all cases, omitting, from the end of the phrase, the secondary rhyming word (which is thereafter implied), making the origin and meaning of the phrase elusive to listeners not in the know.\nExamples.\nThe form of Cockney slang is made clear with the following example. The rhyming phrase \"apples and pears\" is used to mean \"stairs\". Following the pattern of omission, \"and pears\" is dropped, thus the spoken phrase \"I'm going up the apples\" means \"I'm going up the stairs\".\nThe following are further common examples of these phrases:\nIn some examples the meaning is further obscured by adding a second iteration of rhyme and truncation to the original rhymed phrase. For example, the word \"Aris\" is often used to indicate the buttocks. This is the result of a double rhyme, starting with the original rough synonym \"arse\", which is rhymed with \"bottle and glass\", leading to \"bottle\". \"Bottle\" was then rhymed with \"Aristotle\" and truncated to \"Aris\". \"Aris\" was then rhymed with \"plaster of Paris\" and truncated to \"plaster\".\nPhonetic \"versus\" phono-semantic forms.\nGhil'ad Zuckermann, a linguist and revivalist, has proposed a distinction between rhyming slang based on sound only, and phono-semantic rhyming slang, which includes a semantic link between the slang expression and its referent (the thing it refers to). An example of rhyming slang based only on sound is the Cockney \"tea leaf\" (thief). An example of phono-semantic rhyming slang is the Cockney \"sorrowful tale\" ((three months in) jail), in which case the person coining the slang term sees a semantic link, sometimes jocular, between the Cockney expression and its referent.\nMainstream usage.\nThe use of rhyming slang has spread beyond the purely dialectal and some examples are to be found in the mainstream British English lexicon, although many users may be unaware of the origin of those words.\nMost of the words changed by this process are nouns, but a few are adjectival, e.g., \"bales\" of cotton (rotten), or the adjectival phrase \"on one's tod\" for \"on one's own\", after Tod Sloan, a famous jockey.\nHistory.\nRhyming slang is believed to have originated in the mid-19th century in the East End of London, with several sources suggesting some time in the 1840s. \"The Flash Dictionary\", of unknown authorship, published in 1921 by Smeeton (48mo), contains a few rhymes. John Camden Hotten's 1859 \"Dictionary of Modern Slang, Cant, and Vulgar Words\" likewise states that it originated in the 1840s (\"about twelve or fifteen years ago\"), but with \"chaunters\" and \"patterers\" in the Seven Dials area of London. Hotten's \"Dictionary\" included the first known \"Glossary of the Rhyming Slang\", which included later mainstays such as \"frog and toad\" (the main road) and \"apples and pears\" (stairs), as well as many more obscure examples, e.g. \"Battle of the Nile\" (a tile, a common term for a hat), \"Duke of York\" (take a walk), and \"Top of Rome\" (home).\nIt remains a matter of speculation exactly how rhyming slang originated, for example, as a linguistic game among friends or as a cryptolect developed intentionally to confuse non-locals. If deliberate, it may also have been used to maintain a sense of community, or to allow traders to talk amongst themselves in marketplaces to facilitate collusion, without customers knowing what they were saying, or by criminals to confuse the police (see thieves' cant).\nThe academic, lexicographer and radio personality Terence Dolan has suggested that rhyming slang was invented by Irish immigrants to London \"so the actual English wouldn't understand what they were talking about.\"\nDevelopment.\nMany examples of rhyming slang are based on locations in London, such as \"Peckham Rye\", meaning \"tie\", which dates from the late nineteenth century; \"Hampstead Heath\", meaning \"teeth\" (usually as \"Hampsteads\"), which was first recorded in 1887; and \"barnet\" (Barnet Fair), meaning \"hair\", which dates from the 1850s.\nIn the 20th century, rhyming slang began to be based on the names of celebrities \u2014 Gregory Peck (neck; cheque), Ruby Murray [as Ruby] (curry), Alan Whicker [as \"Alan Whickers\"] (knickers), Puff Daddy (caddy), Max Miller (pillow [pronounced ]), Meryl Streep (cheap), Nat King Cole (\"dole\"), Britney Spears (beers, tears), Henry Halls (balls) \u2014 and after pop culture references \u2014 Captain Kirk (work), Pop Goes the Weasel (diesel), Mona Lisa (pizza), Mickey Mouse (Scouse), Wallace and Gromit (vomit), Brady Bunch (lunch), Bugs Bunny (money), Scooby-Doo (clue), Winnie the Pooh (shoe), and \"Schindler's List\" (pissed). Some words have numerous definitions, such as dead (\"Father Ted\", \"gone to bed\", brown bread), door (Roger Moore, Andrea Corr, George Bernard Shaw, Rory O'Moore), cocaine (Kurt Cobain; [as \"Charlie\"] Bob Marley, Boutros Boutros-Ghali, Gianluca Vialli, oats and barley; [as \"line\"] Patsy Cline; [as \"powder\"] Niki Lauda), flares (\"Lionel Blairs\", \"Tony Blairs\", \"Rupert Bears\", \"Dan Dares\"), etc.\nMany examples have passed into common usage. Some substitutions have become relatively widespread in England in their contracted form. \"To have a butcher's\", meaning to have a look, originates from \"butcher's hook\", an S-shaped hook used by butchers to hang up meat, and dates from the late nineteenth century but has existed independently in general use from around the 1930s simply as \"butchers\". Similarly, \"use your loaf\", meaning \"use your head\", derives from \"loaf of bread\" and also dates from the late nineteenth century but came into independent use in the 1930s.\nConversely usages have lapsed, or been usurped (\"Hounslow Heath\" for teeth, was replaced by \"Hampsteads\" from the heath of the same name, starting ).\nIn some cases, false etymologies exist. For example, the term \"barney\" has been used to mean an altercation or fight since the late nineteenth century, although without a clear derivation. In the 2001 feature film \"Ocean's Eleven\", the explanation for the term is that it derives from Barney Rubble, the name of a cartoon character from the \"Flintstones\" television program many decades later in origin.\nRegional and international variations.\nRhyming slang is used mainly in London in England but can, to some degree, be understood across the country. Some constructions, however, rely on particular regional accents for the rhymes to work. For instance, the term \"Charing Cross\" (a place in London), used to mean \"horse\" since the mid-nineteenth century, does not work for a speaker without the lot\u2013cloth split, common in London at that time but not nowadays. A similar example is \"Joanna\" meaning \"piano\", which is based on the pronunciation of \"piano\" as \"pianna\" . Unique formations also exist in other parts of the United Kingdom, such as in the East Midlands, where the local accent has formed \"Derby Road\", which rhymes with \"cold\".\nOutside England, rhyming slang is used in many English-speaking countries in the Commonwealth of Nations, with local variations. For example, in Australian slang, the term for an English person is \"pommy\", which has been proposed as a rhyme on \"pomegranate\", pronounced \"Pummy Grant\", which rhymed with \"immigrant\".\nRhyming slang is continually evolving, and new phrases are introduced all the time; new personalities replace old ones\u2014pop culture introduces new words\u2014as in \"I haven't a Scooby\" (from Scooby Doo, the eponymous cartoon dog of the cartoon series) meaning \"I haven't a clue\".\nTaboo terms.\nRhyming slang is often used as a substitute for words regarded as taboo, often to the extent that the association with the taboo word becomes unknown over time. \"Berk\" (often used to mean \"foolish person\") originates from the most famous of all fox hunts, the \"Berkeley Hunt\" meaning \"cunt\"; \"cobblers\" (often used in the context \"what you said is rubbish\") originates from \"cobbler's awls\", meaning \"balls\" (as in testicles); and \"hampton\" (usually \"'ampton\") meaning \"prick\" (as in penis) originates from \"Hampton Wick\" (a place in London) \u2013 the second part \"wick\" also entered common usage as \"he gets on my wick\" (he is an annoying person).\nLesser taboo terms include \"pony and trap\" for \"crap\" (as in defecate, but often used to denote nonsense or low quality); to blow a raspberry (rude sound of derision) from raspberry tart for \"fart\"; \"D'Oyly Carte\" (an opera company) for \"fart\"; \"Jimmy Riddle\" (an American country musician) for \"piddle\" (as in urinate), \"J. Arthur Rank\" (a film mogul), \"Sherman tank\", \"Jodrell Bank\" or \"ham shank\" for \"wank\", \"Bristol Cities\" (contracted to 'Bristols') for \"titties\", etc. \"Taking the Mick\" or \"taking the Mickey\" is thought to be a rhyming slang form of \"taking the piss\", where \"Mick\" came from \"Mickey Bliss\".\nIn December 2004 Joe Pasquale, winner of the fourth series of ITV's \"I'm a Celebrity... Get Me Out of Here!\", became well known for his frequent use of the term \"Jacobs\", for Jacob's Cream Crackers, a rhyming slang term for knackers i.e. testicles.\nIn popular culture.\nRhyming slang has been widely used in popular culture including film, television, music, literature, sport and degree classification.\nIn university degree classification.\nIn the British undergraduate degree classification system a first class honours degree is known as a \"Geoff Hurst\" (First) after the English 1966 World Cup footballer. An upper second class degree (a.k.a. a \"2:1\") is called an \"Attila the Hun\", and a lower second class (\"2:2\") a \"Desmond Tutu\", while a third class degree is known as a \"Thora Hird\" or \"Douglas Hurd\".\nIn film.\nCary Grant's character teaches rhyming slang to his female companion in \"Mr. Lucky\" (1943), describing it as 'Australian rhyming slang'. Rhyming slang is also used and described in a scene of the 1967 film \"To Sir, with Love\" starring Sidney Poitier, where the English students tell their foreign teacher that the slang is a drag and something for old people. The closing song of the 1969 crime caper, \"The Italian Job\", (\"Getta Bloomin' Move On\" a.k.a. \"The Self Preservation Society\") contains many slang terms.\nRhyming slang has been used to lend authenticity to an East End setting. Examples include \"Lock, Stock and Two Smoking Barrels\" (1998) (wherein the slang is translated via subtitles in one scene); \"The Limey\" (1999); \"Sexy Beast\" (2000); \"Snatch\" (2000); \"Ocean's Eleven\" (2001); and \"Austin Powers in Goldmember\" (2002); \"It's All Gone Pete Tong\" (2004), after BBC radio disc jockey Pete Tong whose name is used in this context as rhyming slang for \"wrong\"; \"Green Street Hooligans\" (2005). In \"Margin Call\" (2011), Will Emerson, played by London-born actor Paul Bettany, asks a friend on the telephone, \"How's the trouble and strife?\" (\"wife\").\n\"Cockneys vs Zombies\" (2012) mocked the genesis of rhyming slang terms when a Cockney character calls zombies \"Trafalgars\" to even his Cockney fellows' puzzlement; he then explains it thus: \"Trafalgar square \u2013 fox and hare \u2013 hairy Greek \u2013 five day week \u2013 weak and feeble \u2013 pins and needles \u2013 needle and stitch \u2013 Abercrombie and Fitch \u2013 Abercrombie: zombie\".\nThe live-action Disney film \"Mary Poppins Returns\" song \"Trip A Little Light Fantastic\" involves Cockney rhyming slang in part of its lyrics, and is primarily spoken by the London lamplighters.\nIn the animated superhero film \"\" (2023), character Spider-Punk, a Camden native, is heard saying: \"I haven't got a scooby\" (\"clue\").\nTelevision.\nSlang had a resurgence of popular interest in Britain beginning in the 1970s, resulting from its use in a number of London-based television programmes such as \"Steptoe and Son\" (1970\u201374); and \"Not On Your Nellie\" (1974\u201375), starring Hylda Baker as Nellie Pickersgill, alludes to the phrase \"not on your Nellie Duff\", rhyming slang for \"not on your puff\" i.e. not on your life. Similarly, \"The Sweeney\" (1975\u201378) alludes to the phrase \"Sweeney Todd\" for \"Flying Squad\", a rapid response unit of London's Metropolitan Police. In \"The Fall and Rise of Reginald Perrin\" (1976\u201379), a comic twist was added to rhyming slang by way of spurious and fabricated examples which a young man had laboriously attempted to explain to his father (e.g. 'dustbins' meaning 'children', as in 'dustbin lids'='kids'; 'Teds' being 'Ted Heath' and thus 'teeth'; and even 'Chitty Chitty' being 'Chitty Chitty Bang Bang', and thus 'rhyming slang'...). It was also featured in an episode of \"The Good Life\" in the first season (1975) where Tom and Barbara purchase a wood-burning range from a junk trader called Sam, who litters his language with phony rhyming slang in hopes of convincing suburban residents that he is an authentic traditional Cockney trader. He comes up with a fake story as to the origin of Cockney rhyming slang and is caught out rather quickly. In \"The Jeffersons\" season 2 (1976) episode \"The Breakup: Part 2\", Mr. Bentley explains Cockney rhyming slang to George Jefferson, in that \"whistle and flute\" means \"suit\", \"apples and pears\" means \"stairs\", \"plates of meat\" means \"feet\".\nThe use of rhyming slang was also prominent in \"Mind Your Language\" (1977\u201379), \"Citizen Smith\" (1977\u201380), \"Minder\" (1979\u201394), \"Only Fools and Horses\" (1981\u201391), and \"EastEnders\" (1985\u2013). \"Minder\" could be quite uncompromising in its use of obscure forms without any clarification. Thus the non-Cockney viewer was obliged to deduce that, say, \"iron\" was \"male homosexual\" ('iron'='iron hoof'='poof'). One episode in Series 5 of \"Steptoe and Son\" was entitled \"Any Old Iron\", for the same reason, when Albert thinks that Harold is 'on the turn'. Variations of rhyming slang were also used in sitcom \"Birds of a Feather\", by main characters Sharon and Tracey, often to the confusion of character, Dorian Green, who was unfamiliar with the terms.\nOne early US show to regularly feature rhyming slang was the Saturday morning children's show \"The Bugaloos\" (1970\u201372), with the character of Harmony (Wayne Laryea) often incorporating it in his dialogue.\nMusic.\nIn popular music, Spike Jones and his City Slickers recorded \"So 'Elp Me\", based on rhyming slang, in 1950. The 1967 Kinks song \"Harry Rag\" was based on the usage of the name Harry Wragg as rhyming slang for \"fag\" (i.e. a cigarette). The idiom made a brief appearance in the UK-based DJ reggae music of the 1980s in the hit \"Cockney Translation\" by Smiley Culture of South London; this was followed a couple of years later by Domenick and Peter Metro's \"Cockney and Yardie\". London-based artists such as Audio Bullys and Chas &amp; Dave (and others from elsewhere in the UK, such as The Streets, who are from Birmingham) frequently use rhyming slang in their songs.\nBritish-born M.C. MF Doom released an ode entitled \"Rhymin' Slang\", after settling in the UK in 2010. The track was released on the 2012 JJ Doom album \"Key to the Kuffs\".\nAnother contributor was Lonnie Donegan who had a song called \"My Old Man's a Dustman\". In it he says his father has trouble putting on his boots \"He's got such a job to pull them up that he calls them daisy roots\".\nLiterature.\nIn modern literature, Cockney rhyming slang is used frequently in the novels and short stories of Kim Newman, for instance in the short story collections \"The Man from the Diogenes Club\" (2006) and \"Secret Files of the Diogenes Club\" (2007), where it is explained at the end of each book.\nIt is also parodied in \"Going Postal\" by Terry Pratchett, which features a geriatric Junior Postman by the name of Tolliver Groat, a speaker of 'Dimwell Arrhythmic Rhyming Slang', the only rhyming slang on the Disc which \"does not actually rhyme\". Thus, a wig is a 'prunes', from 'syrup of prunes', an obvious parody of the Cockney \"syrup\" from \"syrup of figs \u2013 wig\". There are numerous other parodies, though it has been pointed out that the result is even more impenetrable than a conventional rhyming slang and so may not be quite so illogical as it seems, given the assumed purpose of rhyming slang as a means of communicating in a manner unintelligible to all but the initiated.\nIn the book \"Goodbye to All That\" by Robert Graves, a beer is a \"broken square\" as Welch Fusiliers officers walk into a pub and order broken squares when they see men from the Black Watch. The Black Watch had a minor blemish on its record of otherwise unbroken squares. Fistfights ensued.\nIn Dashiell Hammett's \"The Dain Curse\", the protagonist exhibits familiarity with Cockney rhyming slang, referring to gambling at dice with the phrase \"rats and mice.\"\nCockney rhyming slang is one of the main influences for the dialect spoken in \"A Clockwork Orange\" (1962). The author of the novel, Anthony Burgess, also believed the phrase \"as queer as a clockwork orange\" was Cockney slang having heard it in a London pub in 1945, and subsequently named it in the title of his book.\nSport.\nIn Scottish football, a number of clubs have nicknames taken from rhyming slang. Partick Thistle are known as the \"Harry Rags\", which is taken from the rhyming slang of their 'official' nickname \"the jags\". Rangers are known as the \"Teddy Bears\", which comes from the rhyming slang for \"the Gers\" (shortened version of Ran-gers). Heart of Midlothian are known as the \"Jambos\", which comes from \"Jam Tarts\" which is the rhyming slang for \"Hearts\" which is the common abbreviation of the club's name. Hibernian are also referred to as \"The Cabbage\" which comes from Cabbage and Ribs being the rhyming slang for Hibs. The phrase Hampden Roar (originally describing the loud crowd noise emanating from the national stadium) is employed as \"What's the Hampden?\", (\"What's the score?\", idiom for \"What's happening / what's going on?\").\nIn rugby league, \"meat pie\" is used for try."}
{"id": "7265", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=7265", "title": "Canchim", "text": "The Canchim is a breed of beef cattle developed in Central Brazil by crossing European Charolais cattle with Indubrazil cattle already kept in Brazil where Asian Zebu type cattle are best suited to the tropical conditions. When compared with Zebu bulls, Canchim bulls produce the same number of calves, but heavier and of superior quality. Compared to European breeds, the Canchim bull produces calves with the same weight but in larger numbers. The fast-growing progeny, from crossbred zebu cows with Canchim bulls, can be slaughtered at 18 months old from feedlots after weaning, up to 24 months old from feedlots after grazing and at 30 months from grazing on the range.\nOrigin.\nZebu cattle (Bos Indicus), introduced to Brazil in the last century, were extensively crossbred with herds of native cattle. The Indian breed, well known for its ability to survive in the tropics, adapted quickly to Brazil, and soon populated large areas, considerably improving Brazilian beef cattle breeding. Zebu cattle were however found to be inferior to the European breeds in growth rate and yield of meat. It became clear that the beef cattle population required genetic improvement. Simply placing European beef cattle (Bos Taurus), highly productive in temperate climates, in Central Brazil, would not produce good results, due to their inability to adapt to a tropical environment. Besides the climate, other factors such as the high occurrence of parasites, diseases and the very low nutritional value of the native forage were problems.\nFormation of the breed.\nThe European breed used in the formation of Canchim cattle was Charolais. In 1922 the Ministry of Agriculture imported Charolais cattle to the State of Goias, where they remained till 1936, when they were transferred to S\u00e3o Carlos in the State of S\u00e3o Paulo, to the Canchim Farm of the Government Research Station, EMBRAPA. From this herd originated the dams and sires utilised in the program of crossbreeding.\nThe main Zebu breed which contributed to the formation to the Canchim was the Indubrazil, although Guzer\u00e1 and Nelore cattle were also used. Preference was given to the Indubrasil breed, due to the ease of obtaining large herds at reasonable prices, which would have been difficult with Gir, Nelore or Guzer\u00e1.\nThe alternative crossbreeding programs initiated in 1940 by Dr. Antonio Teixeira Viana had the objective of obtaining first, crossbreeds 5/8 Charolais and 3/8 Zebu and second, 3/8 Charolais x 5/8 Zebu, to evaluate which of the two was the most successful. The total number of Zebu cows utilized to produce the half-breeds was 368, of which 292 were Indubrasil, 44 Guzer\u00e1 and 32 Nelore. All the animals produced were reared exclusively on the range. Control of parasites was done every 15 days and the animals were weighed at birth and monthly. The females were weighed up to 30 months and the males up to 40 months.\nThe data collected during various years of work, permitted an evaluation of the various degrees of crossbreeding. The conclusion was that the 5/8 Charolais and 3/8 Zebu was the most suitable, presenting an excellent frame for meat, precocious, resistance to heat and parasites, and a uniform coat. The first crossbred animals, 5/8 Charolais and 3/8 Zebu, were born in 1953. Thus was born a new type of beef cattle for Central Brazil, with the name CANCHIM, derived from the name of a tree very common in the region where the breed was developed. It was not until 1971 that the Brazilian Association of Canchim Cattle Breeders (ABCCAN) was formed, and on 11 November 1972 the Herd Book was initiated. On 18 May 1983 the Ministry of Agriculture, recognized Canchim type cattle as a Breed.\nNew bloodlines.\nThe Canchim breed, being a synthetic breed, permits breeders, in the development of new crossbreeding systems, to use the breeds used to form the Canchim breed, besides the breed itself, in its development.\nThere are many Canchim breeders forming new blood lines. Today the Nelore breed totally dominates the Zebu breed in the formation of Canchim. American and French Charolais semen, from carefully selected bulls is also used and recommended by the ABCCAN to form new bloodlines."}
{"id": "7266", "revid": "157842", "url": "https://en.wikipedia.org/wiki?curid=7266", "title": "Christkindlmarkt", "text": ""}
