{"id": "5247", "revid": "1273176637", "url": "https://en.wikipedia.org/wiki?curid=5247", "title": "Country music", "text": "Country (also called country and western) is a music genre originating in the southern regions of the United States, both the American South and the Southwest. First produced in the 1920s, country music is primarily focused on singing stories about working-class and blue-collar American life.\nCountry music is known for its ballads and dance tunes (i.e., \"honky-tonk music\") with simple form, folk lyrics, and harmonies generally accompanied by instruments such as banjos, fiddles, harmonicas, and many types of guitar (including acoustic, electric, steel, and resonator guitars). Though it is primarily rooted in various forms of American folk music, such as old-time music and Appalachian music, many other traditions, including Mexican, Irish, and Hawaiian music, have had a formative influence on the genre. Blues modes from blues music have been used extensively throughout its history as well.\nOnce called \"hillbilly music\", the term \"country music\" gained popularity in the 1940s. The genre came to encompass western music, which evolved parallel to hillbilly music from similar roots, in the mid-20th century. Contemporary styles of western music include Texas country, red dirt, and Hispano- and Mexican American-led Tejano and New Mexico music, which still exists alongside longstanding indigenous traditions.\nIn 2009, in the United States, country music was the most-listened-to rush-hour radio genre during the evening commute, and second-most popular in the morning commute.\nOrigins.\nThe main components of the modern country music style date back to music traditions throughout the Southern United States and Southwestern United States, while its place in American popular music was established in the 1920s during the early days of music recording. According to country historian Bill C. Malone, country music was \"introduced to the world as a Southern phenomenon.\"\nMigration into the southern Appalachian Mountains, of the Southeastern United States, brought the folk music and instruments of Europe and the Mediterranean Basin along with it for nearly 300 years, which developed into Appalachian music. As the country expanded westward, the Mississippi River and Louisiana became a crossroads for country music, giving rise to Cajun music. In the Southwestern United States, it was the Rocky Mountains, American frontier, and Rio Grande that acted as a similar backdrop for Native American, Mexican, and cowboy ballads, which resulted in New Mexico music and the development of western music, and it is directly related to Red Dirt, Texas country, and Tejano music styles. In the Asia-Pacific, the steel guitar sound of country music has its provenance in the music of Hawaii.\nRole of East Tennessee.\nThe U.S. Congress has formally recognized Bristol, Tennessee as the \"Birthplace of Country Music\", based on the historic Bristol recording sessions of 1927. Since 2014, the city has been home to the Birthplace of Country Music Museum. Historians have also noted the influence of the less-known Johnson City sessions of 1928 and 1929, and the Knoxville sessions of 1929 and 1930. In addition, the Mountain City Fiddlers Convention, held in 1925, helped to inspire modern country music. Before these, pioneer settlers, in the Great Smoky Mountains region, had developed a rich musical heritage.\nGenerations.\nThe first generation emerged in the 1920s, with Atlanta's music scene playing a major role in launching country's earliest recording artists. James Gideon \"Gid\" Tanner (1885\u20131960) was an American old-time fiddler and one of the earliest stars of what would come to be known as country music. His band, the Skillet Lickers, was one of the most innovative and influential string bands of the 1920s and 1930s. Its most notable members were Clayton McMichen (fiddle and vocal), Dan Hornsby (vocals), Riley Puckett (guitar and vocal) and Robert Lee Sweat (guitar). New York City record label Okeh Records began issuing hillbilly music records by Fiddlin' John Carson as early as 1923, followed by Columbia Records (series 15000D \"Old Familiar Tunes\") (Samantha Bumgarner) in 1924, and RCA Victor Records in 1927 with the first famous pioneers of the genre Jimmie Rodgers, who is widely considered the \"Father of Country Music\", and the first family of country music the Carter Family. Many \"hillbilly\" musicians recorded blues songs throughout the 1920s.\nDuring the second generation (1930s\u20131940s), radio became a popular source of entertainment, and \"barn dance\" shows featuring country music were started all over the South, as far north as Chicago, and as far west as California. The most important was the \"Grand Ole Opry\", aired starting in 1925 by WSM in Nashville and continuing to the present day. During the 1930s and 1940s, cowboy songs, or western music, which had been recorded since the 1920s, were popularized by films made in Hollywood, many featuring Gene Autry, who was known as king of the \"singing cowboys,\" and Hank Williams. Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a \"hot string band,\" and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as western swing. Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. Country musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded \"Boogie Woogie\".\nThe third generation (1950s\u20131960s) started at the end of World War II with \"mountaineer\" string band music known as bluegrass, which emerged when Bill Monroe, along with Lester Flatt and Earl Scruggs, were introduced by Roy Acuff at the Grand Ole Opry. Gospel music remained a popular component of country music. The Native American, Hispano, and American frontier music of the Southwestern United States and Northern Mexico, became popular among poor communities in New Mexico, Oklahoma, and Texas; the basic ensemble consisted of classical guitar, bass guitar, dobro or steel guitar, though some larger ensembles featured electric guitars, trumpets, keyboards (especially the honky-tonk piano, a type of tack piano), banjos, and drums. By the early 1950s it blended with rock and roll, becoming the rockabilly sound produced by Sam Phillips, Norman Petty, and Bob Keane. Musicians like Elvis Presley, Buddy Holly, Jerry Lee Lewis, Ritchie Valens, Carl Perkins, Roy Orbison, and Johnny Cash emerged as enduring representatives of the style. Beginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee; Patsy Cline and Jim Reeves were two of the most broadly popular Nashville sound artists, and their deaths in separate plane crashes in the early 1960s were a factor in the genre's decline. Starting in the 1950s to the mid-1960s, western singer-songwriters such as Marty Robbins rose in prominence as did others, throughout western music traditions, like New Mexico music's Al Hurricane. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the \"old values\" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock.\nFourth generation (1970s\u20131980s) music included outlaw country with roots in the Bakersfield sound, and country pop with roots in the countrypolitan, folk music and soft rock. Between 1972 and 1975 singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles. By the mid-1970s, Texas country and Tejano music gained popularity with performers like Freddie Fender. During the early 1980s country artists continued to see their records perform well on the pop charts. In 1980 a style of \"neocountry disco music\" was popularized. During the mid-1980s a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts in favor of more traditional \"back-to-basics\" production.\nDuring the fifth generation (the 1990s), neotraditionalists and stadium country acts prospered.\nThe sixth generation (2000s\u2013present) has seen a certain amount of diversification in regard to country music styles. It has also, however, seen a shift into patriotism and conservative politics since 9/11, though such themes are less prevalent in more modern trends. The influence of rock music in country has become more overt during the late 2000s and early 2010s. Most of the best-selling country songs of this era were those by Lady A, Florida Georgia Line, Carrie Underwood, and Taylor Swift. Hip hop also made its mark on country music with the emergence of country rap.\nHistory.\nFirst generation (1920s).\nThe first commercial recordings of what was considered instrumental music in the traditional country style were \"Arkansas Traveler\" and \"Turkey in the Straw\" by fiddlers Henry Gilliland &amp; A.C. (Eck) Robertson on June 30, 1922, for Victor Records and released in April 1923. Columbia Records began issuing records with \"hillbilly\" music (series 15000D \"Old Familiar Tunes\") as early as 1924.\nThe first commercial recording of what is widely considered to be the first country song featuring vocals and lyrics was Fiddlin' John Carson with \"Little Log Cabin in the Lane\" for Okeh Records on June 14, 1923.\nVernon Dalhart was the first country singer to have a nationwide hit in May 1924 with \"Wreck of the Old 97\". The flip side of the record was \"Lonesome Road Blues\", which also became very popular. In April 1924, \"Aunt\" Samantha Bumgarner and Eva Davis became the first female musicians to record and release country songs. The record 129-D produced by Columbia features Samantha playing fiddle and singing Big-Eyed Rabbit while Eva Davis plays banjo. The other side features Eva Davis playing banjo while singing Wild Bill Jones. Many of the early country musicians, such as the yodeler Cliff Carlisle, recorded blues songs into the 1930s. Other important early recording artists were Riley Puckett, Don Richardson, Fiddlin' John Carson, Uncle Dave Macon, Al Hopkins, Ernest V. Stoneman, Blind Alfred Reed, Charlie Poole and the North Carolina Ramblers and the Skillet Lickers. The steel guitar entered country music as early as 1922, when Jimmie Tarlton met famed Hawaiian guitarist Frank Ferera on the West Coast.\nJimmie Rodgers and the Carter Family are widely considered to be important early country musicians. From Scott County, Virginia, the Carters had learned sight reading of hymnals and sheet music using solfege. Their songs were first captured at a historic recording session in Bristol, Tennessee, on August 1, 1927, where Ralph Peer was the talent scout and sound recordist. A scene in the movie \"O Brother, Where Art Thou?\" depicts a similar occurrence in the same timeframe.\nRodgers fused hillbilly country, gospel, jazz, blues, pop, cowboy, and folk, and many of his best songs were his compositions, including \"Blue Yodel\", which sold over a million records and established Rodgers as the premier singer of early country music. Beginning in 1927, and for the next 17 years, the Carters recorded some 300 old-time ballads, traditional tunes, country songs and gospel hymns, all representative of America's southeastern folklore and heritage. Maybelle Carter went on to continue the family tradition with her daughters as The Carter Sisters; her daughter June would marry (in succession) Carl Smith, Rip Nix and Johnny Cash, having children with each who would also become country singers.\nSecond generation (1930s\u20131940s).\nRecord sales declined during the Great Depression. However, radio became a popular source of entertainment, and \"barn dance\" shows featuring country music were started by radio stations all over the South, as far north as Chicago, and as far west as California.\nThe most important was the \"Grand Ole Opry\", aired starting in 1925 by WSM in Nashville and continuing to the present day. Some of the early stars on the \"Opry\" were Uncle Dave Macon, Roy Acuff and African American harmonica player DeFord Bailey. WSM's 50,000-watt signal (in 1934) could often be heard across the country. Many musicians performed and recorded songs in any number of styles. Moon Mullican, for example, played western swing but also recorded songs that can be called rockabilly. Between 1947 and 1949, country crooner Eddy Arnold placed eight songs in the top 10. From 1945 to 1955 Jenny Lou Carson was one of the most prolific songwriters in country music.\nSinging cowboys and western swing.\nIn the 1930s and 1940s, cowboy songs, or western music, which had been recorded since the 1920s, were popularized by films made in Hollywood. Some of the popular singing cowboys from the era were Gene Autry, the Sons of the Pioneers, and Roy Rogers. Country music and western music were frequently played together on the same radio stations, hence the term \"country and western\" music, despite country and western being two distinct genres.\nCowgirls contributed to the sound in various family groups. Patsy Montana opened the door for female artists with her history-making song \"I Want To Be a Cowboy's Sweetheart\". This would begin a movement toward opportunities for women to have successful solo careers. Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a \"hot string band,\" and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as western swing. Cliff Bruner, Moon Mullican, Milton Brown and Adolph Hofner were other early western swing pioneers. Spade Cooley and Tex Williams also had very popular bands and appeared in films. At its height, western swing rivaled the popularity of big band swing music.\nChanging instrumentation.\nDrums were scorned by early country musicians as being \"too loud\" and \"not pure\", but by 1935 western swing big band leader Bob Wills had added drums to the Texas Playboys. In the mid-1940s, the Grand Ole Opry did not want the Playboys' drummer to appear on stage. Although drums were commonly used by rockabilly groups by 1955, the less-conservative-than-the-Grand-Ole-Opry \"Louisiana Hayride\" kept its infrequently used drummer backstage as late as 1956. By the early 1960s, however, it was rare for a country band not to have a drummer. Bob Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. A decade later (1948) Arthur Smith achieved top 10 US country chart success with his MGM Records recording of \"Guitar Boogie\", which crossed over to the US pop chart, introducing many people to the potential of the electric guitar. For several decades Nashville session players preferred the warm tones of the Gibson and Gretsch archtop electrics, but a \"hot\" Fender style, using guitars which became available beginning in the early 1950s, eventually prevailed as the signature guitar sound of country.\nHillbilly boogie.\nCountry musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded \"Boogie Woogie\". The trickle of what was initially called hillbilly boogie, or okie boogie (later to be renamed country boogie), became a flood beginning in late 1945. One notable release from this period was the Delmore Brothers' \"Freight Train Boogie\", considered to be part of the combined evolution of country music and blues towards rockabilly. In 1948, Arthur \"Guitar Boogie\" Smith achieved top ten US country chart success with his MGM Records recordings of \"Guitar Boogie\" and \"Banjo Boogie\", with the former crossing over to the US pop charts. Other country boogie artists included Moon Mullican, Merrill Moore and Tennessee Ernie Ford. The hillbilly boogie period lasted into the 1950s and remains one of many subgenres of country into the 21st century.\nBluegrass, folk and gospel.\nBy the end of World War II, \"mountaineer\" string band music known as bluegrass had emerged when Bill Monroe joined with Lester Flatt and Earl Scruggs, introduced by Roy Acuff at the Grand Ole Opry. That was the ordination of bluegrass music and how Bill Monroe came to be known as the \"Father of Bluegrass.\" Gospel music, too, remained a popular component of bluegrass and other sorts of country music. Red Foley, the biggest country star following World War II, had one of the first million-selling gospel hits (\"Peace in the Valley\") and also sang boogie, blues and rockabilly. In the post-war period, country music was called \"folk\" in the trades, and \"hillbilly\" within the industry. In 1944, \"Billboard\" replaced the term \"hillbilly\" with \"folk songs and blues,\" and switched to \"country and western\" in 1949.\nHonky tonk.\nAnother type of stripped-down and raw music with a variety of moods and a basic ensemble of guitar, bass, dobro or steel guitar (and later) drums became popular, especially among rural residents in the three states of Texhomex, those being \"Tex\"as, Okla\"ho\"ma, and New \"Mex\"ico. It became known as honky tonk and had its roots in western swing and the ranchera music of Mexico and the border states, particularly New Mexico and Texas, together with the blues of the American South. Bob Wills and His Texas Playboys personified this music which has been described as \"a little bit of this, and a little bit of that, a little bit of black and a little bit of white\u00a0... just loud enough to keep you from thinking too much and to go right on ordering the whiskey.\" East Texan Al Dexter had a hit with \"Honky Tonk Blues\", and seven years later \"Pistol Packin' Mama\". These \"honky tonk\" songs were associated with barrooms, and was performed by the likes of Ernest Tubb, Kitty Wells (the first major female country solo singer), Ted Daffan, Floyd Tillman, the Maddox Brothers and Rose, Lefty Frizzell and Hank Williams; the music of these artists would later be called \"traditional\" country. Williams' influence in particular would prove to be enormous, inspiring many of the pioneers of rock and roll, such as Elvis Presley, Jerry Lee Lewis, Chuck Berry and Ike Turner, while providing a framework for emerging honky tonk talents like George Jones. Webb Pierce was the top-charting country artist of the 1950s, with 13 of his singles spending 113 weeks at number one. He charted 48 singles during the decade; 31 reached the top ten and 26 reached the top four.\nThird generation (1950s\u20131960s).\nBy the early 1950s, a blend of western swing, country boogie, and honky tonk was played by most country bands, a mixture which followed in the footsteps of Gene Autry, Lydia Mendoza, Roy Rogers, and Patsy Montana. Western music, influenced by the cowboy ballads, New Mexico, Texas country and Tejano music rhythms of the Southwestern United States and Northern Mexico, reached its peak in popularity in the late 1950s, most notably with the song \"El Paso\", first recorded by Marty Robbins in September 1959. Western music's influence would continue to grow within the country music sphere, western musicians like Michael Martin Murphey, New Mexico music artists Al Hurricane and Antonia Apodaca, Tejano music performer Little Joe, and even folk revivalist John Denver, all first rose to prominence during this time. This western music influence largely kept the music of the folk revival and folk rock from influencing the country music genre much, despite the similarity in instrumentation and origins (see, for instance, the Byrds' negative reception during their appearance on the \"Grand Ole Opry\"). The main concern was largely political: most folk revival was largely driven by progressive activists, a stark contrast to the culturally conservative audiences of country music. John Denver was perhaps the only musician to have major success in both the country and folk revival genres throughout his career, later only a handful of artists like Burl Ives and Canadian musician Gordon Lightfoot successfully made the crossover to country after folk revival fell out of fashion. During the mid-1950s a new style of country music became popular, eventually to be referred to as rockabilly.\nIn 1953, the first all-country radio station was established in Lubbock, Texas. The music of the 1960s and 1970s targeted the American working class, and truckers in particular. As country radio became more popular, trucking songs like the 1963 hit song \"Six Days on the Road\" by Dave Dudley began to make up their own subgenre of country. These revamped songs sought to portray American truckers as a \"new folk hero\", marking a significant shift in sound from earlier country music. The song was written by actual truckers and contained numerous references to the trucker culture of the time like \"ICC\" for Interstate Commerce Commission and \"little white pills\" as a reference to amphetamines. Starday Records in Nashville followed up on Dudley's initial success with the release of \"Give Me 40 Acres\" by the Willis Brothers.\nRockabilly.\nRockabilly was most popular with country fans in the 1950s; one of the first rock and roll superstars was former western yodeler Bill Haley, who repurposed his Four Aces of Western Swing into a rock and roll band in the early 1950s and renamed it the Comets. Bill Haley &amp; His Comets are credited with two of the first successful rock and roll records, \"Crazy Man, Crazy\" of 1953 and \"Rock Around the Clock\" in 1954.\n1956 could be called the year of rockabilly in country music. Rockabilly was an early form of rock and roll, an upbeat combination of blues and country music. The number two, three and four songs on \"Billboard's\" charts for that year were Elvis Presley, \"Heartbreak Hotel\"; Johnny Cash, \"I Walk the Line\"; and Carl Perkins, \"Blue Suede Shoes\". Reflecting this success, George Jones released a rockabilly record that year under the pseudonym \"Thumper Jones\", wanting to capitalize on the popularity of rockabilly without alienating his traditional country base. Cash and Presley placed songs in the top 5 in 1958 with No. 3 \"Guess Things Happen That Way/Come In, Stranger\" by Cash, and No. 5 by Presley \"Don't/I Beg of You.\" Presley acknowledged the influence of rhythm and blues artists and his style, saying \"The colored folk been singin' and playin' it just the way I'm doin' it now, man for more years than I know.\" Within a few years, many rockabilly musicians returned to a more mainstream style or had defined their own unique style.\nCountry music gained national television exposure through \"Ozark Jubilee\" on ABC-TV and radio from 1955 to 1960 from Springfield, Missouri. The program showcased top stars including several rockabilly artists, some from the Ozarks. As Webb Pierce put it in 1956, \"Once upon a time, it was almost impossible to sell country music in a place like New York City. Nowadays, television takes us everywhere, and country music records and sheet music sell as well in large cities as anywhere else.\"\nThe Country Music Association was founded in 1958, in part because numerous country musicians were appalled by the increased influence of rock and roll on country music.\nThe Nashville and countrypolitan sounds.\nBeginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee. Under the direction of producers such as Chet Atkins, Bill Porter, Paul Cohen, Owen Bradley, Bob Ferguson, and later Billy Sherrill, the sound brought country music to a diverse audience and helped revive country as it emerged from a commercially fallow period. This subgenre was notable for borrowing from 1950s pop stylings: a prominent and smooth vocal, backed by a string section (violins and other orchestral strings) and vocal chorus. Instrumental soloing was de-emphasized in favor of trademark \"licks\". Leading artists in this genre included Jim Reeves, Skeeter Davis, Connie Smith, the Browns, Patsy Cline, and Eddy Arnold. The \"slip note\" piano style of session musician Floyd Cramer was an important component of this style. The Nashville Sound collapsed in mainstream popularity in 1964, a victim of both the British Invasion and the deaths of Reeves and Cline in separate airplane crashes. By the mid-1960s, the genre had developed into countrypolitan. Countrypolitan was aimed straight at mainstream markets, and it sold well throughout the later 1960s into the early 1970s. Top artists included Tammy Wynette, Lynn Anderson and Charlie Rich, as well as such former \"hard country\" artists as Ray Price and Marty Robbins. Despite the appeal of the Nashville sound, many traditional country artists emerged during this period and dominated the genre: Loretta Lynn, Merle Haggard, Buck Owens, Porter Wagoner, George Jones, and Sonny James among them.\nCountry-soul crossover.\nIn 1962, Ray Charles surprised the pop world by turning his attention to country and western music, topping the charts and rating number three for the year on \"Billboard's\" pop chart with the \"I Can't Stop Loving You\" single, and recording the landmark album \"Modern Sounds in Country and Western Music\".\nBakersfield sound.\nAnother subgenre of country music grew out of hardcore honky tonk with elements of western swing and originated north-northwest of Los Angeles in Bakersfield, California, where many \"Okies\" and other Dust Bowl migrants had settled. Influenced by one-time West Coast residents Bob Wills and Lefty Frizzell, by 1966 it was known as the Bakersfield sound. It relied on electric instruments and amplification, in particular the Telecaster electric guitar, more than other subgenres of the country music of the era, and it can be described as having a sharp, hard, driving, no-frills, edgy flavor\u2014hard guitars and honky-tonk harmonies. Leading practitioners of this style were Buck Owens, Merle Haggard, Tommy Collins, Dwight Yoakam, Gary Allan, and Wynn Stewart, each of whom had his own style.\nKen Nelson, who had produced Owens and Haggard and Rose Maddox became interested in the trucking song subgenre following the success of \"Six Days on the Road\" and asked Red Simpson to record an album of trucking songs. Haggard's \"White Line Fever\" was also part of the trucking subgenre.\nWestern music merges with country.\nThe country music scene of the 1940s until the 1970s was largely dominated by western music influences, so much so that the genre began to be called \"country and western\". Even today, cowboy and frontier values continue to play a role in the larger country music, with western wear, cowboy boots, and cowboy hats continues to be in fashion for country artists.\nWest of the Mississippi River, many of these western genres continue to flourish, including the Red Dirt of Oklahoma, New Mexico music of New Mexico, and both Texas country music and Tejano music of Texas. During the 1950s until the early 1970s, the latter part of the western heyday in country music, many of these genres featured popular artists that continue to influence both their distinctive genres and larger country music. Red Dirt featured Bob Childers and Steve Ripley; for New Mexico music Al Hurricane, Al Hurricane Jr., and Antonia Apodaca; and within the Texas scenes Willie Nelson, Freddie Fender, Johnny Rodriguez, and Little Joe.\nAs Outlaw country music emerged as subgenre in its own right, Red Dirt, New Mexico, Texas country, and Tejano grew in popularity as a part of the Outlaw country movement. Originating in the bars, fiestas, and honky-tonks of Oklahoma, New Mexico, and Texas, their music supplemented outlaw country's singer-songwriter tradition as well as 21st-century rock-inspired alternative country and hip hop-inspired country rap artists.\nFourth generation (1970s\u20131980s).\nOutlaw movement.\nOutlaw country was derived from the traditional western, including Red Dirt, New Mexico, Texas country, Tejano, and honky-tonk musical styles of the late 1950s and 1960s. Songs such as the 1963 Johnny Cash popularized \"Ring of Fire\" show clear influences from the likes of Al Hurricane and Little Joe, this influence just happened to culminate with artists such as Ray Price (whose band, the \"Cherokee Cowboys\", included Willie Nelson and Roger Miller) and mixed with the anger of an alienated subculture of the nation during the period, a collection of musicians that came to be known as the outlaw movement revolutionized the genre of country music in the early 1970s. \"After I left Nashville (the early 70s), I wanted to relax and play the music that I wanted to play, and just stay around Texas, maybe Oklahoma. Waylon and I had that outlaw image going, and when it caught on at colleges and we started selling records, we were O.K. The whole outlaw thing, it had nothing to do with the music, it was something that got written in an article, and the young people said, 'Well, that's pretty cool.' And started listening.\" (Willie Nelson) The term \"outlaw country\" is traditionally associated with Willie Nelson, Jerry Jeff Walker, Hank Williams, Jr., Merle Haggard, Waylon Jennings and Joe Ely. It was encapsulated in the 1976 album \"Wanted! The Outlaws\".\nThough the outlaw movement as a cultural fad had died down after the late 1970s (with Jennings noting in 1978 that it had gotten out of hand and led to real-life legal scrutiny), many western and outlaw country music artists maintained their popularity during the 1980s by forming supergroups, such as The Highwaymen, Texas Tornados, and Bandido.\nCountry pop.\nCountry pop or soft pop, with roots in the countrypolitan sound, folk music, and soft rock, is a subgenre that first emerged in the 1970s. Although the term first referred to country music songs and artists that crossed over to top 40 radio, country pop acts are now more likely to cross over to adult contemporary music. It started with pop music singers like Glen Campbell, Bobbie Gentry, John Denver, Olivia Newton-John, Anne Murray, B. J. Thomas, the Bellamy Brothers, and Linda Ronstadt having hits on the country charts. Between 1972 and 1975, singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles (\"Rocky Mountain High\", \"Sunshine on My Shoulders\", \"Annie's Song\", \"Thank God I'm a Country Boy\", and \"I'm Sorry\"), and was named Country Music Entertainer of the Year in 1975. The year before, Olivia Newton-John, an Australian pop singer, won the \"Best Female Country Vocal Performance\" as well as the Country Music Association's most coveted award for females, \"Female Vocalist of the Year\". In response George Jones, Tammy Wynette, Jean Shepard and other traditional Nashville country artists dissatisfied with the new trend formed the short-lived \"Association of Country Entertainers\" in 1974; the ACE soon unraveled in the wake of Jones and Wynette's bitter divorce and Shepard's realization that most others in the industry lacked her passion for the movement.\nDuring the mid-1970s, Dolly Parton, a successful mainstream country artist since the late 1960s, mounted a high-profile campaign to cross over to pop music, culminating in her 1977 hit \"Here You Come Again\", which topped the U.S. country singles chart, and also reached No. 3 on the pop singles charts. Parton's male counterpart, Kenny Rogers, came from the opposite direction, aiming his music at the country charts, after a successful career in pop, rock and folk music with the First Edition, achieving success the same year with \"Lucille\", which topped the country charts and reached No. 5 on the U.S. pop singles charts, as well as reaching Number 1 on the British all-genre chart. Parton and Rogers would both continue to have success on both country and pop charts simultaneously, well into the 1980s. Country music propelled Kenny Rogers\u2019 career, making him a three-time Grammy Award winner and six-time Country Music Association Awards winner. Having sold more than 50 million albums in the US, one of his Song \"The Gambler,\" inspired several TV films, with Rogers as the main character. Artists like Crystal Gayle, Ronnie Milsap and Barbara Mandrell would also find success on the pop charts with their records. In 1975, author Paul Hemphill stated in the \"Saturday Evening Post\", \"Country music isn't really country anymore; it is a hybrid of nearly every form of popular music in America.\"\nDuring the early 1980s, country artists continued to see their records perform well on the pop charts. Willie Nelson and Juice Newton each had two songs in the top 5 of the Billboard Hot 100 in the early eighties: Nelson charted \"Always on My Mind\" (#5, 1982) and \"To All the Girls I've Loved Before\" (#5, 1984, a duet with Julio Iglesias), and Newton achieved success with \"Queen of Hearts\" (#2, 1981) and \"Angel of the Morning\" (#4, 1981). Four country songs topped the \"Billboard\" Hot 100 in the 1980s: \"Lady\" by Kenny Rogers, from the late fall of 1980; \"9 to 5\" by Dolly Parton, \"I Love a Rainy Night\" by Eddie Rabbitt (these two back-to-back at the top in early 1981); and \"Islands in the Stream\", a duet by Dolly Parton and Kenny Rogers in 1983, a pop-country crossover hit written by Barry, Robin, and Maurice Gibb of the Bee Gees. Newton's \"Queen of Hearts\" almost reached No. 1, but was kept out of the spot by the pop ballad juggernaut \"Endless Love\" by Diana Ross and Lionel Richie. The move of country music toward neotraditional styles led to a marked decline in country/pop crossovers in the late 1980s, and only one song in that period\u2014Roy Orbison's \"You Got It\", from 1989\u2014made the top 10 of both the \"Billboard\" Hot Country Singles\" and Hot 100 charts, due largely to a revival of interest in Orbison after his sudden death. The only song with substantial country airplay to reach number one on the pop charts in the late 1980s was \"At This Moment\" by Billy Vera and the Beaters, an R&amp;B song with slide guitar embellishment that appeared at number 42 on the country charts from minor crossover airplay. The record-setting, multi-platinum group Alabama was named Artist of the Decade for the 1980s by the Academy of Country Music.\nCountry rock.\nCountry rock is a genre that started in the 1960s but became prominent in the 1970s. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the \"old values\" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock. Early innovators in this new style of music in the 1960s and 1970s included Bob Dylan, who was the first to revert to country music with his 1967 album \"John Wesley Harding\" (and even more so with that album's follow-up, \"Nashville Skyline\"), followed by Gene Clark, Clark's former band the Byrds (with Gram Parsons on \"Sweetheart of the Rodeo\") and its spin-off the Flying Burrito Brothers (also featuring Gram Parsons), guitarist Clarence White, Michael Nesmith (the Monkees and the First National Band), the Grateful Dead, Neil Young, Commander Cody, the Allman Brothers Band, Charlie Daniels, the Marshall Tucker Band, Poco, Buffalo Springfield, Stephen Stills' band Manassas and Eagles, among many, even the former folk music duo Ian &amp; Sylvia, who formed Great Speckled Bird in 1969. The Eagles would become the most successful of these country rock acts, and their compilation album \"Their Greatest Hits (1971\u20131975)\" remains the second-best-selling album in the US with 29\u00a0million copies sold. The Rolling Stones also got into the act with songs like \"Dead Flowers\"; the original recording of \"Honky Tonk Women\" was performed in a country style, but it was subsequently re-recorded in a hard rock style for the single version, and the band's preferred country version was later released on the album \"Let It Bleed\", under the title \"Country Honk\".\nDescribed by AllMusic as the \"father of country-rock\", Gram Parsons' work in the early 1970s was acclaimed for its purity and for his appreciation for aspects of traditional country music. Though his career was cut tragically short by his 1973 death, his legacy was carried on by his prot\u00e9g\u00e9 and duet partner Emmylou Harris; Harris would release her debut solo in 1975, an amalgamation of country, rock and roll, folk, blues and pop. Subsequent to the initial blending of the two polar opposite genres, other offspring soon resulted, including Southern rock, heartland rock and in more recent years, alternative country. In the decades that followed, artists such as Juice Newton, Alabama, Hank Williams, Jr. (and, to an even greater extent, Hank Williams III), Gary Allan, Shania Twain, Brooks &amp; Dunn, Faith Hill, Garth Brooks, Dwight Yoakam, Steve Earle, Dolly Parton, Rosanne Cash and Linda Ronstadt moved country further towards rock influence.\nNeocountry.\nIn 1980, a style of \"neocountry disco music\" was popularized by the film \"Urban Cowboy\". It was during this time that a glut of pop-country crossover artists began appearing on the country charts: former pop stars Bill Medley (of the Righteous Brothers), \"England Dan\" Seals (of England Dan and John Ford Coley), Tom Jones, and Merrill Osmond (both alone and with some of his brothers; his younger sister Marie Osmond was already an established country star) all recorded significant country hits in the early 1980s. Sales in record stores rocketed to $250\u00a0million in 1981; by 1984, 900 radio stations began programming country or neocountry pop full-time. As with most sudden trends, however, by 1984 sales had dropped below 1979 figures.\nTruck-driving country.\nTruck-driving country music is a genre of country music\nand is a fusion of honky-tonk, country rock and the Bakersfield sound.\nIt has the tempo of country rock and the emotion of honky-tonk, and its lyrics focus on a truck driver's lifestyle. Truck-driving country songs often deal with the profession of trucking and love. Well-known artists who sing truck driving country include Dave Dudley, Red Sovine, Dick Curless, Red Simpson, Del Reeves, the Willis Brothers and Jerry Reed, with C. W. McCall and Cledus Maggard (pseudonyms of Bill Fries and Jay Huguely, respectively) being more humorous entries in the subgenre. Dudley is known as the father of truck driving country.\nNeotraditionalist movement.\nDuring the mid-1980s, a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts, in favor of more, traditional, \"back-to-basics\" production. Many of the artists during the latter half of the 1980s drew on traditional honky-tonk, bluegrass, folk and western swing. Artists who typified this sound included Travis Tritt, Reba McEntire, George Strait, Keith Whitley, Alan Jackson, John Anderson, Patty Loveless, Kathy Mattea, Randy Travis, Dwight Yoakam, Clint Black, Ricky Skaggs, and the Judds.\nFifth generation (1990s).\nCountry music was aided by the U.S. Federal Communications Commission's (FCC) Docket 80\u201390, which led to a significant expansion of FM radio in the 1980s by adding numerous higher-fidelity FM signals to rural and suburban areas. At this point, country music was mainly heard on rural AM radio stations; the expansion of FM was particularly helpful to country music, which migrated to FM from the AM band as AM became overcome by talk radio (the country music stations that stayed on AM developed the classic country format for the AM audience). At the same time, beautiful music stations already in rural areas began abandoning the format (leading to its effective demise) to adopt country music as well. This wider availability of country music led to producers seeking to polish their product for a wider audience. In 1990, \"Billboard\", which had published a country music chart since the 1940s, changed the methodology it used to compile the chart: singles sales were removed from the methodology, and only airplay on country radio determined a song's place on the chart.\nIn the 1990s, country music became a worldwide phenomenon thanks to Garth Brooks, who enjoyed one of the most successful careers in popular music history, breaking records for both sales and concert attendance throughout the decade. The RIAA has certified his recordings at a combined (128\u00d7 platinum), denoting roughly 113\u00a0million U.S. shipments. Other artists who experienced success during this time included Clint Black, John Michael Montgomery, Tracy Lawrence, Tim McGraw, Kenny Chesney, Travis Tritt, Alan Jackson and the newly formed duo of Brooks &amp; Dunn; George Strait, whose career began in the 1980s, also continued to have widespread success in this decade and beyond. Toby Keith began his career as a more pop-oriented country singer in the 1990s, evolving into an outlaw persona in the early 2000s with \"Pull My Chain\" and its follow-up, \"Unleashed\".\nSuccess of female artists.\nFemale artists such as Reba McEntire, Patty Loveless, Faith Hill, Martina McBride, Deana Carter, LeAnn Rimes, Mindy McCready, Pam Tillis, Lorrie Morgan, Shania Twain, and Mary Chapin Carpenter all released platinum-selling albums in the 1990s. The Dixie Chicks became one of the most popular country bands in the 1990s and early 2000s. Their 1998 debut album \"Wide Open Spaces\" went on to become certified 12\u00d7 platinum while their 1999 album \"Fly\" went on to become 10\u00d7 platinum. After their third album, \"Home\", was released in 2003, the band made political news in part because of lead singer Natalie Maines's comments disparaging then-President George W. Bush while the band was overseas (Maines stated that she and her bandmates were ashamed to be from the same state as Bush, who had just commenced the Iraq War a few days prior). The comments caused a rift between the band and the country music scene, and the band's fourth (and most recent) album, 2006's \"Taking the Long Way\", took a more rock-oriented direction; the album was commercially successful overall among non-country audiences but largely ignored among country audiences. After \"Taking the Long Way\", the band broke up for a decade (with two of its members continuing as the Court Yard Hounds) before reuniting in 2016 and releasing new material in 2020.\nCanadian artist Shania Twain became the best selling female country artist of the decade. This was primarily due to the success of her breakthrough sophomore 1995 album, \"The Woman in Me\", which was certified 12\u00d7 platinum sold over 20\u00a0million copies worldwide and its follow-up, 1997's \"Come On Over\", which was certified 20\u00d7 platinum and sold over 40\u00a0million copies. The album became a major worldwide phenomenon and became one of the world's best selling albums for three years (1998, 1999 and 2000); it also went on to become the best selling country album of all time.\nUnlike the majority of her contemporaries, Twain enjoyed large international success that had been seen by very few country artists, before or after her. Critics have noted that Twain enjoyed much of her success due to breaking free of traditional country stereotypes and for incorporating elements of rock and pop into her music. In 2002, she released her successful fourth studio album, titled \"Up!\", which was certified 11\u00d7 platinum and sold over 15\u00a0million copies worldwide. Shania Twain has been nominated eighteen times for Grammy Awards and won five Grammys. [] She was the best-paid country music star in 2016 according to Forbes, with a net worth of $27.5 million. []Twain has been credited with breaking international boundaries for country music, as well as inspiring many country artists to incorporate different genres into their music in order to attract a wider audience. She is also credited with changing the way in which many female country performers would market themselves, as unlike many before her she used fashion and her sex appeal to get rid of the stereotypical 'honky-tonk' image the majority of country singers had in order to distinguish herself from many female country artists of the time.\nLine dancing revival.\nIn the early-mid-1990s, country western music was influenced by the popularity of line dancing. This influence was so great that Chet Atkins was quoted as saying, \"The music has gotten pretty bad, I think. It's all that damn line dancing.\" By the end of the decade, however, at least one line dance choreographer complained that good country line dance music was no longer being released. In contrast, artists such as Don Williams and George Jones who had more or less had consistent chart success through the 1970s and 1980s suddenly had their fortunes fall rapidly around 1991 when the new chart rules took effect.\nAlternative country.\nCountry influences combined with Punk rock and alternative rock to forge the \"cowpunk\" scene in Southern California during the 1980s, which included bands such as the Long Ryders, Lone Justice and the Beat Farmers, as well as the established punk group X, whose music had begun to include country and rockabilly influences. Simultaneously, a generation of diverse country artists outside of California emerged that rejected the perceived cultural and musical conservatism associated with Nashville's mainstream country musicians in favor of more countercultural outlaw country and the folk singer-songwriter traditions of artists such as Woody Guthrie, Gram Parsons and Bob Dylan.\nArtists from outside California who were associated with early alternative country included singer-songwriters such as Lucinda Williams, Lyle Lovett and Steve Earle, the Nashville country rock band Jason and the Scorchers, the Providence \"cowboy pop\" band Rubber Rodeo, and the British post-punk band the Mekons. Earle, in particular, was noted for his popularity with both country and college rock audiences: He promoted his 1986 debut album \"Guitar Town\" with a tour that saw him open for both country singer Dwight Yoakam and alternative rock band the Replacements. Yoakam also cultivated a fanbase spanning multiple genres through his stripped-down honky-tonk influenced sound, association with the cowpunk scene, and performances at Los Angeles punk rock clubs.\nThese early styles had coalesced into a genre by the time the Illinois group Uncle Tupelo released their influential debut album \"No Depression\" in 1990. The album is widely credited as being the first \"alternative country\" album, and inspired the name of \"No Depression\" magazine, which exclusively covered the new genre. Following Uncle Tupelo's disbanding in 1994, its members formed two significant bands in the genre: Wilco and Son Volt. Although Wilco's sound had moved away from country and towards indie rock by the time they released their critically acclaimed album \"Yankee Hotel Foxtrot\" in 2002, they have continued to be an influence on later alt-country artists.\nOther acts who became prominent in the alt-country genre during the 1990s and 2000s included the Bottle Rockets, the Handsome Family, Blue Mountain, Robbie Fulks, Blood Oranges, Bright Eyes, Drive-By Truckers, Old 97's, Old Crow Medicine Show, Nickel Creek, Neko Case, and Whiskeytown, whose lead singer Ryan Adams later had a successful solo-career. Alt-country, in various iterations overlapped with other genres, including Red Dirt country music (Cross Canadian Ragweed), jam bands (My Morning Jacket and the String Cheese Incident), and indie folk (the Avett Brothers).\nDespite the genre's growing popularity in the 1980s, 1990s and 2000s, alternative country and neo-traditionalist artists saw minimal support from country radio in those decades, despite strong sales and critical acclaim for albums such as the soundtrack to the 2000 film \"O Brother, Where Art Thou?\". In 1987, the Beat Farmers gained airplay on country music stations with their song \"Make It Last\", but the single was pulled from the format when station programmers decreed the band's music was too rock-oriented for their audience. However, some alt-country songs have been crossover hits to mainstream country radio in cover versions by established artists on the format; Lucinda Williams' \"Passionate Kisses\" was a hit for Mary Chapin Carpenter in 1993, Ryan Adams' \"When the Stars Go Blue\" was a hit for Tim McGraw in 2007, and Old Crow Medicine Show's \"Wagon Wheel\" was a hit for Darius Rucker (member of Hootie &amp; The Blowfish) in 2013.\nIn the 2010s, the alt-country genre saw an increase in its critical and commercial popularity, owing to the success of artists such as the Civil Wars, Chris Stapleton, Sturgill Simpson, Jason Isbell, Lydia Loveless and Margo Price. In 2019, Kacey Musgraves \u2013 a country artist who had gained a following with indie rock fans and music critics despite minimal airplay on country radio \u2013 won the Grammy Award for Album of the Year for her album \"Golden Hour\".\nSixth generation (2000s\u2013present).\nThe sixth generation of country music continued to be influenced by other genres such as pop, rock, and R&amp;B. Richard Marx crossed over with his \"Days in Avalon\" album, which features five country songs and several singers and musicians. Alison Krauss sang background vocals to Marx's single \"Straight from My Heart.\" Also, Bon Jovi had a hit single, \"Who Says You Can't Go Home\", with Jennifer Nettles of Sugarland. Kid Rock's collaboration with Sheryl Crow, \"Picture,\" was a major crossover hit in 2001 and began Kid Rock's transition from hard rock to a country-rock hybrid that would later produce another major crossover hit, 2008's \"All Summer Long.\" (Crow, whose music had often incorporated country elements, would also officially cross over into country with her hit \"Easy\" from her debut country album \"Feels like Home\"). Darius Rucker, frontman for the 1990s pop-rock band Hootie &amp; the Blowfish, began a country solo career in the late 2000s, one that to date has produced five albums and several hits on both the country charts and the Billboard Hot 100. Singer-songwriter Unknown Hinson became famous for his appearance in the Charlotte television show \"Wild, Wild, South\", after which Hinson started his own band and toured in southern states. Other rock stars who featured a country song on their albums were Don Henley (who released \"Cass County\" in 2015, an album which featured collaborations with numerous country artists) and Poison.\nThe back half of the 2010\u20132020 decade saw an increasing number of mainstream country acts collaborate with pop and R&amp;B acts; many of these songs achieved commercial success by appealing to fans across multiple genres; examples include collaborations between Kane Brown and Marshmello and Maren Morris and Zedd. There has also been interest from pop singers in country music, including Beyonc\u00e9, Lady Gaga, Alicia Keys, Gwen Stefani, Justin Timberlake, Justin Bieber and Pink. Supporting this movement is the new generation of contemporary pop-country, including Taylor Swift, Miranda Lambert, Carrie Underwood, Kacey Musgraves, Miley Cyrus, Billy Ray Cyrus, Sam Hunt, Chris Young, who introduced new themes in their works, touching on fundamental rights, feminism, and controversies about racism and religion of the older generations.\nPopular culture.\nIn 2005, country singer Carrie Underwood rose to fame as the winner of the fourth season of \"American Idol;\" she has since become one of the most prominent recording artists in the genre, with worldwide sales of more than 65\u00a0million records and seven Grammy Awards. With her first single, \"Inside Your Heaven\", Underwood became the only solo country artist to have a number 1 hit on the \"Billboard\" Hot 100 chart in the 2000\u20132009 decade and also broke \"Billboard\" chart history as the first country music artist ever to debut at No. 1 on the Hot 100. Underwood's debut album, \"Some Hearts\", became the best-selling solo female debut album in country music history, the fastest-selling debut country album in the history of the SoundScan era and the best-selling country album of the last 10 years, being ranked by \"Billboard\" as the number 1 Country Album of the 2000\u20132009 decade. She has also become the female country artist with the most number one hits on the \"Billboard\" Hot Country Songs chart in the Nielsen SoundScan era (1991\u2013present), having 14 #1s and breaking her own \"Guinness Book\" record of ten. In 2007, Underwood won the Grammy Award for Best New Artist, becoming only the second Country artist in history (and the first in a decade) to win it. She also made history by becoming the seventh woman to win Entertainer of the Year at the Academy of Country Music Awards, and the first woman in history to win the award twice, as well as twice consecutively. \"Time\" has listed Underwood as one of the 100 most influential people in the world.\nIn 2016, Underwood topped the Country Airplay chart for the 15th time, becoming the female artist with the most number ones on that chart.\nCarrie Underwood was only one of several country stars produced by a television series in the 2000s. In addition to Underwood, \"American Idol\" launched the careers of Kellie Pickler, Josh Gracin, Bucky Covington, Kristy Lee Cook, Danny Gokey, Lauren Alaina and Scotty McCreery (as well as that of occasional country singer Kelly Clarkson) in the decade, and would continue to launch country careers in the 2010s. The series \"Nashville Star\", while not nearly as successful as \"Idol\", did manage to bring Miranda Lambert, Kacey Musgraves and Chris Young to mainstream success, also launching the careers of lower-profile musicians such as Buddy Jewell, Sean Patrick McGraw, and Canadian musician George Canyon. \"Can You Duet?\" produced the duos Steel Magnolia and Joey + Rory. Teen sitcoms also have influenced modern country music; in 2008, actress Jennette McCurdy (best known as the sidekick Sam on the teen sitcom \"iCarly\") released her first single, \"So Close\", following that with the single \"Generation Love\" in 2011. Another teen sitcom star, Miley Cyrus (of Disney Channel's \"Hannah Montana\"), also had a crossover hit in the late 2000s with \"The Climb\" and another with a duet with her father, Billy Ray Cyrus, with \"Ready, Set, Don't Go.\" Jana Kramer, an actress in the teen drama \"One Tree Hill\", released a country album in 2012 that has produced two hit singles as of 2013. Actresses Hayden Panettiere and Connie Britton began recording country songs as part of their roles in the TV shows \"Nashville\" and \"Pretty Little Liars\" star Lucy Hale released her debut album \"Road Between\" in 2014.\nIn 2010, the group Lady Antebellum won five Grammys, including the coveted Song of the Year and Record of the Year for \"Need You Now\". A large number of duos and vocal groups emerged on the charts in the 2010s, many of which feature close harmony in the lead vocals. In addition to Lady A, groups such as Little Big Town, the Band Perry, Gloriana, Thompson Square, Eli Young Band, Zac Brown Band and British duo the Shires have emerged to occupy a large share of mainstream success alongside solo singers such as Kacey Musgraves and Miranda Lambert.\nOne of the most commercially successful country artists of the late 2000s and early 2010s has been singer-songwriter Taylor Swift. Swift first became widely known in 2006 when her debut single, \"Tim McGraw\", was released when Swift was only 16 years old. In 2006, Swift released her self-titled debut studio album, which spent 275 weeks on \"Billboard\" 200, one of the longest runs of any album on that chart. In 2008, Taylor Swift released her second studio album, \"Fearless\", which made her the second longest number-one charted on \"Billboard\" 200 and the second best-selling album (just behind Adele's \"21\") within the past 5 years. At the 2010 Grammys, Taylor Swift was 20 and won Album of the Year for \"Fearless\", which made her the youngest artist to win this award. Swift has received fourteen Grammys already.\nBuoyed by her teen idol status among girls and a change in the methodology of compiling the \"Billboard\" charts to favor pop-crossover songs, Swift's 2012 single \"We Are Never Ever Getting Back Together\" spent the most weeks at the top of Billboard's Hot 100 chart and Hot Country Songs chart of any song in nearly five decades. The song's long run at the top of the chart was somewhat controversial, as the song is largely a pop song without much country influence and its success on the charts was driven by a change to the chart's criteria to include airplay on non-country radio stations, prompting disputes over what constitutes a country song; many of Swift's later releases, such as album \"1989\" (2014), \"Reputation\" (2017), and \"Lover\" (2019) were released solely to pop audiences. Swift returned to country music in her recent folk-inspired releases, \"Folklore\" (2020) and \"Evermore\" (2020), with songs like \"Betty\" and \"No Body, No Crime\".\nModern variations.\nInfluence of rock, pop and hip-hop.\nIn the mid to late 2010s, country music began to increasingly sound more like the style of modern-day Pop music, with more simple and repetitive lyrics, more electronic-based instrumentation, and experimentation with \"talk-singing\" and rap, pop-country pulled farther away from the traditional sounds of country music and received criticisms from country music purists while gaining in popularity with mainstream audiences. The topics addressed have also changed, turning controversial such as acceptance of the LGBT community, safe sex, recreational marijuana use, and questioning religious sentiment. Influences also come from some pop artists' interest in the country genre, including Justin Timberlake with the album \"Man of the Woods,\" Beyonc\u00e9's song \"Daddy Lessons\" from \"Lemonade\", Kelly Clarkson, Gwen Stefani with \"Nobody but You\", Bruno Mars, Lady Gaga, Alicia Keys, and Pink.\nThe influence of rock music in country has become more overt during the late 2000s and early 2010s as artists like Eric Church, Jason Aldean, and Brantley Gilbert have had success; Aaron Lewis, former frontman for the rock group Staind, had a moderately successful entry into country music in 2011 and 2012, as did Dallas Smith, former frontman of the band Default.\nMaren Morris success collaboration \"The Middle\" with EDM producer Zedd is considered one of the representations of the fusion of electro-pop with country music.\nLil Nas X song \"Old Town Road\" spent 19 weeks atop the US \"Billboard\" Hot 100 chart, becoming the longest-running number-one song since the chart debuted in 1958, winning Billboard Music Awards, MTV Video Music Awards and Grammy Award. Sam Hunt \"Leave the Night On\" peaked concurrently on the Hot Country Songs and Country Airplay charts, making Hunt the first country artist in 22 years, since Billy Ray Cyrus, to reach the top of three country charts simultaneously in the Nielsen SoundScan-era. With the fusion genre of \"country trap\"\u2014a fusion of country/western themes to a hip hop beat, but usually with fully sung lyrics\u2014emerging in the late 2010s, line dancing country had a minor revival, examples of the phenomenon include \"The Git Up\" by Blanco Brown. Blanco Brown has gone on to make more traditional country soul songs such as \"I Need Love\" and a rendition of \"Don't Take the Girl\" with Tim McGraw, and collaborations like \"Just the Way\" with Parmalee. Another country trap artist known as Breland has seen success with \"My Truck, \"Throw It Back\" with Keith Urban, and \"Praise the Lord\" featuring Thomas Rhett.\nEmo rap musician Sueco, released a cowpunk song in collaboration is country musician Warren Zeiders titled \"Ride It Hard\".\nBro country.\nIn the early 2010s, \"bro-country\", a genre noted primarily for its themes on drinking and partying, girls, and pickup trucks became particularly popular. Notable artists associated with this genre are Luke Bryan, Jason Aldean, Blake Shelton, Jake Owen and Florida Georgia Line whose song \"Cruise\" became the best-selling country song of all time. Research in the mid-2010s suggested that about 45 percent of country's best-selling songs could be considered bro-country, with the top two artists being Luke Bryan and Florida Georgia Line. Albums by bro-country singers also sold very well\u2014in 2013, Luke Bryan's \"Crash My Party\" was the third best-selling of all albums in the United States, with Florida Georgia Line's \"Here's to the Good Times\" at sixth, and Blake Shelton's \"Based on a True Story\" at ninth. It is also thought that the popularity of bro-country helped country music to surpass classic rock as the most popular genre in the American country in 2012. The genre however is controversial as it has been criticized by other country musicians and commentators over its themes and depiction of women, opening up a divide between the older generation of country singers and the younger bro country singers that was described as \"civil war\" by musicians, critics, and journalists.\" In 2014, Maddie &amp; Tae's \"Girl in a Country Song\", addressing many of the controversial bro-country themes, peaked at number one on the \"Billboard\" Country Airplay chart.\nBluegrass and Americana.\nBluegrass is a genre that contain songs about going through hard times, country loving, and telling stories. Its history can be traced back to the 1600s. During this time, many people were coming to America from Ireland, Scotland and England. Those people brought the first version of Bluegrass to the Americas. After several years of bluegrass' development, Bill Monroe became the \"father\" of bluegrass. Other sources argue that The Monroe Brothers were the first stars of bluegrass. Newer artists like Billy Strings, the Grascals, Molly Tuttle, Tyler Childers and the Infamous Stringdusters have been increasing the popularity of this genre, alongside some of the genres more established stars who still remain popular including Rhonda Vincent, Alison Krauss and Union Station, Ricky Skaggs and Del McCoury. The genre has developed in the Northern Kentucky and Cincinnati area. Other artists include New South (band), Doc Watson, Osborne Brothers, and many others.\nIn an effort to combat the over-reliance of mainstream country music on pop-infused artists, the sister genre of Americana began to gain popularity and increase in prominence, receiving eight Grammy categories of its own in 2009. Though Americana music gained popularity in 2009, the first Americana singer was likely Hank Williams in the 1950s. Americana music incorporates elements of country music, bluegrass, folk, blues, gospel, rhythm and blues, roots rock and southern soul and is overseen by the Americana Music Association and the Americana Music Honors &amp; Awards. As a result of an increasingly pop-leaning mainstream, many more traditional-sounding artists such as Tyler Childers, Zach Bryan and Old Crow Medicine Show began to associate themselves more with Americana and the alternative country scene where their sound was more celebrated. Similarly, many established country acts who no longer received commercial airplay, including Emmylou Harris and Lyle Lovett, began to flourish again.\nContemporary country and western revival.\nDuring the mid-1980s, a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts, in favor of more, traditional, \"back-to-basics\" production. Many of the artists during the latter half of the 1980s drew on traditional honky-tonk, bluegrass, folk and western swing. Artists who typified this sound included Travis Tritt, Reba McEntire, George Strait, Keith Whitley, Alan Jackson, John Anderson, Patty Loveless, Kathy Mattea, Randy Travis, Dwight Yoakam, Clint Black, Ricky Skaggs, and the Judds.\nBeginning in 1989, a confluence of events brought an unprecedented commercial boom to country music. New marketing strategies were used to engage fans, powered by technology that more accurately tracked the popularity of country music, and boosted by a political and economic climate that focused attention on the genre. Garth Brooks (\"Friends in Low Places\") in particular attracted fans with his fusion of neotraditionalist country and stadium rock. Other artists such as Brooks and Dunn (\"Boot Scootin' Boogie\") also combined conventional country with slick, rock elements, while Lorrie Morgan, Mary Chapin Carpenter, and Kathy Mattea updated neotraditionalist styles.\nRoots of conservative country was Lee Greenwood's \"God Bless the USA\". The September 11 attacks of 2001 and the economic recession helped move country music back into the spotlight. Many country artists, such as Alan Jackson with his ballad on terrorist attacks, \"Where Were You (When the World Stopped Turning)\", wrote songs that celebrated the military, highlighted the gospel, and emphasized home and family values over wealth. Alt-Country singer Ryan Adams song \"New York, New York\" pays tribute to New York City, and its popular music video (which was shot 4 days before the attacks) shows Adams playing in front of the Manhattan skyline, Along with several shots of the city. In contrast, more rock-oriented country singers took more direct aim at the attacks' perpetrators; Toby Keith's \"Courtesy of the Red, White and Blue (The Angry American)\" threatened to \"a boot in\" the posterior of the enemy, while Charlie Daniels's \"This Ain't No Rag, It's a Flag\" promised to \"hunt\" the perpetrators \"down like a mad dog hound.\" These songs gained such recognition that it put country music back into popular culture. Darryl Worley recorded \"Have You Forgotten\" also. There have been numerous patriotic country songs throughout the years.\nSome modern artists that primarily or entirely produce country pop music include Kacey Musgraves, Maren Morris, Kelsea Ballerini, Sam Hunt, Kane Brown, Chris Lane, and Dan + Shay. The singers who are part of this country movement are also defined as \"Nashville's new generation of country\".\nAlthough the changes made by the new generation, it has been recognized by major music awards associations and successes in Billboard and international charts. \"Golden Hour\" by Kacey Musgraves won album of the year at 61st Annual Grammy Awards, Academy of Country Music Awards, Country Music Association Awards, although it has received criticism from some traditional country music fans.\nInternational.\nAustralia.\nAustralian country music has a long tradition. Influenced by US country music, it has developed a distinct style, shaped by British and Irish folk ballads and Australian bush balladeers like Henry Lawson and Banjo Paterson. Country instruments, including the guitar, banjo, fiddle and harmonica, create the distinctive sound of country music in Australia and accompany songs with strong storyline and memorable chorus.\nFolk songs sung in Australia between the 1780s and 1920s, based around such themes as the struggle against government tyranny, or the lives of bushrangers, swagmen, drovers, stockmen and shearers, continue to influence the genre. This strain of Australian country, with lyrics focusing on Australian subjects, is generally known as \"bush music\" or \"bush band music\". \"Waltzing Matilda\", often regarded as Australia's unofficial national anthem, is a quintessential Australian country song, influenced more by British and Irish folk ballads than by US country and western music. The lyrics were composed by the poet Banjo Paterson in 1895. Other popular songs from this tradition include \"The Wild Colonial Boy\", \"Click Go the Shears\", \"The Queensland Drover\" and \"The Dying Stockman\". Later themes which endure to the present include the experiences of war, of droughts and flooding rains, of Aboriginality and of the railways and trucking routes which link Australia's vast distances.\nPioneers of a more Americanised popular country music in Australia included Tex Morton (known as \"The Father of Australian Country Music\") in the 1930s. Author Andrew Smith delivers a through research and engaged view of Tex Morton's life and his impact on the country music scene in Australia in the 1930s and 1940s. Other early stars included Buddy Williams, Shirley Thoms and Smoky Dawson. Buddy Williams (1918\u20131986) was the first Australian-born to record country music in Australia in the late 1930s and was the pioneer of a distinctly Australian style of country music called the bush ballad that others such as Slim Dusty would make popular in later years. During the Second World War, many of Buddy Williams recording sessions were done whilst on leave from the Army. At the end of the war, Williams would go on to operate some of the largest travelling tent rodeo shows Australia has ever seen.\nIn 1952, Dawson began a radio show and went on to national stardom as a singing cowboy of radio, TV and film. Slim Dusty (1927\u20132003) was known as the \"King of Australian Country Music\" and helped to popularise the Australian bush ballad. His successful career spanned almost six decades, and his 1957 hit \"A Pub with No Beer\" was the biggest-selling record by an Australian to that time, and with over seven million record sales in Australia he is the most successful artist in Australian musical history. Dusty recorded and released his one-hundredth album in the year 2000 and was given the honour of singing \"Waltzing Matilda\" in the closing ceremony of the Sydney 2000 Olympic Games. Dusty's wife Joy McKean penned several of his most popular songs.\nChad Morgan, who began recording in the 1950s, has represented a vaudeville style of comic Australian country; Frank Ifield achieved considerable success in the early 1960s, especially in the UK Singles Charts and Reg Lindsay was one of the first Australians to perform at Nashville's Grand Ole Opry in 1974. Eric Bogle's 1972 folk lament to the Gallipoli Campaign \"And the Band Played Waltzing Matilda\" recalled the British and Irish origins of Australian folk-country. Singer-songwriter Paul Kelly, whose music style straddles folk, rock and country, is often described as the poet laureate of Australian music. \nBy the 1990s, country music had attained crossover success in the pop charts, with artists like James Blundell and James Reyne singing \"Way Out West\", and country star Kasey Chambers winning the ARIA Award for Best Female Artist in three years (2000, 2002 and 2004), tying with pop stars Wendy Matthews and Sia for the most wins in that category. Furthermore, Chambers has gone on to win nine ARIA Awards for Best Country Album and, in 2018, became the youngest artist to ever be inducted into the ARIA Hall of Fame. The crossover influence of Australian country is also evident in the music of successful contemporary bands the Waifs and the John Butler Trio. Nick Cave has been heavily influenced by the country artist Johnny Cash. In 2000, Cash, covered Cave's \"The Mercy Seat\" on the album ', seemingly repaying Cave for the compliment he paid by covering Cash's \"The Singer\" (originally \"The Folk Singer\") on his \"Kicking Against the Pricks\" album. Subsequently, Cave cut a duet with Cash on a version of Hank Williams' \"I'm So Lonesome I Could Cry\" for Cash's ' album (2002).\nPopular contemporary performers of Australian country music include John Williamson (who wrote the iconic \"True Blue\"), Lee Kernaghan (whose hits include \"Boys from the Bush\" and \"The Outback Club\"), Gina Jeffreys, Forever Road and Sara Storer. In the U.S., Olivia Newton-John, Sherri\u00e9 Austin and Keith Urban have attained great success. During her time as a country singer in the 1970s, Newton-John became the first (and to date only) non-US winner of the Country Music Association Award for Female Vocalist of the Year which many considered a controversial decision by the CMA; after starring in the rock-and-roll musical film \"Grease\" in 1978, Newton-John (mirroring the character she played in the film) shifted to pop music in the 1980s. Urban is arguably considered the most successful international Australian country star, winning nine CMA Awards, including three Male Vocalist of the Year wins and two wins of the CMA's top honour Entertainer of the Year. Pop star Kylie Minogue found success with her 2018 country pop album \"Golden\" which she recorded in Nashville reaching number one in Scotland, the UK and her native Australia.\nCountry music has been a particularly popular form of musical expression among Indigenous Australians. Troy Cassar-Daley is among Australia's successful contemporary indigenous performers, and Kev Carmody and Archie Roach employ a combination of folk-rock and country music to sing about Aboriginal rights issues.\nThe Tamworth Country Music Festival began in 1973 and now attracts up to 100,000 visitors annually. Held in Tamworth, New South Wales (country music capital of Australia), it celebrates the culture and heritage of Australian country music. During the festival the CMAA holds the Country Music Awards of Australia ceremony awarding the Golden Guitar trophies. Other significant country music festivals include the Whittlesea Country Music Festival (near Melbourne) and the Mildura Country Music Festival for \"independent\" performers during October, and the Canberra Country Music Festival held in the national capital during November.\n\"Country HQ\" showcases new talent on the rise in the country music scene down under. CMC (the Country Music Channel), a 24\u2011hour music channel dedicated to non-stop country music, can be viewed on pay TV and features once a year the Golden Guitar Awards, CMAs and CCMAs alongside international shows such as \"The Wilkinsons\", \"The Road Hammers\", and \"Country Music Across America\".\nCanada.\nOutside of the United States, Canada has the largest country music fan and artist base, something that is to be expected given the two countries' proximity and cultural parallels. Mainstream country music is culturally ingrained in the prairie provinces, the British Columbia Interior, Northern Ontario, and in Atlantic Canada. Celtic traditional music developed in Atlantic Canada in the form of Scottish, Acadian and Irish folk music popular amongst Irish, French and Scottish immigrants to Canada's Atlantic Provinces (Newfoundland, Nova Scotia, New Brunswick, and Prince Edward Island). Like the southern United States and Appalachia, all four regions are of heavy British Isles stock and rural; as such, the development of traditional music in the Maritimes somewhat mirrored the development of country music in the US South and Appalachia. Country and western music never really developed separately in Canada; however, after its introduction to Canada, following the spread of radio, it developed quite quickly out of the Atlantic Canadian traditional scene. While true Atlantic Canadian traditional music is very Celtic or \"sea shanty\" in nature, even today, the lines have often been blurred. Certain areas often are viewed as embracing one strain or the other more openly. For example, in Newfoundland the traditional music remains unique and Irish in nature, whereas traditional musicians in other parts of the region may play both genres interchangeably.\n\"Don Messer's Jubilee\" was a Halifax, Nova Scotia-based country/folk variety television show that was broadcast nationally from 1957 to 1969. In Canada it out-performed \"The Ed Sullivan Show\" broadcast from the United States and became the top-rated television show throughout much of the 1960s. \"Don Messer's Jubilee\" followed a consistent format throughout its years, beginning with a tune named \"Goin' to the Barndance Tonight\", followed by fiddle tunes by Messer, songs from some of his \"Islanders\" including singers Marg Osburne and Charlie Chamberlain, the featured guest performance, and a closing hymn. It ended with \"Till We Meet Again\". The guest performance slot gave national exposure to numerous Canadian folk musicians, including Stompin' Tom Connors and Catherine McKinnon. Some Maritime country performers went on to further fame beyond Canada. Hank Snow, Wilf Carter (also known as Montana Slim), and Anne Murray are the three most notable. The cancellation of the show by the public broadcaster in 1969 caused a nationwide protest, including the raising of questions in the Parliament of Canada.\nThe Prairie provinces, due to their western cowboy and agrarian nature, are the true heartland of Canadian country music. While the Prairies never developed a traditional music culture anything like the Maritimes, the folk music of the Prairies often reflected the cultural origins of the settlers, who were a mix of Scottish, Ukrainian, German and others. For these reasons polkas and western music were always popular in the region, and with the introduction of the radio, mainstream country music flourished. As the culture of the region is western and frontier in nature, the specific genre of country and western is more popular today in the Prairies than in any other part of the country. No other area of the country embraces all aspects of the culture, from two-step dancing, to the cowboy dress, to rodeos, to the music itself, like the Prairies do. The Atlantic Provinces, on the other hand, produce far more traditional musicians, but they are not usually specifically country in nature, usually bordering more on the folk or Celtic genres.\nCanadian country pop star Shania Twain is the best-selling female country artist of all time and one of the best-selling artists of all time in any genre. Furthermore, she is the only woman to have three consecutive albums be certified Diamond.\nMexico and Latin America.\nCountry music artists from the U.S. have seen crossover with Latin American audiences, particularly in Mexico. Country music artists from throughout the U.S. have recorded renditions of Mexican folk songs, including \"El Rey\" which was performed on George Strait's \"Twang\" album and during Al Hurricane's tribute concert. American Latin pop crossover musicians, like Lorenzo Antonio's \"Ranchera Jam\" have also combined Mexican songs with country songs in a New Mexico music style.\nWhile Tejano and New Mexico music is typically thought of as being Spanish language, the genres have also had charting musicians focused on English language music. During the 1970s, singer-songwriter Freddy Fender had two #1 country music singles, that were popular throughout North America, with \"Before the Next Teardrop Falls\" and \"Wasted Days and Wasted Nights\". Notable songs which have been influenced by Hispanic and Latin culture as performed by US country music artists include Marty Robbins' \"El Paso\" trilogy, Willie Nelson and Merle Haggard covering the Townes Van Zandt song \"Pancho and Lefty\", \"Toes\" by Zac Brown Band, and \"Sangria\" by Blake Shelton.\nRegional Mexican is a radio format featuring many of Mexico's versions of country music. It includes a number of different styles, usually named after their region of origin. One specific song style, the Canci\u00f3n Ranchera, or simply Ranchera, literally meaning \"ranch song\", found its origins in the Mexican countryside and was first popularized with Mariachi. It has since also become popular with Grupero, Banda, Norte\u00f1o, Tierra Caliente, Duranguense and other regional Mexican styles. The Corrido, a different song style with a similar history, is also performed in many other regional styles, and is most related to the western style of the United States and Canada. Other song styles performed in regional Mexican music include Ballads, Cumbias, Boleros, among others. Country en Espa\u00f1ol (Country in Spanish) is also popular in Mexico. Some Mexican artists began performing country songs in Spanish during the 1970s, and the genre became prominent mainly in the northern regions of the country during the 1980s. A Country en Espa\u00f1ol popularity boom also reached the central regions of Mexico during the 1990s. For most of its history, Country en Espa\u00f1ol mainly resembled Neotraditional country. However, in more modern times, some artists have incorporated influences from other country music subgenres.\nIn Argentina, on the last weekend of September, the yearly San Pedro Country Music Festival takes place in the town of San Pedro, Buenos Aires. The festival features bands from different places in Argentina, as well as international artists from Brazil, Uruguay, Chile, Peru and the U.S.\nUnited Kingdom.\nCountry music is popular in the United Kingdom, although somewhat less so than in other English-speaking countries. There are some British country music acts and publications. Although radio stations devoted to country are among the most popular in other Anglophone nations, none of the top ten most-listened-to stations in the UK are country stations, and national broadcaster BBC Radio does not offer a full-time country station (BBC Radio 2 Country, a \"pop-up\" station, operated four days each year between 2015 and 2017). The BBC does offer a country show on BBC Radio 2 each week hosted by Bob Harris.\nThe most successful British country music act of the 21st century are Ward Thomas and the Shires. In 2015, the Shires' album \"Brave\", became the first UK country act ever to chart in the Top 10 of the UK Albums Chart and they became the first UK country act to receive an award from the American Country Music Association. In 2016, Ward Thomas then became the first UK country act to hit number 1 in the UK Albums Chart with their album \"Cartwheels\".\nThere is the festival held every year, and for many years there was a festival at Wembley Arena, which was broadcast on the BBC, the International Festivals of Country Music, promoted by Mervyn Conn, held at the venue between 1969 and 1991. The shows were later taken into Europe, and featured such stars as Johnny Cash, Dolly Parton, Tammy Wynette, David Allan Coe, Emmylou Harris, Boxcar Willie, Johnny Russell and Jerry Lee Lewis. A handful of country musicians had even greater success in mainstream British music than they did in the U.S., despite a certain amount of disdain from the music press. Britain's largest music festival Glastonbury has featured major US country acts in recent years, such as Kenny Rogers in 2013 and Dolly Parton in 2014.\nFrom within the UK, few country musicians achieved widespread mainstream success. Many British singers who performed the occasional country songs are of other genres. Tom Jones, by this point near the end of his peak success as a pop singer, had a string of country hits in the late 1970s and early 1980s. The Bee Gees had some fleeting success in the genre, with one country hit as artists (\"Rest Your Love on Me\") and a major hit as songwriters (\"Islands in the Stream\"); Barry Gibb, the band's usual lead singer and last surviving member, acknowledged that country music was a major influence on the band's style. Singer Engelbert Humperdinck, while charting only once in the U.S. country top 40 with \"After the Lovin'\", achieved widespread success on both the U.S. and British pop charts with his covers of Nashville country ballads such as \"Release Me\", \"Am I That Easy to Forget\" and \"There Goes My Everything\". Welsh singer Bonnie Tyler initially started her career making country records, and in 1978 her single \"It's a Heartache\" reached number four on the UK Singles Chart. In 2013, Tyler returned to her roots, blending the country elements of her early work with the rock of her successful material on her album \"Rocks and Honey\" which featured a duet with Vince Gill. The songwriting tandem of Roger Cook and Roger Greenaway wrote a number of country hits, in addition to their widespread success in pop songwriting; Cook is notable for being the only Briton to be inducted into the Nashville Songwriters Hall of Fame.\nA niche country subgenre popular in the West Country is Scrumpy and Western, which consists mostly of novelty songs and comedy music recorded there (its name comes from scrumpy, an alcoholic beverage). A primarily local interest, the largest Scrumpy and Western hit in the UK and Ireland was \"The Combine Harvester\", which pioneered the genre and reached number one in both the UK and Ireland; Fred Wedlock had a number-six hit in 1981 with \"The Oldest Swinger in Town\". In 1975, comedian Billy Connolly topped the UK Singles Chart with \"D.I.V.O.R.C.E.\", a parody of the Tammy Wynette song \"D-I-V-O-R-C-E\".\nThe British Country Music Festival is an annual three-day festival held in the seaside resort of Blackpool. It uniquely promotes artists from the United Kingdom and Ireland to celebrate the impact that Celtic and British settlers to America had on the origins of country music. Past headline artists have included Amy Wadge, Ward Thomas, Tom Odell, Nathan Carter, Lisa McHugh, Catherine McGrath, Wildwood Kin, The Wandering Hearts and Henry Priestman.\nIreland.\nIn Ireland, Country and Irish is a music genre that combines traditional Irish folk music with US country music. Television channel TG4 began a quest for Ireland's next country star called \"Gl\u00f3r T\u00edre\", translated as \"Country Voice\". It is now in its sixth season and is one of TG4's most-watched TV shows. Over the past ten years, country and gospel recording artist James Kilbane has reached multi-platinum success with his mix of Christian and traditional country influenced albums. James Kilbane like many other Irish artists is today working closer with Nashville. Daniel O'Donnell achieved international success with his brand of music crossing country, Irish folk and European easy listening, earning a strong following among older women both in the British Isles and in North America. A recent success in the Irish arena has been Crystal Swing.\nJapan and Asia.\nIn Japan, country and western music first developed a following before World War II, but many Japanese became exposed to it after the war due to the Far East Network. One of the first Japanese western acts was Biji Kuroda &amp; The Chuck Wagon Boys, other vintage artists include Jimmie Tokita and His Mountain Playboys, The Blue Rangers, Wagon Aces, and Tomi Fujiyama. While the majority of these musicians sung in English, a few of them sang in the Japanese language, such as Fujiyama and Kazuya Kosaka. The genre continues to have a dedicated following in Japan, thanks to Charlie Nagatani, Katsuoshi Suga, J.T. Kanehira, Dicky Kitano, and Manami Sekiya. Country and western venues in Japan include the former annual Country Gold which were put together by Charlie Nagatani, and the modern honky tonks at Little Texas in Tokyo and Armadillo in Nagoya.\nIn India, there is an annual concert festival called \"Blazing Guitars\" held in Chennai brings together Anglo-Indian musicians from all over the country (including some who have emigrated to places like Australia). The year 2003 brought home-grown Indian, Bobby Cash to the forefront of the country music culture in India when he became India's first international country music artist to chart singles in Australia.\nIn the Philippines, country music has found their way into Cordilleran way of life, which often compares the Igorot lifestyle to that of US cowboys. The Philippines was once a US Commonwealth from 1900 to 1946, and country music began to be exported to the islands in the early 20th centurty. Baguio City has an FM station that caters to country music, DZWR 99.9 Country, which is part of the Catholic Media Network. Bombo Radyo Baguio has a segment on its Sunday slot for Igorot, Ilocano and country music. And as of recently, DWUB occasionally plays country music. Many country music musicians tour the Philippines. Original Pinoy Music has influences from country.\nOther international country music.\nTom Roland, from the Country Music Association International, explains country music's global popularity: \"In this respect, at least, Country Music listeners around the globe have something in common with those in the United States. In Germany, for instance, Rohrbach identifies three general groups that gravitate to the genre: people intrigued with the US cowboy icon, middle-aged fans who seek an alternative to harder rock music and younger listeners drawn to the pop-influenced sound that underscores many current Country hits.\" One of the first US people to perform country music abroad was George Hamilton IV. He was the first country musician to perform in the Soviet Union; he also toured in Australia and the Middle East. He was deemed the \"International Ambassador of Country Music\" for his contributions to the globalization of country music. Johnny Cash, Emmylou Harris, Keith Urban, and Dwight Yoakam have also made numerous international tours. The Country Music Association undertakes various initiatives to promote country music internationally.\nMiddle East.\nIn Iran, country music has appeared in recent years. According to \"Melody Music Magazine\", the pioneer of country music in Iran is the English-speaking country music band Dream Rovers, whose founder, singer and songwriter is Erfan Rezayatbakhsh (elf). The band was formed in 2007 in Tehran, and during this time they have been trying to introduce and popularize country music in Iran by releasing two studio albums and performing live at concerts, despite the difficulties that the Islamic regime in Iran makes for bands that are active in the western music field.\nMusician Toby Keith performed alongside Saudi Arabian folk musician Rabeh Sager in 2017. This concert was similar to the performances of Jazz ambassadors that performed distinctively American style music internationally.\nContinental Europe.\nIn Sweden, Rednex rose to stardom combining country music with electro-pop in the 1990s. In 1994, the group had a worldwide hit with their version of the traditional Southern tune \"Cotton-Eyed Joe\". Artists popularizing more traditional country music in Sweden have been Ann-Louise Hanson, Hasse Andersson, Kikki Danielsson, Elisabeth Andreassen and Jill Johnson. In Poland an international country music festival, known as Piknik Country, has been organised in Mr\u0105gowo in Masuria since 1983. The number of country music artists in France has increased. Some of the most important are Liane Edwards, Annabel, Rockie Mountains, Tahiana, and Lili West. French rock and roll singer Eddy Mitchell is also inspired by Americana and country music.\nIn the Netherlands there are many artists producing popular country and Americana music, which is mostly in the English language, as well as Dutch country and country-like music in the Dutch language. The latter is mainly popular on the countrysides in the northern and eastern parts of the Netherlands and is less associated with its US brethren, although it sounds sometimes very similar. Well-known popular artists mainly performing in English are Waylon, Danny Vera, Ilse DeLange, Douwe Bob and Henk Wijngaard.\nNorway had a significant country scene from the late 1970s to the late 2000s, with bands and artists including Hellbillies, Bj\u00f8ro H\u00e5land, Terje Tysland, Vassendgutane, \u00d8ystein Sunde, and Rotlaus. The scene and its concerts were considered mostly a rural scene, such that most bands sang in dialects, but occasional songs made it to national fame even in the larger cities. The songs occasionally used inspirations from rock music, Norwegian folk music, and polka, but remained recognisable as country music.\nPerformers and shows.\nUS cable television.\nSeveral US television networks are at least partly devoted to the genre: Country Music Television (CMT) (the first channel devoted to country music) and CMT Music (both owned by Paramount Global), RFD-TV and The Cowboy Channel (both owned by Rural Media Group), Heartland (owned by Get After It Media), Circle Country (a joint venture of the \"Grand Ole Opry\" and Gray Television), The Country Network (owned by TCN Country, LLC), and Country Music Channel (the country-oriented sister channel of California Music Channel).\nThe Nashville Network (TNN) was launched in 1983 as a channel devoted to country music, and later added sports and outdoor lifestyle programming. It actually launched just two days after CMT. In 2000, after TNN and CMT fell under the same corporate ownership, TNN was stripped of its country format and rebranded as \"The National Network\", then \"Spike TV\" in 2003, \"Spike\" in 2006, and finally Paramount Network in 2018. TNN was later revived from 2012 to 2013 after Jim Owens Entertainment (the company responsible for prominent TNN hosts Crook &amp; Chase) acquired the trademark and licensed it to Luken Communications; that channel renamed itself Heartland after Luken was embroiled in an unrelated dispute that left the company bankrupt.\nGreat American Country (GAC) was launched in 1995, also as a country music-oriented channel that would later add lifestyle programming pertaining to the American Heartland and South. In Spring 2021, GAC's then-owner, Discovery, Inc. divested the network to GAC Media, which also acquired the equestrian network Ride TV. Later, in the summer of that year, GAC Media relaunched Great American Country as GAC Family, a family-oriented general entertainment network, while Ride TV was relaunched as GAC Living, a network devoted to programming pertaining to lifestyles of the American South. The GAC acronym which once stood for \"Great American Country\" now stands for \"Great American Channels\".\nCanadian television.\nOnly one television channel was dedicated to country music in Canada: CMT owned by Corus Entertainment (90%) and Viacom (10%). However, the lifting of strict genre licensing restrictions saw the network remove the last of its music programming at the end of August 2017 for a schedule of generic off-network family sitcoms, Cancom-compliant lifestyle programming, and reality programming. In the past, the current-day Cottage Life network saw some country focus as Country Canada and later, CBC Country Canada before that network drifted into an alternate network for overflow CBC content as Bold. Stingray Music continues to maintain several country music audio-only channels on cable radio.\nIn the past, country music had an extensive presence, especially on the Canadian national broadcaster, CBC Television. The show \"Don Messer's Jubilee\" significantly affected country music in Canada; for instance, it was the program that launched Anne Murray's career. Gordie Tapp's \"Country Hoedown\" and its successor, \"The Tommy Hunter Show\", ran for a combined 36 years on the CBC, from 1956 to 1992; in its last nine years on air, the U.S. cable network TNN carried Hunter's show.\nAustralian cable television.\nThe only network dedicated to country music in Australia was the Country Music Channel owned by Foxtel. It ceased operations in June 2020 and was replaced by CMT (owned by Network 10 parent company Paramount Networks UK &amp; Australia).\nBritish digital television.\nOne music video channel is dedicated to country music in the United Kingdom: Music &amp; Memories, owned by Canis Media. Music &amp; Memories, formerly known as Keep it Country and Spotlight, features a mix of country-western, pop oldies and Celtic folk music."}
{"id": "5248", "revid": "49118185", "url": "https://en.wikipedia.org/wiki?curid=5248", "title": "Cold War (1948\u20131953)", "text": "The Cold War (1948\u20131953) is the period within the Cold War from the incapacitation of the Allied Control Council in 1948 to the conclusion of the Korean War in 1953.\nThe list of world leaders in these years is as follows:\nEurope.\nBerlin Blockade.\nAfter the Marshall Plan, the introduction of a new currency to Western Germany to replace the debased Reichsmark and massive electoral losses for communist parties in 1946, in June 1948, the Soviet Union cut off surface road access to Berlin.\nOn the day of the Berlin Blockade, a Soviet representative told the other occupying powers \"We are warning both you and the population of Berlin that we shall apply economic and administrative sanctions that will lead to circulation in Berlin exclusively of the currency of the Soviet occupation zone.\"\nThereafter, street and water communications were severed, rail and barge traffic was stopped and the Soviets initially stopped supplying food to the civilian population in the non-Soviet sectors of Berlin. Because Berlin was located within the Soviet-occupied zone of Germany and the other occupying powers had previously relied on Soviet good will for access to Berlin, the only available methods of supplying the city were three limited air corridors.\nBy February 1948, because of massive post-war military cuts, the entire United States army had been reduced to 552,000 men. Military forces in non-Soviet Berlin sectors totaled only 8,973 Americans, 7,606 British and 6,100 French. Soviet military forces in the Soviet sector that surrounded Berlin totaled one and a half million men. The two United States regiments in Berlin would have provided little resistance against a Soviet attack. Believing that Britain, France and the United States had little option other than to acquiesce, the Soviet Military Administration in Germany celebrated the beginning of the blockade. Thereafter, a massive aerial supply campaign of food, water and other goods was initiated by the United States, Britain, France and other countries. The Soviets derided \"the futile attempts of the Americans to save face and to maintain their untenable position in Berlin.\" The success of the airlift eventually caused the Soviets to lift their blockade in May 1949.\nHowever, the Soviet Army was still capable of conquering Western Europe without much difficulty. In September 1948, US military intelligence experts estimated that the Soviets had about 485,000 troops in their German occupation zone and in Poland, and some 1.785 million troops in Europe in total. At the same time, the number of US troops in 1948 was about 140,000.\nTito\u2013Stalin Split.\nAfter disagreements between Yugoslavian leader Josip Broz Tito and the Soviet Union regarding Greece and the People's Republic of Albania, a Tito\u2013Stalin Split occurred, followed by Yugoslavia being expelled from the Cominform in June 1948 and a brief failed Soviet putsch in Belgrade. The split created two separate communist forces in Europe. A vehement campaign against \"Titoism\" was immediately started in the Eastern Bloc, describing agents of both the West and Tito in all places engaging in subversive activity. This resulted in the persecution of many major party cadres, including those in East Germany.\nwas split up and dissolved in 1954 and 1975, also because of the d\u00e9tente between the West and Tito.\nNATO.\nThe United States joined Britain, France, Canada, Denmark, Portugal, Norway, Belgium, Iceland, Luxembourg, Italy, and the Netherlands in 1949 to form the North Atlantic Treaty Organization (NATO), the United States' first \"entangling\" European alliance in 170 years. West Germany, Spain, Greece, and Turkey would later join this alliance. The Eastern leaders retaliated against these steps by integrating the economies of their nations in Comecon, their version of the Marshall Plan; exploding the first Soviet atomic device in 1949; signing an alliance with People's Republic of China in February 1950; and forming the Warsaw Pact, Eastern Europe's counterpart to NATO, in 1955. The Soviet Union, Albania, Czechoslovakia, Hungary, East Germany, Bulgaria, Romania, and Poland founded this military alliance.\nNSC 68.\nU.S. officials quickly moved to escalate and expand \"containment.\" In a secret 1950 document, NSC 68, they proposed to strengthen their alliance systems, quadruple defense spending, and embark on an elaborate propaganda campaign to convince the U.S. public to fight this costly cold war. Truman ordered the development of a hydrogen bomb. In early 1950, the U.S. took its first efforts to oppose communist forces in Vietnam; planned to form a West German army, and prepared proposals for a peace treaty with Japan that would guarantee long-term U.S. military bases there.\nOutside Europe.\nThe Cold War took place worldwide, but it had a partially different timing and trajectory outside Europe.\nIn Africa, decolonization took place first; it was largely accomplished in the 1950s. The main rivals then sought bases of support in the new national political alignments. In Latin America, the first major confrontation took place in Guatemala in 1954. When the new Castro government of Cuba turned to Soviets support in 1960, Cuba became the center of the anti-American Cold War forces, supported by the Soviet Union.\nChinese Civil War.\nAs Japan's empire collapsed in 1945 the civil war resumed in China between the Kuomintang (KMT) led by Generalissimo Chiang Kai-shek and the Chinese Communist Party led by Mao Zedong. The USSR had signed a Treaty of Friendship with the Kuomintang in 1945 and disavowed support for the Chinese Communists. The outcome was closely fought, with the Communists finally prevailing with superior military tactics. Although the Nationalists had an advantage in numbers of men and weapons, initially controlled a much larger territory and population than their adversaries, and enjoyed considerable international support, they were exhausted by the long war with Japan and the attendant internal responsibilities. In addition, the Chinese Communists were able to fill the political vacuum left in Manchuria after Soviet forces withdrew from the area and thus gained China's prime industrial base. The Chinese Communists were able to fight their way from the north and northeast, and virtually all of mainland China was taken by the end of 1949. On October 1, 1949, Mao Zedong proclaimed the People's Republic of China (PRC). Chiang Kai-shek and 600,000 Nationalist troops and 2 million refugees, predominantly from the government and business community, fled from the mainland to the island of Taiwan. In December 1949, Chiang proclaimed Taipei the temporary capital of the Republic of China (ROC) and continued to assert his government as the sole legitimate authority in China.\nThe continued hostility between the Communists on the mainland and the Nationalists on Taiwan continued throughout the Cold War. Though the United States refused to aide Chiang Kai-shek in his hope to \"recover the mainland,\" it continued supporting the Republic of China with military supplies and expertise to prevent Taiwan from falling into PRC hands. Through the support of the Western bloc (most Western countries continued to recognize the ROC as the sole legitimate government of China), the Republic of China on Taiwan retained China's seat in the United Nations until 1971.\nMadiun Affair.\nMadiun Affair took place on September 18, 1948, in the city of Madiun, East Java. This rebellion was carried out by the Front Demokrasi Rakyat (FDR, People's Democratic Front) which united all socialist and communist groups in Indonesia. This rebellion ended 3 months later after its leaders were arrested and executed by the TNI.\nThis revolt began with the fall of the Amir Syarifuddin Cabinet due to the signing of the Renville Agreement which benefited the Dutch and was eventually replaced by the Hatta Cabinet which did not belong to the left wing. This led Amir Syarifuddin to declare opposition to the Hatta Cabinet government and to declare the formation of the People's Democratic Front.\nBefore it, In the PKI Politburo session on August 13\u201314, 1948, Musso, an Indonesian communist figure, introduced a political concept called \"Jalan Baru\". He also wanted a single Marxism party called the PKI (Communist Party of Indonesia) consisting of illegal communists, the Labour Party of Indonesia, and Partai Sosialis(Socialist Party).\nOn September 18, 1948, the FDR declared the formation of the Republic of Soviet-Indonesia. In addition, the communists also carried out a rebellion in the Pati Residency and the kidnapping of groups who were considered to be against communists. Even this rebellion resulted in the murder of the Governor of East Java at the time, Raden Mas Tumenggung Ario Soerjo.\nThe crackdown operation against this movement began. This operation was led by A.H. Nasution. The Indonesian government also applied Commander General Sudirman to the Military Operations Movement I where General Sudirman ordered Colonel Gatot Soebroto and Colonel Sungkono to mobilize the TNI and police to crush the rebellion.\nOn September 30, 1948, Madiun was captured again by the Republic of Indonesia. Musso was shot dead on his escape in Sumoroto and Amir Syarifuddin was executed after being captured in Central Java. In early December 1948, the Madiun Affair crackdown was declared complete.\nKorean War.\nIn early 1950, the United States made its first commitment to form a peace treaty with Japan that would guarantee long-term U.S. military bases. Some observers (including George Kennan) believed that the Japanese treaty led Stalin to approve a plan to invade U.S.-supported South Korea on June 25, 1950. Korea had been divided at the end of World War II along the 38th parallel into Soviet and U.S. occupation zones, in which a communist government was installed in the North by the Soviets, and an elected government in the South came to power after UN-supervised elections in 1948.\nIn June 1950, Kim Il Sung's North Korean People's Army invaded South Korea. Fearing that communist Korea under a Kim Il Sung dictatorship could threaten Japan and foster other communist movements in Asia, Truman committed U.S. forces and obtained help from the United Nations to counter the North Korean invasion. The Soviets boycotted UN Security Council meetings while protesting the council's failure to seat the People's Republic of China and, thus, did not veto the council's approval of UN action to oppose the North Korean invasion. A joint UN force of personnel from South Korea, the United States, Britain, Turkey, Canada, Australia, France, the Philippines, the Netherlands, Belgium, New Zealand and other countries joined to stop the invasion. After a Chinese invasion to assist the North Koreans, fighting stabilized along the 38th parallel, which had separated the Koreas. Truman faced a hostile China, a Sino-Soviet partnership, and a defense budget that had quadrupled in eighteen months.\nThe Korean Armistice Agreement was signed in July 1953 after the death of Stalin, who had been insisting that the North Koreans continue fighting. In North Korea, Kim Il Sung created a highly centralized and brutal dictatorship, according himself unlimited power and generating a formidable cult of personality.\nHydrogen bomb.\nA hydrogen bomb\u2014which produced nuclear fusion instead of nuclear fission\u2014was first tested by the United States in November 1952 and the Soviet Union in August 1953. Such bombs were first deployed in the 1960s.\nCulture and media.\nFear of a nuclear war spurred the production of public safety films by the United States federal government's Civil Defense branch that demonstrated ways on protecting oneself from a Soviet nuclear attack. The 1951 children's film \"Duck and Cover\" is a prime example.\nGeorge Orwell's classic dystopia Nineteen Eighty-Four was published in 1949. The novel explores life in an imagined future world where a totalitarian government has achieved terrifying levels of power and control. With Nineteen Eighty-Four, Orwell taps into the anti-communist fears that would continue to haunt so many in the West for decades to come. In a Cold War setting his descriptions could hardly fail to evoke comparison to Soviet communism and the seeming willingness of Stalin and his successors to control those within the Soviet bloc by whatever means necessary. Orwell's famous allegory of totalitarian rule, Animal Farm, published in 1945, provoked similar anti-communist sentiments."}
{"id": "5249", "revid": "39435755", "url": "https://en.wikipedia.org/wiki?curid=5249", "title": "Crony capitalism", "text": "Crony capitalism, sometimes also called simply cronyism, is a pejorative term used in political discourse to describe a situation in which businesses profit from a close relationship with state power, either through an anti-competitive regulatory environment, direct government largesse, and/or corruption. Examples given for crony capitalism include obtainment of permits, government grants, tax breaks, or other undue influence from businesses over the state's deployment of public goods, for example, mining concessions for primary commodities or contracts for public works. In other words, it is used to describe a situation where businesses thrive not as a result of free enterprise, but rather collusion between a business class and the political class.\nWealth is then accumulated not merely by making a profit in the market, but through profiteering by rent seeking using this monopoly or oligopoly. Entrepreneurship and innovative practices that seek to reward risk are stifled since the value-added is little by crony businesses, as hardly anything of significant value is created by them, with transactions taking the form of trading. Crony capitalism spills over into the government, the politics, and the media, when this nexus distorts the economy and affects society to an extent it corrupts public-serving economic, political, and social ideals.\nHistorical usage.\nThe first extensive use of the term \"crony capitalism\" came about in the 1980s, to characterize the Philippine economy under the dictatorship of Ferdinand Marcos. Early uses of this term to describe the economic practices of the Marcos regime included that of Ricardo Manapat, who introduced it in his 1979 pamphlet \"Some are Smarter than Others\", which was later published in 1991; former \"Time\" magazine business editor George M. Taber, who used the term in a \"Time\" magazine article in 1980, and activist (and later Finance Minister) Jaime Ongpin, who used the term extensively in his writing and is sometimes credited for having coined it.\nThe term crony capitalism made a significant impact in the public as an explanation of the Asian financial crisis. \nIt is also used to describe governmental decisions favoring cronies of governmental officials. \nThe term is used largely interchangeably with the related term corporate welfare, although the latter is by definition specific to corporations.\nIn practice.\nCrony capitalism exists along a continuum. In its lightest form, crony capitalism consists of collusion among market players which is officially tolerated or encouraged by the government. While perhaps lightly competing against each other, they will present a unified front (sometimes called a trade association or industry trade group) to the government in requesting subsidies aid or regulation. For instance, newcomers to a market then need to surmount significant barriers to entry in seeking loans, acquiring shelf space, or receiving official sanction. Some such systems are very formalized, such as sports leagues and the Medallion System of the taxicabs of New York City, but often the process is more subtle, such as expanding training and certification exams to make it more expensive for new entrants to enter a market and thereby limiting potential competition. In technological fields, there may evolve a system whereby new entrants may be accused of infringing on patents that the established competitors never assert against each other. In spite of this, some competitors may succeed when the legal barriers are light. The term crony capitalism is generally used when these practices either come to dominate the economy as a whole, or come to dominate the most valuable industries in an economy. Intentionally ambiguous laws and regulations are common in such systems. Taken strictly, such laws would greatly impede practically all business activity, but in practice, they are only erratically enforced. The specter of having such laws suddenly brought down upon a business provides an incentive to stay in the good graces of political officials. Troublesome rivals who have overstepped their bounds can have these laws suddenly enforced against them, leading to fines or even jail time. Even in high-income democracies with well-established legal systems and freedom of the press in place, a larger state is generally associated with increased political corruption.\nThe term crony capitalism was initially applied to states involved in the 1997 Asian financial crisis such as Indonesia, South Korea and Thailand. In these cases, the term was used to point out how family members of the ruling leaders became extremely wealthy with no non-political justification. Southeast Asian nations, such as Hong Kong and Malaysia, still score very poorly in rankings measuring this. It was also used in this context as part of a broader liberal critique of economic dirigisme. The term has also been applied to the system of oligarchs in Russia. Other states to which the term has been applied include India, in particular the system after the 1990s liberalization, whereby land and other resources were given at throwaway prices in the name of public-private partnerships, the more recent coal-gate scam and cheap allocation of land and resources to Adani SEZ under the Congress and BJP governments. Similar references to crony capitalism have been made to other countries such as Argentina and Greece. Wu Jinglian, one of China's leading economists and a longtime advocate of its transition to free markets, says that it faces two starkly contrasting futures, namely a market economy under the rule of law or crony capitalism. A dozen years later, prominent political scientist Pei Minxin concluded that the latter course had become deeply embedded in China. The anti-corruption campaign under Xi Jinping (2012\u2013) has seen more than 100,000 high- and low-ranking Chinese officials indicted and jailed.\nMany prosperous nations have also had varying amounts of cronyism throughout their history, including the United Kingdom, especially in the 1600s and 1700s, the United States and Japan.\nCrony capitalism index.\n\"The Economist\" benchmarks countries based on a crony-capitalism index calculated via how much economic activity occurs in industries prone to cronyism. Its 2014 Crony Capitalism Index ranking listed Hong Kong, Russia and Malaysia in the top three spots.\nIn finance.\nCrony capitalism in finance was found in the Second Bank of the United States. It was a private company, but its largest stockholder was the federal government which owned 20%. It was an early bank regulator and grew to be one of the most powerful organizations in the country due largely to being the depository of the government's revenue.\nThe Gramm\u2013Leach\u2013Bliley Act in 1999 completely removed Glass\u2013Steagall\u2019s separation between commercial banks and investment banks. After this repeal, commercial banks, investment banks and insurance companies combined their lobbying efforts. Critics claim this was instrumental in the passage of the Bankruptcy Abuse Prevention and Consumer Protection Act of 2005.\nIn sections of an economy.\nMore direct government involvement in a specific sector can also lead to specific areas of crony capitalism, even if the economy as a whole may be competitive. This is most common in natural resource sectors through the granting of mining or drilling concessions, but it is also possible through a process known as regulatory capture where the government agencies in charge of regulating an industry come to be controlled by that industry. Governments will often establish in good faith government agencies to regulate an industry. However, the members of an industry have a very strong interest in the actions of that regulatory body while the rest of the citizenry are only lightly affected. As a result, it is not uncommon for current industry players to gain control of the watchdog and use it against competitors. This typically takes the form of making it very expensive for a new entrant to enter the market. An 1824 landmark United States Supreme Court ruling overturned a New York State-granted monopoly (\"a veritable model of state munificence\" facilitated by Robert R. Livingston, one of the Founding Fathers) for the then-revolutionary technology of steamboats. Leveraging the Supreme Court's establishment of Congressional supremacy over commerce, the Interstate Commerce Commission was established in 1887 with the intent of regulating railroad robber barons. President Grover Cleveland appointed Thomas M. Cooley, a railroad ally, as its first chairman and a permit system was used to deny access to new entrants and legalize price fixing.\nThe defense industry in the United States is often described as an example of crony capitalism in an industry. Connections with the Pentagon and lobbyists in Washington are described by critics as more important than actual competition due to the political and secretive nature of defense contracts. In the Airbus-Boeing WTO dispute, Airbus (which receives outright subsidies from European governments) has stated Boeing receives similar subsidies which are hidden as inefficient defense contracts. Other American defense companies were put under scrutiny for no-bid contracts for the Iraq War and Hurricane Katrina related contracts purportedly due to having cronies in the Bush administration.\nGerald P. O'Driscoll, former vice president at the Federal Reserve Bank of Dallas, stated that Fannie Mae and Freddie Mac became examples of crony capitalism as government backing let Fannie and Freddie dominate mortgage underwriting, saying: \"The politicians created the mortgage giants, which then returned some of the profits to the pols\u2014sometimes directly, as campaign funds; sometimes as \"contributions\" to favored constituents\".\nIn developing economies.\nIn its worst form, crony capitalism can devolve into simple corruption where any pretense of a free market is dispensed with, bribes to government officials are considered \"de rigueur\" and tax evasion is common. This is seen in many parts of Africa and is sometimes called plutocracy (rule by wealth) or kleptocracy (rule by theft). Kenyan economist David Ndii has repeatedly brought to light how this system has manifested over time, occasioned by the reign of Uhuru Kenyatta as president.\nCorrupt governments may favor one set of business owners who have close ties to the government over others. This may also be done with, religious, or ethnic favoritism. For instance, Alawites in Syria have a disproportionate share of power in the government and business there (President Assad himself is an Alawite). This can be explained by considering personal relationships as a social network. As government and business leaders try to accomplish various things, they naturally turn to other powerful people for support in their endeavors. These people form hubs in the network. In a developing country, those hubs may be very few, thus concentrating economic and political power in a small interlocking group.\nNormally, this will be untenable to maintain in business as new entrants will affect the market. However, if business and government are entwined, then the government can maintain the small-hub network.\nRaymond Vernon, a specialist in economics and international affairs, wrote that the Industrial Revolution began in Great Britain because they were the first to successfully limit the power of veto groups (typically cronies of those with power in government) to block innovations, writing: \"Unlike most other national environments, the British environment of the early 19th century contained relatively few threats to those who improved and applied existing inventions, whether from business competitors, labor, or the government itself. In other European countries, by contrast, the merchant guilds ... were a pervasive source of veto for many centuries. This power was typically bestowed upon them by government.\" Vermon further stated that \"a steam-powered horseless carriage produced in France in 1769 was officially suppressed.\" James Watt began experimenting with steam in 1763, got a patent in 1769 and began commercial production in 1775.\nRaghuram Rajan, former governor of the Reserve Bank of India, has said: \"One of the greatest dangers to the growth of developing countries is the middle income trap, where crony capitalism creates oligarchies that slow down growth. If the debate during the elections is any pointer, this is a very real concern of the public in India today\". Tavleen Singh, columnist for \"The Indian Express\", has disagreed. According to Singh, India's corporate success is not a product of crony capitalism, but because India is no longer under the influence of crony socialism.\nPolitical viewpoints.\nWhile the problem is generally accepted across the political spectrum, ideology shades the view of the problem's causes and therefore its solutions. Political views mostly fall into two camps which might be called the socialist and capitalist critique. The socialist position is that crony capitalism is the inevitable result of any strictly capitalist system and thus broadly democratic government must regulate economic, or wealthy, interests to restrict monopoly. The capitalist position is that natural monopolies are rare, therefore governmental regulations generally abet established wealthy interests by restricting competition.\nSocialist critique.\nCritics of crony capitalism including socialists and anti-capitalists often assert that so-called crony capitalism is simply the inevitable result of any strictly capitalist system. Jane Jacobs described it as a natural consequence of collusion between those managing power and trade while Noam Chomsky has argued that the word crony is superfluous when describing capitalism. Since businesses make money and money leads to political power, business will inevitably use their power to influence governments. Much of the impetus behind campaign finance reform in the United States and in other countries is an attempt to prevent economic power from being used to take political power.\nRavi Batra argues that \"all official economic measures adopted since 1981 ... have devastated the middle class\" and that the Occupy Wall Street movement should push for their repeal and thus end the influence of the super wealthy in the political process which he considers a manifestation of crony capitalism.\nSocialist economists, such as Robin Hahnel, have criticized the term as an ideologically motivated attempt to cast what is in their view the fundamental problems of capitalism as avoidable irregularities. Socialist economists dismiss the term as an apologetic for failures of neoliberal policy and more fundamentally their perception of the weaknesses of market allocation.\nCapitalist critique.\nSupporters of capitalism also generally oppose crony capitalism. Further, supporters such as classical liberals, neoliberals and right-libertarians consider it an aberration brought on by governmental favors incompatible with the free market.. In the capitalist view, cronyism is the result of an excess of interference in the market which inevitably will result in a toxic combination of corporations and government officials running sectors of the economy. For instance, the \"Financial Times\" observed that, in Vietnam during the 2010s, the primary beneficiaries of cronyism were Communist party officials, noting also the \"common practice of employing only party members and their family members and associates to government jobs or to jobs in state-owned enterprises.\"\nConservative commentator Ben Shapiro prefers to equate this problem with terms such as corporatocracy or corporatism, considered \"a modern form of mercantilism\", to emphasize that the only way to run a profitable business in such a system is to have help from corrupt government officials. Likewise, Hernando de Soto said that mercantilism \"is also known as 'crony' or 'noninclusive' capitalism\".\nEven if the initial regulation was well-intentioned (to curb actual abuses) and even if the initial lobbying by corporations was well-intentioned (to reduce illogical regulations), the mixture of business and government stifles competition, a collusive result called regulatory capture. Burton W. Folsom Jr. distinguishes those who engage in crony capitalism\u2014designated by him political entrepreneurs\u2014from those who compete in the marketplace without special aid from the government, whom he calls market entrepreneurs. The market entrepreneurs such as James J. Hill, Cornelius Vanderbilt and John D. Rockefeller succeeded by producing a quality product at a competitive price. For example, political entrepreneurs such as Edward Collins in steamships and the leaders of the Union Pacific Railroad in railroads were men who used the power of government to succeed. They tried to gain subsidies or in some way use the government to stop competitors."}
{"id": "5252", "revid": "43918024", "url": "https://en.wikipedia.org/wiki?curid=5252", "title": "Lists of universities and colleges", "text": "This is a list of lists of universities and colleges."}
{"id": "5253", "revid": "43263260", "url": "https://en.wikipedia.org/wiki?curid=5253", "title": "Constitution", "text": "A constitution is the aggregate of fundamental principles or established precedents that constitute the legal basis of a polity, organization or other type of entity, and commonly determines how that entity is to be governed.\nWhen these principles are written down into a single document or set of legal documents, those documents may be said to embody a \"written constitution\"; if they are encompassed in a single comprehensive document, it is said to embody a \"codified constitution\". The Constitution of the United Kingdom is a notable example of an \"uncodified constitution\"; it is instead written in numerous fundamental acts of a legislature, court cases, and treaties.\nConstitutions concern different levels of organizations, from sovereign countries to companies and unincorporated associations. A treaty that establishes an international organization is also its constitution, in that it would define how that organization is constituted. Within states, a constitution defines the principles upon which the state is based, the procedure in which laws are made and by whom. Some constitutions, especially codified constitutions, also act as limiters of state power, by establishing lines which a state's rulers cannot cross, such as fundamental rights. Changes to constitutions frequently require consensus or supermajority.\nThe Constitution of India is the longest written constitution of any country in the world, with 146,385 words in its English-language version, while the Constitution of Monaco is the shortest written constitution with 3,814 words. The Constitution of San Marino might be the world's oldest active written constitution, since some of its core documents have been in operation since 1600, while the Constitution of the United States is the oldest active codified constitution. The historical life expectancy of a constitution since 1789 is approximately 19 years.\nEtymology.\nThe term \"constitution\" comes through French from the Latin word , used for regulations and orders, such as the imperial enactments (\"constitutiones principis\": edicta, mandata, decreta, rescripta). Later, the term was widely used in canon law for an important determination, especially a decree issued by the Pope, now referred to as an \"apostolic constitution\".\nWilliam Blackstone used the term for significant and egregious violations of public trust, of a nature and extent that the transgression would justify a revolutionary response. The term as used by Blackstone was not for a legal text, nor did he intend to include the later American concept of judicial review: \"for that were to set the judicial power above that of the legislature, which would be subversive of all government\".\nGeneral features.\nGenerally, every modern written constitution confers specific powers on an organization or institutional entity, established upon the primary condition that it abides by the constitution's limitations. According to Scott Gordon, a political organization is constitutional to the extent that it \"contain[s] institutionalized mechanisms of power control for the protection of the interests and liberties of the citizenry, including those that may be in the minority\".\nActivities of officials within an organization or polity that fall within the constitutional or statutory authority of those officials are termed \"within power\" (or, in Latin, \"intra vires\"); if they do not, they are termed \"beyond power\" (or, in Latin, \"ultra vires\"). For example, a students' union may be prohibited as an organization from engaging in activities not concerning students; if the union becomes involved in non-student activities, these activities are considered to be \"ultra vires\" of the union's charter, and nobody would be compelled by the charter to follow them. An example from the constitutional law of sovereign states would be a provincial parliament in a federal state trying to legislate in an area that the constitution allocates exclusively to the federal parliament, such as ratifying a treaty. Action that appears to be beyond power may be judicially reviewed and, if found to be beyond power, must cease. Legislation that is found to be beyond power will be \"invalid\" and of no force; this applies to primary legislation, requiring constitutional authorization, and secondary legislation, ordinarily requiring statutory authorization. In this context, \"within power\", \"intra vires\", \"authorized\" and \"valid\" have the same meaning; as do \"beyond power\", \"ultra vires\", \"not authorized\" and \"invalid\".\nIn most but not all modern states the constitution has supremacy over ordinary statutory law (see Uncodified constitution below); in such states when an official act is unconstitutional, i.e. it is not a power granted to the government by the constitution, that act is \"null and void\", and the nullification is \"ab initio\", that is, from inception, not from the date of the finding. It was never \"law\", even though, if it had been a statute or statutory provision, it might have been adopted according to the procedures for adopting legislation. Sometimes the problem is not that a statute is unconstitutional, but that the application of it is, on a particular occasion, and a court may decide that while there are ways it could be applied that are constitutional, that instance was not allowed or legitimate. In such a case, only that application may be ruled unconstitutional. Historically, the remedies for such violations have been petitions for common law writs, such as \"quo warranto\".\nScholars debate whether a constitution must necessarily be autochthonous, resulting from the nations \"spirit\". Hegel said \"A constitution...is the work of centuries; it is the idea, the consciousness of rationality so far as that consciousness is developed in a particular nation.\"\nHistory and development.\nSince 1789, along with the Constitution of the United States of America (U.S. Constitution), which is the oldest and shortest written constitution still in force, close to 800 constitutions have been adopted and subsequently amended around the world by independent states.\nIn the late 18th century, Thomas Jefferson predicted that a period of 20 years would be the optimal time for any constitution to be still in force, since \"the earth belongs to the living, and not to the dead\". Indeed, according to recent studies, the average life of any new written constitution is around 19 years. However, a great number of constitutions do not last more than 10 years, and around 10% do not last more than one year, as was the case of the French Constitution of 1791. By contrast, some constitutions, notably that of the United States, have remained in force for several centuries, often without major revision for long periods of time.\nThe most common reasons for these frequent changes are the political desire for an immediate outcome and the short time devoted to the constitutional drafting process. A study in 2009 showed that the average time taken to draft a constitution is around 16 months, however there were also some extreme cases registered. For example, the Myanmar 2008 Constitution was being secretly drafted for more than 17 years, whereas at the other extreme, during the drafting of Japan's 1946 Constitution, the bureaucrats drafted everything in no more than a week. Japan has the oldest unamended constitution in the world. The record for the shortest overall process of drafting, adoption, and ratification of a national constitution belongs to the Romania's 1938 constitution, which installed a royal dictatorship in less than a month. Studies showed that typically extreme cases where the constitution-making process either takes too long or is extremely short were non-democracies. \nIn principle, constitutional rights are not a specific characteristic of democratic countries. Autocratic states have constitutions, such as that of North Korea, which officially grants every citizen, among other things, the freedom of expression. However, the extent to which governments abide by their own constitutional provisions varies. In North Korea, for example, the Ten Principles for the Establishment of a Monolithic Ideological System are said to have eclipsed the constitution in importance as a frame of government in practice. Developing a legal and political tradition of strict adherence to constitutional provisions is considered foundational to the rule of law.\nPre-modern constitutions.\nAncient.\nExcavations in modern-day Iraq by Ernest de Sarzec in 1877 found evidence of the earliest known code of justice, issued by the Sumerian king Urukagina of Lagash . Perhaps the earliest prototype for a law of government, this document itself has not yet been discovered; however it is known that it allowed some rights to his citizens. For example, it is known that it relieved tax for widows and orphans, and protected the poor from the usury of the rich.\nAfter that, many governments ruled by special codes of written laws. The oldest such document still known to exist seems to be the Code of Ur-Nammu of Ur (c. 2050 BC). Some of the better-known ancient law codes are the code of Lipit-Ishtar of Isin, the code of Hammurabi of Babylonia, the Hittite code, the Assyrian code, and Mosaic law.\nIn 621 BC, a scribe named Draco codified the oral laws of the city-state of Athens; this code prescribed the death penalty for many offenses (thus creating the modern term \"draconian\" for very strict rules). In 594 BC, Solon, the ruler of Athens, created the new \"Solonian Constitution\". It eased the burden of the workers, and determined that membership of the ruling class was to be based on wealth (plutocracy), rather than on birth (aristocracy). Cleisthenes again reformed the Athenian constitution and set it on a democratic footing in 508 BC.\nAristotle (c. 350 BC) was the first to make a formal distinction between ordinary law and constitutional law, establishing ideas of constitution and constitutionalism, and attempting to classify different forms of constitutional government. The most basic definition he used to describe a constitution in general terms was \"the arrangement of the offices in a state\". In his works \"Constitution of Athens\", \"Politics\", and \"Nicomachean Ethics\", he explores different constitutions of his day, including those of Athens, Sparta, and Carthage. He classified both what he regarded as good and what he regarded as bad constitutions, and came to the conclusion that the best constitution was a mixed system including monarchic, aristocratic, and democratic elements. He also distinguished between citizens, who had the right to participate in the state, and non-citizens and slaves, who did not.\nThe Romans initially codified their constitution in 450 BC as the \"Twelve Tables\". They operated under a series of laws that were added from time to time, but Roman law was not reorganized into a single code until the \"Codex Theodosianus\" (438 AD); later, in the Eastern Empire, the \"Codex repetit\u00e6 pr\u00e6lectionis\" (534) was highly influential throughout Europe. This was followed in the east by the \"Ecloga\" of Leo III the Isaurian (740) and the \"Basilica\" of Basil I (878).\nThe \"Edicts of Ashoka\" established constitutional principles for the 3rd century BC Maurya king's rule in India. For constitutional principles almost lost to antiquity, see the code of Manu.\nEarly Middle Ages.\nMany of the Germanic peoples that filled the power vacuum left by the Western Roman Empire in the Early Middle Ages codified their laws. One of the first of these Germanic law codes to be written was the Visigothic \"Code of Euric\" (471 AD). This was followed by the \"Lex Burgundionum\", applying separate codes for Germans and for Romans; the \"Pactus Alamannorum\"; and the Salic Law of the Franks, all written soon after 500. In 506, the \"Breviarum\" or \"Lex Romana\" of Alaric II, king of the Visigoths, adopted and consolidated the \"Codex Theodosianus\" together with assorted earlier Roman laws. Systems that appeared somewhat later include the \"Edictum Rothari\" of the Lombards (643), the \"Lex Visigothorum\" (654), the \"Lex Alamannorum\" (730), and the \"Lex Frisionum\" (c. 785). These continental codes were all composed in Latin, while Anglo-Saxon was used for those of England, beginning with the Code of \u00c6thelberht of Kent (602). Around 893, Alfred the Great combined this and two other earlier Saxon codes, with various Mosaic and Christian precepts, to produce the \"Doom book\" code of laws for England.\nJapan's \"Seventeen-article constitution\" written in 604, reportedly by Prince Sh\u014dtoku, is an early example of a constitution in Asian political history. Influenced by Buddhist teachings, the document focuses more on social morality than on institutions of government, and remains a notable early attempt at a government constitution.\nThe Constitution of Medina (, \u1e62a\u1e25\u012bfat al-Mad\u012bna), also known as the Charter of Medina, was drafted by the Islamic prophet Muhammad after his flight (hijra) to Yathrib where he became political leader. It constituted a formal agreement between Muhammad and all of the significant tribes and families of Yathrib (later known as Medina), including Muslims, Jews, and pagans. The document was drawn up with the explicit concern of bringing to an end the bitter intertribal fighting between the clans of the Aws (Aus) and Khazraj within Medina. To this effect it instituted a number of rights and responsibilities for the Muslim, Jewish, and pagan communities of Medina bringing them within the fold of one community\u00a0\u2013 the Ummah. The precise dating of the Constitution of Medina remains debated, but generally, scholars agree it was written shortly after the Hijra (622).\nIn Wales, the Cyfraith Hywel (Law of Hywel) was codified by Hywel Dda c. 942\u2013950. It served as the main law code in Wales until it was superseded by the Laws in Wales Acts 1535 and 1542.\nMiddle Ages after 1000.\nThe \"Pravda Yaroslava\", originally combined by Yaroslav the Wise the Grand Prince of Kiev, was granted to Great Novgorod around 1017, and in 1054 was incorporated into the \"Russkaya Pravda\"; it became the law for all of Kievan Rus'. It survived only in later editions of the 15th century.\nIn England, Henry I's proclamation of the Charter of Liberties in 1100 bound the king for the first time in his treatment of the clergy and the nobility. This idea was extended and refined by the English barony when they forced King John to sign Magna Carta in 1215. The most important single article of Magna Carta, related to \"habeas corpus\", provided that the king was not permitted to imprison, outlaw, exile or kill anyone at a whim\u00a0\u2013 there must be due process of law first. This article, Article 39, of Magna Carta read:\nThis provision became the cornerstone of English liberty after that point. The social contract in the original case was between the king and the nobility but was gradually extended to all of the people. It led to the system of Constitutional Monarchy, with further reforms shifting the balance of power from the monarchy and nobility to the House of Commons.\nThe Nomocanon of Saint Sava () was the first Serbian constitution from 1219. St. Sava's Nomocanon was the compilation of civil law, based on Roman Law, and canon law, based on Ecumenical Councils. Its basic purpose was to organize the functioning of the young Serbian kingdom and the Serbian church. Saint Sava began the work on the Serbian Nomocanon in 1208 while he was at Mount Athos, using \"The Nomocanon in Fourteen Titles\", \"Synopsis of Stefan the Efesian\", \"Nomocanon of John Scholasticus\", and Ecumenical Council documents, which he modified with the canonical commentaries of Aristinos and Joannes Zonaras, local church meetings, rules of the Holy Fathers, the law of Moses, the translation of Prohiron, and the Byzantine emperors' Novellae (most were taken from Justinian's Novellae). The Nomocanon was a completely new compilation of civil and canonical regulations, taken from Byzantine sources but completed and reformed by St. Sava to function properly in Serbia. Besides decrees that organized the life of the church, there are various norms regarding civil life; most of these were taken from Prohiron. Legal transplants of Roman-Byzantine law became the basis of the Serbian medieval law. The essence of Zakonopravilo was based on Corpus Iuris Civilis.\nStefan Du\u0161an, emperor of Serbs and Greeks, enacted Du\u0161an's Code () in Serbia, in two state congresses: in 1349 in Skopje and in 1354 in Serres. It regulated all social spheres, so it was the second Serbian constitution, after St. Sava's Nomocanon (Zakonopravilo). The Code was based on Roman-Byzantine law. The legal transplanting within articles 171 and 172 of Du\u0161an's Code, which regulated juridical independence, is notable. They were taken from the Byzantine code Basilika (book VII, 1, 16\u201317).\nIn 1222, Hungarian King Andrew II issued the Golden Bull of 1222.\nBetween 1220 and 1230, a Saxon administrator, Eike von Repgow, composed the \"Sachsenspiegel\", which became the supreme law used in parts of Germany as late as 1900.\nAround 1240, the Coptic Egyptian Christian writer, 'Abul Fada'il Ibn al-'Assal, wrote the \"Fetha Negest\" in Arabic. 'Ibn al-Assal took his laws partly from apostolic writings and Mosaic law and partly from the former Byzantine codes. There are a few historical records claiming that this law code was translated into Ge'ez and entered Ethiopia around 1450 in the reign of Zara Yaqob. Even so, its first recorded use in the function of a constitution (supreme law of the land) is with Sarsa Dengel beginning in 1563. The \"Fetha Negest\" remained the supreme law in Ethiopia until 1931, when a modern-style Constitution was first granted by Emperor Haile Selassie I.\nIn the Principality of Catalonia, the Catalan constitutions were promulgated by the Court from 1283 (or even two centuries before, if Usatges of Barcelona is considered part of the compilation of Constitutions) until 1716, when Philip V of Spain gave the Nueva Planta decrees, finishing with the historical laws of Catalonia. These Constitutions were usually made formally as a royal initiative, but required for its approval or repeal the favorable vote of the Catalan Courts, the medieval antecedent of the modern Parliaments. These laws, like other modern constitutions, had preeminence over other laws, and they could not be contradicted by mere decrees or edicts of the king.\nThe \"Kouroukan Founga\" was a 13th-century charter of the Mali Empire in West Africa, reconstructed from oral tradition in 1988 by Siriman Kouyat\u00e9. It included the \"right to life and to the preservation of physical integrity\" and significant protections for women.\nThe Golden Bull of 1356 was a decree issued by a \"Reichstag\" in Nuremberg headed by Emperor Charles IV that fixed, for a period of more than four hundred years, an important aspect of the constitutional structure of the Holy Roman Empire.\nIn China, the Hongwu Emperor created and refined a document he called \"Ancestral Injunctions\" (first published in 1375, revised twice more before he died in 1398). These rules served as a constitution for the Ming dynasty for the next 250 years.\nThe oldest written document still governing a sovereign nation today is that of San Marino. The \"Leges Statutae Republicae Sancti Marini\" was written in Latin and consists of six books. The first book, with 62 articles, establishes councils, courts, various executive officers, and the powers assigned to them. The remaining books cover criminal and civil law and judicial procedures and remedies. Written in 1600, the document was based upon the \"Statuti Comunali\" (Town Statute) of 1300, itself influenced by the \"Codex Justinianus\", and it remains in force today.\nIn 1392 the \"Carta de Logu\" was legal code of the Giudicato of Arborea promulgated by the \"giudicessa\" Eleanor. It was in force in Sardinia until it was superseded by the code of Charles Felix in April 1827. The Carta was a work of great importance in Sardinian history. It was an organic, coherent, and systematic work of legislation encompassing the civil and penal law.\nThe \"Gayanashagowa\", the oral constitution of the Haudenosaunee nation also known as the Great Law of Peace, established a system of governance as far back as 1190 AD (though perhaps more recently at 1451) in which the Sachems, or tribal chiefs, of the Iroquois League's member nations made decisions on the basis of universal consensus of all chiefs following discussions that were initiated by a single nation. The position of Sachem descends through families and are allocated by the senior female clan heads, though, prior to the filling of the position, candidacy is ultimately democratically decided by the community itself.\nModern constitutions.\nIn 1634 the Kingdom of Sweden adopted the 1634 Instrument of Government, drawn up under the Lord High Chancellor of Sweden Axel Oxenstierna after the death of king Gustavus Adolphus. This can be seen as the first written constitution adopted by a modern state.\nEnglish civil war era.\nOn 4 January 1649, the Rump Parliament declared \"that the people are, under God, the original of all just power; that the Commons of England, being chosen by and representing the people, have the supreme power in this nation\".\nThe English Protectorate set up by Oliver Cromwell after the English Civil War promulgated the first detailed written constitution adopted by a modern state; it was called the Instrument of Government. This formed the basis of government for the short-lived republic from 1653 to 1657 by providing a legal rationale for the increasing power of Cromwell after Parliament consistently failed to govern effectively. Most of the concepts and ideas embedded into modern constitutional theory, especially bicameralism, separation of powers, the written constitution, and judicial review, can be traced back to the experiments of that period. Drafted by Major-General John Lambert in 1653, the \"Instrument of Government\" included elements incorporated from an earlier document \"Heads of Proposals\", which had been agreed to by the Army Council in 1647, as a set of propositions intended to be a basis for a constitutional settlement after King Charles I was defeated in the First English Civil War. Charles had rejected the propositions, but before the start of the Second Civil War, the Grandees of the New Model Army had presented the \"Heads of Proposals\" as their alternative to the more radical Agreement of the People presented by the Agitators and their civilian supporters at the Putney Debates. The \"Instrument of Government\" was adopted by Parliament on 15 December 1653, and Oliver Cromwell was installed as Lord Protector on the following day. The constitution set up a state council consisting of 21 members while executive authority was vested in the office of \"Lord Protector of the Commonwealth.\" This position was designated as a non-hereditary life appointment. The \"Instrument\" also required the calling of triennial Parliaments, with each sitting for at least five months.\nThe \"Instrument of Government\" was replaced in May 1657 by England's second, and last, codified constitution, the Humble Petition and Advice, proposed by Sir Christopher Packe. The Petition offered hereditary monarchy to Oliver Cromwell, asserted Parliament's control over issuing new taxation, provided an independent council to advise the king and safeguarded \"Triennial\" meetings of Parliament. A modified version of the Humble Petition with the clause on kingship removed was ratified on 25 May. This finally met its demise in conjunction with the death of Cromwell and the Restoration of the monarchy.\nBritish colonies in North America.\nIn 1639, the Colony of Connecticut adopted the Fundamental Orders, which was the first North American constitution. It is the basis for every new Connecticut constitution since, and is also the reason for Connecticut's nickname, \"the Constitution State\".\nAll of the British colonies in North America that were to become the 13 original United States, adopted their own constitutions in 1776 and 1777, during the American Revolution (and before the later Articles of Confederation and United States Constitution), with the exceptions of Massachusetts, Connecticut and Rhode Island. The Commonwealth of Massachusetts adopted its Constitution in 1780, the oldest still-functioning constitution of any U.S. state; while Connecticut and Rhode Island officially continued to operate under their old colonial charters, until they adopted their first state constitutions in 1818 and 1843, respectively.\nDemocratic constitutions: 18th century.\nWhat is sometimes called the \"enlightened constitution\" model was developed by philosophers of the Age of Enlightenment such as Thomas Hobbes, Jean-Jacques Rousseau, and John Locke. The model proposed that constitutional governments should be stable, adaptable, accountable, open and should represent the people (i.e., support democracy).\n\"Agreements and Constitutions of Laws and Freedoms of the Zaporizian Host\" was written in 1710 by Pylyp Orlyk, \"hetman\" of the Zaporozhian Host. It was written to establish a free Zaporozhian-Ukrainian Republic, with the support of Charles XII of Sweden. It is notable in that it established a democratic standard for the separation of powers in government between the legislative, executive, and judiciary branches, well before the publication of Montesquieu's \"Spirit of the Laws\". This Constitution also limited the executive authority of the \"hetman\", and established a democratically elected Cossack parliament called the General Council. However, Orlyk's project for an independent Ukrainian State never materialized, and his constitution, written in exile, never went into effect.\nCorsican Constitutions of 1755 and 1794 were inspired by Jean-Jacques Rousseau. The latter introduced universal suffrage for property owners.\nThe Swedish constitution of 1772 was enacted under King Gustavus III and was inspired by the separation of powers by Montesquieu. The king also cherished other enlightenment ideas (as an enlighted despot) and repealed torture, liberated agricultural trade, diminished the use of the death penalty and instituted a form of religious freedom. The constitution was commended by Voltaire.\nThe United States Constitution, ratified 21 June 1788, was influenced by the writings of Polybius, Locke, Montesquieu, and others. The document became a benchmark for republicanism and codified constitutions written thereafter.\nThe Polish\u2013Lithuanian Commonwealth Constitution was passed on 3 May 1791. Its draft was developed by the leading minds of the Enlightenment in Poland such as King Stanislaw August Poniatowski, Stanis\u0142aw Staszic, Scipione Piattoli, Julian Ursyn Niemcewicz, Ignacy Potocki and Hugo Ko\u0142\u0142\u0105taj. It was adopted by the Great Sejm and is considered the first constitution of its kind in Europe and the world's second oldest one after the American Constitution.\nAnother landmark document was the French Constitution of 1791.\nThe 1811 Constitution of Venezuela was the first Constitution of Venezuela and Latin America, promulgated and drafted by Crist\u00f3bal Mendoza and Juan Germ\u00e1n Roscio and in Caracas. It established a federal government but was repealed one year later.\nOn 19 March 1812, the Spanish Constitution of 1812 was ratified by a parliament gathered in Cadiz, the only Spanish continental city which was safe from French occupation. The Spanish Constitution served as a model for other liberal constitutions of several South European and Latin American nations, for example, the Portuguese Constitution of 1822, constitutions of various Italian states during Carbonari revolts (i.e., in the Kingdom of the Two Sicilies), the Norwegian constitution of 1814, or the Mexican Constitution of 1824.\nIn Brazil, the Constitution of 1824 expressed the option for the monarchy as a political system after Brazilian Independence. The leader of the national emancipation process was the Portuguese prince Pedro I, the elder son of the king of Portugal. Pedro was crowned in 1822 as the first emperor of Brazil. The country was ruled by a Constitutional Monarchy until 1889 when it adopted the Republican model.\nIn Denmark, as a result of the Napoleonic Wars, the absolute monarchy lost its personal possession of Norway to Sweden. Sweden had already enacted its 1809 Instrument of Government, which saw the division of power between the Riksdag, the king and the judiciary. However the Norwegians managed to infuse a radically democratic and liberal constitution in 1814, adopting many facets from the American constitution and the revolutionary French ones, but maintaining a hereditary monarch limited by the constitution, like the Spanish one.\nThe first Swiss Federal Constitution was put in force in September 1848 (with official revisions in 1878, 1891, 1949, 1971, 1982 and 1999).\nThe Serbian revolution initially led to a proclamation of a proto-constitution in 1811; the full-fledged Constitution of Serbia followed few decades later, in 1835. The first Serbian constitution (Sretenjski ustav) was adopted at the national assembly in Kragujevac on 15 February 1835.\nThe Constitution of Canada came into force on 1 July 1867, as the British North America Act, an act of the British Parliament. Over a century later, the BNA Act was patriated to the Canadian Parliament and augmented with the Canadian Charter of Rights and Freedoms. Apart from the \"Constitution Acts, 1867 to 1982\", Canada's constitution also has unwritten elements based in common law and convention.\nPrinciples of constitutional design.\nAfter tribal people first began to live in cities and establish nations, many of these functioned according to unwritten customs, while some developed autocratic, even tyrannical monarchs, who ruled by decree, or mere personal whim. Such rule led some thinkers to take the position that what mattered was not the design of governmental institutions and operations, as much as the character of the rulers. This view can be seen in Plato, who called for rule by \"philosopher-kings\". Later writers, such as Aristotle, Cicero and Plutarch, would examine designs for the government from a legal and historical standpoint.\nThe Renaissance brought a series of political philosophers who wrote implied criticisms of the practices of monarchs and sought to identify principles of constitutional design that would be likely to yield more effective and just governance from their viewpoints. This began with revival of the Roman law of nations concept and its application to the relations among nations, and they sought to establish customary \"laws of war and peace\" to ameliorate wars and make them less likely. This led to considerations of what authority monarchs or other officials have and don't have, from where that authority derives, and the remedies for the abuse of such authority.\nA seminal juncture in this line of discourse arose in England from the Civil War, the Cromwellian Protectorate, the writings of Thomas Hobbes, Samuel Rutherford, the Levellers, John Milton, and James Harrington, leading to the debate between Robert Filmer, arguing for the divine right of monarchs, on the one side, and on the other, Henry Neville, James Tyrrell, Algernon Sidney, and John Locke. What arose from the latter was a concept of government being erected on the foundations of first, a state of nature governed by natural laws, then a state of society, established by a social contract or compact, which bring underlying natural or social laws, before governments are formally established on them as foundations.\nAlong the way, several writers examined how the design of government was important, even if the government were headed by a monarch. They also classified various historical examples of governmental designs, typically into democracies, aristocracies, or monarchies, and considered how just and effective each tended to be and why, and how the advantages of each might be obtained by combining elements of each into a more complex design that balanced competing tendencies. Some, such as Montesquieu, also examined how the functions of government, such as legislative, executive, and judicial, might appropriately be separated into branches. The prevailing theme among these writers was that the design of constitutions was not completely arbitrary or a matter of taste. They generally held that there are underlying principles of design that constrain all constitutions for every polity or organization. Each built on the ideas of those before concerning what those principles might be.\nThe later writings of Orestes Brownson would try to explain what constitutional designers were trying to do. According to Brownson there are, in a sense, three \"constitutions\" involved: The first the \"constitution of nature\" that includes all of what was called \"natural law\". The second is the \"constitution of society\", an unwritten and commonly understood set of rules for the society formed by a social contract before it establishes a government, by which it establishes the third, a \"constitution of government\". The second would include such elements as the making of decisions by the public conventions called by public notice and conducted by established rules of procedure. Each constitution must be consistent with, and derive its authority from, the ones before it, as well as from a historical act of society formation or constitutional ratification. Brownson argued that a state is a society with effective dominion over a well-defined territory, that consent to a well-designed constitution of government arises from presence on that territory, and that provisions of a written constitution of government can be \"unconstitutional\" if they are inconsistent with the constitutions of nature or society. Brownson argued that it is not ratification alone that makes a written constitution of government legitimate, but that it must also be competently designed and applied.\nOther writers have argued that such considerations apply not only to all national constitutions of government, but also to the constitutions of private organizations, that it is not an accident that the constitutions that tend to satisfy their members contain certain elements, as a minimum, or that their provisions tend to become very similar as they are amended after experience with their use. Provisions that give rise to certain kinds of questions are seen to need additional provisions for how to resolve those questions, and provisions that offer no course of action may best be omitted and left to policy decisions. Provisions that conflict with what Brownson and others can discern are the underlying \"constitutions\" of nature and society tend to be difficult or impossible to execute, or to lead to unresolvable disputes.\nConstitutional design has been treated as a kind of metagame in which play consists of finding the best design and provisions for a written constitution that will be the rules for the game of government, and that will be most likely to optimize a balance of the utilities of justice, liberty, and security. An example is the metagame Nomic.\nPolitical economy theory regards constitutions as coordination devices that help citizens to prevent rulers from abusing power. If the citizenry can coordinate a response to police government officials in the face of a constitutional fault, then the government have the incentives to honor the rights that the constitution guarantees. An alternative view considers that constitutions are not enforced by the citizens at-large, but rather by the administrative powers of the state. Because rulers cannot themselves implement their policies, they need to rely on a set of organizations (armies, courts, police agencies, tax collectors) to implement it. In this position, they can directly sanction the government by refusing to cooperate, disabling the authority of the rulers. Therefore, constitutions could be characterized by a self-enforcing equilibria between the rulers and powerful administrators.\nKey features.\nMost commonly, the term \"constitution\" refers to a set of rules and principles that define the nature and extent of government. Most constitutions seek to regulate the relationship between institutions of the state, in a basic sense the relationship between the executive, legislature and the judiciary, but also the relationship of institutions within those branches. For example, executive branches can be divided into a head of government, government departments/ministries, executive agencies and a civil service/administration. Most constitutions also attempt to define the relationship between individuals and the state, and to establish the broad rights of individual citizens. It is thus the most basic law of a territory from which all the other laws and rules are hierarchically derived; in some territories it is in fact called \"Basic Law\".\nClassification.\nA fundamental classification is codification or lack of codification. A codified constitution is one that is contained in a single document, which is the single source of constitutional law in a state. An uncodified constitution is one that is not contained in a single document, consisting of several different sources, which may be written or unwritten; see constitutional convention.\nCodified constitution.\nMost states in the world have codified constitutions.\nCodified constitutions are often the product of some dramatic political change, such as a revolution. The process by which a country adopts a constitution is closely tied to the historical and political context driving this fundamental change. The legitimacy (and often the longevity) of codified constitutions has often been tied to the process by which they are initially adopted and some scholars have pointed out that high constitutional turnover within a given country may itself be detrimental to the separation of powers and the rule of law.\nStates that have codified constitutions normally give the constitution supremacy over ordinary statute law. That is, if there is any conflict between a legal statute and the codified constitution, all or part of the statute can be declared \"ultra vires\" by a court and struck down as unconstitutional. In addition, exceptional procedures are often required to amend a constitution. These procedures may include: the convocation of a special constituent assembly or constitutional convention, requiring a supermajority of legislators' votes, approval in two terms of parliament, the consent of regional legislatures, a referendum process, and/or other procedures that make amending a constitution more difficult than passing a simple law.\nConstitutions may also provide that their most basic principles can never be abolished, even by amendment. In case a formally valid amendment of a constitution infringes these principles protected against any amendment, it may constitute a so-called \"unconstitutional constitutional law\".\nCodified constitutions normally consist of a ceremonial preamble, which sets forth the goals of the state and the motivation for the constitution, and several articles containing the substantive provisions. The preamble, which is omitted in some constitutions, may contain a reference to God and/or to fundamental values of the state such as liberty, democracy or human rights. In ethnic nation-states such as Estonia, the mission of the state can be defined as preserving a specific nation, language and culture.\nUncodified constitution.\n only two sovereign states, New Zealand and the United Kingdom, have wholly uncodified constitutions. The Basic Laws of Israel have since 1950 been intended to be the basis for a constitution, but as of 2017 it had not been drafted. The various Laws are considered to have precedence over other laws, and give the procedure by which they can be amended, typically by a simple majority of members of the Knesset (parliament).\nUncodified constitutions are the product of an \"evolution\" of laws and conventions over centuries (such as in the Westminster System that developed in Britain). By contrast to codified constitutions, uncodified constitutions include both written sources \u2013 e.g. constitutional statutes enacted by the Parliament \u2013 and unwritten sources \u2013 constitutional conventions, observation of precedents, royal prerogatives, customs and traditions, such as holding general elections on Thursdays; together these constitute British constitutional law.\nMixed constitutions.\nSome constitutions are largely, but not wholly, codified. For example, in the Constitution of Australia, most of its fundamental political principles and regulations concerning the relationship between branches of government, and concerning the government and the individual are codified in a single document, the Constitution of the Commonwealth of Australia. However, the presence of statutes with constitutional significance, namely the Statute of Westminster, as adopted by the Commonwealth in the Statute of Westminster Adoption Act 1942, and the Australia Act 1986 means that Australia's constitution is not contained in a single constitutional document. It means the Constitution of Australia is uncodified, it also contains constitutional conventions, thus is partially unwritten.\nThe Constitution of Canada resulted from the passage of several British North America Acts from 1867 to the Canada Act 1982, the act that formally severed British Parliament's ability to amend the Canadian constitution. The Canadian constitution includes specific legislative acts as mentioned in section 52(2) of the Constitution Act, 1982. However, some documents not explicitly listed in section 52(2) are also considered constitutional documents in Canada, entrenched via reference; such as the Proclamation of 1763. Although Canada's constitution includes a number of different statutes, amendments, and references, some constitutional rules that exist in Canada are derived from unwritten sources and constitutional conventions.\nThe terms \"written constitution\" and \"codified constitution\" are often used interchangeably, as are \"unwritten constitution\" and \"uncodified constitution\", although this usage is technically inaccurate. A codified constitution is a single document; states that do not have such a document have uncodified, but not entirely unwritten, constitutions, since much of an uncodified constitution is usually written in laws such as the Basic Laws of Israel and the Parliament Acts of the United Kingdom. Uncodified constitutions largely lack protection against amendment by the government of the time. For example, the U.K. Fixed-term Parliaments Act 2011 legislated by simple majority for strictly fixed-term parliaments; until then the ruling party could call a general election at any convenient time up to the maximum term of five years. This change would require a constitutional amendment in most nations.\nAmendments.\nA constitutional amendment is a modification of the constitution of a polity, organization or other type of entity. Amendments are often interwoven into the relevant sections of an existing constitution, directly altering the text. Conversely, they can be appended to the constitution as supplemental additions (codicils), thus changing the frame of government without altering the existing text of the document.\nMost constitutions require that amendments cannot be enacted unless they have passed a special procedure that is more stringent than that required of ordinary legislation.\nMethods of amending.\n\"Some countries are listed under more than one method because alternative procedures may be used.\"\nEntrenched clauses.\nAn entrenched clause or entrenchment clause of a basic law or constitution is a provision that makes certain amendments either more difficult or impossible to pass, making such amendments inadmissible. Overriding an entrenched clause may require a supermajority, a referendum, or the consent of the minority party. For example, the U.S. Constitution has an entrenched clause that prohibits abolishing equal suffrage of the States within the Senate without their consent. The term eternity clause is used in a similar manner in the constitutions of the Czech Republic, Germany, Turkey, Greece, Italy, Morocco, the Islamic Republic of Iran, Brazil and Norway. India's constitution does not contain specific provisions on entrenched clauses but the basic structure doctrine makes it impossible for certain basic features of the Constitution to be altered or destroyed by the Parliament of India through an amendment. The Constitution of Colombia also lacks explicit entrenched clauses, but has a similar substantive limit on amending its fundamental principles through judicial interpretations.\nConstitutional rights and duties.\nConstitutions include various rights and duties. These include the following:\nSeparation of powers.\nConstitutions usually explicitly divide power between various branches of government. The standard model, described by the Baron de Montesquieu, involves three branches of government: executive, legislative and judicial. Some constitutions include additional branches, such as an auditory branch. Constitutions vary extensively as to the degree of separation of powers between these branches.\nAccountability.\nIn presidential and semi-presidential systems of government, department secretaries/ministers are accountable to the president, who has patronage powers to appoint and dismiss ministers. The president is accountable to the people in an election.\nIn parliamentary systems, Cabinet Ministers are accountable to Parliament, but it is the prime minister who appoints and dismisses them. In the case of the United Kingdom and other countries with a monarchy, it is the monarch who appoints and dismisses ministers, on the advice of the prime minister. In turn the prime minister will resign if the government loses the confidence of the parliament (or a part of it). Confidence can be lost if the government loses a vote of no confidence or, depending on the country, loses a particularly important vote in parliament, such as vote on the budget. When a government loses confidence, it stays in office until a new government is formed; something which normally but not necessarily required the holding of a general election.\nOther independent institutions.\nOther independent institutions which some constitutions have set out include a central bank, an anti-corruption commission, an electoral commission, a judicial oversight body, a human rights commission, a media commission, an ombudsman, and a truth and reconciliation commission.\nPower structure.\nConstitutions also establish where sovereignty is located in the state. There are three basic types of distribution of sovereignty according to the degree of centralisation of power: unitary, federal, and confederal. The distinction is not absolute.\nIn a unitary state, sovereignty resides in the state itself, and the constitution determines this. The territory of the state may be divided into regions, but they are not sovereign and are subordinate to the state. In the UK, the constitutional doctrine of Parliamentary sovereignty dictates that sovereignty is ultimately contained at the centre. Some powers have been devolved to Northern Ireland, Scotland, and Wales (but not England). Some unitary states (Spain is an example) devolve more and more power to sub-national governments until the state functions in practice much like a federal state.\nA federal state has a central structure with at most a small amount of territory mainly containing the institutions of the federal government, and several regions (called \"states\", \"provinces\", etc.) which compose the territory of the whole state. Sovereignty is divided between the centre and the constituent regions. The constitutions of Canada and the United States establish federal states, with power divided between the federal government and the provinces or states. Each of the regions may in turn have its own constitution (of unitary nature).\nA confederal state comprises again several regions, but the central structure has only limited coordinating power, and sovereignty is located in the regions. Confederal constitutions are rare, and there is often dispute to whether so-called \"confederal\" states are actually federal.\nTo some extent a group of states which do not constitute a federation as such may by treaties and accords give up parts of their sovereignty to a supranational entity. For example, the countries constituting the European Union have agreed to abide by some Union-wide measures which restrict their absolute sovereignty in some ways, e.g., the use of the metric system of measurement instead of national units previously used.\nState of emergency.\nMany constitutions allow the declaration under exceptional circumstances of some form of state of emergency during which some rights and guarantees are suspended. This provision can be and has been abused to allow a government to suppress dissent without regard for human rights\u00a0\u2013 see the article on state of emergency.\nFacade constitutions.\nItalian political theorist Giovanni Sartori noted the existence of national constitutions which are a facade for authoritarian sources of power. While such documents may express respect for human rights or establish an independent judiciary, they may be ignored when the government feels threatened, or never put into practice. An extreme example was the Constitution of the Soviet Union that on paper supported freedom of assembly and freedom of speech; however, citizens who transgressed unwritten limits were summarily imprisoned. The example demonstrates that the protections and benefits of a constitution are ultimately provided not through its written terms but through deference by government and society to its principles. A constitution may change from being real to a facade and back again as democratic and autocratic governments succeed each other.\nConstitutional courts.\nConstitutions are often, but by no means always, protected by a legal body whose job it is to interpret those constitutions and, where applicable, declare void executive and legislative acts which infringe the constitution. In some countries, such as Germany, this function is carried out by a dedicated constitutional court which performs this (and only this) function. In other countries, such as Ireland, the ordinary courts may perform this function in addition to their other responsibilities. While elsewhere, like in the United Kingdom, the concept of declaring an act to be unconstitutional does not exist.\nA constitutional violation is an action or legislative act that is judged by a constitutional court to be contrary to the constitution, that is, unconstitutional. An example of constitutional violation by the executive could be a public office holder who acts outside the powers granted to that office by a constitution. An example of constitutional violation by the legislature is an attempt to pass a law that would contradict the constitution, without first going through the proper constitutional amendment process.\nSome countries, mainly those with uncodified constitutions, have no such courts at all. For example, the United Kingdom has traditionally operated under the principle of parliamentary sovereignty under which the laws passed by United Kingdom Parliament could not be questioned by the courts."}
{"id": "5254", "revid": "35041181", "url": "https://en.wikipedia.org/wiki?curid=5254", "title": "Common law", "text": "Common law (also known as judicial precedent, judge-made law, or case law) is the body of law primarily developed through judicial decisions rather than statutes. Although common law may incorporate certain statutes, it is largely based on precedent\u2014judicial rulings made in previous similar cases. The presiding judge determines which precedents to apply in deciding each new case.\nCommon law is deeply rooted in stare decisis (\"to stand by things decided\"), where courts follow precedents established by previous decisions. When a similar case has been resolved, courts typically align their reasoning with the precedent set in that decision. However, in a \"case of first impression\" with no precedent or clear legislative guidance, judges are empowered to resolve the issue and establish new precedent.\nThe common law, so named because it was \"common\" to all the king's courts across England, originated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066. It established a unified legal system, gradually supplanting the local folk courts and manorial courts. England spread the English legal system across the British Isles, first to Wales, and then to Ireland and overseas colonies; this was continued by the later British Empire. Many former colonies retain the common law system today. These common law systems are legal systems that give great weight to judicial precedent, and to the style of reasoning inherited from the English legal system. Today, approximately one-third of the world's population lives in common law jurisdictions or in mixed legal systems that integrate common law and civil law.\nTerminology.\nAccording to \"Black's Law Dictionary,\" common law is \"the body of law derived from judicial decisions, rather than from statutes or constitutions.\" Legal systems that rely on common law as precedent are known as \"common law jurisdictions.\"\nUntil the early 20th century, common law was widely considered to derive its authority from ancient Anglo-Saxon customs. Well into the 19th century, common law was still defined as \"unwritten law\" (\"lex non scripta\") in legal dictionaries including \"Bouvier's Law Dictionary\" and \"Black's Law Dictionary\". According to William Blackstone's declaratory theory the common law reaffirmed pre-existing customs but did not make new law. The term \"judge-made law\" was introduced by Jeremy Bentham as a criticism of this pretense of the legal profession. \nMany notable writers, including A. V. Dicey, William Markby, Oliver Wendell Holmes, John Austin, Roscoe Pound, and Ezra Ripley Thayer, eventually adopted the modern definition of common law as \"case law\" or \"ratio decidendi,\" which serves as binding precedent.\nBasic principles of common law.\nCommon law adjudication.\nIn a common law jurisdiction several stages of research and analysis are required to determine \"what the law is\" in a given situation. First, one must ascertain the facts. Then, one must locate any relevant statutes and cases. Then one must extract the principles, analogies and statements by various courts of what they consider important to determine how the next court is likely to rule on the facts of the present case. More recent decisions, and decisions of higher courts or legislatures carry more weight than earlier cases and those of lower courts. Finally, one integrates all the lines drawn and reasons given, and determines \"what the law is\". Then, one applies that law to the facts.\nIn practice, common law systems are considerably more complicated than the simplified system described above. The decisions of a court are binding only in a particular jurisdiction, and even within a given jurisdiction, some courts have more power than others. For example, in most jurisdictions, decisions by appellate courts are binding on lower courts in the same jurisdiction, and on future decisions of the same appellate court, but decisions of lower courts are only non-binding persuasive authority. Interactions between common law, constitutional law, statutory law and regulatory law also give rise to considerable complexity.\nCommon law evolves to meet changing social needs and improved understanding.\nOliver Wendell Holmes Jr. cautioned that \"the proper derivation of general principles in both common and constitutional law ... arise gradually, in the emergence of a consensus from a multitude of particularized prior decisions\". Justice Cardozo noted the \"common law does not work from pre-established truths of universal and inflexible validity to conclusions derived from them deductively\", but \"[i]ts method is inductive, and it draws its generalizations from particulars\".\nThe common law is more malleable than statutory law. First, common law courts are not absolutely bound by precedent, but can (when extraordinarily good reason is shown) reinterpret and revise the law, without legislative intervention, to adapt to new trends in political, legal and social philosophy. Second, the common law evolves through a series of gradual steps, that gradually works out all the details, so that over a decade or more, the law can change substantially but without a sharp break, thereby reducing disruptive effects. In contrast to common law incrementalism, the legislative process is very difficult to get started, as the work begins much earlier than just introducing a bill. Once the legislation is introduced, the process to getting it passed is long, involving the committee system, debate, the potential of conference committee, voting, and President approval. Because of the involved process, many pieces must fall into place in order for it to be passed.\nOne example of the gradual change that typifies evolution of the common law is the gradual change in liability for negligence. The traditional common law rule through most of the 19th century was that a plaintiff could not recover for a defendant's negligent production or distribution of a harmful instrumentality unless the two were parties to a contract (privity of contract). Thus, only the immediate purchaser could recover for a product defect, and if a part was built up out of parts from parts manufacturers, the ultimate buyer could not recover for injury caused by a defect in the part. In an 1842 English case, \"Winterbottom v Wright\", the postal service had contracted with Wright to maintain its coaches. Winterbottom was a driver for the post. When the coach failed and injured Winterbottom, he sued Wright. The \"Winterbottom\" court recognized that there would be \"absurd and outrageous consequences\" if an injured person could sue any person peripherally involved, and knew it had to draw a line somewhere, a limit on the causal connection between the negligent conduct and the injury. The court looked to the contractual relationships, and held that liability would only flow as far as the person in immediate contract (\"privity\") with the negligent party.\nA first exception to this rule arose in 1852, in the case of \"Thomas v. Winchester\", when New York's highest court held that mislabeling a poison as an innocuous herb, and then selling the mislabeled poison through a dealer who would be expected to resell it, put \"human life in imminent danger\". \"Thomas\" relied on this reason to create an exception to the \"privity\" rule. In 1909, New York held in \"Statler v. Ray Mfg. Co.\" that a coffee urn manufacturer was liable to a person injured when the urn exploded, because the urn \"was of such a character inherently that, when applied to the purposes for which it was designed, it was liable to become a source of great danger to many people if not carefully and properly constructed\".\nYet the privity rule survived. In \"Cadillac Motor Car Co. v. Johnson\" (decided in 1915 by the federal appeals court for New York and several neighboring states), the court held that a car owner could not recover for injuries from a defective wheel, when the automobile owner had a contract only with the automobile dealer and not with the manufacturer, even though there was \"no question that the wheel was made of dead and 'dozy' wood, quite insufficient for its purposes\". The \"Cadillac\" court was willing to acknowledge that the case law supported exceptions for \"an article dangerous in its nature or likely to become so in the course of the ordinary usage to be contemplated by the vendor\". However, held the \"Cadillac\" court, \"one who manufactures articles dangerous only if defectively made, or installed, e.g., tables, chairs, pictures or mirrors hung on the walls, carriages, automobiles, and so on, is not liable to third parties for injuries caused by them, except in case of willful injury or fraud\".\nFinally, in the famous case of \"MacPherson v. Buick Motor Co.\", in 1916, Judge Benjamin Cardozo for New York's highest court pulled a broader principle out of these predecessor cases. The facts were almost identical to \"Cadillac\" a year earlier: a wheel from a wheel manufacturer was sold to Buick, to a dealer, to MacPherson, and the wheel failed, injuring MacPherson. Judge Cardozo held:\nCardozo's new \"rule\" exists in no prior case, but is inferrable as a synthesis of the \"thing of danger\" principle stated in them, merely extending it to \"foreseeable danger\" even if \"the purposes for which it was designed\" were not themselves \"a source of great danger\". \"MacPherson\" takes some care to present itself as foreseeable progression, not a wild departure. Cardozo continues to adhere to the original principle of \"Winterbottom\", that \"absurd and outrageous consequences\" must be avoided, and he does so by drawing a new line in the last sentence quoted above: \"There must be knowledge of a danger, not merely possible, but probable.\" But while adhering to the underlying principle that \"some\" boundary is necessary, \"MacPherson\" overruled the prior common law by rendering the formerly dominant factor in the boundary, that is, the privity formality arising out of a contractual relationship between persons, totally irrelevant. Rather, the most important factor in the boundary would be the nature of the thing sold and the foreseeable uses that downstream purchasers would make of the thing.\nThe example of the evolution of the law of negligence in the preceding paragraphs illustrates two crucial principles: (a) The common law evolves, this evolution is in the hands of judges, and judges have \"made law\" for hundreds of years. (b) The reasons given for a decision are often more important in the long run than the outcome in a particular case. This is the reason that judicial opinions are usually quite long, and give rationales and policies that can be balanced with judgment in future cases, rather than the bright-line rules usually embodied in statutes.\nPublication of decisions.\nIn common law systems, precedents are maintained over time through court records and historically documented in collections of case law referred to as yearbooks and law reports.\nAfter the American Revolution, Massachusetts became the first state to establish an official Reporter of Decisions. As newer states needed law, they often looked first to the Massachusetts Reports for authoritative precedents as a basis for their own common law. The United States federal courts relied on private publishers until after the Civil War, and only began publishing as a government function in 1874. West Publishing in Minnesota is the largest private-sector publisher of law reports in the United States. Government publishers typically issue only decisions \"in the raw\", while private sector publishers often add indexing, including references to the key principles of the common law involved, editorial analysis, and similar finding aids.\nComparison with statutory law.\nStatutes are generally understood to supersede common law. They may codify existing common law, create new causes of action that did not exist in the common law, or legislatively overrule the common law. Common law still has practical applications in some areas of law. Examples are contract law and the law of torts. \n\"Legislating from the bench\".\nAt earlier stages in the development of modern legal systems and government, courts exercised their authority in performing what Roscoe Pound described as an essentially legislative function. As legislation became more comprehensive, courts began to operate within narrower limits of statutory interpretation.\nJeremy Bentham famously criticized judicial lawmaking when he argued in favor of codification and narrow judicial decisions. Pound comments that critics of judicial lawmaking are not always consistent - sometimes siding with Bentham and decrying judicial overreach, at other times unsatisfied with judicial reluctance to sweep broadly and employ case law as a means to redress certain challenges to established law. Oliver Wendell Holmes once dissented: \"judges do and must legislate\".\nStatutory construction.\nThere is a controversial legal maxim in American law that \"Statutes in derogation of the common law ought to be narrowly construed\". Henry Campbell Black once wrote that the canon \"no longer has any foundation in reason\". It is generally associated with the Lochner era. \nThe presumption is that legislatures may take away common law rights, but modern jurisprudence will look for the statutory purpose or legislative intent and apply rules of statutory construction like the plain meaning rule to reach decisions. As the United States Supreme Court explained in \"United States v Texas\", 507 U.S. 529 (1993):\nAs another example, the Supreme Court of the United States in 1877, held that a Michigan statute that established rules for solemnization of marriages did not abolish pre-existing common-law marriage, because the statute did not affirmatively require statutory solemnization and was silent as to preexisting common law.\nCourt decisions that analyze, interpret and determine the fine boundaries and distinctions in law promulgated by other bodies are sometimes called \"interstitial common law,\" which includes judicial interpretation of fundamental laws, such as the US Constitution, of legislative statutes, and of agency regulations, and the application of law to specific facts.\nOverruling precedent\u2014the limits of \"stare decisis\".\nThe United States federal courts are divided into twelve regional circuits, each with a circuit court of appeals (plus a thirteenth, the Court of Appeals for the Federal Circuit, which hears appeals in patent cases and cases against the federal government, without geographic limitation). Decisions of one circuit court are binding on the district courts within the circuit and on the circuit court itself, but are only persuasive authority on sister circuits. District court decisions are not binding precedent at all, only persuasive.\nMost of the U.S. federal courts of appeal have adopted a rule under which, in the event of any conflict in decisions of panels (most of the courts of appeal almost always sit in panels of three), the earlier panel decision is controlling, and a panel decision may only be overruled by the court of appeals sitting \"en banc\" (that is, all active judges of the court) or by a higher court. In these courts, the older decision remains controlling when an issue comes up the third time.\nOther courts, for example, the Court of Appeals for the Federal Circuit (formerly known as Court of Customs and Patent Appeals) and the US Supreme Court, always sit \"en banc\", and thus the \"later\" decision controls. These courts essentially overrule all previous cases in each new case, and older cases survive only to the extent they do not conflict with newer cases. The interpretations of these courts\u2014for example, Supreme Court interpretations of the constitution or federal statutes\u2014are stable only so long as the older interpretation maintains the support of a majority of the court. Older decisions persist through some combination of belief that the old decision is right, and that it is not sufficiently wrong to be overruled.\nIn the jurisdictions of England and Wales and of Northern Ireland, since 2009, the Supreme Court of the United Kingdom has the authority to overrule and unify criminal law decisions of lower courts; it is the final court of appeal for civil law cases in all three of the UK jurisdictions, but not for criminal law cases in Scotland, where the High Court of Justiciary has this power instead (except on questions of law relating to reserved matters such as devolution and human rights). From 1966 to 2009, this power lay with the House of Lords, granted by the Practice Statement of 1966.\nCanada's federal system, described below, avoids regional variability of federal law by giving national jurisdiction to both layers of appellate courts.\nCommon law as a foundation for commercial economies.\nThe reliance on judicial opinion is a strength of common law systems, and is a significant contributor to the robust commercial systems in the United Kingdom and United States. Because there is reasonably precise guidance on almost every issue, parties (especially commercial parties) can predict whether a proposed course of action is likely to be lawful or unlawful, and have some assurance of consistency. As Justice Brandeis famously expressed it, \"in most matters it is more important that the applicable rule of law be settled than that it be settled right.\" This ability to predict gives more freedom to come close to the boundaries of the law. For example, many commercial contracts are more economically efficient, and create greater wealth, because the parties know ahead of time that the proposed arrangement, though perhaps close to the line, is almost certainly legal. Newspapers, taxpayer-funded entities with some religious affiliation, and political parties can obtain fairly clear guidance on the boundaries within which their freedom of expression rights apply.\nIn contrast, in jurisdictions with very weak respect for precedent, fine questions of law are redetermined anew each time they arise, making consistency and prediction more difficult, and procedures far more protracted than necessary because parties cannot rely on written statements of law as reliable guides. In jurisdictions that do not have a strong allegiance to a large body of precedent, parties have less \"a priori\" guidance (unless the written law is very clear and kept updated) and must often leave a bigger \"safety margin\" of unexploited opportunities, and final determinations are reached only after far larger expenditures on legal fees by the parties.\nThis is the reason for the frequent choice of the law of the State of New York in commercial contracts, even when neither entity has extensive contacts with New York\u2014and remarkably often even when neither party has contacts with the United States. Commercial contracts almost always include a \"choice of law clause\" to reduce uncertainty. Somewhat surprisingly, contracts throughout the world (for example, contracts involving parties in Japan, France and Germany, and from most of the other states of the United States) often choose the law of New York, even where the relationship of the parties and transaction to New York is quite attenuated. Because of its history as the United States' commercial center, New York common law has a depth and predictability not (yet) available in any other jurisdictions of the United States. Similarly, American corporations are often formed under Delaware corporate law, and American contracts relating to corporate law issues (merger and acquisitions of companies, rights of shareholders, and so on) include a Delaware choice of law clause, because of the deep body of law in Delaware on these issues. On the other hand, some other jurisdictions have sufficiently developed bodies of law so that parties have no real motivation to choose the law of a foreign jurisdiction (for example, England and Wales, and the state of California), but not yet so fully developed that parties with no relationship to the jurisdiction choose that law. Outside the United States, parties that are in different jurisdictions from each other often choose the law of England and Wales, particularly when the parties are each in former British colonies and members of the Commonwealth. The common theme in all cases is that commercial parties seek predictability and simplicity in their contractual relations, and frequently choose the law of a common law jurisdiction with a well-developed body of common law to achieve that result.\nLikewise, for litigation of commercial disputes arising out of unpredictable torts (as opposed to the prospective choice of law clauses in contracts discussed in the previous paragraph), certain jurisdictions attract an unusually high fraction of cases, because of the predictability afforded by the depth of decided cases. For example, London is considered the pre-eminent centre for litigation of admiralty cases.\nThis is not to say that common law is better in every situation. For example, civil law can be clearer than case law when the legislature has had the foresight and diligence to address the precise set of facts applicable to a particular situation. For that reason, civil law statutes tend to be somewhat more detailed than statutes written by common law legislatures\u2014but, conversely, that tends to make the statute more difficult to read.\nHistory.\nOrigins.\nThe common lawso named because it was \"common\" to all the king's courts across Englandoriginated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066. Prior to the Norman Conquest, much of England's legal business took place in the local folk courts of its various shires and hundreds. A variety of other individual courts also existed across the land: urban boroughs and merchant fairs held their own courts, and large landholders also held their own manorial and seigniorial courts as needed. The degree to which common law drew from earlier Anglo-Saxon traditions such as the jury, ordeals, the penalty of outlawry, and writs all of which were incorporated into the Norman common law is still a subject of much discussion. Additionally, the Catholic Church operated its own court system that adjudicated issues of canon law.\nThe main sources for the history of the common law in the Middle Ages are the plea rolls and the Year Books. The plea rolls, which were the official court records for the Courts of Common Pleas and King's Bench, were written in Latin. The rolls were made up in bundles by law term: Hilary, Easter, Trinity, and Michaelmas, or winter, spring, summer, and autumn. They are currently deposited in the UK National Archives, by whose permission images of the rolls for the Courts of Common Pleas, King's Bench, and Exchequer of Pleas, from the 13th century to the 17th, can be viewed online at the Anglo-American Legal Tradition site (The O'Quinn Law Library of the University of Houston Law Center).\nThe doctrine of precedent developed during the 12th and 13th centuries, as the collective judicial decisions that were based in tradition, custom and precedent.\nThe form of reasoning used in common law is known as casuistry or case-based reasoning. The common law, as applied in civil cases (as distinct from criminal cases), was devised as a means of compensating someone for wrongful acts known as torts, including both intentional torts and torts caused by negligence, and as developing the body of law recognizing and regulating contracts. The type of procedure practiced in common law courts is known as the adversarial system; this is also a development of the common law.\nMedieval English common law.\nIn 1154, Henry II became the first Plantagenet king. Among many achievements, Henry institutionalized common law by creating a unified system of law \"common\" to the country through incorporating and elevating local custom to the national, ending local control and peculiarities, eliminating arbitrary remedies and reinstating a jury system\u2014citizens sworn on oath to investigate reliable criminal accusations and civil claims. The jury reached its verdict through evaluating common local knowledge, not necessarily through the presentation of evidence, a distinguishing factor from today's civil and criminal court systems.\nAt the time, royal government centered on the \"Curia Regis\" (king's court), the body of aristocrats and prelates who assisted in the administration of the realm and the ancestor of Parliament, the Star Chamber, and Privy Council. Henry II developed the practice of sending judges (numbering around 20 to 30 in the 1180s) from his Curia Regis to hear the various disputes throughout the country, and return to the court thereafter. The king's itinerant justices would generally receive a writ or commission under the great seal. They would then resolve disputes on an ad hoc basis according to what they interpreted the customs to be. The king's judges would then return to London and often discuss their cases and the decisions they made with the other judges. These decisions would be recorded and filed. In time, a rule, known as \"stare decisis\" (also commonly known as precedent) developed, whereby a judge would be bound to follow the decision of an earlier judge; he was required to adopt the earlier judge's interpretation of the law and apply the same principles promulgated by that earlier judge if the two cases had similar facts to one another. Once judges began to regard each other's decisions to be binding precedent, the pre-Norman system of local customs and law varying in each locality was replaced by a system that was (at least in theory, though not always in practice) common throughout the whole country, hence the name \"common law\".\nThe king's object was to preserve public order, but providing law and order was also extremely profitable \u2013 cases on forest use as well as fines and forfeitures can generate \"great treasure\" for the government. Eyres (a Norman French word for judicial circuit, originating from Latin \"iter\") are more than just courts; they would supervise local government, raise revenue, investigate crimes, and enforce feudal rights of the king. There were complaints of the \"eyre\" of 1198 reducing the kingdom to poverty and Cornishmen fleeing to escape the eyre of 1233.\nHenry II's creation of a powerful and unified court system, which curbed somewhat the power of canonical (church) courts, brought him (and England) into conflict with the church, most famously with Thomas Becket, the Archbishop of Canterbury. The murder of the archbishop gave rise to a wave of popular outrage against the King. International pressure on Henry grew, and in May 1172 he negotiated a settlement with the papacy in which the King swore to go on crusade as well as effectively overturned the more controversial clauses of the Constitutions of Clarendon. Henry nevertheless continued to exert influence in any ecclesiastical case which interested him and royal power was exercised more subtly with considerable success.\nThe English Court of Common Pleas was established after Magna Carta to try lawsuits between commoners in which the monarch had no interest. Its judges sat in open court in the Great Hall of the king's Palace of Westminster, permanently except in the vacations between the four terms of the Legal year.\nJudge-made common law operated as the primary source of law for several hundred years, before Parliament acquired legislative powers to create statutory law. In England, judges have devised a number of rules as to how to deal with precedent decisions. The early development of case-law in the thirteenth century has been traced to Bracton's \"On the Laws and Customs of England\" and led to the yearly compilations of court cases known as Year Books, of which the first extant was published in 1268, the same year that Bracton died. The Year Books are known as the law reports of medieval England, and are a principal source for knowledge of the developing legal doctrines, concepts, and methods in the period from the 13th to the 16th centuries, when the common law developed into recognizable form.\nInfluence of Roman law.\nThe term \"common law\" is often used as a contrast to Roman-derived \"civil law\", and the fundamental processes and forms of reasoning in the two are quite different. Nonetheless, there has been considerable cross-fertilization of ideas, while the two traditions and sets of foundational principles remain distinct.\nBy the time of the rediscovery of the Roman law in Europe in the 12th and 13th centuries, the common law had already developed far enough to prevent a Roman law reception as it occurred on the continent. However, the first common law scholars, most notably Glanvill and Bracton, as well as the early royal common law judges, had been well accustomed with Roman law. Often, they were clerics trained in the Roman canon law. One of the first and throughout its history one of the most significant treatises of the common law, Bracton's \"De Legibus et Consuetudinibus Angliae\" (On the Laws and Customs of England), was heavily influenced by the division of the law in Justinian's \"Institutes\". The impact of Roman law had decreased sharply after the age of Bracton, but the Roman divisions of actions into \"in rem\" (typically, actions against a \"thing\" or property for the purpose of gaining title to that property; must be filed in a court where the property is located) and \"in personam\" (typically, actions directed against a person; these can affect a person's rights and, since a person often owns things, his property too) used by Bracton had a lasting effect and laid the groundwork for a return of Roman law structural concepts in the 18th and 19th centuries. Signs of this can be found in Blackstone's \"Commentaries on the Laws of England\", and Roman law ideas regained importance with the revival of academic law schools in the 19th century. As a result, today, the main systematic divisions of the law into property, contract, and tort (and to some extent unjust enrichment) can be found in the civil law as well as in the common law.\nEarly modern era.\nThe \"ancient unwritten universal custom\" view was the foundation of the first treatises by Blackstone and Coke, and was universal among lawyers and judges from the earliest times to the mid-19th century. However, for 100 years, lawyers and judges have recognized that the \"ancient unwritten universal custom\" view does not accord with the facts of the origin and growth of the law. \nWest's encyclopedia of American law, defines common law as \"The ancient law of England based upon societal customs and recognized and enforced by the judgments and decrees of the courts.\"\nCoke.\nThe first attempt at a comprehensive compilation of centuries of common law was by Lord Chief Justice Edward Coke, in his treatise, \"Institutes of the Lawes of England\" in the 17th century.\nAs Sir Edward Coke (1552\u20131634) put it in the preface to the eighth volume of his \"Reports\" (1600\u20131615), 'the grounds of our common laws' were 'beyond the memorie or register of any beginning.\nBlackstone.\nAccording to William Blackstone the unwritten law derived its authority from immemorial usage and 'universal reception throughout the kingdom' While its precise meaning may have changed since Blackstone's time, in modern usage it is generally understood to mean law that is independent of statutes. This was repeated by the United States Supreme Court in \"Levy v. McCartee\": \"It is too plain for argument that the common law is here spoken of, in its appropriate sense, as the unwritten law of the land, independent of statutory enactments\". \nMore specifically, in modern usage, this is understood to mean law that is made by judges, not the declaratory statutes of Blackstone's era.\nJeremy Bentham.\nThe term \"judge made law\" comes from Jeremy Bentham and the modern practice of adjudication as application of precedent derived from case law begins with Jeremy Bentham's attack on the legitimacy of the common law. The modern legal practice of applying case law as precedent made obsolete the declaratory theory of common law that prevailed in Blackstone's time.\nPropagation of the common law to the colonies and Commonwealth by reception statutes.\nA reception statute is a statutory law adopted as a former British colony becomes independent, by which the new nation adopts (i.e. receives) pre-independence common law, to the extent not explicitly rejected by the legislative body or constitution of the new nation. Reception statutes generally consider the English common law dating prior to independence, and the precedent originating from it, as the default law, because of the importance of using an extensive and predictable body of law to govern the conduct of citizens and businesses in a new state. All U.S. states, with the partial exception of Louisiana, have either implemented reception statutes or adopted the common law by judicial opinion.\nOther examples of reception statutes in the United States, the states of the U.S., Canada and its provinces, and Hong Kong, are discussed in the reception statute article.\nYet, adoption of the common law in the newly independent United States was not a foregone conclusion, and was controversial. Immediately after the American Revolution, there was widespread distrust and hostility to anything British, and the common law was no exception. Jeffersonians decried lawyers and their common law tradition as threats to the new republic. The Jeffersonians preferred a legislatively enacted civil law under the control of the political process, rather than the common law developed by judges that\u2014by design\u2014were insulated from the political process. The Federalists believed that the common law was the birthright of Independence: after all, the natural rights to \"life, liberty, and the pursuit of happiness\" were the rights protected by common law. Even advocates for the common law approach noted that it was not an ideal fit for the newly independent colonies: judges and lawyers alike were severely hindered by a lack of printed legal materials. Before Independence, the most comprehensive law libraries had been maintained by Tory lawyers, and those libraries vanished with the loyalist expatriation, and the ability to print books was limited. Lawyer (later President) John Adams complained that he \"suffered very much for the want of books\". To bootstrap this most basic need of a common law system\u2014knowable, written law\u2014in 1803, lawyers in Massachusetts donated their books to found a law library. A Jeffersonian newspaper criticized the library, as it would carry forward \"all the old authorities practiced in England for centuries back ... whereby a new system of jurisprudence [will be founded] on the high monarchical system [to] become the Common Law of this Commonwealth... [The library] may hereafter have a very unsocial purpose.\"\nFor several decades after independence, English law still exerted influence over American common law\u2014for example, with \"Byrne v Boadle\" (1863), which first applied the res ipsa loquitur doctrine.\nDecline of Latin maxims and \"blind imitation of the past\", and adding flexibility to \"stare decisis\".\nWell into the 19th century, ancient maxims played a large role in common law adjudication. Many of these maxims had originated in Roman Law, migrated to England before the introduction of Christianity to the British Isles, and were typically stated in Latin even in English decisions. Many examples are familiar in everyday speech even today, \"One cannot be a judge in one's own cause\" (see Dr. Bonham's Case), rights are reciprocal to obligations, and the like. Judicial decisions and treatises of the 17th and 18th centuries, such as those of Lord Chief Justice Edward Coke, presented the common law as a collection of such maxims.\nReliance on old maxims and rigid adherence to precedent, no matter how old or ill-considered, came under critical discussion in the late 19th century, starting in the United States. Oliver Wendell Holmes Jr. in his famous article, \"The Path of the Law\", commented, \"It is revolting to have no better reason for a rule of law than that so it was laid down in the time of Henry IV. It is still more revolting if the grounds upon which it was laid down have vanished long since, and the rule simply persists from blind imitation of the past.\" Justice Holmes noted that study of maxims might be sufficient for \"the man of the present\", but \"the man of the future is the man of statistics and the master of economics\". In an 1880 lecture at Harvard, he wrote:\nIn the early 20th century, Louis Brandeis, later appointed to the United States Supreme Court, became noted for his use of policy-driving facts and economics in his briefs, and extensive appendices presenting facts that lead a judge to the advocate's conclusion. By this time, briefs relied more on facts than on Latin maxims.\nReliance on old maxims is now deprecated. Common law decisions today reflect both precedent and policy judgment drawn from economics, the social sciences, business, decisions of foreign courts, and the like. The degree to which these external factors \"should\" influence adjudication is the subject of active debate, but it is indisputable that judges \"do\" draw on experience and learning from everyday life, from other fields, and from other jurisdictions.\n1870 through 20th century, and the procedural merger of law and equity.\nAs early as the 15th century, it became the practice that litigants who felt they had been cheated by the common law system would petition the King in person. For example, they might argue that an award of damages (at common law (as opposed to equity)) was not sufficient redress for a trespasser occupying their land, and instead request that the trespasser be evicted. From this developed the system of equity, administered by the Lord Chancellor, in the courts of chancery. By their nature, equity and law were frequently in conflict and litigation would frequently continue for years as one court countermanded the other, even though it was established by the 17th century that equity should prevail.\nIn England, courts of law (as opposed to equity) were merged with courts of equity by the Judicature Acts of 1873 and 1875, with equity prevailing in case of conflict.\nIn the United States, parallel systems of law (providing money damages, with cases heard by a jury upon either party's request) and equity (fashioning a remedy to fit the situation, including injunctive relief, heard by a judge) survived well into the 20th century. The United States federal courts procedurally separated law and equity: the same judges could hear either kind of case, but a given case could only pursue causes in law or in equity, and the two kinds of cases proceeded under different procedural rules. This became problematic when a given case required both money damages and injunctive relief. In 1937, the new Federal Rules of Civil Procedure combined law and equity into one form of action, the \"civil action\". Fed.R.Civ.P. . The distinction survives to the extent that issues that were \"common law (as opposed to equity)\" as of 1791 (the date of adoption of the Seventh Amendment) are still subject to the right of either party to request a jury, and \"equity\" issues are decided by a judge.\nThe states of Delaware, Illinois, Mississippi, South Carolina, and Tennessee continue to have divided courts of law and courts of chancery, for example, the Delaware Court of Chancery. In New Jersey, the appellate courts are unified, but the trial courts are organized into a Chancery Division and a Law Division.\nCommon law pleading and its abolition in the early 20th century.\nFor centuries, through to the 19th century, the common law acknowledged only specific forms of action, and required very careful drafting of the opening pleading (called a writ) to slot into exactly one of them: debt, detinue, covenant, special assumpsit, general assumpsit, trespass, trover, replevin, case (or trespass on the case), and ejectment. To initiate a lawsuit, a pleading had to be drafted to meet myriad technical requirements: correctly categorizing the case into the correct legal pigeonhole (pleading in the alternative was not permitted), and using specific legal terms and phrases that had been traditional for centuries. Under the old common law pleading standards, a suit by a \"pro se\" (\"for oneself\", without a lawyer) party was all but impossible, and there was often considerable procedural jousting at the outset of a case over minor wording issues.\nOne of the major reforms of the late 19th century and early 20th century was the abolition of common law pleading requirements. A plaintiff can initiate a case by giving the defendant \"a short and plain statement\" of facts that constitute an alleged wrong. This reform moved the attention of courts from technical scrutiny of words to a more rational consideration of the facts, and opened access to justice far more broadly.\nComparison with civil law.\nCivil law systems.\nCommon law is usually contrasted with the civil law system, which is used in Continental Europe, most of Central and South America, and some African countries including Egypt and the Francophone countries of the Maghreb and west Africa.\nCommon law systems trace their history to the English common law, while civil law systems trace their history through the Napoleonic Code back to the of Roman law. \nRole of precedent and judicial review.\nThe primary contrast between the two systems is the role of written decisions and precedent as a source of law (one of the defining features of common law legal systems).\nWhile Common law systems place great weight on precedent, civil law judges tend to give less weight to judicial precedent. For example, the Napoleonic Code expressly forbade French judges to pronounce general principles of law.\nIn some civil law jurisdictions the judiciary does not have the authority to invalidate legislative provisions. For example, after the fall of the Soviet Union the Armenian parliament, with substantial support from USAID, adopted new legal codes. Some of the codes introduced problems which the judiciary was not empowered to adjudicate under the established principles of the common law of contracts - they could only apply the code as written. \nThere is no doctrine of \"stare decisis\" in the French civil law tradition. Civil law codes must be changed constantly because the precedent of courts is not binding and because courts lack authority to act if there is no statute. There are regular, good quality law reports in France, but it is not a consistent practice in many of the existing civil law jurisdictions. In French-speaking colonial Africa there were no law reports and what little we know of those historical cases comes from publication in journals.\nAdversarial system vs. inquisitorial system.\nCommon law systems tend to give more weight to separation of powers between the judicial branch and the executive branch. In contrast, civil law systems are typically more tolerant of allowing individual officials to exercise both powers. One example of this contrast is the difference between the two systems in allocation of responsibility between prosecutor and adjudicator.\nCommon law courts usually use an adversarial system, in which two sides present their cases to a neutral judge. For example, in criminal cases, in adversarial systems, the prosecutor and adjudicator are two separate people. The prosecutor is lodged in the executive branch, and conducts the investigation to locate evidence. That prosecutor presents the evidence to a neutral adjudicator, who makes a decision.\nIn contrast, in civil law systems, criminal proceedings proceed under an inquisitorial system in which an examining magistrate serves two roles by first developing the evidence and arguments for one side and then the other during the investigation phase. The examining magistrate then presents the dossier detailing his or her findings to the president of the bench that will adjudicate on the case where it has been decided that a trial shall be conducted. Therefore, the president of the bench's view of the case is not neutral and may be biased while conducting the trial after the reading of the dossier. Unlike the common law proceedings, the president of the bench in the inquisitorial system is not merely an umpire and is entitled to directly interview the witnesses or express comments during the trial, as long as he or she does not express his or her view on the guilt of the accused.\nThe proceeding in the inquisitorial system is essentially by writing. Most of the witnesses would have given evidence in the investigation phase and such evidence will be contained in the dossier under the form of police reports. In the same way, the accused would have already put his or her case at the investigation phase but he or she will be free to change his or her evidence at trial. Whether the accused pleads guilty or not, a trial will be conducted. Unlike the adversarial system, the conviction and sentence to be served (if any) will be released by the trial jury together with the president of the trial bench, following their common deliberation.\nIn contrast, in an adversarial system, on issues of fact, the onus of framing the case rests on the parties, and judges generally decide the case presented to them, rather than acting as active investigators, or actively reframing the issues presented. \"In our adversary system, in both civil and criminal cases, in the first instance and on appeal, we follow the principle of party presentation. That is, we rely on the parties to frame the issues for decision and assign to courts the role of neutral arbiter of matters the parties present.\" This principle applies with force in all issues in criminal matters, and to factual issues: courts seldom engage in fact gathering on their own initiative, but decide facts on the evidence presented (even here, there are exceptions, for \"legislative facts\" as opposed to \"adjudicative facts\").\nOn the other hand, on issues of law, common law courts regularly raise new issues (such as matters of jurisdiction or standing), perform independent research, and reformulate the legal grounds on which to analyze the facts presented to them. The United States Supreme Court regularly decides based on issues raised only in amicus briefs from non-parties. One of the most notable such cases was \"Erie Railroad v. Tompkins\", a 1938 case in which neither party questioned the ruling from the 1842 case \"Swift v. Tyson\" that served as the foundation for their arguments, but which led the Supreme Court to overturn \"Swift\" during their deliberations. To avoid lack of notice, courts may invite briefing on an issue to ensure adequate notice. However, there are limits\u2014an appeals court may not introduce a theory that contradicts the party's own contentions.\nThere are many exceptions in both directions. For example, most proceedings before U.S. federal and state agencies are inquisitorial in nature, at least the initial stages (\"e.g.\", a patent examiner, a social security hearing officer, and so on), even though the law to be applied is developed through common law processes.\nConvergence of common law and civil law.\nThe contrast between civil law and common law legal systems has become increasingly blurred, with the growing importance of jurisprudence (similar to case law but not binding) in civil law countries, and the growing importance of statute law and codes in common law countries.\nCommon law countries are increasingly adopting codes, similar to civil law systems, in areas such as bankruptcy, intellectual property, antitrust, banking regulation, securities, and tax law. In the United States, the Uniform Commercial Code (UCC) is an example of a codified framework governing various aspects of commercial law. Widely regarded as one of the most significant developments in American law, the UCC has been enacted, with some local variations, in all 50 states, the District of Columbia, Puerto Rico, and the Virgin Islands.\nAn example of convergence from the other direction is shown in the 1982 decision \"Srl CILFIT and Lanificio di Gavardo SpA v Ministry of Health\" (), in which the European Court of Justice held that questions it has already answered need not be resubmitted. This showed how a historically distinctly common law principle is used by a court composed of judges (at that time) of essentially civil law jurisdiction.\nCommon law legal systems in the present day.\nIn jurisdictions around the world.\nThe common law constitutes the basis of the legal systems of:\nand many other generally English-speaking countries or Commonwealth countries (except Scotland, which is bijuridicial, and Malta). Essentially, every country that was colonised at some time by England, Great Britain, or the United Kingdom uses common law except those that were formerly colonised by other nations, such as Quebec (which follows the bijuridicial law or civil code of France in part), South Africa and Sri Lanka (which follow Roman Dutch law), where the prior civil law system was retained to respect the civil rights of the local colonists. Guyana and Saint Lucia have mixed common law and civil law systems.\nThe remainder of this section discusses jurisdiction-specific variants, arranged chronologically.\nScotland.\nScotland is often said to use the civil law system, but it has a unique system that combines elements of an uncodified civil law dating back to the with an element of its own common law long predating the Treaty of Union with England in 1707 (see Legal institutions of Scotland in the High Middle Ages), founded on the customary laws of the tribes residing there. Historically, Scottish common law differed in that the use of \"precedent\" was subject to the courts' seeking to discover the principle that justifies a law rather than searching for an example as a \"precedent\", and principles of natural justice and fairness have always played a role in Scots Law. From the 19th century, the Scottish approach to precedent developed into a \"stare decisis\" akin to that already established in England thereby reflecting a narrower, more modern approach to the application of case law in subsequent instances. This is not to say that the substantive rules of the common laws of both countries are the same, but in many matters (particularly those of UK-wide interest), they are similar.\nScotland shares the Supreme Court with England, Wales and Northern Ireland for civil cases; the court's decisions are binding on the jurisdiction from which a case arises but only influential on similar cases arising in Scotland. This has had the effect of converging the law in certain areas. For instance, the modern UK law of negligence is based on \"Donoghue v Stevenson\", a case originating in Paisley, Scotland.\nScotland maintains a separate criminal law system from the rest of the UK, with the High Court of Justiciary being the final court for criminal appeals. The highest court of appeal in civil cases brought in Scotland is now the Supreme Court of the United Kingdom (before October 2009, final appellate jurisdiction lay with the House of Lords).\nThe United States \u2013 states, federal courts, and executive branch agencies (17th century on).\nNew York (17th century).\nThe original colony of New Netherland was settled by the Dutch and the law was also Dutch. When the English captured pre-existing colonies they continued to allow the local settlers to keep their civil law. However, the Dutch settlers revolted against the English and the colony was recaptured by the Dutch. In 1664, the colony of New York had two distinct legal systems: on Manhattan Island and along the Hudson River, sophisticated courts modeled on those of the Netherlands were resolving disputes learnedly in accordance with Dutch customary law. On Long Island, Staten Island, and in Westchester, on the other hand, English courts were administering a crude, untechnical variant of the common law carried from Puritan New England and practiced without the intercession of lawyers. When the English finally regained control of New Netherland they imposed common law upon all the colonists, including the Dutch. This was problematic, as the patroon system of land holding, based on the feudal system and civil law, continued to operate in the colony until it was abolished in the mid-19th century. New York began a codification of its law in the 19th century. The only part of this codification process that was considered complete is known as the Field Code applying to civil procedure. The influence of Roman-Dutch law continued in the colony well into the late 19th century. The codification of a law of general obligations shows how remnants of the civil law tradition in New York continued on from the Dutch days.\nLouisiana (1700s).\nUnder Louisiana's codified system, the Louisiana Civil Code, private law\u2014that is, substantive law between private sector parties\u2014is based on principles of law from continental Europe, with some common law influences. These principles derive ultimately from Roman law, transmitted through French law and Spanish law, as the state's current territory intersects the area of North America colonized by Spain and by France. Contrary to popular belief, the Louisiana code does not directly derive from the Napoleonic Code, as the latter was enacted in 1804, one year after the Louisiana Purchase. However, the two codes are similar in many respects due to common roots.\nLouisiana's criminal law largely rests on English common law. Louisiana's administrative law is generally similar to the administrative law of the U.S. federal government and other U.S. states. Louisiana's procedural law is generally in line with that of other U.S. states, which in turn is generally based on the U.S. Federal Rules of Civil Procedure.\nHistorically notable among the Louisiana code's differences from common law is the role of property rights among women, particularly in inheritance gained by widows.\nCalifornia (1850s).\nThe U.S. state of California has a system based on common law, but it has codified the law in the manner of civil law jurisdictions. The reason for the enactment of the California Codes in the 19th century was to replace a pre-existing system based on Spanish civil law with a system based on common law, similar to that in most other states. California and a number of other Western states, however, have retained the concept of community property derived from civil law. The California courts have treated portions of the codes as an extension of the common-law tradition, subject to judicial development in the same manner as judge-made common law. (Most notably, in the case \"Li v. Yellow Cab Co.\", 13 Cal.3d 804 (1975), the California Supreme Court adopted the principle of comparative negligence in the face of a California Civil Code provision codifying the traditional common-law doctrine of contributory negligence.)\nUnited States federal courts (1789 and 1938).\nAfter \"Erie v. Tompkins\", 304 U.S. 64, 78 (1938) overruled Joseph Storey's decision in \"Swift v. Tyson\", the federal common law was limited to some jurisdictions stated in the Constitution, such as admiralty, and possibly some areas that may not be the traditional jurisdiction of state law. \nLater courts have limited \"Erie\" slightly, to create a few situations where United States federal courts are permitted to create federal common law rules without express statutory authority, for example, where a federal rule of decision is necessary to protect uniquely federal interests, such as foreign affairs, or financial instruments issued by the federal government. Except on Constitutional issues, and some procedural issues, Congress is free to legislatively overrule federal courts' common law.\nIn \"Swift\", the United States Supreme Court had held that federal courts hearing cases brought under their diversity jurisdiction (allowing them to hear cases between parties from different states) had to apply the statutory law of the states, but not the common law developed by state courts. Instead, the Supreme Court permitted the federal courts to make their own common law based on general principles of law. \"Erie\" overruled \"Swift v. Tyson\", and instead held that federal courts exercising diversity jurisdiction had to use all of the same substantive law as the courts of the states in which they were located. As the \"Erie\" Court put it, there is no \"general federal common law\". \nPost-1938, federal courts deciding issues that arise under state law are required to defer to state court interpretations of state statutes, or reason what a state's highest court would rule if presented with the issue, or to certify the question to the state's highest court for resolution. Outside diversity jurisdiction and when there is no federal statute, post-Erie federal courts have continued to create causes of action. Justice Lewis Powell strongly objected to this practice in an influential dissent for the case \"Cannon v. University of Chicago\".\nUnited States executive branch agencies (1946).\nMost executive branch agencies in the United States federal government have some adjudicatory authority. To greater or lesser extent, agencies honor their own precedent to ensure consistent results. Agency decision making is governed by the Administrative Procedure Act of 1946.\nFor example, the National Labor Relations Board issues relatively few regulations, but instead promulgates most of its substantive rules through common law (connotation 1).\nIndia, Pakistan, and Bangladesh (19th century and 1948).\nThe law of India, Pakistan, and Bangladesh are largely based on English common law because of the long period of British colonial influence during the period of the British Raj.\nAncient India represented a distinct tradition of law, and had a historically independent school of legal theory and practice. The \"Arthashastra\", dating from 400 BCE and the \"Manusmriti\", from 100 CE, were influential treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. Early in this period, which finally culminated in the creation of the Gupta Empire, relations with ancient Greece and Rome were not infrequent. The appearance of similar fundamental institutions of international law in various parts of the world show that they are inherent in international society, irrespective of culture and tradition. Inter-State relations in the pre-Islamic period resulted in clear-cut rules of warfare of a high humanitarian standard, in rules of neutrality, of treaty law, of customary law embodied in religious charters, in exchange of embassies of a temporary or semi-permanent character.\nWhen India became part of the British Empire, there was a break in tradition, and Hindu and Islamic law were supplanted by the common law. After the failed rebellion against the British in 1857, the British Parliament took over control of India from the British East India Company, and British India came under the direct rule of the Crown. The British Parliament passed the Government of India Act 1858 to this effect, which set up the structure of British government in India. It established in Britain the office of the Secretary of State for India through whom the Parliament would exercise its rule, along with a Council of India to aid him. It also established the office of the Governor-General of India along with an Executive Council in India, which consisted of high officials of the British Government. As a result, the present judicial system of the country derives largely from the British system and has little correlation to the institutions of the pre-British era.\nPost-partition India (1948).\nPost-partition, India retained its common law system. Much of contemporary Indian law shows substantial European and American influence. Legislation first introduced by the British is still in effect in modified form today. During the drafting of the Indian Constitution, laws from Ireland, the United States, Britain, and France were all synthesized to produce a refined set of Indian laws. Indian laws also adhere to the United Nations guidelines on human rights law and environmental law. Certain international trade laws, such as those on intellectual property, are also enforced in India.\nPost-partition Pakistan (1948).\nPost-partition, Pakistan retained its common law system.\nPost-partition Bangladesh (1968).\nPost-partition, Bangladesh retained its common law system.\nCanada (1867).\nCanada has separate federal and provincial legal systems.\nCanadian provincial legal systems.\nEach province and territory is considered a separate jurisdiction with respect to case law. Each has its own procedural law in civil matters, statutorily created provincial courts and superior trial courts with inherent jurisdiction culminating in the Court of Appeal of the province. These Courts of Appeal are then subject to the Supreme Court of Canada in terms of appeal of their decisions.\nAll but one of the provinces of Canada use a common law system for civil matters (the exception being Quebec, which uses a French-heritage civil law system for issues arising within provincial jurisdiction, such as property ownership and contracts).\nCanadian federal legal system.\nCanadian Federal Courts operate under a separate system throughout Canada and deal with narrower range of subject matter than superior courts in each province and territory. They only hear cases on subjects assigned to them by federal statutes, such as immigration, intellectual property, judicial review of federal government decisions, and admiralty. The Federal Court of Appeal is the appellate court for federal courts and hears cases in multiple cities; unlike the United States, the Canadian Federal Court of Appeal is not divided into appellate circuits.\nCanadian federal statutes must use the terminology of both the common law and civil law for civil matters; this is referred to as legislative bijuralism.\nCanadian criminal law.\nCriminal law is uniform throughout Canada. It is based on the federal statutory Criminal Code, which in addition to substance also details procedural law. The administration of justice are the responsibilities of the provinces. Canadian criminal law uses a common law system no matter which province a case proceeds.\nNicaragua.\nNicaragua's legal system is also a mixture of the English Common Law and Civil Law. This situation was brought through the influence of British administration of the Eastern half of the Mosquito Coast from the mid-17th century until about 1894, the William Walker period from about 1855 through 1857, US interventions/occupations during the period from 1909 to 1933, the influence of US institutions during the Somoza family administrations (1933 through 1979) and the considerable importation between 1979 and the present of US culture and institutions.\nIsrael (1948).\nIsrael has no formal written constitution. Its basic principles are inherited from the law of the British Mandate of Palestine and thus resemble those of British and American law, namely: the role of courts in creating the body of law and the authority of the supreme court in reviewing and if necessary overturning legislative and executive decisions, as well as employing the adversarial system. However, because Israel has no written constitution, basic laws can be changed by a vote of 61 out of 120 votes in the parliament. One of the primary reasons that the Israeli constitution remains unwritten is the fear by whatever party holds power that creating a written constitution, combined with the common-law elements, would severely limit the powers of the Knesset (which, following the doctrine of parliamentary sovereignty, holds near-unlimited power).\nRoman Dutch common law.\nRoman Dutch common law is a bijuridical or mixed system of law similar to the common law system in Scotland and Louisiana. Roman Dutch common law jurisdictions include South Africa, Botswana, Lesotho, Namibia, Swaziland, Sri Lanka and Zimbabwe. Many of these jurisdictions recognise customary law, and in some, such as South Africa the Constitution requires that the common law be developed in accordance with the Bill of Rights. Roman Dutch common law is a development of Roman Dutch law by courts in the Roman Dutch common law jurisdictions. During the Napoleonic wars the Kingdom of the Netherlands adopted the French \"code civil\" in 1809, however the Dutch colonies in the Cape of Good Hope and Sri Lanka, at the time called Ceylon, were seized by the British to prevent them being used as bases by the French Navy. The system was developed by the courts and spread with the expansion of British colonies in Southern Africa. Roman Dutch common law relies on legal principles set out in Roman law sources such as Justinian's Institutes and Digest, and also on the writing of Dutch jurists of the 17th century such as Grotius and Voet. In practice, the majority of decisions rely on recent precedent.\nGhana.\nGhana follows the English common law tradition which was inherited from the British during her colonisation. Consequently, the laws of Ghana are, for the most part, a modified version of imported law that is continuously adapting to changing socio-economic and political realities of the country. The Bond of 1844 marked the period when the people of Ghana (then Gold Coast) ceded their independence to the British and gave the British judicial authority. Later, the Supreme Court Ordinance of 1876 formally introduced British law, be it the common law or statutory law, in the Gold Coast. Section 14 of the Ordinance formalised the application of the common-law tradition in the country.\nGhana, after independence, did not do away with the common law system inherited from the British, and today it has been enshrined in the 1992 Constitution of the country. Chapter four of Ghana's Constitution, entitled \"The Laws of Ghana\", has in Article 11(1) the list of laws applicable in the state. This comprises (a) the Constitution; (b) enactments made by or under the authority of the Parliament established by the Constitution; (c) any Orders, Rules and Regulations made by any person or authority under a power conferred by the Constitution; (d) the existing law; and (e) the common law. Thus, the modern-day Constitution of Ghana, like those before it, embraced the English common law by entrenching it in its provisions. The doctrine of judicial precedence which is based on the principle of \"stare decisis\" as applied in England and other pure common law countries also applies in Ghana.\nScholarly works.\nEdward Coke, a 17th-century Lord Chief Justice of the English Court of Common Pleas and a Member of Parliament (MP), wrote several legal texts that collected and integrated centuries of case law. Lawyers in both England and America learned the law from his \"Institutes\" and \"Reports\" until the end of the 18th century. His works are still cited by common law courts around the world.\nThe next definitive historical treatise on the common law is \"Commentaries on the Laws of England\", written by Sir William Blackstone and first published in 1765\u20131769. Since 1979, a facsimile edition of that first edition has been available in four paper-bound volumes. Today it has been superseded in the English part of the United Kingdom by Halsbury's Laws of England that covers both common and statutory English law.\nWhile he was still on the Massachusetts Supreme Judicial Court, and before being named to the U.S. Supreme Court, Justice Oliver Wendell Holmes Jr. published a short volume called \"The Common Law\", which remains a classic in the field. Unlike Blackstone and the Restatements, Holmes' book only briefly discusses what the law \"is\"; rather, Holmes describes the common law \"process\". Law professor John Chipman Gray's \"The Nature and Sources of the Law\", an examination and survey of the common law, is also still commonly read in U.S. law schools.\nIn the United States, Restatements of various subject matter areas (Contracts, Torts, Judgments, and so on.), edited by the American Law Institute, collect the common law for the area. The ALI Restatements are often cited by American courts and lawyers for propositions of uncodified common law, and are considered highly persuasive authority, just below binding precedential decisions. The Corpus Juris Secundum is an encyclopedia whose main content is a compendium of the common law and its variations throughout the various state jurisdictions.\nScots \"common law\" covers matters including murder and theft, and has sources in custom, in legal writings and previous court decisions. The legal writings used are called \"Institutional Texts\" and come mostly from the 17th, 18th and 19th centuries. Examples include Craig, \"Jus Feudale\" (1655) and Stair, \"The Institutions of the Law of Scotland\" (1681)."}
{"id": "5255", "revid": "15449111", "url": "https://en.wikipedia.org/wiki?curid=5255", "title": "Civil law", "text": "Civil law may refer to:"}
{"id": "5257", "revid": "36064032", "url": "https://en.wikipedia.org/wiki?curid=5257", "title": "Court of appeals (disambiguation)", "text": "A court of appeals is generally an appellate court.\nCourt of Appeals may refer to:"}
{"id": "5258", "revid": "2135234", "url": "https://en.wikipedia.org/wiki?curid=5258", "title": "Computer Storage", "text": ""}
{"id": "5259", "revid": "5128741", "url": "https://en.wikipedia.org/wiki?curid=5259", "title": "Common descent", "text": "Common descent is a concept in evolutionary biology applicable when one species is the ancestor of two or more species later in time. According to modern evolutionary biology, all living beings could be descendants of a unique ancestor commonly referred to as the last universal common ancestor (LUCA) of all life on Earth.\nCommon descent is an effect of speciation, in which multiple species derive from a single ancestral population. The more recent the ancestral population two species have in common, the more closely are they related. The most recent common ancestor of all currently living organisms is the last universal ancestor, which lived about 3.9 billion years ago. The two earliest pieces of evidence for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. All currently living organisms on Earth share a common genetic heritage, though the suggestion of substantial horizontal gene transfer during early evolution has led to questions about the monophyly (single ancestry) of life. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago in the Precambrian.\nUniversal common descent through an evolutionary process was first proposed by the British naturalist Charles Darwin in the concluding sentence of his 1859 book \"On the Origin of Species\":\nHistory.\nThe idea that all living things (including things considered non-living by science) are related is a recurring theme in many indigenous worldviews across the world. Later on, in the 1740s, the French mathematician Pierre Louis Maupertuis arrived at the idea that all organisms had a common ancestor, and had diverged through random variation and natural selection.\nIn 1790, the philosopher Immanuel Kant wrote in \"Kritik der Urteilskraft\" (\"Critique of Judgment\") that the similarity of animal forms implies a common original type, and thus a common parent.\nIn 1794, Charles Darwin's grandfather, Erasmus Darwin asked:\n[W]ould it be too bold to imagine, that in the great length of time, since the earth began to exist, perhaps millions of ages before the commencement of the history of mankind, would it be too bold to imagine, that all warm-blooded animals have arisen from one living filament, which endued with animality, with the power of acquiring new parts attended with new propensities, directed by irritations, sensations, volitions, and associations; and thus possessing the faculty of continuing to improve by its own inherent activity, and of delivering down those improvements by generation to its posterity, world without end?\nCharles Darwin's views about common descent, as expressed in \"On the Origin of Species\", were that it was probable that there was only one progenitor for all life forms:\nTherefore I should infer from analogy that probably all the organic beings which have ever lived on this earth have descended from some one primordial form, into which life was first breathed.\nBut he precedes that remark by, \"Analogy would lead me one step further, namely, to the belief that all animals and plants have descended from some one prototype. But analogy may be a deceitful guide.\" And in the subsequent edition, he asserts rather, \"We do not know all the possible transitional gradations between the simplest and the most perfect organs; it cannot be pretended that we know all the varied means of Distribution during the long lapse of years, or that we know how imperfect the Geological Record is. Grave as these several difficulties are, in my judgment they do not overthrow the theory of descent from a few created forms with subsequent modification\". \nCommon descent was widely accepted amongst the scientific community after Darwin's publication. In 1907, Vernon Kellogg commented that \"practically no naturalists of position and recognized attainment doubt the theory of descent.\"\nIn 2008, biologist T. Ryan Gregory noted that:\nNo reliable observation has ever been found to contradict the general notion of common descent. It should come as no surprise, then, that the scientific community at large has accepted evolutionary descent as a historical reality since Darwin's time and considers it among the most reliably established and fundamentally important facts in all of science.\nEvidence.\nCommon biochemistry.\nAll known forms of life are based on the same fundamental biochemical organization: genetic information encoded in DNA, transcribed into RNA, through the effect of protein- and RNA-enzymes, then translated into proteins by (highly similar) ribosomes, with ATP, NADPH and others as energy sources. Analysis of small sequence differences in widely shared substances such as cytochrome c further supports universal common descent. Some 23 proteins are found in all organisms, serving as enzymes carrying out core functions like DNA replication. The fact that only one such set of enzymes exists is convincing evidence of a single ancestry. 6,331 genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago in the Precambrian.\nCommon genetic code.\nThe genetic code (the \"translation table\" according to which DNA information is translated into amino acids, and hence proteins) is nearly identical for all known lifeforms, from bacteria and archaea to animals and plants. The universality of this code is generally regarded by biologists as definitive evidence in favor of universal common descent.\nThe way that codons (DNA triplets) are mapped to amino acids seems to be strongly optimised. Richard Egel argues that in particular the hydrophobic (non-polar) side-chains are well organised, suggesting that these enabled the earliest organisms to create peptides with water-repelling regions able to support the essential electron exchange (redox) reactions for energy transfer.\nSelectively neutral similarities.\nSimilarities which have no adaptive relevance cannot be explained by convergent evolution, and therefore they provide compelling support for universal common descent. Such evidence has come from two areas: amino acid sequences and DNA sequences. Proteins with the same three-dimensional structure need not have identical amino acid sequences; any irrelevant similarity between the sequences is evidence for common descent. In certain cases, there are several codons (DNA triplets) that code redundantly for the same amino acid. Since many species use the same codon at the same place to specify an amino acid that can be represented by more than one codon, that is evidence for their sharing a recent common ancestor. Had the amino acid sequences come from different ancestors, they would have been coded for by any of the redundant codons, and since the correct amino acids would already have been in place, natural selection would not have driven any change in the codons, however much time was available. Genetic drift could change the codons, but it would be extremely unlikely to make all the redundant codons in a whole sequence match exactly across multiple lineages. Similarly, shared nucleotide sequences, especially where these are apparently neutral such as the positioning of introns and pseudogenes, provide strong evidence of common ancestry.\nOther similarities.\nBiologists often point to the universality of many aspects of cellular life as supportive evidence to the more compelling evidence listed above. These similarities include the energy carrier adenosine triphosphate (ATP), and the fact that all amino acids found in proteins are left-handed. It is, however, possible that these similarities resulted because of the laws of physics and chemistry - rather than through universal common descent - and therefore resulted in convergent evolution. In contrast, there is evidence for homology of the central subunits of transmembrane ATPases throughout all living organisms, especially how the rotating elements are bound to the membrane. This supports the assumption of a LUCA as a cellular organism, although primordial membranes may have been semipermeable and evolved later to the membranes of modern bacteria, and on a second path to those of modern archaea also.\nPhylogenetic trees.\nAnother important piece of evidence is from detailed phylogenetic trees (i.e., \"genealogic trees\" of species) mapping out the proposed divisions and common ancestors of all living species. In 2010, Douglas L. Theobald published a statistical analysis of available genetic data, mapping them to phylogenetic trees, that gave \"strong quantitative support, by a formal test, for the unity of life.\"\nTraditionally, these trees have been built using morphological methods, such as appearance, embryology, etc. Recently, it has been possible to construct these trees using molecular data, based on similarities and differences between genetic and protein sequences. All these methods produce essentially similar results, even though most genetic variation has no influence over external morphology. That phylogenetic trees based on different types of information agree with each other is strong evidence of a real underlying common descent.\nObjections.\nGene exchange clouds phylogenetic analysis.\nTheobald noted that substantial horizontal gene transfer could have occurred during early evolution. Bacteria today remain capable of gene exchange between distantly-related lineages. This weakens the basic assumption of phylogenetic analysis, that similarity of genomes implies common ancestry, because sufficient gene exchange would allow lineages to share much of their genome whether or not they shared an ancestor (monophyly). This has led to questions about the single ancestry of life. However, biologists consider it very unlikely that completely unrelated proto-organisms could have exchanged genes, as their different coding mechanisms would have resulted only in garble rather than functioning systems. Later, however, many organisms all derived from a single ancestor could readily have shared genes that all worked in the same way, and it appears that they have.\nConvergent evolution.\nIf early organisms had been driven by the same environmental conditions to evolve similar biochemistry convergently, they might independently have acquired similar genetic sequences. Theobald's \"formal test\" was accordingly criticised by Takahiro Yonezawa and colleagues for not including consideration of convergence. They argued that Theobald's test was insufficient to distinguish between the competing hypotheses. Theobald has defended his method against this claim, arguing that his tests distinguish between phylogenetic structure and mere sequence similarity. Therefore, Theobald argued, his results show that \"real universally conserved proteins are homologous.\"\nRNA world.\nThe possibility is mentioned, above, that all living organisms may be descended from an original single-celled organism with a DNA genome, and that this implies a single origin for life. Although such a universal common ancestor may have existed, such a complex entity is unlikely to have arisen spontaneously from non-life and thus a cell with a DNA genome cannot reasonably be regarded as the origin of life. To understand the origin of life, it has been proposed that DNA based cellular life descended from relatively simple pre-cellular self-replicating RNA molecules able to undergo natural selection. During the course of evolution, this RNA world was replaced by the evolutionary emergence of the DNA world. A world of independently self-replicating RNA genomes apparently no longer exists (RNA viruses are dependent on host cells with DNA genomes). Because the RNA world is apparently gone, it is not clear how scientific evidence could be brought to bear on the question of whether there was a single origin of life event from which all life descended."}
{"id": "5261", "revid": "49145554", "url": "https://en.wikipedia.org/wiki?curid=5261", "title": "Celtic music", "text": "Celtic music is a broad grouping of music genres that evolved out of the folk music traditions of the Celtic people of Northwestern Europe (the modern Celtic nations). It refers to both orally-transmitted traditional music and recorded music and the styles vary considerably to include everything from traditional music to a wide range of hybrids.\nDescription and definition.\n\"Celtic music\" means two things mainly. First, it is the music of the people that identify themselves as Celts. Secondly, it refers to whatever qualities may be unique to the music of the Celtic nations. Many notable Celtic musicians such as Alan Stivell and Paddy Moloney claim that the different Celtic music genres have a lot in common.\nThese styles are known because of the importance of Irish and Scottish people in the English speaking world, especially in the United States, where they had a profound impact on American music, particularly bluegrass and country music. The music of Wales, Cornwall, the Isle of Man, Brittany, Galician traditional music (Spain) and music of Portugal are also considered Celtic music, the tradition being particularly strong in Brittany, where Celtic festivals large and small take place throughout the year, and in Wales, where the ancient eisteddfod tradition has been revived and flourishes. Additionally, the musics of ethnically Celtic peoples abroad are vibrant, especially in Canada and the United States. In Canada the provinces of Atlantic Canada are known for being a home of Celtic music, most notably on the islands of Newfoundland, Cape Breton and Prince Edward Island. The traditional music of Atlantic Canada is heavily influenced by the Irish, Scottish and Acadian ethnic makeup of much of the region's communities. In some parts of Atlantic Canada, such as Newfoundland, Celtic music is as or more popular than in the old country. Further, some older forms of Celtic music that are rare in Scotland and Ireland today, such as the practice of accompanying a fiddle with a piano, or the Gaelic spinning songs of Cape Breton remain common in the Maritimes. Much of the music of this region is Celtic in nature, but originates in the local area and celebrates the sea, seafaring, fishing and other primary industries.\nInstruments associated with Celtic Music include the Celtic harp, uilleann pipes or Great Highland bagpipe, fiddle, tin whistle, flute, bodhr\u00e1n, bones, concertina, accordion and a recent addition, the Irish bouzouki.\nDivisions.\nIn \"Celtic Music: A Complete Guide\", June Skinner Sawyers acknowledges six Celtic nationalities divided into two groups according to their linguistic heritage. The Q-Celtic nationalities are the Irish, Scottish and Manx peoples, while the P-Celtic groups are the Cornish, Bretons and Welsh peoples. Musician Alan Stivell uses a similar dichotomy, between the Gaelic (Irish/Scottish/Manx) and the Brythonic (Breton/Welsh/Cornish) branches, which differentiate \"mostly by the extended range (sometimes more than two octaves) of Irish and Scottish melodies and the closed range of Breton and Welsh melodies (often reduced to a half-octave), and by the frequent use of the pure pentatonic scale in Gaelic music.\"\nThere is also tremendous variation between \"Celtic\" regions. Ireland, Scotland, Wales, Cornwall, and Brittany have living traditions of language and music, and there has been a recent major revival of interest in Celtic heritage in the Isle of Man. Galicia has a Celtic language revival movement to revive the Q-Celtic \"Gallaic language\" used into Roman times, which is not an attested language, unlike Celtiberian. A Brythonic language may have been spoken in parts of Galicia and Asturias into early Medieval times brought by Britons fleeing the Anglo-Saxon settlement of Britain via Brittany., but here again there are several hypotheses and very little traces of it : lack of archeological, linguistic evidence and documents. The Romance language currently spoken in Galicia, Galician (\"Galego\") is closely related to the Portuguese language used mainly in Brazil and Portugal and in many ways closer to Latin than other Romance languages. Galician music is claimed to be \"Celtic\". The same is true of the music of Asturias, Cantabria, and that of Northern Portugal (some say even traditional music from Central Portugal can be labeled Celtic).\nBreton artist Alan Stivell was one of the earliest musicians to use the word \"Celtic\" and \"Keltia\" in his marketing materials, starting in the early 1960s as part of the worldwide folk music revival of that era with the term quickly catching on with other artists worldwide. Today, the genre is well established and incredibly diverse.\nForms.\nThere are musical genres and styles specific to each Celtic country, due in part to the influence of individual song traditions and the characteristics of specific languages:\nFestivals.\nThe modern Celtic music scene involves a large number of music festivals, as it has traditionally. Some of the most prominent festivals focused solely on music include:\nCeltic fusion.\nThe oldest musical tradition which fits under the label of Celtic fusion originated in the rural American south in the early colonial period and incorporated English, Scottish, Irish, Welsh, German, and African influences. Variously referred to as roots music, American folk music, or old-time music, this tradition has exerted a strong influence on all forms of American music, including country, blues, and rock and roll. In addition to its lasting effects on other genres, it marked the first modern large-scale mixing of musical traditions from multiple ethnic and religious communities within the Celtic diaspora.\nIn the 1960s several bands put forward modern adaptations of Celtic music pulling influences from several of the Celtic nations at once to create a modern pan-celtic sound. A few of those include bagado\u00f9 (Breton pipe bands), Fairport Convention, Pentangle, Steeleye Span and Horslips.\nIn the 1970s Clannad made their mark initially in the folk and traditional scene, and then subsequently went on to bridge the gap between traditional Celtic and pop music in the 1980s and 1990s, incorporating elements from new-age, smooth jazz, and folk rock. Traces of Clannad's legacy can be heard in the music of many artists, including Altan, An\u00fana, Capercaillie, the Corrs, Dexys Midnight Runners, Enya, Loreena McKennitt, Riverdance, Donna Taggart, and U2. The solo music of Clannad's lead singer, Moya Brennan (often referred to as the First Lady of Celtic Music) has further enhanced this influence.\nLater, beginning in 1982 with the Pogues' invention of Celtic folk-punk and Stockton's Wing blend of Irish traditional and Pop, Rock and Reggae, there has been a movement to incorporate Celtic influences into other genres of music. Bands like Flogging Molly, Black 47, Dropkick Murphys, the Young Dubliners, the Tossers introduced a hybrid of Celtic rock, punk, reggae, hardcore and other elements in the 1990s that has become popular with Irish-American youth.\nToday there are Celtic-influenced subgenres of virtually every type of popular music including electronica, rock, metal, punk, hip hop, reggae, new-age, Latin, Andean and pop. Collectively these modern interpretations of Celtic music are sometimes referred to as Celtic fusion.\nOther modern adaptations.\nOutside of America, the first deliberate attempts to create a \"Pan-Celtic music\" were made by the Breton Taldir Jaffrennou, having translated songs from Ireland, Scotland, and Wales into Breton between the two world wars. One of his major works was to bring \"Hen Wlad Fy Nhadau\" (the Welsh national anthem) back in Brittany and create lyrics in Breton. Eventually this song became \"Bro goz va zado\u00f9\" (\"Old land of my fathers\") and is the most widely accepted Breton anthem. In the 70s, the Breton Alan Cochevelou (future Alan Stivell) began playing a mixed repertoire from the main Celtic countries on the Celtic harp his father created. \nProbably the most successful all-inclusive Celtic music composition in recent years is Shaun Daveys composition \"The Pilgrim\". This suite depicts the journey of St. Colum Cille through the Celtic nations of Ireland, Scotland, the Isle of Man, Wales, Cornwall, Brittany and Galicia. The suite which includes a Scottish pipe band, Irish and Welsh harpists, Galician gaitas, Irish uilleann pipes, the bombardes of Brittany, two vocal soloists and a narrator is set against a background of a classical orchestra and a large choir.\nModern music may also be termed \"Celtic\" because it is written and recorded in a Celtic language, regardless of musical style. Many of the Celtic languages have experienced resurgences in modern years, spurred on partly by the action of artists and musicians who have embraced them as hallmarks of identity and distinctness. In 1971, the Irish band \"Skara Brae\" recorded its only LP (simply called \"Skara Brae\"), all songs in Irish. In 1978 Runrig recorded an album in Scottish Gaelic. In 1992 Capercaillie recorded \"A Prince Among Islands\", the first Scottish Gaelic language record to reach the UK top 40. In 1996, a song in Breton represented France in the 41st Eurovision Song Contest, the first time in history that France had a song without a word in French. Since about 2005, Oi Polloi (from Scotland) have recorded in Scottish Gaelic. Mill a h-Uile Rud (a Scottish Gaelic punk band from Seattle) recorded in the language in 2004.\nSeveral contemporary bands have Welsh language songs, such as Ceredwen, which fuses traditional instruments with trip hop beats, the Super Furry Animals, Fernhill, and so on (see the Music of Wales article for more Welsh and Welsh-language bands). The same phenomenon occurs in Brittany, where many singers record songs in Breton, traditional or modern (hip hop, rap, and so on.)."}
{"id": "5264", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5264", "title": "Computer Monitor", "text": ""}
{"id": "5267", "revid": "3492060", "url": "https://en.wikipedia.org/wiki?curid=5267", "title": "Constellation", "text": "A constellation is an area on the celestial sphere in which a group of visible stars forms a perceived pattern or outline, typically representing an animal, mythological subject, or inanimate object.\nThe first constellations were likely defined in prehistory. People used them to relate stories of their beliefs, experiences, creation, and mythology. Different cultures and countries invented their own constellations, some of which lasted into the early 20th century before today's constellations were internationally recognized. The recognition of constellations has changed significantly over time. Many changed in size or shape. Some became popular, only to drop into obscurity. Some were limited to a single culture or nation. Naming constellations also helped astronomers and navigators identify stars more easily.\nTwelve (or thirteen) ancient constellations belong to the zodiac (straddling the ecliptic, which the Sun, Moon, and planets all traverse). The origins of the zodiac remain historically uncertain; its astrological divisions became prominent in Babylonian or Chaldean astronomy. Constellations appear in Western culture via Greece and are mentioned in the works of Hesiod, Eudoxus and Aratus. The traditional 48 constellations, consisting of the zodiac and 36 more (now 38, following the division of Argo Navis into three constellations) are listed by Ptolemy, a Greco-Roman astronomer from Alexandria, Egypt, in his \"Almagest\". The formation of constellations was the subject of extensive mythology, most notably in the \"Metamorphoses\" of the Latin poet Ovid. Constellations in the far southern sky were added from the 15th century until the mid-18th century when European explorers began traveling to the Southern Hemisphere. Due to Roman and European transmission, each constellation has a Latin name.\nIn 1922, the International Astronomical Union (IAU) formally accepted the modern list of 88\u00a0constellations, and in 1928 adopted official constellation boundaries that together cover the entire celestial sphere. Any given point in a celestial coordinate system lies in one of the modern constellations. Some astronomical naming systems include the constellation where a given celestial object is found to convey its approximate location in the sky. The Flamsteed designation of a star, for example, consists of a number and the genitive form of the constellation's name.\nOther star patterns or groups called asterisms are not constellations under the formal definition, but are also used by observers to navigate the night sky. Asterisms may be several stars within a constellation, or they may share stars with more than one constellation. Examples of asterisms include the teapot within the constellation Sagittarius, or the big dipper in the constellation of Ursa Major.\nTerminology.\nThe word \"constellation\" comes from the Late Latin term , which can be translated as \"set of stars\"; it came into use in Middle English during the 14th century. The Ancient Greek word for constellation is \u1f04\u03c3\u03c4\u03c1\u03bf\u03bd (). These terms historically referred to any recognisable pattern of stars whose appearance was associated with mythological characters or creatures, earthbound animals, or objects. Over time, among European astronomers, the constellations became clearly defined and widely recognised. In the 20th century, the International Astronomical Union (IAU) recognized 88\u00a0constellations.\nA constellation or star that never sets below the horizon when viewed from a particular latitude on Earth is termed circumpolar. From the North Pole or South Pole, all constellations south or north of the celestial equator are circumpolar. Depending on the definition, equatorial constellations may include those that lie between declinations 45\u00b0 north and 45\u00b0 south, or those that pass through the declination range of the ecliptic (or zodiac) ranging between 23.5\u00b0\u00a0north and 23.5\u00b0\u00a0south.\nStars in constellations can appear near each other in the sky, but they usually lie at a variety of distances away from the Earth. Since each star has its own independent motion, all constellations will change slowly over time. After tens to hundreds of thousands of years, familiar outlines will become unrecognizable. Astronomers can predict the past or future constellation outlines by measuring common proper motions of individual stars by accurate astrometry and their radial velocities by astronomical spectroscopy.\nThe 88 constellations recognized by the IAU as well as those by cultures throughout history are imagined figures and shapes derived from the patterns of stars in the observable sky. Many officially recognized constellations are based on the imaginations of ancient, Near Eastern and Mediterranean mythologies. Some of these stories seem to relate to the appearance of the constellations, e.g. the assassination of Orion by Scorpius, their constellations appearing at opposite times of year.\nObservation.\nConstellation positions change throughout the year due to night on Earth occurring at gradually different portions of its\u00a0orbit around the Sun. As Earth rotates toward the east, the celestial sphere appears to rotate west, with stars circling counterclockwise around the northern pole star and clockwise around the southern pole star.\nBecause of Earth's 23.5\u00b0 axial tilt, the zodiac is distributed equally across hemispheres (along the ecliptic), approximating a great circle. Zodiacal constellations of the northern sky are Pisces, Aries, Taurus, Gemini, Cancer, and Leo. In the southern sky are Virgo, Libra, Scorpius, Sagittarius, Capricornus, and Aquarius. The zodiac appears directly overhead from latitudes of 23.5\u00b0\u00a0north to 23.5\u00b0\u00a0south, depending on the time of year. In summer, the ecliptic appears higher up in the daytime and lower at night, while in winter the reverse is true, for both hemispheres.\nDue to the Solar System's 60\u00b0 tilt, the galactic plane of the Milky Way is inclined 60\u00b0 from the ecliptic, between Taurus and Gemini (north) and Scorpius and Sagittarius (south and near which the Galactic Center can be found). The galaxy appears to pass through Aquila (near the celestial equator) and northern constellations Cygnus, Cassiopeia, Perseus, Auriga, and Orion (near Betelgeuse), as well as Monoceros (near the celestial equator), and southern constellations Puppis, Vela, Carina, Crux, Centaurus, Triangulum Australe, and Ara.\nNorthern hemisphere.\nPolaris, being the North Star, is the approximate center of the northern celestial hemisphere. It is part of Ursa Minor, constituting the end of the Little Dipper's handle. \nFrom latitudes of around 35\u00b0 north, in January, Ursa Major (containing the Big Dipper) appears to the northeast, while Cassiopeia is the northwest. To the west are Pisces (above the horizon) and Aries. To the southwest Cetus is near the horizon. Up high and to the south are Orion and Taurus. To the southeast above the horizon is Canis Major. Appearing above and to the east of Orion is Gemini: also in the east (and progressively closer to the horizon) are Cancer and Leo. In addition to Taurus, Perseus and Auriga appear overhead.\nFrom the same latitude, in July, Cassiopeia (low in the sky) and Cepheus appear to the northeast. Ursa Major is now in the northwest. Bo\u00f6tes is high up in the west. Virgo is to the west, with Libra southwest and Scorpius south. Sagittarius and Capricorn are southeast. Cygnus (containing the Northern Cross) is to the east. Hercules is high in the sky along with Corona Borealis.\nSouthern hemisphere.\nJanuary constellations include Pictor and Reticulum (near Hydrus and Mensa, respectively).\nIn July, Ara (adjacent to Triangulum Australe) and Scorpius can be seen.\nConstellations near the pole star include Chamaeleon, Apus and Triangulum Australe (near Centaurus), Pavo, Hydrus, and Mensa.\nSigma Octantis is the closest star approximating a southern pole star, but is faint in the night sky. Thus, the pole can be triangulated using the constellation Crux as well as the stars Alpha and Beta Centauri (about 30\u00b0 counterclockwise from Crux) of the constellation Centaurus (arching over Crux).\nHistory of the early constellations.\nLascaux Caves, southern France.\nIt has been suggested that the 17,000-year-old cave paintings in Lascaux, southern France, depict star constellations such as Taurus, Orion's Belt, and the Pleiades. However, this view is not generally accepted among scientists.\nMesopotamia.\nInscribed stones and clay writing tablets from Mesopotamia (in modern Iraq) dating to 3000\u00a0BC provide the earliest generally accepted evidence for humankind's identification of constellations. It seems that the bulk of the Mesopotamian constellations were created within a relatively short interval from around 1300 to 1000\u00a0BC. Mesopotamian constellations appeared later in many of the classical Greek constellations.\nAncient Near East.\nThe oldest Babylonian catalogues of stars and constellations date back to the beginning of the Middle Bronze Age, most notably the \"Three Stars Each\" texts and the \"MUL.APIN\", an expanded and revised version based on more accurate observation from around 1000\u00a0BC. However, the numerous Sumerian names in these catalogues suggest that they built on older, but otherwise unattested, Sumerian traditions of the Early Bronze Age.\nThe classical Zodiac is a revision of Neo-Babylonian constellations from the 6th century BC. The Greeks adopted the Babylonian constellations in the 4th century BC. Twenty Ptolemaic constellations are from the Ancient Near East. Another ten have the same stars but different names.\nBiblical scholar E. W. Bullinger interpreted some of the creatures mentioned in the books of Ezekiel and Revelation as the middle signs of the four-quarters of the Zodiac, with the Lion as Leo, the Bull as Taurus, the Man representing Aquarius, and the Eagle standing in for Scorpio. The biblical Book of Job also makes reference to a number of constellations, including \"bier\", \"fool\" and \"heap\" (Job 9:9, 38:31\u201332), rendered as \"Arcturus, Orion and Pleiades\" by the KJV, but \"\u2018Ayish\" \"the bier\" actually corresponding to Ursa Major. The term \"Mazzaroth\" , translated as \"a garland of crowns\", is a \"hapax legomenon\" in Job 38:32, and it might refer to the zodiacal constellations.\nClassical antiquity.\nThere is only limited information on ancient Greek constellations, with some fragmentary evidence being found in the \"Works and Days\" of the Greek poet Hesiod, who mentioned the \"heavenly bodies\". Greek astronomy essentially adopted the older Babylonian system in the Hellenistic era, first introduced to Greece by Eudoxus of Cnidus in the 4th century BC. The original work of Eudoxus is lost, but it survives as a versification by Aratus, dating to the 3rd century BC. The most complete existing works dealing with the mythical origins of the constellations are by the Hellenistic writer termed pseudo-Eratosthenes and an early Roman writer styled pseudo-Hyginus. The basis of Western astronomy as taught during Late Antiquity and until the Early Modern period is the \"Almagest\" by Ptolemy, written in the 2nd century.\nIn the Ptolemaic Kingdom, native Egyptian tradition of anthropomorphic figures represented the planets, stars, and various constellations. Some of these were combined with Greek and Babylonian astronomical systems culminating in the Zodiac of Dendera; it remains unclear when this occurred, but most were placed during the Roman period between 2nd to 4th centuries AD. The oldest known depiction of the zodiac showing all the now familiar constellations, along with some original Egyptian constellations, decans, and planets. Ptolemy's \"Almagest\" remained the standard definition of constellations in the medieval period both in Europe and in Islamic astronomy.\nAncient China.\nAncient China had a long tradition of observing celestial phenomena. Nonspecific Chinese star names, later categorized in the twenty-eight mansions, have been found on oracle bones from Anyang, dating back to the middle Shang dynasty. These constellations are some of the most important observations of Chinese sky, attested from the 5th century BC. Parallels to the earliest Babylonian (Sumerian) star catalogues suggest that the ancient Chinese system did not arise independently.\nThree schools of classical Chinese astronomy in the Han period are attributed to astronomers of the earlier Warring States period. The constellations of the three schools were conflated into a single system by Chen Zhuo, an astronomer of the 3rd century (Three Kingdoms period). Chen Zhuo's work has been lost, but information on his system of constellations survives in Tang period records, notably by Qutan Xida. The oldest extant Chinese star chart dates to that period and was preserved as part of the Dunhuang Manuscripts. Native Chinese astronomy flourished during the Song dynasty, and during the Yuan dynasty became increasingly influenced by medieval Islamic astronomy (see Treatise on Astrology of the Kaiyuan Era). As maps were prepared during this period on more scientific lines, they were considered as more reliable.\nA well-known map from the Song period is the Suzhou Astronomical Chart, which was prepared with carvings of stars on the planisphere of the Chinese sky on a stone plate; it is done accurately based on observations, and it shows the 1054 supernova in Taurus.\nInfluenced by European astronomy during the late Ming dynasty, charts depicted more stars but retained the traditional constellations. Newly observed stars were incorporated as supplementary to old constellations in the southern sky, which did not depict the traditional stars recorded by ancient Chinese astronomers. Further improvements were made during the later part of the Ming dynasty by Xu Guangqi and Johann Adam Schall von Bell, the German Jesuit and was recorded in Chongzhen Lishu (Calendrical Treatise of Chongzhen period, 1628). Traditional Chinese star maps incorporated 23 new constellations with 125 stars of the southern hemisphere of the sky based on the knowledge of Western star charts; with this improvement, the Chinese Sky was integrated with the World astronomy.\nEarly modern astronomy.\nHistorically, the origins of the constellations of the northern and southern skies are distinctly different. Most northern constellations date to antiquity, with names based mostly on Classical Greek legends. Evidence of these constellations has survived in the form of star charts, whose oldest representation appears on the statue known as the Farnese Atlas, based perhaps on the star catalogue of the Greek astronomer Hipparchus. Southern constellations are more modern inventions, sometimes as substitutes for ancient constellations (e.g. Argo Navis). Some southern constellations had long names that were shortened to more usable forms; e.g. Musca Australis became simply Musca.\nSome of the early constellations were never universally adopted. Stars were often grouped into constellations differently by different observers, and the arbitrary constellation boundaries often led to confusion as to which constellation a celestial object belonged. Before astronomers delineated precise boundaries (starting in the 19th century), constellations generally appeared as ill-defined regions of the sky. Today they now follow officially accepted designated lines of right ascension and declination based on those defined by Benjamin Gould in epoch 1875.0 in his star catalogue \"Uranometria Argentina\".\nThe 1603 star atlas \"Uranometria\" of Johann Bayer assigned stars to individual constellations and formalized the division by assigning a series of Greek and Latin letters to the stars within each constellation. These are known today as Bayer designations. Subsequent star atlases led to the development of today's accepted modern constellations.\nOrigin of the southern constellations.\nThe southern sky, below about \u221265\u00b0 declination, was only partially catalogued by ancient Babylonians, Egyptians, Greeks, Chinese, and Persian astronomers of the north. The knowledge that northern and southern star patterns differed goes back to Classical writers, who describe, for example, the African circumnavigation expedition commissioned by Egyptian Pharaoh Necho II in c.\u00a0600\u00a0BC and those of Hanno the Navigator in c.\u00a0500\u00a0BC.\nThe history of southern constellations is not straightforward. Different groupings and different names were proposed by various observers, some reflecting national traditions or designed to promote various sponsors. Southern constellations were important from the 14th to 16th centuries, when sailors used the stars for celestial navigation. Italian explorers who recorded new southern constellations include Andrea Corsali, Antonio Pigafetta, and Amerigo Vespucci.\nMany of the 88 IAU-recognized constellations in this region first appeared on celestial globes developed in the late 16th century by Petrus Plancius, based mainly on observations of the Dutch navigators Pieter Dirkszoon Keyser and Frederick de Houtman. These became widely known through Johann Bayer's star atlas \"Uranometria\" of 1603. more were created in 1763 by the French astronomer Nicolas Louis de Lacaille, who also split the ancient constellation Argo Navis into three; these new figures appeared in his star catalogue, published in 1756.\nSeveral modern proposals have not survived. The French astronomers Pierre Lemonnier and Joseph Lalande, for example, proposed constellations that were once popular but have since been dropped. The northern constellation Quadrans Muralis survived into the 19th century (when its name was attached to the Quadrantid meteor shower), but is now divided between Bo\u00f6tes and Draco.\n88 modern constellations.\nA list of 88 constellations was produced for the IAU in 1922. It is roughly based on the traditional Greek constellations listed by Ptolemy in his \"Almagest\" in the 2nd century and Aratus' work \"Phenomena\", with early modern modifications and additions (most importantly introducing constellations covering the parts of the southern sky unknown to Ptolemy) by Petrus Plancius (1592, 1597/98 and 1613), Johannes Hevelius (1690) and Nicolas Louis de Lacaille (1763), who introduced fourteen new constellations. Lacaille studied the stars of the southern hemisphere from 1751 until 1752 from the Cape of Good Hope, when he was said to have observed more than 10,000 stars using a refracting telescope with an aperture of .\nIn 1922, Henry Norris Russell produced a list of 88 constellations with three-letter abbreviations for them. However, these constellations did not have clear borders between them. In 1928, the IAU formally accepted the 88 modern constellations, with contiguous boundaries along vertical and horizontal lines of right ascension and declination developed by Eugene Delporte that, together, cover the entire celestial sphere; this list was finally published in 1930. Where possible, these modern constellations usually share the names of their Graeco-Roman predecessors, such as Orion, Leo, or Scorpius. The aim of this system is area-mapping, i.e. the division of the celestial sphere into contiguous fields. Out of the 88 modern constellations, 36 lie predominantly in the northern sky, and the other 52 predominantly in the southern.\nThe boundaries developed by Delporte used data that originated back to epoch B1875.0, which was when Benjamin A. Gould first made his proposal to designate boundaries for the celestial sphere, a suggestion on which Delporte based his work. The consequence of this early date is that because of the precession of the equinoxes, the borders on a modern star map, such as epoch J2000, are already somewhat skewed and no longer perfectly vertical or horizontal. This effect will increase over the years and centuries to come.\nSymbols.\nThe constellations have no official symbols, though those of the ecliptic may take the signs of the zodiac. Symbols for the other modern constellations, as well as older ones that still occur in modern nomenclature, have occasionally been published.\nDark cloud constellations.\nThe Great Rift, a series of dark patches in the Milky Way, is most visible in the southern sky. Some cultures have discerned shapes in these patches. Members of the Inca civilization identified various dark areas or dark nebulae in the Milky Way as animals and associated their appearance with the seasonal rains. Australian Aboriginal astronomy also describes dark cloud constellations, the most famous being the \"emu in the sky\" whose head is formed by the Coalsack, a dark nebula, instead of the stars.\nReferences.\nFootnotes\nCitations"}
{"id": "5269", "revid": "1152308", "url": "https://en.wikipedia.org/wiki?curid=5269", "title": "Character", "text": "Character or Characters may refer to:"}
{"id": "5270", "revid": "6056090", "url": "https://en.wikipedia.org/wiki?curid=5270", "title": "Car (disambiguation)", "text": "A car is a wheeled motor vehicle used for transporting passengers.\nCar(s), CAR(s), or The Car(s) may also refer to:"}
{"id": "5272", "revid": "42342156", "url": "https://en.wikipedia.org/wiki?curid=5272", "title": "Printer (computing)", "text": "In computing, a printer is a peripheral machine which makes a durable representation of graphics or text, usually on paper. While most output is human-readable, bar code printers are an example of an expanded use for printers. Different types of printers include 3D printers, inkjet printers, laser printers, and thermal printers.\nHistory.\nThe first computer printer designed was a mechanically driven apparatus by Charles Babbage for his difference engine in the 19th century; however, his mechanical printer design was not built until 2000.\nThe first patented printing mechanism for applying a marking medium to a recording medium or more particularly an electrostatic inking apparatus and a method for electrostatically depositing ink on controlled areas of a receiving medium, was in 1962 by C. R. Winston, Teletype Corporation, using continuous inkjet printing. The ink was a red stamp-pad ink manufactured by Phillips Process Company of Rochester, NY under the name Clear Print. This patent (US3060429) led to the Teletype Inktronic Printer product delivered to customers in late 1966.\nThe first compact, lightweight digital printer was the EP-101, invented by Japanese company Epson and released in 1968, according to Epson.\nThe first commercial printers generally used mechanisms from electric typewriters and Teletype machines. The demand for higher speed led to the development of new systems specifically for computer use. In the 1980s there were daisy wheel systems similar to typewriters, line printers that produced similar output but at much higher speed, and dot-matrix systems that could mix text and graphics but produced relatively low-quality output. The plotter was used for those requiring high-quality line art like blueprints.\nThe introduction of the low-cost laser printer in 1984, with the first HP LaserJet, and the addition of PostScript in next year's Apple LaserWriter set off a revolution in printing known as desktop publishing. Laser printers using PostScript mixed text and graphics, like dot-matrix printers, but at quality levels formerly available only from commercial typesetting systems. By 1990, most simple printing tasks like fliers and brochures were now created on personal computers and then laser printed; expensive offset printing systems were being dumped as scrap. The HP Deskjet of 1988 offered the same advantages as a laser printer in terms of flexibility, but produced somewhat lower-quality output (depending on the paper) from much less-expensive mechanisms. Inkjet systems rapidly displaced dot-matrix and daisy-wheel printers from the market. By the 2000s, high-quality printers of this sort had fallen under the $100 price point and became commonplace.\nThe rapid improvement of internet email through the 1990s and into the 2000s has largely displaced the need for printing as a means of moving documents, and a wide variety of reliable storage systems means that a \"physical backup\" is of little benefit today.\nStarting around 2010, 3D printing became an area of intense interest, allowing the creation of physical objects with the same sort of effort as an early laser printer required to produce a brochure. As of the 2020s, 3D printing has become a widespread hobby due to the abundance of cheap 3D printer kits, with the most common process being Fused deposition modeling.\nTypes.\nPersonal printer.\n\"Personal\" printers are mainly designed to support individual users, and may be connected to only a single computer. These printers are designed for low-volume, short-turnaround print jobs, requiring minimal setup time to produce a hard copy of a given document. They are generally slow devices ranging from 6 to around 25 pages per minute (ppm), and the cost per page is relatively high. However, this is offset by the on-demand convenience. Some printers can print documents stored on memory cards or from digital cameras and scanners.\nNetworked printer.\n\"Networked\" or \"shared\" printers are \"designed for high-volume, high-speed printing\". They are usually shared by many users on a network and can print at speeds of 45 to around 100 ppm. The Xerox 9700 could achieve 120 ppm.\nAn \"ID Card printer\" is used for printing plastic ID cards. These can now be customised with important features such as holographic overlays, HoloKotes and watermarks. This is either a direct to card printer (the more feasible option) or a retransfer printer.\nVirtual printer.\nA \"virtual printer\" is a piece of computer software whose user interface and API resembles that of a printer driver, but which is not connected with a physical computer printer. A virtual printer can be used to create a file which is an image of the data which would be printed, for archival purposes or as input to another program, for example to create a PDF or to transmit to another system or user.\nBarcode printer.\nA \"barcode printer\" is a computer peripheral for printing barcode labels or tags that can be attached to, or printed directly on, physical objects. Barcode printers are commonly used to label cartons before shipment, or to label retail items with UPCs or EANs.\n3D printer.\nA \"3D printer\" is a device for making a three-dimensional object from a 3D model or other electronic data source through additive processes in which successive layers of material (including plastics, metals, food, cement, wood, and other materials) are laid down under computer control. It is called a printer by analogy with an inkjet printer which produces a two-dimensional document by a similar process of depositing a layer of ink on paper.\nID card printer.\nA card printer is an electronic desktop printer with single card feeders which print and personalize plastic cards. In this respect they differ from, for example, label printers which have a continuous supply feed. Card dimensions are usually 85.60 \u00d7 53.98\u00a0mm, standardized under ISO/IEC 7810 as ID-1. This format is also used in EC-cards, telephone cards, credit cards, driver's licenses and health insurance cards. This is commonly known as the bank card format. Card printers are controlled by corresponding printer drivers or by means of a specific programming language. Generally card printers are designed with laminating, striping, and punching functions, and use desktop or web-based software. The hardware features of a card printer differentiate a card printer from the more traditional printers, as ID cards are usually made of PVC plastic and require laminating and punching. Different card printers can accept different card thickness and dimensions.\nThe principle is the same for practically all card printers: the plastic card is passed through a thermal print head at the same time as a color ribbon. The color from the ribbon is transferred onto the card through the heat given out from the print head. The standard performance for card printing is 300 dpi (300 dots per inch, equivalent to 11.8 dots per mm). There are different printing processes, which vary in their detail:\nVariations.\nBroadly speaking there are three main types of card printers, differing mainly by the method used to print onto the card. They are:\nDifferent ID Card Printers use different encoding techniques to facilitate disparate business environments and to support security initiatives. Known encoding techniques are:\nSoftware.\nThere are basically two categories of card printer software: desktop-based, and web-based (online). The biggest difference between the two is whether or not a customer has a printer on their network that is capable of printing identification cards. If a business already owns an ID card printer, then a desktop-based badge maker is probably suitable for their needs. Typically, large organizations who have high employee turnover will have their own printer. A desktop-based badge maker is also required if a company needs their IDs make instantly. An example of this is the private construction site that has restricted access. However, if a company does not already have a local (or network) printer that has the features they need, then the web-based option is a perhaps a more affordable solution. The web-based solution is good for small businesses that do not anticipate a lot of rapid growth, or organizations who either can not afford a card printer, or do not have the resources to learn how to set up and use one. Generally speaking, desktop-based solutions involve software, a database (or spreadsheet) and can be installed on a single computer or network.\nOther options.\nAlongside the basic function of printing cards, card printers can also read and encode magnetic stripes as well as contact and contact free RFID chip cards (smart cards). Thus card printers enable the encoding of plastic cards both visually and logically. Plastic cards can also be laminated after printing. Plastic cards are laminated after printing to achieve a considerable increase in durability and a greater degree of counterfeit prevention. Some card printers come with an option to print both sides at the same time, which cuts down the time taken to print and less margin of error. In such printers one side of id card is printed and then the card is flipped in the flip station and other side is printed.\nApplications.\nAlongside the traditional uses in time attendance and access control (in particular with photo personalization), countless other applications have been found for plastic cards, e.g. for personalized customer and members' cards, for sports ticketing and in local public transport systems for the production of season tickets, for the production of school and college identity cards as well as for the production of national ID cards.\nTechnology.\nThe choice of print technology has a great effect on the cost of the printer and cost of operation, speed, quality and permanence of documents, and noise. Some printer technologies do not work with certain types of physical media, such as carbon paper or transparencies.\nA second aspect of printer technology that is often forgotten is resistance to alteration: liquid ink, such as from an inkjet head or fabric ribbon, becomes absorbed by the paper fibers, so documents printed with liquid ink are more difficult to alter than documents printed with toner or solid inks, which do not penetrate below the paper surface.\nCheques can be printed with liquid ink or on special cheque paper with toner anchorage so that alterations may be detected. The machine-readable lower portion of a cheque must be printed using MICR toner or ink. Banks and other clearing houses employ automation equipment that relies on the magnetic flux from these specially printed characters to function properly.\nModern print technology.\nThe following printing technologies are routinely found in modern printers:\nLaser printers and other toner-based printers.\nA laser printer rapidly produces high quality text and graphics. As with digital photocopiers and multifunction printers (MFPs), laser printers employ a xerographic printing process but differ from analog photocopiers in that the image is produced by the direct scanning of a laser beam across the printer's photoreceptor.\nAnother toner-based printer is the LED printer which uses an array of LEDs instead of a laser to cause toner adhesion to the print drum.\nLiquid inkjet printers.\nInkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.\nSolid ink printers.\nSolid ink printers, also known as phase-change ink or hot-melt ink printers, are a type of thermal transfer printer, graphics sheet printer or 3D printer . They use solid sticks, crayons, pearls or granular ink materials. Common inks are CMYK-colored ink, similar in consistency to candle wax, which are melted and fed into a piezo crystal operated print-head. A Thermal transfer printhead jets the liquid ink on a rotating, oil coated drum. The paper then passes over the print drum, at which time the image is immediately transferred, or transfixed, to the page. Solid ink printers are most commonly used as color office printers and are excellent at printing on transparencies and other non-porous media. Solid ink is also called phase-change or hot-melt ink and was first used by Data Products and Howtek, Inc., in 1984. Solid ink printers can produce excellent results with text and images. Some solid ink printers have evolved to print 3D models, for example, Visual Impact Corporation of Windham, NH was started by retired Howtek employee, Richard Helinski whose 3D patents US4721635 and then US5136515 was licensed to Sanders Prototype, Inc., later named Solidscape, Inc. Acquisition and operating costs are similar to laser printers. Drawbacks of the technology include high energy consumption and long warm-up times from a cold state. Also, some users complain that the resulting prints are difficult to write on, as the wax tends to repel inks from pens, and are difficult to feed through automatic document feeders, but these traits have been significantly reduced in later models. This type of thermal transfer printer is only available from one manufacturer, Xerox, manufactured as part of their Xerox Phaser office printer line. Previously, solid ink printers were manufactured by Tektronix, but Tektronix sold the printing business to Xerox in 2001.\nDye-sublimation printers.\nA dye-sublimation printer (or dye-sub printer) is a printer that employs a printing process that uses heat to transfer dye to a medium such as a plastic card, paper, or canvas. The process is usually to lay one color at a time using a ribbon that has color panels. Dye-sub printers are intended primarily for high-quality color applications, including color photography; and are less well-suited for text. While once the province of high-end print shops, dye-sublimation printers are now increasingly used as dedicated consumer photo printers.\nThermal printers.\nThermal printers work by selectively heating regions of special heat-sensitive paper. Monochrome thermal printers are used in cash registers, ATMs, gasoline dispensers and some older inexpensive fax machines. Colors can be achieved with special papers and different temperatures and heating rates for different colors; these colored sheets are not required in black-and-white output. One example is Zink (a portmanteau of \"zero ink\").\nObsolete and special-purpose printing technologies.\nThe following technologies are either obsolete, or limited to special applications though most were, at one time, in widespread use.\nImpact printers.\n Impact printers rely on a forcible impact to transfer ink to the media. The impact printer uses a print head that either hits the surface of the ink ribbon, pressing the ink ribbon against the paper (similar to the action of a typewriter), or, less commonly, hits the back of the paper, pressing the paper against the ink ribbon (the IBM 1403 for example). All but the dot matrix printer rely on the use of \"fully formed characters\", letterforms that represent each of the characters that the printer was capable of printing. In addition, most of these printers were limited to monochrome, or sometimes two-color, printing in a single typeface at one time, although bolding and underlining of text could be done by \"overstriking\", that is, printing two or more impressions either in the same character position or slightly offset. Impact printers varieties include typewriter-derived printers, teletypewriter-derived printers, daisywheel printers, dot matrix printers, and line printers. Dot-matrix printers remain in common use in businesses where multi-part forms are printed. \"An overview of impact printing\" contains a detailed description of many of the technologies used.\nTypewriter-derived printers.\nSeveral different computer printers were simply computer-controllable versions of existing electric typewriters. The Friden Flexowriter and IBM Selectric-based printers were the most-common examples. The Flexowriter printed with a conventional typebar mechanism while the Selectric used IBM's well-known \"golf ball\" printing mechanism. In either case, the letter form then struck a ribbon which was pressed against the paper, printing one character at a time. The maximum speed of the Selectric printer (the faster of the two) was 15.5 characters per second.\nTeletypewriter-derived printers.\nThe common teleprinter could easily be interfaced with the computer and became very popular except for those computers manufactured by IBM. Some models used a \"typebox\" that was positioned, in the X- and Y-axes, by a mechanism, and the selected letter form was struck by a hammer. Others used a type cylinder in a similar way as the Selectric typewriters used their type ball. In either case, the letter form then struck a ribbon to print the letterform. Most teleprinters operated at ten characters per second although a few achieved 15 CPS.\nDaisy wheel printers.\nDaisy wheel printers operate in much the same fashion as a typewriter. A hammer strikes a wheel with petals, the \"daisy wheel\", each petal containing a letter form at its tip. The letter form strikes a ribbon of ink, depositing the ink on the page and thus printing a character. By rotating the daisy wheel, different characters are selected for printing. These printers were also referred to as \"letter-quality printers\" because they could produce text which was as clear and crisp as a typewriter. The fastest letter-quality printers printed at 30 characters per second.\nDot-matrix printers.\nThe term dot matrix printer is used for impact printers that use a matrix of small pins to transfer ink to the page. The advantage of dot matrix over other impact printers is that they can produce graphical images in addition to text; however the text is generally of poorer quality than impact printers that use letterforms (\"type\").\nDot-matrix printers can be broadly divided into two major classes:\nDot matrix printers can either be character-based or line-based (that is, a single horizontal series of pixels across the page), referring to the configuration of the print head.\nIn the 1970s and '80s, dot matrix printers were one of the more common types of printers used for general use, such as for home and small office use. Such printers normally had either 9 or 24 pins on the print head (early 7 pin printers also existed, which did not print descenders). There was a period during the early home computer era when a range of printers were manufactured under many brands such as the Commodore VIC-1525 using the Seikosha Uni-Hammer system. This used a single solenoid with an oblique striker that would be actuated 7 times for each column of 7 vertical pixels while the head was moving at a constant speed. The angle of the striker would align the dots vertically even though the head had moved one dot spacing in the time. The vertical dot position was controlled by a synchronized longitudinally ribbed platen behind the paper that rotated rapidly with a rib moving vertically seven dot spacings in the time it took to print one pixel column. 24-pin print heads were able to print at a higher quality and started to offer additional type styles and were marketed as Near Letter Quality by some vendors. Once the price of inkjet printers dropped to the point where they were competitive with dot matrix printers, dot matrix printers began to fall out of favour for general use.\nSome dot matrix printers, such as the NEC P6300, can be upgraded to print in color. This is achieved through the use of a four-color ribbon mounted on a mechanism (provided in an upgrade kit that replaces the standard black ribbon mechanism after installation) that raises and lowers the ribbons as needed. Color graphics are generally printed in four passes at standard resolution, thus slowing down printing considerably. As a result, color graphics can take up to four times longer to print than standard monochrome graphics, or up to 8-16 times as long at high resolution mode.\nDot matrix printers are still commonly used in low-cost, low-quality applications such as cash registers, or in demanding, very high volume applications like invoice printing. Impact printing, unlike laser printing, allows the pressure of the print head to be applied to a stack of two or more forms to print multi-part documents such as sales invoices and credit card receipts using continuous stationery with carbonless copy paper. It also has security advantages as ink impressed into a paper matrix by force is harder to erase invisibly. Dot-matrix printers were being superseded even as receipt printers after the end of the twentieth century.\nLine printers.\nLine printers print an entire line of text at a time. Four principal designs exist.\nIn each case, to print a line, precisely timed hammers strike against the back of the paper at the exact moment that the correct character to be printed is passing in front of the paper. The paper presses forward against a ribbon which then presses against the character form and the impression of the character form is printed onto the paper. Each system could have slight timing issues, which could cause minor misalignment of the resulting printed characters. For drum or typebar printers, this appeared as vertical misalignment, with characters being printed slightly above or below the rest of the line. In chain or bar printers, the misalignment was horizontal, with printed characters being crowded closer together or farther apart. This was much less noticeable to human vision than vertical misalignment, where characters seemed to bounce up and down in the line, so they were considered as higher quality print.\nLine printers are the fastest of all impact printers and are used for bulk printing in large computer centres. A line printer can print at 1100 lines per minute or faster, frequently printing pages more rapidly than many current laser printers. On the other hand, the mechanical components of line printers operate with tight tolerances and require regular preventive maintenance (PM) to produce a top quality print. They are virtually never used with personal computers and have now been replaced by high-speed laser printers. The legacy of line printers lives on in many operating systems, which use the abbreviations \"lp\", \"lpr\", or \"LPT\" to refer to printers.\nLiquid ink electrostatic printers.\nLiquid ink electrostatic printers use a chemical coated paper, which is charged by the print head according to the image of the document. The paper is passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image. This process was developed from the process of electrostatic copying. Color reproduction is very accurate, and because there is no heating the scale distortion is less than \u00b10.1%. (All laser printers have an accuracy of \u00b11%.)\nWorldwide, most survey offices used this printer before color inkjet plotters become popular. Liquid ink electrostatic printers were mostly available in width and also 6 color printing. These were also used to print large billboards. It was first introduced by Versatec, which was later bought by Xerox. 3M also used to make these printers.\nPlotters.\nPen-based plotters were an alternate printing technology once common in engineering and architectural firms. Pen-based plotters rely on contact with the paper (but not impact, per se) and special purpose pens that are mechanically run over the paper to create text and images. Since the pens output continuous lines, they were able to produce technical drawings of higher resolution than was achievable with dot-matrix technology. Some plotters used roll-fed paper, and therefore had a minimal restriction on the size of the output in one dimension. These plotters were capable of producing quite sizable drawings.\nOther printers.\nA number of other sorts of printers are important for historical reasons, or for special purpose uses.\nAttributes.\nConnectivity.\nPrinters can be connected to computers in many ways: directly by a dedicated data cable such as the USB, through a short-range radio like Bluetooth, a local area network using cables (such as the Ethernet) or radio (such as WiFi), or on a standalone basis without a computer, using a memory card or other portable data storage device.\nPrinter control languages.\nMost printers other than line printers accept control characters or unique character sequences to control various printer functions. These may range from shifting from lower to upper case or from black to red ribbon on typewriter printers to switching fonts and changing character sizes and colors on raster printers. Early printer controls were not standardized, with each manufacturer's equipment having its own set. The IBM Personal Printer Data Stream (PPDS) became a commonly used command set for dot-matrix printers.\nToday, most printers accept one or more page description languages (PDLs). Laser printers with greater processing power frequently offer support for variants of Hewlett-Packard's Printer Command Language (PCL), PostScript or XML Paper Specification. Most inkjet devices support manufacturer proprietary PDLs such as ESC/P. The diversity in mobile platforms have led to various standardization efforts around device PDLs such as the Printer Working Group (PWG's) PWG Raster.\nPrinting speed.\nThe speed of early printers was measured in units of \"characters per minute\" (cpm) for character printers, or \"lines per minute\" (lpm) for line printers. Modern printers are measured in \"pages per minute\" (ppm). These measures are used primarily as a marketing tool, and are not as well standardised as toner yields. Usually pages per minute refers to sparse monochrome office documents, rather than dense pictures which usually print much more slowly, especially color images. Speeds in ppm usually apply to A4 paper in most countries in the world, and letter paper size, about 6% shorter, in North America.\nPrinting mode.\nThe data received by a printer may be:\nSome printers can process all four types of data, others not.\nToday it is possible to print everything (even plain text) by sending ready bitmapped images to the printer. This allows better control over formatting, especially among machines from different vendors. Many printer drivers do not use the text mode at all, even if the printer is capable of it.\nMonochrome, color and photo printers.\nA monochrome printer can only produce monochrome images, with only shades of a single color. Most printers can produce only two colors, black (ink) and white (no ink). With half-tonning techniques, however, such a printer can produce acceptable grey-scale images too\nA color printer can produce images of multiple colors. A photo printer is a color printer that can produce images that mimic the color range (gamut) and resolution of prints made from photographic film.\nPage yield.\nThe page yield is the number of pages that can be printed from a toner cartridge or ink cartridge\u2014before the cartridge needs to be refilled or replaced.\nThe actual number of pages yielded by a specific cartridge depends on a number of factors.\nFor a fair comparison, many laser printer manufacturers use the ISO/IEC 19752 process to measure the toner cartridge yield.\nEconomics.\nIn order to fairly compare operating expenses of printers with a relatively small ink cartridge to printers with a larger, more expensive toner cartridge that typically holds more toner and so prints more pages before the cartridge needs to be replaced, many people prefer to estimate operating expenses in terms of cost per page (CPP).\nRetailers often apply the \"razor and blades\" model: a company may sell a printer at cost and make profits on the ink cartridge, paper, or some other replacement part. This has caused legal disputes regarding the right of companies other than the printer manufacturer to sell compatible ink cartridges. To protect their business model, several manufacturers invest heavily in developing new cartridge technology and patenting it.\nOther manufacturers, in reaction to the challenges from using this business model, choose to make more money on printers and less on ink, promoting the latter through their advertising campaigns. Finally, this generates two clearly different proposals: \"cheap printer\u00a0\u2013 expensive ink\" or \"expensive printer\u00a0\u2013 cheap ink\". Ultimately, the consumer decision depends on their reference interest rate or their time preference. From an economics viewpoint, there is a clear trade-off between cost per copy and cost of the printer.\nPrinter steganography.\nPrinter steganography is a type of steganography \u2013 \"hiding data within data\" \u2013 produced by color printers, including Brother, Canon, Dell, Epson, HP, IBM, Konica Minolta, Kyocera, Lanier, Lexmark, Ricoh, Toshiba and Xerox brand color laser printers, where tiny yellow dots are added to each page. The dots are barely visible and contain encoded printer serial numbers, as well as date and time stamps.\nManufacturers and market share.\nAs of 2020\u20132021, the largest worldwide vendor of printers is Hewlett-Packard, followed by Canon, Brother, Seiko Epson and Kyocera. Other known vendors include NEC, Ricoh, Xerox, Lexmark, OKI, Sharp, Konica Minolta, Samsung, Kodak, Dell, Toshiba, Star Micronics, Citizen and Panasonic."}
{"id": "5278", "revid": "45460150", "url": "https://en.wikipedia.org/wiki?curid=5278", "title": "Copyright", "text": "A copyright is a type of intellectual property that gives its owner the exclusive legal right to copy, distribute, adapt, display, and perform a creative work, usually for a limited time. The creative work may be in a literary, artistic, educational, or musical form. Copyright is intended to protect the original expression of an idea in the form of a creative work, but not the idea itself. A copyright is subject to limitations based on public interest considerations, such as the fair use doctrine in the United States and fair dealings doctrine in the United Kingdom.\nSome jurisdictions require \"fixing\" copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders. These rights normally include reproduction, control over derivative works, distribution, public performance, and moral rights such as attribution.\nCopyrights can be granted by public law and are in that case considered \"territorial rights\". This means that copyrights granted by the law of a certain state do not extend beyond the territory of that specific jurisdiction. Copyrights of this type vary by country; many countries, and sometimes a large group of countries, have made agreements with other countries on procedures applicable when works \"cross\" national borders or national rights are inconsistent.\nTypically, the public law duration of a copyright expires 50 to 100 years after the creator dies, depending on the jurisdiction. Some countries require certain copyright formalities to establishing copyright, others recognize copyright in any completed work, without a formal registration. When the copyright of a work expires, it enters the public domain.\nHistory.\nBackground.\nThe concept of copyright developed after the printing press came into use in Europe in the 15th and 16th centuries. It was associated with a common law and rooted in the civil law system. The printing press made it much cheaper to produce works, but as there was initially no copyright law, anyone could buy or rent a press and print any text. \nPopular new works were immediately re-set and re-published by competitors, so printers needed a constant stream of new material. Fees paid to authors for new works were high and significantly supplemented the incomes of many academics.\nPrinting brought profound social changes. The rise in literacy across Europe led to a dramatic increase in the demand for reading matter. Prices of reprints were low, so publications could be bought by poorer people, creating a mass audience. In German-language markets before the advent of copyright, technical materials, like popular fiction, were inexpensive and widely available; it has been suggested this contributed to Germany's industrial and economic success.\nConception.\nThe concept of copyright first developed in England. In reaction to the printing of \"scandalous books and pamphlets\", the English Parliament passed the Licensing of the Press Act 1662, which required all intended publications to be registered with the government-approved Stationers' Company, giving the Stationers the right to regulate what material could be printed.\nThe Statute of Anne, enacted in 1710 in England and Scotland, provided the first legislation to protect copyrights (but not authors' rights). The Copyright Act 1814 extended more rights for authors but did not protect British publications from being reprinted in the US. The Berne International Copyright Convention of 1886 finally provided protection for authors among the countries who signed the agreement, although the US did not join the Berne Convention until 1989.\nIn the US, the Constitution grants Congress the right to establish copyright and patent laws. Shortly after the Constitution was passed, Congress enacted the \"Copyright Act of 1790\", modeling it after the Statute of Anne. While the national law protected authors' published works, authority was granted to the states to protect authors' unpublished works. The most recent major overhaul of copyright in the US, the \"Copyright Act of 1976\", extended federal copyright to works as soon as they are created and \"fixed\", without requiring publication or registration. State law continues to apply to unpublished works that are not otherwise copyrighted by federal law. This act also changed the calculation of copyright term from a fixed term (then a maximum of fifty-six years) to \"life of the author plus 50 years\". These changes brought the US closer to conformity with the Berne Convention, and in 1989 the United States further revised its copyright law and joined the Berne Convention officially.\nCopyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in Europe and not, for example, in Asia. In the Middle Ages in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which capitalism led to the commodification of many aspects of social life that earlier had no monetary or economic value per\u00a0se.\nCopyright has developed into a concept that has a significant effect on nearly every modern industry, including not just literary work, but also forms of creative work such as sound recordings, films, photographs, software, and architecture.\nNational copyrights.\nOften seen as the first real copyright law, the 1709 British Statute of Anne gave authors and the publishers to whom they did chose to license their works, the right to publish the author's creations for a fixed period, after which the copyright expired. It was \"An Act for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or the Purchasers of such Copies, during the Times therein mentioned.\" \nThe act also alluded to individual rights of the artist. It began:\nA right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.\nThe Copyright Clause of the United States, Constitution (1787) authorized copyright legislation: \"To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.\" That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs. Yet scholars like Lawrence Lessig have argued that copyright terms have been extended beyond the scope imagined by the Framers. Lessig refers to the Copyright Clause as the \"Progress Clause\" to emphasize the social dimension of intellectual property rights.\nThe original length of copyright in the United States was 14\u00a0years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14\u2011year monopoly grant, but after that the work entered the public domain, so it could be used and built upon by others.\nContinental law.\nIn many jurisdictions of the European continent, comparable legal concepts to copyright did exist from the 16th century on but did change under Napoleonic rule into another legal concept: \"authors' rights\" or \"creator's right\" laws, from French: \"droits d'auteur\" and German \"Urheberrecht\". In many modern-day publications the terms copyright and authors' rights are being mixed, or used as translations, but in a juridical sense the legal concepts do essentially differ. Authors' rights are, generally speaking, from the start absolute property rights of an author of original work that one does not have to apply for. The law is automatically connecting an original work as intellectual property to its creator. Although the concepts throughout the years have been mingled globally, due to international treaties and contracts, distinct differences between jurisdictions continue to exist.\nCreator's law was enacted rather late in German speaking states and the economic historian Eckhard H\u00f6ffner argues that the absence of possibilities to maintain copyright laws in all these states in the early 19th century, encouraged the publishing of low-priced paperbacks for the masses. This was profitable for authors and led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century. After the introduction of creator's rights, German publishers started to follow English customs, in issuing only expensive book editions for wealthy customers.\nEmpirical evidence derived from the exogenous differential introduction of author's right (Italian: \"diritto d\u2019autore\") in Napoleonic Italy shows that \"basic copyrights increased both the number and the quality of operas, measured by their popularity and durability\".\nInternational copyright treaties.\nThe 1886 Berne Convention first established recognition of authors' rights among sovereign nations, rather than merely bilaterally. Under the Berne Convention, protective rights for creative works do not have to be asserted or declared, as they are automatically in force at creation: an author need not \"register\" or \"apply for\" these protective rights in countries adhering to the Berne Convention. As soon as a work is \"fixed\", that is, written or recorded on some physical medium, its author is automatically entitled to all intellectual property rights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the rights expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100\u00a0years later with the passage of the Copyright, Designs and Patents Act 1988. Specially, for educational and scientific research purposes, the Berne Convention provides the developing countries issue compulsory licenses for the translation or reproduction of copyrighted works within the limits prescribed by the convention. This was a special provision that had been added at the time of 1971 revision of the convention, because of the strong demands of the developing countries. The United States did not sign the Berne Convention until 1989.\nThe United States and most Latin American countries instead entered into the Buenos Aires Convention in 1910, which required a copyright notice on the work (such as \"all rights reserved\"), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms. The Universal Copyright Convention was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the Soviet Union and developing nations.\nThe regulations of the Berne Convention are incorporated into the World Trade Organization's TRIPS agreement (1995), thus giving the Berne Convention effectively near-global application.\nIn 1961, the United International Bureaux for the Protection of Intellectual Property signed the Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations. In 1996, this organization was succeeded by the founding of the World Intellectual Property Organization, which launched the 1996 WIPO Performances and Phonograms Treaty and the 2002 WIPO Copyright Treaty, which enacted greater restrictions on the use of technology to copy works in the nations that ratified it. The Trans-Pacific Partnership includes intellectual property provisions relating to copyright.\nCopyright laws and authors' right laws are standardized somewhat through these international conventions such as the Berne Convention and Universal Copyright Convention. These multilateral treaties have been ratified by nearly all countries, and international organizations such as the European Union require their member states to comply with them. All member states of the World Trade Organization are obliged to establish minimum levels of copyright protection. Nevertheless, important differences between the national regimes continue to exist.\nObtaining protection.\nOwnership.\nThe original holder of the copyright may be the employer of the author rather than the author themself if the work is a \"work for hire\". For example, in English law the Copyright, Designs and Patents Act 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a \"Work for Hire\". Typically, the first owner of a copyright is the person who created the work i.e. the author. But when more than one person creates the work, then a case of joint authorship can be made provided some criteria are met.\nEligible works.\nCopyright may apply to a wide range of creative, intellectual, or artistic forms, or \"works\". Specifics vary by jurisdiction, but these can include poems, theses, fictional characters, plays and other literary works, motion pictures, choreography, musical compositions, sound recordings, paintings, drawings, sculptures, photographs, computer software, radio and television broadcasts, and industrial designs. Graphic designs and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.\nCopyright does not cover ideas and information themselves, only the form or manner in which they are expressed. For example, the copyright to a Mickey Mouse cartoon restricts others from making copies of the cartoon or creating derivative works based on Disney's particular anthropomorphic mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough not to be judged copies of Disney's.\nOriginality.\nTypically, a work must meet minimal standards of originality in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the United Kingdom there has to be some \"skill, labour, and judgment\" that has gone into it. In Australia and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a trademark instead.\nCopyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.\nRegistration.\nIn all countries where the Berne Convention standards apply, copyright is automatic and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce their exclusive rights. However, while registration is not needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as \"prima facie\" evidence of a valid copyright and enables the copyright holder to seek statutory damages and attorney's fees. (In the US, registering after an infringement only enables one to receive actual damages and lost profits.)\nA widely circulated strategy to avoid the cost of copyright registration is referred to as the poor man's copyright. It proposes that the creator send the work to themself in a sealed envelope by registered mail, using the postmark to establish the date. This technique has not been recognized in any published opinions of the United States courts. The United States Copyright Office says the technique is not a substitute for actual registration. The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original or establish who created the work. \nFixing.\nThe Berne Convention allows member countries to decide whether creative works must be \"fixed\" to enjoy copyright. Article 2, Section 2 of the Berne Convention states:\nSome countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be \"fixed in a tangible medium of expression\" to obtain copyright protection. US law requires that the fixation be stable and permanent enough to be \"perceived, reproduced or communicated for a period of more than transitory duration\". Similarly, Canadian courts consider fixation to require that the work be \"expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance\".\nNote this provision of US law:\nCopyright notice.\nBefore 1989, United States law required the use of a copyright notice, consisting of the copyright symbol (\u00a9, the letter C inside a circle; Unicode ), the abbreviation \"Copr.\", or the word \"Copyright\", followed by the year of the first publication of the work and the name of the copyright holder. Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a sound recording copyright symbol (\u2117, the letter\u00a0P inside a circle, Unicode ), which indicates a sound recording copyright, with the letter\u00a0P indicating a \"phonorecord\". In addition, the phrase \"All rights reserved\" which indicates that the copyright holder reserves, or holds for their own use was once required to assert copyright, but that phrase is now legally obsolete. Almost everything on the Internet has some sort of copyright attached to it. Whether these things are watermarked, signed, or have any other sort of indication of the copyright is a different story however.\nIn 1989 the United States enacted the \"Berne Convention Implementation Act\", amending the Copyright Act of 1976 to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic. However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit\u00a0\u2013 using notices of this form may reduce the likelihood of a defense of \"innocent infringement\" being successful.\nPublisher's copyright.\nIn the UK, the publisher of a work automatically owns the copyright in the \"typographical arrangement of a published work\", i.e. its layout and general appearance as a published work. This copyright lasts for 25 years after the end of the year in which the edition containing that arrangement was first published.\nEnforcement.\nCopyrights are generally enforced by the holder in a civil law court, but there are also criminal infringement statutes in some jurisdictions. While central registries are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily prove that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the RIAA are increasingly targeting the file sharing home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (\"See Legal aspects of file sharing\")\nIn most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.\nSelf-enforcement measures.\nWith older technology like paintings, books, phonographs, and film, it is generally not feasible for consumers to make copies on their own, so producers can simply require payment when transferring physical possession of the storage medium. The equivalent for digital online content is a paywall.\nThe introduction of the photocopier, cassette tape, and videotape made it easier for consumers to copy materials like books and music, but each time a copy was made, it lost some fidelity. Digital media like text, audio, video, and software (even when stored on physical media like compact discs and DVDs) can be copied losslessly, and shared on the Internet, creating a much bigger threat to producer revenue. Some have used digital rights management technology to restrict non-playback access through encryption and other means. Digital watermarks can be used to trace copies, deterring infringement with a more credible threat of legal consequences. Copy protection is used for both digital and pre-Internet electronic media.\nCopyright infringement.\nFor a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws or adheres to a bilateral treaty or established international convention such as the Berne Convention or WIPO Copyright Treaty. Improper use of materials outside of legislation is deemed \"unauthorized edition\", not copyright infringement.\nStatistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available. Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect. In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.\nAccording to the IP Commission Report the annual cost of intellectual property infringement to the US economy \"continues to exceed $225 billion in counterfeit goods, pirated software, and theft of trade secrets and could be as high as $600 billion.\" A 2019 study sponsored by the US Chamber of Commerce Global Innovation Policy Center (GIPC), in partnership with NERA Economic Consulting \"estimates that global online piracy costs the U.S. economy at least $29.2 billion in lost revenue each year.\" An August 2021 report by the Digital Citizens Alliance states that \"online criminals who offer stolen movies, TV shows, games, and live events through websites and apps are reaping $1.34 billion in annual advertising revenues.\" This comes as a result of users visiting pirate websites who are then subjected to pirated content, malware, and fraud.\nRights granted.\nAccording to World Intellectual Property Organisation, copyright protects two types of rights. Economic rights allow right owners to derive financial reward from the use of their works by others. Moral rights allow authors and creators to take certain actions to preserve and protect their link with their work. The author or creator may be the owner of the economic rights, or those rights may be transferred to one or more copyright owners. Many countries do not allow the transfer of moral rights.\nEconomic rights.\nWith any kind of property, its owner may decide how it is to be used, and others can use it lawfully only if they have the owner's permission, often through a license. The owner's use of the property must, however, respect the legally recognised rights and interests of other members of society. So the owner of a copyright-protected work may decide how to use the work and may prevent others from using it without permission. National laws usually grant copyright owners exclusive rights to allow third parties to use their works, subject to the legally recognised rights and interests of others. Most copyright laws state that authors or other right owners have the right to authorise or prevent certain acts in relation to a work. Right owners can authorise or prohibit:\nMoral rights.\nMoral rights are concerned with the non-economic rights of a creator. They protect the creator's connection with a work as well as the integrity of the work. Moral rights are only accorded to individual authors and in many national laws they remain with the authors even after the authors have transferred their economic rights. In some EU countries, such as France, moral rights last indefinitely. In the UK, however, moral rights are finite. That is, the right of attribution and the right of integrity last only as long as the work is in copyright. When the copyright term comes to an end, so too do the moral rights in that work. This is just one reason why the moral rights regime within the UK is often regarded as weaker or inferior to the protection of moral rights in continental Europe and elsewhere in the world. The Berne Convention, in Article 6bis, requires its members to grant authors the following rights:\nThese and other similar rights granted in national laws are generally known as the moral rights of authors. The Berne Convention requires these rights to be independent of authors' economic rights. Moral rights are only accorded to individual authors and in many national laws they remain with the authors even after the authors have transferred their economic rights. This means that even where, for example, a film producer or publisher owns the economic rights in a work, in many jurisdictions the individual author continues to have moral rights. Recently, as a part of the debates being held at the US Copyright Office on the question of inclusion of Moral Rights as a part of the framework of the Copyright Law in United States, the Copyright Office concluded that many diverse aspects of the current moral rights patchwork \u2013 including copyright law's derivative work right, state moral rights statutes, and contract law \u2013 are generally working well and should not be changed. Further, the Office concludes that there is no need for the creation of a blanket moral rights statute at this time. However, there are aspects of the US moral rights patchwork that could be improved to the benefit of individual authors and the copyright system as a whole.\nIn the copyright law of the United States, several exclusive rights are granted to the holder of a copyright, as are listed below:\nThe basic right when a work is protected by copyright is that the holder may determine and decide how and under what conditions the protected work may be used by others. This includes the right to decide to distribute the work for free. This part of copyright is often overseen. The phrase \"exclusive right\" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a \"negative right\", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the unregistered design right in English law and European law. The rights of the copyright holder also permit them to not use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a philosophical interpretation of copyright law that is not universally shared. There is also debate on whether copyright should be considered a property right or a moral right.\nUK copyright law gives creators both economic rights and moral rights. While 'copying' someone else's work without permission may constitute an infringement of their economic rights, that is, the reproduction right or the right of communication to the public, whereas, 'mutilating' it might infringe the creator's moral rights. In the UK, moral rights include the right to be identified as the author of the work, which is generally identified as the right of attribution, and the right not to have your work subjected to 'derogatory treatment', that is the right of integrity.\nIndian copyright law is at parity with the international standards as contained in TRIPS. The Indian \"Copyright Act, 1957\", pursuant to the amendments in 1999, 2002 and 2012, fully reflects the Berne Convention and the Universal Copyrights Convention, to which India is a party. India is also a party to the Geneva Convention for the Protection of Rights of Producers of Phonograms and is an active member of the World Intellectual Property Organization (WIPO) and United Nations Educational, Scientific and Cultural Organization (UNESCO). The Indian system provides both the economic and moral rights under different provisions of its Indian Copyright Act of 1957.\nDuration.\nCopyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been published, and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States and the United Kingdom), copyrights expire at the end of the calendar year in which they would otherwise expire.\nThe length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.\nIn the United States, all books and other works, except for sound recordings, published before 1929 have expired copyrights and are in the public domain. The applicable date for sound recordings in the United States is before 1923. In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain. Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.\nBut if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the US, the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.\nIn 1998, the length of a copyright in the United States was increased by 20 years under the \"Copyright Term Extension Act\". This legislation was the subject of substantial criticism following allegations that the bill was strongly promoted by corporations which had valuable copyrights which otherwise would have expired.\nLimitations and exceptions.\nIn many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses. United States copyright law does not cover names, titles, short phrases or listings (such as ingredients, recipes, labels, or formulas). However, there are protections available for those areas copyright does not cover, such as trademarks and patents.\nIdea\u2013expression dichotomy and the merger doctrine.\nThe idea\u2013expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of \"Baker v. Selden\", has since been codified by the \"Copyright Act of 1976\" at 17 U.S.C. \u00a7\u00a0102(b).\nThe first-sale doctrine and exhaustion of rights.\nCopyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or CD. In the United States this is known as the first-sale doctrine, and was established by the courts to clarify the legality of reselling books in second-hand bookstores.\nSome countries may have parallel importation restrictions that allow the copyright holder to control the aftermarket. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as exhaustion of rights in other countries and is a principle which also applies, though somewhat differently, to patent and trademark rights. While this doctrine permits the transfer of the particular legitimate copy involved, it does not permit making or distributing additional copies.\nIn \"Kirtsaeng v. John Wiley &amp; Sons, Inc.\", in 2013, the United States Supreme Court held in a 6\u20133 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission. The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.\nIn addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying one's own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement moral rights, a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.\nFair use and fair dealing.\nCopyright does not prohibit all copying or replication. In the United States, the fair use doctrine, codified by the \"Copyright Act of 1976\" as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:\nIn the United Kingdom and many other Commonwealth countries, a similar notion of fair dealing was established by the courts or through legislation. The concept is sometimes not well defined; however, in Canada, private copying for personal use has been expressly permitted by statute since 1999. In \"Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)\", 2012 SCC 37, the Supreme Court of Canada concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the \"Copyright Act 1968\" (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. legal advice). Under current Australian law, although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to \"format shift\" that work from one medium to another for personal, private use, or to \"time shift\" a broadcast work for later, once and only once, viewing or listening. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.\nIn the United States the AHRA (\"Audio Home Recording Act\" Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.\nLater acts amended US copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The \"Digital Millennium Copyright Act\" prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner. An appellate court has held that fair use is not a defense to engaging in such distribution. In \"Lenz v. Universal Music Corp.,\" the United States Court of Appeals for the Ninth Circuit affirmed the lower court decision, holding that \"fair use is 'authorized by the law' and a copyright holder must consider the existence of fair use before sending a takedown notification\" under the \"Digital Millennium Copyright Act\". \nEU copyright laws recognise the right of EU member states to implement some national exceptions to copyright. Examples of those exceptions are:\nAccessible copies.\nIt is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired people without permission from the copyright holder.\nReligious Service Exemption.\nIn the US there is a Religious Service Exemption (1976 law, section 110[3]), namely \"performance of a non-dramatic literary or musical work or of a dramatico-musical work of a religious nature or display of a work, in the course of services at a place of worship or other religious assembly\" shall not constitute infringement of copyright.\nUseful articles.\nIn Canada, items deemed \"useful articles\" such as clothing designs are exempted from copyright protection under the \"Copyright Act\" if reproduced more than 50 times. Fast fashion brands may reproduce clothing designs from smaller companies without violating copyright protections.\nTransfer, assignment and licensing.\nA copyright, or aspects of it (e.g. reproduction alone, all but moral rights), may be assigned or transferred from one party to another. For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the Internet; however, the record industry attempts to provide promotion and marketing for the artist and their work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy or distribute the work in a particular region or for a specified period of time.\nA transfer or license may have to meet particular formal requirements in order to be effective, for example under the Australian Copyright Act 1968 the copyright itself must be expressly transferred in writing. Under the US Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus, exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under US law. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a real estate transaction.\nCopyright may also be licensed. Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed statutory license (e.g. musical works in the United States used for radio broadcast or performance). This is also called a compulsory license, because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made. Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, copyright collectives or collecting societies and performing rights organizations (such as ASCAP, BMI, and SESAC) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.\nFree licenses.\nCopyright licenses known as \"open\" or free licenses seek to grant several rights to licensees, either for a fee or not. \"Free\" in this context is not as much of a reference to price as it is to freedom. What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the Free Software Definition, the Debian Free Software Guidelines, the Open Source Definition and the Definition of Free Cultural Works. Further refinements to these definitions have resulted in categories such as copyleft and permissive. Common examples of free licenses are the GNU General Public License, BSD licenses and some Creative Commons licenses.\nFounded in 2001 by James Boyle, Lawrence Lessig, and Hal Abelson, the Creative Commons (CC) is a non-profit organization which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of generic copyright license options to the public, gratis. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.\nTerms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available (although some of them are not properly free per the above definitions and per Creative Commons' own advice). These are based upon copyright-holder stipulations such as whether they are willing to allow modifications to the work, whether they permit the creation of derivative works and whether they are willing to permit commercial use of the work. approximately 130 million individuals had received such licenses.\nCriticism.\nSome sources are critical of particular aspects of the copyright system. This is known as a debate over copynorms. Particularly to the background of uploading content to internet platforms and the digital exchange of original work, there is discussion about the copyright aspects of downloading and streaming, the copyright aspects of hyperlinking and framing.\nConcerns are often couched in the language of digital rights, digital freedom, database rights, open data or censorship. Discussions include \"Free Culture\", a 2004 book by Lawrence Lessig. Lessig coined the term permission culture to describe a worst-case system. The documentaries \"Good Copy Bad Copy\" and \"\" discuss copyright. Some suggest an alternative compensation system. In Europe consumers are acting up against the rising costs of music, film and books, and as a result Pirate Parties have been created. Some groups reject copyright altogether, taking an anti-copyright stance. The perceived inability to enforce copyright online leads some to advocate ignoring legal statutes when on the web.\nPublic domain.\nCopyright, like other intellectual property rights, is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be used or exploited by anyone without obtaining permission, and normally without payment. However, in paying public domain regimes the user may still have to pay royalties to the state or to an authors' association. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a common law copyright. Public domain works should not be confused with works that are publicly available. Works posted in the internet, for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright."}
{"id": "5282", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=5282", "title": "Catalan language", "text": "Catalan () is a Western Romance language. It is the official language of Andorra, and an official language of three autonomous communities in eastern Spain: Catalonia, the Balearic Islands and the Valencian Community, where it is called \"Valencian\" (). It has semi-official status in the Italian \"comune\" of Alghero, and it is spoken in the Pyr\u00e9n\u00e9es-Orientales department of France and in two further areas in eastern Spain: the eastern strip of Aragon and the Carche area in the Region of Murcia. The Catalan-speaking territories are often called the or \"Catalan Countries\".\nThe language evolved from Vulgar Latin in the Middle Ages around the eastern Pyrenees. Nineteenth-century Spain saw a Catalan literary revival, culminating in the early 1900s.\nEtymology and pronunciation.\nThe word \"Catalan\" is derived from the territorial name of Catalonia, itself of disputed etymology. The main theory suggests that () derives from the name or ('Land of the Goths'), since the origins of the Catalan counts, lords and people were found in the March of Gothia, whence \"Gothland\" &gt; \"Gothlandia\" &gt; \"Gothalania\" &gt; \"Catalonia\" theoretically derived.\nIn English, the term referring to a person first appears in the mid 14th century as \"Catelaner\", followed in the 15th century as (from Middle French). It is attested a language name since at least 1652. The word \"Catalan\" can be pronounced in English as or .\nThe endonym is pronounced in the Eastern Catalan dialects, and in the Western dialects. In the Valencian Community and Carche, the term is frequently used instead. Thus, the name \"Valencian\", although often employed for referring to the varieties specific to the Valencian Community and Carche, is also used by Valencians as a name for the language as a whole, synonymous with \"Catalan\". Both uses of the term have their respective entries in the dictionaries by the \"Acad\u00e8mia Valenciana de la Llengua\" (AVL) and the \"Institut d'Estudis Catalans\" (IEC). (See also status of Valencian below).\nHistory.\nMiddle Ages.\nBy the 9th century, Catalan had evolved from Vulgar Latin on both sides of the eastern end of the Pyrenees, as well as the territories of the Roman province of Hispania Tarraconensis to the south. From the 8th century onwards the Catalan counts extended their territory southwards and westwards at the expense of the Muslims, bringing their language with them. This process was given definitive impetus with the separation of the County of Barcelona from the Carolingian Empire in 988.\nIn the 11th century, documents written in macaronic Latin begin to show Catalan elements, with texts written almost completely in Romance appearing by 1080. Old Catalan shared many features with Gallo-Romance, diverging from Old Occitan between the 11th and 14th centuries.\nDuring the 11th and 12th centuries the Catalan rulers expanded southward to the Ebro river, and in the 13th century they conquered the lands that would become the Kingdoms of Valencia and the Majorca. The city of Alghero in Sardinia was repopulated with Catalan speakers in the 14th century. The language also reached Murcia, which became Spanish-speaking in the 15th century.\nIn the Low Middle Ages, Catalan went through a golden age, reaching a peak of maturity and cultural richness. Examples include the work of Majorcan polymath Ramon Llull (1232\u20131315), the Four Great Chronicles (13th\u201314th centuries), and the Valencian school of poetry culminating in Ausi\u00e0s March (1397\u20131459). By the 15th century, the city of Valencia had become the sociocultural center of the Crown of Aragon, and Catalan was present all over the Mediterranean world. During this period, the Royal Chancery propagated a highly standardized language. Catalan was widely used as an official language in Sicily until the 15th century, and in Sardinia until the 17th. During this period, the language was what Costa Carreras terms \"one of the 'great languages' of medieval Europe\".\nMartorell's novel of chivalry \"Tirant lo Blanc\" (1490) shows a transition from Medieval to Renaissance values, something that can also be seen in Metge's work. The first book produced with movable type in the Iberian Peninsula was printed in Catalan.\nStart of the modern era.\nSpain.\nWith the union of the crowns of Castille and Aragon in 1479, the Spanish kings ruled over different kingdoms, each with its own cultural, linguistic and political particularities, and they had to swear by the laws of each territory before the respective parliaments. But after the War of the Spanish Succession, Spain became an absolute monarchy under Philip V, which led to the assimilation of the Crown of Aragon by the Crown of Castile through the Nueva Planta decrees, as a first step in the creation of the Spanish nation-state; as in other contemporary European states, this meant the imposition of the political and cultural characteristics of the dominant groups. Since the political unification of 1714, Spanish assimilation policies towards national minorities have been a constant.\nThe process of assimilation began with secret instructions to the corregidores of the Catalan territory: they \"will take the utmost care to introduce the Castilian language, for which purpose he will give the most temperate and disguised measures so that the effect is achieved, without the care being noticed\". From there, actions in the service of assimilation, discreet or aggressive, were continued, and reached to the last detail, such as, in 1799, the Royal Certificate forbidding anyone to \"represent, sing and dance pieces that were not in Spanish\". The use of Spanish gradually became more prestigious and marked the start of the decline of Catalan. Starting in the 16th century, Catalan literature came under the influence of Spanish, and the nobles, part of the urban and literary classes became bilingual.\nFrance.\nWith the Treaty of the Pyrenees (1659), Spain ceded the northern part of Catalonia to France, and soon thereafter the local Catalan varieties came under the influence of French, which in 1700 became the sole official language of the region.\nShortly after the French Revolution (1789), the French First Republic prohibited official use of, and enacted discriminating policies against, the regional languages of France, such as Catalan, Alsatian, Breton, Occitan, Flemish, and Basque.\nFrance: 19th to 20th century.\nAfter the French colony of Algeria was established in 1830, many Catalan-speaking settlers moved there. People from the Spanish province of Alicante settled around Oran, while those from French Catalonia and Menorca migrated to Algiers.\nBy 1911, there were around 100,000 speakers of \"Patuet\", as their speech was called. After the Algerian declaration of independence in 1962, almost all the \"Pied-Noir\" Catalan speakers fled to Northern Catalonia or Alicante.\nThe French government only recognizes French as an official language. Nevertheless, on 10 December 2007, the then General Council of the Pyr\u00e9n\u00e9es-Orientales officially recognized Catalan as one of the d\u00e9partment's languages and seeks to further promote it in public life and education.\nSpain: 18th to 20th century.\nIn 1807, the Statistics Office of the French Ministry of the Interior asked the prefects for an official survey on the limits of the French language. The survey found that in Roussillon, almost only Catalan was spoken, and since Napoleon wanted to incorporate Catalonia into France, as happened in 1812, the consul in Barcelona was also asked. He declared that Catalan \"is taught in schools, it is printed and spoken, not only among the lower class, but also among people of first quality, also in social gatherings, as in visits and congresses\", indicating that it was spoken everywhere \"with the exception of the royal courts\". He also indicated that Catalan was spoken \"in the Kingdom of Valencia, in the islands of Mallorca, Menorca, Ibiza, Sardinia, Corsica and much of Sicily, in the Vall d \"Aran and Cerda\u00f1a\".\nThe defeat of the pro-Habsburg coalition in the War of Spanish Succession (1714) initiated a series of laws which, among other centralizing measures, imposed the use of Spanish in legal documentation all over Spain. Because of this, use of the Catalan language declined into the 18th century.\nHowever, the 19th century saw a Catalan literary revival (), which has continued up to the present day. This period starts with Aribau's \"Ode to the Homeland\" (1833); followed in the second half of the 19th century, and the early 20th by the work of Verdaguer (poetry), Oller (realist novel), and Guimer\u00e0 (drama). In the 19th century, the region of Carche, in the province of Murcia was repopulated with Valencian speakers. Catalan spelling was standardized in 1913 and the language became official during the Second Spanish Republic (1931\u20131939). The Second Spanish Republic saw a brief period of tolerance, with most restrictions against Catalan lifted. The Generalitat (the autonomous government of Catalonia, established during the Republic in 1931) made a normal use of Catalan in its administration and put efforts to promote it at the social level, including in schools and the University of Barcelona.\nThe Catalan language and culture were still vibrant during the Spanish Civil War (1936\u20131939), but were crushed at an unprecedented level throughout the subsequent decades due to Francoist dictatorship (1939\u20131975), which abolished the official status of Catalan and imposed the use of Spanish in schools and in public administration in all of Spain, while banning the use of Catalan in them. Between 1939 and 1943 newspapers and book printing in Catalan almost disappeared. Francisco Franco's desire for a homogeneous Spanish population resonated with some Catalans in favor of his regime, primarily members of the upper class, who began to reject the use of Catalan. Despite all of these hardships, Catalan continued to be used privately within households, and it was able to survive Franco's dictatorship. At the end of World War II, however, some of the harsh measures began to be lifted and, while Spanish language remained the sole promoted one, limited number of Catalan literature began to be tolerated. Several prominent Catalan authors resisted the suppression through literature. Private initiative contests were created to reward works in Catalan, among them \"Joan Martorell\" prize (1947), \"V\u00edctor Catal\u00e0\" prize (1953) \"Carles Riba\" award (1950), or the Honor Award of Catalan Letters (1969). The first Catalan-language TV show was broadcast in 1964. At the same time, oppression of the Catalan language and identity was carried out in schools, through governmental bodies, and in religious centers.\nIn addition to the loss of prestige for Catalan and its prohibition in schools, migration during the 1950s into Catalonia from other parts of Spain also contributed to the diminished use of the language. These migrants were often unaware of the existence of Catalan, and thus felt no need to learn or use it. Catalonia was the economic powerhouse of Spain, so these migrations continued to occur from all corners of the country. Employment opportunities were reduced for those who were not bilingual. Daily newspapers remained exclusively in Spanish until after Franco's death, when the first one in Catalan since the end of the Civil War, Avui, began to be published in 1976.\nPresent day.\nSince the Spanish transition to democracy (1975\u20131982), Catalan has been institutionalized as an official language, language of education, and language of mass media; all of which have contributed to its increased prestige. In Catalonia, there is an unparalleled large bilingual European non-state linguistic community. The teaching of Catalan is mandatory in all schools, but it is possible to use Spanish for studying in the public education system of Catalonia in two situations\u2014if the teacher assigned to a class chooses to use Spanish, or during the learning process of one or more recently arrived immigrant students. There is also some intergenerational shift towards Catalan.\nMore recently, several Spanish political forces have tried to increase the use of Spanish in the Catalan educational system. As a result, in May 2022 the Spanish Supreme Court urged the Catalan regional government to enforce a measure by which 25% of all lessons must be taught in Spanish.\nAccording to the Statistical Institute of Catalonia, in 2013 the Catalan language is the second most commonly used in Catalonia, after Spanish, as a native or self-defining language: 7% of the population self-identifies with both Catalan and Spanish equally, 36.4% with Catalan and 47.5% only Spanish. In 2003 the same studies concluded no language preference for self-identification within the population above 15 years old: 5% self-identified with both languages, 44.3% with Catalan and 47.5% with Spanish. To promote use of Catalan, the Generalitat de Catalunya (Catalonia's official Autonomous government) spends part of its annual budget on the promotion of the use of Catalan in Catalonia and in other territories, with entities such as (Consortium for Linguistic Normalization).\nIn Andorra, Catalan has always been the sole official language. Since the promulgation of the 1993 constitution, several policies favoring Catalan have been enforced, such as Catalan medium education.\nOn the other hand, there are several language shift processes currently taking place. In the Northern Catalonia area of France, Catalan has followed the same trend as the other minority languages of France, with most of its native speakers being 60 or older (as of 2004). Catalan is studied as a foreign language by 30% of the primary education students, and by 15% of the secondary. The cultural association promotes a network of community-run schools engaged in Catalan language immersion programs.\nIn Alicante province, Catalan is being replaced by Spanish and in Alghero by Italian. There is also well ingrained diglossia in the Valencian Community, Ibiza, and to a lesser extent, in the rest of the Balearic islands.\nDuring the 20th century many Catalans emigrated or went into exile to Venezuela, Mexico, Cuba, Argentina, and other South American countries. They formed a large number of Catalan colonies that today continue to maintain the Catalan language. They also founded many Catalan casals (associations).\nClassification and relationship with other Romance languages.\nOne classification of Catalan is given by P\u00e8ire B\u00e8c:\nHowever, the ascription of Catalan to the Occitano-Romance branch of Gallo-Romance languages is not shared by all linguists and philologists, particularly among Spanish ones, such as Ram\u00f3n Men\u00e9ndez Pidal.\nCatalan bears varying degrees of similarity to the linguistic varieties subsumed under the cover term \"Occitan language\" (see also differences between Occitan and Catalan and Gallo-Romance languages). Thus, as it should be expected from closely related languages, Catalan today shares many traits with other Romance languages.\nRelationship with other Romance languages.\nSome include Catalan in Occitan, as the linguistic distance between this language and some Occitan dialects (such as the Gascon dialect) is similar to the distance among different Occitan dialects. Catalan was considered a dialect of Occitan until the end of the 19th century and still today remains its closest relative.\nCatalan shares many traits with the other neighboring Romance languages (Occitan, French, Italian, Sardinian as well as Spanish and Portuguese among others). However, despite being spoken mostly on the Iberian Peninsula, Catalan has marked differences with the Iberian Romance group (Spanish and Portuguese) in terms of pronunciation, grammar, and especially vocabulary; it shows instead its closest affinity with languages native to France and northern Italy, particularly Occitan and to a lesser extent Gallo-Romance (Franco-Proven\u00e7al, French, Gallo-Italian).\nAccording to Ethnologue, the lexical similarity between Catalan and other Romance languages is: 87% with Italian; 85% with Portuguese and Spanish; 76% with Ladin and Romansh; 75% with Sardinian; and 73% with Romanian.\nDuring much of its history, and especially during the Francoist dictatorship (1939\u20131975), the Catalan language was ridiculed as a mere dialect of Spanish. This view, based on political and ideological considerations, has no linguistic validity. Spanish and Catalan have important differences in their sound systems, lexicon, and grammatical features, placing the language in features closer to Occitan (and French).\nThere is evidence that, at least from the 2nd century AD, the vocabulary and phonology of Roman Tarraconensis was different from the rest of Roman Hispania. Differentiation arose generally because Spanish, Asturian, and Galician-Portuguese share certain peripheral archaisms (Spanish , Asturian and Portuguese vs. Catalan , Occitan \"to boil\") and innovatory regionalisms (Spanish , Asturian vs. Catalan , Occitan \"bullock\"), while Catalan has a shared history with the Western Romance innovative core, especially Occitan.\nLike all Romance languages, Catalan has a handful of native words which are unique to it, or rare elsewhere. These include:\nThe Gothic superstrate produced different outcomes in Spanish and Catalan. For example, Catalan \"mud\" and \"to roast\", of Germanic origin, contrast with Spanish and , of Latin origin; whereas Catalan \"spinning wheel\" and \"temple\", of Latin origin, contrast with Spanish and , of Germanic origin.\nThe same happens with Arabic loanwords. Thus, Catalan \"large earthenware jar\" and \"tile\", of Arabic origin, contrast with Spanish and , of Latin origin; whereas Catalan \"oil\" and \"olive\", of Latin origin, contrast with Spanish and . However, the Arabic element is generally much more prevalent in Spanish.\nSituated between two large linguistic blocks (Iberian Romance and Gallo-Romance), Catalan has many unique lexical choices, such as \"to miss somebody\", \"to calm somebody down\", and \"reject\".\nGeographic distribution.\nCatalan-speaking territories.\nTraditionally Catalan-speaking territories are sometimes called the (Catalan Countries), a denomination based on cultural affinity and common heritage, that has also had a subsequent political interpretation but no official status. Various interpretations of the term may include some or all of these regions.\nNumber of speakers.\nThe number of people known to be fluent in Catalan varies depending on the sources used. A 2004 study did not count the total number of speakers, but estimated a total of 9\u20139.5\u00a0million by matching the percentage of speakers to the population of each area where Catalan is spoken. The web site of the Generalitat de Catalunya estimated that as of 2004 there were 9,118,882 speakers of Catalan. These figures only reflect potential speakers; today it is the native language of only 35.6% of the Catalan population. According to \"Ethnologue\", Catalan had 4.1\u00a0million native speakers and 5.1\u00a0million second-language speakers in 2021.\nAccording to a 2011 study the total number of Catalan speakers was over 9.8\u00a0million, with 5.9\u00a0million residing in Catalonia. More than half of them spoke Catalan as a second language, with native speakers being about 4.4\u00a0million of those (more than 2.8 in Catalonia). Very few Catalan monoglots exist; virtually all of the Catalan speakers in Spain are bilingual speakers of Catalan and Spanish, with 99.7% of Catalan speakers in Catalonia able to speak Spanish and 99.9% able to understand it.\nIn Roussillon, only a minority of French Catalans speak Catalan nowadays, with French being the majority language for the inhabitants after a continued process of language shift. According to a 2019 survey by the Catalan government, 31.5% of the inhabitants of Catalonia predominantly spoke Catalan at home whereas 52.7% spoke Spanish, 2.8% both Catalan and Spanish and 10.8% other languages.\nSpanish was the most spoken language in Barcelona (according to the linguistic census held by the Government of Catalonia in 2013) and it is understood almost universally. According to 2013 census, Catalan was also very commonly spoken in the city of 1,501,262: it was understood by 95% of the population, while 72.3% over the age of two could speak it (1,137,816), 79% could read it (1,246.555), and 53% could write it (835,080). The share of Barcelona residents who could speak it (72.3%) was lower than that of the overall Catalan population, of whom 81.2% over the age of 15 spoke the language. Knowledge of Catalan has increased significantly in recent decades thanks to a language immersion educational system. An important social characteristic of the Catalan language is that all the areas where it is spoken are bilingual in practice: together with French in Roussillon, with Italian in Alghero, with Spanish and French in Andorra, and with Spanish in the rest of the territories.\nLevel of knowledge.\n(% of the population 15 years old and older).\nSocial use.\n(% of the population 15 years old and older).\nPhonology.\nCatalan phonology varies by dialect. Notable features include:\nIn contrast to other Romance languages, Catalan has many monosyllabic words, and these may end in a wide variety of consonants, including some consonant clusters. Additionally, Catalan has final obstruent devoicing, which gives rise to an abundance of such couplets as (\"male friend\") vs. (\"female friend\").\nCentral Catalan pronunciation is considered to be standard for the language. The descriptions below are mostly representative of this variety. For the differences in pronunciation between the different dialects, see the section on pronunciation of dialects in this article.\nVowels.\nCatalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: , a common feature in Western Romance, with the exception of Spanish. Balearic also has instances of stressed . Dialects differ in the different degrees of vowel reduction, and the incidence of the pair .\nIn Central Catalan, unstressed vowels reduce to three: ; ; remains distinct. The other dialects have different vowel reduction processes (see the section pronunciation of dialects in this article).\nConsonants.\nThe consonant system of Catalan is rather conservative.\nPhonological evolution.\nCatalan shares features with neighboring Romance languages (Occitan, Italian, Sardinian, French, Spanish).\nIn contrast with other Romance languages, Catalan has many monosyllabic words; and those ending in a wide variety of consonants and some consonant clusters. Also, Catalan has final obstruent devoicing, thus featuring many couplets like ('male friend') vs. ('female friend').\nSociolinguistics.\nCatalan sociolinguistics studies the situation of Catalan in the world and the different varieties that this language presents. It is a subdiscipline of Catalan philology and other affine studies and has as an objective to analyze the relation between the Catalan language, the speakers and the close reality (including the one of other languages in contact).\nDialects.\nOverview.\nThe dialects of the Catalan language feature a relative uniformity, especially when compared to other Romance languages; both in terms of vocabulary, semantics, syntax, morphology, and phonology. Mutual intelligibility between dialects is very high, estimates ranging from 90% to 95%. The only exception is the isolated idiosyncratic Algherese dialect.\nCatalan is split in two major dialectal blocks: Eastern and Western. The main difference lies in the treatment of unstressed and ; which have merged to in Eastern dialects, but which remain distinct as and in Western dialects. There are a few other differences in pronunciation, verbal morphology, and vocabulary.\nWestern Catalan comprises the two dialects of North-Western Catalan and Valencian; the Eastern block comprises four dialects: Central Catalan, Balearic, Roussillonese, and Algherese. Each dialect can be further subdivided in several subdialects. The terms \"Catalan\" and \"Valencian\" (respectively used in Catalonia and the Valencian Community) refer to two varieties of the same language. There are two institutions regulating the two standard varieties, the Institute of Catalan Studies in Catalonia and the Valencian Academy of the Language in the Valencian Community.\nCentral Catalan is considered the standard pronunciation of the language and has the largest number of speakers. It is spoken in the densely populated regions of the Barcelona province, the eastern half of the province of Tarragona, and most of the province of Girona.\nCatalan has an inflectional grammar. Nouns have two genders (masculine, feminine), and two numbers (singular, plural). Pronouns additionally can have a neuter gender, and some are also inflected for case and politeness, and can be combined in very complex ways. Verbs are split in several paradigms and are inflected for person, number, tense, aspect, mood, and gender. In terms of pronunciation, Catalan has many words ending in a wide variety of consonants and some consonant clusters, in contrast with many other Romance languages.\nPronunciation.\nVowels.\nCatalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: , a common feature in Western Romance, except Spanish. Balearic has also instances of stressed . Dialects differ in the different degrees of vowel reduction, and the incidence of the pair .\nIn Eastern Catalan (except Majorcan), unstressed vowels reduce to three: ; ; remains distinct. There are a few instances of unreduced , in some words. Algherese has lowered to .\nIn Majorcan, unstressed vowels reduce to four: follow the Eastern Catalan reduction pattern; however reduce to , with remaining distinct, as in Western Catalan.\nIn Western Catalan, unstressed vowels reduce to five: ; ; remain distinct. This reduction pattern, inherited from Proto-Romance, is also found in Italian and Portuguese. Some Western dialects present further reduction or vowel harmony in some cases.\nCentral, Western, and Balearic differ in the lexical incidence of stressed and . Usually, words with in Central Catalan correspond to in Balearic and in Western Catalan. Words with in Balearic almost always have in Central and Western Catalan as well. As a result, Central Catalan has a much higher incidence of .\nConsonants.\nCatalan dialects are characterized by final-obstruent devoicing, lenition and voicing assimilation. Additionally, many dialects contrast two rhotics () and two laterals (().\nMost Catalan dialects are also renowned by the usage of \"dark l\" (i.e. velarization of \u2192 ), which is especially noticeable in syllable final position, in comparison to neighbouring languages, such as Spanish, Italian and French (that lack this pronunciation).\nThere is dialectal variation in regard to:\nMorphology.\nWestern Catalan: In verbs, the ending for 1st-person present indicative is in verbs of the 1st conjugation and -\u2205 in verbs of the 2nd and 3rd conjugations in most of the Valencian Community, or in all verb conjugations in the Northern Valencian Community and Western Catalonia.E.g. , , (Valencian); , , (North-Western Catalan).\nEastern Catalan: In verbs, the ending for 1st-person present indicative is , , or -\u2205 in all conjugations. E.g. (Central), (Balearic), and (Northern), all meaning ('I speak').\nWestern Catalan: In verbs, the inchoative endings are /, , , /.\nEastern Catalan: In verbs, the inchoative endings are , , , .\nWestern Catalan: In nouns and adjectives, maintenance of of medieval plurals in proparoxytone words.E.g. 'men', 'youth'.\nEastern Catalan: In nouns and adjectives, loss of of medieval plurals in proparoxytone words.E.g. 'men', 'youth' (Ibicencan, however, follows the model of Western Catalan in this case).\nVocabulary.\nDespite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.\nStandards.\nStandard Catalan, virtually accepted by all speakers, is mostly based on Eastern Catalan, which is the most widely used dialect. Nevertheless, the standards of the Valencian Community and the Balearics admit alternative forms, mostly traditional ones, which are not current in eastern Catalonia.\nThe most notable difference between both standards is some tonic accentuation, for instance: (IEC) \u2013 (AVL). Nevertheless, AVL's standard keeps the grave accent , while pronouncing it as rather than , in some words such as: ('what'), or . Other divergences include the use of (AVL) in some words instead of like in / ('almond'), / ('back'), the use of elided demonstratives ( 'this', 'that') in the same level as reinforced ones () or the use of many verbal forms common in Valencian, and some of these common in the rest of Western Catalan too, such as subjunctive mood or inchoative conjugation in at the same level as or the priority use of morpheme in 1st person singular in present indicative ( verbs): instead of ('I buy').\nIn the Balearic Islands, IEC's standard is used but adapted for the Balearic dialect by the University of the Balearic Islands's philological section. In this way, for instance, IEC says it is correct writing as much as ('we sing'), but the university says that the priority form in the Balearic Islands must be in all fields. Another feature of the Balearic standard is the non-ending in the 1st person singular present indicative: ('I buy'), ('I fear'), ('I sleep').\nIn Alghero, the IEC has adapted its standard to the Algherese dialect. In this standard one can find, among other features: the definite article instead of , special possessive pronouns and determinants ('mine'), ('his/her'), ('yours'), and so on, the use of in the imperfect tense in all conjugations: , , ; the use of many archaic words, usual words in Algherese: instead of ('less'), instead of ('someone'), instead of ('which'), and so on; and the adaptation of weak pronouns. In 1999, Catalan (Algherese dialect) was among the twelve minority languages officially recognized as Italy's \"historical linguistic minorities\" by the Italian State under Law No. 482/1999.\nIn 2011, the Aragonese government passed a decree approving the statutes of a new language regulator of Catalan in La Franja (the so-called Catalan-speaking areas of Aragon) as originally provided for by Law 10/2009. The new entity, designated as , shall allow a facultative education in Catalan and a standardization of the Catalan language in La Franja.\nStatus of Valencian.\nValencian is classified as a Western dialect, along with the North-Western varieties spoken in Western Catalonia (provinces of Lleida and the western half of Tarragona). Central Catalan has 90% to 95% inherent intelligibility for speakers of Valencian.\nLinguists, including Valencian scholars, deal with Catalan and Valencian as the same language. The official regulating body of the language of the Valencian Community, the Valencian Academy of Language (\"Acad\u00e8mia Valenciana de la Llengua\", AVL) declares the linguistic unity between Valencian and Catalan varieties.\nThe AVL, created by the Valencian parliament, is in charge of dictating the official rules governing the use of Valencian, and its standard is based on the Norms of Castell\u00f3 (\"Normes de Castell\u00f3\"). Currently, everyone who writes in Valencian uses this standard, except the Royal Academy of Valencian Culture (\"Real Acad\u00e8mia de Cultura Valenciana\", RACV), which uses an independent standard for Valencian.\nDespite the position of the official organizations, an opinion poll carried out between 2001 and 2004 showed that the majority of the Valencian people consider Valencian different from Catalan. This position is promoted by people who do not use Valencian regularly. Furthermore, the data indicates that younger generations educated in Valencian are much less likely to hold these views. A minority of Valencian scholars active in fields other than linguistics defends the position of the Royal Academy of Valencian Culture (\"Real Acad\u00e8mia de Cultura Valenciana\", RACV), which uses for Valencian a standard independent from Catalan.\nThis clash of opinions has sparked much controversy. For example, during the drafting of the European Constitution in 2004, the Spanish government supplied the EU with translations of the text into Basque, Galician, Catalan, and Valencian, but the latter two were identical.\nVocabulary.\nWord choices.\nDespite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.\nLiterary Catalan allows the use of words from different dialects, except those of very restricted use. However, from the 19th century onwards, there has been a tendency towards favoring words of Northern dialects to the detriment of others.\nLatin and Greek loanwords.\nLike other languages, Catalan has a large list of loanwords from Greek and Latin. This process started very early, and one can find such examples in Ramon Llull's work. In the 14th and 15th centuries Catalan had a far greater number of Greco-Latin loanwords than other Romance languages, as is attested for example in Ro\u00eds de Corella's writings. The incorporation of learned, or \"bookish\" words from its own ancestor language, Latin, into Catalan is arguably another form of lexical borrowing through the influence of written language and the liturgical language of the Church. Throughout the Middle Ages and into the early modern period, most literate Catalan speakers were also literate in Latin; and thus they easily adopted Latin words into their writing\u2014and eventually speech\u2014in Catalan.\nWord formation.\nThe process of morphological derivation in Catalan follows the same principles as the other Romance languages, where inflection is common. Many times, several affixes are appended to a preexisting lexeme, and some sound alternations can occur, for example (\"electrical\") vs. . Prefixes are usually appended to verbs, as in (\"foresee\").\nThere is greater regularity in the process of word-compounding, where one can find compounded words formed much like those in English.\nWriting system.\nCatalan uses the Latin script, with some added symbols and digraphs. The Catalan orthography is systematic and largely phonologically based. Standardization of Catalan was among the topics discussed during the First International Congress of the Catalan Language, held in Barcelona October 1906. Subsequently, the Philological Section of the \"Institut d'Estudis Catalans\" (IEC, founded in 1911) published the \"Normes ortogr\u00e0fiques\" in 1913 under the direction of Antoni Maria Alcover and Pompeu Fabra. In 1932, Valencian writers and intellectuals gathered in Castell\u00f3 de la Plana to make a formal adoption of the so-called \"Normes de Castell\u00f3\", a set of guidelines following Pompeu Fabra's Catalan language norms.\nGrammar.\nThe grammar of Catalan is similar to other Romance languages. Features include:\nGender and number inflection.\nIn gender inflection, the most notable feature is (compared to Portuguese, Spanish or Italian), the loss of the typical masculine suffix . Thus, the alternance of /, has been replaced by \"\u00f8\"/. There are only a few exceptions, such as / (\"scarce\"). Many not completely predictable morphological alternations may occur, such as:\nCatalan has few suppletive couplets, like Italian and Spanish, and unlike French. Thus, Catalan has / (\"boy\"/\"girl\") and / (\"cock\"/\"hen\"), whereas French has / and /.\nThere is a tendency to abandon traditionally gender-invariable adjectives in favor of marked ones, something prevalent in Occitan and French. Thus, one can find / (\"boiling\") in contrast with traditional /.\nAs in the other Western Romance languages, the main plural expression is the suffix , which may create morphological alternations similar to the ones found in gender inflection, albeit more rarely. The most important one is the addition of before certain consonant groups, a phonetic phenomenon that does not affect feminine forms: / (\"the pulse\"/\"the pulses\") vs. / (\"the dust\"/\"the dusts\").\nDeterminers.\nThe inflection of determinatives is complex, specially because of the high number of elisions, but is similar to the neighboring languages. Catalan has more contractions of preposition + article than Spanish, such as (\"of + the [plural]\"), but not as many as Italian (which has , , , etc.).\nCentral Catalan has abandoned almost completely unstressed possessives (, etc.) in favor of constructions of article + stressed forms (, etc.), a feature shared with Italian.\nPersonal pronouns.\nThe morphology of Catalan personal pronouns is complex, especially in unstressed forms, which are numerous (13 distinct forms, compared to 11 in Spanish or 9 in Italian). Features include the gender-neutral and the great degree of freedom when combining different unstressed pronouns (65 combinations).\nCatalan pronouns exhibit T\u2013V distinction, like all other Romance languages (and most European languages, but not Modern English). This feature implies the use of a different set of second person pronouns for formality.\nThis flexibility allows Catalan to use extraposition extensively, much more than French or Spanish. Thus, Catalan can have (\"they recommended me to him\"), whereas in French one must say , and Spanish . This allows the placement of almost any nominal term as a sentence topic, without having to use so often the passive voice (as in French or English), or identifying the direct object with a preposition (as in Spanish).\nVerbs.\nLike all the Romance languages, Catalan verbal inflection is more complex than the nominal. Suffixation is omnipresent, whereas morphological alternations play a secondary role. Vowel alternances are active, as well as infixation and suppletion. However, these are not as productive as in Spanish, and are mostly restricted to irregular verbs.\nThe Catalan verbal system is basically common to all Western Romance, except that most dialects have replaced the synthetic indicative perfect with a periphrastic form of (\"to go\") + infinitive.\nCatalan verbs are traditionally divided into three conjugations, with vowel themes , , , the last two being split into two subtypes. However, this division is mostly theoretical. Only the first conjugation is nowadays productive (with about 3500 common verbs), whereas the third (the subtype of , with about 700 common verbs) is semiproductive. The verbs of the second conjugation are fewer than 100, and it is not possible to create new ones, except by compounding.\nSyntax.\nThe grammar of Catalan follows the general pattern of Western Romance languages. The primary word order is subject\u2013verb\u2013object. However, word order is very flexible. Commonly, verb-subject constructions are used to achieve a semantic effect. The sentence \"The train has arrived\" could be translated as or . Both sentences mean \"the train has arrived\", but the former puts a focus on the train, while the latter puts a focus on the arrival. This subtle distinction is described as \"what you might say while waiting in the station\" versus \"what you might say on the train\".\nCatalan names.\nIn Spain, every person officially has two surnames, one of which is the father's first surname and the other is the mother's first surname. The law contemplates the possibility of joining both surnames with the Catalan conjunction \"i\" (\"and\").\nSample text.\nSelected text from Manuel de Pedrolo's 1970 novel (\"A love affair outside the city\").\nExternal links.\nInstitutions\nAbout the Catalan/Valencian language\nMonolingual dictionaries\nBilingual and multilingual dictionaries\nAutomated translation systems\nLearning resources\nCatalan-language online encyclopedia"}
{"id": "5283", "revid": "1110281609", "url": "https://en.wikipedia.org/wiki?curid=5283", "title": "Cryptomonads", "text": ""}
{"id": "5285", "revid": "32159694", "url": "https://en.wikipedia.org/wiki?curid=5285", "title": "STS-51-F", "text": "STS-51-F (also known as Spacelab 2) was the 19th flight of NASA's Space Shuttle program and the eighth flight of Space Shuttle \"Challenger\". It launched from Kennedy Space Center, Florida, on July 29, 1985, and landed eight days later on August 6, 1985.\nWhile STS-51-F's primary payload was the Spacelab 2 laboratory module, the payload that received the most publicity was the Carbonated Beverage Dispenser Evaluation, which was an experiment in which both Coca-Cola and Pepsi tried to make their carbonated drinks available to astronauts. A helium-cooled infrared telescope (IRT) was also flown on this mission, and while it did have some problems, it observed 60% of the galactic plane in infrared light.\nDuring launch, \"Challenger\" experienced multiple sensor failures in its Engine 1 Center SSME engine, which led to it shutting down and the shuttle had to perform an \"Abort to Orbit\" (ATO) emergency procedure. It is the only Shuttle mission to have carried out an abort after launching. As a result of the ATO, the mission was carried out at a slightly lower orbital altitude.\nCrew.\nAs with previous Spacelab missions, the crew was divided between two 12-hour shifts. Acton, Bridges and Henize made up the \"Red Team\" while Bartoe, England and Musgrave comprised the \"Blue Team\"; commander Fullerton could take either shift when needed. \"Challenger\" carried two Extravehicular Mobility Units (EMU) in the event of an emergency spacewalk, which would have been performed by England and Musgrave.\nLaunch.\nSTS-51-F's first launch attempt on July 12, 1985, was halted with the countdown at T\u22123 seconds after main engine ignition, when a malfunction of the number two RS-25 coolant valve caused an automatic launch abort. \"Challenger\" launched successfully on its second attempt on July 29, 1985, at 17:00\u00a0p.m. EDT, after a delay of 1 hour 37 minutes due to a problem with the table maintenance block update uplink.\nAt 3 minutes 31 seconds into the ascent, one of the center engine's two high-pressure fuel turbopump turbine discharge temperature sensors failed. Two minutes and twelve seconds later, the second sensor failed, causing the shutdown of the center engine. This was the only in-flight RS-25 failure of the Space Shuttle program. Approximately 8 minutes into the flight, one of the same temperature sensors in the right engine failed, and the remaining right-engine temperature sensor displayed readings near the redline for engine shutdown. Booster Systems Engineer Jenny M. Howard acted quickly to recommend that the crew inhibit any further automatic RS-25 shutdowns based on readings from the remaining sensors, preventing the potential shutdown of a second engine and a possible abort mode that may have resulted in the loss of crew and vehicle (LOCV).\nThe failed RS-25 resulted in an Abort to Orbit (ATO) trajectory, whereby the shuttle achieved a lower-than-planned orbital altitude. The plan had been for a by orbit, but the mission was carried out at by .\nMission summary.\nSTS-51-F's primary payload was the laboratory module Spacelab 2. A special part of the modular Spacelab system, the \"igloo\", which was located at the head of a three-pallet train, provided on-site support to instruments mounted on pallets. The main mission objective was to verify performance of Spacelab systems, determine the interface capability of the orbiter, and measure the environment created by the spacecraft. Experiments covered life sciences, plasma physics, astronomy, high-energy astrophysics, solar physics, atmospheric physics and technology research. Despite mission replanning necessitated by \"Challenger\"s abort to orbit trajectory, the Spacelab mission was declared a success.\nThe flight marked the first time the European Space Agency (ESA) Instrument Pointing System (IPS) was tested in orbit. This unique pointing instrument was designed with an accuracy of one arcsecond. Initially, some problems were experienced when it was commanded to track the Sun, but a series of software fixes were made and the problem was corrected. In addition, Anthony W. England became the second amateur radio operator to transmit from space during the mission.\nSpacelab Infrared Telescope.\nThe Spacelab Infrared Telescope (IRT) was also flown on the mission. The IRT was a aperture helium-cooled infrared telescope, observing light between wavelengths of 1.7 to 118 \u03bcm. It was thought heat emissions from the Shuttle would corrupt long-wavelength data, however it still returned useful astronomical data. Another problem was that a piece of mylar insulation broke loose and floated in the line-of-sight of the telescope. IRT collected infrared data on 60% of the galactic plane. (see also List of largest infrared telescopes) A later space mission that experienced a stray light problem from debris was \"Gaia\" astrometry spacecraft launch in 2013 by the ESA - the source of the stray light was later identified as the fibers of the sunshield, protruding beyond the edges of the shield.\nOther payloads.\nThe Plasma Diagnostics Package (PDP), which had been previously flown on STS-3, made its return on the mission, and was part of a set of plasma physics experiments designed to study the Earth's ionosphere. During the third day of the mission, it was grappled out of the payload bay by the Remote Manipulator System (Canadarm) and released for six hours. During this time, \"Challenger\" maneuvered around the PDP as part of a targeted proximity operations exercise. The PDP was successfully grappled by the Canadarm and returned to the payload bay at the beginning of the fourth day of the mission.\nIn a heavily publicized marketing experiment, astronauts aboard STS-51-F drank carbonated beverages from specially designed cans from Cola Wars competitors Coca-Cola and Pepsi. According to Acton, after Coke developed its experimental dispenser for an earlier shuttle flight, Pepsi insisted to American president Ronald Reagan that Coke should not be the first cola in space. The experiment was delayed until Pepsi could develop its own system, and the two companies' products were assigned to STS-51-F.\nBlue Team tested Coke, and Red Team tested Pepsi. As part of the experiment, each team was photographed with the cola logo. Acton said that while the sophisticated Coke system \"dispensed soda kind of like what we're used to drinking on Earth\", the Pepsi can was a shaving cream can with the Pepsi logo on a paper wrapper, which \"dispensed soda filled with bubbles\" that was \"not very drinkable\". Acton said that when he gives speeches in schools, audiences are much more interested in hearing about the cola experiment than in solar physics. Post-flight, the astronauts revealed that they preferred Tang, in part because it could be mixed on-orbit with existing chilled-water supplies, whereas there was no dedicated refrigeration equipment on board to chill the cans, which also fizzed excessively in microgravity.\nIn an experiment during the mission, thruster rockets were fired at a point over Tasmania and also above Boston to create two \"holes\" \u2013 plasma depletion regions \u2013 in the ionosphere. A worldwide group of geophysicists collaborated with the observations made from Spacelab 2.\nLanding.\n\"Challenger\" landed at Edwards Air Force Base, California, on August 6, 1985, at 12:45:26\u00a0p.m. PDT. Its rollout distance was . The mission had been extended by 17 orbits for additional payload activities due to the Abort to Orbit. The orbiter arrived back at Kennedy Space Center on August 11, 1985.\nMission insignia.\nThe mission insignia was designed by Houston, Texas, artist Skip Bradley. is depicted ascending toward the heavens in search of new knowledge in the field of solar and stellar astronomy, with its Spacelab 2 payload. The constellations Leo and Orion are shown in the positions they were in relative to the Sun during the flight. The nineteen stars indicate that the mission is the 19th shuttle flight.\nLegacy.\nOne of the purposes of the mission was to test how suitable the Shuttle was for conducting infrared observations, and the IRT was operated on this mission. However, the orbiter was found to have some draw-backs for infrared astronomy, and this led to later infrared telescopes being free-flying from the Shuttle orbiter."}
{"id": "5287", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5287", "title": "Classical Music", "text": ""}
{"id": "5288", "revid": "6908984", "url": "https://en.wikipedia.org/wiki?curid=5288", "title": "Classical period (music)", "text": "The Classical Period was an era of classical music between roughly 1750 and 1820.\nThe classical period falls between the Baroque and Romantic periods. Classical music has a lighter, clearer texture than Baroque music but a more varying use of musical form, which is, in simpler terms, the rhythm and organization of any given piece of music. It is mainly homophonic, using a clear melody line over a subordinate chordal accompaniment, but counterpoint was by no means forgotten, especially in liturgical vocal music and, later in the period, secular instrumental music. It also makes use of \"style galant\" which emphasizes light elegance in place of the Baroque's dignified seriousness and impressive grandeur. Variety and contrast within a piece became more pronounced than before, and the orchestra increased in size, range, and power.\nThe harpsichord was replaced as the main keyboard instrument by the piano (or fortepiano). Unlike the harpsichord, which plucks strings with quills, pianos strike the strings with leather-covered hammers when the keys are pressed, which enables the performer to play louder or softer (hence the original name \"fortepiano,\" literally \"loud soft\") and play with more expression; in contrast, the force with which a performer plays the harpsichord keys does not change the sound. Instrumental music was considered important by Classical period composers. The main kinds of instrumental music were the sonata, trio, string quartet, quintet, symphony (performed by an orchestra), and the solo concerto, which featured a virtuoso solo performer playing a solo work for violin, piano, flute, or another instrument, accompanied by an orchestra. Vocal music, such as songs for a singer and piano (notably the work of Schubert), choral works, and opera (a staged dramatic work for singers and orchestra), was also important during this period.\nThe best-known composers from this period are Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and Franz Schubert; other names in this period include: Carl Philipp Emanuel Bach, Johann Christian Bach, Luigi Boccherini, Domenico Cimarosa, Joseph Martin Kraus, Muzio Clementi, Christoph Willibald Gluck, Carl Ditters von Dittersdorf, Andr\u00e9 Gr\u00e9try, Pierre-Alexandre Monsigny, Leopold Mozart, Michael Haydn, Giovanni Paisiello, Johann Baptist Wanhal, Fran\u00e7ois-Andr\u00e9 Danican Philidor, Niccol\u00f2 Piccinni, Antonio Salieri, Etienne Nicolas Mehul, Georg Christoph Wagenseil, Johann Simon Mayr, Georg Matthias Monn, Johann Gottlieb Graun, Carl Heinrich Graun, Franz Benda, Georg Anton Benda, Johann Georg Albrechtsberger, Mauro Giuliani, Christian Cannabich and the Chevalier de Saint-Georges. Beethoven is regarded either as a Romantic composer or a Classical period composer who was part of the transition to the Romantic era. Schubert is also a transitional figure, as were Johann Nepomuk Hummel, Luigi Cherubini, Gaspare Spontini, Gioachino Rossini, Carl Maria von Weber, Jan Ladislav Dussek and Niccol\u00f2 Paganini. The period is sometimes referred to as the era of \"Viennese Classicism\" (), since Gluck, Haydn, Salieri, Mozart, Beethoven, and Schubert all worked in Vienna.\nClassicism.\nIn the middle of the 18th century, Europe began to move toward a new style in architecture, literature, and the arts, generally known as Neoclassicism. This style sought to emulate the ideals of Classical antiquity, especially those of Classical Greece. Classical music used formality and emphasis on order and hierarchy and a \"clearer\", \"cleaner\" style that used clearer divisions between parts (notably a clear, single melody accompanied by chords), brighter contrasts, and \"tone colors\" (achieved by the use of dynamic changes and modulations to more keys). In contrast with the richly layered music of the Baroque era, Classical music moved towards simplicity rather than complexity. In addition, the typical size of orchestras began to increase, giving orchestras a more powerful sound.\nThe remarkable development of ideas in \"natural philosophy\" had already established itself in the public consciousness. In particular, Newton's physics was taken as a paradigm: structures should be well-founded in axioms and be both well-articulated and orderly. This taste for structural clarity began to affect music, which moved away from the layered polyphony of the Baroque period toward a style known as homophony, in which the melody is played over a subordinate harmony. This move meant that chords became a much more prevalent feature of music, even if they interrupted the melodic smoothness of a single part. As a result, the tonal structure of a piece of music became more audible.\nThe new style was also encouraged by changes in the economic order and social structure. As the 18th century progressed well, the nobility became the primary patrons of instrumental music, while public taste increasingly preferred lighter, funny comic operas. This led to changes in the way music was performed, the most crucial of which was the move to standard instrumental groups and the reduction in the importance of the \"continuo\"\u2014the rhythmic and harmonic groundwork of a piece of music, typically played by a keyboard (harpsichord or organ) and usually accompanied by a varied group of bass instruments, including cello, double bass, bass viol, and theorbo. One way to trace the decline of the continuo and its figured chords is to examine the disappearance of the term \"obbligato\", meaning a mandatory instrumental part in a work of chamber music. In Baroque compositions, additional instruments could be added to the continuo group according to the group or leader's preference; in Classical compositions, all parts were specifically noted, though not always \"notated\", so the term \"obbligato\" became redundant. By 1800, basso continuo was practically extinct, except for the occasional use of a pipe organ continuo part in a religious Mass in the early 1800s.\nEconomic changes also had the effect of altering the balance of availability and quality of musicians. While in the late Baroque, a major composer would have the entire musical resources of a town to draw on, the musical forces available at an aristocratic hunting lodge or small court were smaller and more fixed in their level of ability. This was a spur to having simpler parts for ensemble musicians to play, and in the case of a resident virtuoso group, a spur to writing spectacular, idiomatic parts for certain instruments, as in the case of the Mannheim orchestra, or virtuoso solo parts for particularly skilled violinists or flutists. In addition, the appetite by audiences for a continual supply of new music carried over from the Baroque. This meant that works had to be performable with, at best, one or two rehearsals. Even after 1790, Mozart writes about \"the rehearsal,\" with the implication that his concerts would have only one rehearsal.\nSince there was a greater emphasis on a single melodic line, there was greater emphasis on notating that line for dynamics and phrasing. This contrasts with the Baroque era, when melodies were typically written with no dynamics, phrasing marks, ornaments, as it was assumed that the performer would improvise these elements on the spot. In the Classical era, it became more common for composers to indicate where they wanted performers to play ornaments such as trills or turns. The simplification of texture made such instrumental detail more important, and alit so made the use of characteristic rhythms, such as attention-getting opening fanfares, the funeral march rhythm, or the minuet genre, more important in establishing and unifying the tone of a single movement.\nThe Classical period also saw the gradual development of sonata form, a set of structural principles for music that reconciled the Classical preference for melodic material with harmonic development, which could be applied across musical genres. The sonata itself continued to be the principal form for solo and chamber music, while later in the Classical period, the string quartet became a prominent genre. The symphony form for orchestra was created in this period (this is popularly attributed to Joseph Haydn). The \"concerto grosso\" (a concerto for more than one musician), a very popular form in the Baroque era, began to be replaced by the \"solo concerto\", featuring only one soloist. Composers began to place more importance on the particular soloist's ability to show off virtuoso skills, with challenging, fast scale and arpeggio runs. Nonetheless, some \"concerti grossi\" remained, the most famous of which was Mozart's Sinfonia Concertante for Violin and Viola in E-flat major.\nMain characteristics.\nIn the classical period, the theme consists of phrases with contrasting melodic figures and rhythms. These phrases are relatively brief, typically four bars in length, and can occasionally seem sparse or terse. The texture is mainly homophonic, with a clear melody above a subordinate chordal accompaniment, for instance an Alberti bass. This contrasts with the practice in Baroque music, where a piece or movement would typically have only one musical subject, which would then be worked out in a number of voices according to the principles of counterpoint, while maintaining a consistent rhythm or metre throughout. As a result, Classical music tends to have a lighter, clearer texture than the Baroque. The classical style draws on the \"style galant\", a musical style which emphasized light elegance in place of the Baroque's dignified seriousness and impressive grandeur.\nStructurally, Classical music generally has a clear musical form, with a well-defined contrast between tonic and dominant, introduced by clear cadences. Dynamics are used to highlight the structural characteristics of the piece. In particular, sonata form and its variants were developed during the early classical period and was frequently used. The Classical approach to structure again contrasts with the Baroque, where a composition would normally move between tonic and dominant and back again, but through a continual progress of chord changes and without a sense of \"arrival\" at the new key. While counterpoint was less emphasised in the classical period, it was by no means forgotten, especially later in the period, and composers still used counterpoint in \"serious\" works such as symphonies and string quartets, as well as religious pieces, such as Masses.\nThe classical musical style was supported by technical developments in instruments. The widespread adoption of equal temperament made classical musical structure possible, by ensuring that cadences in all keys sounded similar. The fortepiano and then the pianoforte replaced the harpsichord, enabling more dynamic contrast and more sustained melodies. Over the Classical period, keyboard instruments became richer, more sonorous and more powerful.\nThe orchestra increased in size and range, and became more standardised. The harpsichord or pipe organ basso continuo role in orchestra fell out of use between 1750 and 1775, leaving the string section. Woodwinds became a self-contained section, consisting of clarinets, oboes, flutes and bassoons.\nWhile vocal music such as comic opera was popular, great importance was given to instrumental music. The main kinds of instrumental music were the sonata, trio, string quartet, quintet, symphony, concerto (usually for a virtuoso solo instrument accompanied by orchestra), and light pieces such as serenades and divertimentos. Sonata form developed and became the most important form. It was used to build up the first movement of most large-scale works in symphonies and string quartets. Sonata form was also used in other movements and in single, standalone pieces such as overtures.\nHistory.\nBaroque/Classical transition c. 1750\u20131760.\nIn his book \"The Classical Style\", author and pianist Charles Rosen claims that from 1755 to 1775, composers groped for a new style that was more effectively dramatic. In the High Baroque period, dramatic expression was limited to the representation of individual \"affects\" (the \"doctrine of affections\", or what Rosen terms \"dramatic sentiment\"). For example, in Handel's oratorio \"Jephtha\", the composer renders four emotions separately, one for each character, in the quartet \"O, spare your daughter\". Eventually this depiction of individual emotions came to be seen as simplistic and unrealistic; composers sought to portray multiple emotions, simultaneously or progressively, within a single character or movement (\"dramatic action\"). Thus in the finale of act 2 of Mozart's \"Die Entf\u00fchrung aus dem Serail\", the lovers move \"from joy through suspicion and outrage to final reconciliation.\"\nMusically speaking, this \"dramatic action\" required more musical variety. Whereas Baroque music was characterized by seamless flow within individual movements and largely uniform textures, composers after the High Baroque sought to interrupt this flow with abrupt changes in texture, dynamic, harmony, or tempo. Among the stylistic developments which followed the High Baroque, the most dramatic came to be called \"Empfindsamkeit\", (roughly \"sensitive style\"), and its best-known practitioner was Carl Philipp Emanuel Bach. Composers of this style employed the above-discussed interruptions in the most abrupt manner, and the music can sound illogical at times. The Italian composer Domenico Scarlatti took these developments further. His more than five hundred single-movement keyboard sonatas also contain abrupt changes of texture, but these changes are organized into periods, balanced phrases that became a hallmark of the classical style. However, Scarlatti's changes in texture still sound sudden and unprepared. The outstanding achievement of the great classical composers (Haydn, Mozart and Beethoven) was their ability to make these dramatic surprises sound logically motivated, so that \"the expressive and the elegant could join hands.\"\nBetween the death of J. S. Bach and the maturity of Haydn and Mozart (roughly 1750\u20131770), composers experimented with these new ideas, which can be seen in the music of Bach's sons. Johann Christian developed a style which we now call \"Roccoco\", comprising simpler textures and harmonies, and which was \"charming, undramatic, and a little empty.\" As mentioned previously, Carl Philipp Emmanuel sought to increase drama, and his music was \"violent, expressive, brilliant, continuously surprising, and often incoherent.\" And finally Wilhelm Friedemann, J.S. Bach's eldest son, extended Baroque traditions in an idiomatic, unconventional way.\nAt first the new style took over Baroque forms\u2014the ternary \"da capo aria\", the \"sinfonia\" and the \"concerto\"\u2014but composed with simpler parts, more notated ornamentation, rather than the improvised ornaments that were common in the Baroque era, and more emphatic division of pieces into sections. However, over time, the new aesthetic caused radical changes in how pieces were put together, and the basic formal layouts changed. Composers from this period sought dramatic effects, striking melodies, and clearer textures. One of the big textural changes was a shift away from the complex, dense polyphonic style of the Baroque, in which multiple interweaving melodic lines were played simultaneously, and towards homophony, a lighter texture which uses a clear single melody line accompanied by chords.\nBaroque music generally uses many harmonic fantasies and polyphonic sections that focus less on the structure of the musical piece, and there was less emphasis on clear musical phrases. In the classical period, the harmonies became simpler. However, the structure of the piece, the phrases and small melodic or rhythmic motives, became much more important than in the Baroque period.\nAnother important break with the past was the radical overhaul of opera by Christoph Willibald Gluck, who cut away a great deal of the layering and improvisational ornaments and focused on the points of modulation and transition. By making these moments where the harmony changes more of a focus, he enabled powerful dramatic shifts in the emotional color of the music. To highlight these transitions, he used changes in instrumentation (orchestration), melody, and mode. Among the most successful composers of his time, Gluck spawned many emulators, including Antonio Salieri. Their emphasis on accessibility brought huge successes in opera, and in other vocal music such as songs, oratorios, and choruses. These were considered the most important kinds of music for performance and hence enjoyed greatest public success.\nThe phase between the Baroque and the rise of the Classical (around 1730), was home to various competing musical styles. The diversity of artistic paths are represented in the sons of Johann Sebastian Bach: Wilhelm Friedemann Bach, who continued the Baroque tradition in a personal way; Johann Christian Bach, who simplified textures of the Baroque and most clearly influenced Mozart; and Carl Philipp Emanuel Bach, who composed passionate and sometimes violently eccentric music of the \"Empfindsamkeit\" movement. Musical culture was caught at a crossroads: the masters of the older style had the technique, but the public hungered for the new. This is one of the reasons C. P. E. Bach was held in such high regard: he understood the older forms quite well and knew how to present them in new garb, with an enhanced variety of form.\n1750\u20131775.\nBy the late 1750s there were flourishing centers of the new style in Italy, Vienna, Mannheim, and Paris; dozens of symphonies were composed and there were bands of players associated with musical theatres. Opera or other vocal music accompanied by orchestra was the feature of most musical events, with concertos and symphonies (arising from the overture) serving as instrumental interludes and introductions for operas and church services. Over the course of the Classical period, symphonies and concertos developed and were presented independently of vocal music.\nThe \"normal\" orchestra ensemble\u2014a body of strings supplemented by winds\u2014and movements of particular rhythmic character were established by the late 1750s in Vienna. However, the length and weight of pieces was still set with some Baroque characteristics: individual movements still focused on one \"affect\" (musical mood) or had only one sharply contrasting middle section, and their length was not significantly greater than Baroque movements. There was not yet a clearly enunciated theory of how to compose in the new style. It was a moment ripe for a breakthrough.\nThe first great master of the style was the composer Joseph Haydn. In the late 1750s he began composing symphonies, and by 1761 he had composed a triptych (\"Morning\", \"Noon\", and \"Evening\") solidly in the contemporary mode. As a vice-Kapellmeister and later Kapellmeister, his output expanded: he composed over forty symphonies in the 1760s alone. And while his fame grew, as his orchestra was expanded and his compositions were copied and disseminated, his voice was only one among many.\nWhile some scholars suggest that Haydn was later overshadowed by Mozart and Beethoven, it would be difficult to overstate Haydn's centrality to the new style, and therefore to the future of Western art music as a whole. At the time, before the pre-eminence of Mozart or Beethoven, and with Johann Sebastian Bach known primarily to connoisseurs of keyboard music, Haydn reached a place in music that set him above all other composers except perhaps the Baroque era's George Frideric Handel. Haydn took existing ideas, and radically altered how they functioned\u2014earning him the titles \"father of the symphony\" and \"father of the string quartet\".\nOne of the forces that worked as an impetus for his pressing forward was the first stirring of what would later be called Romanticism\u2014the \"Sturm und Drang\", or \"storm and stress\" phase in the arts, a short period where obvious and dramatic emotionalism was a stylistic preference. Haydn accordingly wanted more dramatic contrast and more emotionally appealing melodies, with sharpened character and individuality in his pieces. This period faded away in music and literature: however, it influenced what came afterward and would eventually be a component of aesthetic taste in later decades.\nThe \"Farewell Symphony\", No. 45 in F minor, exemplifies Haydn's integration of the differing demands of the new style, with surprising sharp turns and a long slow adagio to end the work. In 1772, Haydn completed his Opus 20 set of six string quartets, in which he deployed the polyphonic techniques he had gathered from the previous Baroque era to provide structural coherence capable of holding together his melodic ideas. For some, this marks the beginning of the \"mature\" Classical style, a transitional period in which reaction against late Baroque complexity yielded to integration of Baroque and Classical elements.\n1775\u20131790.\nHaydn, having worked for over a decade as the music director for a prince, had far more resources and scope for composing than most other composers. His position also gave him the ability to shape the forces that would play his music, as he could select skilled musicians. This opportunity was not wasted, as Haydn, beginning quite early on his career, sought to press forward the technique of building and developing ideas in his music. His next important breakthrough was in the Opus 33 string quartets (1781), in which the melodic and the harmonic roles segue among the instruments: it is often momentarily unclear what is melody and what is harmony. This changes the way the ensemble works its way between dramatic moments of transition and climactic sections: the music flows smoothly and without obvious interruption. He then took this integrated style and began applying it to orchestral and vocal music.\nHaydn's gift to music was a way of composing, a way of structuring works, which was at the same time in accord with the governing aesthetic of the new style. However, a younger contemporary, Wolfgang Amadeus Mozart, brought his genius to Haydn's ideas and applied them to two of the major genres of the day: opera, and the virtuoso concerto. Whereas Haydn spent much of his working life as a court composer, Mozart wanted public success in the concert life of cities, playing for the general public. This meant he needed to write operas and write and perform virtuoso pieces. Haydn was not a virtuoso at the international touring level; nor was he seeking to create operatic works that could play for many nights in front of a large audience. Mozart wanted to achieve both. Moreover, Mozart also had a taste for more chromatic chords (and greater contrasts in harmonic language generally), a greater love for creating a welter of melodies in a single work, and a more Italianate sensibility in music as a whole. He found, in Haydn's music and later in his study of the polyphony of J.S. Bach, the means to discipline and enrich his artistic gifts.\nMozart rapidly came to the attention of Haydn, who hailed the new composer, studied his works, and considered the younger man his only true peer in music. In Mozart, Haydn found a greater range of instrumentation, dramatic effect and melodic resource. The learning relationship moved in both directions. Mozart also had a great respect for the older, more experienced composer, and sought to learn from him.\nMozart's arrival in Vienna in 1780 brought an acceleration in the development of the Classical style. There, Mozart absorbed the fusion of Italianate brilliance and Germanic cohesiveness that had been brewing for the previous 20 years. His own taste for flashy brilliances, rhythmically complex melodies and figures, long cantilena melodies, and virtuoso flourishes was merged with an appreciation for formal coherence and internal connectedness. It is at this point that war and economic inflation halted a trend to larger orchestras and forced the disbanding or reduction of many theater orchestras. This pressed the Classical style inwards: toward seeking greater ensemble and technical challenges\u2014for example, scattering the melody across woodwinds, or using a melody harmonized in thirds. This process placed a premium on small ensemble music, called chamber music. It also led to a trend for more public performance, giving a further boost to the string quartet and other small ensemble groupings.\nIt was during this decade that public taste began, increasingly, to recognize that Haydn and Mozart had reached a high standard of composition. By the time Mozart arrived at age 25, in 1781, the dominant styles of Vienna were recognizably connected to the emergence in the 1750s of the early Classical style. By the end of the 1780s, changes in performance practice, the relative standing of instrumental and vocal music, technical demands on musicians, and stylistic unity had become established in the composers who imitated Mozart and Haydn. During this decade Mozart composed his most famous operas, his six late symphonies that helped to redefine the genre, and a string of piano concerti that still stand at the pinnacle of these forms.\nOne composer who was influential in spreading the more serious style that Mozart and Haydn had formed is Muzio Clementi, a gifted virtuoso pianist who tied with Mozart in a musical \"duel\" before the emperor in which they each improvised on the piano and performed their compositions. Clementi's sonatas for the piano circulated widely, and he became the most successful composer in London during the 1780s. Also in London at this time was Jan Ladislav Dussek, who, like Clementi, encouraged piano makers to extend the range and other features of their instruments, and then fully exploited the newly opened up possibilities. The importance of London in the Classical period is often overlooked, but it served as the home to the Broadwood's factory for piano manufacturing and as the base for composers who, while less notable than the \"Vienna School\", had a decisive influence on what came later. They were composers of many fine works, notable in their own right. London's taste for virtuosity may well have encouraged the complex passage work and extended statements on tonic and dominant.\nAround 1790\u20131820.\nWhen Haydn and Mozart began composing, symphonies were played as single movements\u2014before, between, or as interludes within other works\u2014and many of them lasted only ten or twelve minutes; instrumental groups had varying standards of playing, and the continuo was a central part of music-making.\nIn the intervening years, the social world of music had seen dramatic changes. International publication and touring had grown explosively, and concert societies formed. Notation became more specific, more descriptive\u2014and schematics for works had been simplified (yet became more varied in their exact working out). In 1790, just before Mozart's death, with his reputation spreading rapidly, Haydn was poised for a series of successes, notably his late oratorios and London symphonies. Composers in Paris, Rome, and all over Germany turned to Haydn and Mozart for their ideas on form.\nIn the 1790s, a new generation of composers, born around 1770, emerged. While they had grown up with the earlier styles, they heard in the recent works of Haydn and Mozart a vehicle for greater expression. In 1788 Luigi Cherubini settled in Paris and in 1791 composed \"Lodoiska\", an opera that raised him to fame. Its style is clearly reflective of the mature Haydn and Mozart, and its instrumentation gave it a weight that had not yet been felt in the grand opera. His contemporary \u00c9tienne M\u00e9hul extended instrumental effects with his 1790 opera \"Euphrosine et Coradin\", from which followed a series of successes. The final push towards change came from Gaspare Spontini, who was deeply admired by future romantic composers such as Weber, Berlioz and Wagner. The innovative harmonic language of his operas, their refined instrumentation and their \"enchained\" closed numbers (a structural pattern which was later adopted by Weber in Euryanthe and from him handed down, through Marschner, to Wagner), formed the basis from which French and German romantic opera had its beginnings.\nThe most fateful of the new generation was Ludwig van Beethoven, who launched his numbered works in 1794 with a set of three piano trios, which remain in the repertoire. Somewhat younger than the others, though equally accomplished because of his youthful study under Mozart and his native virtuosity, was Johann Nepomuk Hummel. Hummel studied under Haydn as well; he was a friend to Beethoven and Franz Schubert. He concentrated more on the piano than any other instrument, and his time in London in 1791 and 1792 generated the composition and publication in 1793 of three piano sonatas, opus 2, which idiomatically used Mozart's techniques of avoiding the expected cadence, and Clementi's sometimes modally uncertain virtuoso figuration. Taken together, these composers can be seen as the vanguard of a broad change in style and the center of music. They studied one another's works, copied one another's gestures in music, and on occasion behaved like quarrelsome rivals.\nThe crucial differences with the previous wave can be seen in the downward shift in melodies, increasing durations of movements, the acceptance of Mozart and Haydn as paradigmatic, the greater use of keyboard resources, the shift from \"vocal\" writing to \"pianistic\" writing, the growing pull of the minor and of modal ambiguity, and the increasing importance of varying accompanying figures to bring \"texture\" forward as an element in music. In short, the late Classical was seeking music that was internally more complex. The growth of concert societies and amateur orchestras, marking the importance of music as part of middle-class life, contributed to a booming market for pianos, piano music, and virtuosi to serve as exemplars. Hummel, Beethoven, and Clementi were all renowned for their improvising.\nThe direct influence of the Baroque continued to fade: the figured bass grew less prominent as a means of holding performance together, the performance practices of the mid-18th century continued to die out. However, at the same time, complete editions of Baroque masters began to become available, and the influence of Baroque style continued to grow, particularly in the ever more expansive use of brass. Another feature of the period is the growing number of performances where the composer was not present. This led to increased detail and specificity in notation; for example, there were fewer \"optional\" parts that stood separately from the main score.\nThe force of these shifts became apparent with Beethoven's 3rd Symphony, given the name \"Eroica\", which is Italian for \"heroic\", by the composer. As with Stravinsky's \"The Rite of Spring\", it may not have been the first in all of its innovations, but its aggressive use of every part of the Classical style set it apart from its contemporary works: in length, ambition, and harmonic resources as well making it the first symphony of the Romantic era.\nFirst Viennese School.\nThe First Viennese School is a name mostly used to refer to three composers of the Classical period in late-18th-century Vienna: Haydn, Mozart, and Beethoven. Franz Schubert is occasionally added to the list.\nIn German-speaking countries, the term \"Wiener Klassik\" (lit. \"Viennese classical era/art\") is used. That term is often more broadly applied to the Classical era in music as a whole, as a means to distinguish it from other periods that are colloquially referred to as \"classical\", namely Baroque and Romantic music.\nThe term \"Viennese School\" was first used by Austrian musicologist Raphael Georg Kiesewetter in 1834, although he only counted Haydn and Mozart as members of the school. Other writers followed suit, and eventually Beethoven was added to the list. The designation \"first\" is added today to avoid confusion with the Second Viennese School.\nWhilst, Schubert apart, these composers certainly knew each other (with Haydn and Mozart even being occasional chamber-music partners), there is no sense in which they were engaged in a collaborative effort in the sense that one would associate with 20th-century schools such as the Second Viennese School, or Les Six. Nor is there any significant sense in which one composer was \"schooled\" by another (in the way that Berg and Webern were taught by Schoenberg), though it is true that Beethoven for a time received lessons from Haydn.\nAttempts to extend the First Viennese School to include such later figures as Anton Bruckner, Johannes Brahms, and Gustav Mahler are merely journalistic, and never encountered in academic musicology. According to scholar James F. Daugherty, the Classical period itself from approximately 1775 to 1825 is sometimes referred to as \"the Viennese Classic period\".\nClassical influence on later composers.\nMusical eras and their prevalent styles, forms and instruments seldom disappear at once; instead, features are replaced over time, until the old approach is simply felt as \"old-fashioned\". The Classical style did not \"die\" suddenly; rather, it gradually got phased out under the weight of changes. To give just one example, while it is generally stated that the Classical era stopped using the harpsichord in orchestras, this did not happen all of a sudden at the start of the Classical era in 1750. Rather, orchestras slowly stopped using the harpsichord to play basso continuo until the practice was discontinued by the end of the 1700s.\nOne crucial change was the shift towards harmonies centering on \"flatward\" keys: shifts in the subdominant direction . In the Classical style, major key was far more common than minor, chromaticism being moderated through the use of \"sharpward\" modulation (e.g., a piece in C major modulating to G major, D major, or A major, all of which are keys with more sharps). As well, sections in the minor mode were often used for contrast. Beginning with Mozart and Clementi, there began a creeping colonization of the subdominant region (the ii or IV chord, which in the key of C major would be the keys of d minor or F major). With Schubert, subdominant modulations flourished after being introduced in contexts in which earlier composers would have confined themselves to dominant shifts (modulations to the dominant chord, e.g., in the key of C major, modulating to G major). This introduced darker colors to music, strengthened the minor mode, and made structure harder to maintain. Beethoven contributed to this by his increasing use of the fourth as a consonance, and modal ambiguity\u2014for example, the opening of the Symphony No. 9 in D minor.\nLudwig van Beethoven, Franz Schubert, Carl Maria von Weber, Johann Nepomuk Hummel, and John Field are among the most prominent in this generation of \"Proto-Romantics\", along with the young Felix Mendelssohn. Their sense of form was strongly influenced by the Classical style. While they were not yet \"learned\" composers (imitating rules which were codified by others), they directly responded to works by Haydn, Mozart, Clementi, and others, as they encountered them. The instrumental forces at their disposal in orchestras were also quite \"Classical\" in number and variety, permitting similarity with Classical works.\nHowever, the forces destined to end the hold of the Classical style gathered strength in the works of many of the above composers, particularly Beethoven. The most commonly cited one is harmonic innovation. Also important is the increasing focus on having a continuous and rhythmically uniform accompanying figuration: Beethoven's Moonlight Sonata was the model for hundreds of later pieces\u2014where the shifting movement of a rhythmic figure provides much of the drama and interest of the work, while a melody drifts above it. Greater knowledge of works, greater instrumental expertise, increasing variety of instruments, the growth of concert societies, and the unstoppable domination of the increasingly more powerful piano (which was given a bolder, louder tone by technological developments such as the use of steel strings, heavy cast-iron frames and sympathetically vibrating strings) all created a huge audience for sophisticated music. All of these trends contributed to the shift to the \"Romantic\" style.\nDrawing the line between these two styles is very difficult: some sections of Mozart's later works, taken alone, are indistinguishable in harmony and orchestration from music written 80 years later\u2014and some composers continued to write in normative Classical styles into the early 20th century. Even before Beethoven's death, composers such as Louis Spohr were self-described Romantics, incorporating, for example, more extravagant chromaticism in their works (e.g., using chromatic harmonies in a piece's chord progression). Conversely, works such as Schubert's Symphony No. 5, written during the chronological end of the Classical era and dawn of the Romantic era, exhibit a deliberately anachronistic artistic paradigm, harking back to the compositional style of several decades before.\nHowever, Vienna's fall as the most important musical center for orchestral composition during the late 1820s, precipitated by the deaths of Beethoven and Schubert, marked the Classical style's final eclipse\u2014and the end of its continuous organic development of one composer learning in close proximity to others. Franz Liszt and Fr\u00e9d\u00e9ric Chopin visited Vienna when they were young, but they then moved on to other cities. Composers such as Carl Czerny, while deeply influenced by Beethoven, also searched for new ideas and new forms to contain the larger world of musical expression and performance in which they lived.\nRenewed interest in the formal balance and restraint of 18th century classical music led in the early 20th century to the development of so-called Neoclassical style, which numbered Stravinsky and Prokofiev among its proponents, at least at certain times in their careers.\nClassical period instruments.\nGuitar.\nThe Baroque guitar, with four or five sets of double strings or \"courses\" and elaborately decorated soundhole, was a very different instrument from the early classical guitar which more closely resembles the modern instrument with the standard six strings. Judging by the number of instructional manuals published for the instrument \u2013 over three hundred texts were published by over two hundred authors between 1760 and 1860 \u2013 the classical period marked a golden age for guitar.\nStrings.\nIn the Baroque era, there was more variety in the bowed stringed instruments used in ensembles, with instruments such as the viola d'amore and a range of fretted viols being used, ranging from small viols to large bass viols. In the Classical period, the string section of the orchestra was standardized as just four instruments:\nIn the Baroque era, the double bass players were not usually given a separate part; instead, they typically played the same basso continuo bassline that the cellos and other low-pitched instruments (e.g., theorbo, serpent wind instrument, viols), albeit an octave below the cellos, because the double bass is a transposing instrument that sounds one octave lower than it is written. In the Classical era, some composers continued to write only one bass part for their symphony, labeled \"bassi\"; this bass part was played by cellists and double bassists. During the Classical era, some composers began to give the double basses their own part.\nWoodwinds.\nIt was commonplace for all orchestras to have at least 2 winds, usually oboes, flutes, clarinets, or sometimes English horns (see Symphony No. 22 (Haydn). Patrons also usually employed an ensemble of entirely winds, called the \"harmonie\", which would be employed for certain events. The harmonie would sometimes join the larger string orchestra to serve as the wind section."}
{"id": "5289", "revid": "788819460", "url": "https://en.wikipedia.org/wiki?curid=5289", "title": "Card games", "text": ""}
{"id": "5290", "revid": "45710782", "url": "https://en.wikipedia.org/wiki?curid=5290", "title": "Casino games", "text": ""}
{"id": "5291", "revid": "46153661", "url": "https://en.wikipedia.org/wiki?curid=5291", "title": "Computer games", "text": ""}
{"id": "5292", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=5292", "title": "Collectable card games", "text": ""}
{"id": "5295", "revid": "76", "url": "https://en.wikipedia.org/wiki?curid=5295", "title": "Character encoding", "text": "Character encoding is the process of assigning numbers to graphical characters, especially the written characters of human language, allowing them to be stored, transmitted, and transformed using computers. The numerical values that make up a character encoding are known as code points and collectively comprise a code space, a code page, or character map.\nEarly character encodings that originated with optical or electrical telegraphy and in early computers could only represent a subset of the characters used in written languages, sometimes restricted to upper case letters, numerals and some punctuation only. Over time, character encodings capable of representing more characters were created, such as ASCII, the ISO/IEC 8859 encodings, various computer vendor encodings, and Unicode encodings such as UTF-8 and UTF-16.\nThe most popular character encoding on the World Wide Web is UTF-8, which is used in 98.2% of surveyed web sites, as of May 2024. In application programs and operating system tasks, both UTF-8 and UTF-16 are popular options.\nHistory.\nThe history of character codes illustrates the evolving need for machine-mediated character-based symbolic information over a distance, using once-novel electrical means. The earliest codes were based upon manual and hand-written encoding and cyphering systems, such as Bacon's cipher, Braille, international maritime signal flags, and the 4-digit encoding of Chinese characters for a Chinese telegraph code (Hans Schjellerup, 1869). With the adoption of electrical and electro-mechanical techniques these earliest codes were adapted to the new capabilities and limitations of the early machines. The earliest well-known electrically transmitted character code, Morse code, introduced in the 1840s, used a system of four \"symbols\" (short signal, long signal, short space, long space) to generate codes of variable length. Though some commercial use of Morse code was via machinery, it was often used as a manual code, generated by hand on a telegraph key and decipherable by ear, and persists in amateur radio and aeronautical use. Most codes are of fixed per-character length or variable-length sequences of fixed-length codes (e.g. Unicode).\nCommon examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode. Unicode, a well-defined and extensible encoding system, has replaced most earlier character encodings, but the path of code development to the present is fairly well known.\nThe Baudot code, a five-bit encoding, was created by \u00c9mile Baudot in 1870, patented in 1874, modified by Donald Murray in 1901, and standardized by CCITT as International Telegraph Alphabet No.\u00a02 (ITA2) in 1930. The name \"baudot\" has been erroneously applied to ITA2 and its many variants. ITA2 suffered from many shortcomings and was often improved by many equipment manufacturers, sometimes creating compatibility issues.\nHerman Hollerith invented punch card data encoding in the late 19th century to analyze census data. Initially, each hole position represented a different data element, but later, numeric information was encoded by numbering the lower rows 0 to 9, with a punch in a column representing its row number. Later alphabetic data was encoded by allowing more than one punch per column. Electromechanical tabulating machines represented date internally by the timing of pulses relative to the motion of the cards through the machine.\nWhen IBM went to electronic processing, starting with the IBM 603 Electronic Multiplier, it used a variety of binary encoding schemes that were tied to the punch card code. IBM used several binary-coded decimal (BCD) six-bit character encoding schemes, starting as early as 1953 in its 702 and 704 computers, and in its later 7000 Series and 1400 series, as well as in associated peripherals. Since the punched card code then in use only allowed digits, upper-case English letters and a few special characters, six bits were sufficient. These BCD encodings extended existing simple four-bit numeric encoding to include alphabetic and special characters, mapping them easily to punch-card encoding which was already in widespread use. IBM's codes were used primarily with IBM equipment; other computer vendors of the era had their own character codes, often six-bit, such as the encoding used by the , but usually had the ability to read tapes produced on IBM equipment. IBM's BCD encodings were the precursors of their Extended Binary-Coded Decimal Interchange Code (usually abbreviated as EBCDIC), an eight-bit encoding scheme developed in 1963 for the IBM System/360 that featured a larger character set, including lower case letters.\nIn 1959 the U.S. military defined its Fieldata code, a six-or seven-bit code, introduced by the U.S. Army Signal Corps. While Fieldata addressed many of the then-modern issues (e.g. letter and digit codes arranged for machine collation), it fell short of its goals and was short-lived. In 1963 the first ASCII code was released (X3.4-1963) by the ASCII committee (which contained at least one member of the Fieldata committee, W. F. Leubbert), which addressed most of the shortcomings of Fieldata, using a simpler seven-bit code. Many of the changes were subtle, such as collatable character sets within certain numeric ranges. ASCII63 was a success, widely adopted by industry, and with the follow-up issue of the 1967 ASCII code (which added lower-case letters and fixed some \"control code\" issues) ASCII67 was adopted fairly widely. ASCII67's American-centric nature was somewhat addressed in the European ECMA-6 standard. Eight-bit extended ASCII encodings, such as various vendor extensions and the ISO/IEC 8859 series, supported all ASCII characters as well as additional non-ASCII characters.\nIn trying to develop universally interchangeable character encodings, researchers in the 1980s faced the dilemma that, on the one hand, it seemed necessary to add more bits to accommodate additional characters, but on the other hand, for the users of the relatively small character set of the Latin alphabet (who still constituted the majority of computer users), those additional bits were a colossal waste of then-scarce and expensive computing resources (as they would always be zeroed out for such users). In 1985, the average personal computer user's hard disk drive could store only about 10 megabytes, and it cost approximately US$250 on the wholesale market (and much higher if purchased separately at retail), so it was very important at the time to make every bit count.\nThe compromise solution that was eventually found and was to break the assumption (dating back to telegraph codes) that each character should always directly correspond to a particular sequence of bits. Instead, characters would first be mapped to a universal intermediate representation in the form of abstract numbers called code points. Code points would then be represented in a variety of ways and with various default numbers of bits per character (code units) depending on context. To encode code points higher than the length of the code unit, such as above 256 for eight-bit units, the solution was to implement variable-length encodings where an escape sequence would signal that subsequent bits should be parsed as a higher code point.\nTerminology.\nInformally, the terms \"character encoding\", \"character map\", \"character set\" and \"code page\" are often used interchangeably. Historically, the same standard would specify a repertoire of characters and how they were to be encoded into a stream of code units \u2014 usually with a single character per code unit. However, due to the emergence of more sophisticated character encodings, the distinction between these terms has become important.\nCode pages.\n\"Code page\" is a historical name for a coded character set.\nOriginally, a code page referred to a specific page number in the IBM standard character set manual, which would define a particular character encoding. Other vendors, including Microsoft, SAP, and Oracle Corporation, also published their own sets of code pages; the most well-known code page suites are \"Windows\" (based on Windows-1252) and \"IBM\"/\"DOS\" (based on code page 437).\nDespite no longer referring to specific page numbers in a standard, many character encodings are still referred to by their code page number; likewise, the term \"code page\" is often still used to refer to character encodings in general.\nThe term \"code page\" is not used in Unix or Linux, where \"charmap\" is preferred, usually in the larger context of locales. IBM's Character Data Representation Architecture (CDRA) designates entities with coded character set identifiers (CCSIDs), each of which is variously called a \"charset\", \"character set\", \"code page\", or \"CHARMAP\".\nCode units.\nThe code unit size is equivalent to the bit measurement for the particular encoding:\nCode points.\nA code point is represented by a sequence of code units. The mapping is defined by the encoding. Thus, the number of code units required to represent a code point depends on the encoding:\nCharacters.\nExactly what constitutes a character varies between character encodings.\nFor example, for letters with diacritics, there are two distinct approaches that can be taken to encode them: they can be encoded either as a single unified character (known as a precomposed character), or as separate characters that combine into a single glyph. The former simplifies the text handling system, but the latter allows any letter/diacritic combination to be used in text. Ligatures pose similar problems.\nExactly how to handle glyph variants is a choice that must be made when constructing a particular character encoding. Some writing systems, such as Arabic and Hebrew, need to accommodate things like graphemes that are joined in different ways in different contexts, but represent the same semantic character.\nUnicode encoding model.\nUnicode and its parallel standard, the ISO/IEC 10646 Universal Character Set, together constitute a unified standard for character encoding. Rather than mapping characters directly to bytes, Unicode separately defines a coded character set that maps characters to unique natural numbers (code points), how those code points are mapped to a series of fixed-size natural numbers (code units), and finally how those units are encoded as a stream of octets (bytes). The purpose of this decomposition is to establish a universal set of characters that can be encoded in a variety of ways. To describe this model precisely, Unicode uses its own set of terminology to describe its process:\nAn abstract character repertoire (ACR) is the full set of abstract characters that a system supports. Unicode has an open repertoire, meaning that new characters will be added to the repertoire over time.\nA coded character set (CCS) is a function that maps characters to \"code points\" (each code point represents one character). For example, in a given repertoire, the capital letter \"A\" in the Latin alphabet might be represented by the code point 65, the character \"B\" by 66, and so on. Multiple coded character sets may share the same character repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different code points.\nA character encoding form (CEF) is the mapping of code points to \"code units\" to facilitate storage in a system that represents numbers as bit sequences of fixed length (i.e. practically any computer system). For example, a system that stores numeric information in 16-bit units can only directly represent code points 0 to 65,535 in each unit, but larger code points (say, 65,536 to 1.4\u00a0million) could be represented by using multiple 16-bit units. This correspondence is defined by a CEF.\nA character encoding scheme (CES) is the mapping of code units to a sequence of octets to facilitate storage on an octet-based file system or transmission over an octet-based network. Simple character encoding schemes include UTF-8, UTF-16BE, UTF-32BE, UTF-16LE, and UTF-32LE; compound character encoding schemes, such as UTF-16, UTF-32 and ISO/IEC 2022, switch between several simple schemes by using a byte order mark or escape sequences; compressing schemes try to minimize the number of bytes used per code unit (such as SCSU and BOCU).\nAlthough UTF-32BE and UTF-32LE are simpler CESes, most systems working with Unicode use either UTF-8, which is backward compatible with fixed-length ASCII and maps Unicode code points to variable-length sequences of octets, or UTF-16BE, which is backward compatible with fixed-length UCS-2BE and maps Unicode code points to variable-length sequences of 16-bit words. See comparison of Unicode encodings for a detailed discussion.\nFinally, there may be a higher-level protocol which supplies additional information to select the particular variant of a Unicode character, particularly where there are regional variants that have been 'unified' in Unicode as the same character. An example is the XML attribute xml:lang.\nThe Unicode model uses the term \"character map\" for other systems which directly assign a sequence of characters to a sequence of bytes, covering all of the CCS, CEF and CES layers.\nUnicode code points.\nIn Unicode, a character can be referred to as 'U+' followed by its codepoint value in hexadecimal. The range of valid code points (the codespace) for the Unicode standard is U+0000 to U+10FFFF, inclusive, divided in 17 planes, identified by the numbers 0 to 16. Characters in the range U+0000 to U+FFFF are in plane 0, called the Basic Multilingual Plane (BMP). This plane contains the most commonly-used characters. Characters in the range U+10000 to U+10FFFF in the other planes are called supplementary characters.\nThe following table shows examples of code point values:\nExample.\nConsider a string of the letters \"ab\u0332c\ud801\udc00\"\u2014that is, a string containing a Unicode combining character () as well as a supplementary character (). This string has several Unicode representations which are logically equivalent, yet while each is suited to a diverse set of circumstances or range of requirements:\nNote in particular that \ud801\udc00 is represented with either one 32-bit value (UTF-32), two 16-bit values (UTF-16), or four 8-bit values (UTF-8). Although each of those forms uses the same total number of bits (32) to represent the glyph, it is not obvious how the actual numeric byte values are related.\nTranscoding.\nAs a result of having many character encoding methods in use (and the need for backward compatibility with archived data), many computer programs have been developed to translate data between character encoding schemes, a process known as transcoding. Some of these are cited below.\nCross-platform:\nWindows:\nCommon character encodings.\nThe most used character encoding on the web is UTF-8, used in 98.2% of surveyed web sites, as of May 2024. In application programs and operating system tasks, both UTF-8 and UTF-16 are popular options."}
{"id": "5297", "revid": "35313544", "url": "https://en.wikipedia.org/wiki?curid=5297", "title": "Computer character", "text": ""}
{"id": "5298", "revid": "1543097", "url": "https://en.wikipedia.org/wiki?curid=5298", "title": "Control character", "text": "In computing and telecommunications, a control character or non-printing character (NPC) is a code point in a character set that does not represent a written character or symbol. They are used as in-band signaling to cause effects other than the addition of a symbol to the text. All other characters are mainly \"graphic characters\", also known as \"printing characters\" (or \"printable characters\"), except perhaps for \"space\" characters. In the ASCII standard there are 33 control characters, such as code 7, , which rings a terminal bell.\nHistory.\nProcedural signs in Morse code are a form of control character.\nA form of control characters were introduced in the 1870 Baudot code: NUL and DEL.\nThe 1901 Murray code added the carriage return (CR) and line feed (LF), and other versions of the Baudot code included other control characters.\nThe bell character (BEL), which rang a bell to alert operators, was also an early teletype control character.\nSome control characters have also been called \"format effectors\".\nIn ASCII.\nThere were quite a few control characters defined (33 in ASCII, and the ECMA-48 standard adds 32 more). This was because early terminals had very primitive mechanical or electrical controls that made any kind of state-remembering API quite expensive to implement, thus a different code for each and every function looked like a requirement. It quickly became possible and inexpensive to interpret sequences of codes to perform a function, and device makers found a way to send hundreds of device instructions. Specifically, they used ASCII code 2710 (escape), followed by a series of characters called a \"control sequence\" or \"escape sequence\". The mechanism was invented by Bob Bemer, the father of ASCII. For example, the sequence of code 2710, followed by the printable characters \"[2;10H\", would cause a Digital Equipment Corporation VT100 terminal to move its cursor to the 10th cell of the 2nd line of the screen. Several standards exist for these sequences, notably ANSI X3.64, but the number of non-standard variations is large.\nAll entries in the ASCII table below code 3210 (technically the C0 control code set) are of this kind, including CR and LF used to separate lines of text. The code 12710 (DEL) is also a control character. Extended ASCII sets defined by ISO 8859 added the codes 12810 through 15910 as control characters. This was primarily done so that if the high bit was stripped, it would not change a printing character to a C0 control code. This second set is called the C1 set.\nThese 65 control codes were carried over to Unicode. Unicode added more characters that could be considered controls, but it makes a distinction between these \"Formatting characters\" (such as the zero-width non-joiner) and the 65 control characters.\nThe Extended Binary Coded Decimal Interchange Code (EBCDIC) character set contains 65 control codes, including all of the ASCII control codes plus additional codes which are mostly used to control IBM peripherals.\nThe control characters in ASCII still in common use include:\nControl characters may be described as doing something when the user inputs them, such as code 3 (End-of-Text character, ETX, ) to interrupt the running process, or code 4 (End-of-Transmission character, EOT, ), used to end text input on Unix or to exit a Unix shell. These uses usually have little to do with their use when they are in text being output.\nIn Unicode.\nIn Unicode, \"Control-characters\" are U+0000\u2014U+001F (C0 controls), U+007F (delete), and U+0080\u2014U+009F (C1 controls). Their General Category is \"Cc\". Formatting codes are distinct, in General Category \"Cf\". The Cc control characters have no Name in Unicode, but are given labels such as \"&lt;control-001A&gt;\" instead.\nDisplay.\nThere are a number of techniques to display non-printing characters, which may be illustrated with the bell character in ASCII encoding:\nHow control characters map to keyboards.\nASCII-based keyboards have a key labelled \"Control\", \"Ctrl\", or (rarely) \"Cntl\" which is used much like a shift key, being pressed in combination with another letter or symbol key. In one implementation, the control key generates the code 64 places below the code for the (generally) uppercase letter it is pressed in combination with (i.e., subtract 0x40 from ASCII code value of the (generally) uppercase letter). The other implementation is to take the ASCII code produced by the key and bitwise AND it with 0x1F, forcing bits 5 to 7 to zero. For example, pressing \"control\" and the letter \"g\" (which is 0110 0111 in binary), produces the code 7 (BELL, 7 in base ten, or 0000 0111 in binary). The NULL character (code 0) is represented by Ctrl-@, \"@\" being the code immediately before \"A\" in the ASCII character set. For convenience, some terminals accept Ctrl-Space as an alias for Ctrl-@. In either case, this produces one of the 32 ASCII control codes between 0 and 31. Neither approach works to produce the DEL character because of its special location in the table and its value (code 12710), Ctrl-? is sometimes used for this character.\nWhen the control key is held down, letter keys produce the same control characters regardless of the state of the shift or caps lock keys. In other words, it does not matter whether the key would have produced an upper-case or a lower-case letter. The interpretation of the control key with the space, graphics character, and digit keys (ASCII codes 32 to 63) varies between systems. Some will produce the same character code as if the control key were not held down. Other systems translate these keys into control characters when the control key is held down. The interpretation of the control key with non-ASCII (\"foreign\") keys also varies between systems.\nControl characters are often rendered into a printable form known as caret notation by printing a caret (^) and then the ASCII character that has a value of the control character plus 64. Control characters generated using letter keys are thus displayed with the upper-case form of the letter. For example, ^G represents code 7, which is generated by pressing the G key when the control key is held down.\nKeyboards also typically have a few single keys which produce control character codes. For example, the key labelled \"Backspace\" typically produces code 8, \"Tab\" code 9, \"Enter\" or \"Return\" code 13 (though some keyboards might produce code 10 for \"Enter\").\nMany keyboards include keys that do not correspond to any ASCII printable or control character, for example cursor control arrows and word processing functions. The associated keypresses are communicated to computer programs by one of four methods: appropriating otherwise unused control characters; using some encoding other than ASCII; using multi-character control sequences; or using an additional mechanism outside of generating characters. \"Dumb\" computer terminals typically use control sequences. Keyboards attached to stand-alone personal computers made in the 1980s typically use one (or both) of the first two methods. Modern computer keyboards generate scancodes that identify the specific physical keys that are pressed; computer software then determines how to handle the keys that are pressed, including any of the four methods described above.\nThe design purpose.\nThe control characters were designed to fall into a few groups: printing and display control, data structuring, transmission control, and miscellaneous.\nPrinting and display control.\nPrinting control characters were first used to control the physical mechanism of printers, the earliest output device. An early example of this idea was the use of Figures (FIGS) and Letters (LTRS) in Baudot code to shift between two code pages. A later, but still early, example was the out-of-band ASA carriage control characters. Later, control characters were integrated into the stream of data to be printed.\nThe carriage return character (CR), when sent to such a device, causes it to put the character at the edge of the paper at which writing begins (it may, or may not, also move the printing position to the next line).\nThe line feed character (LF/NL) causes the device to put the printing position on the next line. It may (or may not), depending on the device and its configuration, also move the printing position to the start of the next line (which would be the leftmost position for left-to-right scripts, such as the alphabets used for Western languages, and the rightmost position for right-to-left scripts such as the Hebrew and Arabic alphabets).\nThe vertical and horizontal tab characters (VT and HT/TAB) cause the output device to move the printing position to the next tab stop in the direction of reading.\nThe form feed character (FF/NP) starts a new sheet of paper, and may or may not move to the start of the first line.\nThe backspace character (BS) moves the printing position one character space backwards. On printers, including hard-copy terminals, this is most often used so the printer can overprint characters to make other, not normally available, characters. On video terminals and other electronic output devices, there are often software (or hardware) configuration choices that allow a destructive backspace (e.g., a BS, SP, BS sequence), which erases, or a non-destructive one, which does not.\nThe shift in and shift out characters (SI and SO) selected alternate character sets, fonts, underlining, or other printing modes. Escape sequences were often used to do the same thing.\nWith the advent of computer terminals that did not physically print on paper and so offered more flexibility regarding screen placement, erasure, and so forth, printing control codes were adapted. Form feeds, for example, usually cleared the screen, there being no new paper page to move to. More complex escape sequences were developed to take advantage of the flexibility of the new terminals, and indeed of newer printers. The concept of a control character had always been somewhat limiting, and was extremely so when used with new, much more flexible, hardware. Control sequences (sometimes implemented as escape sequences) could match the new flexibility and power and became the standard method. However, there were, and remain, a large variety of standard sequences to choose from.\nData structuring.\nThe separators (File, Group, Record, and Unit: FS, GS, RS and US) were made to structure data, usually on a tape, in order to simulate punched cards.\nEnd of medium (EM) warns that the tape (or other recording medium) is ending.\nWhile many systems use CR/LF and TAB for structuring data, it is possible to encounter the separator control characters in data that needs to be structured. The separator control characters are not overloaded; there is no general use of them except to separate data into structured groupings. Their numeric values are contiguous with the space character, which can be considered a member of the group, as a word separator.\nFor example, the RS separator is used by (JSON Text Sequences) to encode a sequence of JSON elements. Each sequence item starts with a RS character and ends with a line feed. This allows to serialize open-ended JSON sequences. It is one of the JSON streaming protocols.\nTransmission control.\nThe transmission control characters were intended to structure a data stream, and to manage re-transmission or graceful failure, as needed, in the face of transmission errors.\nThe start of heading (SOH) character was to mark a non-data section of a data stream\u2014the part of a stream containing addresses and other housekeeping data. The start of text character (STX) marked the end of the header, and the start of the textual part of a stream. The end of text character (ETX) marked the end of the data of a message. A widely used convention is to make the two characters preceding ETX a checksum or CRC for error-detection purposes. The end of transmission block character (ETB) was used to indicate the end of a block of data, where data was divided into such blocks for transmission purposes.\nThe escape character (ESC) was intended to \"quote\" the next character, if it was another control character it would print it instead of performing the control function. It is almost never used for this purpose today. Various printable characters are used as visible \"escape characters\", depending on context.\nThe substitute character (SUB) was intended to request a translation of the next character from a printable character to another value, usually by setting bit 5 to zero. This is handy because some media (such as sheets of paper produced by typewriters) can transmit only printable characters. However, on MS-DOS systems with files opened in text mode, \"end of text\" or \"end of file\" is marked by this Ctrl-Z character, instead of the Ctrl-C or Ctrl-D, which are common on other operating systems.\nThe cancel character (CAN) signaled that the previous element should be discarded. The negative acknowledge character (NAK) is a definite flag for, usually, noting that reception was a problem, and, often, that the current element should be sent again. The acknowledge character (ACK) is normally used as a flag to indicate no problem detected with current element.\nWhen a transmission medium is half duplex (that is, it can transmit in only one direction at a time), there is usually a master station that can transmit at any time, and one or more slave stations that transmit when they have permission. The enquire character (ENQ) is generally used by a master station to ask a slave station to send its next message. A slave station indicates that it has completed its transmission by sending the end of transmission character (EOT).\nThe device control codes (DC1 to DC4) were originally generic, to be implemented as necessary by each device. However, a universal need in data transmission is to request the sender to stop transmitting when a receiver is temporarily unable to accept any more data. Digital Equipment Corporation invented a convention which used 19 (the device control 3 character (DC3), also known as control-S, or XOFF) to \"S\"top transmission, and 17 (the device control 1 character (DC1), a.k.a. control-Q, or XON) to start transmission. It has become so widely used that most don't realize it is not part of official ASCII. This technique, however implemented, avoids additional wires in the data cable devoted only to transmission management, which saves money. A sensible protocol for the use of such transmission flow control signals must be used, to avoid potential deadlock conditions, however.\nThe data link escape character (DLE) was intended to be a signal to the other end of a data link that the following character is a control character such as STX or ETX. For example a packet may be structured in the following way (DLE) &lt;STX&gt; &lt;PAYLOAD&gt; (DLE) &lt;ETX&gt;.\nMiscellaneous codes.\nCode 7 (BEL) is intended to cause an audible signal in the receiving terminal.\nMany of the ASCII control characters were designed for devices of the time that are not often seen today. For example, code 22, \"synchronous idle\" (SYN), was originally sent by synchronous modems (which have to send data constantly) when there was no actual data to send. (Modern systems typically use a start bit to announce the beginning of a transmitted word\u2014 this is a feature of \"asynchronous\" communication. \"Synchronous\" communication links were more often seen with mainframes, where they were typically run over corporate leased lines to connect a mainframe to another mainframe or perhaps a minicomputer.)\nCode 0 (ASCII code name NUL) is a special case. In paper tape, it is the case when there are no holes. It is convenient to treat this as a \"fill\" character with no meaning otherwise. Since the position of a NUL character has no holes punched, it can be replaced with any other character at a later time, so it was typically used to reserve space, either for correcting errors or for inserting information that would be available at a later time or in another place. In computing, it is often used for padding in fixed length records; to mark the end of a string; and formerly to give printing devices enough time to execute a control function.\nCode 127 (DEL, a.k.a. \"rubout\") is likewise a special case. Its 7-bit code is \"all-bits-on\" in binary, which essentially erased a character cell on a paper tape when overpunched. Paper tape was a common storage medium when ASCII was developed, with a computing history dating back to WWII code breaking equipment at Biuro Szyfr\u00f3w. Paper tape became obsolete in the 1970s, so this clever aspect of ASCII rarely saw any use after that. Some systems (such as the original Apples) converted it to a backspace. But because its code is in the range occupied by other printable characters, and because it had no official assigned glyph, many computer equipment vendors used it as an additional printable character (often an all-black \"box\" character useful for erasing text by overprinting with ink).\nNon-erasable programmable ROMs are typically implemented as arrays of fusible elements, each representing a bit, which can only be switched one way, usually from one to zero. In such PROMs, the DEL and NUL characters can be used in the same way that they were used on punched tape: one to reserve meaningless fill bytes that can be written later, and the other to convert written bytes to meaningless fill bytes. For PROMs that switch one to zero, the roles of NUL and DEL are reversed; also, DEL will only work with 7-bit characters, which are rarely used today; for 8-bit content, the character code 255, commonly defined as a nonbreaking space character, can be used instead of DEL.\nMany file systems do not allow control characters in filenames, as they may have reserved functions."}
{"id": "5299", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5299", "title": "Carbon", "text": "Carbon () is a chemical element; it has symbol C and atomic number 6. It is nonmetallic and tetravalent\u2014meaning that its atoms are able to form up to four covalent bonds due to its valence shell exhibiting 4 electrons. It belongs to group 14 of the periodic table. Carbon makes up about 0.025 percent of Earth's crust. Three isotopes occur naturally, C and C being stable, while C is a radionuclide, decaying with a half-life of 5,700\u00a0years. Carbon is one of the few elements known since antiquity.\nCarbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass after hydrogen, helium, and oxygen. Carbon's abundance, its unique diversity of organic compounds, and its unusual ability to form polymers at the temperatures commonly encountered on Earth, enables this element to serve as a common element of all known life. It is the second most abundant element in the human body by mass (about 18.5%) after oxygen.\nThe atoms of carbon can bond together in diverse ways, resulting in various allotropes of carbon. Well-known allotropes include graphite, diamond, amorphous carbon, and fullerenes. The physical properties of carbon vary widely with the allotropic form. For example, graphite is opaque and black, while diamond is highly transparent. Graphite is soft enough to form a streak on paper (hence its name, from the Greek verb \"\u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd\" which means \"to write\"), while diamond is the hardest naturally occurring material known. Graphite is a good electrical conductor while diamond has a low electrical conductivity. Under normal conditions, diamond, carbon nanotubes, and graphene have the highest thermal conductivities of all known materials. All carbon allotropes are solids under normal conditions, with graphite being the most thermodynamically stable form at standard temperature and pressure. They are chemically resistant and require high temperature to react even with oxygen.\nThe most common oxidation state of carbon in inorganic compounds is +4, while +2 is found in carbon monoxide and transition metal carbonyl complexes. The largest sources of inorganic carbon are limestones, dolomites and carbon dioxide, but significant quantities occur in organic deposits of coal, peat, oil, and methane clathrates. Carbon forms a vast number of compounds, with about two hundred million having been described and indexed; and yet that number is but a fraction of the number of theoretically possible compounds under standard conditions.\nCharacteristics.\nThe allotropes of carbon include graphite, one of the softest known substances, and diamond, the hardest naturally occurring substance. It bonds readily with other small atoms, including other carbon atoms, and is capable of forming multiple stable covalent bonds with suitable multivalent atoms. Carbon is a component element in the large majority of all chemical compounds, with about two hundred million examples having been described in the published chemical literature. Carbon also has the highest sublimation point of all elements. At atmospheric pressure it has no melting point, as its triple point is at and , so it sublimes at about . Graphite is much more reactive than diamond at standard conditions, despite being more thermodynamically stable, as its delocalised pi system is much more vulnerable to attack. For example, graphite can be oxidised by hot concentrated nitric acid at standard conditions to mellitic acid, C6(CO2H)6, which preserves the hexagonal units of graphite while breaking up the larger structure.\nCarbon sublimes in a carbon arc, which has a temperature of about 5800 K (5,530\u00a0\u00b0C or 9,980\u00a0\u00b0F). Thus, irrespective of its allotropic form, carbon remains solid at higher temperatures than the highest-melting-point metals such as tungsten or rhenium. Although thermodynamically prone to oxidation, carbon resists oxidation more effectively than elements such as iron and copper, which are weaker reducing agents at room temperature.\nCarbon is the sixth element, with a ground-state electron configuration of 1s22s22p2, of which the four outer electrons are valence electrons. Its first four ionisation energies, 1086.5, 2352.6, 4620.5 and 6222.7\u00a0kJ/mol, are much higher than those of the heavier group-14 elements. The electronegativity of carbon is 2.5, significantly higher than the heavier group-14 elements (1.8\u20131.9), but close to most of the nearby nonmetals, as well as some of the second- and third-row transition metals. Carbon's covalent radii are normally taken as 77.2\u00a0pm (C\u2212C), 66.7\u00a0pm (C=C) and 60.3\u00a0pm (C\u2261C), although these may vary depending on coordination number and what the carbon is bonded to. In general, covalent radius decreases with lower coordination number and higher bond order.\nCarbon-based compounds form the basis of all known life on Earth, and the carbon-nitrogen-oxygen cycle provides a small portion of the energy produced by the Sun, and most of the energy in larger stars (e.g. Sirius). Although it forms an extraordinary variety of compounds, most forms of carbon are comparatively unreactive under normal conditions. At standard temperature and pressure, it resists all but the strongest oxidizers. It does not react with sulfuric acid, hydrochloric acid, chlorine or any alkalis. At elevated temperatures, carbon reacts with oxygen to form carbon oxides and will rob oxygen from metal oxides to leave the elemental metal. This exothermic reaction is used in the iron and steel industry to smelt iron and to control the carbon content of steel:\nCarbon reacts with sulfur to form carbon disulfide, and it reacts with steam in the coal-gas reaction used in coal gasification:\nCarbon combines with some metals at high temperatures to form metallic carbides, such as the iron carbide cementite in steel and tungsten carbide, widely used as an abrasive and for making hard tips for cutting tools.\nThe system of carbon allotropes spans a range of extremes:\nAllotropes.\nAtomic carbon is a very short-lived species and, therefore, carbon is stabilized in various multi-atomic structures with diverse molecular configurations called allotropes. The three relatively well-known allotropes of carbon are amorphous carbon, graphite, and diamond. Once considered exotic, fullerenes are nowadays commonly synthesized and used in research; they include buckyballs, carbon nanotubes, carbon nanobuds and nanofibers. Several other exotic allotropes have also been discovered, such as lonsdaleite, glassy carbon, carbon nanofoam and linear acetylenic carbon (carbyne).\nGraphene is a two-dimensional sheet of carbon with the atoms arranged in a hexagonal lattice. As of 2009, graphene appears to be the strongest material ever tested. The process of separating it from graphite will require some further technological development before it is economical for industrial processes. If successful, graphene could be used in the construction of a space elevator. It could also be used to safely store hydrogen for use in a hydrogen based engine in cars.\n The amorphous form is an assortment of carbon atoms in a non-crystalline, irregular, glassy state, not held in a crystalline macrostructure. It is present as a powder, and is the main constituent of substances such as charcoal, lampblack (soot), and activated carbon. At normal pressures, carbon takes the form of graphite, in which each atom is bonded trigonally to three others in a plane composed of fused hexagonal rings, just like those in aromatic hydrocarbons. The resulting network is 2-dimensional, and the resulting flat sheets are stacked and loosely bonded through weak van der Waals forces. This gives graphite its softness and its cleaving properties (the sheets slip easily past one another). Because of the delocalization of one of the outer electrons of each atom to form a \u03c0-cloud, graphite conducts electricity, but only in the plane of each covalently bonded sheet. This results in a lower bulk electrical conductivity for carbon than for most metals. The delocalization also accounts for the energetic stability of graphite over diamond at room temperature.\nAt very high pressures, carbon forms the more compact allotrope, diamond, having nearly twice the density of graphite. Here, each atom is bonded tetrahedrally to four others, forming a 3-dimensional network of puckered six-membered rings of atoms. Diamond has the same cubic structure as silicon and germanium, and because of the strength of the carbon-carbon bonds, it is the hardest naturally occurring substance measured by resistance to scratching. Contrary to the popular belief that \"diamonds are forever\", they are thermodynamically unstable (\u0394f\"G\"\u00b0(diamond, 298\u00a0K) = 2.9\u00a0kJ/mol) under normal conditions (298\u00a0K, 105\u00a0Pa) and should theoretically transform into graphite. But due to a high activation energy barrier, the transition into graphite is so slow at normal temperature that it is unnoticeable. However, at very high temperatures diamond will turn into graphite, and diamonds can burn up in a house fire. The bottom left corner of the phase diagram for carbon has not been scrutinized experimentally. Although a computational study employing density functional theory methods reached the conclusion that as and , diamond becomes more stable than graphite by approximately 1.1\u00a0kJ/mol, more recent and definitive experimental and computational studies show that graphite is more stable than diamond for , without applied pressure, by 2.7\u00a0kJ/mol at \"T\"\u00a0=\u00a00\u00a0K and 3.2\u00a0kJ/mol at \"T\"\u00a0=\u00a0298.15\u00a0K. Under some conditions, carbon crystallizes as lonsdaleite, a hexagonal crystal lattice with all atoms covalently bonded and properties similar to those of diamond.\nFullerenes are a synthetic crystalline formation with a graphite-like structure, but in place of flat hexagonal cells only, some of the cells of which fullerenes are formed may be pentagons, nonplanar hexagons, or even heptagons of carbon atoms. The sheets are thus warped into spheres, ellipses, or cylinders. The properties of fullerenes (split into buckyballs, buckytubes, and nanobuds) have not yet been fully analyzed and represent an intense area of research in nanomaterials. The names \"fullerene\" and \"buckyball\" are given after Richard Buckminster Fuller, popularizer of geodesic domes, which resemble the structure of fullerenes. The buckyballs are fairly large molecules formed completely of carbon bonded trigonally, forming spheroids (the best-known and simplest is the soccerball-shaped C buckminsterfullerene). Carbon nanotubes (buckytubes) are structurally similar to buckyballs, except that each atom is bonded trigonally in a curved sheet that forms a hollow cylinder. Nanobuds were first reported in 2007 and are hybrid buckytube/buckyball materials (buckyballs are covalently bonded to the outer wall of a nanotube) that combine the properties of both in a single structure.\nOf the other discovered allotropes, carbon nanofoam is a ferromagnetic allotrope discovered in 1997. It consists of a low-density cluster-assembly of carbon atoms strung together in a loose three-dimensional web, in which the atoms are bonded trigonally in six- and seven-membered rings. It is among the lightest known solids, with a density of about 2\u00a0kg/m. Similarly, glassy carbon contains a high proportion of closed porosity, but contrary to normal graphite, the graphitic layers are not stacked like pages in a book, but have a more random arrangement. Linear acetylenic carbon has the chemical structure \u2212(C\u2261C)\u2212\u00a0. Carbon in this modification is linear with \"sp\" orbital hybridization, and is a polymer with alternating single and triple bonds. This carbyne is of considerable interest to nanotechnology as its Young's modulus is 40\u00a0times that of the hardest known material\u00a0\u2013 diamond.\nIn 2015, a team at the North Carolina State University announced the development of another allotrope they have dubbed Q-carbon, created by a high-energy low-duration laser pulse on amorphous carbon dust. Q-carbon is reported to exhibit ferromagnetism, fluorescence, and a hardness superior to diamonds.\nIn the vapor phase, some of the carbon is in the form of highly reactive diatomic carbon dicarbon (). When excited, this gas glows green.\nOccurrence.\nCarbon is the fourth most abundant chemical element in the observable universe by mass after hydrogen, helium, and oxygen. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets. Some meteorites contain microscopic diamonds that were formed when the Solar System was still a protoplanetary disk. Microscopic diamonds may also be formed by the intense pressure and high temperature at the sites of meteorite impacts.\nIn 2014 NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. More than 20% of the carbon in the universe may be associated with PAHs, complex compounds of carbon and hydrogen without oxygen. These compounds figure in the PAH world hypothesis where they are hypothesized to have a role in abiogenesis and formation of life. PAHs seem to have been formed \"a couple of billion years\" after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\nIt has been estimated that the solid earth as a whole contains 730 ppm of carbon, with 2000 ppm in the core and 120 ppm in the combined mantle and crust. Since the mass of the earth is , this would imply 4360 million gigatonnes of carbon. This is much more than the amount of carbon in the oceans or atmosphere (below).\nIn combination with oxygen in carbon dioxide, carbon is found in the Earth's atmosphere (approximately 900\u00a0gigatonnes of carbon \u2014 each ppm corresponds to 2.13\u00a0Gt) and dissolved in all water bodies (approximately 36,000\u00a0gigatonnes of carbon). Carbon in the biosphere has been estimated at 550\u00a0gigatonnes but with a large uncertainty, due mostly to a huge uncertainty in the amount of terrestrial deep subsurface bacteria. Hydrocarbons (such as coal, petroleum, and natural gas) contain carbon as well. Coal \"reserves\" (not \"resources\") amount to around 900\u00a0gigatonnes with perhaps 18,000 Gt of resources. Oil reserves are around 150\u00a0gigatonnes. Proven sources of natural gas are about (containing about 105 gigatonnes of carbon), but studies estimate another of \"unconventional\" deposits such as shale gas, representing about 540 gigatonnes of carbon.\nCarbon is also found in methane hydrates in polar regions and under the seas. Various estimates put this carbon between 500, 2500, or 3,000 Gt.\nAccording to one source, in the period from 1751 to 2008 about 347 gigatonnes of carbon were released as carbon dioxide to the atmosphere from burning of fossil fuels. Another source puts the amount added to the atmosphere for the period since 1750 at 879 Gt, and the total going to the atmosphere, sea, and land (such as peat bogs) at almost 2,000 Gt.\nCarbon is a constituent (about 12% by mass) of the very large masses of carbonate rock (limestone, dolomite, marble, and others). Coal is very rich in carbon (anthracite contains 92\u201398%) and is the largest commercial source of mineral carbon, accounting for 4,000\u00a0gigatonnes or 80% of fossil fuel.\nAs for individual carbon allotropes, graphite is found in large quantities in the United States (mostly in New York and Texas), Russia, Mexico, Greenland, and India. Natural diamonds occur in the rock kimberlite, found in ancient volcanic \"necks\", or \"pipes\". Most diamond deposits are in Africa, notably in South Africa, Namibia, Botswana, the Republic of the Congo, and Sierra Leone. Diamond deposits have also been found in Arkansas, Canada, the Russian Arctic, Brazil, and in Northern and Western Australia. Diamonds are now also being recovered from the ocean floor off the Cape of Good Hope. Diamonds are found naturally, but about 30% of all industrial diamonds used in the U.S. are now manufactured.\nCarbon-14 is formed in upper layers of the troposphere and the stratosphere at altitudes of 9\u201315\u00a0km by a reaction that is precipitated by cosmic rays. Thermal neutrons are produced that collide with the nuclei of nitrogen-14, forming carbon-14 and a proton. As such, of atmospheric carbon dioxide contains carbon-14.\nCarbon-rich asteroids are relatively preponderant in the outer parts of the asteroid belt in the Solar System. These asteroids have not yet been directly sampled by scientists. The asteroids can be used in hypothetical space-based carbon mining, which may be possible in the future, but is currently technologically impossible.\nIsotopes.\nIsotopes of carbon are atomic nuclei that contain six protons plus a number of neutrons (varying from 2 to 16). Carbon has two stable, naturally occurring isotopes. The isotope carbon-12 (C) forms 98.93% of the carbon on Earth, while carbon-13 (C) forms the remaining 1.07%. The concentration of C is further increased in biological materials because biochemical reactions discriminate against C. In 1961, the International Union of Pure and Applied Chemistry (IUPAC) adopted the isotope carbon-12 as the basis for atomic weights. Identification of carbon in nuclear magnetic resonance (NMR) experiments is done with the isotope C.\nCarbon-14 (C) is a naturally occurring radioisotope, created in the upper atmosphere (lower stratosphere and upper troposphere) by interaction of nitrogen with cosmic rays. It is found in trace amounts on Earth of 1 part per trillion (0.0000000001%) or more, mostly confined to the atmosphere and superficial deposits, particularly of peat and other organic materials. This isotope decays by 0.158\u00a0MeV \u03b2 emission. Because of its relatively short half-life of \u00a0years, C is virtually absent in ancient rocks. The amount of C in the atmosphere and in living organisms is almost constant, but decreases predictably in their bodies after death. This principle is used in radiocarbon dating, invented in 1949, which has been used extensively to determine the age of carbonaceous materials with ages up to about 40,000\u00a0years.\nThere are 15 known isotopes of carbon and the shortest-lived of these is C which decays through proton emission and has a half-life of 3.5 s. The exotic C exhibits a nuclear halo, which means its radius is appreciably larger than would be expected if the nucleus were a sphere of constant density.\nFormation in stars.\nFormation of the carbon atomic nucleus occurs within a giant or supergiant star through the triple-alpha process. This requires a nearly simultaneous collision of three alpha particles (helium nuclei), as the products of further nuclear fusion reactions of helium with hydrogen or another helium nucleus produce lithium-5 and beryllium-8 respectively, both of which are highly unstable and decay almost instantly back into smaller nuclei. The triple-alpha process happens in conditions of temperatures over 100 megakelvins and helium concentration that the rapid expansion and cooling of the early universe prohibited, and therefore no significant carbon was created during the Big Bang.\nAccording to current physical cosmology theory, carbon is formed in the interiors of stars on the horizontal branch. When massive stars die as supernova, the carbon is scattered into space as dust. This dust becomes component material for the formation of the next-generation star systems with accreted planets. The Solar System is one such star system with an abundance of carbon, enabling the existence of life as we know it. It is the opinion of most scholars that all the carbon in the Solar System and the Milky Way comes from dying stars.\nThe CNO cycle is an additional hydrogen fusion mechanism that powers stars, wherein carbon operates as a catalyst.\nRotational transitions of various isotopic forms of carbon monoxide (for example, CO, CO, and CO) are detectable in the submillimeter wavelength range, and are used in the study of newly forming stars in molecular clouds.\nCarbon cycle.\nUnder terrestrial conditions, conversion of one element to another is very rare. Therefore, the amount of carbon on Earth is effectively constant. Thus, processes that use carbon must obtain it from somewhere and dispose of it somewhere else. The paths of carbon in the environment form the carbon cycle. For example, photosynthetic plants draw carbon dioxide from the atmosphere (or seawater) and build it into biomass, as in the Calvin cycle, a process of carbon fixation. Some of this biomass is eaten by animals, while some carbon is exhaled by animals as carbon dioxide. The carbon cycle is considerably more complicated than this short loop; for example, some carbon dioxide is dissolved in the oceans; if bacteria do not consume it, dead plant or animal matter may become petroleum or coal, which releases carbon when burned.\nCompounds.\nOrganic compounds.\nCarbon can form very long chains of interconnecting carbon\u2013carbon bonds, a property that is called catenation. Carbon-carbon bonds are strong and stable. Through catenation, carbon forms a countless number of compounds. A tally of unique compounds shows that more contain carbon than do not. A similar claim can be made for hydrogen because most organic compounds contain hydrogen chemically bonded to carbon or another common element like oxygen or nitrogen.\nThe simplest form of an organic molecule is the hydrocarbon\u2014a large family of organic molecules that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other atoms, known as heteroatoms. Common heteroatoms that appear in organic compounds include oxygen, nitrogen, sulfur, phosphorus, and the nonradioactive halogens, as well as the metals lithium and magnesium. Organic compounds containing bonds to metal are known as organometallic compounds (\"see below\"). Certain groupings of atoms, often including heteroatoms, recur in large numbers of organic compounds. These collections, known as \"functional groups\", confer common reactivity patterns and allow for the systematic study and categorization of organic compounds. Chain length, shape and functional groups all affect the properties of organic molecules.\nIn most stable compounds of carbon (and nearly all stable \"organic\" compounds), carbon obeys the octet rule and is \"tetravalent\", meaning that a carbon atom forms a total of four covalent bonds (which may include double and triple bonds). Exceptions include a small number of stabilized \"carbocations\" (three bonds, positive charge), \"radicals\" (three bonds, neutral), \"carbanions\" (three bonds, negative charge) and \"carbenes\" (two bonds, neutral), although these species are much more likely to be encountered as unstable, reactive intermediates.\nCarbon occurs in all known organic life and is the basis of organic chemistry. When united with hydrogen, it forms various hydrocarbons that are important to industry as refrigerants, lubricants, solvents, as chemical feedstock for the manufacture of plastics and petrochemicals, and as fossil fuels.\nWhen combined with oxygen and hydrogen, carbon can form many groups of important biological compounds including sugars, lignans, chitins, alcohols, fats, aromatic esters, carotenoids and terpenes. With nitrogen, it forms alkaloids, and with the addition of sulfur also it forms antibiotics, amino acids, and rubber products. With the addition of phosphorus to these other elements, it forms DNA and RNA, the chemical-code carriers of life, and adenosine triphosphate (ATP), the most important energy-transfer molecule in all living cells. Norman Horowitz, head of the Mariner and Viking missions to Mars (1965\u20131976), considered that the unique characteristics of carbon made it unlikely that any other element could replace carbon, even on another planet, to generate the biochemistry necessary for life.\nInorganic compounds.\nCommonly carbon-containing compounds which are associated with minerals or which do not contain bonds to the other carbon atoms, halogens, or hydrogen, are treated separately from classical organic compounds; the definition is not rigid, and the classification of some compounds can vary from author to author (see reference articles above). Among these are the simple oxides of carbon. The most prominent oxide is carbon dioxide (). This was once the principal constituent of the paleoatmosphere, but is a minor component of the Earth's atmosphere today. Dissolved in water, it forms carbonic acid (), but as most compounds with multiple single-bonded oxygens on a single carbon it is unstable. Through this intermediate, though, resonance-stabilized carbonate ions are produced. Some important minerals are carbonates, notably calcite. Carbon disulfide () is similar. Nevertheless, due to its physical properties and its association with organic synthesis, carbon disulfide is sometimes classified as an \"organic\" solvent.\nThe other common oxide is carbon monoxide (CO). It is formed by incomplete combustion, and is a colorless, odorless gas. The molecules each contain a triple bond and are fairly polar, resulting in a tendency to bind permanently to hemoglobin molecules, displacing oxygen, which has a lower binding affinity. Cyanide (CN), has a similar structure, but behaves much like a halide ion (pseudohalogen). For example, it can form the nitride cyanogen molecule ((CN)), similar to diatomic halides. Likewise, the heavier analog of cyanide, cyaphide (CP), is also considered inorganic, though most simple derivatives are highly unstable. Other uncommon oxides are carbon suboxide (), the unstable dicarbon monoxide (CO), carbon trioxide (CO), cyclopentanepentone (CO), cyclohexanehexone (CO), and mellitic anhydride (CO). However, mellitic anhydride is the triple acyl anhydride of mellitic acid; moreover, it contains a benzene ring. Thus, many chemists consider it to be organic.\nWith reactive metals, such as tungsten, carbon forms either carbides (C) or acetylides () to form alloys with high melting points. These anions are also associated with methane and acetylene, both very weak acids. With an electronegativity of 2.5, carbon prefers to form covalent bonds. A few carbides are covalent lattices, like carborundum (SiC), which resembles diamond. Nevertheless, even the most polar and salt-like of carbides are not completely ionic compounds.\nOrganometallic compounds.\nOrganometallic compounds by definition contain at least one carbon-metal covalent bond. A wide range of such compounds exist; major classes include simple alkyl-metal compounds (for example, tetraethyllead), \u03b7-alkene compounds (for example, Zeise's salt), and \u03b7-allyl compounds (for example, allylpalladium chloride dimer); metallocenes containing cyclopentadienyl ligands (for example, ferrocene); and transition metal carbene complexes. Many metal carbonyls and metal cyanides exist (for example, tetracarbonylnickel and potassium ferricyanide); some workers consider metal carbonyl and cyanide complexes without other carbon ligands to be purely inorganic, and not organometallic. However, most organometallic chemists consider metal complexes with any carbon ligand, even 'inorganic carbon' (e.g., carbonyls, cyanides, and certain types of carbides and acetylides) to be organometallic in nature. Metal complexes containing organic ligands without a carbon-metal covalent bond (e.g., metal carboxylates) are termed \"metalorganic\" compounds.\nWhile carbon is understood to strongly prefer formation of four covalent bonds, other exotic bonding schemes are also known. Carboranes are highly stable dodecahedral derivatives of the [B12H12]2- unit, with one BH replaced with a CH+. Thus, the carbon is bonded to five boron atoms and one hydrogen atom. The cation [(PhPAu)C] contains an octahedral carbon bound to six phosphine-gold fragments. This phenomenon has been attributed to the aurophilicity of the gold ligands, which provide additional stabilization of an otherwise labile species. In nature, the iron-molybdenum cofactor (FeMoco) responsible for microbial nitrogen fixation likewise has an octahedral carbon center (formally a carbide, C(-IV)) bonded to six iron atoms. In 2016, it was confirmed that, in line with earlier theoretical predictions, the hexamethylbenzene dication contains a carbon atom with six bonds. More specifically, the dication could be described structurally by the formulation [MeC(\u03b75-C5Me5)]2+, making it an \"organic metallocene\" in which a MeC3+ fragment is bonded to a \u03b75-C5Me5\u2212 fragment through all five of the carbons of the ring.\nIt is important to note that in the cases above, each of the bonds to carbon contain less than two formal electron pairs. Thus, the formal electron count of these species does not exceed an octet. This makes them hypercoordinate but not hypervalent. Even in cases of alleged 10-C-5 species (that is, a carbon with five ligands and a formal electron count of ten), as reported by Akiba and co-workers, electronic structure calculations conclude that the electron population around carbon is still less than eight, as is true for other compounds featuring four-electron three-center bonding.\nHistory and etymology.\nThe English name \"carbon\" comes from the Latin \"carbo\" for coal and charcoal, whence also comes the French \"charbon\", meaning charcoal. In German, Dutch and Danish, the names for carbon are \"Kohlenstoff\", \"koolstof\", and \"kulstof\" respectively, all literally meaning coal-substance.\nCarbon was discovered in prehistory and was known in the forms of soot and charcoal to the earliest human civilizations. Diamonds were known probably as early as 2500\u00a0BCE in China, while carbon in the form of charcoal was made by the same chemistry as it is today, by heating wood in a pyramid covered with clay to exclude air.\n In 1722, Ren\u00e9 Antoine Ferchault de R\u00e9aumur demonstrated that iron was transformed into steel through the absorption of some substance, now known to be carbon. In 1772, Antoine Lavoisier showed that diamonds are a form of carbon; when he burned samples of charcoal and diamond and found that neither produced any water and that both released the same amount of carbon dioxide per gram. In 1779, Carl Wilhelm Scheele showed that graphite, which had been thought of as a form of lead, was instead identical with charcoal but with a small admixture of iron, and that it gave \"aerial acid\" (his name for carbon dioxide) when oxidized with nitric acid. In 1786, the French scientists Claude Louis Berthollet, Gaspard Monge and C. A. Vandermonde confirmed that graphite was mostly carbon by oxidizing it in oxygen in much the same way Lavoisier had done with diamond. Some iron again was left, which the French scientists thought was necessary to the graphite structure. In their publication they proposed the name \"carbone\" (Latin \"carbonum\") for the element in graphite which was given off as a gas upon burning graphite. Antoine Lavoisier then listed carbon as an element in his 1789 textbook.\nA new allotrope of carbon, fullerene, that was discovered in 1985 includes nanostructured forms such as buckyballs and nanotubes. Their discoverers\u00a0\u2013 Robert Curl, Harold Kroto, and Richard Smalley\u00a0\u2013 received the Nobel Prize in Chemistry in 1996. The resulting renewed interest in new forms led to the discovery of further exotic allotropes, including glassy carbon, and the realization that \"amorphous carbon\" is not strictly amorphous.\nProduction.\nGraphite.\nCommercially viable natural deposits of graphite occur in many parts of the world, but the most important sources economically are in China, India, Brazil, and North Korea. Graphite deposits are of metamorphic origin, found in association with quartz, mica, and feldspars in schists, gneisses, and metamorphosed sandstones and limestone as lenses or veins, sometimes of a metre or more in thickness. Deposits of graphite in Borrowdale, Cumberland, England were at first of sufficient size and purity that, until the 19th century, pencils were made by sawing blocks of natural graphite into strips before encasing the strips in wood. Today, smaller deposits of graphite are obtained by crushing the parent rock and floating the lighter graphite out on water.\nThere are three types of natural graphite\u2014amorphous, flake or crystalline flake, and vein or lump. Amorphous graphite is the lowest quality and most abundant. Contrary to science, in industry \"amorphous\" refers to very small crystal size rather than complete lack of crystal structure. Amorphous is used for lower value graphite products and is the lowest priced graphite. Large amorphous graphite deposits are found in China, Europe, Mexico and the United States. Flake graphite is less common and of higher quality than amorphous; it occurs as separate plates that crystallized in metamorphic rock. Flake graphite can be four times the price of amorphous. Good quality flakes can be processed into expandable graphite for many uses, such as flame retardants. The foremost deposits are found in Austria, Brazil, Canada, China, Germany and Madagascar. Vein or lump graphite is the rarest, most valuable, and highest quality type of natural graphite. It occurs in veins along intrusive contacts in solid lumps, and it is only commercially mined in Sri Lanka.\nAccording to the USGS, world production of natural graphite was 1.1\u00a0million tonnes in 2010, to which China contributed 800,000\u00a0t, India 130,000 t, Brazil 76,000\u00a0t, North Korea 30,000\u00a0t and Canada 25,000 t. No natural graphite was reported mined in the United States, but 118,000 t of synthetic graphite with an estimated value of $998\u00a0million was produced in 2009.\nDiamond.\nThe diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world (see figure).\nOnly a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care has to be taken in order to prevent larger diamonds from being destroyed in this process and subsequently the particles are sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.\nHistorically diamonds were known to be found only in alluvial deposits in southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BC to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725.\nDiamond production of primary deposits (kimberlites and lamproites) only started in the 1870s after the discovery of the diamond fields in South Africa. Production has increased over time and an accumulated total of over 4.5\u00a0billion carats have been mined since that date. Most commercially viable diamond deposits were in Russia, Botswana, Australia and the Democratic Republic of Congo. By 2005, Russia produced almost one-fifth of the global diamond output (mostly in Yakutia territory; for example, Mir pipe and Udachnaya pipe) but the Argyle mine in Australia became the single largest source, producing 14 million carats in 2018. New finds, the Canadian mines at Diavik and Ekati, are expected to become even more valuable owing to their production of gem quality stones.\nIn the United States, diamonds have been found in Arkansas, Colorado, and Montana. In 2004, a startling discovery of a microscopic diamond in the United States led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana.\nApplications.\nCarbon is essential to all known living systems, and without it life as we know it could not exist (see alternative biochemistry). The major economic use of carbon other than food and wood is in the form of hydrocarbons, most notably the fossil fuel methane gas and crude oil (petroleum). Crude oil is distilled in refineries by the petrochemical industry to produce gasoline, kerosene, and other products. Cellulose is a natural, carbon-containing polymer produced by plants in the form of wood, cotton, linen, and hemp. Cellulose is used primarily for maintaining structure in plants. Commercially valuable carbon polymers of animal origin include wool, cashmere, and silk. Plastics are made from synthetic carbon polymers, often with oxygen and nitrogen atoms included at regular intervals in the main polymer chain. The raw materials for many of these synthetic substances come from crude oil.\nThe uses of carbon and its compounds are extremely varied. It can form alloys with iron, of which the most common is carbon steel. Graphite is combined with clays to form the 'lead' used in pencils used for writing and drawing. It is also used as a lubricant and a pigment, as a moulding material in glass manufacture, in electrodes for dry batteries and in electroplating and electroforming, in brushes for electric motors, and as a neutron moderator in nuclear reactors.\nCharcoal is used as a drawing material in artwork, barbecue grilling, iron smelting, and in many other applications. Wood, coal and oil are used as fuel for production of energy and heating. Gem quality diamond is used in jewelry, and industrial diamonds are used in drilling, cutting and polishing tools for machining metals and stone. Plastics are made from fossil hydrocarbons, and carbon fiber, made by pyrolysis of synthetic polyester fibers is used to reinforce plastics to form advanced, lightweight composite materials.\nCarbon fiber is made by pyrolysis of extruded and stretched filaments of polyacrylonitrile (PAN) and other organic substances. The crystallographic structure and mechanical properties of the fiber depend on the type of starting material, and on the subsequent processing. Carbon fibers made from PAN have structure resembling narrow filaments of graphite, but thermal processing may re-order the structure into a continuous rolled sheet. The result is fibers with higher specific tensile strength than steel.\nCarbon black is used as the black pigment in printing ink, artist's oil paint, and water colours, carbon paper, automotive finishes, India ink and laser printer toner. Carbon black is also used as a filler in rubber products such as tyres and in plastic compounds. Activated charcoal is used as an absorbent and adsorbent in filter material in applications as diverse as gas masks, water purification, and kitchen extractor hoods, and in medicine to absorb toxins, poisons, or gases from the digestive system. Carbon is used in chemical reduction at high temperatures. Coke is used to reduce iron ore into iron (smelting). Case hardening of steel is achieved by heating finished steel components in carbon powder. Carbides of silicon, tungsten, boron, and titanium are among the hardest known materials, and are used as abrasives in cutting and grinding tools. Carbon compounds make up most of the materials used in clothing, such as natural and synthetic textiles and leather, and almost all of the interior surfaces in the built environment other than glass, stone, drywall, and metal.\nDiamonds.\nThe diamond industry falls into two categories: one dealing with gem-grade diamonds and the other, with industrial-grade diamonds. While a large trade in both types of diamonds exists, the two markets function dramatically differently.\nUnlike precious metals such as gold or platinum, gem diamonds do not trade as a commodity. There is a substantial mark-up in the sale of diamonds, and there is not a very active market for resale of diamonds.\nIndustrial diamonds are valued mostly for their hardness and heat conductivity, with the gemological qualities of clarity and color being mostly irrelevant. About 80% of mined diamonds (equal to about 100 million carats or 20\u00a0tonnes annually) are unsuitable for use as gemstones and relegated for industrial use (known as \"bort)\". Synthetic diamonds, invented in the 1950s, found almost immediate industrial applications; 3\u00a0billion carats (600\u00a0tonnes) of synthetic diamond is produced annually.\nThe dominant industrial use of diamond is in cutting, drilling, grinding, and polishing. Most of these applications do not require large diamonds; in fact, most diamonds of gem-quality except for their small size can be used industrially. Diamonds are embedded in drill tips or saw blades, or ground into a powder for use in grinding and polishing applications. Specialized applications include use in laboratories as containment for high-pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances in the production of synthetic diamonds, new applications are becoming feasible. Garnering much excitement is the possible use of diamond as a semiconductor suitable for microchips, and because of its exceptional heat conductance property, as a heat sink in electronics.\nPrecautions.\nPure carbon has extremely low toxicity to humans and can be handled safely in the form of graphite or charcoal. It is resistant to dissolution or chemical attack, even in the acidic contents of the digestive tract. Consequently, once it enters into the body's tissues it is likely to remain there indefinitely. Carbon black was probably one of the first pigments to be used for tattooing, and \u00d6tzi the Iceman was found to have carbon tattoos that survived during his life and for 5200\u00a0years after his death. Inhalation of coal dust or soot (carbon black) in large quantities can be dangerous, irritating lung tissues and causing the congestive lung disease, coalworker's pneumoconiosis. Diamond dust used as an abrasive can be harmful if ingested or inhaled. Microparticles of carbon are produced in diesel engine exhaust fumes, and may accumulate in the lungs. In these examples, the harm may result from contaminants (e.g., organic chemicals, heavy metals) rather than from the carbon itself.\nCarbon generally has low toxicity to life on Earth; but carbon nanoparticles are deadly to \"Drosophila\".\nCarbon may burn vigorously and brightly in the presence of air at high temperatures. Large accumulations of coal, which have remained inert for hundreds of millions of years in the absence of oxygen, may spontaneously combust when exposed to air in coal mine waste tips, ship cargo holds and coal bunkers, and storage dumps.\nIn nuclear applications where graphite is used as a neutron moderator, accumulation of Wigner energy followed by a sudden, spontaneous release may occur. Annealing to at least 250\u00a0\u00b0C can release the energy safely, although in the Windscale fire the procedure went wrong, causing other reactor materials to combust.\nThe great variety of carbon compounds include such lethal poisons as tetrodotoxin, the lectin ricin from seeds of the castor oil plant \"Ricinus communis\", cyanide (CN), and carbon monoxide; and such essentials to life as glucose and protein."}
{"id": "5300", "revid": "910180", "url": "https://en.wikipedia.org/wiki?curid=5300", "title": "Computer data storage", "text": "Computer data storage or digital data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.\nThe central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally, the fast technologies are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".\nEven the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.\nFunctionality.\nWithout a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk calculators, digital signal processors, and other specialized devices. Von Neumann machines differ in having a memory in which they store their operating instructions and data. Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be reprogrammed with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann machines.\nData organization and representation.\nA modern digital computer represents data using the binary numeral system. Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of bits, or binary digits, each of which has a value of 0\u00a0or\u00a01. The most common unit of storage is the byte, equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate \"the binary representation of the piece of information\", or simply data. For example, the complete works of Shakespeare, about 1250\u00a0pages in print, can be stored in about five megabytes (40\u00a0million bits) with one byte per character.\nData are encoded by assigning a bit pattern to each character, digit, or multimedia object. Many standards exist for encoding (e.g. character encodings like ASCII, image encodings like JPEG, and video encodings like MPEG-4).\nBy adding bits to each encoded unit, redundancy allows the computer to detect errors in coded data and correct them based on mathematical algorithms. Errors generally occur in low probabilities due to random bit value flipping, or \"physical bit fatigue\", loss of the physical bit in the storage of its ability to maintain a distinguishable value (0\u00a0or\u00a01), or due to errors in inter or intra-computer communication. A random bit flip (e.g. due to random radiation) is typically corrected upon detection. A bit or a group of malfunctioning physical bits (the specific defective bit is not always known; group definition depends on the specific storage device) is typically automatically fenced out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The cyclic redundancy check (CRC) method is typically used in communications and storage for error detection. A detected error is then retried.\nData compression methods allow in many cases (such as a database) to represent a string of bits by a shorter bit string (\"compress\") and reconstruct the original string (\"decompress\") when needed. This utilizes substantially less storage (tens of percent) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of the trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data compressed or not.\nFor security reasons, certain types of data (e.g. credit card information) may be kept encrypted in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots.\nHierarchy of storage.\nGenerally, the lower a storage is in the hierarchy, the lesser its bandwidth and the greater its access latency is from the CPU. This traditional division of storage to primary, secondary, tertiary, and off-line storage is also guided by cost per bit.\nIn contemporary usage, \"memory\" is usually fast but temporary semiconductor read-write memory, typically DRAM (dynamic RAM) or other such devices. \"Storage\" consists of storage devices and their media not directly accessible by the CPU (secondary or tertiary storage), typically hard disk drives, optical disc drives, and other devices slower than RAM but non-volatile (retaining contents when powered down).\nHistorically, \"memory\" has, depending on technology, been called \"central memory\", \"core memory\", \"core storage\", \"drum\", \"main memory\", \"real storage\", or \"internal memory\". Meanwhile, slower persistent storage devices have been referred to as \"secondary storage\", \"external memory\", or \"auxiliary/peripheral storage\".\nPrimary storage.\n\"Primary storage\" (also known as \"main memory\", \"internal memory\", or \"prime memory\"), often referred to simply as \"memory\", is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in a uniform manner.\nHistorically, early computers used delay lines, Williams tubes, or rotating magnetic drums as primary storage. By 1954, those unreliable methods were mostly replaced by magnetic-core memory. Core memory remained dominant until the 1970s, when advances in integrated circuit technology allowed semiconductor memory to become economically competitive.\nThis led to modern random-access memory (RAM). It is small-sized, light, but quite expensive at the same time. The particular types of RAM used for primary storage are volatile, meaning that they lose the information when not powered. Besides storing opened programs, it serves as disk cache and write buffer to improve both reading and writing performance. Operating systems borrow RAM capacity for caching so long as it's not needed by running software. Spare memory can be utilized as RAM drive for temporary high-speed data storage.\nAs shown in the diagram, traditionally there are two more sub-layers of the primary storage, besides main large-capacity RAM:\nMain memory is directly or indirectly connected to the central processing unit via a \"memory bus\". It is actually two buses (not on the diagram): an address bus and a data bus. The CPU firstly sends a number through an address bus, a number called memory address, that indicates the desired location of data. Then it reads or writes the data in the memory cells using the data bus. Additionally, a memory management unit (MMU) is a small device between CPU and RAM recalculating the actual memory address, for example to provide an abstraction of virtual memory or other tasks.\nAs the RAM types used for primary storage are volatile (uninitialized at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence, non-volatile primary storage containing a small startup program (BIOS) is used to bootstrap the computer, that is, to read a larger program from non-volatile \"secondary\" storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for read-only memory (the terminology may be somewhat confusing as most ROM types are also capable of \"random access\").\nMany types of \"ROM\" are not literally \"read only\", as updates to them are possible; however it is slow and memory must be erased in large portions before it can be re-written. Some embedded systems run programs directly from ROM (or similar), because such programs are rarely changed. Standard computers do not store non-rudimentary programs in ROM, and rather, use large capacities of secondary storage, which is non-volatile as well, and not as costly.\nRecently, \"primary storage\" and \"secondary storage\" in some uses refer to what was historically called, respectively, \"secondary storage\" and \"tertiary storage\".\nThe primary storage, including ROM, EEPROM, NOR flash, and RAM, are usually byte-addressable.\nSecondary storage.\n\"Secondary storage\" (also known as \"external memory\" or \"auxiliary storage\") differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile (retaining data when its power is shut off). Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive.\nIn modern computers, hard disk drives (HDDs) or solid-state drives (SSDs) are usually used as secondary storage. The access time per byte for HDDs or SSDs is typically measured in milliseconds (thousandths of a second), while the access time per byte for primary storage is measured in nanoseconds (billionths of a second). Thus, secondary storage is significantly slower than primary storage. Rotating optical storage devices, such as CD and DVD drives, have even longer access times. Other examples of secondary storage technologies include USB flash drives, floppy disks, magnetic tape, paper tape, punched cards, and RAM disks.\nOnce the disk read/write head on HDDs reaches the proper placement and the data, subsequent data on the track are very fast to access. To reduce the seek time and rotational latency, data are transferred to and from disks in large contiguous blocks. Sequential or block access on disks is orders of magnitude faster than random access, and many sophisticated paradigms have been developed to design efficient algorithms based on sequential and block access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel to increase the bandwidth between primary and secondary memory.\nSecondary storage is often formatted according to a file system format, which provides the abstraction necessary to organize data into files and directories, while also providing metadata describing the owner of a certain file, the access time, the access permissions, and other information.\nMost computer operating systems use the concept of virtual memory, allowing the utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks (pages) to a swap file or page file on secondary storage, retrieving them later when needed. If a lot of pages are moved to slower secondary storage, the system performance is degraded.\nThe secondary storage, including HDD, ODD and SSD, are usually block-addressable.\nTertiary storage.\n\"Tertiary storage\" or \"tertiary memory\" is a level below secondary storage. Typically, it involves a robotic mechanism which will \"mount\" (insert) and \"dismount\" removable mass storage media into a storage device according to the system's demands; such data are often copied to secondary storage before use. It is primarily used for archiving rarely accessed information since it is much slower than secondary storage (e.g. 5\u201360 seconds vs. 1\u201310 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include tape libraries and optical jukeboxes.\nWhen a computer needs to read information from the tertiary storage, it will first consult a catalog database to determine which tape or disc contains the information. Next, the computer will instruct a robotic arm to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library.\nTertiary storage is also known as \"nearline storage\" because it is \"near to online\". The formal distinction between online, nearline, and offline storage is:\nFor example, always-on spinning hard disk drives are online storage, while spinning drives that spin down automatically, such as in massive arrays of idle disks (MAID), are nearline storage. Removable media such as tape cartridges that can be automatically loaded, as in tape libraries, are nearline storage, while tape cartridges that must be manually loaded are offline storage.\nOff-line storage.\n\"Off-line storage\" is computer data storage on a medium or a device that is not under the control of a processing unit. The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction.\nOff-line storage is used to transfer information since the detached medium can easily be physically transported. Additionally, it is useful for cases of disaster, where, for example, a fire destroys the original data, a medium in a remote location will be unaffected, enabling disaster recovery. Off-line storage increases general information security since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage.\nIn modern personal computers, most secondary and tertiary storage media are also used for off-line storage. Optical discs and flash memory devices are the most popular, and to a much lesser extent removable hard disk drives; older examples include floppy disks and Zip disks. In enterprise uses, magnetic tape cartridges are predominant; older examples include open-reel magnetic tape and punched cards.\nCharacteristics of storage.\nStorage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressability. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance.\nVolatility.\nNon-volatile memory retains the stored information even if not constantly supplied with electric power. It is suitable for long-term storage of information. Volatile memory requires constant power to maintain the stored information. The fastest memory technologies are volatile ones, although that is not a universal rule. Since the primary storage is required to be very fast, it predominantly uses volatile memory.\nDynamic random-access memory is a form of volatile memory that also requires the stored information to be periodically reread and rewritten, or refreshed, otherwise it would vanish. Static random-access memory is a form of volatile memory similar to DRAM with the exception that it never needs to be refreshed as long as power is applied; it loses its content when the power supply is lost.\nAn uninterruptible power supply (UPS) can be used to give a computer a brief window of time to move information from primary volatile storage into non-volatile storage before the batteries are exhausted. Some systems, for example EMC Symmetrix, have integrated batteries that maintain volatile storage for several minutes.\nPerformance.\nUtilities such as hdparm and sar can be used to measure IO performance in Linux.\nSecurity.\nFull disk encryption, volume and virtual disk encryption, andor file/folder encryption is readily available for most storage devices.\nHardware memory encryption is available in Intel Architecture, supporting Total Memory Encryption (TME) and page granular memory encryption with multiple keys (MKTME). and in SPARC M7 generation since October 2015.\nVulnerability and reliability.\nDistinct types of data storage have different points of failure and various methods of predictive failure analysis.\nVulnerabilities that can instantly lead to total loss are head crashing on mechanical hard drives and failure of electronic components on flash storage.\nError detection.\nImpending failure on hard disk drives is estimable using S.M.A.R.T. diagnostic data that includes the hours of operation and the count of spin-ups, though its reliability is disputed.\nFlash storage may experience downspiking transfer rates as a result of accumulating errors, which the flash memory controller attempts to correct.\nThe health of optical media can be determined by measuring correctable minor errors, of which high counts signify deteriorating and/or low-quality media. Too many consecutive minor errors can lead to data corruption. Not all vendors and models of optical drives support error scanning.\nStorage media.\n, the most commonly used data storage media are semiconductor, magnetic, and optical, while paper still sees some limited usage. Some other fundamental storage technologies, such as all-flash arrays (AFAs) are proposed for development.\nSemiconductor.\nSemiconductor memory uses semiconductor-based integrated circuit (IC) chips to store information. Data are typically stored in metal\u2013oxide\u2013semiconductor (MOS) memory cells. A semiconductor memory chip may contain millions of memory cells, consisting of tiny MOS field-effect transistors (MOSFETs) and/or MOS capacitors. Both \"volatile\" and \"non-volatile\" forms of semiconductor memory exist, the former using standard MOSFETs and the latter using floating-gate MOSFETs.\nIn modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor random-access memory (RAM), particularly dynamic random-access memory (DRAM). Since the turn of the century, a type of non-volatile floating-gate semiconductor memory known as flash memory has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers that are designed for them.\nAs early as 2006, notebook and desktop computer manufacturers started using flash-based solid-state drives (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD.\nMagnetic.\nMagnetic storage uses different patterns of magnetization on a magnetically coated surface to store information. Magnetic storage is \"non-volatile\". The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms:\nIn early computers, magnetic storage was also used as:\nMagnetic storage does not have a definite limit of rewriting cycles like flash storage and re-writeable optical media, as altering magnetic fields causes no physical wear. Rather, their life span is limited by mechanical parts.\nOptical.\nOptical storage, the typical optical disc, stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a laser diode and observing the reflection. Optical disc storage is \"non-volatile\". The deformities may be permanent (read only media), formed once (write once media) or reversible (recordable or read/write media). The following forms are in common use :\nMagneto-optical disc storage is optical disc storage where the magnetic state on a ferromagnetic surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is \"non-volatile\", \"sequential access\", slow write, fast read storage used for tertiary and off-line storage.\n3D optical data storage has also been proposed.\nLight induced magnetization melting in magnetic photoconductors has also been proposed for high-speed low-energy consumption magneto-optical storage.\nPaper.\nPaper data storage, typically in the form of paper tape or punched cards, has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole. Barcodes make it possible for objects that are sold or transported to have some computer-readable information securely attached.\nRelatively small amounts of digital data (compared to other digital data storage) may be backed up on paper as a matrix barcode for very long-term storage, as the longevity of paper typically exceeds even magnetic data storage.\nRelated technologies.\nRedundancy.\nWhile a group of bits malfunction may be resolved by error detection and correction mechanisms (see above), storage device malfunction requires different solutions. The following solutions are commonly used and valid for most storage devices:\nDevice mirroring and typical RAID are designed to handle a single device failure in the RAID group of devices. However, if a second failure occurs before the RAID group is completely repaired from the first failure, then data can be lost. The probability of a single failure is typically small. Thus the probability of two failures in the same RAID group in time proximity is much smaller (approximately the probability squared, i.e., multiplied by itself). If a database cannot tolerate even such a smaller probability of data loss, then the RAID group itself is replicated (mirrored). In many cases such mirroring is done geographically remotely, in a different storage array, to handle recovery from disasters (see disaster recovery above).\nNetwork connectivity.\nA secondary or tertiary storage may connect to a computer utilizing computer networks. This concept does not pertain to the primary storage, which is shared between multiple processors to a lesser degree.\nRobotic storage.\nLarge quantities of individual magnetic tapes, and optical or magneto-optical discs may be stored in robotic tertiary storage devices. In tape storage field they are known as tape libraries, and in optical storage field optical jukeboxes, or optical disk libraries per analogy. The smallest forms of either technology containing just one drive device are referred to as autoloaders or autochangers.\nRobotic-access storage devices may have a number of slots, each holding individual media, and usually one or more picking robots that traverse the slots and load media to built-in drives. The arrangement of the slots and picking devices affects performance. Important characteristics of such storage are possible expansion options: adding slots, modules, drives, robots. Tape libraries may have from 10 to more than 100,000 slots, and provide terabytes or petabytes of near-line information. Optical jukeboxes are somewhat smaller solutions, up to 1,000 slots.\nRobotic storage is used for backups, and for high-capacity archives in imaging, medical, and video industries. Hierarchical storage management is a most known archiving strategy of automatically \"migrating\" long-unused files from fast hard disk storage to libraries or jukeboxes. If the files are needed, they are \"retrieved\" back to disk."}
{"id": "5302", "revid": "48344112", "url": "https://en.wikipedia.org/wiki?curid=5302", "title": "Conditional", "text": "Conditional (if then) may refer to:"}
{"id": "5303", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5303", "title": "Conic sections", "text": ""}
{"id": "5304", "revid": "36664386", "url": "https://en.wikipedia.org/wiki?curid=5304", "title": "Cone (disambiguation)", "text": "A cone is a basic geometrical shape.\nCone may also refer to:"}
{"id": "5306", "revid": "5320876", "url": "https://en.wikipedia.org/wiki?curid=5306", "title": "Chemical equilibrium", "text": "In a chemical reaction, chemical equilibrium is the state in which both the reactants and products are present in concentrations which have no further tendency to change with time, so that there is no observable change in the properties of the system. This state results when the forward reaction proceeds at the same rate as the reverse reaction. The reaction rates of the forward and backward reactions are generally not zero, but they are equal. Thus, there are no net changes in the concentrations of the reactants and products. Such a state is known as dynamic equilibrium.\nHistorical introduction.\nThe concept of chemical equilibrium was developed in 1803, after Berthollet found that some chemical reactions are reversible. For any reaction mixture to exist at equilibrium, the rates of the forward and backward (reverse) reactions must be equal. In the following chemical equation, arrows point both ways to indicate equilibrium. A and B are reactant chemical species, S and T are product species, and \"\u03b1\", \"\u03b2\", \"\u03c3\", and \"\u03c4\" are the stoichiometric coefficients of the respective reactants and products:\nThe equilibrium concentration position of a reaction is said to lie \"far to the right\" if, at equilibrium, nearly all the reactants are consumed. Conversely the equilibrium position is said to be \"far to the left\" if hardly any product is formed from the reactants.\nGuldberg and Waage (1865), building on Berthollet's ideas, proposed the law of mass action:\nwhere A, B, S and T are active masses and \"k\"+ and \"k\"\u2212 are rate constants. Since at equilibrium forward and backward rates are equal:\nand the ratio of the rate constants is also a constant, now known as an equilibrium constant.\nBy convention, the products form the numerator.\nHowever, the law of mass action is valid only for concerted one-step reactions that proceed through a single transition state and is not valid in general because rate equations do not, in general, follow the stoichiometry of the reaction as Guldberg and Waage had proposed (see, for example, nucleophilic aliphatic substitution by SN1 or reaction of hydrogen and bromine to form hydrogen bromide). Equality of forward and backward reaction rates, however, is a necessary condition for chemical equilibrium, though it is not sufficient to explain why equilibrium occurs.\nDespite the limitations of this derivation, the equilibrium constant for a reaction is indeed a constant, independent of the activities of the various species involved, though it does depend on temperature as observed by the van 't Hoff equation. Adding a catalyst will affect both the forward reaction and the reverse reaction in the same way and will not have an effect on the equilibrium constant. The catalyst will speed up both reactions thereby increasing the speed at which equilibrium is reached.\nAlthough the macroscopic equilibrium concentrations are constant in time, reactions do occur at the molecular level. For example, in the case of acetic acid dissolved in water and forming acetate and hydronium ions,\na proton may hop from one molecule of acetic acid onto a water molecule and then onto an acetate anion to form another molecule of acetic acid and leaving the number of acetic acid molecules unchanged. This is an example of dynamic equilibrium. Equilibria, like the rest of thermodynamics, are statistical phenomena, averages of microscopic behavior.\nLe Ch\u00e2telier's principle (1884) predicts the behavior of an equilibrium system when changes to its reaction conditions occur. \"If a dynamic equilibrium is disturbed by changing the conditions, the position of equilibrium moves to partially reverse the change\". For example, adding more S (to the chemical reaction above) from the outside will cause an excess of products, and the system will try to counteract this by increasing the reverse reaction and pushing the equilibrium point backward (though the equilibrium constant will stay the same).\nIf mineral acid is added to the acetic acid mixture, increasing the concentration of hydronium ion, the amount of dissociation must decrease as the reaction is driven to the left in accordance with this principle. This can also be deduced from the equilibrium constant expression for the reaction:\nIf {H3O+} increases {CH3CO2H} must increase and must decrease. The H2O is left out, as it is the solvent and its concentration remains high and nearly constant.\nJ. W. Gibbs suggested in 1873 that equilibrium is attained when the \"available energy\" (now known as Gibbs free energy or Gibbs energy) of the system is at its minimum value, assuming the reaction is carried out at a constant temperature and pressure. What this means is that the derivative of the Gibbs energy with respect to reaction coordinate (a measure of the extent of reaction that has occurred, ranging from zero for all reactants to a maximum for all products) vanishes (because dG = 0), signaling a stationary point. This derivative is called the reaction Gibbs energy (or energy change) and corresponds to the difference between the chemical potentials of reactants and products at the composition of the reaction mixture. This criterion is both necessary and sufficient. If a mixture is not at equilibrium, the liberation of the excess Gibbs energy (or Helmholtz energy at constant volume reactions) is the \"driving force\" for the composition of the mixture to change until equilibrium is reached. The equilibrium constant can be related to the standard Gibbs free energy change for the reaction by the equation\nwhere \"R\" is the universal gas constant and \"T\" the temperature.\nWhen the reactants are dissolved in a medium of high ionic strength the quotient of activity coefficients may be taken to be constant. In that case the concentration quotient, \"K\"c,\nwhere [A] is the concentration of A, etc., is independent of the analytical concentration of the reactants. For this reason, equilibrium constants for solutions are usually determined in media of high ionic strength. \"Kc\" varies with ionic strength, temperature and pressure (or volume). Likewise \"Kp\" for gases depends on partial pressure. These constants are easier to measure and encountered in high-school chemistry courses.\nThermodynamics.\nAt constant temperature and pressure, one must consider the Gibbs free energy, \"G\", while at constant temperature and volume, one must consider the Helmholtz free energy, \"A\", for the reaction; and at constant internal energy and volume, one must consider the entropy, \"S\", for the reaction.\nThe constant volume case is important in geochemistry and atmospheric chemistry where pressure variations are significant. Note that, if reactants and products were in standard state (completely pure), then there would be no reversibility and no equilibrium. Indeed, they would necessarily occupy disjoint volumes of space. The mixing of the products and reactants contributes a large entropy increase (known as entropy of mixing) to states containing equal mixture of products and reactants and gives rise to a distinctive minimum in the Gibbs energy as a function of the extent of reaction. The standard Gibbs energy change, together with the Gibbs energy of mixing, determine the equilibrium state.\nIn this article only the constant pressure case is considered. The relation between the Gibbs free energy and the equilibrium constant can be found by considering chemical potentials.\nAt constant temperature and pressure in the absence of an applied voltage, the Gibbs free energy, \"G\", for the reaction depends only on the extent of reaction: \"\u03be\" (Greek letter xi), and can only decrease according to the second law of thermodynamics. It means that the derivative of \"G\" with respect to \"\u03be\" must be negative if the reaction happens; at the equilibrium this derivative is equal to zero.\nIn order to meet the thermodynamic condition for equilibrium, the Gibbs energy must be stationary, meaning that the derivative of \"G\" with respect to the extent of reaction, \"\u03be\", must be zero. It can be shown that in this case, the sum of chemical potentials times the stoichiometric coefficients of the products is equal to the sum of those corresponding to the reactants. Therefore, the sum of the Gibbs energies of the reactants must be the equal to the sum of the Gibbs energies of the products.\nwhere \"\u03bc\" is in this case a partial molar Gibbs energy, a chemical potential. The chemical potential of a reagent A is a function of the activity, {A} of that reagent.\n(where \"\u03bc\" is the standard chemical potential).\nThe definition of the Gibbs energy equation interacts with the fundamental thermodynamic relation to produce\nInserting \"dNi\"\u00a0= \"\u03bdi\u00a0d\u03be\" into the above equation gives a stoichiometric coefficient (formula_11) and a differential that denotes the reaction occurring to an infinitesimal extent (\"d\u03be\"). At constant pressure and temperature the above equations can be written as\nwhich is the Gibbs free energy change for the reaction. This results in:\nBy substituting the chemical potentials:\nthe relationship becomes:\nwhich is the standard Gibbs energy change for the reaction that can be calculated using thermodynamical tables.\nThe reaction quotient is defined as:\nTherefore,\nAt equilibrium:\nleading to:\nand\nObtaining the value of the standard Gibbs energy change, allows the calculation of the equilibrium constant.\nAddition of reactants or products.\nFor a reactional system at equilibrium: \"Q\"r\u00a0=\u00a0\"K\"eq; \"\u03be\"\u00a0=\u00a0\"\u03be\"eq.\nNote that activities and equilibrium constants are dimensionless numbers.\nTreatment of activity.\nThe expression for the equilibrium constant can be rewritten as the product of a concentration quotient, \"K\"c and an activity coefficient quotient, \"\u0393\".\n[A] is the concentration of reagent A, etc. It is possible in principle to obtain values of the activity coefficients, \u03b3. For solutions, equations such as the Debye\u2013H\u00fcckel equation or extensions such as Davies equation Specific ion interaction theory or Pitzer equations may be used. However this is not always possible. It is common practice to assume that \"\u0393\" is a constant, and to use the concentration quotient in place of the thermodynamic equilibrium constant. It is also general practice to use the term \"equilibrium constant\" instead of the more accurate \"concentration quotient\". This practice will be followed here.\nFor reactions in the gas phase partial pressure is used in place of concentration and fugacity coefficient in place of activity coefficient. In the real world, for example, when making ammonia in industry, fugacity coefficients must be taken into account. Fugacity, \"f\", is the product of partial pressure and fugacity coefficient. The chemical potential of a species in the real gas phase is given by\nso the general expression defining an equilibrium constant is valid for both solution and gas phases.\nConcentration quotients.\nIn aqueous solution, equilibrium constants are usually determined in the presence of an \"inert\" electrolyte such as sodium nitrate, NaNO3, or potassium perchlorate, KClO4. The ionic strength of a solution is given by\nwhere \"ci\" and \"zi\" stand for the concentration and ionic charge of ion type \"i\", and the sum is taken over all the \"N\" types of charged species in solution. When the concentration of dissolved salt is much higher than the analytical concentrations of the reagents, the ions originating from the dissolved salt determine the ionic strength, and the ionic strength is effectively constant. Since activity coefficients depend on ionic strength, the activity coefficients of the species are effectively independent of concentration. Thus, the assumption that \"\u0393\" is constant is justified. The concentration quotient is a simple multiple of the equilibrium constant.\nHowever, \"K\"c will vary with ionic strength. If it is measured at a series of different ionic strengths, the value can be extrapolated to zero ionic strength. The concentration quotient obtained in this manner is known, paradoxically, as a thermodynamic equilibrium constant.\nBefore using a published value of an equilibrium constant in conditions of ionic strength different from the conditions used in its determination, the value should be adjusted.\nMetastable mixtures.\nA mixture may appear to have no tendency to change, though it is not at equilibrium. For example, a mixture of SO2 and O2 is metastable as there is a kinetic barrier to formation of the product, SO3.\nThe barrier can be overcome when a catalyst is also present in the mixture as in the contact process, but the catalyst does not affect the equilibrium concentrations.\nLikewise, the formation of bicarbonate from carbon dioxide and water is very slow under normal conditions\nbut almost instantaneous in the presence of the catalytic enzyme carbonic anhydrase.\nPure substances.\nWhen pure substances (liquids or solids) are involved in equilibria their activities do not appear in the equilibrium constant because their numerical values are considered one.\nApplying the general formula for an equilibrium constant to the specific case of a dilute solution of acetic acid in water one obtains\nFor all but very concentrated solutions, the water can be considered a \"pure\" liquid, and therefore it has an activity of one. The equilibrium constant expression is therefore usually written as\nA particular case is the self-ionization of water\nBecause water is the solvent, and has an activity of one, the self-ionization constant of water is defined as\nIt is perfectly legitimate to write [H+] for the hydronium ion concentration, since the state of solvation of the proton is constant (in dilute solutions) and so does not affect the equilibrium concentrations. \"K\"w varies with variation in ionic strength and/or temperature.\nThe concentrations of H+ and OH\u2212 are not independent quantities. Most commonly [OH\u2212] is replaced by \"K\"w[H+]\u22121 in equilibrium constant expressions which would otherwise include hydroxide ion.\nSolids also do not appear in the equilibrium constant expression, if they are considered to be pure and thus their activities taken to be one. An example is the Boudouard reaction:\nfor which the equation (without solid carbon) is written as:\nMultiple equilibria.\nConsider the case of a dibasic acid H2A. When dissolved in water, the mixture will contain H2A, HA\u2212 and A2\u2212. This equilibrium can be split into two steps in each of which one proton is liberated.\n\"K\"1 and\" K\"2 are examples of \"stepwise\" equilibrium constants. The \"overall\" equilibrium constant, \"\u03b2\"D, is product of the stepwise constants.\nNote that these constants are dissociation constants because the products on the right hand side of the equilibrium expression are dissociation products. In many systems, it is preferable to use association constants.\n\"\u03b2\"1 and \"\u03b2\"2 are examples of association constants. Clearly and ; and \nFor multiple equilibrium systems, also see: theory of Response reactions.\nEffect of temperature.\nThe effect of changing temperature on an equilibrium constant is given by the van 't Hoff equation\nThus, for exothermic reactions (\u0394\"H\" is negative), \"K\" decreases with an increase in temperature, but, for endothermic reactions, (\u0394H is positive) \"K\" increases with an increase temperature. An alternative formulation is\nAt first sight this appears to offer a means of obtaining the standard molar enthalpy of the reaction by studying the variation of \"K\" with temperature. In practice, however, the method is unreliable because error propagation almost always gives very large errors on the values calculated in this way.\nEffect of electric and magnetic fields.\nThe effect of electric field on equilibrium has been studied by Manfred Eigen among others.\nTypes of equilibrium.\nEquilibrium can be broadly classified as heterogeneous and homogeneous equilibrium. Homogeneous equilibrium consists of reactants and products belonging in the same phase whereas heterogeneous equilibrium comes into play for reactants and products in different phases. \nIn these applications, terms such as stability constant, formation constant, binding constant, affinity constant, association constant and dissociation constant are used. In biochemistry, it is common to give units for binding constants, which serve to define the concentration units used when the constant's value was determined.\nComposition of a mixture.\nWhen the only equilibrium is that of the formation of a 1:1 adduct as the composition of a mixture, there are many ways that the composition of a mixture can be calculated. For example, see ICE table for a traditional method of calculating the pH of a solution of a weak acid.\nThere are three approaches to the general calculation of the composition of a mixture at equilibrium.\nMass-balance equations.\nIn general, the calculations are rather complicated or complex. For instance, in the case of a dibasic acid, H2A dissolved in water the two reactants can be specified as the conjugate base, A2\u2212, and the proton, H+. The following equations of mass-balance could apply equally well to a base such as 1,2-diaminoethane, in which case the base itself is designated as the reactant A:\nwith TA the total concentration of species A. Note that it is customary to omit the ionic charges when writing and using these equations.\nWhen the equilibrium constants are known and the total concentrations are specified there are two equations in two unknown \"free concentrations\" [A] and [H]. This follows from the fact that [HA]\u00a0=\u00a0\"\u03b2\"1[A][H], [H2A]\u00a0=\u00a0\"\u03b2\"2[A][H]2 and [OH]\u00a0=\u00a0\"K\"w[H]\u22121\nso the concentrations of the \"complexes\" are calculated from the free concentrations and the equilibrium constants.\nGeneral expressions applicable to all systems with two reagents, A and B would be\nIt is easy to see how this can be extended to three or more reagents.\nPolybasic acids.\nThe composition of solutions containing reactants A and H is easy to calculate as a function of p[H]. When [H] is known, the free concentration [A] is calculated from the mass-balance equation in A.\nThe diagram alongside, shows an example of the hydrolysis of the aluminium Lewis acid Al3+(aq) shows the species concentrations for a 5\u00a0\u00d7\u00a010\u22126\u00a0M solution of an aluminium salt as a function of pH. Each concentration is shown as a percentage of the total aluminium.\nSolution and precipitation.\nThe diagram above illustrates the point that a precipitate that is not one of the main species in the solution equilibrium may be formed. At pH just below 5.5 the main species present in a 5\u00a0\u03bcM solution of Al3+ are aluminium hydroxides Al(OH)2+, and , but on raising the pH Al(OH)3 precipitates from the solution. This occurs because Al(OH)3 has a very large lattice energy. As the pH rises more and more Al(OH)3 comes out of solution. This is an example of Le Ch\u00e2telier's principle in action: Increasing the concentration of the hydroxide ion causes more aluminium hydroxide to precipitate, which removes hydroxide from the solution. When the hydroxide concentration becomes sufficiently high the soluble aluminate, , is formed.\nAnother common instance where precipitation occurs is when a metal cation interacts with an anionic ligand to form an electrically neutral complex. If the complex is hydrophobic, it will precipitate out of water. This occurs with the nickel ion Ni2+ and dimethylglyoxime, (dmgH2): in this case the lattice energy of the solid is not particularly large, but it greatly exceeds the energy of solvation of the molecule Ni(dmgH)2.\nMinimization of Gibbs energy.\nAt equilibrium, at a specified temperature and pressure, and with no external forces, the Gibbs free energy \"G\" is at a minimum:\nwhere \u03bcj is the chemical potential of molecular species \"j\", and \"Nj\" is the amount of molecular species \"j\". It may be expressed in terms of thermodynamic activity as:\nwhere formula_51 is the chemical potential in the standard state, \"R\" is the gas constant \"T\" is the absolute temperature, and \"Aj\" is the activity.\nFor a closed system, no particles may enter or leave, although they may combine in various ways. The total number of atoms of each element will remain constant. This means that the minimization above must be subjected to the constraints:\nwhere \"aij\" is the number of atoms of element \"i\" in molecule \"j\" and \"b\" is the total number of atoms of element \"i\", which is a constant, since the system is closed. If there are a total of \"k\" types of atoms in the system, then there will be \"k\" such equations. If ions are involved, an additional row is added to the aij matrix specifying the respective charge on each molecule which will sum to zero.\nThis is a standard problem in optimisation, known as constrained minimisation. The most common method of solving it is using the method of Lagrange multipliers (although other methods may be used).\nDefine:\nwhere the \"\u03bbi\" are the Lagrange multipliers, one for each element. This allows each of the \"Nj\" and \"\u03bbj\" to be treated independently, and it can be shown using the tools of multivariate calculus that the equilibrium condition is given by\n(For proof see Lagrange multipliers.) This is a set of (\"m\"\u00a0+\u00a0\"k\") equations in (\"m\"\u00a0+\u00a0\"k\") unknowns (the \"Nj\" and the \"\u03bbi\") and may, therefore, be solved for the equilibrium concentrations \"Nj\" as long as the chemical activities are known as functions of the concentrations at the given temperature and pressure. (In the ideal case, activities are proportional to concentrations.) (See Thermodynamic databases for pure substances.) Note that the second equation is just the initial constraints for minimization.\nThis method of calculating equilibrium chemical concentrations is useful for systems with a large number of different molecules. The use of \"k\" atomic element conservation equations for the mass constraint is straightforward, and replaces the use of the stoichiometric coefficient equations. The results are consistent with those specified by chemical equations. For example, if equilibrium is specified by a single chemical equation:,\nwhere \u03bdj is the stoichiometric coefficient for the \"j\" th molecule (negative for reactants, positive for products) and \"Rj\" is the symbol for the \"j\" th molecule, a properly balanced equation will obey:\nMultiplying the first equilibrium condition by \u03bdj and using the above equation yields:\nAs above, defining \u0394G\nwhere \"Kc\" is the equilibrium constant, and \u0394G will be zero at equilibrium.\nAnalogous procedures exist for the minimization of other thermodynamic potentials."}
{"id": "5308", "revid": "13974845", "url": "https://en.wikipedia.org/wiki?curid=5308", "title": "Combination", "text": "In mathematics, a combination is a selection of items from a set that has distinct members, such that the order of selection does not matter (unlike permutations). For example, given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange. More formally, a \"k\"-combination of a set \"S\" is a subset of \"k\" distinct elements of \"S\". So, two combinations are identical if and only if each combination has the same members. (The arrangement of the members in each set does not matter.) If the set has \"n\" elements, the number of \"k\"-combinations, denoted by formula_1 or formula_2, is equal to the binomial coefficient\nformula_3\nwhich can be written using factorials as formula_4 whenever formula_5, and which is zero when formula_6. This formula can be derived from the fact that each \"k\"-combination of a set \"S\" of \"n\" members has formula_7 permutations so formula_8 or formula_9. The set of all \"k\"-combinations of a set \"S\" is often denoted by formula_10.\nA combination is a selection of \"n\" things taken \"k\" at a time \"without repetition\". To refer to combinations in which repetition is allowed, the terms \"k\"-combination with repetition, \"k\"-multiset, or \"k\"-selection, are often used. If, in the above example, it were possible to have two of any one kind of fruit there would be 3 more 2-selections: one with two apples, one with two oranges, and one with two pears.\nAlthough the set of three fruits was small enough to write a complete list of combinations, this becomes impractical as the size of the set increases. For example, a poker hand can be described as a 5-combination (\"k\"\u00a0=\u00a05) of cards from a 52 card deck (\"n\"\u00a0=\u00a052). The 5 cards of the hand are all distinct, and the order of cards in the hand does not matter. There are 2,598,960 such combinations, and the chance of drawing any one hand at random is\u00a01\u00a0/\u00a02,598,960.\nNumber of \"k\"-combinations.\nThe number of \"k\"-combinations from a given set \"S\" of \"n\" elements is often denoted in elementary combinatorics texts by formula_1, or by a variation such as formula_2, formula_13, formula_14, formula_15 or even formula_16 (the last form is standard in French, Romanian, Russian, and Chinese texts). The same number however occurs in many other mathematical contexts, where it is denoted by formula_17 (often read as \"\"n\" choose \"k\"\"); notably it occurs as a coefficient in the binomial formula, hence its name binomial coefficient. One can define formula_17 for all natural numbers \"k\" at once by the relation\nformula_19\nfrom which it is clear that\nformula_20\nand further\nformula_21\nfor formula_6.\nTo see that these coefficients count \"k\"-combinations from \"S\", one can first consider a collection of \"n\" distinct variables \"X\"\"s\" labeled by the elements \"s\" of \"S\", and expand the product over all elements of\u00a0\"S\":\nformula_23\nit has 2\"n\" distinct terms corresponding to all the subsets of \"S\", each subset giving the product of the corresponding variables \"X\"\"s\". Now setting all of the \"X\"\"s\" equal to the unlabeled variable \"X\", so that the product becomes , the term for each \"k\"-combination from \"S\" becomes \"X\"\"k\", so that the coefficient of that power in the result equals the number of such \"k\"-combinations.\nBinomial coefficients can be computed explicitly in various ways. To get all of them for the expansions up to , one can use (in addition to the basic cases already given) the recursion relation\nformula_24\nfor 0 &lt; \"k\" &lt; \"n\", which follows from =; this leads to the construction of Pascal's triangle.\nFor determining an individual binomial coefficient, it is more practical to use the formula\nformula_25\nThe numerator gives the number of \"k\"-permutations of \"n\", i.e., of sequences of \"k\" distinct elements of \"S\", while the denominator gives the number of such \"k\"-permutations that give the same \"k\"-combination when the order is ignored.\nWhen \"k\" exceeds \"n\"/2, the above formula contains factors common to the numerator and the denominator, and canceling them out gives the relation\nformula_26\nfor 0 \u2264 \"k\" \u2264 \"n\". This expresses a symmetry that is evident from the binomial formula, and can also be understood in terms of \"k\"-combinations by taking the complement of such a combination, which is an -combination.\nFinally there is a formula which exhibits this symmetry directly, and has the merit of being easy to remember:\nformula_27\nwhere \"n\"! denotes the factorial of \"n\". It is obtained from the previous formula by multiplying denominator and numerator by !, so it is certainly computationally less efficient than that formula.\nThe last formula can be understood directly, by considering the \"n\"! permutations of all the elements of \"S\". Each such permutation gives a \"k\"-combination by selecting its first \"k\" elements. There are many duplicate selections: any combined permutation of the first \"k\" elements among each other, and of the final (\"n\"\u00a0\u2212\u00a0\"k\") elements among each other produces the same combination; this explains the division in the formula.\nFrom the above formulas follow relations between adjacent numbers in Pascal's triangle in all three directions:\nformula_28\nTogether with the basic cases formula_29, these allow successive computation of respectively all numbers of combinations from the same set (a row in Pascal's triangle), of \"k\"-combinations of sets of growing sizes, and of combinations with a complement of fixed size .\nExample of counting combinations.\nAs a specific example, one can compute the number of five-card hands possible from a standard fifty-two card deck as:\nformula_30\nAlternatively one may use the formula in terms of factorials and cancel the factors in the numerator against parts of the factors in the denominator, after which only multiplication of the remaining factors is required:\nformula_31\nAnother alternative computation, equivalent to the first, is based on writing\nformula_32\nwhich gives\nformula_33\nWhen evaluated in the following order, , this can be computed using only integer arithmetic. The reason is that when each division occurs, the intermediate result that is produced is itself a binomial coefficient, so no remainders ever occur.\nUsing the symmetric formula in terms of factorials without performing simplifications gives a rather extensive calculation:\nformula_34\nEnumerating \"k\"-combinations.\nOne can enumerate all \"k\"-combinations of a given set \"S\" of \"n\" elements in some fixed order, which establishes a bijection from an interval of formula_17 integers with the set of those \"k\"-combinations. Assuming \"S\" is itself ordered, for instance \"S\" = { 1, 2, ..., \"n\" }, there are two natural possibilities for ordering its \"k\"-combinations: by comparing their smallest elements first (as in the illustrations above) or by comparing their largest elements first. The latter option has the advantage that adding a new largest element to \"S\" will not change the initial part of the enumeration, but just add the new \"k\"-combinations of the larger set after the previous ones. Repeating this process, the enumeration can be extended indefinitely with \"k\"-combinations of ever larger sets. If moreover the intervals of the integers are taken to start at\u00a00, then the \"k\"-combination at a given place \"i\" in the enumeration can be computed easily from \"i\", and the bijection so obtained is known as the combinatorial number system. It is also known as \"rank\"/\"ranking\" and \"unranking\" in computational mathematics.\nThere are many ways to enumerate \"k\" combinations. One way is to track \"k\" index numbers of the elements selected, starting with {0 .. \"k\"\u22121} (zero-based) or {1 .. \"k\"} (one-based) as the first allowed \"k\"-combination. Then, repeatedly move to the next allowed \"k\"-combination by incrementing the smallest index number for which this would not create two equal index numbers, at the same time resetting all smaller index numbers to their initial values.\nNumber of combinations with repetition.\nA \"k\"-combination with repetitions, or \"k\"-multicombination, or multisubset of size \"k\" from a set \"S\" of size \"n\" is given by a set of \"k\" not necessarily distinct elements of \"S\", where order is not taken into account: two sequences define the same multiset if one can be obtained from the other by permuting the terms. In other words, it is a sample of \"k\" elements from a set of \"n\" elements allowing for duplicates (i.e., with replacement) but disregarding different orderings (e.g. {2,1,2} = {1,2,2}). Associate an index to each element of \"S\" and think of the elements of \"S\" as \"types\" of objects, then we can let formula_36 denote the number of elements of type \"i\" in a multisubset. The number of multisubsets of size \"k\" is then the number of nonnegative integer (so allowing zero) solutions of the Diophantine equation:\nformula_37\nIf \"S\" has \"n\" elements, the number of such \"k\"-multisubsets is denoted by\nformula_38\na notation that is analogous to the binomial coefficient which counts \"k\"-subsets. This expression, \"n\" multichoose \"k\", can also be given in terms of binomial coefficients:\nformula_39\nThis relationship can be easily proved using a representation known as stars and bars. \nA solution of the above Diophantine equation can be represented by formula_40 \"stars\", a separator (a \"bar\"), then formula_41 more stars, another separator, and so on. The total number of stars in this representation is \"k\" and the number of bars is \"n\" - 1 (since a separation into n parts needs n-1 separators). Thus, a string of \"k\" + \"n\" - 1 (or \"n\" + \"k\" - 1) symbols (stars and bars) corresponds to a solution if there are \"k\" stars in the string. Any solution can be represented by choosing \"k\" out of positions to place stars and filling the remaining positions with bars. For example, the solution formula_42 of the equation formula_43 (\"n\" = 4 and \"k\" = 10) can be represented by\nformula_44\nThe number of such strings is the number of ways to place 10 stars in 13 positions, formula_45 which is the number of 10-multisubsets of a set with 4 elements.\nAs with binomial coefficients, there are several relationships between these multichoose expressions. For example, for formula_46,\nformula_47\nThis identity follows from interchanging the stars and bars in the above representation.\nExample of counting multisubsets.\nFor example, if you have four types of donuts (\"n\"\u00a0=\u00a04) on a menu to choose from and you want three donuts (\"k\"\u00a0=\u00a03), the number of ways to choose the donuts with repetition can be calculated as\nformula_48\nThis result can be verified by listing all the 3-multisubsets of the set \"S\" = {1,2,3,4}. This is displayed in the following table. The second column lists the donuts you actually chose, the third column shows the nonnegative integer solutions formula_49 of the equation formula_50 and the last column gives the stars and bars representation of the solutions.\nNumber of \"k\"-combinations for all \"k\".\nThe number of \"k\"-combinations for all \"k\" is the number of subsets of a set of \"n\" elements. There are several ways to see that this number is 2\"n\". In terms of combinations, formula_51, which is the sum of the \"n\"th row (counting from 0) of the binomial coefficients in Pascal's triangle. These combinations (subsets) are enumerated by the 1 digits of the set of base 2 numbers counting from 0 to 2\"n\"\u00a0\u2212\u00a01, where each digit position is an item from the set of \"n\".\nGiven 3 cards numbered 1 to 3, there are 8 distinct combinations (subsets), including the empty set:\nformula_52\nRepresenting these subsets (in the same order) as base 2 numerals:\nProbability: sampling a random combination.\nThere are various algorithms to pick out a random combination from a given set or list. Rejection sampling is extremely slow for large sample sizes. One way to select a \"k\"-combination efficiently from a population of size \"n\" is to iterate across each element of the population, and at each step pick that element with a dynamically changing probability of formula_53 (see Reservoir sampling). Another is to pick a random non-negative integer less than formula_54 and convert it into a combination using the combinatorial number system.\nNumber of ways to put objects into bins.\nA combination can also be thought of as a selection of \"two\" sets of items: those that go into the chosen bin and those that go into the unchosen bin. This can be generalized to any number of bins with the constraint that every item must go to exactly one bin. The number of ways to put objects into bins is given by the multinomial coefficient\nformula_55\nwhere \"n\" is the number of items, \"m\" is the number of bins, and formula_56 is the number of items that go into bin \"i\".\nOne way to see why this equation holds is to first number the objects arbitrarily from \"1\" to \"n\" and put the objects with numbers formula_57 into the first bin in order, the objects with numbers formula_58 into the second bin in order, and so on. There are formula_59 distinct numberings, but many of them are equivalent, because only the set of items in a bin matters, not their order in it. Every combined permutation of each bins' contents produces an equivalent way of putting items into bins. As a result, every equivalence class consists of formula_60 distinct numberings, and the number of equivalence classes is formula_61.\nThe binomial coefficient is the special case where \"k\" items go into the chosen bin and the remaining formula_62 items go into the unchosen bin:\nformula_63"}
{"id": "5309", "revid": "8810108", "url": "https://en.wikipedia.org/wiki?curid=5309", "title": "Software", "text": "Software consists of computer programs that instruct the execution of a computer. Software also includes design documents and specifications.\nThe history of software is closely tied to the development of digital computers in the mid-20th century. Early programs were written in the machine language specific to the hardware. The introduction of high-level programming languages in 1958 allowed for more human-readable instructions, making software development easier and more portable across different computer architectures. Software in a programming language is run through a compiler or interpreter to execute on the architecture's hardware. Over time, software has become complex, owing to developments in networking, operating systems, and databases.\nSoftware can generally be categorized into two main types:\nThe rise of cloud computing has introduced the new software delivery model Software as a Service (SaaS). In SaaS, applications are hosted by a provider and accessed over the Internet.\nThe process of developing software involves several stages. The stages include software design, programming, testing, release, and maintenance. Software quality assurance and security are critical aspects of software development, as bugs and security vulnerabilities can lead to system failures and security breaches. Additionally, legal issues such as software licenses and intellectual property rights play a significant role in the distribution of software products.\nHistory.\nThe first use of the word \"software\" is credited to mathematician John Wilder Tukey in 1958.\nThe first programmable computers, which appeared at the end of the 1940s, were programmed in machine language. Machine language is difficult to debug and not portable across different computers. Initially, hardware resources were more expensive than human resources. As programs became complex, programmer productivity became the bottleneck. The introduction of high-level programming languages in 1958 hid the details of the hardware and expressed the underlying algorithms into the code . Early languages include Fortran, Lisp, and COBOL.\nTypes.\nThere are two main types of software:\nSoftware can also be categorized by how it is deployed. Traditional applications are purchased with a perpetual license for a specific version of the software, downloaded, and run on hardware belonging to the purchaser. The rise of the Internet and cloud computing enabled a new model, software as a service (SaaS), in which the provider hosts the software (usually built on top of rented infrastructure or platforms) and provides the use of the software to customers, often in exchange for a subscription fee. By 2023, SaaS products\u2014which are usually delivered via a web application\u2014had become the primary method that companies deliver applications.\nSoftware development and maintenance.\nSoftware companies aim to deliver a high-quality product on time and under budget. A challenge is that software development effort estimation is often inaccurate. Software development begins by conceiving the project, evaluating its feasibility, analyzing the business requirements, and making a software design. Most software projects speed up their development by reusing or incorporating existing software, either in the form of commercial off-the-shelf (COTS) or open-source software. Software quality assurance is typically a combination of manual code review by other engineers and automated software testing. Due to time constraints, testing cannot cover all aspects of the software's intended functionality, so developers often focus on the most critical functionality. Formal methods are used in some safety-critical systems to prove the correctness of code, while user acceptance testing helps to ensure that the product meets customer expectations. There are a variety of software development methodologies, which vary from completing all steps in order to concurrent and iterative models. Software development is driven by requirements taken from prospective users, as opposed to maintenance, which is driven by events such as a change request.\nFrequently, software is released in an incomplete state when the development team runs out of time or funding. Despite testing and quality assurance, virtually all software contains bugs where the system does not work as intended. Post-release software maintenance is necessary to remediate these bugs when they are found and keep the software working as the environment changes over time. New features are often added after the release. Over time, the level of maintenance becomes increasingly restricted before being cut off entirely when the product is withdrawn from the market. As software ages, it becomes known as legacy software and can remain in use for decades, even if there is no one left who knows how to fix it. Over the lifetime of the product, software maintenance is estimated to comprise 75 percent or more of the total development cost.\nCompleting a software project involves various forms of expertise, not just in software programmers but also testing, documentation writing, project management, graphic design, user experience, user support, marketing, and fundraising.\nQuality and security.\nSoftware quality is defined as meeting the stated requirements as well as customer expectations. Quality is an overarching term that can refer to a code's correct and efficient behavior, its reusability and portability, or the ease of modification. It is usually more cost-effective to build quality into the product from the beginning rather than try to add it later in the development process. Higher quality code will reduce lifetime cost to both suppliers and customers as it is more reliable and easier to maintain. Software failures in safety-critical systems can be very serious including death. By some estimates, the cost of poor quality software can be as high as 20 to 40 percent of sales. Despite developers' goal of delivering a product that works entirely as intended, virtually all software contains bugs.\nThe rise of the Internet also greatly increased the need for computer security as it enabled malicious actors to conduct cyberattacks remotely. If a bug creates a security risk, it is called a vulnerability. Software patches are often released to fix identified vulnerabilities, but those that remain unknown (zero days) as well as those that have not been patched are still liable for exploitation. Vulnerabilities vary in their ability to be exploited by malicious actors, and the actual risk is dependent on the nature of the vulnerability as well as the value of the surrounding system. Although some vulnerabilities can only be used for denial of service attacks that compromise a system's availability, others allow the attacker to inject and run their own code (called malware), without the user being aware of it. To thwart cyberattacks, all software in the system must be designed to withstand and recover from external attack. Despite efforts to ensure security, a significant fraction of computers are infected with malware.\nEncoding and execution.\nProgramming languages.\nProgramming languages are the format in which software is written. Since the 1950s, thousands of different programming languages have been invented; some have been in use for decades, while others have fallen into disuse. Some definitions classify machine code\u2014the exact instructions directly implemented by the hardware\u2014and assembly language\u2014a more human-readable alternative to machine code whose statements can be translated one-to-one into machine code\u2014as programming languages. Programs written in the high-level programming languages used to create software share a few main characteristics: knowledge of machine code is not necessary to write them, they can be ported to other computer systems, and they are more concise and human-readable than machine code. They must be both human-readable and capable of being translated into unambiguous instructions for computer hardware.\nCompilation, interpretation, and execution.\nThe invention of high-level programming languages was simultaneous with the compilers needed to translate them automatically into machine code. Most programs do not contain all the resources needed to run them and rely on external libraries. Part of the compiler's function is to link these files in such a way that the program can be executed by the hardware. Once compiled, the program can be saved as an object file and the loader (part of the operating system) can take this saved file and execute it as a process on the computer hardware. Some programming languages use an interpreter instead of a compiler. An interpreter converts the program into machine code at run time, which makes them 10 to 100 times slower than compiled programming languages.\nLegal issues.\nLiability.\nSoftware is often released with the knowledge that it is incomplete or contains bugs. Purchasers knowingly buy it in this state, which has led to a legal regime where liability for software products is significantly curtailed compared to other products.\nLicenses.\nSource code is protected by copyright law that vests the owner with the exclusive right to copy the code. The underlying ideas or algorithms are not protected by copyright law, but are often treated as a trade secret and concealed by such methods as non-disclosure agreements. Software copyright has been recognized since the mid-1970s and is vested in the company that makes the software, not the employees or contractors who wrote it. The use of most software is governed by an agreement (software license) between the copyright holder and the user. Proprietary software is usually sold under a restrictive license that limits copying and reuse (often enforced with tools such as digital rights management (DRM)). Open-source licenses, in contrast, allow free use and redistribution of software with few conditions. Most open-source licenses used for software require that modifications be released under the same license, which can create complications when open-source software is reused in proprietary projects.\nPatents.\nPatents give an inventor an exclusive, time-limited license for a novel product or process. Ideas about what software could accomplish are not protected by law and concrete implementations are instead covered by copyright law. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid. Software patents have been historically controversial. Before the 1998 case \"State Street Bank &amp; Trust Co. v. Signature Financial Group, Inc.\", software patents were generally not recognized in the United States. In that case, the Supreme Court decided that business processes could be patented. Patent applications are complex and costly, and lawsuits involving patents can drive up the cost of products. Unlike copyrights, patents generally only apply in the jurisdiction where they were issued.\nImpact.\nEngineer Capers Jones writes that \"computers and software are making profound changes to every aspect of human life: education, work, warfare, entertainment, medicine, law, and everything else\". It has become ubiquitous in everyday life in developed countries. In many cases, software augments the functionality of existing technologies such as household appliances and elevators. Software also spawned entirely new technologies such as the Internet, video games, mobile phones, and GPS. New methods of communication, including email, forums, blogs, microblogging, wikis, and social media, were enabled by the Internet. Massive amounts of knowledge exceeding any paper-based library are now available with a quick web search. Most creative professionals have switched to software-based tools such as computer-aided design, 3D modeling, digital image editing, and computer animation. Almost every complex device is controlled by software."}
{"id": "5310", "revid": "17300358", "url": "https://en.wikipedia.org/wiki?curid=5310", "title": "Personal computer hardware", "text": ""}
{"id": "5311", "revid": "6908984", "url": "https://en.wikipedia.org/wiki?curid=5311", "title": "Computer programming", "text": "Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\nAuxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term \"software development\" is used for this larger overall process \u2013 with the terms \"programming\", \"implementation\", and \"coding\" reserved for the writing and editing of code per se. Sometimes software development is known as \"software engineering\", especially when it employs formal methods or follows an engineering design process.\nHistory.\nProgrammable devices have existed for centuries. As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the \"Book of Ingenious Devices\". In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where a musical mechanical automaton could be made to play different rhythms and drum patterns, via pegs and cams. In 1801, the Jacquard loom could produce entirely different weaves by changing the \"program\" \u2013 a series of pasteboard cards with holes punched in them.\nCode-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in \"A Manuscript on Deciphering Cryptographic Messages\". He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.\nThe first computer program is generally dated to 1843 when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine. However, Charles Babbage himself had written a program for the AE in 1837.\nIn the 1880s, Herman Hollerith invented the concept of storing \"data\" in machine-readable form. Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers. However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.\nMachine language.\nMachine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instructions in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language, two machines with different instruction sets also have different assembly languages.\nCompiler languages.\nHigh-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware. \nThe first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'. FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developed\u2014in particular, COBOL aimed at commercial data processing, and Lisp for computer research.\nThese compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.\nSource code entry.\nPrograms were mostly entered using punched cards or paper tape. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.\nModern programming.\nQuality requirements.\nWhatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:\nUsing automated tests and fitness functions can help to maintain some of the aforementioned attributes.\nReadability of source code.\nIn computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\nReadability is important because programmers spend the majority of their time reading, trying to understand, reusing, and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.\nFollowing a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:\nThe presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.\nVarious visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.\nAlgorithmic complexity.\nThe academic field and the engineering practice of computer programming are concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into \"orders\" using Big O notation, which expresses resource use\u2014such as execution time or memory consumption\u2014in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.\nMethodologies.\nThe first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.\nPopular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.\nA similar technique used for database design is Entity-Relationship Modeling (ER Modeling).\nImplementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic programming languages.\nMeasuring language usage.\nIt is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).\nSome languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).\nDebugging.\nDebugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash. Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if the remaining actions are sufficient for bugs to appear. Scripting and breakpointing are also part of this process.\nDebugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.\nProgramming languages.\nDifferent programming languages support different styles of programming (called \"programming paradigms\"). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from \"low-level\" to \"high-level\"; \"low-level\" languages are typically more machine-oriented and faster to execute, whereas \"high-level\" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in \"high-level\" languages than in \"low-level\" ones.\nProgramming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.\nAllen Downey, in his book \"How To Think Like A Computer Scientist\", writes:\nMany computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.\nLearning to program.\nLearning to program has a long history related to professional standards and practices, academic initiatives and curriculum, and commercial books and materials for students, self-taught learners, hobbyists, and others who desire to create or customize software for personal use. Since the 1960s, learning to program has taken on the characteristics of a \"popular movement\", with the rise of academic disciplines, inspirational leaders, collective identities, and strategies to grow the movement and make institutionalize change. Through these social ideals and educational agendas, learning to code has become important not just for scientists and engineers, but for millions of citizens who have come to believe that creating software is beneficial to society and its members.\nContext.\nIn 1957, there were approximately 15,000 computer programmers employed in the U.S., a figure that accounts for 80% of the world's active developers. In 2014, there were approximately 18.5 million professional programmers in the world, of which 11 million can be considered professional and 7.5 million student or hobbyists. Before the rise of the commercial Internet in the mid-1990s, most programmers learned about software construction through books, magazines, user groups, and informal instruction methods, with academic coursework and corporate training playing important roles for professional workers.\nThe first book containing specific instructions about how to program a computer may have been Maurice Wilkes, David Wheeler, and Stanley Gill's \"Preparation of Programs for an Electronic Digital Computer\" (1951). The book offered a selection of common subroutines for handling basic operations on the EDSAC, one of the world's first stored-program computers.\nWhen high-level languages arrived, they were introduced by numerous books and materials that explained language keywords, managing program flow, working with data, and other concepts. These languages included FLOW-MATIC, COBOL, FORTRAN, ALGOL, Pascal, BASIC, and C. An example of an early programming primer from these years is Marshal H. Wrubel's \"A Primer of Programming for Digital Computers\" (1959), which included step-by-step instructions for filling out coding sheets, creating punched cards, and using the keywords in IBM's early FORTRAN system. Daniel McCracken's \"A Guide to FORTRAN Programming\" (1961) presented FORTRAN to a larger audience, including students and office workers.\nIn 1961, Alan Perlis suggested that all university freshmen at Carnegie Technical Institute take a course in computer programming. His advice was published in the popular technical journal \"Computers and Automation\", which became a regular source of information for professional programmers.\nProgrammers soon had a range of learning texts at their disposal. \"Programmer's references\" listed keywords and functions related to a language, often in alphabetical order, as well as technical information about compilers and related systems. An early example was IBM's \"Programmers' Reference Manual: the FORTRAN Automatic Coding System for the IBM 704 EDPM\" (1956).\nOver time, the genre of \"programmer's guides\" emerged, which presented the features of a language in tutorial or step by step format. Many early primers started with a program known as \u201cHello, World\u201d, which presented the shortest program a developer could create in a given system. Programmer's guides then went on to discuss core topics like declaring variables, data types, formulas, flow control, user-defined functions, manipulating data, and other topics.\nEarly and influential programmer's guides included John G. Kemeny and Thomas E. Kurtz's \"BASIC Programming\" (1967), Kathleen Jensen and Niklaus Wirth's \"The Pascal User Manual and Report\" (1971), and Brian Kernighan and Dennis Ritchie's \"The C Programming Language\" (1978). Similar books for popular audiences (but with a much lighter tone) included Bob Albrecht's \"My Computer Loves Me When I Speak BASIC\" (1972), Al Kelley and Ira Pohl's \"A Book on C\" (1984), and Dan Gookin's \"C for Dummies\" (1994).\nBeyond language-specific primers, there were numerous books and academic journals that introduced professional programming practices. Many were designed for university courses in computer science, software engineering, or related disciplines. Donald Knuth's \"The Art of Computer Programming\" (1968 and later), presented hundreds of computational algorithms and their analysis. \"The Elements of Programming Style\" (1974), by Brian W. Kernighan and P. J. Plauger, concerned itself with programming \"style\", the idea that programs should be written not only to satisfy the compiler but human readers. Jon Bentley's \"Programming Pearls\" (1986) offered practical advice about the art and craft of programming in professional and academic contexts. Texts specifically designed for students included Doug Cooper and Michael Clancy's \"Oh Pascal!\" (1982), Alfred Aho's \"Data Structures and Algorithms\" (1983), and Daniel Watt's \"Learning with Logo\" (1983).\nTechnical publishers.\nAs personal computers became mass-market products, thousands of trade books and magazines sought to teach professional, hobbyist, and casual users to write computer programs. A sample of these learning resources includes \"BASIC Computer Games, Microcomputer Edition\" (1978), by David Ahl; \"Programming the Z80\" (1979), by Rodnay Zaks; \"Programmer's CP/M Handbook\" (1983), by Andy Johnson-Laird; \"C Primer Plus\" (1984), by Mitchell Waite and The Waite Group; \"The Peter Norton Programmer's Guide to the IBM PC\" (1985), by Peter Norton; \"Advanced MS-DOS\" (1986), by Ray Duncan; \"Learn BASIC Now\" (1989), by Michael Halvorson and David Rygymr; \"Programming Windows\" (1992 and later), by Charles Petzold; \"Code Complete: A Practical Handbook for Software Construction\" (1993), by Steve McConnell; and \"Tricks of the Game-Programming Gurus\" (1994), by Andr\u00e9 LaMothe.\nThe PC software industry spurred the creation of numerous book publishers that offered programming primers and tutorials, as well as books for advanced software developers. These publishers included Addison-Wesley, IDG, Macmillan Inc., McGraw-Hill, Microsoft Press, O'Reilly Media, Prentice Hall, Sybex, Ventana Press, Waite Group Press, Wiley, Wrox Press, and Ziff-Davis.\nComputer magazines and journals also provided learning content for professional and hobbyist programmers. A partial list of these resources includes \"Amiga World\", \"Byte (magazine)\", \"Communications of the ACM\", \"Computer (magazine)\", \"Compute!\", \"Computer Language (magazine)\", \"Computers and Electronics\", \"Dr. Dobb's Journal\", \"IEEE Software\", \"Macworld\", \"PC Magazine\", \"PC/Computing\", and \"UnixWorld\".\nDigital learning / online resources.\nBetween 2000 and 2010, computer book and magazine publishers declined significantly as providers of programming instruction, as programmers moved to Internet resources to expand their access to information. This shift brought forward new digital products and mechanisms to learn programming skills. During the transition, digital books from publishers transferred information that had traditionally been delivered in print to new and expanding audiences.\nImportant Internet resources for learning to code included blogs, wikis, videos, online databases, subscription sites, and custom websites focused on coding skills. New commercial resources included YouTube videos, Lynda.com tutorials (later LinkedIn Learning), Khan Academy, Codecademy, GitHub, and numerous coding bootcamps.\nMost software development systems and game engines included rich online help resources, including integrated development environments (IDEs), context-sensitive help, APIs, and other digital resources. Commercial software development kits (SDKs) also provided a collection of software development tools and documentation in one installable package.\nCommercial and non-profit organizations published learning websites for developers, created blogs, and established newsfeeds and social media resources about programming. Corporations like Apple, Microsoft, Oracle, Google, and Amazon built corporate websites providing support for programmers, including resources like the Microsoft Developer Network (MSDN). Contemporary movements like Hour of Code (Code.org) show how learning to program has become associated with digital learning strategies, education agendas, and corporate philanthropy.\nProgrammers.\nComputer programmers are those who write computer software. Their jobs usually involve:\nAlthough programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language."}
{"id": "5312", "revid": "13326552", "url": "https://en.wikipedia.org/wiki?curid=5312", "title": "On the Consolation of Philosophy", "text": "On the Consolation of Philosophy (), often titled as The Consolation of Philosophy or simply the Consolation, is a philosophical work by the Roman philosopher Boethius. Written in 523 while he was imprisoned and awaiting execution by the Ostrogothic King Theodoric, it is often described as the last great Western work of the Classical Period. Boethius' \"Consolation\" heavily influenced the philosophy of late antiquity, as well as Medieval and early Renaissance Christianity.\nDescription.\n\"On the Consolation of Philosophy\" was written in AD 523 during a one-year imprisonment Boethius served while awaiting trial\u2014and eventual execution\u2014for the alleged crime of treason under the Ostrogothic King Theodoric the Great. Boethius was at the very heights of power in Rome, holding the prestigious office of \"magister officiorum\", and was brought down by treachery. This experience inspired the text, which reflects on how evil can exist in a world governed by God (the problem of theodicy), and how happiness is still attainable amidst fickle fortune, while also considering the nature of happiness and God. In 1891, the academic Hugh Fraser Stewart described the work as \"by far the most interesting example of prison literature the world has ever seen.\"\nBoethius writes the book as a conversation between himself and a female personification of philosophy, referred to as \"Lady Philosophy\". Philosophy consoles Boethius by discussing the transitory nature of wealth, fame, and power (\"no man can ever truly be secure until he has been forsaken by Fortune\"), and the ultimate superiority of things of the mind, which she calls the \"one true good\". She contends that happiness comes from within, and that virtue is all that one truly has because it is not imperiled by the vicissitudes of fortune.\nBoethius engages with the nature of predestination and free will, the problem of evil and the \"problem of desert\", human nature, virtue, and justice. He speaks about the nature of free will and determinism when he asks whether God knows and sees all, or does man have free will. On human nature, Boethius says that humans are essentially good, and only when they give in to \"wickedness\" do they \"sink to the level of being an animal.\" On justice, he says criminals are not to be abused, but rather treated with sympathy and respect, using the analogy of doctor and patient to illustrate the ideal relationship between prosecutor and criminal.\nOutline.\n\"On the Consolation of Philosophy\" is laid out as follows:\nInterpretation.\nIn the \"Consolation\", Boethius answered religious questions without reference to Christianity, relying solely on natural philosophy and the Classical Greek tradition. He believed in the correspondence between faith and reason. The truths found in Christianity would be no different from the truths found in philosophy. In the words of Henry Chadwick, \"If the \"Consolation\" contains nothing distinctively Christian, it is also relevant that it contains nothing specifically pagan either...[it] is a work written by a Platonist who is also a Christian.\"\nBoethius repeats the Macrobius model of the Earth in the center of a spherical cosmos.\nThe philosophical message of the book fits well with the religious piety of the Middle Ages. Boethius encouraged readers not to pursue worldly goods such as money and power, but to seek internalized virtues. Evil had a purpose, to provide a lesson to help change for good; while suffering from evil was seen as virtuous. Because God ruled the universe through Love, prayer to God and the application of Love would lead to true happiness. The Middle Ages, with their vivid sense of an overruling fate, found in Boethius an interpretation of life closely akin to the spirit of Christianity. The \"Consolation\" stands, by its note of fatalism and its affinities with the Christian doctrine of humility, midway between the pagan philosophy of Seneca the Younger and the later Christian philosophy of consolation represented by Thomas \u00e0 Kempis.\nThe book is heavily influenced by Plato and his dialogues (as was Boethius himself). Its popularity can in part be explained by its Neoplatonic and Christian ethical messages, although current scholarly research is still far from clear exactly why and how the work became so vastly popular in the Middle Ages.\nInfluence.\nFrom the Carolingian epoch to the end of the Middle Ages and beyond, \"The Consolation of Philosophy\" was one of the most popular and influential philosophical works, read by statesmen, poets, historians, philosophers, and theologians. It is through Boethius that much of the thought of the Classical period was made available to the Western Medieval world. It has often been said Boethius was the \"last of the Romans and the first of the Scholastics\".\nTranslations into the vernacular were done by famous notables, including King Alfred (Old English), Jean de Meun (Old French), Geoffrey Chaucer (Middle English), Queen Elizabeth I (Early Modern English) and Notker Labeo (Old High German). Boethius's \"Consolation of Philosophy\" was translated into Italian by Alberto della Piagentina (1332), Anselmo Tanso (Milan, 1520), Lodovico Domenichi (Florence, 1550), Benedetto Varchi (Florence, 1551), Cosimo Bartoli (Florence, 1551) and Tommaso Tamburini (Palermo, 1657).\nFound within the \"Consolation\" are themes that have echoed throughout the Western canon: the female figure of wisdom that informs Dante, the ascent through the layered universe that is shared with Milton, the reconciliation of opposing forces that find their way into Chaucer in \"The Knight's Tale\", and the Wheel of Fortune so popular throughout the Middle Ages.\nCitations from it occur frequently in Dante's \"Divina Commedia\". Of Boethius, Dante remarked: \"The blessed soul who exposes the deceptive world to anyone who gives ear to him.\"\nBoethian influence can be found nearly everywhere in Geoffrey Chaucer's poetry, e.g. in \"Troilus and Criseyde\", \"The Knight's Tale\", \"The Clerk's Tale\", \"The Franklin's Tale\", \"The Parson's Tale\" and \"The Tale of Melibee\", in the character of Lady Nature in \"The Parliament of Fowls\" and some of the shorter poems, such as \"Truth\", \"The Former Age\" and \"Lak of Stedfastnesse\". Chaucer translated the work in his \"Boece\".\nThe Italian composer Luigi Dallapiccola used some of the text in his choral work \"Canti di prigionia\" (1938). The Australian composer Peter Sculthorpe quoted parts of it in his opera or music theatre work \"Rites of Passage\" (1972\u201373), which was commissioned for the opening of the Sydney Opera House but was not ready in time.\nTom Shippey in \"The Road to Middle-earth\" says how \"Boethian\" much of the treatment of evil is in Tolkien's \"The Lord of the Rings\". Shippey says that Tolkien knew well the translation of Boethius that was made by King Alfred and he quotes some \"Boethian\" remarks from Frodo, Treebeard, and Elrond.\nBoethius and \"Consolatio Philosophiae\" are cited frequently by the main character Ignatius J. Reilly in the Pulitzer Prize-winning \"A Confederacy of Dunces\" (1980).\nIt is a prosimetrical text, meaning that it is written in alternating sections of prose and metered verse. In the course of the text, Boethius displays a virtuosic command of the forms of Latin poetry. It is classified as a Menippean satire, a fusion of allegorical tale, platonic dialogue, and lyrical poetry.\nEdward Gibbon described the work as \"a golden volume not unworthy of the leisure of Plato or Tully.\"\nIn the 20th century, there were close to four hundred manuscripts still surviving, a testament to its popularity.\nOf the work, C. S. Lewis wrote: \"To acquire a taste for it is almost to become naturalised in the Middle Ages.\"\nReconstruction of lost songs.\nHundreds of Latin songs were recorded in neumes from the ninth century through to the thirteenth century, including settings of the poetic passages from Boethius's \"The Consolation of Philosophy\". The music of this song repertory had long been considered irretrievably lost because the notational signs indicated only melodic outlines, relying on now-lapsed oral traditions to fill in the missing details. However, research conducted by Sam Barrett at the University of Cambridge, extended in collaboration with Medieval music ensemble Sequentia, has shown that principles of musical setting for this period can be identified, providing crucial information to enable modern realisations. Sequentia performed the world premiere of the reconstructed songs from Boethius's \"The Consolation of Philosophy\" at Pembroke College, Cambridge, in April 2016, bringing to life music not heard in over 1,000 years; a number of the songs were subsequently recorded on the CD \"Boethius: Songs of Consolation. Metra from 11th-Century Canterbury\" (Glossa, 2018). The detective story behind the recovery of these lost songs is told in a documentary film, and a website launched by the University of Cambridge in 2018 provides further details of the reconstruction process, bringing together manuscripts, reconstructions, and video resources."}
{"id": "5313", "revid": "32983869", "url": "https://en.wikipedia.org/wiki?curid=5313", "title": "Crouching Tiger, Hidden Dragon", "text": "Crouching Tiger, Hidden Dragon is a 2000 wuxia martial arts film directed by Ang Lee and written for the screen by Wang Hui-ling, James Schamus, and Tsai Kuo-jung. The film stars Chow Yun-fat, Michelle Yeoh, Zhang Ziyi, and Chang Chen. It is based on the Chinese novel of the same name, serialized between 1941 and 1942 by Wang Dulu, the fourth part of his \"Crane-Iron Series\". Set in 19th-century Imperial China, the plot follows two master warriors, Li Mu Bai (Chow) and Yu Shu Lien (Yeoh), who are faced with their greatest challenge when the treasured Green Destiny sword is stolen by the mysterious thief Jen Yu (Zhang).\nA multinational venture, the film was made on a US$17 million budget, and was produced by Edko Films and Zoom Hunt Productions in collaboration with China Film Co-productions Corporation and Asian Union Film &amp; Entertainment for Columbia Pictures Film Production Asia in association with Good Machine International. The film premiered at the Cannes Film Festival on 18 May 2000, and was theatrically released in the United States on 8 December. With dialogue in Standard Chinese, subtitled for various markets, \"Crouching Tiger, Hidden Dragon\" became a surprise international success, grossing $213.5 million worldwide. It grossed US$128 million in the United States, becoming the highest-grossing foreign produced Mandarin-language film in American history. The film was the first foreign-language film to break the $100 million mark in the United States.\nThe film was praised by critics for its story, direction, cinematography, and martial arts sequences. \"Crouching Tiger, Hidden Dragon\" won over 40 awards and was nominated for 10 Academy Awards in 2001, including Best Picture, and won Best Foreign Language Film, Best Art Direction, Best Original Score, and Best Cinematography, receiving the most nominations ever for a non-English-language film at the time, the record was later tied by 2018's \"Roma\" , and broken by 2024's \"Emilia P\u00e9rez\". The film also won four BAFTAs and two Golden Globe Awards, each of them for Best Foreign Film. For retrospective years, \"Crouching Tiger\" is often cited as one of the finest wuxia films ever made and has been widely regarded as one of the greatest films of the 21st century.\nPlot.\nIn Qing dynasty China, Li Mu Bai is a renowned Wudang swordsman, and his friend Yu Shu Lien, a warrior, heads a private security company. Shu Lien and Mu Bai have long had feelings for each other, but because Shu Lien had been engaged to Mu Bai's close friend, Meng Sizhao before his death, Shu Lien and Mu Bai feel bound by loyalty to Meng Sizhao and have not revealed their feelings to each other. Mu Bai, choosing to retire from the life of a swordsman, asks Shu Lien to give his fabled 400-year-old sword \"Green Destiny\" to their benefactor Sir Te in Beijing. Long ago, Mu Bai's teacher was killed by Jade Fox, a woman who sought to learn Wudang secrets. While at Sir Te's place, Shu Lien meets Yu Jiaolong, or Jen, who is the daughter of the rich and powerful Governor Yu and is about to get married.\nOne evening, a masked thief sneaks into Sir Te's estate and steals the Green Destiny. Sir Te's servant Master Bo and Shu Lien trace the theft to Governor Yu's compound, where Jade Fox had been posing as Jen's governess for many years. Soon after, Mu Bai arrives in Beijing and discusses the theft with Shu Lien. Master Bo makes the acquaintance of Inspector Tsai, a police investigator from the provinces, and his daughter May, who have come to Beijing in pursuit of Fox. Fox challenges the pair and Master Bo to a showdown that night. Following a protracted battle, the group is on the verge of defeat when Mu Bai arrives and outmaneuvers Fox. She reveals that she killed Mu Bai's teacher because he would sleep with her, but refuse to take a woman as a disciple, and she felt it poetic justice for him to die at a woman's hand. Just as Mu Bai is about to kill her, the masked thief reappears and helps Fox. Fox kills Tsai before fleeing with the thief (who is revealed to be Jen). After seeing Jen fight Mu Bai, Fox realizes Jen had been secretly studying the Wudang manual. Fox is illiterate and could only follow the diagrams, whereas Jen's ability to read the manual allowed her to surpass her teacher in martial arts.\nAt night, a bandit named Lo breaks into Jen's bedroom and asks her to leave with him. In the past, when Governor Yu and his family were traveling in the western deserts of Xinjiang, Lo and his bandits raided Jen's caravan and Lo stole her comb. She pursued him to his desert cave to retrieve her comb. However, the pair soon fell in love. Lo eventually convinced Jen to return to her family, though not before telling her a legend of a man who jumped off a mountain to make his wishes come true. Because the man's heart was pure, his wish was granted and he was unharmed, but flew away never to be seen again. Lo has come now to Beijing to persuade Jen not to go through with her arranged marriage. However, Jen refuses to leave with him. Later, Lo interrupts Jen's wedding procession, begging her to leave with him. Shu Lien and Mu Bai convince Lo to wait for Jen at Mount Wudang, where he will be safe from Jen's family, who are furious with him. Jen runs away from her husband on their wedding night before the marriage can be consummated. Disguised in men's clothing, she is accosted at an inn by a large group of warriors; armed with the Green Destiny and her own superior combat skills, she emerges victorious.\nJen visits Shu Lien, who tells her that Lo is waiting for her at Mount Wudang. After an angry exchange, the two women engage in a duel. Shu Lien is the superior fighter, but Jen wields the Green Destiny and is able to destroy each weapon that Shu Lien wields, until Shu Lien finally manages to defeat Jen with a broken sword. When Shu Lien shows mercy, Jen wounds Shu Lien in the arm. Mu Bai arrives and pursues Jen into a bamboo forest, where he offers to take her as his student. Jen agrees if he can take Green Destiny from her in three moves. Mu Bai is able to take the sword in only one move, but Jen reneges on her promise, and Mu Bai throws the sword over a waterfall. Jen dives after the sword and is rescued by Fox. Fox puts Jen into a drugged sleep and places her in a cavern, where Mu Bai and Shu Lien discover her. Fox suddenly attacks them with poisoned needles. Mu Bai mortally wounds Fox, only to realize that one of the needles has hit him in the neck. Before dying, Fox confesses that her goal had been to kill Jen because Jen had hidden the secrets of Wudang's fighting techniques from her.\nContrite, Jen leaves to prepare an antidote for the poisoned dart. With his last breath, Mu Bai finally confesses his love for Shu Lien. He dies in her arms as Jen returns. Shu Lien forgives Jen, telling her to go to Lo and always be true to herself. The Green Destiny is returned to Sir Te. Jen goes to Mount Wudang and spends the night with Lo. The next morning, Lo finds Jen standing on a bridge overlooking the edge of the mountain. In an echo of the legend that they spoke about in the desert, she asks him to make a wish. Lo wishes for them to be together again, back in the desert. Jen leaps from the bridge, falling into the mists below.\nCast.\nCredits from British Film Institute:\nThemes and interpretations.\nTitle.\nThe title \"Crouching Tiger, Hidden Dragon\" is a literal translation of the Chinese idiom \"\u81e5\u864e\u85cf\u9f8d\" which describes a place or situation that is full of unnoticed masters. It is from a poem of the ancient Chinese poet Yu Xin (513\u2013581) that reads \"\u6697\u77f3\u7591\u85cf\u864e\uff0c\u76e4\u6839\u4f3c\u81e5\u9f8d\", which means \"behind the rock in the dark probably hides a tiger, and the coiling giant root resembles a crouching dragon\". The title also has several other layers of meaning. On one level, the Chinese characters in the title connect to the narrative that the last character in Xiaohu and Jiaolong's names mean \"tiger\" and \"dragon\", respectively. On another level, the Chinese idiomatic phrase is an expression referring to the undercurrents of emotion, passion, and secret desire that lie beneath the surface of polite society and civil behavior, which alludes to the film's storyline.\nGender roles.\nThe success of the Disney animated feature \"Mulan\" (1998) popularized the image of the Chinese woman warrior in the west. The storyline of \"Crouching Tiger, Hidden Dragon\" is mostly driven by the three female characters. In particular, Jen is driven by her desire to be free from the gender role imposed on her, while Shu Lien, herself oppressed by the gender role, tries to lead Jen back into the role deemed appropriate for her. Some prominent martial arts disciplines are traditionally held to have been originated by women, e.g., Wing Chun. The film's title refers to masters one does not notice, which necessarily includes mostly women, and therefore suggests the advantage of a female bodyguard.\nPoison.\nPoison is also a significant theme in the film. The Chinese word \"\u6bd2\" (\"d\u00fa\") means not only physical poison but also cruelty and sinfulness. In the world of martial arts, the use of poison is considered an act of one who is too cowardly and dishonorable to fight; and indeed, the only character who explicitly fits these characteristics is Jade Fox. The poison is a weapon of her bitterness and quest for vengeance: she poisons the master of Wudang, attempts to poison Jen, and succeeds in killing Mu Bai using a poisoned needle. In further play on this theme by the director, Jade Fox, as she dies, refers to the poison from a young child, \"the deceit of an eight-year-old girl\", referring to what she considers her own spiritual poisoning by her young apprentice Jen. Li Mu Bai himself warns that, without guidance, Jen could become a \"poison dragon\".\nChina of the imagination.\nThe story is set during the Qing dynasty (1644\u20131912), but it does not specify an exact time. Lee sought to present a \"China of the imagination\" rather than an accurate vision of Chinese history. At the same time, Lee also wanted to make a film that Western audiences would want to see. Thus, the film is shot for a balance between Eastern and Western aesthetics. There are some scenes showing uncommon artistry for the typical martial arts film such as an airborne battle among wispy bamboo plants.\nProduction.\nThe film was adapted from the novel \"Crouching Tiger, Hidden Dragon\" by Wang Dulu, serialized between 1941 and 1942 in \"Qingdao Xinmin News\". The novel is the fourth in a sequence of five. In the contract reached between Columbia Pictures and Ang Lee and Hsu Li-kong, they agreed to invest US$6 million in filming, but the stipulated recovery amount must be more than six times before the two parties will start to pay dividends.\nCasting.\nShu Qi was Ang Lee's first choice for the role of Jen, but she turned it down.\nFilming.\nAlthough its Academy Award for Best Foreign Language Film was presented to Taiwan, \"Crouching Tiger, Hidden Dragon\" was in fact an international co-production between companies in four regions: the Chinese company China Film Co-production Corporation, the American companies Columbia Pictures Film Production Asia, Sony Pictures Classics, and Good Machine, the Hong Kong company Edko Films, and the Taiwanese Zoom Hunt Productions, as well as the unspecified United China Vision and Asia Union Film &amp; Entertainment, created solely for this film.\nThe film was made in Beijing, with location shooting in Urumchi, Western Provinces, Taklamakan Plateau, \nShanghai and Anji of China. The first phase of shooting was in the Gobi Desert where it consistently rained. Director Ang Lee noted: \"I didn't take one break in eight months, not even for half a day. I was miserable\u2014I just didn't have the extra energy to be happy. Near the end, I could hardly breathe. I thought I was about to have a stroke.\" The stunt work was mostly performed by the actors themselves and Ang Lee stated in an interview that computers were used \"only to remove the safety wires that held the actors\" aloft. \"Most of the time you can see their faces,\" he added. \"That's really them in the trees.\"\nAnother compounding issue was the difference between accents of the four lead actors: Chow Yun-fat is from Hong Kong and speaks Cantonese natively; Michelle Yeoh is from Malaysia and grew up speaking English and Malay, so she learned the Standard Chinese lines phonetically; Chang Chen is from Taiwan and he speaks Standard Chinese in a Taiwanese accent. Only Zhang Ziyi spoke with a native Mandarin accent that Ang Lee wanted. Chow Yun Fat said, on \"the first day [of shooting], I had to do 28 takes just because of the language. That's never happened before in my life.\"\nThe film specifically targeted Western audiences rather than the domestic audiences who were already used to Wuxia films. As a result, high-quality English subtitles were needed. Ang Lee, who was educated in the West, personally edited the subtitles to ensure they were satisfactory for Western audiences.\nSoundtrack.\nThe score was composed by Dun Tan in 1999. It was played for the movie by the Shanghai Symphony Orchestra, the Shanghai National Orchestra and the Shanghai Percussion Ensemble. It features solo passages for cello played by Yo-Yo Ma. The \"last track\" (\"A Love Before Time\") features Coco Lee, who later sang it at the Academy Awards. The composer Chen Yuanlin also collaborated in the project. The music for the entire film was produced in two weeks. Tan the next year (2000) adapted his filmscore as a cello concerto called simply \"Crouching Tiger.\"\nRelease.\nMarketing.\nThe film was adapted into a video game and a series of comics, and it led to the original novel being adapted into a 34-episode Taiwanese television series. The latter was released in 2004 as \"New Crouching Tiger, Hidden Dragon\" for Northern American release.\nHome media.\nThe film was released on VHS and DVD on 5 June 2001 by Columbia TriStar Home Entertainment. It was also released on UMD on 26 June 2005. In the United Kingdom, it was watched by viewers on television in 2004, making it the year's most-watched foreign-language film on television.\nRestoration.\nThe film was re-released in a 4K restoration by Sony Pictures Classics in 2023.\nReception.\nBox office.\nThe film premiered in cinemas on 8 December 2000, in limited release within the United States. During its opening weekend, the film opened in 15th place, grossing $663,205 in business, showing at 16 locations. On 12 January 2001, \"Crouching Tiger, Hidden Dragon\" premiered in cinemas in wide release throughout the U.S., grossing $8,647,295 in business, ranking in sixth place. The film \"Save the Last Dance\" came in first place during that weekend, grossing $23,444,930. The film's revenue dropped by almost 30% in its second week of release, earning $6,080,357. For that particular weekend, the film fell to eighth place, screening in 837 theaters. \"Save the Last Dance\" remained unchanged in first place, grossing $15,366,047 in box-office revenue. During its final week in release, \"Crouching Tiger, Hidden Dragon\" opened in a distant 50th place with $37,233 in revenue. The film went on to top out domestically at $128,078,872 in total ticket sales through a 31-week theatrical run. Internationally, the film took in an additional $85,446,864 in box-office business for a combined worldwide total of $213,525,736. For 2000 as a whole, the film cumulatively ranked at a worldwide box-office performance position of 19.\nCritical response.\n\"Crouching Tiger, Hidden Dragon\" was widely acclaimed in the Western world, receiving numerous awards. On Rotten Tomatoes, the film holds an approval rating of 98% based on 168 reviews, with an average rating of 8.6/10. The site's critical consensus states: \"The movie that catapulted Ang Lee into the ranks of upper echelon Hollywood filmmakers, \"Crouching Tiger, Hidden Dragon\" features a deft mix of amazing martial arts battles, beautiful scenery, and tasteful drama.\" Metacritic reported the film had an average score of 94 out of 100, based on 32 reviews, indicating \"universal acclaim\".\nSome Chinese-speaking viewers were bothered by the accents of the leading actors. Neither Chow (a native Cantonese speaker) nor Yeoh (who was born and raised in Malaysia) spoke Mandarin Chinese as a mother tongue. All four main actors spoke Standard Chinese with vastly different accents: Chow speaks with a Cantonese accent, Yeoh with a Malaysian accent, Chang Chen with a Taiwanese accent, and Zhang Ziyi with a Beijing accent. Yeoh responded to this complaint in a 28 December 2000, interview with \"Cinescape\". She argued: \"My character lived outside of Beijing, and so I didn't have to do the Beijing accent.\" When the interviewer, Craig Reid, remarked: \"My mother-in-law has this strange Sichuan-Mandarin accent that's hard for me to understand,\" Yeoh responded: \"Yes, provinces all have their very own strong accents. When we first started the movie, Cheng Pei Pei was going to have her accent, and Chang Zhen was going to have his accent, and this person would have that accent. And in the end nobody could understand what they were saying. Forget about us, even the crew from Beijing thought this was all weird.\"\nThe film led to a boost in popularity of Chinese wuxia films in the western world, where they were previously little known, and led to films such as \"Hero\" and \"House of Flying Daggers\", both directed by Zhang Yimou, being marketed towards Western audiences. The film also provided the breakthrough role for Zhang Ziyi's career, who noted:\n\"Film Journal\" noted that \"Crouching Tiger, Hidden Dragon\" \"pulled off the rare trifecta of critical acclaim, boffo box-office and gestalt shift\", in reference to its ground-breaking success for a subtitled film in the American market.\nAccolades.\nGarnering widespread critical acclaim at the Toronto and New York film festivals, the film also became a favorite when Academy Awards nominations were announced in 2001. The film was screened out of competition at the 2000 Cannes Film Festival. The film received ten Academy Award nominations, which was the highest ever for a non-English language film, up until it was tied by \"Roma\" (2018).\nThe film is ranked at number 497 on \"Empire\"'s 2008 list of the 500 greatest movies of all time. and at number 66 in the magazine's 100 Best Films of World Cinema, published in 2010.\nIn 2010, the Independent Film &amp; Television Alliance selected the film as one of the 30 Most Significant Independent Films of the last 30 years.\nIn 2016, it was voted the 35th-best film of the 21st century as picked by 177 film critics from around the world in a poll conducted by BBC.\nThe film was included in BBC's 2018 list of The 100 greatest foreign language films ranked by 209 critics from 43 countries around the world. In 2019, \"The Guardian\" ranked the film 51st in its 100 best films of the 21st century list. In 2024, \"Looper\" ranked it number 12 on its list of the \"50 Best PG-13 Movies of All Time,\" writing \"It's rare for a movie to conjure up the word \"sweeping,\"\u00a0but that's just what \"Crouching Tiger, Hidden Dragon\"\u00a0does. Whether it's the sight of human beings flying through the sky or the absorbing human drama that drives the plot, Ang\u00a0Lee's 2000 wuxia feature is a remarkable movie that makes one's jaw drop as often as it makes your heart soar.\"\nSequel.\nIn 2001, it was reported that director Ang Lee was planning to make a sequel to the film. \"\", was released in 2016. It was directed by Yuen Wo-ping, who was the action choreographer for the first film. It is a co-production between Pegasus Media, China Film Group Corporation, and the Weinstein Company. Unlike the original film, the sequel was filmed in English for international release and dubbed into Chinese for Chinese releases.\n\"Sword of Destiny\" is based on \"Iron Knight, Silver Vase\", the next (and last) novel in the \"Crane-Iron_Series\". It features a mostly new cast, headed by Donnie Yen. Michelle Yeoh reprised her role from the original. Zhang Ziyi was also approached to appear in \"Sword of Destiny\" but refused, stating that she would only appear in a sequel if Ang Lee were directing it.\nIn the West, the sequel was for the most part not shown in theaters, instead being distributed direct-to-video by the streaming service Netflix.\nIn popular culture.\nThe names of the pterosaur genus \"Kryptodrakon\" and the ceratopsian genus \"Yinlong\" (both meaning \"hidden dragon\" in Greek and Chinese respectively) allude to the film.\nThe character of Lo, or \"Dark Cloud\" the desert bandit, influenced the development of the protagonist of the \"Prince of Persia\" series of video games."}
{"id": "5314", "revid": "37076523", "url": "https://en.wikipedia.org/wiki?curid=5314", "title": "Charlemagne", "text": "Charlemagne ( ; 2 April 748 \u2013 28 January 814) was King of the Franks from 768, King of the Lombards from 774, and Emperor of what is now known as the Carolingian Empire from 800, holding these titles until his death in 814. He united most of Western and Central Europe, and was the first recognised emperor to rule from the west after the fall of the Western Roman Empire approximately three centuries earlier. Charlemagne's reign was marked by political and social changes that had lasting influence on Europe throughout the Middle Ages.\nA member of the Frankish Carolingian dynasty, Charlemagne was the eldest son of Pepin the Short and Bertrada of Laon. With his brother, Carloman I, he became king of the Franks in 768 following Pepin's death and became the sole ruler three years later. Charlemagne continued his father's policy of protecting the papacy and became its chief defender, removing the Lombards from power in northern Italy in 774. His reign saw a period of expansion that led to the conquests of Bavaria, Saxony, and northern Spain, as well as other campaigns that led Charlemagne to extend his rule over a large part of Europe. Charlemagne spread Christianity to his new conquests (often by force), as seen at the Massacre of Verden against the Saxons. He also sent envoys and initiated diplomatic contact with the Abbasid caliph Harun al-Rashid in the 790s, due to their mutual interest in Iberian affairs.\nIn 800, Charlemagne was crowned emperor in Rome by Pope Leo III. Although historians debate the coronation's significance, the title represented the height of his prestige and authority. Charlemagne's position as the first emperor in the West in over 300 years brought him into conflict with the Eastern Roman Empire in Constantinople. Through his assumption of the imperial title, he is considered the forerunner to the line of Holy Roman Emperors, which persisted into the nineteenth century. As king and emperor, Charlemagne engaged in a number of reforms in administration, law, education, military organisation, and religion, which shaped Europe for centuries. The stability of his reign began a period of cultural activity known as the Carolingian Renaissance.\nCharlemagne died in 814 and was buried at Aachen Cathedral in Aachen, his imperial capital city. He was succeeded by his only surviving legitimate son, Louis the Pious. After Louis, the Frankish kingdom was divided and eventually coalesced into West and East Francia, which later became France and Germany, respectively. Charlemagne's profound influence on the Middle Ages and influence on the territory he ruled has led him to be called the \"Father of Europe\" by many historians. He is seen as a founding figure by multiple European states and a number of historical royal houses of Europe trace their lineage back to him. Charlemagne has been the subject of artworks, monuments and literature during and after the medieval period and is venerated by the Catholic Church.\nName.\nSeveral languages were spoken in Charlemagne's world, but he used (or ) in Medieval Latin, the formal language of writing and diplomacy. \"Charles\" is the modern English form of these names. The name , as the emperor is normally known in English, comes from the French ('Charles the Great'). In modern German and Dutch, he is known as and respectively. The Latin epithet ('great') may have been associated with him during his lifetime, but this is not certain. The contemporary \"Royal Frankish Annals\" routinely call him (\"Charles the great king\"). That epithet is attested in the works of the Poeta Saxo around 900, and it had become commonly applied to him by 1000.\nCharlemagne was named after his grandfather, Charles Martel. That name, and its derivatives, are unattested before their use by Charles Martel and Charlemagne. \"Karolus\" was adapted by Slavic languages as their word for \"king\" (, and ) through Charlemagne's influence or that of his great-grandson, Charles the Fat.\nEarly life and rise to power.\nPolitical background and ancestry.\nBy the sixth century, the western Germanic tribe of the Franks had been Christianised; this was due in considerable measure to the conversion of their king, Clovis\u00a0I, to Catholicism. The Franks had established a kingdom in Gaul in the wake of the Fall of the Western Roman Empire. This kingdom, Francia, grew to encompass nearly all of present-day France and Switzerland, along with parts of modern Germany and the Low Countries under the rule of the Merovingian dynasty. Francia was often divided under different Merovingian kings, due to the partible inheritance practised by the Franks. The late seventh century saw a period of war and instability following the murder of King Childeric II, which led to factional struggles among the Frankish aristocrats.\nPepin of Herstal, mayor of the palace of Austrasia, ended the strife between various kings and their mayors with his 687 victory at the Battle of Tertry. Pepin was the grandson of two important figures of Austrasia: Arnulf of Metz and Pepin of Landen. The mayors of the palace had gained influence as the Merovingian kings' power waned due to divisions of the kingdom and several succession crises. Pepin was eventually succeeded by his son Charles, later known as Charles Martel. Charles did not support a Merovingian successor upon the death of King Theuderic IV in 737, leaving the throne vacant. He made plans to divide the kingdom between his sons, Carloman and Pepin the Short, who succeeded him after his death in 741. The brothers placed the Merovingian Childeric III on the throne in 743. Pepin married Bertrada, a member of an influential Austrasian noble family, in 744. In 747, Carloman abdicated and entered a monastery in Rome. He had at least two sons; the elder, Drogo, took his place.\nBirth.\nCharlemagne's year of birth is uncertain, although it was most likely in 748. An older tradition based on three sources, however, gives a birth year of 742. The ninth-century biographer Einhard reports Charlemagne as being 72 years old at the time of his death; the \"Royal Frankish Annals\" imprecisely gives his age at death as about 71, and his original epitaph called him a septuagenarian. Einhard said that he did not know much about Charlemagne's early life; some modern scholars believe that, not knowing the emperor's true age, he still sought to present an exact date in keeping with the Roman imperial biographies of Suetonius, which he used as a model. All three sources may have been influenced by Psalm 90: \"The days of our years are threescore years and ten\".\nHistorian Karl Ferdinand Werner challenged the acceptance of 742 as the Frankish king's birth year, citing an addition to the \"Annales Petaviani\" which records Charlemagne's birth in 747. Lorsch Abbey commemorated Charlemagne's date of birth as 2 April from the mid-ninth century, and this date is likely to be genuine. Matthias Becher built on Werner's work and showed that 2 April in the year recorded would have actually been in 748, since the annalists recorded the start of the year from Easter rather than 1 January. Presently, most scholars accept April 748 for Charlemagne's birth. Charlemagne's place of birth is unknown. The Frankish palaces in Vaires-sur-Marne and Quierzy are among the places suggested by scholars. Pepin the Short held an assembly in D\u00fcren in 748, but it cannot be proved that it took place in April or if Bertrada was with him.\nLanguage and education.\nThe (\"native tongue\"). that Einhard refers to with regard to Charlemagne, was a Germanic language. Due to the prevalence in Francia of \"rustic Roman\", he was probably functionally bilingual in Germanic and Romance dialects at an early age. Charlemagne also spoke Latin and, according to Einhard, could understand and (perhaps) speak some Greek. Some 19th century historians tried to use the Oaths of Strasbourg (842) to determine Charlemagne's native language. They assumed that the text's copyist, Nithard, being a grandson of Charlemagne, would have spoken the same dialect as his grandfather, giving rise to the assumption that Charlemagne would have spoken language closely related to the one used in the oath, which is a form of Old High German ancestral to the modern Rhenish Franconian dialects.\n Other authors have instead taken Charlemagne's place of birth (Liege, Belgium) and the place of his education and main residence (Aachen), to postulate that Charlemagne most likely spoke a form of Moselle- or Ripuarian Franconian. In any case, all three dialects would have been closely related, mutually intelligible and, while classified as Old High German, none of the dialects involved can be considered typical of Old High German, showing varying degrees of participation in the High German consonant shift as well as certain similarities with Old Dutch, the presumed language of the previous Merovingian dynasty, mirroring the linguistic diversity still typical of the region today. \nCharlemagne's father Pepin had been educated at the abbey of Saint-Denis, although the extent of Charlemagne's formal education is unknown. He almost certainly was trained in military matters as a youth in Pepin's court, which was itinerant. Charlemagne also asserted his own education in the liberal arts in encouraging their study by his children and others, although it is unknown whether his study was as a child or at court during his later life. The question of Charlemagne's literacy is debated, with little direct evidence from contemporary sources. He normally had texts read aloud to him and dictated responses and decrees, but this was not unusual even for a literate ruler at the time. Historian Johannes Fried considers it likely that Charlemagne would have been able to read, but the medievalist Paul Dutton writes that \"the evidence for his ability to read is circumstantial and inferential at best\" and concludes that it is likely that he never properly mastered the skill. Einhard makes no direct mention of Charlemagne reading, and recorded that he only attempted to learn to write later in life.\nAccession and reign with Carloman.\nThere are only occasional references to Charlemagne in the Frankish annals during his father's lifetime. By 751 or 752, Pepin had deposed Childeric and replaced him as king. Early Carolingian-influenced sources claim that Pepin's seizure of the throne was sanctioned beforehand by Pope Stephen II, but modern historians dispute this. It is possible that papal approval came only when Stephen travelled to Francia in 754 (apparently to request Pepin's aid against the Lombards), and on this trip anointed Pepin as king; this legitimised his rule. Charlemagne was sent to greet and escort the Pope, and he and his younger brother Carloman were anointed with their father. Pepin sidelined Drogo around the same time, sending him and his brother to a monastery.\nCharlemagne began issuing charters in his own name in 760. The following year, he joined his father's campaign against Aquitaine. Aquitaine, led by Dukes Hunald and Waiofar, was constantly in rebellion during Pepin's reign. Pepin fell ill on campaign there and died on 24 September 768, and Charlemagne and Carloman succeeded their father. They had separate coronations, Charlemagne at Noyon and Carloman at Soissons, on 9 October. The brothers maintained separate palaces and spheres of influence, although they were considered joint rulers of a single Frankish kingdom. The \"Royal Frankish Annals\" report that Charlemagne ruled Austrasia and Carloman ruled Burgundy, Provence, Aquitaine, and Alamannia, with no mention made of which brother received Neustria. The immediate concern of the brothers was the ongoing uprising in Aquitaine. They marched into Aquitaine together, but Carloman returned to Francia for unknown reasons and Charlemagne completed the campaign on his own. Charlemagne's capture of Duke Hunald marked the end of ten years of war that had been waged in the attempt to bring Aquitaine into line.\nCarloman's refusal to participate in the war against Aquitaine led to a rift between the kings. It is uncertain why Carloman abandoned the campaign; the brothers may have disagreed about control of the territory, or Carloman was focused on securing his rule in the north of Francia. Regardless of the strife between the kings, they maintained a joint rule for practical reasons. Charlemagne and Carloman worked to obtain the support of the clergy and local elites to solidify their positions.\nPope Stephen III was elected in 768, but was briefly deposed by Antipope Constantine II before being restored to Rome. Stephen's papacy experienced continuing factional struggles, so he sought support from the Frankish kings. Both brothers sent troops to Rome, each hoping to exert his own influence. The Lombard king Desiderius also had interests in Roman affairs, and Charlemagne attempted to enlist him as an ally. Desiderius already had alliances with Bavaria and Benevento through the marriages of his daughters to their dukes, and an alliance with Charlemagne would add to his influence. Charlemagne's mother, Bertrada, went on his behalf to Lombardy in 770 and brokered a marriage alliance before returning to Francia with his new bride. Desiderius's daughter is traditionally known as Desiderata, although she may have been named Gerperga. Anxious about the prospect of a Frankish\u2013Lombard alliance, Pope Stephen sent a letter to both Frankish kings decrying the marriage and separately sought closer ties with Carloman.\nCharlemagne had already had a relationship with the Frankish noblewoman Himiltrude, and they had a son in 769 named Pepin. Paul the Deacon wrote in his 784 that Pepin was born \"before legal marriage\", but does not say whether Charles and Himiltrude ever married, were joined in a non-canonical marriage (), or married after Pepin was born. Pope Stephen's letter described the relationship as a legitimate marriage, but he had a vested interest in preventing Charlemagne from marrying Desiderius's daughter.\nCarloman died suddenly on 4 December 771, leaving Charlemagne sole king of the Franks. He moved immediately to secure his hold on his brother's territory, forcing Carloman's widow Gerberga to flee to Desiderius's court in Lombardy with their children. Charlemagne ended his marriage to Desiderius's daughter and married Hildegard, daughter of count Gerold, a powerful magnate in Carloman's kingdom. This was a reaction to Desiderius's sheltering of Carloman's family and a move to secure Gerold's support.\nKing of the Franks and the Lombards.\nAnnexation of the Lombard Kingdom.\nCharlemagne's first campaigning season as sole king of the Franks was spent on the eastern frontier in his first war against the Saxons, who had been engaging in border raids on the Frankish kingdom when Charlemagne responded by destroying the pagan Irminsul at Eresburg and seizing their gold and silver. The success of the war helped secure Charlemagne's reputation among his brother's former supporters and funded further military action. The campaign was the beginning of over thirty years of nearly-continuous warfare against the Saxons by Charlemagne.\nPope Adrian I succeeded Stephen III in 772, and sought the return of papal control of cities that had been captured by Desiderius. Unsuccessful in dealing with the Lombard king directly, Adrian sent emissaries to Charlemagne to gain his support for recovering papal territory. Charlemagne, in response to this appeal and the dynastic threat of Carloman's sons in the Lombard court, gathered his forces to intervene. He first sought a diplomatic solution, offering gold to Desiderius in exchange for the return of the papal territories and his nephews. This overture was rejected, and Charlemagne's army (commanded by himself and his uncle, Bernard) crossed the Alps to besiege the Lombard capital of Pavia in late 773.\nCharlemagne's second son (also named Charles) was born in 772, and Charlemagne brought the child and his wife to the camp at Pavia. Hildegard was pregnant, and gave birth to a daughter named Adelhaid. The baby was sent back to Francia, but died on the way. Charlemagne left Bernard to maintain the siege at Pavia while he took a force to capture Verona, where Desiderius's son Adalgis had taken Carloman's sons. Charlemagne captured the city; no further record exists of his nephews or of Carloman's wife, and their fate is unknown. Recent biographer, Janet Nelson compares them to the Princes in the Tower in the Wars of the Roses. Fried suggests that the boys were forced into a monastery (a common solution of dynastic issues), or \"an act of murder smooth[ed] Charlemagne's ascent to power.\" Adalgis was not captured by Charlemagne, and fled to Constantinople.\nCharlemagne left the siege in April 774 to celebrate Easter in Rome. Pope Adrian arranged a formal welcome for the Frankish king, and they swore oaths to each other over the relics of St. Peter. Adrian presented a copy of the agreement between Pepin and Stephen III outlining the papal lands and rights Pepin had agreed to protect and restore. It is unclear which lands and rights the agreement involved, which remained a point of dispute for centuries. Charlemagne placed a copy of the agreement in the chapel above St. Peter's tomb as a symbol of his commitment, and left Rome to continue the siege.\nDisease struck the Lombards shortly after his return to Pavia, and they surrendered the city by June 774. Charlemagne deposed Desiderius and took the title of King of the Lombards. The takeover of one kingdom by another was \"extraordinary\", and the authors of \"The Carolingian World\" call it \"without parallel\". Charlemagne secured the support of the Lombard nobles and Italian urban elites to seize power in a mainly-peaceful annexation. Historian Rosamond McKitterick suggests that the elective nature of the Lombard monarchy eased Charlemagne's takeover, and Roger Collins attributes the easy conquest to the Lombard elite's \"presupposition that rightful authority was in the hands of the one powerful enough to seize it\". Charlemagne soon returned to Francia with the Lombard royal treasury and with Desiderius and his family, who would be confined to a monastery for the rest of their lives.\nFrontier wars in Saxony and Spain.\nThe Saxons took advantage of Charlemagne's absence in Italy to raid the Frankish borderlands, leading to a Frankish counter-raid in the autumn of 774 and a reprisal campaign the following year. Charlemagne was soon drawn back to Italy as Duke Hrodgaud of Friuli rebelled against him. He quickly crushed the rebellion, distributing Hrodgaud's lands to the Franks to consolidate his rule in Lombardy. Charlemagne wintered in Italy, consolidating his power by issuing charters and legislation and taking Lombard hostages. Amid the 775 Saxon and Friulian campaigns, his daughter Rotrude was born in Francia.\nReturning north, Charlemagne waged another brief, destructive campaign against the Saxons in 776. This led to the submission of many Saxons, who turned over captives and lands and submitted to baptism. In 777, Charlemagne held an assembly at Paderborn with Frankish and Saxon men; many more Saxons came under his rule, but the Saxon magnate Widukind fled to Denmark to prepare for a new rebellion.\nAlso at the Paderborn assembly were representatives of dissident factions from al-Andalus (Muslim Spain). They included the son and son-in-law of Yusuf ibn Abd al-Rahman al-Fihri, the former governor of C\u00f3rdoba ousted by Caliph Abd al-Rahman in 756, who sought Charlemagne's support for al-Fihri's restoration. Also present was Sulayman al-Arabi, governor of Barcelona and Girona, who wanted to become part of the Frankish kingdom and receive Charlemagne's protection rather than remain under the rule of C\u00f3rdoba. Charlemagne, seeing an opportunity to strengthen the security of the kingdom's southern frontier and extend his influence, agreed to intervene. Crossing the Pyrenees, his army found little resistance until an ambush by Basque forces in 778 at the Battle of Roncevaux Pass. The Franks, defeated in the battle, withdrew with most of their army intact.\nBuilding the dynasty.\nCharlemagne returned to Francia to greet his newborn twin sons, Louis and Lothair, who were born while he was in Spain; Lothair died in infancy. Again, Saxons had seized on the king's absence to raid. Charlemagne sent an army to Saxony in 779 while he held assemblies, legislated, and addressed a famine in Francia. Hildegard gave birth to another daughter, Bertha. Charlemagne returned to Saxony in 780, holding assemblies at which he received hostages from Saxon nobles and oversaw their baptism.\nHe and Hildegard travelled with their four younger children to Rome in the spring of 781, leaving Pepin and Charles at Worms, to make a journey first requested by Adrian in 775. Adrian baptised Carloman and renamed him Pepin, a name he shared with his half-brother. Louis and the newly renamed Pepin were then anointed and crowned. Pepin was appointed king of the Lombards, and Louis king of Aquitaine. This act was not nominal, since the young kings were sent to live in their kingdoms under the care of regents and advisers. A delegation from the Byzantine Empire, the remnant of the Roman Empire in the East, met Charlemagne during his stay in Rome; Charlemagne agreed to betroth his daughter Rotrude to Empress Irene's son, Emperor Constantine VI.\nHildegard gave birth to her eighth child, Gisela, during this trip to Italy. After the royal family's return to Francia, she had her final pregnancy and died from its complications on 30 April 783. The child, named after her, died shortly thereafter. Charlemagne commissioned epitaphs for his wife and daughter, and arranged for a Mass to be said daily at Hildegard's tomb. Charlemagne's mother Bertrada died shortly after Hildegard, on 12 July 783. Charlemagne was remarried to Fastrada, daughter of the East Frankish count Radolf, by the end of the year.\nSaxon resistance and reprisal.\nIn summer 782, Widukind returned from Denmark to attack the Frankish positions in Saxony. He defeated a Frankish army, possibly due to rivalry among the Frankish counts leading it. Charlemagne came to Verden after learning of the defeat, but Widukind fled before his arrival. Charlemagne summoned the Saxon magnates to an assembly and compelled them to turn prisoners over to him, since he regarded their previous acts as treachery. The annals record that Charlemagne had 4,500 Saxon prisoners beheaded in the massacre of Verden. Fried writes, \"Although this figure may be exaggerated, the basic truth of the event is not in doubt\", and Alessandro Barbero calls it \"perhaps the greatest stain on his reputation.\" Charlemagne issued the \"Capitulatio de partibus Saxoniae\", probably in the immediate aftermath of (or as a precursor of) the massacre. With a harsh set of laws which included the death penalty for pagan practices, the \"Capitulatio\" \"constituted a program for the forced conversion of the Saxons\" and was \"aimed\u00a0... at suppressing Saxon identity\".\nCharlemagne's focus for the next several years would be on his attempt to complete the subjugation of the Saxons. Concentrating first in Westphalia in 783, he pushed into Thuringia in 784 as his son Charles the Younger continued operations in the west. At each stage of the campaigns, the Frankish armies seized wealth and carried Saxon captives into slavery. Unusually, Charlemagne campaigned through the winter instead of resting his army. By 785, he had suppressed the Saxon resistance and completely commanded Westphalia. That summer, he met Widukind and persuaded him to end his resistance. Widukind agreed to be baptised with Charlemagne as his godfather, ending this phase of the Saxon Wars.\nBenevento, Bavaria, and Pepin's revolt.\nCharlemagne travelled to Italy in 786, arriving by Christmas. Aiming to extend his influence further into southern Italy, he marched into the Duchy of Benevento. Duke Arechis fled to a fortified position at Salerno before offering Charlemagne his fealty. Charlemagne accepted his submission and hostages, who included Arechis's son Grimoald. In Italy, Charlemagne also met with envoys from Constantinople. Empress Irene had called the 787 Second Council of Nicaea, but did not inform Charlemagne or invite any Frankish bishops. Charlemagne, probably in reaction to the perceived slight of the exclusion, broke the betrothal of his daughter Rotrude and Constantine VI.\nAfter Charlemagne left Italy, Arechis sent envoys to Irene to offer an alliance; he suggested that she send a Byzantine army with Adalgis, the exiled son of Desiderus, to remove the Franks from power in Lombardy. Before his plans could be finalised, Aldechis and his elder son Romuald died of illness within weeks of each other. Charlemagne sent Grimoald back to Benevento to serve as duke and return it to Frankish suzerainty. The Byzantine army invaded, but were repulsed by the Frankish and Lombard forces.\nAs affairs were being settled in Italy, Charlemagne turned his attention to Bavaria. Bavaria was ruled by Duke Tassilo, Charlemagne's first cousin, who had been installed by Pepin the Short in 748. Tassilo's sons were also grandsons of Desiderius, and a potential threat to Charlemagne's rule in Lombardy. The neighbouring rulers had a growing rivalry throughout their reigns, but had sworn oaths of peace to each other in 781. In 784, Rotpert (Charlemagne's viceroy in Italy) accused Tassilo of conspiring with Widukind in Saxony and unsuccessfully attacked the Bavarian city of Bolzano. Charlemagne gathered his forces to prepare for an invasion of Bavaria in 787. Dividing the army, the Franks launched a three-pronged attack. Quickly realizing his poor position, Tassilo agreed to surrender and recognise Charlemagne as his overlord. The following year, Tassilo was accused of plotting with the Avars to attack Charlemagne. He was deposed and sent to a monastery, and Charlemagne absorbed Bavaria into his kingdom. Charlemagne spent the next few years based in Regensburg, largely focused on consolidating his rule of Bavaria and warring against the Avars. Successful campaigns against them were launched from Bavaria and Italy in 788, and Charlemagne led campaigns in 791 and 792.\nCharlemagne gave Charles the Younger rule of Maine in Neustria in 789, leaving Pepin the Hunchback his only son without lands. His relationship with Himiltrude was now apparently seen as illegitimate at his court, and Pepin was sidelined from the succession. In 792, as his father and brothers were gathered in Regensburg, Pepin conspired with Bavarian nobles to assassinate them and install himself as king. The plot was discovered and revealed to Charlemagne before it could proceed; Pepin was sent to a monastery, and many of his co-conspirators were executed.\nThe early 790s saw a marked focus on ecclesiastical affairs by Charlemagne. He summoned a council in Regensburg in 792 to address the theological controversy over the adoptionism doctrine in the Spanish church and formulate a response to the Second Council of Nicea. The council condemned adoptionism as heresy and led to the production of the \"Libri Carolini\", a detailed argument against Nicea's canons. In 794, Charlemagne called another council in Frankfurt. The council confirmed Regensburg's positions on adoptionism and Nicea, recognised the deposition of Tassilo, set grain prices, reformed Frankish coinage, forbade abbesses from blessing men, and endorsed prayer in vernacular languages. Soon after the council, Fastrada fell ill and died; Charlemagne married the Alamannian noblewoman Luitgard shortly afterwards.\nContinued wars with the Saxons and Avars.\nCharlemagne gathered an army after the council of Frankfurt as Saxon resistance continued, beginning a series of annual campaigns which lasted through 799. The campaigns of the 790s were even more destructive than those of earlier decades, with the annal writers frequently noting Charlemagne \"burning\", \"ravaging\", \"devastating\", and \"laying waste\" the Saxon lands. Charlemagne forcibly removed a large number of Saxons to Francia, installing Frankish elites and soldiers in their place. His extended wars in Saxony led to his establishing his court in Aachen, which had easy access to the frontier. He built a large palace there, including a chapel which is now part of the Aachen Cathedral. Einhard joined the court at that time. Pepin of Italy (Carloman) engaged in further wars against the Avars in the south, which led to the collapse of their kingdom and the eastward expansion of Frankish rule.\nCharlemagne also worked to expand his influence through diplomatic means during the 790s wars, focusing on the Anglo-Saxon kingdoms of Britain. Charles the Younger proposed a marriage pact with the daughter of King Offa of Mercia, but Offa insisted that Charlemagne's daughter Bertha also be given as a bride for his son. Charlemagne refused the arrangement, and the marriage did not take place. Charlemagne and Offa entered into a formal peace in 796, protecting trade and securing the rights of English pilgrims to pass through Francia on their way to Rome. Charlemagne was also the host and protector of several deposed English rulers who were later restored: Eadbehrt of Kent, Ecgberht, King of Wessex, and Eardwulf of Northumbria. Nelson writes that Charlemagne treated the Anglo-Saxon kingdoms \"like satellite states,\" establishing direct relations with English bishops. Charlemagne also forged an alliance with Alfonso II of Asturias, although Einhard calls Alfonso his \"dependent\". Following his sack of Lisbon in 798, Alfonso sent Charlemagne trophies of his victory, including armour, mules and prisoners.\nReign as emperor.\nCoronation.\nAfter Leo III became pope in 795, he faced political opposition. His enemies accused him of a number of crimes and physically attacked him in April 799, attempting to remove his eyes and tongue. Leo escaped and fled north to seek Charlemagne's help. Charlemagne continued his campaign against the Saxons before breaking off to meet Leo at Paderborn in September. Hearing evidence from the pope and his enemies, he sent Leo back to Rome with royal legates who were instructed to reinstate the pope and conduct a further investigation. In August of the following year, Charlemagne made plans to go to Rome after an extensive tour of his lands in Neustria. Charlemagne met Leo in November near Mentana at the twelfth milestone outside Rome, the traditional location where Roman emperors began their formal entry into the city. Charlemagne presided over an assembly to hear the charges, but believed that no one could sit in judgement of the pope. Leo swore an oath on 23 December, declaring his innocence of all charges. At mass in St. Peter's Basilica on Christmas Day 800, Leo proclaimed Charlemagne \"emperor of the Romans\" (\"Imperator Romanorum\") and crowned him. Charlemagne was the first reigning emperor in the west since the deposition of Romulus Augustulus in 476. His son, Charles the Younger, was anointed king by Leo at the same time.\nHistorians differ about the intentions of the imperial coronation, the extent to which Charlemagne was aware of it or participated in its planning, and the significance of the events for those present and for Charlemagne's reign. Contemporary Frankish and papal sources differ in their emphasis on, and representation of, events. Einhard writes that Charlemagne would not have entered the church if he knew about the pope's plan; modern historians have regarded his report as truthful or rejected it as a literary device demonstrating Charlemagne's humility. Collins says that the actions surrounding the coronation indicate that it was planned by Charlemagne as early as his meeting with Leo in 799, and Fried writes that Charlemagne planned to adopt the title of emperor by 798 \"at the latest.\" During the years before the coronation, Charlemagne's courtier Alcuin referred to his realm as an \"Imperium Christianum\" (\"Christian Empire\") in which \"just as the inhabitants of the Roman Empire had been united by a common Roman citizenship\", the new empire would be united by a common Christian faith. This is the view of Henri Pirenne, who says that \"Charles was the Emperor of the \"ecclesia\" as the Pope conceived it, of the Roman Church, regarded as the universal Church\".\nThe Eastern Roman (Byzantine) Empire remained a significant contemporary power in European politics for Leo and Charlemagne, especially in Italy. The Byzantines continued to hold a substantial portion of Italy, with their borders not far south of Rome. Empress Irene had seized the throne from her son Constantine VI in 797, deposing and blinding him. Irene, the first Byzantine empress, faced opposition in Constantinople because of her gender and her means of accession. One of the earliest narrative sources for the coronation, the \"Annals of Lorsch\", presented a female ruler in Constantinople as a vacancy in the imperial title which justified Leo's coronation of Charlemagne. Pirenne disagrees, saying that the coronation \"was not in any sense explained by the fact that at this moment a woman was reigning in Constantinople.\" Leo's main motivations may have been the desire to increase his standing after his political difficulties, placing himself as a power broker and securing Charlemagne as a powerful ally and protector. The Byzantine Empire's lack of ability to influence events in Italy and support the papacy were also important to Leo's position. According to the \"Royal Frankish Annals\", Leo prostrated himself before Charlemagne after crowning him (an act of submission standard in Roman coronation rituals from the time of Diocletian). This account presents Leo not as Charlemagne's superior, but as the agent of the Roman people who acclaimed Charlemagne as emperor.\nHistorian Henry Mayr-Harting claims that the assumption of the imperial title by Charlemagne was an effort to incorporate the Saxons into the Frankish realm, since they did not have a native tradition of kingship. However, Costambeys \"et al.\" note in \"The Carolingian World\" that \"since Saxony had not been in the Roman empire it is hard to see on what basis an emperor would have been any more welcomed.\" These authors write that the decision to take the title of emperor was aimed at furthering Charlemagne's influence in Italy, as an appeal to traditional authority recognised by Italian elites within and (especially) outside his control.\nCollins also writes that becoming emperor gave Charlemagne \"the right to try to impose his rule over the whole of [Italy]\", considering this a motivation for the coronation. He notes the \"element of political and military risk\" inherent in the affair due to the opposition of the Byzantine Empire and potential opposition from the Frankish elite, as the imperial title could draw him further into Mediterranean politics. Collins sees several of Charlemagne's actions as attempts to ensure that his new title had a distinctly-Frankish context.\nCharlemagne's coronation led to a centuries-long ideological conflict between his successors and Constantinople known as the problem of two emperors, which could be seen as a rejection or usurpation of the Byzantine emperors' claim to be the universal, preeminent rulers of Christendom. Historian James Muldoon writes that Charlemagne may have had a more limited view of his role, seeing the title as representing dominion over lands he already ruled. However, the title of emperor gave Charlemagne enhanced prestige and ideological authority. He immediately incorporated his new title into documents he issued, adopting the formula \"Charles, most serene augustus, crowned by God, great peaceful emperor governing the Roman empire, and who is by the mercy of God king of the Franks and the Lombards\" instead of the earlier form \"Charles, by the grace of God king of the Franks and Lombards and patrician of the Romans.\" Leo acclaimed Charlemagne as \"emperor of the Romans\" during the coronation, but Charlemagne never used this title. The avoidance of the specific claim of being a \"Roman emperor\", as opposed to the more-neutral \"emperor governing the Roman empire\", may have been to improve relations with the Byzantines. This formulation (with the continuation of his earlier royal titles) may also represent a view of his role as emperor as being the ruler of the people of the city of Rome, as he was of the Franks and the Lombards.\nGoverning the empire.\nCharlemagne left Italy in the summer of 801 after adjudicating several ecclesiastical disputes in Rome and experiencing an earthquake in Spoleto. He never returned to the city. Continuing trends and a ruling style established in the 790s, Charlemagne's reign from 801 onward is a \"distinct phase\" characterised by more sedentary rule from Aachen. Although conflict continued until the end of his reign, the relative peace of the imperial period allowed for attention on internal governance. The Franks continued to wage war, though these wars were defending and securing the empire's frontiers, and Charlemagne rarely led armies personally. A significant expansion of the Spanish March was achieved with a series of campaigns by Louis against the Emirate of Cordoba, culminating in the 801 capture of Barcelona.\nThe 802 \"Capitulare missorum generale\" was an expansive piece of legislation, with provisions governing the conduct of royal officials and requiring that all free men take an oath of loyalty to Charlemagne. The capitulary reformed the institution of the , officials who would now be assigned in pairs (a cleric and a lay aristocrat) to administer justice and oversee governance in defined territories. The emperor also ordered the revision of the Lombard and Frankish legal codes.\nIn addition to the , Charlemagne also ruled parts of the empire with his sons as sub-kings. Although Pepin and Louis had some authority as kings in Italy and Aquitaine, Charlemagne had the ultimate authority and directly intervened. Charles, their elder brother, had been given lands in Neustria in 789 or 790 and made a king in 800.\nThe 806 charter (\"Division of the Realm\") set the terms of Charlemagne's succession. Charles, as his eldest son in good favour, was given the largest share of the inheritance: rule of Francia, Saxony, Nordgau, and parts of Alemannia. The two younger sons were confirmed in their kingdoms and gained additional territories; most of Bavaria and Alemmannia was given to Pepin, and Provence, Septimania, and parts of Burgundy were given to Louis. Charlemagne did not address the inheritance of the imperial title. The also provided that if any of the brothers predeceased Charlemagne, their sons would inherit their share; peace was urged among his descendants.\nConflict and diplomacy with the east.\nAfter his coronation, Charlemagne sought recognition of his imperial title from Constantinople. Several delegations were exchanged between Charlemagne and Irene in 802 and 803. According to the contemporary Byzantine chronicler Thophanes, Charlemagne made an offer of marriage to Irene which she was close to accepting. Irene was deposed and replaced by Nikephoros I, who was unwilling to recognise Charlemagne as emperor. The two empires conflicted over control of the Adriatic Sea (especially Istria and Veneto) several times during Nikephoros' reign. Charlemagne sent envoys to Constantinople in 810 to make peace, giving up his claims to Veneto. Nikephoros died in battle before the envoys could leave Constantinople but his son-in-law and successor Michael I confirmed the peace, sending his own envoys to Aachen to recognise Charlemagne as emperor. Charlemagne soon issued the first Frankish coins bearing his imperial title, although papal coins minted in Rome had used the title as early as 800.\nHe sent envoys and initiated diplomatic contact with the Abbasid caliph Harun al-Rashid during the 790s, due to their mutual interest in Spanish affairs. As an early sign of friendship, Charlemagne requested an elephant as a gift from Harun. Harun later provided an elephant named Abul-Abbas, which arrived at Aachen in 802. Harun also sought to undermine Charlemagne's relations with the Byzantines, with whom he was at war. As part of his outreach, Harun gave Charlemagne nominal rule of the Church of the Holy Sepulchre in Jerusalem and other gifts. According to Einhard, Charlemagne \"zealously strove to make friendships with kings beyond the seas\" in order \"that he might get some help and relief to the Christians living under their rule.\" A surviving administrative document, the Basel roll, shows the work done by his agents in Palestine in furtherance of this goal.\nHarun's death lead to a succession crisis and, under his successors, churches and synagogues were destroyed in the caliphate. Unable to intervene directly, Charlemagne sent specially-minted coins and arms to the eastern Christians to defend and restore their churches and monasteries. The coins with their inscriptions were also an important tool of imperial propaganda. Johannes Fried writes that deteriorating relations with Baghdad after Harun's death may have been the impetus for renewed negotiations with Constantinople which led to Charlemagne's peace with Michael in 811.\nAs emperor, Charlemagne became involved in a religious dispute between Eastern and Western Christians over the recitation of the Niceno-Constantinopolitan Creed, the fundamental statement of orthodox Christian belief. The original text of the creed, adopted at the Council of Constantinople, professed that the Holy Spirit proceeded from the Father. A tradition developed in Western Europe that the Holy Spirit proceeded from the Father \"and the Son\", inserting the Latin term into the creed. The difference did not cause significant conflict until 807, when Frankish monks in Bethlehem were denounced as heretics by a Greek monk for using the form. The Frankish monks appealed the dispute to Rome, where Pope Leo affirmed the text of the creed omitting the phrase and passed the report on to Charlemagne. Charlemagne summoned a council at Aachen in 809 which defended the use of , and sent the decision to Rome. Leo said that the Franks could maintain their tradition, but asserted that the canonical creed did not include . He commissioned two silver shields with the creed in Latin and Greek (omitting the ), which he hung in St. Peter's Basilica. Another product of the 809 Aachen council was the \"Handbook of 809\", an illustrated calendrical and astronomical compendium.\nWars with the Danes.\nScandinavia had been brought into contact with the Frankish world through Charlemagne's wars with the Saxons. Raids on Charlemagne's lands by the Danes began around 800. Charlemagne engaged in his final campaign in Saxony in 804, seizing Saxon territory east of the Elbe, removing its Saxon population, and giving the land to his Obotrite allies. The Danish king Gudfred, uneasy at the extension of Frankish power, offered to meet with Charlemagne to arrange peace and (possibly) hand over Saxons who had fled to him; the talks were unsuccessful.\nThe northern frontier was quiet until 808, when Gudfred and some allied Slavic tribes led an incursion into the Obotrite lands and extracted tribute from over half the territory. Charles the Younger led an army across the Elbe in response, but only attacked some of Gudfred's Slavic allies. Gudfred again attempted diplomatic overtures in 809, but no peace was apparently made. Danish pirates raided Frisia in 810, although it is uncertain if they were connected to Gudfred. Charlemagne sent an army to secure Frisia while he led a force against Gudfred, who had reportedly challenged the emperor to face him in battle. The battle never took place, since Gudfred was murdered by two of his own men before Charlemagne's arrival. Gudfred's nephew and successor Hemming immediately sued for peace, and a commission led by Charlemagne's cousin Wala reached a settlement with the Danes in 811. The Danes did not pose a threat for the remainder of Charlemagne's reign, but the effects of this war and their earlier expansion in Saxony helped set the stage for the intense Viking raids across Europe later in the ninth century.\nFinal years and death.\nThe Carolingian dynasty experienced a number of losses in 810 and 811, when Charlemagne's sister Gisela, his daughter Rotrude, and his sons Pepin the Hunchback, Pepin of Italy, and Charles the Younger died. The deaths of Charles and Pepin of Italy left Charlemagne's earlier plans for succession in disarray. He declared Pepin of Italy's son Bernard ruler of Italy and made his own only surviving son, Louis, heir to the rest of the empire. Charlemagne also made a new will detailing the disposal of his property at his death, with bequests to the church, his children, and his grandchildren. Einhard (possibly relying on tropes from Suetonius's \"The Twelve Caesars\") says that Charlemagne viewed the deaths of his family members, his fall from a horse, astronomical phenomena, and the collapse of part of the palace in his last years as signs of his impending death. Charlemagne continued to govern with energy during his final year, ordering bishops to assemble in five ecclesiastical councils. These culminated in a large assembly at Aachen, where Charlemagne crowned Louis as his co-emperor and Bernard as king in a ceremony on 11 September 813.\nCharlemagne became ill in the autumn of 813 and spent his last months praying, fasting, and studying the gospels. He developed pleurisy, and was bedridden for seven days before dying on the morning of 28 January 814. Thegan, a biographer of Louis, records the emperor's last words as \"Into your hands, Lord, I commend my spirit\" (quoting from ). Charlemagne's body was prepared and buried in the chapel at Aachen by his daughters and palace officials that day. Louis arrived at Aachen thirty days after his father's death, making a formal and taking charge of the palace and the empire. Charlemagne's remains were exhumed by Holy Roman Emperor Frederick Barbarossa in 1165, and reinterred in a new casket by Frederick II in 1215.\nLegacy.\nPolitical legacy.\nThe stability and peace of Charlemagne's reign did not long outlive him. Louis' reign was marked by strife, including a number of rebellions by his sons. After Louis' death, the empire was divided among his sons into West, East, and Middle Francia by the Treaty of Verdun. Middle Francia was divided several more times over the course of subsequent generations. Carolingians would rulewith some interruptionsin East Francia (later the Kingdom of Germany) until 911, and in West Francia (which would become France) until 987. After 887, the imperial title was held sporadically by a series of non-dynastic Italian rulers before it lapsed in 924. The East Frankish king Otto the Great conquered Italy, and was crowned emperor in 962. By this time, the eastern and western parts of Charlemagne's former empire had already developed distinct languages and cultures. Otto founded (or re-established) the Holy Roman Empire, which would last until its dissolution in 1806, during the Napoleonic Wars.\nAccording to historian Jennifer Davis, Charlemagne \"invented medieval rulership\" and his influence can be seen at least into the nineteenth century. Charlemagne is often known as \"the father of Europe\" because of the influence of his reign and the legacy he left across the large area of the continent. The political structures he established remained in place through his Carolingian successors, and continued to exert influence into the eleventh century.\nCharlemagne was an ancestor of several European ruling houses, including the Capetian dynasty, the Ottonian dynasty, the House of Luxembourg, and the House of Ivrea. The Ottonians and Capetians, direct successors of the Carolingans, drew on the legacy of Charlemagne to bolster their legitimacy and prestige; the Ottonians and their successors held their German coronations in Aachen through the Middle Ages. The marriage of Philip II of France to Isabella of Hainault (a direct descendant of Charlemagne) was seen as a sign of increased legitimacy for their son, Louis VIII, and the French kings' association with Charlemagne's legacy was stressed until the monarchy's end. German and French rulers, such as Frederick Barbarossa and Napoleon, cited the influence of Charlemagne and associated themselves with him. Both German and French monarchs considered themselves as successors of Charlemagne, enumerating him as \"Charles I\" in their regnal lists.\nThe city of Aachen has, since 1949, awarded an international prize (the Karlspreis der Stadt Aachen) in honour of Charlemagne. It is awarded annually to those who promote European unity. Recipients of the prize include Richard von Coudenhove-Kalergi (founder of the pan-European movement), Alcide De Gasperi, and Winston Churchill.\nCarolingian Renaissance.\nContacts with the wider Mediterranean world through Spain and Italy, the influx of foreign scholars at court, and the relative stability and length of Charlemagne's reign led to a cultural revival known as the Carolingian Renaissance. Although the beginnings of this revival can be seen under his predecessors, Charles Martel and Pepin, Charlemagne took an active and direct role in shaping intellectual life which led to the revival's zenith. Charlemagne promoted learning as a matter of policy and direct patronage, with the aim of creating a more effective clergy. The \"Admonitio generalis\" and \"Epistola de litteris colendis\" outlined his policies and aims for education.\nIntellectual life at court was dominated by Irish, Anglo-Saxon, Visigothic and Italian scholars, including Dungal of Bobbio, Alcuin of York, Theodulf of Orl\u00e9ans, and Peter of Pisa; Franks such as Einhard and Angelbert also made substantial contributions. Aside from the intellectual activity at the palace, Charlemagne promoted ecclesiastical schools and publicly funded schools for the children of the elite and future clergy. Students learned basic Latin literacy and grammar, arithmetic, and other subjects of the medieval liberal arts. From their education, it was expected that even rural priests could provide their parishioners with basic instruction in religious matters and (possibly) the literacy required for worship. Latin was standardised and its use brought into territories well beyond the former Roman Empire, forming a second language community of speakers and writers and sustaining Latin creativity in the Middle Ages.\nCarolingian authors produced extensive works, including legal treatises, histories, poetry, and religious texts. Scriptoria in monasteries and cathedrals focused on copying new and old works, producing an estimated 90,000 manuscripts during the ninth century. The Carolingian minuscule script was developed and popularised in medieval copying, influencing Renaissance and modern typefaces. Scholar John J. Contreni considers the educational and learning revival under Charlemagne and his successors \"one of the most durable and resilient elements of the Carolingian legacy\".\nMemory and historiography.\nCharlemagne was a frequent subject of, and inspiration for, medieval writers after his death. Einhard's \"Vita Karoli Magni\", according to Johannes Fired, \"can be said to have revived the defunct literary genre of the secular biography.\" Einhard drew on classical sources, such as Suetonius' \"The Twelve Caesars\", the orations of Cicero, and Tacitus' \"Agricola\" to frame his work's structure and style. The Carolingian period also saw a revival of the mirrors for princes genre. The author of the Latin poem \"Visio Karoli Magni\", written , uses facts (apparently from Einhard) and his own observations on the decline of Charlemagne's family after their civil wars later in the ninth century as the bases of a visionary tale about Charles meeting a prophetic spectre in a dream. Notker's \"Gesta Karoli Magni\", written for Charlemagne's great-grandson Charles the Fat, presents moral anecdotes (\"exempla\") to highlight the emperor's qualities as a ruler.\nCharlemagne, as a figure of myth and emulation, grew over the centuries; Matthias Becher writes that over 1,000 legends are recorded about him, far outstripping subsequent emperors and kings. Later medieval writers depicted Charlemagne as a crusader and Christian warrior. Charlemagne is the main figure of the medieval literary cycle known as the Matter of France. Works in this cycle, which originated during the Crusades, centre on characterisations of the emperor as a leader of Christian knights in wars against Muslims. The cycle includes (epic poems) such as the \"Song of Roland\" and chronicles such as the \"Historia Caroli Magni\", also known as the \"(Pseudo-)Turpin Chronicle\". Charlemagne was depicted as one of the Nine Worthies, a fixture in medieval literature and art as an exemplar of a Christian king. Despite his central role in these legends, author Thomas Bulfinch noted that \"romancers represent him as often weak and passionate, the victim of treacherous counsellors, and at the mercy of turbulent barons, on whose prowess he depends for the maintenance of his throne.\"\nAttention to Charlemagne became more scholarly in the early modern period as Eindhard's \"Vita\" and other sources began to be published. Political philosophers debated his legacy; Montesquieu viewed him as the first constitutional monarch and protector of freemen, but Voltaire saw him as a despotic ruler and representative of the medieval period as a Dark Age. As early as the sixteenth century, debate between German and French writers began about Charlemagne's \"nationality\". These contrasting portraits\u2014a French Charlemagne versus a German \"Karl der Gro\u00dfe\"\u2014became especially pronounced during the nineteenth century with Napoleon's use of Charlemagne's legacy and the rise of German nationalism. German historiography and popular perception focused on the Massacre of Verden, emphasised with Charlemagne as the \"butcher\" of the Germanic Saxons or downplayed as an unfortunate part of the legacy of a great German ruler. Propaganda in Nazi Germany initially portrayed Charlemagne as an enemy of Germany, a French ruler who worked to take away the freedom and native religion of the German people. This quickly shifted as Adolf Hitler endorsed a portrait of Charlemagne as a great unifier of disparate German tribes into a common nation, allowing Hitler to co-opt Charlemagne's legacy as an ideological model for his expansionist policies.\nHistoriography after World War II focused on Charlemagne as \"the father of Europe\" rather than a nationalistic figure, a view first advanced during the nineteenth century by German romantic philosopher Friedrich Schlegel. This view has led to Charlemagne's adoption as a political symbol of European integration. Modern historians increasingly place Charlemagne in the context of the wider Mediterranean world, following the work of Henri Pirenne.\nReligious influence and veneration.\nCharlemagne gave much attention to religious and ecclesiastical affairs, holding 23 synods during his reign. His synods were called to address specific issues at particular times, but generally dealt with church administration and organisation, education of the clergy, and the proper forms of liturgy and worship. Charlemagne used the Christian faith as a unifying factor in the realm and, in turn, worked to impose unity on the church. He implemented an edited version of the \"Dionysio-Hadriana\" book of canon law acquired from Pope Adrian, required use of the Rule of St. Benedict in monasteries throughout the empire, and promoted a standardised liturgy adapted from the rites of the Roman Church to conform with Frankish practices. Carolingian policies promoting unity did not eliminate the diverse practices throughout the empire, but created a shared ecclesiastical identity\u2014according to Rosamond McKitterick, \"unison, not unity.\"\nThe condition of all his subjects as a \"Christian people\" was an important concern. Charlemagne's policies encouraged preaching to the laity, particularly in vernacular languages they would understand. He believed it essential to be able to recite the Lord's Prayer and the Apostles' Creed, and made efforts to ensure that the clergy taught them and other basics of Christian morality.\nThomasF.X.Noble writes that the efforts of Charlemagne and his successors to standardise Christian doctrine and practices and harmonise Frankish practices were essential steps in the development of Christianity in Europe, and the Roman Catholic or Latin Church \"as a historical phenomenon, not as a theological or ecclesiological one, is a Carolingian construction.\" He says that the medieval European concept of Christendom as an overarching community of Western Christians, rather than a collection of local traditions, is the result of Carolingian policies and ideology. Charlemagne's doctrinal policies promoting the use of and opposing the Second Council of Nicea were key steps in the growing divide between Western and Eastern Christianity.\nEmperor Otto III attempted to have Charlemagne canonised in 1000. In 1165, Frederick Barbarossa persuaded Antipope Paschal III to elevate Charlemagne to sainthood. Since Paschal's acts were not considered valid, Charlemagne was not recognised as a saint by the Holy See. Despite this lack of official recognition, his cult was observed in Aachen, Reims, Frankfurt, Zurich and Regensburg, and he has been venerated in France since the reign of Charles V.\nCharlemagne also drew attention from figures of the Protestant Reformation, with Martin Luther criticising his apparent subjugation to the papacy by accepting his coronation from Leo. John Calvin and other Protestant thinkers viewed him as a forerunner of the Reformation, however, noting the \"Libri Carolini\" condemnation of the worship of images and relics and conflicts by Charlemagne and his successors with the temporal power of the popes.\nWives, concubines, and children.\nWives and their children\nConcubines and their children\nCharlemagne had at least twenty children with his wives and other partners. After the death of his wife Luitgard in 800, he did not remarry, but had children with unmarried partners. He was determined that all his children, including his daughters, should receive an education in the liberal arts. His children were taught in accordance with their aristocratic status, which included training in riding and weaponry for his sons, and embroidery, spinning and weaving for his daughters.\nRosamond McKitterick writes that Charlemagne exercised \"a remarkable degree of patriarchal control\u00a0... over his progeny,\" noting that only a handful of his children and grandchildren were raised outside his court. Pepin of Italy and Louis reigned as kings from childhood and lived at their courts. Careers in the church were arranged for his illegitimate sons. His daughters were resident at court or at Chelles Abbey (where Charlemagne's sister was abbess), and those at court may have fulfilled the duties of queen after 800.\nLouis and Pepin of Italy married and had children during their father's lifetime, and Charlemagne brought Pepin's daughters into his household after Pepin's death. Rotrude had been betrothed to Emperor Constantine VI, but the betrothal was ended. None of Charlemagne's daughters married, although several had children with unmarried partners. Bertha had two sons, Nithard and Hartnid, with Charlemagne's courtier Angilbert; Rotrude had a son named Louis, possibly with Count Rorgon; and Hiltrude had a son named Richbod, possibly with a count named Richwin. The issued by Charlemagne in 806 provided that his legitimate daughters be allowed to marry or become nuns after his death. Theodrada entered a convent, but the decisions of his other daughters are unknown.\nAppearance and iconography.\nEinhard gives a first-hand description of Charlemagne's appearance later in life:\nCharlemagne's tomb was opened in 1861 by scientists who reconstructed his skeleton and measured it at in length, roughly equivalent to Einhard's seven feet. A 2010 estimate of his height from an X-ray and CT scan of his tibia was ; this puts him in the 99th percentile of height for his period, given that average male height of his time was . The width of the bone suggested that he was slim.\nCharlemagne wore his hair short, abandoning the Merovingian tradition of long-haired monarchs. He had a moustache (possibly imitating the Ostrogothic king Theoderic the Great), in contrast with the bearded Merovingian kings; future Carolingian monarchs would adopt this style. Paul Dutton notes the ubiquitous crown in portraits of Charlemagne and other Carolingian rulers, replacing the earlier Merovingian long hair. A ninth-century statuette depicts Charlemagne or his grandson, Charles the Bald and shows the subject as moustachioed with short hair; this also appears on contemporary coinage.\nBy the twelfth century, Charlemagne was described as bearded rather than moustachioed in literary sources such as the \"Song of Roland\", the \"Pseudo-Turpin Chronicle\", and other works in Latin, French, and German. The \"Pseudo-Turpin\" uniquely says that his hair was brown. Later art and iconography of Charlemagne followed suit, generally depicting him in a later medieval style as bearded with longer hair."}
{"id": "5315", "revid": "48720609", "url": "https://en.wikipedia.org/wiki?curid=5315", "title": "Character encodings in HTML", "text": "While Hypertext Markup Language (HTML) has been in use since 1991, HTML 4.0 from December 1997 was the first standardized version where international characters were given reasonably complete treatment. When an HTML document includes special characters outside the range of seven-bit ASCII, two goals are worth considering: the information's integrity, and universal browser display.\nSpecifying the document's character encoding.\nThere are two general ways to specify which character encoding is used in the document.\nFirst, the web server can include the character encoding or \"codice_1\" in the Hypertext Transfer Protocol (HTTP) codice_2 header, which would typically look like this:\nContent-Type: text/html; charset=utf-8\nThis method gives the HTTP server a convenient way to alter document's encoding according to content negotiation; certain HTTP server software can do it, for example Apache with the module codice_3.\nSecond, a declaration can be included within the document itself.\nFor HTML it is possible to include this information inside the codice_4 element near the top of the document:\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"&gt;\nHTML5 also allows the following syntax to mean exactly the same:\n&lt;meta charset=\"utf-8\"&gt;\nXHTML documents have a third option: to express the character encoding via XML declaration, as follows:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\nWith this second approach, because the character encoding cannot be known until the declaration is parsed, there is a problem knowing which character encoding is used in the document up to and including the declaration itself. If the character encoding is an ASCII extension then the content up to and including the declaration itself should be pure ASCII and this will work correctly. For character encodings that are not ASCII extensions (i.e. not a superset of ASCII), such as UTF-16BE and UTF-16LE, a processor of HTML, such as a web browser, should be able to parse the declaration in some cases through the use of heuristics.\nEncoding detection algorithm.\nAs of HTML5 the recommended charset is UTF-8. An \"encoding sniffing algorithm\" is defined in the specification to determine the character encoding of the document based on multiple sources of input, including:\nCharacters outside of the printable ASCII range (32 to 126) usually appear incorrectly. This presents few problems for English-speaking users, but other languages regularly\u2014in some cases, always\u2014require characters outside that range. In Chinese, Japanese, and Korean (CJK) language environments where there are several different multi-byte encodings in use, auto-detection is also often employed. Finally, browsers usually permit the user to override \"incorrect\" charset label manually as well.\nIt is increasingly common for multilingual websites and websites in non-Western languages to use UTF-8, which allows use of the same encoding for all languages. UTF-16 or UTF-32, which can be used for all languages as well, are less widely used because they can be harder to handle in programming languages that assume a byte-oriented ASCII superset encoding, and they are less efficient for text with a high frequency of ASCII characters, which is usually the case for HTML documents.\nSuccessful viewing of a page is not necessarily an indication that its encoding is specified correctly. If the page's creator and reader are both assuming some platform-specific character encoding, and the server does not send any identifying information, then the reader will nonetheless see the page as the creator intended, but other readers on different platforms or with different native languages will not see the page as intended.\nPermitted encodings.\nThe WHATWG Encoding Standard, referenced by recent HTML standards (the current WHATWG HTML Living Standard, as well as the formerly competing W3C HTML 5.0 and 5.1) specifies a list of encodings which browsers must support. The HTML standards forbid support of other encodings. The Encoding Standard further stipulates that new formats, new protocols (even when existing formats are used) and authors of new documents are required to use UTF-8 exclusively.\nBesides UTF-8, the following encodings are explicitly listed in the HTML standard itself, with reference to the Encoding Standard:\nThe following additional encodings are listed in the Encoding Standard, and support for them is therefore also required:\nThe following encodings are listed as explicit examples of forbidden encodings:\nThe standard also defines a \"replacement\" decoder, which maps all content labelled as certain encodings to the replacement character (\ufffd), refusing to process it at all. This is intended to prevent attacks (e.g. cross site scripting) which may exploit a difference between the client and server in what encodings are supported in order to mask malicious content. Although the same security concern applies to ISO-2022-JP and UTF-16, which also allow sequences of ASCII bytes to be interpreted differently, this approach was not seen as feasible for them since they are comparatively more frequently used in deployed content. The following encodings receive this treatment:\nCharacter references.\nIn addition to native character encodings, characters can also be encoded as \"character references\", which can be \"numeric character references\" (decimal or hexadecimal) or \"character entity references\". Character entity references are also sometimes referred to as \"named entities\", or \"HTML entities\" for HTML. HTML's usage of character references derives from SGML.\nHTML character references.\nA \"numeric character reference\" in HTML refers to a character by its Universal Character Set/Unicode \"code point\", and uses the format\nor\nwhere \"nnnn\" is the code point in decimal form, and \"hhhh\" is the code point in hexadecimal form. The \"x\" must be lowercase in XML documents. The \"nnnn\" or \"hhhh\" may be any number of digits and may include leading zeros. The \"hhhh\" may mix uppercase and lowercase, though uppercase is the usual style.\nNot all web browsers or email clients used by receivers of HTML documents, or text editors used by authors of HTML documents, will be able to render all HTML characters. Most modern software is able to display most or all of the characters for the user's language, and will draw a box or other clear indicator for characters they cannot render.\nFor codes from 0 to 127, the original 7-bit ASCII standard set, most of these characters can be used without a character reference. Codes from 160 to 255 can all be created using character entity names. Only a few higher-numbered codes can be created using entity names, but all can be created by decimal number character reference.\nCharacter entity references can also have the format codice_7 where \"name\" is a case-sensitive alphanumeric string. For example, \"\u03bb\" can also be encoded as codice_8 in an HTML document. The character entity references codice_9, codice_10, codice_11 and codice_12 are predefined in HTML and SGML, because codice_13, codice_14, codice_15 and codice_16 are already used to delimit markup. This notably did not include XML's codice_17 (') entity prior to HTML5. For a list of all named HTML character entity references along with the versions in which they were introduced, see List of XML and HTML character entity references.\nUnnecessary use of HTML character references may significantly reduce HTML readability. If the character encoding for a web page is chosen appropriately, then HTML character references are usually only required for markup delimiting characters as mentioned above, and for a few special characters (or none at all if a native Unicode encoding like UTF-8 is used). Incorrect HTML entity escaping may also open up security vulnerabilities for injection attacks such as cross-site scripting. If HTML attributes are left unquoted, certain characters, most importantly whitespace, such as space and tab, must be escaped using entities. Other languages related to HTML have their own methods of escaping characters.\nXML character references.\nUnlike traditional HTML with its large range of character entity references, in XML there are only five predefined character entity references. These are used to escape characters that are markup sensitive in certain contexts:\nAll other character entity references have to be defined before they can be used. For example, use of codice_18 (which gives \u00e9, Latin lower-case E with acute accent, U+00E9 in Unicode) in an XML document will generate an error unless the entity has already been defined. XML also requires that the codice_19 in hexadecimal numeric references be in lowercase: for example codice_20 rather than codice_21. XHTML, which is an XML application, supports the HTML entity set, along with XML's predefined entities."}
{"id": "5318", "revid": "45789152", "url": "https://en.wikipedia.org/wiki?curid=5318", "title": "Computer/Time-sharing", "text": ""}
{"id": "5319", "revid": "45789152", "url": "https://en.wikipedia.org/wiki?curid=5319", "title": "Computer/Multitasking", "text": ""}
{"id": "5320", "revid": "48838494", "url": "https://en.wikipedia.org/wiki?curid=5320", "title": "Carbon nanotube", "text": "A carbon nanotube (CNT) is a tube made of carbon with a diameter in the nanometre range (nanoscale). They are one of the allotropes of carbon. Two broad classes of carbon nanotubes are recognized:\nCarbon nanotubes can exhibit remarkable properties, such as exceptional tensile strength and thermal conductivity because of their nanostructure and strength of the bonds between carbon atoms. Some SWCNT structures exhibit high electrical conductivity while others are semiconductors. In addition, carbon nanotubes can be chemically modified. These properties are expected to be valuable in many areas of technology, such as electronics, optics, composite materials (replacing or complementing carbon fibres), nanotechnology (including nanomedicine), and other applications of materials science.\nThe predicted properties for SWCNTs were tantalising, but a path to synthesising them was lacking until 1993, when Iijima and Ichihashi at NEC, and Bethune and others at IBM independently discovered that co-vaporising carbon and transition metals such as iron and cobalt could specifically catalyse SWCNT formation. These discoveries triggered research that succeeded in greatly increasing the efficiency of the catalytic production technique, and led to an explosion of work to characterise and find applications for SWCNTs.\nHistory.\nThe true identity of the discoverers of carbon nanotubes is a subject of some controversy. A 2006 editorial written by Marc Monthioux and Vladimir Kuznetsov in the journal \"Carbon\" described the origin of the carbon nanotube. A large percentage of academic and popular literature attributes the discovery of hollow, nanometre-size tubes composed of graphitic carbon to Sumio Iijima of NEC in 1991. His paper initiated a flurry of excitement and could be credited with inspiring the many scientists now studying applications of carbon nanotubes. Though Iijima has been given much of the credit for discovering carbon nanotubes, it turns out that the timeline of carbon nanotubes goes back much further than 1991.\nIn 1952, L. V. Radushkevich and V. M. Lukyanovich published clear images of 50-nanometre diameter tubes made of carbon in the \"Journal of Physical Chemistry Of Russia\". This discovery was largely unnoticed, as the article was published in Russian, and Western scientists' access to Soviet press was limited during the Cold War. Monthioux and Kuznetsov mentioned in their \"Carbon\" editorial: \nIn 1976, Morinobu Endo of CNRS observed hollow tubes of rolled up graphite sheets synthesised by a chemical vapour-growth technique. The first specimens observed would later come to be known as single-walled carbon nanotubes (SWNTs). Endo, in his early review of vapor-phase-grown carbon fibers (VPCF), also reminded us that he had observed a hollow tube, linearly extended with parallel carbon layer faces near the fiber core. This appears to be the observation of multi-walled carbon nanotubes at the center of the fiber. The mass-produced MWCNTs today are strongly related to the VPGCF developed by Endo. In fact, they call it the \"Endo-process\", out of respect for his early work and patents. In 1979, John Abrahamson presented evidence of carbon nanotubes at the 14th Biennial Conference of Carbon at Pennsylvania State University. The conference paper described carbon nanotubes as carbon fibers that were produced on carbon anodes during arc discharge. A characterization of these fibers was given, as well as hypotheses for their growth in a nitrogen atmosphere at low pressures.\nIn 1981, a group of Soviet scientists published the results of chemical and structural characterization of carbon nanoparticles produced by a thermocatalytic disproportionation of carbon monoxide. Using TEM images and XRD patterns, the authors suggested that their \"carbon multi-layer tubular crystals\" were formed by rolling graphene layers into cylinders. They speculated that via this rolling, many different arrangements of graphene hexagonal nets are possible. They suggested two such possible arrangements: a circular arrangement (armchair nanotube); and a spiral, helical arrangement (chiral tube).\nIn 1987, Howard G. Tennent of Hyperion Catalysis was issued a U.S. patent for the production of \"cylindrical discrete carbon fibrils\" with a \"constant diameter between about 3.5 and about 70\u00a0nanometers..., length 102 times the diameter, and an outer region of multiple essentially continuous layers of ordered carbon atoms and a distinct inner core...\"\nHelping to create the initial excitement associated with carbon nanotubes were Iijima's 1991 discovery of multi-walled carbon nanotubes in the insoluble material of arc-burned graphite rods; and Mintmire, Dunlap, and White's independent prediction that if single-walled carbon nanotubes could be made, they would exhibit remarkable conducting properties. Nanotube research accelerated greatly following the independent discoveries by Iijima and Ichihashi at NEC and Bethune \"et al.\" at IBM of methods to specifically produce \"single-walled\" carbon nanotubes by adding transition-metal catalysts to the carbon in an arc discharge. Thess et al. refined this catalytic method by vaporizing the carbon/transition-metal combination in a high-temperature furnace, which greatly improved the yield and purity of the SWNTs and made them widely available for characterization and application experiments. The arc discharge technique, well known to produce the famed Buckminsterfullerene, thus played a role in the discoveries of both multi- and single-wall nanotubes, extending the run of serendipitous discoveries relating to fullerenes. The discovery of nanotubes remains a contentious issue. Many believe that Iijima's report in 1991 is of particular importance because it brought carbon nanotubes into the awareness of the scientific community as a whole.\nIn 2020, during an archaeological excavation of Keezhadi in Tamil Nadu, India, ~2600-year-old pottery was discovered whose coatings appear to contain carbon nanotubes. The robust mechanical properties of the nanotubes are partially why the coatings have lasted for so many years, say the scientists.\nStructure of SWCNTs.\nBasic details.\nThe structure of an ideal (infinitely long) single-walled carbon nanotube is that of a regular hexagonal lattice drawn on an infinite cylindrical surface, whose vertices are the positions of the carbon atoms. Since the length of the carbon-carbon bonds is fairly fixed, there are constraints on the diameter of the cylinder and the arrangement of the atoms on it.\nIn the study of nanotubes, one defines a zigzag path on a graphene-like lattice as a path that turns 60 degrees, alternating left and right, after stepping through each bond. It is also conventional to define an armchair path as one that makes two left turns of 60 degrees followed by two right turns every four steps. On some carbon nanotubes, there is a closed zigzag path that goes around the tube. One says that the tube is of the zigzag type or configuration, or simply is a zigzag nanotube. If the tube is instead encircled by a closed armchair path, it is said to be of the armchair type, or an armchair nanotube. An infinite nanotube that is of one type consists entirely of closed paths of that type, connected to each other.\nThe zigzag and armchair configurations are not the only structures that a single-walled nanotube can have. To describe the structure of a general infinitely long tube, one should imagine it being sliced open by a cut parallel to its axis, that goes through some atom \"A\", and then unrolled flat on the plane, so that its atoms and bonds coincide with those of an imaginary graphene sheet\u2014more precisely, with an infinitely long strip of that sheet. The two halves of the atom \"A\" will end up on opposite edges of the strip, over two atoms \"A1\" and \"A2\" of the graphene. The line from \"A1\" to \"A2\" will correspond to the circumference of the cylinder that went through the atom \"A\", and will be perpendicular to the edges of the strip. In the graphene lattice, the atoms can be split into two classes, depending on the directions of their three bonds. Half the atoms have their three bonds directed the same way, and half have their three bonds rotated 180 degrees relative to the first half. The atoms \"A1\" and \"A2\", which correspond to the same atom \"A\" on the cylinder, must be in the same class. It follows that the circumference of the tube and the angle of the strip are not arbitrary, because they are constrained to the lengths and directions of the lines that connect pairs of graphene atoms in the same class.\nLet u and v be two linearly independent vectors that connect the graphene atom \"A1\" to two of its nearest atoms with the same bond directions. That is, if one numbers consecutive carbons around a graphene cell with C1 to C6, then u can be the vector from C1 to C3, and v be the vector from C1 to C5. Then, for any other atom \"A2\" with same class as \"A1\", the vector from \"A1\" to \"A2\" can be written as a linear combination \"n\" u + \"m\" v, where \"n\" and \"m\" are integers. And, conversely, each pair of integers (\"n\",\"m\") defines a possible position for \"A2\". Given \"n\" and \"m\", one can reverse this theoretical operation by drawing the vector w on the graphene lattice, cutting a strip of the latter along lines perpendicular to w through its endpoints \"A1\" and \"A2\", and rolling the strip into a cylinder so as to bring those two points together. If this construction is applied to a pair (\"k\",0), the result is a zigzag nanotube, with closed zigzag paths of 2\"k\" atoms. If it is applied to a pair (\"k\",\"k\"), one obtains an armchair tube, with closed armchair paths of 4\"k\" atoms.\nTypes.\nThe structure of the nanotube is not changed if the strip is rotated by 60 degrees clockwise around \"A1\" before applying the hypothetical reconstruction above. Such a rotation changes the corresponding pair (\"n\",\"m\") to the pair (\u22122\"m\",\"n\"+\"m\"). It follows that many possible positions of \"A2\" relative to \"A1\" \u2014 that is, many pairs (\"n\",\"m\") \u2014 correspond to the same arrangement of atoms on the nanotube. That is the case, for example, of the six pairs (1,2), (\u22122,3), (\u22123,1), (\u22121,\u22122), (2,\u22123), and (3,\u22121). In particular, the pairs (\"k\",0) and (0,\"k\") describe the same nanotube geometry. These redundancies can be avoided by considering only pairs (\"n\",\"m\") such that \"n\" &gt; 0 and \"m\" \u2265 0; that is, where the direction of the vector w lies between those of u (inclusive) and v (exclusive). It can be verified that every nanotube has exactly one pair (\"n\",\"m\") that satisfies those conditions, which is called the tube's type. Conversely, for every type there is a hypothetical nanotube. In fact, two nanotubes have the same type if and only if one can be conceptually rotated and translated so as to match the other exactly. Instead of the type (\"n\",\"m\"), the structure of a carbon nanotube can be specified by giving the length of the vector w (that is, the circumference of the nanotube), and the angle \"\u03b1\" between the directions of u and w,\nmay range from 0 (inclusive) to 60 degrees clockwise (exclusive). If the diagram is drawn with u horizontal, the latter is the tilt of the strip away from the vertical.\nChirality and mirror symmetry.\nA nanotube is chiral if it has type (\"n\",\"m\"), with \"m\" &gt; 0 and \"m\" \u2260 \"n\"; then its enantiomer (mirror image) has type (\"m\",\"n\"), which is different from (\"n\",\"m\"). This operation corresponds to mirroring the unrolled strip about the line \"L\" through \"A1\" that makes an angle of 30 degrees clockwise from the direction of the u vector (that is, with the direction of the vector u+v). The only types of nanotubes that are achiral are the (\"k\",0) \"zigzag\" tubes and the (\"k\",\"k\") \"armchair\" tubes. If two enantiomers are to be considered the same structure, then one may consider only types (\"n\",\"m\") with 0 \u2264 \"m\" \u2264 \"n\" and \"n\" &gt; 0. Then the angle \"\u03b1\" between u and w, which may range from 0 to 30 degrees (inclusive both), is called the \"chiral angle\" of the nanotube.\nCircumference and diameter.\nFrom \"n\" and \"m\" one can also compute the circumference \"c\", which is the length of the vector w, which turns out to be:\nin picometres. The diameter formula_2 of the tube is then formula_3, that is\nalso in picometres. (These formulas are only approximate, especially for small \"n\" and \"m\" where the bonds are strained; and they do not take into account the thickness of the wall.)\nThe tilt angle \"\u03b1\" between u and w and the circumference \"c\" are related to the type indices \"n\" and \"m\" by:\nwhere arg(\"x\",\"y\") is the clockwise angle between the \"X\"-axis and the vector (\"x\",\"y\"); a function that is available in many programming languages as codice_1(\"y\",\"x\"). Conversely, given \"c\" and \"\u03b1\", one can get the type (\"n\",\"m\") by the formulas:\nwhich must evaluate to integers.\nPhysical limits.\nNarrowest examples.\nIf \"n\" and \"m\" are too small, the structure described by the pair (\"n\",\"m\") will describe a molecule that cannot be reasonably called a \"tube\", and may not even be stable. For example, the structure theoretically described by the pair (1,0) (the limiting \"zigzag\" type) would be just a chain of carbons. That is a real molecule, the carbyne; which has some characteristics of nanotubes (such as orbital hybridization, high tensile strength, etc.) \u2014 but has no hollow space, and may not be obtainable as a condensed phase. The pair (2,0) would theoretically yield a chain of fused 4-cycles; and (1,1), the limiting \"armchair\" structure, would yield a chain of bi-connected 4-rings. These structures may not be realizable.\nThe thinnest carbon nanotube proper is the armchair structure with type (2,2), which has a diameter of 0.3\u00a0nm. This nanotube was grown inside a multi-walled carbon nanotube. Assigning of the carbon nanotube type was done by a combination of high-resolution transmission electron microscopy (HRTEM), Raman spectroscopy, and density functional theory (DFT) calculations.\nThe thinnest \"freestanding\" single-walled carbon nanotube is about 0.43\u00a0nm in diameter. Researchers suggested that it can be either (5,1) or (4,2) SWCNT, but the exact type of the carbon nanotube remains questionable. (3,3), (4,3), and (5,1) carbon nanotubes (all about 0.4\u00a0nm in diameter) were unambiguously identified using aberration-corrected high-resolution transmission electron microscopy inside double-walled CNTs.\nLength.\nThe observation of the \"longest\" carbon nanotubes grown so far, around 0.5\u00a0metre (550\u00a0mm) long, was reported in 2013. These nanotubes were grown on silicon substrates using an improved chemical vapor deposition (CVD) method and represent electrically uniform arrays of single-walled carbon nanotubes.\nThe \"shortest\" carbon nanotube can be considered to be the organic compound cycloparaphenylene, which was synthesized in 2008 by Ramesh Jasti. Other small molecule carbon nanotubes have been synthesized since.\nDensity.\nThe \"highest density\" of CNTs was achieved in 2013, grown on a conductive titanium-coated copper surface that was coated with co-catalysts cobalt and molybdenum at lower than typical temperatures of 450\u00a0\u00b0C. The tubes averaged a height of 380\u00a0nm and a mass density of 1.6\u2009g cm\u22123. The material showed ohmic conductivity (lowest resistance ~22\u2009k\u03a9).\nVariants.\nThere is no consensus on some terms describing carbon nanotubes in the scientific literature: both \"-wall\" and \"-walled\" are being used in combination with \"single\", \"double\", \"triple\", or \"multi\", and the letter C is often omitted in the abbreviation, for example, multi-walled carbon nanotube (MWNT). The International Standards Organization typically uses \"single-walled carbon nanotube (SWCNT)\" or \"multi-walled carbon nanotube (MWCNT)\" in its documents.\nMulti-walled.\nMulti-walled nanotubes (MWNTs) consist of multiple rolled layers (concentric tubes) of graphene. There are two models that can be used to describe the structures of multi-walled nanotubes. In the \"Russian Doll\" model, sheets of graphite are arranged in concentric cylinders, e.g., a (0,8) single-walled nanotube (SWNT) within a larger (0,17) single-walled nanotube. In the \"Parchment\" model, a single sheet of graphite is rolled in around itself, resembling a scroll of parchment or a rolled newspaper. The interlayer distance in multi-walled nanotubes is close to the distance between graphene layers in graphite, approximately 3.4 \u00c5. The Russian Doll structure is observed more commonly. Its individual shells can be described as SWNTs, which can be metallic or semiconducting. Because of statistical probability and restrictions on the relative diameters of the individual tubes, one of the shells, and thus the whole MWNT, is usually a zero-gap metal.\nDouble-walled carbon nanotubes (DWNTs) form a special class of nanotubes because their morphology and properties are similar to those of SWNTs but they are more resistant to attacks by chemicals. This is especially important when it is necessary to graft chemical functions to the surface of the nanotubes (functionalization) to add properties to the CNT. Covalent functionalization of SWNTs will break some C=C double bonds, leaving \"holes\" in the structure on the nanotube and thus modifying both its mechanical and electrical properties. In the case of DWNTs, only the outer wall is modified. DWNT synthesis on the gram-scale by the CCVD technique was first proposed in 2003 from the selective reduction of oxide solutions in methane and hydrogen.\nThe telescopic motion ability of inner shells, allowing them to act as low-friction, low-wear nanobearings and nanosprings, may make them a desirable material in nanoelectromechanical systems (NEMS) . The retraction force that occurs to telescopic motion is caused by the Lennard-Jones interaction between shells, and its value is about 1.5\u00a0nN.\nJunctions and crosslinking.\n Junctions between two or more nanotubes have been widely discussed theoretically. Such junctions are quite frequently observed in samples prepared by arc discharge as well as by chemical vapor deposition. The electronic properties of such junctions were first considered theoretically by Lambin et al., who pointed out that a connection between a metallic tube and a semiconducting one would represent a nanoscale heterojunction. Such a junction could therefore form a component of a nanotube-based electronic circuit. The adjacent image shows a junction between two multiwalled nanotubes.\nJunctions between nanotubes and graphene have been considered theoretically and studied experimentally. Nanotube-graphene junctions form the basis of pillared graphene, in which parallel graphene sheets are separated by short nanotubes. Pillared graphene represents a class of three-dimensional carbon nanotube architectures.\nRecently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (&gt;100\u00a0nm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical-initiated thermal crosslinking method to fabricate macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano-structured pores, and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices, implants, and sensors.\nOther morphologies.\nCarbon nanobuds are a newly created material combining two previously discovered allotropes of carbon: carbon nanotubes and fullerenes. In this new material, fullerene-like \"buds\" are covalently bonded to the outer sidewalls of the underlying carbon nanotube. This hybrid material has useful properties of both fullerenes and carbon nanotubes. In particular, they have been found to be exceptionally good field emitters. In composite materials, the attached fullerene molecules may function as molecular anchors preventing slipping of the nanotubes, thus improving the composite's mechanical properties.\nA carbon peapod is a novel hybrid carbon material which traps fullerene inside a carbon nanotube. It can possess interesting magnetic properties with heating and irradiation. It can also be applied as an oscillator during theoretical investigations and predictions.\nIn theory, a nanotorus is a carbon nanotube bent into a torus (doughnut shape). Nanotori are predicted to have many unique properties, such as magnetic moments 1000 times larger than that previously expected for certain specific radii. Properties such as magnetic moment, thermal stability, etc. vary widely depending on the radius of the torus and the radius of the tube.\nGraphenated carbon nanotubes are a relatively new hybrid that combines graphitic foliates grown along the sidewalls of multiwalled or bamboo-style CNTs. The foliate density can vary as a function of deposition conditions (e.g., temperature and time) with their structure ranging from a few layers of graphene (&lt; 10) to thicker, more graphite-like. The fundamental advantage of an integrated graphene-CNT structure is the high surface area three-dimensional framework of the CNTs coupled with the high edge density of graphene. Depositing a high density of graphene foliates along the length of aligned CNTs can significantly increase the total charge capacity per unit of nominal area as compared to other carbon nanostructures.\nCup-stacked carbon nanotubes (CSCNTs) differ from other quasi-1D carbon structures, which normally behave as quasi-metallic conductors of electrons. CSCNTs exhibit semiconducting behavior because of the stacking microstructure of graphene layers.\nProperties.\nMany properties of single-walled carbon nanotubes depend significantly on the (\"n\",\"m\") type, and this dependence is non-monotonic (see Kataura plot). In particular, the band gap can vary from zero to about 2 eV and the electrical conductivity can show metallic or semiconducting behavior.\nMechanical.\nCarbon nanotubes are the strongest and stiffest materials yet discovered in terms of tensile strength and elastic modulus. This strength results from the covalent sp2 bonds formed between the individual carbon atoms. In 2000, a multiwalled carbon nanotube was tested to have a tensile strength of . (For illustration, this translates into the ability to endure tension of a weight equivalent to on a cable with cross-section of ). Further studies, such as one conducted in 2008, revealed that individual CNT shells have strengths of up to \u2248, which is in agreement with quantum/atomistic models. Because carbon nanotubes have a low density for a solid of 1.3 to 1.4\u00a0g/cm3, its specific strength of up to 48,000\u00a0kN\u00b7m/kg is the best of known materials, compared to high-carbon steel's 154\u00a0kN\u00b7m/kg.\nAlthough the strength of individual CNT shells is extremely high, weak shear interactions between adjacent shells and tubes lead to significant reduction in the effective strength of multiwalled carbon nanotubes and carbon nanotube bundles down to only a few GPa. This limitation has been recently addressed by applying high-energy electron irradiation, which crosslinks inner shells and tubes, and effectively increases the strength of these materials to \u224860 GPa for multiwalled carbon nanotubes and \u224817 GPa for double-walled carbon nanotube bundles. CNTs are not nearly as strong under compression. Because of their hollow structure and high aspect ratio, they tend to undergo buckling when placed under compressive, torsional, or bending stress.\nOn the other hand, there is evidence that in the radial direction they are rather soft. The first transmission electron microscope observation of radial elasticity suggested that even van der Waals forces can deform two adjacent nanotubes. Later, nanoindentations with an atomic force microscope were performed by several groups to quantitatively measure the radial elasticity of multiwalled carbon nanotubes and tapping/contact mode atomic force microscopy was also performed on single-walled carbon nanotubes. Their high Young's modulus in the linear direction, of on the order of several GPa (and even up to an experimentally-measured 1.8 TPa, for nanotubes near 2.4 \u03bcm in length), further suggests they may be soft in the radial direction.\nElectrical.\nUnlike graphene, which is a two-dimensional semimetal, carbon nanotubes are either metallic or semiconducting along the tubular axis. For a given (\"n\",\"m\") nanotube, if \"n\" = \"m\", the nanotube is metallic; if \"n\" \u2212 \"m\" is a multiple of 3 and n \u2260 m, then the nanotube is quasi-metallic with a very small band gap, otherwise the nanotube is a moderate semiconductor.\nThus, all armchair (\"n\" = \"m\") nanotubes are metallic, and nanotubes (6,4), (9,1), etc. are semiconducting.\nCarbon nanotubes are not semimetallic because the degenerate point (the point where the \u03c0 [bonding] band meets the \u03c0* [anti-bonding] band, at which the energy goes to zero) is slightly shifted away from the \"K\" point in the Brillouin zone because of the curvature of the tube surface, causing hybridization between the \u03c3* and \u03c0* anti-bonding bands, modifying the band dispersion.\nThe rule regarding metallic versus semiconductor behavior has exceptions because curvature effects in small-diameter tubes can strongly influence electrical properties. Thus, a (5,0) SWCNT that should be semiconducting in fact is metallic according to the calculations. Likewise, zigzag and chiral SWCNTs with small diameters that should be metallic have a finite gap (armchair nanotubes remain metallic). In theory, metallic nanotubes can carry an electric current density of 4 \u00d7 109 A/cm2, which is more than 1,000 times greater than those of metals such as copper, where for copper interconnects, current densities are limited by electromigration. Carbon nanotubes are thus being explored as interconnects and conductivity-enhancing components in composite materials, and many groups are attempting to commercialize highly conducting electrical wire assembled from individual carbon nanotubes. There are significant challenges to be overcome however, such as undesired current saturation under voltage, and the much more resistive nanotube-to-nanotube junctions and impurities, all of which lower the electrical conductivity of the macroscopic nanotube wires by orders of magnitude, as compared to the conductivity of the individual nanotubes.\nBecause of its nanoscale cross-section, electrons propagate only along the tube's axis. As a result, carbon nanotubes are frequently referred to as one-dimensional conductors. The maximum electrical conductance of a single-walled carbon nanotube is 2\"G\"0, where \"G\"0 = 2\"e\"2/\"h\" is the conductance of a single ballistic quantum channel.\nBecause of the role of the \u03c0-electron system in determining the electronic properties of graphene, doping in carbon nanotubes differs from that of bulk crystalline semiconductors from the same group of the periodic table (e.g., silicon). Graphitic substitution of carbon atoms in the nanotube wall by boron or nitrogen dopants leads to p-type and n-type behavior, respectively, as would be expected in silicon. However, some non-substitutional (intercalated or adsorbed) dopants introduced into a carbon nanotube, such as alkali metals and electron-rich metallocenes, result in n-type conduction because they donate electrons to the \u03c0-electron system of the nanotube. By contrast, \u03c0-electron acceptors such as FeCl3 or electron-deficient metallocenes function as p-type dopants because they draw \u03c0-electrons away from the top of the valence band.\nIntrinsic superconductivity has been reported, although other experiments found no evidence of this, leaving the claim a subject of debate.\nIn 2021, Michael Strano, the Carbon P. Dubbs Professor of Chemical Engineering at MIT, published department findings on the use of carbon nanotubes to create an electric current. By immersing the structures in an organic solvent, the liquid drew electrons out of the carbon particles. Strano was quoted as saying, \"This allows you to do electrochemistry, but with no wires,\" and represents a significant breakthrough in the technology. Future applications include powering micro- or nanoscale robots, as well as driving alcohol oxidation reactions, which are important in the chemicals industry.\nCrystallographic defects also affect the tube's electrical properties. A common result is lowered conductivity through the defective region of the tube. A defect in metallic armchair-type tubes (which can conduct electricity) can cause the surrounding region to become semiconducting, and single monatomic vacancies induce magnetic properties.\nElectromechanical.\nSemiconducting carbon nanotubes have shown piezoresistive property when applying mechanical force. The structural deformation causes a change in the band gap which effects the conductance. This property has the potential to be used in strain sensors.\nOptical.\nCarbon nanotubes have useful absorption, photoluminescence (fluorescence), and Raman spectroscopy properties. Spectroscopic methods offer the possibility of quick and non-destructive characterization of relatively large amounts of carbon nanotubes. There is a strong demand for such characterization from the industrial point of view: numerous parameters of nanotube synthesis can be changed, intentionally or unintentionally, to alter the nanotube quality, such as the non-tubular carbon content, structure (chirality) of the produced nanotubes, and structural defects. These features then determine nearly all other significant optical, mechanical, and electrical properties.\nCarbon nanotube optical properties have been explored for use in applications such as for light-emitting diodes (LEDs) and photo-detectors based on a single nanotube have been produced in the lab. Their unique feature is not the efficiency, which is yet relatively low, but the narrow selectivity in the wavelength of emission and detection of light and the possibility of its fine-tuning through the nanotube structure. In addition, bolometer and optoelectronic memory devices have been realised on ensembles of single-walled carbon nanotubes. Nanotube fluorescence has been investigated for the purposes of imaging and sensing in biomedical applications.\nThermal.\nAll nanotubes are expected to be very good thermal conductors along the tube, exhibiting a property known as \"ballistic conduction\", but good insulators lateral to the tube axis. Measurements show that an individual SWNT has a room-temperature thermal conductivity along its axis of about 3500 W\u00b7m\u22121\u00b7K\u22121; compare this to copper, a metal well known for its good thermal conductivity, which transmits 385 W\u00b7m\u22121\u00b7K\u22121. An individual SWNT has a room-temperature thermal conductivity lateral to its axis (in the radial direction) of about 1.52 W\u00b7m\u22121\u00b7K\u22121, which is about as thermally conductive as soil. Macroscopic assemblies of nanotubes such as films or fibres have reached up to 1500 W\u00b7m\u22121\u00b7K\u22121 so far. Networks composed of nanotubes demonstrate different values of thermal conductivity, from the level of thermal insulation with the thermal conductivity of 0.1 W\u00b7m\u22121\u00b7K\u22121 to such high values. That is dependent on the amount of contribution to the thermal resistance of the system caused by the presence of impurities, misalignments and other factors. The temperature stability of carbon nanotubes is estimated to be up to 2800\u00a0\u00b0C in vacuum and about 750\u00a0\u00b0C in air.\nCrystallographic defects strongly affect the tube's thermal properties. Such defects lead to phonon scattering, which in turn increases the relaxation rate of the phonons. This reduces the mean free path and reduces the thermal conductivity of nanotube structures. Phonon transport simulations indicate that substitutional defects such as nitrogen or boron will primarily lead to the scattering of high-frequency optical phonons. However, larger-scale defects such as Stone\u2013Wales defects cause phonon scattering over a wide range of frequencies, leading to a greater reduction in thermal conductivity.\nAntibacterial.\nRecently, carbon-nanotubes have been shown to have antibacterial properties. They disrupt normal bacterial function by causing physical/mechanical damage, facilitating oxidative stress or lipid extraction, inhibiting bacterial metabolism, and isolating functional sites via wrapping with CNM-containing nanomaterials.\nSynthesis.\nTechniques have been developed to produce nanotubes in sizeable quantities, including arc discharge, laser ablation, chemical vapor deposition (CVD) and high-pressure carbon monoxide disproportionation (HiPCO). Among these arc discharge, laser ablation are batch by batch process, Chemical Vapor Deposition can be used both for batch by batch or continuous processes, and HiPCO is gas phase continuous process. Most of these processes take place in a vacuum or with process gases. The CVD growth method is popular, as it yields high quantity and has a degree of control over diameter, length and morphology. Using particulate catalysts, large quantities of nanotubes can be synthesized by these methods, and industrialisation is well on its way, with several CNT and CNT fibers factory around the world. One problem of CVD processes is the high variability in the nanotube's characteristics The HiPCO process advances in catalysis and continuous growth are making CNTs more commercially viable. The HiPCO process helps in producing high purity single-walled carbon nanotubes in higher quantity. The HiPCO reactor operates at high temperature 900\u20131100\u00a0\u00b0C and high pressure ~30\u201350 bar. It uses carbon monoxide as the carbon source and iron pentacarbonyl or nickel tetracarbonyl as a catalyst. These catalysts provide a nucleation site for the nanotubes to grow, while cheaper iron-based catalysts like Ferrocene can be used for CVD process.\nVertically aligned carbon nanotube arrays are also grown by thermal chemical vapor deposition. A substrate (quartz, silicon, stainless steel, carbon fibers, etc.) is coated with a catalytic metal (Fe, Co, Ni) layer. Typically that layer is iron and is deposited via sputtering to a thickness of 1\u20135\u00a0nm. A 10\u201350\u00a0nm underlayer of alumina is often also put down on the substrate first. This imparts controllable wetting and good interfacial properties.\nWhen the substrate is heated to the growth temperature (~600 to 850\u00a0\u00b0C), the continuous iron film breaks up into small islands with each island then nucleating a carbon nanotube. The sputtered thickness controls the island size and this in turn determines the nanotube diameter. Thinner iron layers drive down the diameter of the islands and drive down the diameter of the nanotubes grown. The amount of time the metal island can sit at the growth temperature is limited as they are mobile and can merge into larger (but fewer) islands. Annealing at the growth temperature reduces the site density (number of CNT/mm2) while increasing the catalyst diameter.\nThe as-prepared carbon nanotubes always have impurities such as other forms of carbon (amorphous carbon, fullerene, etc.) and non-carbonaceous impurities (metal used for catalyst). These impurities need to be removed to make use of the carbon nanotubes in applications.\nPurification.\nAs-synthesized carbon nanotubes typically contain impurities and most importantly different chiralities of carbon nanotubes. Therefore, multiple methods have been developed to purify them including polymer-assisted, density gradient ultracentrifugation (DGU), chromatography and aqueous two-phase extraction (ATPE). These methods have been reviewed in multiple articles.\nCertain polymers selectively disperse or wrap CNTs of a particular chirality, metallic character or diameter. For example, poly(phenylenevinylenes) disperses CNTs of specific diameters (0.75\u20130.84\u00a0nm) and polyfluorenes are highly selective for semiconducting CNTs. It involves mainly two steps, sonicate the mixture (CNTs and polymers in solvent), centrifuge and the supernatant are desired CNTs.\nDensity gradient ultracentrifugation is a method based on the density difference of CNTs, so that different components are layered in centrifuge tubes under centrifugal force. Chromatography-based methods include size exclusion (SEC), ion-exchange (IEX) and gel chromatography. For SEC, CNTs are separated due to the difference in size using a stationary phase with different pore size. As for IEX, the separation is achieved based on their differential adsorption and desorption onto chemically functionalized resins packed in an IEX column, so understanding the interaction between CNTs mixtures and resins is important. The first IEX is reported to separate DNA-SWCNTs. Gel chromatography is based on the partition of CNTs between stationary and mobile phase, it's found semiconducting CNTs are more strongly attracted by gel than metallic CNTs. While it shows potential, the current application is limited to the separation of semiconducting (n,m) species.\nATPE uses two water-soluble polymers such as polyethylene glycol (PEG) and dextran. When mixed, two immiscible aqueous phases form spontaneously, and each of the two phases shows a different affinity to CNTs. Partition depends on the solvation energy difference between two similar phases of microscale volumes. By changing the separation system or temperatures, and adding strong oxidants, reductants, or salts, the partition of CNTs species into the two phases can be adjusted.\nDespite the progress that has been made to separate and purify CNTs, many challenges remain, such as the growth of chirality-controlled CNTs, so that no further purification is needed, or large-scale purification.\nAdvantages of monochiral CNTs.\nMonochiral CNTs have the advantage that they do contain less or no impurities, well-defined non-congested optical spectra. This allows to create for example CNT-based biosensors with higher sensitivity and selectivity. For example, monochiral SWCNTs are necessary for multiplexed and ratiometric sensing schemes, enhanced sensitivity of biocompatibility.\nFunctionalization.\nCarbon nanotubes can be functionalized to attain desired properties that can be used in a wide variety of applications. The two main methods of carbon nanotube functionalization are covalent and non-covalent modifications. Because of their apparent hydrophobic nature, carbon nanotubes tend to agglomerate hindering their dispersion in solvents or viscous polymer melts. The resulting nanotube bundles or aggregates reduce the mechanical performance of the final composite. The surface of the carbon nanotubes can be modified to reduce the hydrophobicity and improve interfacial adhesion to a bulk polymer through chemical attachment.\nChemical routes such as covalent functionalization have been studied extensively, which involves the oxidation of CNTs via strong acids (e.g. sulfuric acid, nitric acid, or a mixture of both) in order to set the carboxylic groups onto the surface of the CNTs as the final product or for further modification by esterification or amination. Free radical grafting is a promising technique among covalent functionalization methods, in which alkyl or aryl peroxides, substituted anilines, and diazonium salts are used as the starting agents.\nFunctionalization can improve CNTs characteristically weak dispersibility in many solvents, such as water - a consequence of their strong intermolecular p\u2013p interactions. This can enhance the processing and manipulation of insoluble CNTs, rendering them useful for synthesizing innovative CNT nanofluids with impressive properties that are tunable for a wide range of applications.\nFree radical grafting of macromolecules (as the functional group) onto the surface of CNTs can improve the solubility of CNTs compared to common acid treatments which involve the attachment of small molecules such as hydroxyl onto the surface of CNTs. The solubility of CNTs can be improved significantly by free-radical grafting because the large functional molecules facilitate the dispersion of CNTs in a variety of solvents even at a low degree of functionalization. Recently an innovative environmentally friendly approach has been developed for the covalent functionalization of multi-walled carbon nanotubes (MWCNTs) using clove buds. This approach is innovative and green because it does not use toxic and hazardous acids which are typically used in common carbon nanomaterial functionalization procedures. The MWCNTs are functionalized in one pot using a free radical grafting reaction. The clove-functionalized MWCNTs are then dispersed in water producing a highly stable multi-walled carbon nanotube aqueous suspension (nanofluids).\nThe surface of carbon nanotubes can be chemically modified by coating spinel nanoparticles by hydrothermal synthesis and can be used for water oxidation purposes.\nIn addition, the surface of carbon nanotubes can be fluorinated or halofluorinated by heating while in contact with a fluoroorganic substance, thereby forming partially fluorinated carbons (so-called Fluocar materials) with grafted (halo)fluoroalkyl functionality.\nModeling.\nCarbon nanotubes are modelled in a similar manner as traditional composites in which a reinforcement phase is surrounded by a matrix phase. Ideal models such as cylindrical, hexagonal and square models are common. The size of the micromechanics model is highly function of the studied mechanical properties. The concept of representative volume element (RVE) is used to determine the appropriate size and configuration of the computer model to replicate the actual behavior of the CNT-reinforced nanocomposite. Depending on the material property of interest (thermal, electrical, modulus, creep), one RVE might predict the property better than the alternatives. While the implementation of the ideal model is computationally efficient, they do not represent microstructural features observed in scanning electron microscopy of actual nanocomposites. To incorporate realistic modeling, computer models are also generated to incorporate variability such as waviness, orientation and agglomeration of multiwall or single-wall carbon nanotubes.\nMetrology.\nThere are many metrology standards and reference materials available for carbon nanotubes.\nFor single-wall carbon nanotubes, ISO/TS 10868 describes a measurement method for the diameter, purity, and fraction of metallic nanotubes through optical absorption spectroscopy, while ISO/TS 10797 and ISO/TS 10798 establish methods to characterize the morphology and elemental composition of single-wall carbon nanotubes, using transmission electron microscopy and scanning electron microscopy respectively, coupled with energy dispersive X-ray spectrometry analysis.\nNIST SRM 2483 is a soot of single-wall carbon nanotubes used as a reference material for elemental analysis, and was characterized using thermogravimetric analysis, prompt gamma activation analysis, induced neutron activation analysis, inductively coupled plasma mass spectroscopy, resonant Raman scattering, UV-visible-near infrared fluorescence spectroscopy and absorption spectroscopy, scanning electron microscopy, and transmission electron microscopy. The Canadian National Research Council also offers a certified reference material SWCNT-1 for elemental analysis using neutron activation analysis and inductively coupled plasma mass spectroscopy. NIST RM 8281 is a mixture of three lengths of single-wall carbon nanotube.\nFor multiwall carbon nanotubes, ISO/TR 10929 identifies the basic properties and the content of impurities, while ISO/TS 11888 describes morphology using scanning electron microscopy, transmission electron microscopy, viscometry, and light scattering analysis. ISO/TS 10798 is also valid for multiwall carbon nanotubes.\nSafety and health.\nThe National Institute for Occupational Safety and Health (NIOSH) is the leading United States federal agency conducting research and providing guidance on the occupational safety and health implications and applications of nanomaterials. Early scientific studies have indicated that nanoscale particles may pose a greater health risk than bulk materials due to a relative increase in surface area per unit mass. Increase in\u00a0length and diameter of CNT is correlated to increased toxicity and pathological alterations in lung. The biological interactions of nanotubes are not well understood, and the field is open to continued toxicological studies. It is often difficult to separate confounding factors, and since carbon is relatively biologically inert, some of the toxicity attributed to carbon nanotubes may be instead due to residual metal catalyst contamination. In previous studies, only Mitsui-7 was reliably demonstrated to be carcinogenic, although for unclear/unknown reasons. Unlike many common mineral fibers (such as asbestos), most SWCNTs and MWCNTs do not fit the size and aspect-ratio criteria to be classified as respirable fibers. In 2013, given that the long-term health effects have not yet been measured, NIOSH published a Current Intelligence Bulletin detailing the potential hazards and recommended exposure limit for carbon nanotubes and fibers. The U.S. National Institute for Occupational Safety and Health has determined non-regulatory recommended exposure limits (RELs) of 1 \u03bcg/m3 for carbon nanotubes and carbon nanofibers as background-corrected elemental carbon as an 8-hour time-weighted average (TWA) respirable mass concentration. Although CNT caused pulmonary inflammation and toxicity in mice, exposure to aerosols generated from sanding of composites containing polymer-coated MWCNTs, representative of the actual end-product, did not exert such toxicity.\nAs of October 2016, single-wall carbon nanotubes have been registered through the European Union's Registration, Evaluation, Authorization and Restriction of Chemicals (REACH) regulations, based on evaluation of the potentially hazardous properties of SWCNT. Based on this registration, SWCNT commercialization is allowed in the EU up to 100 metric tons. Currently, the type of SWCNT registered through REACH is limited to the specific type of single-wall carbon nanotubes manufactured by OCSiAl, which submitted the application.\nApplications.\nCarbon nanotubes are currently used in multiple industrial and consumer applications. These include battery components, polymer composites, to improve the mechanical, thermal and electrical properties of the bulk product, and as a highly absorptive black paint. Many other applications are under development, including field effect transistors for electronics, high-strength fabrics, biosensors for biomedical and agricultural applications, and many others.\nBiomedical Applications.\nBecause of their relatively large surface area, CNTs are capable of interacting with a wide variety of therapeutic and diagnostic agents (drugs, genes, vaccines, antibodies, biosensors, etc.). This can be utilized to assist in drug delivery directly into cells. In addition, CNTs have recently been used as reinforcements in implants and scaffolds due to their suitable reaction area, high elastic modulus, and load transfer capability.\nCNTs have been shown to increase the effectiveness of bioactive coatings for the attachment, proliferation, and differentiation of osteoblasts, and has been used as a bone substitution material.\nCNTs may be used as reinforcing materials for chitosan-containing coatings used on implants and medical scaffolds.\nBiosensing.\nSWCNTs have nanoscale dimensions that fit to the size of biological species. Due to this size compatibility and their large surface-to-volume ratio, they are sensitive to changes in their chemical environment. Through covalent and non-covalent surface functionalization, SWCNTs can be precisely tailored for selective molecular interactions with a target analyte. The SWCNT represents the transduction unit that converts the interaction into a signal change (optical or electrical). Due to continuous progress in the development of detection strategies, there are numerous examples of the use of SWCNTs as highly sensitive nanosensors (even down to the single molecule level) for a variety of important biomolecules. Examples include the detection of reactive oxygen and nitrogen species, neurotransmitters, other small molecules, lipids, proteins, sugars, DNA/RNA, enzymes as well as bacteria.\nThe signal change manifests itself in an increase or decrease in the current (electrical) or in a change in the intensity or wavelength of the fluorescence emission (optical). Depending on the type of application, both electrical or optical signal transmission can be advantageous. For sensitive measurement of electronic changes, field-effect transistors (FET) are often used in which the flow of charges within the SWCNTs is measured. The FET structures allow easy on-chip integration and can be parallelized to detect multiple target analytes simultaneously. However, such sensors are more invasive for in vivo applications, as the entire device has to be inserted into the body. Optical detection with semiconducting SWCNTs is based on the radiative recombination of excitons in the near-infrared (NIR) by prior optical (fluorescence) or electrical excitation (electroluminescence). The emission in the NIR enables detection in the biological transparency window, where optical sensor applications benefit from reduced scattering and autofluorescence of biological samples and consequently a high signal-to-noise ratio. Compared to optical sensors in the UV or visible range, the penetration depth in biological tissue is also increased. In addition to the advantage of a contactless readout SWCNTs have excellent photostability, which enables long-term sensor applications. Furthermore, the nanoscale size of SWCNTs allows dense coating of surfaces which enables chemical imaging, e.g. of cellular release processes with high spatial and temporal resolution. Detection of several target analytes is possible by the spatial arrangement of different SWCNT sensors in arrays or by hyperspectral detection based on monochiral SWCNT sensors that emit at different emission wavelengths. For fluorescence applications, however, optical filters to distinguish between excitation and emission and a NIR-sensitive detector must be used. Standard silicon detectors can also be used if monochiral SWCNTs (extractable by special purification processes) emitting closer to the visible range (800 \u2013 900\u00a0nm) are used. In order to avoid susceptibility of optical sensors to fluctuating ambient light, internal references such as SWCNTs that are modified to be non-responsive or stable NIR emitters can be used. An alternative is to measure fluorescence lifetimes instead of fluorescence intensities. Overall, SWCNTs therefore have great potential as building blocks for various biosensors.\nTo render SWCNTs suitable for biosensing, their surface needs to be modified to ensure colloidal stability and provide a handle for biological recognition. Therefore, biosensing and surface modifications (functionalization) are closely related.\nPotential future applications include biomedical and environmental applications such as monitoring plant health in agriculture, standoff process control in bioreactors, research/diagnostics of neuronal communication and numerous diseases such as coagulation disorders, diabetes, cancer, microbial and viral infections, testing the efficacy of pharmaceuticals or infection monitoring using smart implants. In industry, SWCNTs are already used as sensors in the detection of gases and odors in the form of an electronic nose or in enzyme screening.\nApplications under development.\nApplications of nanotubes in development in academia and industry include:\nCarbon nanotubes can serve as additives to various structural materials. For instance, nanotubes form a tiny portion of the material(s) in some (primarily carbon fiber) baseball bats, golf clubs, car parts, or damascus steel.\nIBM expected carbon nanotube transistors to be used on Integrated Circuits by 2020.\nSWCNTs have found use in long lasting, faster charged lithium ion batteries; polyamide car parts for e-painting; automotive primers for cost benefits and better aesthetics of topcoats; ESD floors; electrically conductive lining coatings for tanks and pipes; rubber parts with improved heat and oil aging stability; conductive gelcoats for ATEX requirements and tooling conductive gelcoats for increased safety and efficiency; and heating fiber coatings for infrastructure elements.\nPotential/Future applications.\nThe strength and flexibility of carbon nanotubes makes them of potential use in controlling other nanoscale structures, which suggests they will have an important role in nanotechnology engineering. The highest tensile strength of an individual multi-walled carbon nanotube has been tested to be 63\u00a0GPa. Carbon nanotubes were found in Damascus steel from the 17th century, possibly helping to account for the legendary strength of the swords made of it. Recently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (&gt;1mm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical initiated thermal crosslinking method to fabricated macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano- structured pores and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices and implants.\nCNTs are potential candidates for future via and wire material in nano-scale VLSI circuits. Eliminating electromigration reliability concerns that plague today's Cu interconnects, isolated (single and multi-wall) CNTs can carry current densities in excess of 1000\u00a0MA/cm2 without electromigration damage.\nSingle-walled nanotubes are likely candidates for miniaturizing electronics. The most basic building block of these systems is an electric wire, and SWNTs with diameters of an order of a nanometre can be excellent conductors. One useful application of SWNTs is in the development of the first intermolecular field-effect transistors (FET). The first intermolecular logic gate using SWCNT FETs was made in 2001. A logic gate requires both a p-FET and an n-FET. Because SWNTs are p-FETs when exposed to oxygen and n-FETs otherwise, it is possible to expose half of an SWNT to oxygen and protect the other half from it. The resulting SWNT acts as a \"not\" logic gate with both p- and n-type FETs in the same molecule.\nLarge quantities of pure CNTs can be made into a freestanding sheet or film by surface-engineered tape-casting (SETC) fabrication technique which is a scalable method to fabricate flexible and foldable sheets with superior properties. Another reported form factor is CNT fiber (a.k.a. filament) by wet spinning. The fiber is either directly spun from the synthesis pot or spun from pre-made dissolved CNTs. Individual fibers can be turned into a yarn. Apart from its strength and flexibility, the main advantage is making an electrically conducting yarn. The electronic properties of individual CNT fibers (i.e. bundle of individual CNT) are governed by the two-dimensional structure of CNTs. The fibers were measured to have a resistivity only one order of magnitude higher than metallic conductors at . By further optimizing the CNTs and CNT fibers, CNT fibers with improved electrical properties could be developed.\nCNT-based yarns are suitable for applications in energy and electrochemical water treatment when coated with an ion-exchange membrane. Also, CNT-based yarns could replace copper as a winding material. Pyrh\u00f6nen et al. (2015) have built a motor using CNT winding.\nReferences.\n\"This article incorporates public domain text from the National Institute of Environmental Health Sciences (NIEHS) as quoted.\""}
{"id": "5321", "revid": "15996738", "url": "https://en.wikipedia.org/wiki?curid=5321", "title": "Czech Republic", "text": "The Czech Republic, also known as Czechia, and historically known as Bohemia, is a landlocked country in Central Europe. The country is bordered by Austria to the south, Germany to the west, Poland to the northeast, and Slovakia to the southeast. The Czech Republic has a hilly landscape that covers an area of with a mostly temperate continental and oceanic climate. The capital and largest city is Prague; other major cities and urban areas include Brno, Ostrava, Plze\u0148 and Liberec.\nThe Duchy of Bohemia was founded in the late 9th century under Great Moravia. It was formally recognized as an Imperial Estate of the Holy Roman Empire in 1002 and became a kingdom in 1198. Following the Battle of Moh\u00e1cs in 1526, all of the Lands of the Bohemian Crown were gradually integrated into the Habsburg monarchy. Nearly a hundred years later, the Protestant Bohemian Revolt led to the Thirty Years' War. After the Battle of White Mountain, the Habsburgs consolidated their rule. With the dissolution of the Holy Roman Empire in 1806, the Crown lands became part of the Austrian Empire.\nIn the 19th century, the Czech lands became more industrialized; further, in 1918, most of the country became part of the First Czechoslovak Republic following the collapse of Austria-Hungary after World War I. Czechoslovakia was the only country in Central and Eastern Europe to remain a parliamentary democracy during the entirety of the interwar period. After the Munich Agreement in 1938, Nazi Germany systematically took control over the Czech lands. Czechoslovakia was restored in 1945 and three years later became an Eastern Bloc communist state following a coup d'\u00e9tat in 1948. Attempts to liberalize the government and economy were suppressed by a Soviet-led invasion of the country during the Prague Spring in 1968. In November 1989, the Velvet Revolution ended communist rule in the country and restored democracy. On 31 December 1992, Czechoslovakia was peacefully dissolved, with its constituent states becoming the independent states of the Czech Republic and Slovakia.\nThe Czech Republic is a unitary parliamentary republic and developed country with an advanced, high-income social market economy. It is a welfare state with a European social model, universal health care and free-tuition university education. It ranks 32nd in the Human Development Index. The Czech Republic is a member of the United Nations, NATO, the European Union, the OECD, the OSCE, the Council of Europe and the Visegr\u00e1d Group.\nEtymology.\nThe traditional English name \"Bohemia\" derives from Latin \"Boiohaemum\", which means \"home of the Boii\" (a Gallic tribe). The current English name ultimately comes from the Czech word . The name comes from the Slavic tribe () and, according to legend, their leader \u010cech, who brought them to Bohemia, to settle on \u0158\u00edp Mountain. The etymology of the word is uncertain, but according to the most common derivation can be traced back to the Proto-Slavic root , meaning \"member of the people; kinsman\", thus making it cognate to the Czech word (a person).\nThe country has been traditionally divided into three lands, namely Bohemia () in the west, Moravia () in the east, and Czech Silesia (; the smaller, south-eastern part of historical Silesia, most of which is located within modern Poland) in the northeast. Known as the \"lands of the Bohemian Crown\" since the 14th century, a number of other names for the country have been used, including \"Czech/Bohemian lands\", \"Bohemian Crown\", \"Czechia\", and the \"lands of the Crown of Saint Wenceslaus\". When the country regained its independence after the dissolution of the Austro-Hungarian empire in 1918, the new name of \"Czechoslovakia\" was coined to reflect the union of the Czech and Slovak nations within one country.\nAfter Czechoslovakia dissolved on the last day of 1992, was adopted as the Czech short name for the new state and the Ministry of Foreign Affairs of the Czech Republic recommended \"Czechia\" for the English-language equivalent. This form was not widely adopted at the time, leading to the long name \"Czech Republic\" being used in English in nearly all circumstances. The Czech government directed use of \"Czechia\" as the official English short name in 2016. The short name has been listed by the United Nations and is used by other organizations such as the European Union, NATO, the CIA, Google Maps, and the European Broadcasting Union. In 2022, the American \"AP Stylebook\" stated in its entry on the country that \"both [Czechia and the Czech Republic] are acceptable. The shorter name Czechia is preferred by the Czech government. If using Czechia, clarify in the story that the country is more widely known in English as the Czech Republic.\"\nHistory.\nPrehistory.\nArchaeologists have found evidence of prehistoric human settlements in the area, dating back to the Paleolithic era.\nIn the classical era, as a result of the 3rd century BC Celtic migrations, Bohemia became associated with the Boii. The Boii founded an oppidum near the site of modern Prague. Later in the 1st century, the Germanic tribes of the Marcomanni and Quadi settled there.\nSlavs from the Black Sea\u2013Carpathian region settled in the area (their migration was pushed by an invasion of peoples from Siberia and Eastern Europe into their area: Huns, Avars, Bulgars and Magyars). In the sixth century, the Huns had moved westwards into Bohemia, Moravia, and some of present-day Austria and Germany.\nDuring the 7th century, the Frankish merchant Samo, supporting the Slavs fighting against nearby settled Avars, became the ruler of the first documented Slavic state in Central Europe, Samo's Empire. The principality of Great Moravia, controlled by Moymir dynasty, arose in the 8th century. It reached its zenith in the 9th (during the reign of Svatopluk I of Moravia), holding off the influence of the Franks. Great Moravia was Christianized, with a role being played by the Byzantine mission of Cyril and Methodius. They codified the Old Church Slavonic language, the first literary and liturgical language of the Slavs, and the Glagolitic script.\nBohemia.\nThe Duchy of Bohemia emerged in the late 9th century when it was unified by the P\u0159emyslid dynasty. Bohemia was from 1002 until 1806 an Imperial Estate of the Holy Roman Empire.\nIn 1212, P\u0159emysl Ottokar I extracted the Golden Bull of Sicily from the emperor, confirming Ottokar and his descendants' royal status; the Duchy of Bohemia was raised to a Kingdom. German immigrants settled in the Bohemian periphery in the 13th century. The Mongols in the invasion of Europe carried their raids into Moravia but were defensively defeated at Olomouc.\nAfter a series of dynastic wars, the House of Luxembourg gained the Bohemian throne.\nEfforts for a reform of the church in Bohemia started already in the late 14th century. Jan Hus' followers seceded from some practices of the Roman Church and in the Hussite Wars (1419\u20131434) defeated five crusades organized against them by Sigismund. During the next two centuries, 90% of the population in Bohemia and Moravia were considered Hussites. The pacifist thinker Petr Chel\u010dick\u00fd inspired the movement of the Moravian Brethren (by the middle of the 15th century) that completely separated from the Roman Catholic Church.\nOn 21 December 1421, Jan \u017di\u017eka, a successful military commander and mercenary, led his group of forces in the Battle of Kutn\u00e1 Hora, resulting in a victory for the Hussites. He is honoured to this day as a national hero.\nAfter 1526, Bohemia came increasingly under Habsburg control as the Habsburgs became first the elected and then in 1627 the hereditary rulers of Bohemia. Between 1583 and 1611 Prague was the official seat of the Holy Roman Emperor Rudolf II and his court.\nThe Defenestration of Prague and subsequent revolt against the Habsburgs in 1618 marked the start of the Thirty Years' War. In 1620, the rebellion in Bohemia was crushed at the Battle of White Mountain and the ties between Bohemia and the Habsburgs' hereditary lands in Austria were strengthened. The leaders of the Bohemian Revolt were executed in 1621. The nobility and the middle class Protestants had to either convert to Catholicism or leave the country.\nThe following era of 1620 to the late 18th century became known as the \"Dark Age\". During the Thirty Years' War, the population of the Czech lands declined by a third through the expulsion of Czech Protestants as well as due to the war, disease and famine. The Habsburgs prohibited all Christian confessions other than Catholicism. The flowering of Baroque culture shows the ambiguity of this historical period.\nOttoman Turks and Tatars invaded Moravia in 1663. In 1679\u20131680 the Czech lands faced the Great Plague of Vienna and an uprising of serfs.\nThere were peasant uprisings influenced by famine. Serfdom was abolished between 1781 and 1848. Several battles of the Napoleonic Wars took place on the current territory of the Czech Republic.\nThe end of the Holy Roman Empire in 1806 led to degradation of the political status of Bohemia which lost its position of an electorate of the Holy Roman Empire as well as its own political representation in the Imperial Diet. Bohemian lands became part of the Austrian Empire. During the 18th and 19th century the Czech National Revival began its rise, with the purpose to revive Czech language, culture, and national identity. The Revolution of 1848 in Prague, striving for liberal reforms and autonomy of the Bohemian Crown within the Austrian Empire, was suppressed.\nIt seemed that some concessions would be made also to Bohemia, but in the end, the Emperor Franz Joseph I affected a compromise with Hungary only. The Austro-Hungarian Compromise of 1867 and the never realized coronation of Franz Joseph as King of Bohemia led to a disappointment of some Czech politicians. The Bohemian Crown lands became part of the so-called Cisleithania.\nThe Czech Social Democratic and progressive politicians started the fight for universal suffrage. The first elections under universal male suffrage were held in 1907.\nCzechoslovakia.\nIn 1918, during the collapse of the Habsburg monarchy at the end of World War I, the independent republic of Czechoslovakia, which joined the winning Allied powers, was created, with Tom\u00e1\u0161 Garrigue Masaryk in the lead. This new country incorporated the Bohemian Crown.\nThe First Czechoslovak Republic comprised only 27% of the population of the former Austria-Hungary, but nearly 80% of the industry, which enabled it to compete with Western industrial states. In 1929 compared to 1913, the gross domestic product increased by 52% and industrial production by 41%. In 1938 Czechoslovakia held 10th place in the world industrial production. Czechoslovakia was the only country in Central and Eastern Europe to remain a liberal democracy throughout the entire \ninterwar period. Although the First Czechoslovak Republic was a unitary state, it provided certain rights to its minorities, the largest being Germans (23.6% in 1921), Hungarians (5.6%) and Ukrainians (3.5%).\nWestern Czechoslovakia was occupied by Nazi Germany, which placed most of the region into the Protectorate of Bohemia and Moravia. The Protectorate was proclaimed part of the Third Reich, and the president and prime minister were subordinated to Nazi Germany's \"Reichsprotektor\". One Nazi concentration camp was located within the Czech territory at Terez\u00edn, north of Prague. The vast majority of the Protectorate's Jews were murdered in Nazi-run concentration camps. The Nazi called for the extermination, expulsion, Germanization or enslavement of most or all Czechs for the purpose of providing more living space for the German people. There was Czechoslovak resistance to Nazi occupation as well as reprisals against the Czechoslovaks for their anti-Nazi resistance. The German occupation ended on 9 May 1945, with the arrival of the Soviet and American armies and the Prague uprising. Most of Czechoslovakia's German-speakers were forcibly expelled from the country, first as a result of local acts of violence and then under the aegis of an \"organized transfer\" confirmed by the Soviet Union, the United States, and Great Britain at the Potsdam Conference.\nIn the 1946 elections, the Communist Party gained 38% of the votes and became the largest party in the Czechoslovak parliament, formed a coalition with other parties, and consolidated power. A coup d'\u00e9tat came in 1948 and a single-party government was formed. For the next 41 years, the Czechoslovak Communist state conformed to Eastern Bloc economic and political features. The Prague Spring political liberalization was stopped by the 1968 Warsaw Pact invasion of Czechoslovakia. Analysts believe that the invasion caused the communist movement to fracture, ultimately leading to the Revolutions of 1989.\nCzech Republic.\nIn November 1989, Czechoslovakia again became a liberal democracy through the Velvet Revolution. However, Slovak national aspirations strengthened (Hyphen War) and on 31 December 1992, the country peacefully split into the independent countries of the Czech Republic and Slovakia. Both countries went through economic reforms and privatizations, with the intention of creating a market economy, as they have been trying to do since 1990, when Czechs and Slovaks still shared the common state. This process was largely successful; in 2006 the Czech Republic was recognized by the World Bank as a \"developed country\", and in 2009 the Human Development Index ranked it as a nation of \"Very High Human Development\".\nFrom 1991, the Czech Republic, originally as part of Czechoslovakia and since 1993 in its own right, has been a member of the Visegr\u00e1d Group and from 1995, the OECD. The Czech Republic joined NATO on 12 March 1999 and the European Union on 1 May 2004. On 21 December 2007 the Czech Republic joined the Schengen Area.\nUntil 2017, either the centre-left Czech Social Democratic Party or the centre-right Civic Democratic Party led the governments of the Czech Republic. In October 2017, the populist movement ANO 2011, led by the country's second-richest man, Andrej Babi\u0161, won the elections with three times more votes than its closest rival, the Civic Democrats. In December 2017, Czech president Milo\u0161 Zeman appointed Andrej Babi\u0161 as the new prime minister.\nIn the 2021 elections, ANO 2011 was narrowly defeated and Petr Fiala became the new prime minister. He formed a government coalition of the alliance SPOLU (Civic Democratic Party, KDU-\u010cSL and TOP 09) and the alliance of Pirates and Mayors. In January 2023, retired general Petr Pavel won the presidential election, becoming new Czech president to succeed Milo\u0161 Zeman. Following the 2022 Russian invasion of Ukraine, the country took in half a million Ukrainian refugees, the largest number per capita in the world.\nGeography.\nThe Czech Republic lies mostly between latitudes 48\u00b0 and 51\u00b0 N and longitudes 12\u00b0 and 19\u00b0 E.\nBohemia, to the west, consists of a basin drained by the Elbe () and the Vltava rivers, surrounded by mostly low mountains, such as the Krkono\u0161e range of the Sudetes. The highest point in the country, Sn\u011b\u017eka at , is located here. Moravia, the eastern part of the country, is also hilly. It is drained mainly by the Morava River, but it also contains the source of the Oder River ().\nWater from the Czech Republic flows to three different seas: the North Sea, Baltic Sea, and Black Sea. The Czech Republic also leases the Moldauhafen, a lot in the middle of the Hamburg Docks, which was awarded to Czechoslovakia by Article 363 of the Treaty of Versailles, to allow the landlocked country a place where goods transported down river could be transferred to seagoing ships. The territory reverts to Germany in 2028.\nPhytogeographically, the Czech Republic belongs to the Central European province of the Circumboreal Region, within the Boreal Kingdom. According to the World Wide Fund for Nature, the territory of the Czech Republic can be subdivided into four ecoregions: the Western European broadleaf forests, Central European mixed forests, Pannonian mixed forests, and Carpathian montane conifer forests.\nThere are four national parks in the Czech Republic. The oldest is Krkono\u0161e National Park (Biosphere Reserve), and the others are \u0160umava National Park (Biosphere Reserve), Podyj\u00ed National Park, and Bohemian Switzerland.\nThe three historical lands of the Czech Republic (formerly some countries of the Bohemian Crown) correspond with the river basins of the Elbe and the Vltava basin for Bohemia, the Morava one for Moravia, and the Oder river basin for Czech Silesia (in terms of the Czech territory).\nClimate.\nThe Czech Republic has a temperate climate, situated in the transition zone between the oceanic and continental climate types, with warm summers and cold, cloudy and snowy winters. The temperature difference between summer and winter is due to the landlocked geographical position.\nTemperatures vary depending on the elevation. In general, at higher altitudes, the temperatures decrease and precipitation increases. The wettest area in the Czech Republic is found around B\u00edl\u00fd Potok in Jizera Mountains and the driest region is the Louny District to the northwest of Prague. Another factor is the distribution of the mountains.\nAt the highest peak of Sn\u011b\u017eka (), the average temperature is , whereas in the lowlands of the South Moravian Region, the average temperature is as high as . The country's capital, Prague, has a similar average temperature, although this is influenced by urban factors.\nThe coldest month is usually January, followed by February and December. During these months, there is snow in the mountains and sometimes in the cities and lowlands. During March, April, and May, the temperature usually increases, especially during April, when the temperature and weather tends to vary during the day. Spring is also characterized by higher water levels in the rivers, due to melting snow with occasional flooding.\nThe warmest month of the year is July, followed by August and June. On average, summer temperatures are about higher than during winter. Summer is also characterized by rain and storms.\nAutumn generally begins in September, which is still warm and dry. During October, temperatures usually fall below or and deciduous trees begin to shed their leaves. By the end of November, temperatures usually range around the freezing point.\nThe coldest temperature ever measured was in Litv\u00ednovice near \u010cesk\u00e9 Bud\u011bjovice in 1929, at and the hottest measured, was at in Dob\u0159ichovice in 2012.\nMost rain falls during the summer. Sporadic rainfall is throughout the year (in Prague, the average number of days per month experiencing at least of rain varies from 12 in September and October to 16 in November) but concentrated rainfall (days with more than per day) are more frequent in the months of May to August (average around two such days per month). Severe thunderstorms, producing damaging straight-line winds, hail, and occasional tornadoes occur, especially during the summer period.\nBiodiversity and conservation.\nAs of 2020, the Czech Republic ranks as the 21st most environmentally conscious country in the world in Environmental Performance Index. It had a 2018 Forest Landscape Integrity Index mean score of 1.71/10, ranking it 160th globally out of 172 countries. The Czech Republic has four National Parks (\u0160umava National Park, Krkono\u0161e National Park, \u010cesk\u00e9 \u0160v\u00fdcarsko National Park, Podyj\u00ed National Park) and 25 Protected Landscape Areas. The fauna of the Czech Republic includes a wide variety of animal species. Some species (especially endangered ones) are bred in reserves. Among the rare animals are, for example, eagles, ospreys, bustards, and storks.\nGovernment and politics.\nThe Czech Republic is a pluralist multi-party parliamentary representative democracy. The Parliament (\"Parlament \u010cesk\u00e9 republiky\") is bicameral, with the Chamber of Deputies (, 200 members) and the Senate (, 81 members). The members of the Chamber of Deputies are elected for a four-year term by proportional representation, with a 5% election threshold. There are 14 voting districts, identical to the country's administrative regions. The Chamber of Deputies, the successor to the Czech National Council, has the powers and responsibilities of the now defunct federal parliament of the former Czechoslovakia. The members of the Senate are elected in single-seat constituencies by two-round runoff voting for a six-year term, with one-third elected every even year in the autumn. This arrangement is modeled on the U.S. Senate, but each constituency is roughly the same size and the voting system used is a two-round runoff.\nThe president is a formal head of state with limited and specific powers, who appoints the prime minister, as well the other members of the cabinet on a proposal by the prime minister. From 1993 until 2012, the President of the Czech Republic was selected by a joint session of the parliament for a five-year term, with no more than two consecutive terms (V\u00e1clav Havel and V\u00e1clav Klaus were both elected twice). Since 2013, the president has been elected directly. Some commentators have argued that, with the introduction of direct election of the President, the Czech Republic has moved away from the parliamentary system and towards a semi-presidential one. The Government's exercise of executive power derives from the Constitution. The members of the government are the Prime Minister, Deputy prime ministers and other ministers. The Government is responsible to the Chamber of Deputies. The Prime Minister is the head of government and wields powers such as the right to set the agenda for most foreign and domestic policy and choose government ministers.\nLaw.\nThe Czech Republic is a unitary state, with a civil law system based on the continental type, rooted in Germanic legal culture. The basis of the legal system is the Constitution of the Czech Republic adopted in 1993. The Penal Code is effective from 2010. A new Civil code became effective in 2014. The court system includes district, county, and supreme courts and is divided into civil, criminal, and administrative branches. The Czech judiciary has a triumvirate of supreme courts. The Constitutional Court consists of 15 constitutional judges and oversees violations of the Constitution by either the legislature or by the government. The Supreme Court is formed of 67 judges and is the court of highest appeal for most legal cases heard in the Czech Republic. The Supreme Administrative Court decides on issues of procedural and administrative propriety. It also has jurisdiction over certain political matters, such as the formation and closure of political parties, jurisdictional boundaries between government entities, and the eligibility of persons to stand for public office. The Supreme Court and the Supreme Administrative Court are both based in Brno, as is the Supreme Public Prosecutor's Office.\nForeign relations.\nThe Czech Republic has ranked as one of the safest or most peaceful countries for the past few decades. It is a member of the United Nations, the European Union, NATO, OECD, Council of Europe and is an observer to the Organization of American States. The embassies of most countries with diplomatic relations with the Czech Republic are located in Prague, while consulates are located across the country.\nThe Czech passport is restricted by visas. According to the 2018 Henley &amp; Partners Visa Restrictions Index, Czech citizens have visa-free access to 173 countries, which ranks them 7th along with Malta and New Zealand. The World Tourism Organization ranks the Czech passport 24th. The US Visa Waiver Program applies to Czech nationals.\nThe Prime Minister and Minister of Foreign Affairs have primary roles in setting foreign policy, although the President also has influence and represents the country abroad. Membership in the European Union and NATO is central to the Czech Republic's foreign policy. The Office for Foreign Relations and Information (\u00daZSI) serves as the foreign intelligence agency responsible for espionage and foreign policy briefings, as well as protection of Czech Republic's embassies abroad.\nThe Czech Republic has ties with Slovakia, Poland and Hungary as a member of the Visegr\u00e1d Group, as well as with Germany, Israel, the United States and the European Union and its members. After 2020, relations with Asian democratic states, such as Taiwan, are being strengthened. Conversely, the Czech Republic has long had bad relations with Russia; from 2021, the Czech Republic appears on Russia's official list of enemy countries. The Czech Republic also has problematic relations with China.\nCzech officials have supported dissenters in Belarus, Moldova, Myanmar and Cuba.\nFamous Czech diplomats of the past included Jaroslav Lev of Ro\u017emit\u00e1l, Humprecht Jan Czernin, Count Philip Kinsky of Wchinitz and Tettau, Wenzel Anton, Prince of Kaunitz-Rietberg, Prince Karl Philipp Schwarzenberg, Alois Lexa von Aehrenthal, Ottokar Czernin, Edvard Bene\u0161, Jan Masaryk, Ji\u0159\u00ed H\u00e1jek, Ji\u0159\u00ed Dienstbier, Michael \u017dantovsk\u00fd, Petr Kol\u00e1\u0159, Alexandr Vondra, Prince Karel Schwarzenberg and Petr Pavel.\nMilitary.\nThe Czech armed forces consist of the Czech Land Forces, the Czech Air Force and of specialized support units. The armed forces are managed by the Ministry of Defence. The President of the Czech Republic is Commander-in-chief of the armed forces. In 2004 the army transformed itself into a fully professional organization and compulsory military service was abolished. The country has been a member of NATO since 12 March 1999. Defence spending is approximately 1.28% of the GDP (2021). The armed forces are charged with protecting the Czech Republic and its allies, promoting global security interests, and contributing to NATO.\nCurrently, as a member of NATO, the Czech military are participating in the Resolute Support and KFOR operations and have soldiers in Afghanistan, Mali, Bosnia and Herzegovina, Kosovo, Egypt, Israel and Somalia. The Czech Air Force also served in the Baltic states and Iceland. The main equipment of the Czech military includes JAS 39 Gripen multi-role fighters, Aero L-159 Alca combat aircraft, AH-1Z Viper attack helicopters, armored vehicles (Pandur II, BVP-2) and tanks (T-72M4CZ and Leopard 2A4).\nHuman rights.\nHuman rights in the Czech Republic are guaranteed by the Charter of Fundamental Rights and Freedoms and international treaties on human rights. Nevertheless, there were cases of human rights violations such as discrimination against Roma children, for which the European Commission asked the Czech Republic to provide an explanation, or the illegal sterilization of Roma women, for which the government apologized.\nPeople of the same sex can enter into a \"registered partnership\" in the Czech Republic. Conducting same-sex marriage is not legal under current Czech law.\nAdministrative divisions.\nSince 2000, the Czech Republic has been divided into thirteen regions (Czech: \"kraje\", singular \"kraj\") and the capital city of Prague. Every region has its own elected regional assembly and a regional governor. In Prague, the assembly and presidential powers are executed by the city council and the mayor.\nThe older seventy-six districts (\"okresy\", singular \"okres\") including three \"statutory cities\" (without Prague, which had special status) lost most of their importance in 1999 in an administrative reform; they remain as territorial divisions and seats of various branches of state administration.\nThe smallest administrative units are \"obce\" (municipalities). As of 2021, the Czech Republic is divided into 6,254 municipalities. Cities and towns are also municipalities. The capital city of Prague is a region and municipality at the same time.\nEconomy.\nThe Czech Republic has a developed, high-income export-oriented social market economy based in services, manufacturing and innovation, that maintains a welfare state and the European social model. The Czech Republic participates in the European Single Market as a member of the European Union and is therefore a part of the economy of the European Union, but uses its own currency, the Czech koruna, instead of the euro. It has a per capita GDP rate that is 91% of the EU average and is a member of the OECD. Monetary policy is conducted by the Czech National Bank, whose independence is guaranteed by the Constitution. The Czech Republic ranks 12th in the UN inequality-adjusted human development and 24th in World Bank Human Capital Index. It was described by \"The Guardian\" as \"one of Europe's most flourishing economies\".\n, the country's GDP per capita at purchasing power parity is $51,329 and $29,856 at nominal value. According to Allianz A.G., in 2018 the country was an MWC (mean wealth country), ranking 26th in net financial assets. The country experienced a 4.5% GDP growth in 2017. The 2016 unemployment rate was the lowest in the EU at 2.4%, and the 2016 poverty rate was the second lowest of OECD members. Czech Republic ranks 27th in the 2021 Index of Economic Freedom, 30th in the 2024 Global Innovation Index, 29th in the Global Competitiveness Report,&lt;ref name=\"GCR 2018/19\"&gt;&lt;/ref&gt; and 25th in the Global Enabling Trade Report. The Czech Republic has a diverse economy that ranks 7th in the 2016 Economic Complexity Index. The industrial sector accounts for 37.5% of the economy, while services account for 60% and agriculture for 2.5%. The largest trading partner for both export and import is Germany and the EU in general. Dividends worth CZK 270 billion were paid to the foreign owners of Czech companies in 2017, which has become a political issue. The country has been a member of the Schengen Area since 1 May 2004, having abolished border controls, completely opening its borders with all of its neighbors on 21 December 2007.\nIndustry.\n the largest companies by revenue in the Czech Republic were: automobile manufacturer \u0160koda Auto, utility company \u010cEZ Group, conglomerate Agrofert, energy trading company EPH, oil processing company Unipetrol, electronics manufacturer Foxconn CZ and steel producer Moravia Steel. Other Czech transportation companies include: \u0160koda Transportation (tramways, trolleybuses, metro), Tatra (heavy trucks, the second oldest car maker in the world), Avia (medium trucks), Karosa and SOR Libchavy (buses), Aero Vodochody (military aircraft), Let Kunovice (civil aircraft), Zetor (tractors), Jawa Moto (motorcycles) and \u010cezeta (electric scooters).\n\u0160koda Transportation is the fourth largest tram producer in the world; nearly one third of all trams in the world come from Czech factories. The Czech Republic is also the world's largest vinyl records manufacturer, with GZ Media producing about 6 million pieces annually in Lod\u011bnice. \u010cesk\u00e1 zbrojovka is among the ten largest firearms producers in the world and five who produce automatic weapons.\nIn the food industry, Czech companies include Agrofert, Kofola and Ham\u00e9.\nEnergy.\nProduction of Czech electricity exceeds consumption by about 10 TWh per year, the excess being exported. Nuclear power presently provides about 30 percent of the total power needs, its share is projected to increase to 40 percent. In 2005, 65.4 percent of electricity was produced by steam and combustion power plants (mostly coal); 30 percent by nuclear plants; and 4.6 percent came from renewable sources, including hydropower. The largest Czech power resource is Temel\u00edn Nuclear Power Station, with another nuclear power plant in Dukovany.\nThe Czech Republic is reducing its dependence on highly polluting low-grade brown coal as a source of energy. Natural gas is purchased from Norwegian companies and as liquefied gas LNG from the Netherlands and Belgium. In the past, three-quarters of gas supplies came from Russia, but after the start of the 2022 Russian invasion of Ukraine, the government gradually stopped these supplies. Gas consumption (approx. 100 TWh in 2003\u20132005) is almost double electricity consumption. South Moravia has small oil and gas deposits.\nTransportation infrastructure.\n the road network in the Czech Republic is long, out of which are motorways. The speed limit is within towns, outside of towns and on motorways.\nThe Czech Republic has one of the densest rail networks in the world. the country has of lines. Of that number, is electrified, are single-line tracks and are double and multiple-line tracks. The length of tracks is , out of which is electrified.\n\u010cesk\u00e9 dr\u00e1hy (the Czech Railways) is the main railway operator in the country, with about 180\u00a0million passengers carried yearly. Maximum speed is limited to .\nV\u00e1clav Havel Airport in Prague is the main international airport in the country. In 2019, it handled 17.8 million passengers. In total, the Czech Republic has 91 airports, six of which provide international air services. The public international airports are in Brno, Karlovy Vary, Mnichovo Hradi\u0161t\u011b, Mo\u0161nov (near Ostrava), Pardubice and Prague. The non-public international airports capable of handling airliners are in Kunovice and Vodochody.\nRussia (via pipelines through Ukraine) and, to a lesser extent, Norway (via pipelines through Germany) supply the Czech Republic with liquid and natural gas.\nCommunications and IT.\nThe Czech Republic ranks in the top 10 countries worldwide with the fastest average internet speed. By the beginning of 2008, there were over 800 mostly local WISPs, with about 350,000 subscribers in 2007. Plans based on either GPRS, EDGE, UMTS or CDMA2000 are being offered by all three mobile phone operators (T-Mobile, O2, Vodafone) and internet provider U:fon. Government-owned \u010cesk\u00fd Telecom slowed down broadband penetration. At the beginning of 2004, local-loop unbundling began and alternative operators started to offer ADSL and also SDSL. This and later privatization of \u010cesk\u00fd Telecom helped drive down prices.\nOn 1 July 2006, \u010cesk\u00fd Telecom was acquired by globalized company (Spain-owned) Telef\u00f3nica group and adopted the new name Telef\u00f3nica O2 Czech Republic. , VDSL and ADSL2+ are offered in variants, with download speeds of up to 50\u00a0Mbit/s and upload speeds of up to 5\u00a0Mbit/s. Cable internet is gaining more popularity with its higher download speeds ranging from 50\u00a0Mbit/s to 1\u00a0Gbit/s.\nTwo computer security companies, Avast and AVG, were founded in the Czech Republic. In 2016, Avast led by Pavel Baudi\u0161 bought rival AVG for US$1.3\u00a0billion, together at the time, these companies had a user base of about 400 million people and 40% of the consumer market outside of China. Avast is the leading provider of antivirus software, with a 20.5% market share.\nTourism.\nPrague is the fifth most visited city in Europe after London, Paris, Istanbul and Rome. In 2001, the total earnings from tourism reached 118\u00a0billion CZK, making up 5.5% of the country's GNP and 9% of its overall export earnings. The industry employs more than 110,000 people \u2013 over 1% of the population.\nGuidebooks and tourists reporting overcharging by taxi drivers and pickpocketing problems talk mainly about Prague, though the situation has improved recently. Since 2005, Prague's mayor, Pavel B\u00e9m, has worked to improve this reputation by cracking down on petty crime and, aside from these problems, Prague is a \"safe\" city. The Czech Republic's crime rate is described by the United States State department as \"low\".\nThe Czech Republic boasts 17 UNESCO World Heritage Sites, 3 of them being transnational. , further 13 sites are on the tentative list.\nArchitectural heritage is an object of interest to visitors \u2013 it includes castles and ch\u00e2teaux from different historical epochs, namely Karl\u0161tejn Castle, \u010cesk\u00fd Krumlov and the Lednice\u2013Valtice Cultural Landscape. There are 12 cathedrals and 15 churches elevated to the rank of basilica by the Pope, as well as many monasteries.\nAway from the towns, areas such as Bohemian Paradise, Bohemian Forest and the Giant Mountains attract visitors seeking outdoor pursuits.\nThe country is also known for its various museums, puppetry and marionette exhibitions that take part within larger puppet festivals, and beer festivals. Aquapalace Prague in \u010cestlice is the largest water park in the country.\nScience.\nThe Czech lands have a long and well-documented history of scientific innovation. Today, the Czech Republic has a highly sophisticated, developed, high-performing, innovation-oriented scientific community supported by the government, industry, and leading universities. Czech scientists are embedded members of the global scientific community. They contribute annually to multiple international academic journals and collaborate with their colleagues across boundaries and fields. The Czech Republic was ranked 24th in the Global Innovation Index in 2020 and 2021, up from 26th in 2019.\nHistorically, the Czech lands, especially Prague, have been the seat of scientific discovery going back to early modern times, including Tycho Brahe, Nicolaus Copernicus, and Johannes Kepler. In 1784 the scientific community was first formally organized under the charter of the Royal Czech Society of Sciences. Currently, this organization is known as the Czech Academy of Sciences. Similarly, the Czech lands have a well-established history of scientists, including Nobel laureates biochemists Gerty and Carl Ferdinand Cori, chemists Jaroslav Heyrovsk\u00fd and Otto Wichterle, physicists Ernst Mach and Peter Gr\u00fcnberg, physiologist Jan Evangelista Purkyn\u011b and chemist Anton\u00edn Hol\u00fd. Sigmund Freud, the founder of psychoanalysis, was born in P\u0159\u00edbor, Gregor Mendel, the founder of genetics, was born in Hyn\u010dice and spent most of his life in Brno, logician and mathematician Kurt G\u00f6del was born in Brno.\nHistorically, most scientific research was recorded in Latin, but from the 18th century onwards increasingly in German and later in Czech, archived in libraries supported and managed by religious groups and other denominations as evidenced by historical locations of international renown and heritage such as the Strahov Monastery and the Clementinum in Prague. Increasingly, Czech scientists publish their work and that of their history in English.\nThe current important scientific institution is the already mentioned Academy of Sciences of the Czech Republic, the CEITEC Institute in Brno or the HiLASE and Eli Beamlines centers with the most powerful laser in the world in Doln\u00ed B\u0159e\u017eany. Prague is the seat of the administrative center of the GSA Agency operating the European navigation system Galileo and the European Union Agency for the Space Programme.\nDemographics.\nThe total fertility rate (TFR) in 2020 was estimated at 1.71 children per woman, which is below the replacement rate of 2.1. The Czech Republic's population has an average age of 43.3 years. The life expectancy in 2021 was estimated to be 79.5 years (76.55 years male, 82.61 years female). About 77,000 people immigrate to the Czech Republic annually. Vietnamese immigrants began settling in the country during the Communist period, when they were invited as guest workers by the Czechoslovak government. In 2009, there were about 70,000 Vietnamese in the Czech Republic. Most decide to stay in the country permanently.\nAccording to results of the 2021 census, the majority of the inhabitants of the Czech Republic are Czechs (57.3%), followed by Moravians (3.4%), Slovaks (0.9%), Ukrainians (0.7%), Viets (0.3%), Poles (0.3%), Russians (0.2%), Silesians (0.1%) and Germans (0.1%). Another 4.0% declared combination of two nationalities (3.6% combination of Czech and other nationality). As the 'nationality' was an optional item, a number of people left this field blank (31.6%). According to some estimates, there are about 250,000 Romani people in the Czech Republic. The Polish minority resides mainly in the Trans-Olza region.\nThere were 658,564 foreigners residing in the country in 2021, according to the Czech Statistical Office, with the largest groups being Ukrainian (22%), Slovak (22%), Vietnamese (12%), Russian (7%) and German (4%). Most of the foreign population lives in Prague (37.3%) and Central Bohemia Region (13.2%).\nThe Jewish population of Bohemia and Moravia, 118,000 according to the 1930 census, was nearly annihilated by the Nazi Germans during the Holocaust. There were approximately 3,900 Jews in the Czech Republic in 2021. The former Czech prime minister, Jan Fischer, is of Jewish faith.\nNationality of residents, who answered the question in the Census 2021:\nReligion.\nAbout 75% to 79% of residents of the Czech Republic do not declare having any religion or faith in surveys, and the proportion of convinced atheists (30%) is the third highest in the world behind those of China (47%) and Japan (31%). The Czech people have been historically characterized as \"tolerant and even indifferent towards religion\". The religious identity of the country has changed drastically since the first half of the 20th century, when more than 90% of Czechs were Christians.\nChristianization in the 9th and 10th centuries introduced Christianity. After the Bohemian Reformation, most Czechs became followers of Jan Hus, Petr Chel\u010dick\u00fd and other regional Protestant Reformers. Taborites and Utraquists were Hussite groups. Towards the end of the Hussite Wars, the Utraquists changed sides and allied with the Roman Catholic Church. Following the joint Utraquist\u2014Roman Catholic victory, Utraquism was accepted as a distinct form of Christianity to be practiced in Bohemia by the Roman Catholic Church while all remaining Hussite groups were prohibited. After the Reformation, some Bohemians went with the teachings of Martin Luther, especially Sudeten Germans. In the wake of the Reformation, Utraquist Hussites took a renewed increasingly anti-Catholic stance, while some of the defeated Hussite factions were revived. After the Habsburgs regained control of Bohemia, the whole population was forcibly converted to Roman Catholicism\u2014even the Utraquist Hussites. Going forward, Czechs have become more wary and pessimistic of religion as such. A history of resistance to the Roman Catholic Church followed. It suffered a schism with the neo-Hussite Czechoslovak Hussite Church in 1920, lost the bulk of its adherents during the Communist era and continues to lose in the modern, ongoing secularization. Protestantism never recovered after the Counter-Reformation was introduced by the Austrian Habsburgs in 1620. Prior to the Holocaust, the Czech Republic had a sizable Jewish community of around 100,000. There are many historically important and culturally relevant Synagogues in the Czech Republic such as Europe's oldest active Synagogue, The Old New Synagogue and the second largest Synagogue in Europe, the Great Synagogue (Plze\u0148). The Holocaust decimated Czech Jewry and the Jewish population as of 2021 is 3,900.\nAccording to the 2011 census, 34% of the population stated they had no religion, 10.3% were Roman Catholic, 0.8% were Protestant (0.5% Czech Brethren and 0.4% Hussite), and 9% followed other forms of religion both denominational or not (of which 863 people answered they are Pagan). 45% of the population did not answer the question about religion. From 1991 to 2001 and further to 2011 the adherence to Roman Catholicism decreased from 39% to 27% and then to 10%; Protestantism similarly declined from 3.7% to 2% and then to 0.8%. The Muslim population is estimated to be 20,000 representing 0.2% of the population.\nThe proportion of religious believers varies significantly across the country, from 55% in Zl\u00edn Region to 16% in \u00dast\u00ed nad Labem Region.\nEducation and health care.\nEducation in the Czech Republic is compulsory for nine years and citizens have access to a free-tuition university education, while the average number of years of education is 13.1. Additionally, the Czech Republic has a \"relatively equal\" educational system in comparison with other countries in Europe. Founded in 1348, Charles University was the first university in Central Europe. Other major universities in the country are Masaryk University, Czech Technical University, Palack\u00fd University, Academy of Performing Arts and University of Economics.\nThe Programme for International Student Assessment, coordinated by the OECD, currently ranks the Czech education system as the 15th most successful in the world, higher than the OECD average. The UN Education Index ranks the Czech Republic 10th (positioned behind Denmark and ahead of South Korea).\nHealth care in the Czech Republic is similar in quality to that of other developed nations. The Czech universal health care system is based on a compulsory insurance model, with fee-for-service care funded by mandatory employment-related insurance plans. According to the 2016 Euro health consumer index, a comparison of healthcare in Europe, the Czech healthcare is 13th, ranked behind Sweden and two positions ahead of the United Kingdom.\nCulture.\nArt.\nVenus of Doln\u00ed V\u011bstonice is an important example of prehistoric art unearthed in the Czech Republic. Theodoric of Prague was a painter in the Gothic era who decorated the castle Karl\u0161tejn. In the Baroque era, there were painters Wenceslaus Hollar, Jan Kupeck\u00fd, Karel \u0160kr\u00e9ta, Anton Raphael Mengs and Petr Brandl and sculptors Matthias Braun and Ferdinand Brokoff.\nIn the first half of the 19th century, Josef M\u00e1nes joined the romantic movement. In the second half the so-called \"National Theatre generation\" rose to prominence: sculptor Josef V\u00e1clav Myslbek and painters Mikol\u00e1\u0161 Ale\u0161, V\u00e1clav Bro\u017e\u00edk, Vojt\u011bch Hynais and Julius Ma\u0159\u00e1k. At the end of the century came Art Nouveau, with Alfons Mucha becoming its main representative. He is known for his Art Nouveau posters and a cycle of 20 large canvases named the Slav Epic, which depicts the history of Czechs and other Slavs. , it can be seen in the Veletr\u017en\u00ed Palace of the National Gallery in Prague, which manages the largest collection of art in the Czech Republic. Max \u0160vabinsk\u00fd was another Art Nouveau painter.\nThe 20th century brought an avant-garde revolution, represented in the Czech lands mainly by expressionists and cubists: Josef \u010capek, Emil Filla, Bohumil Kubi\u0161ta or Jan Zrzav\u00fd. Surrealism emerged particularly through the work of Toyen, Josef \u0160\u00edma and Karel Teige. In the world, however, the most well-known Czech avant-garde artist might be Franti\u0161ek Kupka, a pioneer of abstract painting. Illustrators and cartoonists to gain fame in the first half of the 20th century include Josef Lada, Zden\u011bk Burian or Emil Orl\u00edk. Art photography became a new field represented by Franti\u0161ek Drtikol, Josef Sudek, later Jan Saudek and Josef Koudelka.\nThe Czech Republic is also known for its individually made, mouth-blown, and decorated Bohemian glass.\nArchitecture.\nThe earliest preserved stone buildings in Bohemia and Moravia date back to the time of the Christianization in the 9th and 10th centuries. Since the Middle Ages, the Czech lands have been using the same architectural styles as most of Western and Central Europe. The oldest still standing churches were built in the Romanesque style. During the 13th century, it was replaced by the Gothic style. In the 14th century, Emperor Charles IV invited architects from France and Germany, Matthias of Arras and Peter Parler, to his court in Prague. During the Middle Ages, some fortified castles were built by the king and aristocracy, as well as some monasteries.\nThe Renaissance style penetrated the Bohemian Crown in the late 15th century when the older Gothic style started to be mixed with Renaissance elements. An example of pure Renaissance architecture in Bohemia is the Queen Anne's Summer Palace, which was situated in the garden of Prague Castle. Evidence of the general reception of the Renaissance in Bohemia, involving an influx of Italian architects, can be found in spacious chateaus with arcade courtyards and geometrically arranged gardens. Emphasis was placed on comfort, and buildings that were built for entertainment purposes also appeared.\nIn the 17th century, the Baroque style spread throughout the Crown of Bohemia.\nIn the 18th century, Bohemia produced an architectural peculiarity \u2013 the \"Baroque Gothic style\", a synthesis of the Gothic and Baroque styles.\nDuring the 19th century stands the revival architectural styles. Some churches were restored to their presumed medieval appearance and there were constructed buildings in the Neo-Romanesque, Neo-Gothic and Neo-Renaissance styles. At the turn of the 19th and 20th centuries, the new art style appeared in the Czech lands \u2013 Art Nouveau.\nBohemia contributed an unusual style to the world's architectural heritage when Czech architects attempted to transpose the Cubism of painting and sculpture into architecture.\nBetween World Wars I and II, Functionalism, with its sober, progressive forms, took over as the main architectural style.\nAfter World War II and the Communist coup in 1948, art in Czechoslovakia became Soviet-influenced. The Czechoslovak avant-garde artistic movement is known as the \"Brussels style came up\" in the time of political liberalization of Czechoslovakia in the 1960s. Brutalism dominated in the 1970s and 1980s.\nThe Czech Republic is not shying away from the more modern trends of international architecture, an example is the Dancing House (Tan\u010d\u00edc\u00ed d\u016fm) in Prague, Golden Angel in Prague or Congress Centre in Zl\u00edn.\nLiterature.\nThe literature from the area of today's Czech Republic was mostly written in Czech, but also in Latin and German or even Old Church Slavonic. Franz Kafka, although a competent user of Czech, wrote in his mother tongue, German. His works include \"The Trial\" and \"The Castle\".\nIn the second half of the 13th century, the royal court in Prague became one of the centers of German Minnesang and courtly literature. The Czech German-language literature can be seen in the first half of the 20th century.\nBible translations played a role in the development of Czech literature. The oldest Czech translation of the Psalms originated in the late 13th century and the first complete Czech translation of the Bible was finished around 1360. The first complete printed Czech Bible was published in 1488. The first complete Czech Bible translation from the original languages was published between 1579 and 1593. The Codex Gigas from the 12th century is the largest extant medieval manuscript in the world.\nCzech-language literature can be divided into several periods: the Middle Ages; the Hussite period; the Renaissance humanism; the Baroque period; the Enlightenment and Czech reawakening in the first half of the 19th century, modern literature in the second half of the 19th century; the avant-garde of the interwar period; the years under Communism; and the Czech Republic.\nThe antiwar comedy novel \"The Good Soldier \u0160vejk\" is the most translated Czech book in history.\nThe international literary award the Franz Kafka Prize is awarded in the Czech Republic.\nThe Czech Republic has the densest network of libraries in Europe.\nCzech literature and culture played a role on at least two occasions when Czechs lived under oppression and political activity was suppressed. On both of these occasions, in the early 19th century and then again in the 1960s, the Czechs used their cultural and literary effort to strive for political freedom, establishing a confident, politically aware nation.\nMusic.\nThe musical tradition of the Czech lands arose from the first church hymns, whose first evidence is suggested at the break of the 10th and 11th centuries. Some pieces of Czech music include two chorales, which in their time performed the function of anthems: \"Lord, Have Mercy on Us\" and the hymn \"Saint Wenceslaus\" or \"Saint Wenceslaus Chorale\". The authorship of the anthem \"Lord, Have Mercy on Us\" is ascribed by some historians to Saint Adalbert of Prague (sv.Vojt\u011bch), bishop of Prague, living between 956 and 997.\nThe wealth of musical culture lies in the classical music tradition during all historical periods, especially in the Baroque, Classicism, Romantic, modern classical music and in the traditional folk music of Bohemia, Moravia and Silesia. Since the early era of artificial music, Czech musicians and composers have been influenced the folk music of the region and dance.\nCzech music can be considered to have been \"beneficial\" in both the European and worldwide context, several times co-determined or even determined a newly arriving era in musical art, above all of Classical era, as well as by original attitudes in Baroque, Romantic and modern classical music. Some Czech musical works are \"The Bartered Bride\", \"New World Symphony\", \"Sinfonietta\" and \"Jen\u016ffa\".\nA music festival in the country is Prague Spring International Music Festival of classical music, a permanent showcase for performing artists, symphony orchestras and chamber music ensembles of the world.\nTheatre.\nThe roots of Czech theatre can be found in the Middle Ages, especially in the cultural life of the Gothic period. In the 19th century, the theatre played a role in the national awakening movement and later, in the 20th century, it became a part of modern European theatre art. The original Czech cultural phenomenon came into being at the end of the 1950s. This project called Laterna magika, resulting in productions that combined theater, dance, and film in a poetic manner, considered the first multimedia art project in an international context.\nOne drama is Karel \u010capek's play \"R.U.R.\", which introduced the word \"robot\".\nThe country has a tradition of puppet theater. In 2016, Czech and Slovak Puppetry was included on the UNESCO Intangible Cultural Heritage Lists.\nFilm.\nThe tradition of Czech cinematography started in the second half of the 1890s. Peaks of the production in the era of silent movies include the historical drama \"The Builder of the Temple\" and the social and erotic drama \"Erotikon\" directed by Gustav Machat\u00fd. The early Czech sound film era was productive, above all in mainstream genres, with the comedies of Martin Fri\u010d or Karel Lama\u010d. There were dramatic movies sought internationally.\nHerm\u00edna T\u00fdrlov\u00e1 was a prominent Czech animator, screenwriter, and film director. She was often called the mother of Czech animation. Over the course of her career, she produced over 60 animated children's short films using puppets and the technique of stop motion animation.\nBefore the German occupation, in 1933, filmmaker and animator established the first Czech animation studio \"IRE Film\" with her husband Karel Dodal.\nAfter the period of Nazi occupation and early communist official dramaturgy of socialist realism in movies at the turn of the 1940s and 1950s with fewer exceptions such as \"Krakatit\" or \"Men without wings\" (awarded by in 1946), an era of the Czech film began with animated films, performed in anglophone countries under the name \"The Fabulous World of Jules Verne\" from 1958, which combined acted drama with animation, and Ji\u0159\u00ed Trnka, the founder of the modern puppet film. This began a tradition of animated films (\"Mole\" etc.).\nIn the 1960s, the hallmark of Czechoslovak New Wave's films were improvised dialogues, black and absurd humor and the occupation of non-actors. Directors are trying to preserve natural atmosphere without refinement and artificial arrangement of scenes. A personality of the 1960s and the beginning of the 1970s with original manuscript and psychological impact is Franti\u0161ek Vl\u00e1\u010dil. Another international author is Jan \u0160vankmajer, a filmmaker and artist whose work spans several media. He is a self-labeled surrealist known for animations and features.\nThe Barrandov Studios in Prague are the largest film studios with film locations in the country. Filmmakers have come to Prague to shoot scenery no longer found in Berlin, Paris and Vienna. The city of Karlovy Vary was used as a location for the 2006 James Bond film Casino Royale.\nThe Czech Lion is the highest Czech award for film achievement. Karlovy Vary International Film Festival is one of the film festivals that have been given competitive status by the FIAPF. Other film festivals held in the country include Febiofest, Jihlava International Documentary Film Festival, One World Film Festival, Zl\u00edn Film Festival and Fresh Film Festival.\nMedia.\nCzech journalists and media enjoy a degree of freedom. There are restrictions against writing in support of Nazism, racism or violating Czech law. The Czech press was ranked as the 40th most free press in the World Press Freedom Index by Reporters Without Borders in 2021. Radio Free Europe/Radio Liberty has its headquarters in Prague.\nCzech Television is the country's national public television broadcaster. It operates a number of channels, including \u010cT1, \u010cT2, and the 24-hour news channel \u010cT24, as well as the news website ct24.cz. As of 2020, it is the most watched broadcaster, followed by the private TV Nova and Prima TV. However, TV Nova features the most watched main news program and prime time program. Other public media services include the Czech Radio and the Czech News Agency.\nThe best-selling daily national newspapers in 2020/21 are Blesk (average 703,000 daily readers), Mlad\u00e1 fronta DNES (average 461,000 daily readers), Pr\u00e1vo (average 182,000 daily readers), Lidov\u00e9 noviny (average 163,000 daily readers) and Hospod\u00e1\u0159sk\u00e9 noviny (average 162,000 daily readers).\nMost Czechs (87%) read their news online, with Seznam.cz, iDNES.cz, Novinky.cz, iPrima.cz and Seznam Zpr\u00e1vy.cz being the most visited as of 2021.\nCuisine.\nCzech cuisine is marked by an emphasis on meat dishes with pork, beef, and chicken. Goose, duck, rabbit, and venison are served. Fish is less common, with the occasional exception of fresh trout and carp, which is served at Christmas. One popular Czech menu item is \"sma\u017een\u00fd vep\u0159ov\u00fd \u0159\u00edzek\" (fried breaded pork filet), served with boiled potatoes.\nThere is a variety of local sausages, wurst, p\u00e2t\u00e9s, and smoked and cured meats. Czech desserts include a variety of whipped cream, chocolate, and fruit pastries and tarts, cr\u00eapes, creme desserts and cheese, poppy-seed-filled and other types of traditional cakes such as \"buchty\", \"kol\u00e1\u010de\" and \"\u0161tr\u00fadl\".\nCzech beer has a history extending more than a millennium; the earliest known brewery existed in 993. Today, the Czech Republic has the highest beer consumption per capita in the world. The pilsner style beer (pils) originated in Plze\u0148, where the world's first blond lager Pilsner Urquell is still produced. It has served as the inspiration for more than two-thirds of the beer produced in the world today. The city of \u010cesk\u00e9 Bud\u011bjovice has similarly lent its name to its beer, known as Budweiser Budvar.\nThe South Moravian region has been producing wine since the Middle Ages; about 94% of vineyards in the Czech Republic are Moravian. Aside from beer, slivovitz and wine, the Czech Republic also produces two liquors, Fernet Stock and Becherovka. Kofola is a non-alcoholic domestic cola soft drink which competes with Coca-Cola and Pepsi.\nSport.\nThe most watched and most attended sport in the Czech Republic are football and ice hockey. The most watched sporting events are the Ice hockey at the Olympic Games and the Ice Hockey World Championships. The most popular sports in the Czech Republic according to the size of the membership base of sports clubs are: football, tennis, ice hockey, volleyball, floorball, golf, ball hockey, athletics, basketball and skiing.\nThe country has won 15 gold medals in the Summer Olympics and nine in the Winter Games. (See Olympic history.) The Czech ice hockey team won the gold medal at the 1998 Winter Olympics and has won (along with the Czechoslovakian team) thirteen gold medals at the World Championships, including three straight from 1999 to 2001.\nThe \u0160koda Motorsport is engaged in competition racing since 1901 and has gained a number of titles with various vehicles around the world. MTX automobile company was formerly engaged in the manufacture of racing and formula cars since 1969.\nHiking is a popular sport. The word for 'tourist' in Czech, \"turista\", also means 'trekker' or 'hiker'. For hikers, thanks to the more than 120-year-old tradition, there is the Czech Hiking Markers System of trail blazing, that has been adopted by countries worldwide. There is a network of around 40,000\u00a0km of marked short- and long-distance trails crossing the whole country and all the Czech mountains."}
{"id": "5322", "revid": "48886350", "url": "https://en.wikipedia.org/wiki?curid=5322", "title": "Czechoslovakia", "text": "Czechoslovakia ( ; Czech and , \"\u010cesko-Slovensko\") was a landlocked country in Central Europe, created in 1918, when it declared its independence from Austria-Hungary. In 1938, after the Munich Agreement, the Sudetenland became part of Nazi Germany, while the country lost further territories to Hungary and Poland (the territories of southern Slovakia with a predominantly Hungarian population to Hungary and Zaolzie with a predominantly Polish population to Poland). Between 1939 and 1945, the state ceased to exist, as Slovakia proclaimed its independence and Carpathian Ruthenia became part of Hungary, while the German Protectorate of Bohemia and Moravia was proclaimed in the remainder of the Czech Lands. In 1939, after the outbreak of World War II, former Czechoslovak President Edvard Bene\u0161 formed a government-in-exile and sought recognition from the Allies. \nAfter World War II, Czechoslovakia was reestablished under its pre-1938 borders, with the exception of Carpathian Ruthenia, which became part of the Ukrainian SSR (a republic of the Soviet Union). The Communist Party seized power in a coup in 1948. From 1948 to 1989, Czechoslovakia was part of the Eastern Bloc with a planned economy. Its economic status was formalized in membership of Comecon from 1949 and its defense status in the Warsaw Pact of 1955. A period of political liberalization in 1968, the Prague Spring, ended when the Soviet Union, assisted by other Warsaw Pact countries, invaded Czechoslovakia. In 1989, as Marxist\u2013Leninist governments and communism were ending all over Central and Eastern Europe, Czechoslovaks peacefully deposed their communist government during the Velvet Revolution, which began on 17 November 1989 and ended 11 days later on 28 November when all of the top Communist leaders and Communist party itself resigned. On 31 December 1992, Czechoslovakia peacefully split into the two sovereign states of the Czech Republic and Slovakia.\nCharacteristics.\nThe country was of generally irregular terrain. The western area was part of the north-central European uplands. The eastern region was composed of the northern reaches of the Carpathian Mountains and lands of the Danube River basin.\nThe weather is mild winters and mild summers. Influenced by the Atlantic Ocean from the west, the Baltic Sea from the north, and Mediterranean Sea from the south. There is no continental weather.\nHistory.\nOrigins.\nThe area was part of the Austro-Hungarian Empire until it collapsed at the end of World War I. The new state was founded by Tom\u00e1\u0161 Garrigue Masaryk, who served as its first president from 14\u00a0November 1918 to 14\u00a0December 1935. He was succeeded by his close ally Edvard Bene\u0161 (1884\u20131948).\nThe roots of Czech nationalism go back to the 19th century, when philologists and educators, influenced by Romanticism, promoted the Czech language and pride in the Czech people. Nationalism became a mass movement in the second half of the 19th century. Taking advantage of the limited opportunities for participation in political life under Austrian rule, Czech leaders such as historian Franti\u0161ek Palack\u00fd (1798\u20131876) founded various patriotic, self-help organizations which provided a chance for many of their compatriots to participate in communal life before independence. Palack\u00fd supported Austro-Slavism and worked for a reorganized federal Austrian Empire, which would protect the Slavic speaking peoples of Central Europe against Russian and German threats.\nAn advocate of democratic reform and Czech autonomy within Austria-Hungary, Masaryk was elected twice to the \"Reichsrat\" (Austrian Parliament), from 1891 to 1893 for the Young Czech Party, and from 1907 to 1914 for the Czech Realist Party, which he had founded in 1889 with Karel Kram\u00e1\u0159 and Josef Kaizl.\nDuring World War I a number of Czechs and Slovaks, the Czechoslovak Legions, fought with the Allies in France and Italy, while large numbers deserted to Russia in exchange for its support for the independence of Czechoslovakia from the Austrian Empire. With the outbreak of World War I, Masaryk began working for Czech independence in a union with Slovakia. With Edvard Bene\u0161 and Milan Rastislav \u0160tef\u00e1nik, Masaryk visited several Western countries and won support from influential publicists. The Czechoslovak National Council was the main organization that advanced the claims for a Czechoslovak state.\nFirst Czechoslovak Republic.\nFormation.\nThe Bohemian Kingdom ceased to exist in 1918 when it was incorporated into Czechoslovakia. Czechoslovakia was founded in October 1918, as one of the successor states of the Austro-Hungarian Empire at the end of World War I and as part of the Treaty of Saint-Germain-en-Laye. It consisted of the present day territories of Bohemia, Moravia, parts of Silesia making up present day Czech Republic, Slovakia, and a region of present-day Ukraine called Carpathian Ruthenia. Its territory included some of the most industrialized regions of the former Austria-Hungary.\nEthnicity.\nThe new country was a multi-ethnic state, with Czechs and Slovaks as \"constituent peoples\". The population consisted of Czechs (51%), Slovaks (16%), Germans (22%), Hungarians (5%) and Rusyns (4%). Many of the Germans, Hungarians, Ruthenians and Poles and some Slovaks, felt oppressed because the political elite did not generally allow political autonomy for minority ethnic groups. This policy led to unrest among the non-Czech population, particularly in German-speaking Sudetenland, which initially had proclaimed itself part of the Republic of German-Austria in accordance with the self-determination principle.\nThe state proclaimed the official ideology that there were no separate Czech and Slovak nations, but only one nation of Czechoslovaks (see Czechoslovakism), to the disagreement of Slovaks and other ethnic groups. Once a unified Czechoslovakia was restored after World War II (after the country had been divided during the war), the conflict between the Czechs and the Slovaks surfaced again. The governments of Czechoslovakia and other Central European nations deported ethnic Germans, reducing the presence of minorities in the nation. Most of the Jews had been killed during the war by the Nazis.\nInterwar period.\nDuring the period between the two world wars Czechoslovakia was a democratic state. The population was generally literate, and contained fewer alienated groups. The influence of these conditions was augmented by the political values of Czechoslovakia's leaders and the policies they adopted. Under Tomas Masaryk, Czech and Slovak politicians promoted progressive social and economic conditions that served to defuse discontent.\nForeign minister Bene\u0161 became the prime architect of the Czechoslovak-Romanian-Yugoslav alliance (the \"Little Entente\", 1921\u201338) directed against Hungarian attempts to reclaim lost areas. Bene\u0161 worked closely with France. Far more dangerous was the German element, which after 1933 became allied with the Nazis in Germany.\nCzech-Slovak relations came to be a central issue in Czechoslovak politics during the 1930s. The increasing feeling of inferiority among the Slovaks, who were hostile to the more numerous Czechs, weakened the country in the late 1930s. Slovakia became autonomous in the fall of 1938, and by mid-1939, Slovakia had become independent, with the First Slovak Republic set up as a satellite state of Nazi Germany and the far-right Slovak People's Party in power .\nAfter 1933, Czechoslovakia remained the only democracy in central and eastern Europe.\nMunich Agreement, and Two-Step German Occupation.\nIn September 1938, Adolf Hitler demanded control of the Sudetenland. On 29\u00a0September 1938, Britain and France ceded control in the Appeasement at the Munich Conference; France ignored the military alliance it had with Czechoslovakia. During October 1938, Nazi Germany occupied the Sudetenland border region, effectively crippling Czechoslovak defences.\nThe First Vienna Award assigned a strip of southern Slovakia and Carpathian Ruthenia to Hungary. Poland occupied Zaolzie, an area whose population was majority Polish, in October 1938.\nOn 14 March 1939, the remainder (\"rump\") of Czechoslovakia was dismembered by the proclamation of the Slovak State, the next day the rest of Carpathian Ruthenia was occupied and annexed by Hungary, while the following day the German Protectorate of Bohemia and Moravia was proclaimed.\nThe eventual goal of the German state under Nazi leadership was to eradicate Czech nationality through assimilation, deportation, and extermination of the Czech intelligentsia; the intellectual elites and middle class made up a considerable number of the 200,000 people who passed through concentration camps and the 250,000 who died during German occupation. Under , it was assumed that around 50% of Czechs would be fit for Germanization. The Czech intellectual elites were to be removed not only from Czech territories but from Europe completely. The authors of believed it would be best if they emigrated overseas, as even in Siberia they were considered a threat to German rule. Just like Jews, Poles, Serbs, and several other nations, Czechs were considered to be untermenschen by the Nazi state. In 1940, in a secret Nazi plan for the Germanization of the Protectorate of Bohemia and Moravia it was declared that those considered to be of racially Mongoloid origin and the Czech intelligentsia were not to be Germanized.\nThe deportation of Jews to concentration camps was organized under the direction of Reinhard Heydrich, and the fortress town of Terez\u00edn was made into a ghetto way station for Jewish families. On 4\u00a0June 1942 Heydrich died after being wounded by an assassin in Operation Anthropoid. Heydrich's successor, Colonel General Kurt Daluege, ordered mass arrests and executions and the destruction of the villages of Lidice and Le\u017e\u00e1ky. In 1943 the German war effort was accelerated. Under the authority of Karl Hermann Frank, German minister of state for Bohemia and Moravia, some 350,000 Czech laborers were dispatched to the Reich. Within the protectorate, all non-war-related industry was prohibited. Most of the Czech population obeyed quiescently up until the final months preceding the end of the war, while thousands were involved in the resistance movement.\nFor the Czechs of the Protectorate Bohemia and Moravia, German occupation was a period of brutal oppression. Czech losses resulting from political persecution and deaths in concentration camps totaled between 36,000 and 55,000. The Jewish populations of Bohemia and Moravia (118,000 according to the 1930 census) were virtually annihilated. Many Jews emigrated after 1939; more than 70,000 were killed; 8,000 survived at Terez\u00edn. Several thousand Jews managed to live in freedom or in hiding throughout the occupation.\nDespite the estimated 136,000 deaths at the hands of the Nazi regime, the population in the Reichsprotektorate saw a net increase during the war years of approximately 250,000 in line with an increased birth rate.\nOn 6 May 1945, the third US Army of General Patton entered Plze\u0148 from the south west. On 9\u00a0May 1945, Soviet Red Army troops entered Prague.\nThird and Fourth Republics.\nAfter World War II, pre-war Czechoslovakia was reestablished, with the exception of Subcarpathian Ruthenia, which was annexed by the Soviet Union and incorporated into the Ukrainian Soviet Socialist Republic. The Bene\u0161 decrees were promulgated concerning ethnic Germans (see Potsdam Agreement) and ethnic Hungarians. Under the decrees, citizenship was abrogated for people of German and Hungarian ethnic origin who had accepted German or Hungarian citizenship during the occupations. In 1948, this provision was cancelled for the Hungarians, but only partially for the Germans. The government then confiscated the property of the Germans and expelled about 90% of the ethnic German population, over 2\u00a0million people. Those who remained were collectively accused of supporting the Nazis after the Munich Agreement, as 97.32% of Sudeten Germans had voted for the NSDAP in the December 1938 elections. Almost every decree explicitly stated that the sanctions did not apply to antifascists. Some 250,000 Germans, many married to Czechs, some antifascists, and also those required for the post-war reconstruction of the country, remained in Czechoslovakia. The Bene\u0161 Decrees still cause controversy among nationalist groups in the Czech Republic, Germany, Austria and Hungary.\nFollowing the expulsion of the ethnic German population from Czechoslovakia, parts of the former Sudetenland, especially around Krnov and the surrounding villages of the Jesenik mountain region in northeastern Czechoslovakia, were settled in 1949 by Communist refugees from Northern Greece who had left their homeland as a result of the Greek Civil War. These Greeks made up a large proportion of the town and region's population until the late 1980s/early 1990s. Although defined as \"Greeks\", the Greek Communist community of Krnov and the Jeseniky region actually consisted of an ethnically diverse population, including Greek Macedonians, Macedonians, Vlachs, Pontic Greeks and Turkish speaking Urums or Caucasus Greeks.\nCarpathian Ruthenia (Podkarpatsk\u00e1 Rus) was occupied by (and in June 1945 formally ceded to) the Soviet Union. In the 1946 parliamentary election, the Communist Party of Czechoslovakia was the winner in the Czech lands, and the Democratic Party won in Slovakia. In February 1948 the Communists seized power. Although they would maintain the fiction of political pluralism through the existence of the National Front, except for a short period in the late 1960s (the Prague Spring) the country had no liberal democracy. Since citizens lacked significant electoral methods of registering protest against government policies, periodically there were street protests that became violent. For example, there were riots in the town of Plze\u0148 in 1953, reflecting economic discontent. Police and army units put down the rebellion, and hundreds were injured but no one was killed. While its economy remained more advanced than those of its neighbors in Eastern Europe, Czechoslovakia grew increasingly economically weak relative to Western Europe.\nThe currency reform of 1953 caused dissatisfaction among Czechoslovak laborers. To equalize the wage rate, Czechoslovaks had to turn in their old money for new at a decreased value. The banks also confiscated savings and bank deposits to control the amount of money in circulation. In the 1950s, Czechoslovakia experienced high economic growth (averaging 7% per year), which allowed for a substantial increase in wages and living standards, thus promoting the stability of the regime.\nIn 1968, when the reformer Alexander Dub\u010dek was appointed to the key post of First Secretary of the Czechoslovak Communist Party, there was a brief period of liberalization known as the Prague Spring. In response, after failing to persuade the Czechoslovak leaders to change course, five other members of the Warsaw Pact invaded. Soviet tanks rolled into Czechoslovakia on the night of 20\u201321\u00a0August 1968. Soviet Communist Party General Secretary Leonid Brezhnev viewed this intervention as vital for the preservation of the Soviet, socialist system and vowed to intervene in any state that sought to replace Marxism-Leninism with capitalism.\nIn the week after the invasion, there was a spontaneous campaign of civil resistance against the occupation. This resistance involved a wide range of acts of non-cooperation and defiance: this was followed by a period in which the Czechoslovak Communist Party leadership, having been forced in Moscow to make concessions to the Soviet Union, gradually put the brakes on their earlier liberal policies.\nMeanwhile, one plank of the reform program had been carried out: in 1968\u201369, Czechoslovakia was turned into a federation of the Czech Socialist Republic and Slovak Socialist Republic. The theory was that under the federation, social and economic inequities between the Czech and Slovak halves of the state would be largely eliminated. A number of ministries, such as education, now became two formally equal bodies in the two formally equal republics. However, the centralized political control by the Czechoslovak Communist Party severely limited the effects of federalization.\nThe 1970s saw the rise of the dissident movement in Czechoslovakia, represented among others by V\u00e1clav Havel. The movement sought greater political participation and expression in the face of official disapproval, manifested in limitations on work activities, which went as far as a ban on professional employment, the refusal of higher education for the dissidents' children, police harassment and prison.\nDuring the 1980s, Czechoslovakia became one of the most tightly controlled Communist regimes in the Warsaw Pact in resistance to the mitigation of controls notified by Soviet president Mikhail Gorbachev.\nAfter 1989.\nIn 1989, the Velvet Revolution restored democracy. This occurred around the same time as the fall of communism in Romania, Bulgaria, Hungary, East Germany and Poland.\nThe word \"socialist\" was removed from the country's full name on 29\u00a0March 1990 and replaced by \"federal\".\nPope John Paul II made a papal visit to Czechoslovakia on 21 April 1990, hailing it as a symbolic step of reviving Christianity in the newly-formed post-communist state.\nCzechoslovakia participated in the Gulf War with a small force of 200 troops under the command of the U.S.-led coalition.\nIn 1992, because of growing nationalist tensions in the government, Czechoslovakia was peacefully dissolved by parliament. On 31 December 1992, it formally separated into two independent countries, the Czech Republic and the Slovak Republic.\nGovernment and politics.\nAfter World War II, a political monopoly was held by the Communist Party of Czechoslovakia (KS\u010c). The leader of the KS\u010c was \"de facto\" the most powerful person in the country during this period. Gust\u00e1v Hus\u00e1k was elected first secretary of the KS\u010c in 1969 (changed to general secretary in 1971) and president of Czechoslovakia in 1975. Other parties and organizations existed but functioned in subordinate roles to the KS\u010c. All political parties, as well as numerous mass organizations, were grouped under umbrella of the National Front. Human rights activists and religious activists were severely repressed.\nConstitutional development.\nCzechoslovakia had the following constitutions during its history (1918\u20131992):\nForeign policy.\nInternational agreements and membership.\nIn the 1930s, the nation formed a military alliance with France, which collapsed in the Munich Agreement of 1938. After World War II, an active participant in Council for Mutual Economic Assistance (Comecon), Warsaw Pact, United Nations and its specialized agencies; signatory of conference on Security and Cooperation in Europe.\nEconomy.\nBefore World War II, the economy was about the fourth in all industrial countries in Europe. The state was based on strong economy, manufacturing cars (\u0160koda, Tatra), trams, aircraft (Aero, Avia), ships, ship engines (\u0160koda), cannons, shoes (Ba\u0165a), turbines, guns (Zbrojovka Brno). It was the industrial workshop for the Austro-Hungarian empire. The Slovak lands relied more heavily on agriculture than the Czech lands.\nAfter World War II, the economy was centrally planned, with command links controlled by the communist party, similarly to the Soviet Union. The large metallurgical industry was dependent on imports of iron and non-ferrous ores.\nResource base.\nAfter World War II, the country was short of energy, relying on imported crude oil and natural gas from the Soviet Union, domestic brown coal, and nuclear and hydroelectric energy. Energy constraints were a major factor in the 1980s.\nTransport and communications.\nSlightly after the foundation of Czechoslovakia in 1918, there was a lack of essential infrastructure in many areas \u2013 paved roads, railways, bridges, etc. Massive improvement in the following years enabled Czechoslovakia to develop its industry. Prague's civil airport in Ruzyn\u011b became one of the most modern terminals in the world when it was finished in 1937. Tom\u00e1\u0161 Ba\u0165a, a Czech entrepreneur and visionary, outlined his ideas in the publication \"Budujme st\u00e1t pro 40 milion\u016f lid\u00ed\", where he described the future motorway system. Construction of the first motorways in Czechoslovakia begun in 1939, nevertheless, they were stopped after German occupation during World War II.\nEducation.\nEducation was free at all levels and compulsory from ages 6 to 15. The vast majority of the population was literate. There was a highly developed system of apprenticeship training and vocational schools supplemented general secondary schools and institutions of higher education.\nReligion.\nIn 1991, 46% of the population were Roman Catholics, 5.3% were Evangelical Lutheran, 30% were Atheist, and other religions made up 17% of the country, but there were huge differences in religious practices between the two constituent republics; see Czech Republic and Slovakia.\nHealth, social welfare and housing.\nAfter World War II, free health care was available to all citizens. National health planning emphasized preventive medicine; factory and local health care centres supplemented hospitals and other inpatient institutions. There was a substantial improvement in rural health care during the 1960s and 1970s.\nMass media.\nDuring the era between the World Wars, Czechoslovak democracy and liberalism facilitated conditions for free publication. The most significant daily newspapers in these times were Lidov\u00e9 noviny, N\u00e1rodn\u00ed listy, \u010cesk\u00fd den\u00edk and \u010ceskoslovensk\u00e1 Republika.\nDuring Communist rule, the mass media in Czechoslovakia were controlled by the Communist Party. Private ownership of any publication or agency of the mass media was generally forbidden, although churches and other organizations published small periodicals and newspapers. Even with this information monopoly in the hands of organizations under KS\u010c control, all publications were reviewed by the government's Office for Press and Information.\nSports.\nThe Czechoslovakia national football team was a consistent performer on the international scene, with eight appearances in the FIFA World Cup Finals, finishing in second place in 1934 and 1962. The team also won the European Football Championship in 1976, came in third in 1980, and won the Olympic gold in 1980.\nWell-known football players such as Jan Koller, Pavel Nedv\u011bd, Anton\u00edn Panenka, Milan Baro\u0161, Tom\u00e1\u0161 Rosick\u00fd, Vladim\u00edr \u0160micer, Petr \u010cech, Ladislav Petr\u00e1\u0161, Mari\u00e1n Masn\u00fd, J\u00e1n Pivarn\u00edk, J\u00e1n Mucha, R\u00f3bert Vittek, Peter Pekar\u00edk, and Marek Ham\u0161\u00edk were all born in Czechoslovakia.\nThe International Olympic Committee code for Czechoslovakia is TCH, which is still used in historical listings of results.\nThe Czechoslovak national ice hockey team won many medals from the world championships and Olympic Games. Peter \u0160\u0165astn\u00fd, Jarom\u00edr J\u00e1gr, Dominik Ha\u0161ek, Peter Bondra, Petr Kl\u00edma, Mari\u00e1n G\u00e1bor\u00edk, Mari\u00e1n Hossa, Miroslav \u0160atan and Pavol Demitra all come from Czechoslovakia.\nEmil Z\u00e1topek, winner of four Olympic gold medals in athletics, is considered one of the top athletes in Czechoslovak history.\nV\u011bra \u010c\u00e1slavsk\u00e1 was an Olympic gold medallist in gymnastics, winning seven gold medals and four silver medals. She represented Czechoslovakia in three consecutive Olympics.\nSeveral accomplished professional tennis players including Jaroslav Drobn\u00fd, Ivan Lendl, Jan Kode\u0161, Miloslav Me\u010d\u00ed\u0159, Hana Mandl\u00edkov\u00e1, Martina Hingis, Martina Navratilova, Jana Novotn\u00e1, Petra Kvitov\u00e1, Daniela Hantuchov\u00e1, Barbora Krej\u010d\u00edkov\u00e1, Mark\u00e9ta Vondrou\u0161ov\u00e1 and Karol\u00edna Muchov\u00e1 were born in Czechoslovakia (or what became the Czech Republic in 1993).\nExternal links.\nMaps with Hungarian-language rubrics"}
{"id": "5323", "revid": "29340214", "url": "https://en.wikipedia.org/wiki?curid=5323", "title": "Computer science", "text": "Computer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). \nAlgorithms and data structures are central to computer science.\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human\u2013computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\nThe fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\nHistory.\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first \"automatic mechanical calculator\", his Difference Engine, in 1822, which eventually gave him the idea of the first \"programmable mechanical calculator\", his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his \"Essays on Automatics\", and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff\u2013Berry computer and ENIAC, the term \"computer\" came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\nEtymology and scope.\nAlthough first proposed in 1956, the term \"computer science\" appears in a 1959 article in \"Communications of the ACM\",\nin which Louis Fein argues for the creation of a \"Graduate School in Computer Sciences\" analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term \"computing science\", to emphasize precisely that difference. Danish scientist Peter Naur suggested the term \"datalogy\", to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested (albeit facetiously) in the \"Communications of the ACM\"\u2014\"turingineer\", \"turologist\", \"flow-charts-man\", \"applied meta-mathematician\", and \"applied epistemologist\". Three months later in the same journal, \"comptologist\" was suggested, followed next year by \"hypologist\". The term \"computics\" has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"\" in Italian) or \"information and mathematics\" are often used, e.g. (French), (German), (Italian, Dutch), (Spanish, Portuguese), (Slavic languages and Hungarian) or (, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"\nA folkloric quotation, often attributed to\u2014but almost certainly not first formulated by\u2014Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt G\u00f6del, Alan Turing, John von Neumann, R\u00f3zsa P\u00e9ter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.\nThe relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.\nThe academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\nPhilosophy.\nEpistemology of computer science.\nDespite the word \"science\" in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.\nProponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.\nParadigms of computer science.\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.\nFields.\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.\nCSAB, formerly called Computing Sciences Accreditation Board\u2014which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)\u2014identifies four areas that it considers crucial to the discipline of computer science: \"theory of computation\", \"algorithms and data structures\", \"programming methodology and languages\", and \"computer elements and architecture\". In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human\u2013computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.\nTheoretical computer science.\n\"Theoretical computer science\" is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\nTheory of computation.\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.\nInformation and coding theory.\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\nData structures and algorithms.\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\nProgramming language theory and formal methods.\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\nApplied computer science.\nComputer graphics and visualization.\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\nImage and sound processing.\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier \u2013 whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. \"What is the lower bound on the complexity of fast Fourier transform algorithms?\" is one of the unsolved problems in theoretical computer science.\nComputational science, finance and engineering.\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.\nHuman\u2013computer interaction.\nHuman\u2013computer interaction (HCI) is the field of study and research concerned with the design and use of computer systems, mainly based on the analysis of the interaction between humans and computer interfaces. HCI has several subfields that focus on the relationship between emotions, social behavior and brain activity with computers.\nSoftware engineering.\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software\u2014it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\nArtificial intelligence.\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\nComputer systems.\nComputer architecture and microarchitecture.\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \"architecture\" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM's main research center in 1959.\nConcurrent, parallel and distributed computing.\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the parallel random access machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.\nComputer networks.\nThis branch of computer science aims to manage networks between computers worldwide.\nComputer security and cryptography.\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.\nDatabases and data mining.\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\nDiscoveries.\nThe philosopher of computing Bill Rapaport noted three \"Great Insights of Computer Science\":\nProgramming paradigms.\nProgramming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\nMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.\nResearch.\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals."}
{"id": "5324", "revid": "4842600", "url": "https://en.wikipedia.org/wiki?curid=5324", "title": "Catalan", "text": "Catalan may refer to:\nCatalonia.\nFrom, or related to Catalonia:\nMathematics.\nMathematical concepts named after mathematician Eug\u00e8ne Catalan:"}
{"id": "5326", "revid": "85186", "url": "https://en.wikipedia.org/wiki?curid=5326", "title": "Creationism", "text": "Creationism is the religious belief that nature, and aspects such as the universe, Earth, life, and humans, originated with supernatural acts of divine creation, and is often pseudoscientific. In its broadest sense, creationism includes various religious views, which differ in their acceptance or rejection of modern scientific concepts such as evolution that describe the origin and development of natural phenomena.\nThe term \"creationism\" most often refers to belief in special creation: the claim that the universe and lifeforms were created as they exist today by divine action, and that the only true explanations are those which are compatible with a Christian fundamentalist literal interpretation of the creation myth found in the Bible's Genesis creation narrative. Since the 1970s, the most common form of this has been Young Earth creationism which posits special creation of the universe and lifeforms within the last 10,000 years on the basis of flood geology, and promotes pseudoscientific creation science. From the 18th century onward, Old Earth creationism accepted geological time harmonized with Genesis through gap or day-age theory, while supporting anti-evolution. Modern old-Earth creationists support progressive creationism and continue to reject evolutionary explanations. Following political controversy, creation science was reformulated as intelligent design and neo-creationism.\nMainline Protestants and the Catholic Church reconcile modern science with their faith in Creation through forms of theistic evolution which hold that God purposefully created through the laws of nature, and accept evolution. Some groups call their belief evolutionary creationism. Less prominently, there are also members of the Islamic and Hindu faiths who are creationists. Use of the term \"creationist\" in this context dates back to Charles Darwin's unpublished 1842 sketch draft for what became \"On the Origin of Species\", and he used the term later in letters to colleagues. In 1873, Asa Gray published an article in \"The Nation\" saying a \"special creationist\" who held that species \"were supernaturally originated just as they are, by the very terms of his doctrine places them out of the reach of scientific explanation.\"\nBiblical basis.\nThe basis for many creationists' beliefs is a literal or quasi-literal interpretation of the Book of Genesis. The Genesis creation narratives (Genesis 1\u20132) describe how God brings the Universe into being in a series of creative acts over six days and places the first man and woman (Adam and Eve) in the Garden of Eden. This story is the basis of creationist cosmology and biology. The Genesis flood narrative (Genesis 6\u20139) tells how God destroys the world and all life through a great flood, saving representatives of each form of life by means of Noah's Ark. This forms the basis of creationist geology, better known as flood geology.\nRecent decades have seen attempts to de-link creationism from the Bible and recast it as science; these include creation science and intelligent design.\nTypes.\nTo counter the common misunderstanding that the creation\u2013evolution controversy was a simple dichotomy of views, with \"creationists\" set against \"evolutionists\", Eugenie Scott of the National Center for Science Education produced a diagram and description of a continuum of religious views as a spectrum ranging from extreme literal biblical creationism to materialist evolution, grouped under main headings. This was used in public presentations, then published in 1999 in \"Reports of the NCSE\". Other versions of a taxonomy of creationists were produced, and comparisons made between the different groupings. In 2009 Scott produced a revised continuum taking account of these issues, emphasizing that intelligent design creationism overlaps other types, and each type is a grouping of various beliefs and positions. The revised diagram is labelled to shows a spectrum relating to positions on the age of the Earth, and the part played by special creation as against evolution. This was published in the book \"Evolution Vs. Creationism: An Introduction\", and the NCSE website rewritten on the basis of the book version.\nThe main general types are listed below.\nYoung Earth creationism.\nYoung Earth creationists such as Ken Ham and Doug Phillips believe that God created the Earth within the last ten thousand years, with a literalist interpretation of the Genesis creation narrative, within the approximate time-frame of biblical genealogies. Most young Earth creationists believe that the universe has a similar age as the Earth. A few assign a much older age to the universe than to Earth. Young Earth creationism gives the universe an age consistent with the Ussher chronology and other young Earth time frames. Other young Earth creationists believe that the Earth and the universe were created with the appearance of age, so that the world appears to be much older than it is, and that this appearance is what gives the geological findings and other methods of dating the Earth and the universe their much longer timelines.\nThe Christian organizations Answers in Genesis (AiG), Institute for Creation Research (ICR) and the Creation Research Society (CRS) promote young Earth creationism in the United States. Carl Baugh's Creation Evidence Museum in Texas, United States AiG's Creation Museum and Ark Encounter in Kentucky, United States were opened to promote young Earth creationism. Creation Ministries International promotes young Earth views in Australia, Canada, South Africa, New Zealand, the United States, and the United Kingdom.\nAmong Roman Catholics, the Kolbe Center for the Study of Creation promotes similar ideas.\nOld Earth creationism.\nOld Earth creationism holds that the physical universe was created by God, but that the creation event described in the Book of Genesis is to be taken figuratively. This group generally believes that the age of the universe and the age of the Earth are as described by astronomers and geologists, but that details of modern evolutionary theory are questionable.\nOld Earth creationism itself comes in at least three types:\nGap creationism.\nGap creationism (also known as \"ruin-restoration creationism\", \"restoration creationism\", or \"the Gap Theory\") is a form of old Earth creationism that posits that the six-\"yom\" creation period, as described in the Book of Genesis, involved six literal 24-hour days, but that there was a gap of time between two distinct creations in the first and the second verses of Genesis, which the theory states explains many scientific observations, including the age of the Earth. Thus, the six days of creation (verse 3 onwards) start sometime after the Earth was \"without form and void.\" This allows an indefinite gap of time to be inserted after the original creation of the universe, but prior to the Genesis creation narrative, (when present biological species and humanity were created). Gap theorists can therefore agree with the scientific consensus regarding the age of the Earth and universe, while maintaining a literal interpretation of the biblical text.\nSome gap creationists expand the basic version of creationism by proposing a \"primordial creation\" of biological life within the \"gap\" of time. This is thought to be \"the world that then was\" mentioned in 2 Peter 3:3\u20136. Discoveries of fossils and archaeological ruins older than 10,000 years are generally ascribed to this \"world that then was,\" which may also be associated with Lucifer's rebellion.\nDay-age creationism.\nDay-age creationism, a type of old Earth creationism, is a metaphorical interpretation of the creation accounts in Genesis. It holds that the six days referred to in the Genesis account of creation are not ordinary 24-hour days, but are much longer periods (from thousands to billions of years). The Genesis account is then reconciled with the age of the Earth. Proponents of the day-age theory can be found among both theistic evolutionists, who accept the scientific consensus on evolution, and progressive creationists, who reject it. The theories are said to be built on the understanding that the Hebrew word \"yom\" is also used to refer to a time period, with a beginning and an end and not necessarily that of a 24-hour day.\nThe day-age theory attempts to reconcile the Genesis creation narrative and modern science by asserting that the creation \"days\" were not ordinary 24-hour days, but actually lasted for long periods of time (as day-age implies, the \"days\" each lasted an age). According to this view, the sequence and duration of the creation \"days\" may be paralleled to the scientific consensus for the age of the earth and the universe.\nProgressive creationism.\nProgressive creationism is the religious belief that God created new forms of life gradually over a period of hundreds of millions of years. As a form of old Earth creationism, it accepts mainstream geological and cosmological estimates for the age of the Earth, some tenets of biology such as microevolution as well as archaeology to make its case. In this view creation occurred in rapid bursts in which all \"kinds\" of plants and animals appear in stages lasting millions of years. The bursts are followed by periods of stasis or equilibrium to accommodate new arrivals. These bursts represent instances of God creating new types of organisms by divine intervention. As viewed from the archaeological record, progressive creationism holds that \"species do not gradually appear by the steady transformation of its ancestors; [but] appear all at once and \"fully formed.\"\nThe view rejects macroevolution, claiming it is biologically untenable and not supported by the fossil record, as well as rejects the concept of common descent from a last universal common ancestor. Thus the evidence for macroevolution is claimed to be false, but microevolution is accepted as a genetic parameter designed by the Creator into the fabric of genetics to allow for environmental adaptations and survival. Generally, it is viewed by proponents as a middle ground between literal creationism and evolution. Organizations such as Reasons To Believe, founded by Hugh Ross, promote this version of creationism.\nProgressive creationism can be held in conjunction with hermeneutic approaches to the Genesis creation narrative such as the day-age creationism or framework/metaphoric/poetic views.\nPhilosophic and scientific creationism.\nCreation science.\nCreation science, or initially scientific creationism, is a pseudoscience that emerged in the 1960s with proponents aiming to have young Earth creationist beliefs taught in school science classes as a counter to teaching of evolution. Common features of creation science argument include: creationist cosmologies which accommodate a universe on the order of thousands of years old, criticism of radiometric dating through a technical argument about radiohalos, explanations for the fossil record as a record of the Genesis flood narrative (see flood geology), and explanations for the present diversity as a result of pre-designed genetic variability and partially due to the rapid degradation of the perfect genomes God placed in \"created kinds\" or \"baramins\" due to mutations.\nNeo-creationism.\nNeo-creationism is a pseudoscientific movement which aims to restate creationism in terms more likely to be well received by the public, by policy makers, by educators and by the scientific community. It aims to re-frame the debate over the origins of life in non-religious terms and without appeals to scripture. This comes in response to the 1987 ruling by the United States Supreme Court in \"Edwards v. Aguillard\" that creationism is an inherently religious concept and that advocating it as correct or accurate in public-school curricula violates the Establishment Clause of the First Amendment.\nOne of the principal claims of neo-creationism propounds that ostensibly objective orthodox science, with a foundation in naturalism, is actually a dogmatically atheistic religion. Its proponents argue that the scientific method excludes certain explanations of phenomena, particularly where they point towards supernatural elements, thus effectively excluding religious insight from contributing to understanding the universe. This leads to an open and often hostile opposition to what neo-creationists term \"Darwinism\", which they generally mean to refer to evolution, but which they may extend to include such concepts as abiogenesis, stellar evolution and the Big Bang theory.\nUnlike their philosophical forebears, neo-creationists largely do not believe in many of the traditional cornerstones of creationism such as a young Earth, or in a dogmatically literal interpretation of the Bible.\nIntelligent design.\nIntelligent design (ID) is the pseudoscientific view that \"certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection.\" All of its leading proponents are associated with the Discovery Institute, a think tank whose wedge strategy aims to replace the scientific method with \"a science consonant with Christian and theistic convictions\" which accepts supernatural explanations. It is widely accepted in the scientific and academic communities that intelligent design is a form of creationism, and is sometimes referred to as \"intelligent design creationism.\"\nID originated as a re-branding of creation science in an attempt to avoid a series of court decisions ruling out the teaching of creationism in American public schools, and the Discovery Institute has run a series of campaigns to change school curricula. In Australia, where curricula are under the control of state governments rather than local school boards, there was a public outcry when the notion of ID being taught in science classes was raised by the Federal Education Minister Brendan Nelson; the minister quickly conceded that the correct forum for ID, if it were to be taught, is in religious or philosophy classes.\nIn the US, teaching of intelligent design in public schools has been decisively ruled by a federal district court to be in violation of the Establishment Clause of the First Amendment to the United States Constitution. In Kitzmiller v. Dover, the court found that intelligent design is not science and \"cannot uncouple itself from its creationist, and thus religious, antecedents,\" and hence cannot be taught as an alternative to evolution in public school science classrooms under the jurisdiction of that court. This sets a persuasive precedent, based on previous US Supreme Court decisions in \"Edwards v. Aguillard\" and \"Epperson v. Arkansas\" (1968), and by the application of the Lemon test, that creates a legal hurdle to teaching intelligent design in public school districts in other federal court jurisdictions.\nGeocentrism.\nIn astronomy, the geocentric model (also known as geocentrism, or the Ptolemaic system), is a description of the cosmos where Earth is at the orbital center of all celestial bodies. This model served as the predominant cosmological system in many ancient civilizations such as ancient Greece. As such, they assumed that the Sun, Moon, stars, and naked eye planets circled Earth, including the noteworthy systems of Aristotle (see Aristotelian physics) and Ptolemy.\nArticles arguing that geocentrism was the biblical perspective appeared in some early creation science newsletters associated with the Creation Research Society pointing to some passages in the Bible, which, when taken literally, indicate that the daily apparent motions of the Sun and the Moon are due to their actual motions around the Earth rather than due to the rotation of the Earth about its axis. For example, where the Sun and Moon are said to stop in the sky, and where the world is described as immobile. Contemporary advocates for such religious beliefs include Robert Sungenis, co-author of the self-published \"Galileo Was Wrong: The Church Was Right\" (2006). These people subscribe to the view that a plain reading of the Bible contains an accurate account of the manner in which the universe was created and requires a geocentric worldview. \nMost contemporary creationist organizations reject such perspectives.\nOmphalos hypothesis.\nThe Omphalos hypothesis is one attempt to reconcile the scientific evidence that the universe is billions of years old with a literal interpretation of the Genesis creation narrative, which implies that the Earth is only a few thousand years old. It is based on the religious belief that the universe was created by a divine being, within the past six to ten thousand years (in keeping with flood geology), and that the presence of objective, verifiable evidence that the universe is older than approximately ten millennia is due to the creator introducing false evidence that makes the universe appear significantly older.\nThe idea was named after the title of an 1857 book, \"Omphalos\" by Philip Henry Gosse, in which Gosse argued that in order for the world to be functional God must have created the Earth with mountains and canyons, trees with growth rings, Adam and Eve with fully grown hair, fingernails, and navels (\u1f40\u03bc\u03c6\u03b1\u03bb\u03cc\u03c2 \"omphalos\" is Greek for \"navel\"), and all living creatures with fully formed evolutionary features, etc..., and that, therefore, \"no\" empirical evidence about the age of the Earth or universe can be taken as reliable.\nVarious supporters of Young Earth creationism have given different explanations for their belief that the universe is filled with false evidence of the universe's age, including a belief that some things needed to be created at a certain age for the ecosystems to function, or their belief that the creator was deliberately planting deceptive evidence. The idea has seen some revival in the 20th century by some modern creationists, who have extended the argument to address the \"starlight problem\". The idea has been criticised as Last Thursdayism, and on the grounds that it requires a deliberately deceptive creator.\nTheistic evolution.\nTheistic evolution, or evolutionary creation, is a belief that \"the personal God of the Bible created the universe and life through evolutionary processes.\" According to the American Scientific Affiliation:\nThrough the 19th century the term \"creationism\" most commonly referred to direct creation of individual souls, in contrast to traducianism. Following the publication of \"Vestiges of the Natural History of Creation\", there was interest in ideas of Creation by divine law. In particular, the liberal theologian Baden Powell argued that this illustrated the Creator's power better than the idea of miraculous creation, which he thought ridiculous. When \"On the Origin of Species\" was published, the cleric Charles Kingsley wrote of evolution as \"just as noble a conception of Deity.\" Darwin's view at the time was of God creating life through the laws of nature, and the book makes several references to \"creation,\" though he later regretted using the term rather than calling it an unknown process. In America, Asa Gray argued that evolution is the secondary effect, or \"modus operandi\", of the first cause, design, and published a pamphlet defending the book in theistic terms, \"Natural Selection not inconsistent with Natural Theology\". Theistic evolution, also called, evolutionary creation, became a popular compromise, and St. George Jackson Mivart was among those accepting evolution but attacking Darwin's naturalistic mechanism. Eventually it was realised that supernatural intervention could not be a scientific explanation, and naturalistic mechanisms such as neo-Lamarckism were favoured as being more compatible with purpose than natural selection.\nSome theists took the general view that, instead of faith being in opposition to biological evolution, some or all classical religious teachings about Christian God and creation are compatible with some or all of modern scientific theory, including specifically evolution; it is also known as \"evolutionary creation.\" In \"Evolution versus Creationism\", Eugenie Scott and Niles Eldredge state that it is in fact a type of evolution.\nIt generally views evolution as a tool used by God, who is both the first cause and immanent sustainer/upholder of the universe; it is therefore well accepted by people of strong theistic (as opposed to deistic) convictions. Theistic evolution can synthesize with the day-age creationist interpretation of the Genesis creation narrative; however most adherents consider that the first chapters of the Book of Genesis should not be interpreted as a \"literal\" description, but rather as a literary framework or allegory.\nFrom a theistic viewpoint, the underlying laws of nature were designed by God for a purpose, and are so self-sufficient that the complexity of the entire physical universe evolved from fundamental particles in processes such as stellar evolution, life forms developed in biological evolution, and in the same way the origin of life by natural causes has resulted from these laws.\nIn one form or another, theistic evolution is the view of creation taught at the majority of mainline Protestant seminaries. For Roman Catholics, human evolution is not a matter of religious teaching, and must stand or fall on its own scientific merits. Evolution and the Roman Catholic Church are not in conflict. The Catechism of the Catholic Church comments positively on the theory of evolution, which is neither precluded nor required by the sources of faith, stating that scientific studies \"have splendidly enriched our knowledge of the age and dimensions of the cosmos, the development of life-forms and the appearance of man.\" Roman Catholic schools teach evolution without controversy on the basis that scientific knowledge does not extend beyond the physical, and scientific truth and religious truth cannot be in conflict. Theistic evolution can be described as \"creationism\" in holding that divine intervention brought about the origin of life or that divine laws govern formation of species, though many creationists (in the strict sense) would deny that the position is creationism at all. In the creation\u2013evolution controversy, its proponents generally take the \"evolutionist\" side. This sentiment was expressed by Fr. George Coyne, (the Vatican's chief astronomer between 1978 and 2006):...in America, creationism has come to mean some fundamentalistic, literal, scientific interpretation of Genesis. Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in a belief that everything depends upon God, or better, all is a gift from God.\nWhile supporting the methodological naturalism inherent in modern science, the proponents of theistic evolution reject the implication taken by some atheists that this gives credence to ontological materialism. In fact, many modern philosophers of science, including atheists, refer to the long-standing convention in the scientific method that observable events in nature should be explained by natural causes, with the distinction that it does not assume the actual existence or non-existence of the supernatural. \nReligious views.\nThere are also non-Christian forms of creationism, notably Islamic creationism and Hindu creationism.\nBah\u00e1'\u00ed Faith.\nIn the creation myth taught by Bah\u00e1'u'll\u00e1h, the Bah\u00e1'\u00ed Faith founder, the universe has \"neither beginning nor ending,\" and that the component elements of the material world have always existed and will always exist. With regard to evolution and the origin of human beings, 'Abdu'l-Bah\u00e1 gave extensive comments on the subject when he addressed western audiences in the beginning of the 20th century. Transcripts of these comments can be found in \"Some Answered Questions\", \"Paris Talks\" and \"The Promulgation of Universal Peace\". 'Abdu'l-Bah\u00e1 described the human species as having evolved from a primitive form to modern man, but that the capacity to form human intelligence was always in existence.\nBuddhism.\nBuddhism denies a creator deity and posits that mundane deities such as Mahabrahma are sometimes misperceived to be a creator. While Buddhism includes belief in divine beings called devas, it holds that they are mortal, limited in their power, and that none of them are creators of the universe. In the Sa\u1e43yutta Nik\u0101ya, the Buddha also states that the cycle of rebirths stretches back hundreds of thousands of eons, without discernible beginning.\nMajor Buddhist Indian philosophers such as Nagarjuna, Vasubandhu, Dharmakirti and Buddhaghosa, consistently critiqued Creator God views put forth by Hindu thinkers.\nChristianity.\n, most Christians around the world accepted evolution as the most likely explanation for the origins of species, and did not take a literal view of the Genesis creation narrative. The United States is an exception where belief in religious fundamentalism is much more likely to affect attitudes towards evolution than it is for believers elsewhere. Political partisanship affecting religious belief may be a factor because political partisanship in the US is highly correlated with fundamentalist thinking, unlike in Europe.\nMost contemporary Christian leaders and scholars from mainstream churches, such as Anglicans and Lutherans, consider that there is no conflict between the spiritual meaning of creation and the science of evolution. According to the former archbishop of Canterbury, Rowan Williams, \"for most of the history of Christianity, and I think this is fair enough, most of the history of the Christianity there's been an awareness that a belief that everything depends on the creative act of God, is quite compatible with a degree of uncertainty or latitude about how precisely that unfolds in creative time.\"\nLeaders of the Anglican and Roman Catholic churches have made statements in favor of evolutionary theory, as have scholars such as the physicist John Polkinghorne, who argues that evolution is one of the principles through which God created living beings. Earlier supporters of evolutionary theory include Frederick Temple, Asa Gray and Charles Kingsley who were enthusiastic supporters of Darwin's theories upon their publication, and the French Jesuit priest and geologist Pierre Teilhard de Chardin saw evolution as confirmation of his Christian beliefs, despite condemnation from Church authorities for his more speculative theories. Another example is that of Liberal theology, not providing any creation models, but instead focusing on the symbolism in beliefs of the time of authoring Genesis and the cultural environment.\nMany Christians and Jews had been considering the idea of the creation history as an allegory (instead of historical) long before the development of Darwin's theory of evolution. For example, Philo, whose works were taken up by early Church writers, wrote that it would be a mistake to think that creation happened in six days, or in any set amount of time. Augustine of the late fourth century who was also a former neoplatonist argued that everything in the universe was created by God at the same moment in time (and not in six days as a literal reading of the Book of Genesis would seem to require); It appears that both Philo and Augustine felt uncomfortable with the idea of a seven-day creation because it detracted from the notion of God's omnipotence. In 1950, Pope Pius XII stated limited support for the idea in his encyclical . In 1996, Pope John Paul II stated that \"new knowledge has led to the recognition of the theory of evolution as more than a hypothesis,\" but, referring to previous papal writings, he concluded that \"if the human body takes its origin from pre-existent living matter, the spiritual soul is immediately created by God.\"\nIn the US, Evangelical Christians have continued to believe in a literal Genesis. , members of evangelical Protestant (70%), Mormon (76%) and Jehovah's Witnesses (90%) denominations were the most likely to reject the evolutionary interpretation of the origins of life.\nJehovah's Witnesses assert that scientific evidence about the age of the universe is compatible with the Bible, but that the 'days' after Genesis 1:1 were each thousands of years in length. They view this belief as an alternative to Creationism rather than a variation of Creationism.\nThe historic Christian literal interpretation of creation requires the harmonization of the two creation stories, Genesis 1:1\u20132:3 and Genesis 2:4\u201325, for there to be a consistent interpretation. They sometimes seek to ensure that their belief is taught in science classes, mainly in American schools. Opponents reject the claim that the literalistic biblical view meets the criteria required to be considered scientific. Many religious groups teach that God created the Cosmos. From the days of the early Christian Church Fathers there were allegorical interpretations of the Book of Genesis as well as literal aspects.\nChristian Science, a system of thought and practice derived from the writings of Mary Baker Eddy, interprets the Book of Genesis figuratively rather than literally. It holds that the material world is an illusion, and consequently not created by God: the only real creation is the spiritual realm, of which the material world is a distorted version. Christian Scientists regard the story of the creation in the Book of Genesis as having symbolic rather than literal meaning. According to Christian Science, both creationism and evolution are false from an absolute or \"spiritual\" point of view, as they both proceed from a (false) belief in the reality of a material universe. However, Christian Scientists do not oppose the teaching of evolution in schools, nor do they demand that alternative accounts be taught: they believe that both material science and literalist theology are concerned with the illusory, mortal and material, rather than the real, immortal and spiritual. With regard to material theories of creation, Eddy showed a preference for Darwin's theory of evolution over others.\nHinduism.\nHindu creationists claim that species of plants and animals are material forms adopted by pure consciousness which live an endless cycle of births and rebirths. Ronald Numbers says that: \"Hindu Creationists have insisted on the antiquity of humans, who they believe appeared fully formed as long, perhaps, as trillions of years ago.\" Hindu creationism is a form of old Earth creationism, according to Hindu creationists the universe may even be older than billions of years. These views are based on the Vedas, the creation myths of which depict an extreme antiquity of the universe and history of the Earth.\nIn Hindu cosmology, time cyclically repeats general events of creation and destruction, with many \"first man\", each known as Manu, the progenitor of mankind. Each Manu successively reigns over a 306.72 million year period known as a , each ending with the destruction of mankind followed by a (period of non-activity) before the next . 120.53million years have elapsed in the current (current mankind) according to calculations on Hindu units of time. The universe is cyclically created at the start and destroyed at the end of a (day of Brahma), lasting for 4.32billion years, which is followed by a (period of dissolution) of equal length. 1.97billion years have elapsed in the current (current universe). The universal elements or building blocks (unmanifest matter) exists for a period known as a , lasting for 311.04trillion years, which is followed by a (period of great dissolution) of equal length. 155.52trillion years have elapsed in the current .\nIslam.\nThe creation myths in the Quran are more vague and allow for a wider range of interpretations similar to those in other Abrahamic religions.\nIslam also has its own school of theistic evolutionism, which holds that mainstream scientific analysis of the origin of the universe is supported by the Quran. Some Muslims believe in evolutionary creation, especially among liberal movements within Islam.\nWriting for \"The Boston Globe\", Drake Bennett noted: \"Without a Book of Genesis to account for[...] Muslim creationists have little interest in proving that the age of the Earth is measured in the thousands rather than the billions of years, nor do they show much interest in the problem of the dinosaurs. And the idea that animals might evolve into other animals also tends to be less controversial, in part because there are passages of the Koran that seem to support it. But the issue of whether human beings are the product of evolution is just as fraught among Muslims.\" Khalid Anees, president of the Islamic Society of Britain, states that Muslims do not agree that one species can develop from another.\nOttoman-Lebanese Sunni scholar Hussein al-Jisr, declared that there is no contradiction between evolution and the Islamic scriptures. He stated that \"there is no evidence in the Quran to suggest whether all species, each of which exists by the grace of God, were created all at once or gradually\", and referred to the aforementioned story of creation in S\u016brat al-Anbiy\u0101. In Kemalist Turkey, important scholars strove to accommodate the theory of evolution in Islamic scripture during the first decades of the Turkish Republic; their approach to the theory defended Islamic belief in the face of scientific theories of their times. \nThe Saudi Arabian government, on the other hand, began funding and promoting denial of evolution in the 1970s in accordance to its Salafi-Wahhabi interpretation of Islam. This stance garnered criticism from the governments and academics of mainline Muslim countries such as Turkey, Pakistan, Lebanon, and Iran, where evolution was initially taught and promoted. Since the 1980s, Turkey has been a site of strong advocacy for creationism, supported by American adherents.\nJudaism.\nFor Orthodox Jews who seek to reconcile discrepancies between science and the creation myths in the Bible, the notion that science and the Bible should even be reconciled through traditional scientific means is questioned. To these groups, science is as true as the Torah and if there seems to be a problem, epistemological limits are to blame for apparently irreconcilable points. They point to discrepancies between what is expected and what actually is to demonstrate that things are not always as they appear. They note that even the root word for 'world' in the Hebrew language, , means 'hidden' (). Just as they know from the Torah that God created man and trees and the light on its way from the stars in their observed state, so too can they know that the world was created in its over the six days of Creation that reflects progression to its currently-observed state, with the understanding that physical ways to verify this may eventually be identified. This knowledge has been advanced by Rabbi Dovid Gottlieb, former philosophy professor at Johns Hopkins University.\nKabbalistic sources from well before the scientifically apparent age of the universe was first determined are also in close concord with modern scientific estimates of the age of the universe, according to Rabbi Aryeh Kaplan, and based on Sefer Temunah, an early kabbalistic work attributed to the first-century Tanna Nehunya ben HaKanah. Many kabbalists accepted the teachings of the Sefer HaTemunah, including the medieval Jewish scholar Nahmanides, his close student Isaac ben Samuel of Acre, and David ben Solomon ibn Abi Zimra. Other parallels are derived, among other sources, from Nahmanides, who expounds that there was a Neanderthal-like species with which Adam mated (he did this long before Neanderthals had even been discovered scientifically). Reform Judaism does not take the Torah as a literal text, but rather as a symbolic or open-ended work.\nSome contemporary writers such as Rabbi Gedalyah Nadel have sought to reconcile the discrepancy between the account in the Torah, and scientific findings by arguing that each day referred to in the Bible was not 24 hours, but billions of years long. Others claim that the Earth was created a few thousand years ago, but was deliberately made to look as if it was five billion years old, e.g. by being created with ready made fossils. The best known exponent of this approach being Rabbi Menachem Mendel Schneerson. Others state that although the world was physically created in six 24-hour days, the Torah accounts can be interpreted to mean that there was a period of billions of years before the six days of creation.\nPrevalence.\nMost vocal literalist creationists are from the US, and strict creationist views are much less common in other developed countries. According to a study published in \"Science\", a survey of the US, Turkey, Japan and Europe showed that public acceptance of evolution is most prevalent in Iceland, Denmark and Sweden at 80% of the population. There seems to be no significant correlation between believing in evolution and understanding evolutionary science.\nAustralia.\nA 2009 Nielsen poll showed that 23% of Australians believe \"the biblical account of human origins,\" 42% believe in a \"wholly scientific\" explanation for the origins of life, while 32% believe in an evolutionary process \"guided by God\".\nA 2013 survey conducted by Auspoll and the Australian Academy of Science found that 80% of Australians believe in evolution (70% believe it is currently occurring, 10% believe in evolution but do not think it is currently occurring), 12% were not sure and 9% stated they do not believe in evolution.\nBrazil.\nA 2011 Ipsos survey found that 47% of responders in Brazil identified themselves as \"creationists and believe that human beings were in fact created by a spiritual force such as the God they believe in and do not believe that the origin of man came from evolving from other species such as apes\".\nIn 2004, IBOPE conducted a poll in Brazil that asked questions about creationism and the teaching of creationism in schools. When asked if creationism should be taught in schools, 89% of people said that creationism should be taught in schools. When asked if the teaching of creationism should replace the teaching of evolution in schools, 75% of people said that the teaching of creationism should replace the teaching of evolution in schools.\nCanada.\nA 2012 survey, by Angus Reid Public Opinion revealed that 61 percent of Canadians believe in evolution. The poll asked \"Where did human beings come fromdid we start as singular cells millions of year ago and evolve into our present form, or did God create us in his image 10,000 years ago?\"\nIn 2019, a Research Co. poll asked people in Canada if creationism \"should be part of the school curriculum in their province\". 38% of Canadians said that creationism should be part of the school curriculum, 39% of Canadians said that it should not be part of the school curriculum, and 23% of Canadians were undecided.\nIn 2023, a Research Co. poll found that 21% of Canadians \"believe God created human beings in their present form within the last 10,000 years\". The poll also found that \"More than two-in-five Canadians (43%) think creationism should be part of the school curriculum in their province.\"\nEurope.\nIn Europe, literalist creationism is more widely rejected, though regular opinion polls are not available. Most people accept that evolution is the most widely accepted scientific theory as taught in most schools. In countries with a Roman Catholic majority, papal acceptance of evolutionary creationism as worthy of study has essentially ended debate on the matter for many people.\nIn the UK, a 2006 poll on the \"origin and development of life\", asked participants to choose between three different perspectives on the origin of life: 22% chose creationism, 17% opted for intelligent design, 48% selected evolutionary theory, and the rest did not know. A subsequent 2010 YouGov poll on the correct explanation for the origin of humans found that 9% opted for creationism, 12% intelligent design, 65% evolutionary theory and 13% didn't know. The former Archbishop of Canterbury Rowan Williams, head of the worldwide Anglican Communion, views the idea of teaching creationism in schools as a mistake. In 2009, an Ipsos Mori survey in the United Kingdom found that 54% of Britons agreed with the view: \"Evolutionary theories should be taught in science lessons in schools together with other possible perspectives, such as intelligent design and creationism.\"\nIn Italy, Education Minister Letizia Moratti wanted to retire evolution from the secondary school level; after one week of massive protests, she reversed her opinion.\nThere continues to be scattered and possibly mounting efforts on the part of religious groups throughout Europe to introduce creationism into public education. In response, the Parliamentary Assembly of the Council of Europe has released a draft report titled \"The dangers of creationism in education\" on June 8, 2007, reinforced by a further proposal of banning it in schools dated October 4, 2007.\nSerbia suspended the teaching of evolution for one week in September 2004, under education minister Ljiljana \u010coli\u0107, only allowing schools to reintroduce evolution into the curriculum if they also taught creationism. \"After a deluge of protest from scientists, teachers and opposition parties\" says the BBC report, \u010coli\u0107's deputy made the statement, \"I have come here to confirm Charles Darwin is still alive\" and announced that the decision was reversed. \u010coli\u0107 resigned after the government said that she had caused \"problems that had started to reflect on the work of the entire government.\"\nPoland saw a major controversy over creationism in 2006, when the Deputy Education Minister, Miros\u0142aw Orzechowski, denounced evolution as \"one of many lies\" taught in Polish schools. His superior, Minister of Education Roman Giertych, has stated that the theory of evolution would continue to be taught in Polish schools, \"as long as most scientists in our country say that it is the right theory.\" Giertych's father, Member of the European Parliament Maciej Giertych, has opposed the teaching of evolution and has claimed that dinosaurs and humans co-existed.\nA June 2015 \u2013 July 2016 Pew poll of Eastern European countries, found that 56% of people from Armenia say that humans and other living things have \"Existed in present state since the beginning of time\". Armenia is followed by 52% from Bosnia, 42% from Moldova, 37% from Lithuania, 34% from Georgia and Ukraine, 33% from Croatia and Romania, 31% from Bulgaria, 29% from Greece and Serbia, 26% from Russia, 25% from Latvia, 23% from Belarus and Poland, 21% from Estonia and Hungary, and 16% from the Czech Republic.\nSouth Africa.\nA 2011 Ipsos survey found that 56% of responders in South Africa identified themselves as \"creationists and believe that human beings were in fact created by a spiritual force such as the God they believe in and do not believe that the origin of man came from evolving from other species such as apes\".\nSouth Korea.\nIn 2009, an EBS survey in South Korea found that 63% of people believed that creation and evolution should both be taught in schools simultaneously.\nUnited States.\nA 2017 poll by Pew Research found that 62% of Americans believe humans have evolved over time and 34% of Americans believe humans and other living things have existed in their present form since the beginning of time. A 2019 Gallup creationism survey found that 40% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings.\nAccording to a 2014 Gallup poll, about 42% of Americans believe that \"God created human beings pretty much in their present form at one time within the last 10,000 years or so.\" Another 31% believe that \"human beings have developed over millions of years from less advanced forms of life, but God guided this process,\"and 19% believe that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process.\"\nBelief in creationism is inversely correlated to education; of those with postgraduate degrees, 74% accept evolution. In 1987, \"Newsweek\" reported: \"By one count there are some 700 scientists with respectable academic credentials (out of a total of 480,000 U.S. earth and life scientists) who give credence to creation-science, the general theory that complex life forms did not evolve but appeared 'abruptly.'\"\nA 2000 poll for People for the American Way found 70% of the US public felt that evolution was compatible with a belief in God.\nAccording to a study published in \"Science\", between 1985 and 2005 the number of adult North Americans who accept evolution declined from 45% to 40%, the number of adults who reject evolution declined from 48% to 39% and the number of people who were unsure increased from 7% to 21%. Besides the US the study also compared data from 32 European countries, Turkey, and Japan. The only country where acceptance of evolution was lower than in the US was Turkey (25%).\nAccording to a 2011 Fox News poll, 45% of Americans believe in creationism, down from 50% in a similar poll in 1999. 21% believe in 'the theory of evolution as outlined by Darwin and other scientists' (up from 15% in 1999), and 27% answered that both are true (up from 26% in 1999).\nIn September 2012, educator and television personality Bill Nye spoke with the Associated Press and aired his fears about acceptance of creationism, believing that teaching children that creationism is the only true answer without letting them understand the way science works will prevent any future innovation in the world of science. In February 2014, Nye defended evolution in the classroom in a debate with creationist Ken Ham on the topic of whether creation is a viable model of origins in today's modern, scientific era.\nEducation controversies.\nIn the US, creationism has become centered in the political controversy over creation and evolution in public education, and whether teaching creationism in science classes conflicts with the separation of church and state. Currently, the controversy comes in the form of whether advocates of the intelligent design movement who wish to \"Teach the Controversy\" in science classes have conflated science with religion.\nPeople for the American Way polled 1500 North Americans about the teaching of evolution and creationism in November and December 1999. They found that most North Americans were not familiar with creationism, and most North Americans had heard of evolution, but many did not fully understand the basics of the theory. The main findings were:\nIn such political contexts, creationists argue that their particular religiously based origin belief is superior to those of other belief systems, in particular those made through secular or scientific rationale. Political creationists are opposed by many individuals and organizations who have made detailed critiques and given testimony in various court cases that the alternatives to scientific reasoning offered by creationists are opposed by the consensus of the scientific community.\nCriticism.\nChristian criticism.\nMost Christians disagree with the teaching of creationism as an alternative to evolution in schools. Several religious organizations, among them the Catholic Church, hold that their faith does not conflict with the scientific consensus regarding evolution. The Clergy Letter Project, which has collected more than 13,000 signatures, is an \"endeavor designed to demonstrate that religion and science can be compatible.\"\nIn his 2002 article \"Intelligent Design as a Theological Problem\", George Murphy argues against the view that life on Earth, in all its forms, is direct evidence of God's act of creation (Murphy quotes Phillip E. Johnson's claim that he is speaking \"of a God who acted openly and left his fingerprints on all the evidence.\"). Murphy argues that this view of God is incompatible with the Christian understanding of God as \"the one revealed in the cross and resurrection of Christ.\" The basis of this theology is Isaiah 45:15, \"Verily thou art a God that hidest thyself, O God of Israel, the Saviour.\"\nMurphy observes that the execution of a Jewish carpenter by Roman authorities is in and of itself an ordinary event and did not require divine action. On the contrary, for the crucifixion to occur, God had to limit or \"empty\" himself. It was for this reason that Paul the Apostle wrote, in Philippians 2:5-8:\nLet this mind be in you, which was also in Christ Jesus: Who, being in the form of God, thought it not robbery to be equal with God: But made himself of no reputation, and took upon him the form of a servant, and was made in the likeness of men: And being found in fashion as a man, he humbled himself, and became obedient unto death, even the death of the cross.\nMurphy concludes that,Just as the Son of God limited himself by taking human form and dying on a cross, God limits divine action in the world to be in accord with rational laws which God has chosen. This enables us to understand the world on its own terms, but it also means that natural processes hide God from scientific observation.For Murphy, a theology of the cross requires that Christians accept a \"methodological\" naturalism, meaning that one cannot invoke God to explain natural phenomena, while recognizing that such acceptance does not require one to accept a \"metaphysical\" naturalism, which proposes that nature is all that there is.\nThe Jesuit priest George Coyne has stated that it is \"unfortunate that, especially here in America, creationism has come to mean...some literal interpretation of Genesis.\" He argues that \"...Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in belief that everything depends on God, or better, all is a gift from God.\"\nTeaching of creationism.\nOther Christians have expressed qualms about teaching creationism. In March 2006, then Archbishop of Canterbury Rowan Williams, the leader of the world's Anglicans, stated his discomfort about teaching creationism, saying that creationism was \"a kind of category mistake, as if the Bible were a theory like other theories.\" He also said: \"My worry is creationism can end up reducing the doctrine of creation rather than enhancing it.\" The views of the Episcopal Churcha major American-based branch of the Anglican Communionon teaching creationism resemble those of Williams.\nThe National Science Teachers Association is opposed to teaching creationism as a science, as is the Association for Science Teacher Education, the National Association of Biology Teachers, the American Anthropological Association, the American Geosciences Institute, the Geological Society of America, the American Geophysical Union, and numerous other professional teaching and scientific societies.\nIn April 2010, the American Academy of Religion issued \"Guidelines for Teaching About Religion in K\u201012 Public Schools in the United States\", which included guidance that creation science or intelligent design should not be taught in science classes, as \"Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning.\" However, they, as well as other \"worldviews that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others.\"\nRandy Moore and Sehoya Cotner, from the biology program at the University of Minnesota, reflect on the relevance of teaching creationism in the article \"The Creationist Down the Hall: Does It Matter When Teachers Teach Creationism?\", in which they write: \"Despite decades of science education reform, numerous legal decisions declaring the teaching of creationism in public-school science classes to be unconstitutional, overwhelming evidence supporting evolution, and the many denunciations of creationism as nonscientific by professional scientific societies, creationism remains popular throughout the United States.\"\nScientific criticism.\nScience is a system of knowledge based on observation, empirical evidence, and the development of theories that yield testable explanations and predictions of natural phenomena. By contrast, creationism is often based on literal interpretations of the narratives of particular religious texts. Creationist beliefs involve purported forces that lie outside of nature, such as supernatural intervention, and often do not allow predictions at all. Therefore, these can neither be confirmed nor disproved by scientists. However, many creationist beliefs can be framed as testable predictions about phenomena such as the age of the Earth, its geological history and the origins, distributions and relationships of living organisms found on it. Early science incorporated elements of these beliefs, but as science developed these beliefs were gradually falsified and were replaced with understandings based on accumulated and reproducible evidence that often allows the accurate prediction of future results.\nSome scientists, such as Stephen Jay Gould, consider science and religion to be two compatible and complementary fields, with authorities in distinct areas of human experience, so-called non-overlapping magisteria. This view is also held by many theologians, who believe that ultimate origins and meaning are addressed by religion, but favor verifiable scientific explanations of natural phenomena over those of creationist beliefs. Other scientists, such as Richard Dawkins, reject the non-overlapping magisteria and argue that, in disproving literal interpretations of creationists, the scientific method also undermines religious texts as a source of truth. Irrespective of this diversity in viewpoints, since creationist beliefs are not supported by empirical evidence, the scientific consensus is that any attempt to teach creationism as science should be rejected."}
{"id": "5329", "revid": "1271795885", "url": "https://en.wikipedia.org/wiki?curid=5329", "title": "History of Chad", "text": " \nChad (; ), officially the Republic of Chad, is a landlocked country in Central Africa. It borders Libya to the north, Sudan to the east, the Central African Republic to the south, Cameroon and Nigeria to the southwest, and Niger to the west. Due to its distance from the sea and its largely desert climate, the country is sometimes referred to as the \"Dead Heart of Africa\".\nPrehistory.\nThe territory now known as Chad possesses some of the richest archaeological sites in Africa. A hominid skull was found by Michel Brunet, that is more than 7 million years old, the oldest discovered anywhere in the world; it has been given the name Sahelanthropus tchadensis. In 1996 Michel Brunet had unearthed a hominid jaw, which he named Australopithecus bahrelghazali, unofficially dubbed Abel. It was dated using Beryllium based Radiometric dating as living circa. 3.6\u00a0million years ago.\nDuring the 7th millennium BC, the northern half of Chad was part of a broad expanse of land, stretching from the Indus River in the east to the Atlantic Ocean in the west, in which ecological conditions favored early human settlement. Rock art of the \"Round Head\" style, found in the Ennedi region, has been dated to before the 7th millennium BC and, because of the tools with which the rocks were carved and the scenes they depict, may represent the oldest evidence in the Sahara of Neolithic industries. Many of the pottery-making and Neolithic activities in Ennedi date back further than any of those of the Nile Valley to the east.\nIn the prehistoric period, Chad was much wetter than it is today, as evidenced by large game animals depicted in rock paintings in the Tibesti and Borkou regions.\nRecent linguistic research suggests that all of Africa's major language groupings south of the Sahara Desert (except Khoisan, which is not considered a valid genetic grouping anyway), i.e. the Afro-Asiatic, Nilo-Saharan and Niger\u2013Congo phyla, originated in prehistoric times in a narrow band between Lake Chad and the Nile Valley. The origins of Chad's peoples, however, remain unclear. Several of the proven archaeological sites have been only partially studied, and other sites of great potential have yet to be mapped.\nEra of Empires (AD 900\u20131900).\nAt the end of the 1st millennium AD, the formation of states began across central Chad in the sahelian zone between the desert and the savanna. For almost the next 1,000 years, these states, their relations with each other, and their effects on the peoples who lived in stateless societies along their peripheries dominated Chad's political history. Recent research suggests that indigenous Africans founded these states, not migrating Arabic-speaking groups, as was believed previously. Nonetheless, immigrants, Arabic-speaking or otherwise, played a significant role, along with Islam, in the formation and early evolution of these states.\nMost states began as kingdoms, in which the king was considered divine and endowed with temporal and spiritual powers. All states were militaristic (or they did not survive long), but none was able to expand far into southern Chad, where forests and the tsetse fly complicated the use of cavalry. Control over the trans-Saharan trade routes that passed through the region formed the economic basis of these kingdoms. Although many states rose and fell, the most important and durable of the empires were Kanem\u2013Bornu, Baguirmi, and Ouaddai, according to most written sources (mainly court chronicles and writings of Arab traders and travelers).Chad \u2013 Era of Empires, A.D. 900\u20131900\nKanem\u2013Bornu.\nThe Kanem Empire originated in the 9th century AD to the northeast of Lake Chad. Historians agree that the leaders of the new state were ancestors of the Kanembu people. Toward the end of the 11th century the Sayfawa king (or \"mai\", the title of the Sayfawa rulers) Hummay, converted to Islam. In the following century the Sayfawa rulers expanded southward into Kanem, where was to rise their first capital, Njimi. Kanem's expansion peaked during the long and energetic reign of Mai Dunama Dabbalemi (c. 1221\u20131259).\nBy the end of the 14th century, internal struggles and external attacks had torn Kanem apart. Finally, around 1396 the Bulala invaders forced \"Mai\" Umar Idrismi to abandon Njimi and move the Kanembu people to Bornu on the western edge of Lake Chad. Over time, the intermarriage of the Kanembu and Bornu peoples created a new people and language, the Kanuri, and founded a new capital, Ngazargamu.\nKanem\u2013Bornu peaked during the reign of the outstanding statesman \"Mai\" Idris Aluma (c. 1571\u20131603). Aluma is remembered for his military skills, administrative reforms, and Islamic piety. The administrative reforms and military brilliance of Aluma sustained the empire until the mid-17th century, when its power began to fade. By the early 19th century, Kanem\u2013Bornu was clearly an empire in decline, and in 1808 Fulani warriors conquered Ngazargamu. Bornu survived, but the Sayfawa dynasty ended in 1846 and the Empire itself fell in 1893.\nBaguirmi and Ouaddai.\nThe Kingdom of Baguirmi, located southeast of Kanem-Bornu, was founded in the late 15th or early 16th century, and adopted Islam in the reign of Abdullah IV (1568\u201398). Baguirmi was in a tributary relationship with Kanem\u2013Bornu at various points in the 17th and 18th centuries, then to Ouaddai in the 19th century. In 1893, Baguirmi sultan Abd ar Rahman Gwaranga surrendered the territory to France, and it became a French protectorate.\nThe Ouaddai Kingdom, west of Kanem\u2013Bornu, was established in the early 16th century by Tunjur rulers. In the 1630s, Abd al Karim invaded and established an Islamic sultanate. Among its most impactful rulers for the next three centuries were Muhammad Sabun, who controlled a new trade route to the north and established a currency during the early 19th century, and Muhammad Sharif, whose military campaigns in the mid 19th century fended off an assimilation attempt from Darfur, conquered Baguirmi, and successfully resisted French colonization. However, Ouaddai lost its independence to France after a war from 1909 to 1912.\nColonialism (1900\u20131940).\nThe French first invaded Chad in 1891, establishing their authority through military expeditions primarily against the Muslim kingdoms. The decisive colonial battle for Chad was fought on April 22, 1900, at Battle of Kouss\u00e9ri between forces of French Major Am\u00e9d\u00e9e-Fran\u00e7ois Lamy and forces of the Sudanese warlord Rabih az-Zubayr. Both leaders were killed in the battle.\nIn 1905, administrative responsibility for Chad was placed under a governor-general stationed at Brazzaville, capital of French Equatorial Africa (FEA). Chad did not have a separate colonial status until 1920, when it was placed under a lieutenant-governor stationed in Fort-Lamy (today N'Djamena).\nTwo fundamental themes dominated Chad's colonial experience with the French: an absence of policies designed to unify the territory and an exceptionally slow pace of modernization. In the French scale of priorities, the colony of Chad ranked near the bottom, and the French came to perceive Chad primarily as a source of raw cotton and untrained labour to be used in the more productive colonies to the south.\nThroughout the colonial period, large areas of Chad were never governed effectively: in the huge BET Prefecture, the handful of French military administrators usually left the people alone, and in central Chad, French rule was only slightly more substantive. Truly speaking, France managed to govern effectively only the south. As it was full of resources of gold and aluminum so many countries tried it to colonise but French was successful\nDecolonization (1940\u20131960).\nDuring World War II, Chad was the first French colony to rejoin the Allies (August 26, 1940), after the defeat of France by Germany. Under the administration of F\u00e9lix \u00c9bou\u00e9, France's first black colonial governor, a military column, commanded by Colonel Philippe Leclerc de Hauteclocque, and including two battalions of Sara troops, moved north from N'Djamena (then Fort Lamy) to engage Axis forces in Libya, where, in partnership with the British Army's Long Range Desert Group, they captured Kufra. On January 21, 1942, N'Djamena was bombed by a German aircraft.\nAfter the war ended, local parties started to develop in Chad. The first to be born was the radical Chadian Progressive Party (PPT) in February 1947, initially headed by Panamanian born Gabriel Lisette, but from 1959 headed by Fran\u00e7ois Tombalbaye. The more conservative Chadian Democratic Union (UDT) was founded in November 1947 and represented French commercial interests and a bloc of traditional leaders composed primarily of Muslim and Ouadda\u00efan nobility. The confrontation between the PPT and UDT was more than simply ideological; it represented different regional identities, with the PPT representing the Christian and animist south and the UDT the Islamic north.\nThe PPT won the May 1957 pre-independence elections thanks to a greatly expanded franchise, and Lisette led the government of the Territorial Assembly until he lost a confidence vote on February 11, 1959. After a referendum on territorial autonomy on September 28, 1958, French Equatorial Africa was dissolved, and its four constituent states \u2013 Gabon, Congo (Brazzaville), the Central African Republic, and Chad became autonomous members of the French Community from November 28, 1958. Following Lisette's fall in February 1959 the opposition leaders Gontchome Sahoulba and Ahmed Koulamallah could not form a stable government, so the PPT was again asked to form an administration \u2013 which it did under the leadership of Fran\u00e7ois Tombalbaye on March 26, 1959. On July 12, 1960, France agreed to Chad becoming fully independent. On August 11, 1960, Chad became an independent country and Fran\u00e7ois Tombalbaye became its first president.\nThe Tombalbaye era (1960\u20131975).\nOne of the most prominent aspects of Tombalbaye's rule to prove itself was his authoritarianism and distrust of democracy. Already in January 1962 he banned all political parties except his own PPT, and started immediately concentrating all power in his own hands. His treatment of opponents, real or imagined, was extremely harsh, filling the prisons with thousands of political prisoners.\nFurthermore, he pursued constant discrimination against the central and northern regions of Chad, where the southern Chadian administrators came to be perceived as arrogant and incompetent. This resentment at last exploded in a tax revolt on September 2, 1965, in the Gu\u00e9ra Prefecture, causing 500 deaths. The year after saw the birth in Sudan of the National Liberation Front of Chad (FROLINAT), created to militarily oust Tombalbaye and the Southern dominance. It was the start of a bloody civil war.\nTombalbaye resorted to calling in French troops; while moderately successful, they were not fully able to quell the insurgency. Proving more fortunate was his choice to break with the French and seek friendly ties with Libyan Brotherly Leader Gaddafi, taking away the rebels' principal source of supplies.\nBut while he had reported some success against the rebels, Tombalbaye started behaving more and more irrationally and brutally, continuously eroding his consensus among the southern elites, which dominated all key positions in the army, the civil service and the ruling party. As a consequence on April 13, 1975, several units of N'Djamena's gendarmerie killed Tombalbaye during a coup.\nMilitary rule (1975\u20131978).\nThe coup d'\u00e9tat that terminated Tombalbaye's government received an enthusiastic response in N'Djamena. The southerner General F\u00e9lix Malloum emerged early as the chairman of the new \"junta\".\nThe new military leaders were unable to retain for long the popularity that they had gained through their overthrow of Tombalbaye. Malloum proved himself unable to cope with the FROLINAT and at the end decided his only chance was in coopting some of the rebels: in 1978 he allied himself with the insurgent leader Hiss\u00e8ne Habr\u00e9, who entered the government as prime minister.\nCivil war (1979\u20131982).\nInternal dissent within the government led Prime Minister Habr\u00e9 to send his forces against Malloum's national army in the capital in February 1979. Malloum was ousted from the presidency, but the resulting civil war among the 11 emergent factions was so widespread that it rendered the central government largely irrelevant. At that point, other African governments decided to intervene.\nA series of four international conferences held first under Nigerian and then Organization of African Unity (OAU) sponsorship attempted to bring the Chadian factions together. At the fourth conference, held in Lagos, Nigeria, in August 1979, the Lagos Accord was signed. This accord established a transitional government pending national elections. In November 1979, the Transitional Government of National Unity (GUNT) was created with a mandate to govern for 18 months. Goukouni Oueddei, a northerner, was named president; Colonel Kamougu\u00e9, a southerner, Vice President; and Habr\u00e9, Minister of Defense. This coalition proved fragile; in January 1980, fighting broke out again between Goukouni's and Habr\u00e9's forces. With assistance from Libya, Goukouni regained control of the capital and other urban centers by year's end. However, Goukouni's January 1981 statement that Chad and Libya had agreed to work for the realization of complete unity between the two countries generated intense international pressure and Goukouni's subsequent call for the complete withdrawal of external forces.\nThe Habr\u00e9 era (1982\u20131990).\nLibya's partial withdrawal to the Aozou Strip in northern Chad cleared the way for Habr\u00e9's forces to enter N\u2019Djamena in June. French troops and an OAU peacekeeping force of 3,500 Nigerian, Senegalese, and Zairian troops (partially funded by the United States) remained neutral during the conflict.\nHabr\u00e9 continued to face armed opposition on various fronts, and was brutal in his repression of suspected opponents, massacring and torturing many during his rule. In the summer of 1983, GUNT forces launched an offensive against government positions in northern and eastern Chad with heavy Libyan support. In response to Libya's direct intervention, French and Zairian forces intervened to defend Habr\u00e9, pushing Libyan and rebel forces north of the 16th parallel. In September 1984, the French and the Libyan governments announced an agreement for the mutual withdrawal of their forces from Chad. By the end of the year, all French and Zairian troops were withdrawn. Libya did not honor the withdrawal accord, and its forces continued to occupy the northern third of Chad.\nRebel commando groups (Codos) in southern Chad were broken up by government massacres in 1984. In 1985 Habr\u00e9 briefly reconciled with some of his opponents, including the Democratic Front of Chad (FDT) and the Coordinating Action Committee of the Democratic Revolutionary Council. Goukouni also began to rally toward Habr\u00e9, and with his support Habr\u00e9 successfully expelled Libyan forces from most of Chadian territory. A cease-fire between Chad and Libya held from 1987 to 1988, and negotiations over the next several years led to the 1994 International Court of Justice decision granting Chad sovereignty over the Aouzou strip, effectively ending Libyan occupation.\nThe Idriss D\u00e9by era (1990\u20132021).\nRise to power.\nHowever, rivalry between Hadjerai, Zaghawa and Gorane groups within the government grew in the late 1980s. In April 1989, Idriss D\u00e9by, one of Habr\u00e9's leading generals and a Zaghawa, defected and fled to Darfur in Sudan, from which he mounted a Zaghawa-supported series of attacks on Habr\u00e9 (a Gorane). In December 1990, with Libyan assistance and no opposition from French troops stationed in Chad, D\u00e9by's forces successfully marched on N\u2019Djamena. After 3 months of provisional government, D\u00e9by's Patriotic Salvation Movement (MPS) approved a national charter on February 28, 1991, with D\u00e9by as president.\nDuring the next two years, D\u00e9by faced at least two coup attempts. Government forces clashed violently with rebel forces, including the Movement for Democracy and Development, MDD, National Revival Committee for Peace and Democracy (CSNPD), Chadian National Front (FNT) and the Western Armed Forces (FAO), near Lake Chad and in southern regions of the country. Earlier French demands for the country to hold a National Conference resulted in the gathering of 750 delegates representing political parties (which were legalized in 1992), the government, trade unions and the army to discuss the creation of a pluralist democratic regime.\nHowever, unrest continued, sparked in part by large-scale killings of civilians in southern Chad. The CSNPD, led by Kette Moise and other southern groups entered into a peace agreement with government forces in 1994, which later broke down. Two new groups, the Armed Forces for a Federal Republic (FARF) led by former Kette ally Laokein Barde and the Democratic Front for Renewal (FDR), and a reformulated MDD clashed with government forces from 1994 to 1995.\nMultiparty elections.\nTalks with political opponents in early 1996 did not go well, but D\u00e9by announced his intent to hold presidential elections in June. D\u00e9by won the country's first multi-party presidential elections with support in the second round from opposition leader Kebzabo, defeating General Kamougue (leader of the 1975 coup against Tombalbaye). D\u00e9by's MPS party won 63 of 125 seats in the January 1997 legislative elections. International observers noted numerous serious irregularities in presidential and legislative election proceedings.\nBy mid-1997 the government signed peace deals with FARF and the MDD leadership and succeeded in cutting off the groups from their rear bases in the Central African Republic and Cameroon. Agreements also were struck with rebels from the National Front of Chad (FNT) and Movement for Social Justice and Democracy in October 1997. However, peace was short-lived, as FARF rebels clashed with government soldiers, finally surrendering to government forces in May 1998. Barde was killed in the fighting, as were hundreds of other southerners, most civilians.\nSince October 1998 Chadian Movement for Justice and Democracy (MDJT) rebels, led by Youssuf Togoimi until his death in September 2002, have skirmished with government troops in the Tibesti region, resulting in hundreds of civilian, government, and rebel casualties, but little ground won or lost. No active armed opposition has emerged in other parts of Chad, although Kette Moise, following senior postings at the Ministry of Interior, mounted a smallscale local operation near Moundou which was quickly and violently suppressed by government forces in late 2000.\nD\u00e9by, in the mid-1990s, gradually restored basic functions of government and entered into agreements with the World Bank and IMF to carry out substantial economic reforms. Oil exploitation in the southern Doba region began in June 2000, with World Bank Board approval to finance a small portion of a project, the Chad-Cameroon Petroleum Development Project, aimed at transport of Chadian crude through a 1000-km buried pipeline through Cameroon to the Gulf of Guinea. The project established unique mechanisms for World Bank, private sector, government, and civil society collaboration to guarantee that future oil revenues benefit local populations and result in poverty alleviation. Success of the project depended on multiple monitoring efforts to ensure that all parties keep their commitments. These \"unique\" mechanisms for monitoring and revenue management have faced intense criticism from the beginning. Debt relief was accorded to Chad in May 2001.\nD\u00e9by won a flawed 63% first-round victory in May 2001 presidential elections after legislative elections were postponed until spring 2002. Having accused the government of fraud, six opposition leaders were arrested (twice) and one opposition party activist was killed following the announcement of election results. However, despite claims of government corruption, favoritism of Zaghawas, and abuses by the security forces, opposition party and labor union calls for general strikes and more active demonstrations against the government have been unsuccessful. Despite movement toward democratic reform, power remains in the hands of a northern ethnic oligarchy.\nIn 2003 Chad began receiving refugees from the Darfur region of western Sudan. More than 200,000 refugees fled the fighting between two rebel groups and government-supported militias known as Janjaweed. A number of border incidents led to the Chadian\u2013Sudanese War.\nOil producing and military improvement.\nChad become an oil producer in 2003. To avoid resource curse and corruption, elaborate plans sponsored by World Bank were made. This plan ensured transparency in payments, as well as that 80% of money from oil exports would be spent on five priority development sectors, two most important of these being: education and healthcare. However money started getting diverted towards the military even before the civil war broke out. In 2006 when the civil war escalated, Chad abandoned previous economic plans sponsored by World Bank and added \"national security\" as priority development sector, money from this sector was used to improve the military. During the civil war, more than 600 million dollars were used to buy fighter jets, attack helicopters, and armored personnel carriers.\nChad earned between 10 and 11\u00a0billion dollars from oil production, and estimated 4\u00a0billion dollars were invested in the army.\nWar in the east.\nThe war started on December 23, 2005, when the government of Chad declared a state of war with Sudan and called for the citizens of Chad to mobilize themselves against the \"common enemy,\" which the Chadian government sees as the Rally for Democracy and Liberty (RDL) militants, Chadian rebels, backed by the Sudanese government, and Sudanese militiamen. Militants have attacked villages and towns in eastern Chad, stealing cattle, murdering citizens, and burning houses. Over 200,000 refugees from the Darfur region of northwestern Sudan currently claim asylum in eastern Chad. Chadian president Idriss D\u00e9by accuses Sudanese President Omar Hasan Ahmad al-Bashir of trying to \"destabilize our country, to drive our people into misery, to create disorder and export the war from Darfur to Chad.\"\nAn attack on the Chadian town of Adre near the Sudanese border led to the deaths of either one hundred rebels, as every news source other than CNN has reported, or three hundred rebels. The Sudanese government was blamed for the attack, which was the second in the region in three days, but Sudanese foreign ministry spokesman Jamal Mohammed Ibrahim denies any Sudanese involvement, \"We are not for any escalation with Chad. We technically deny involvement in Chadian internal affairs.\" This attack was the final straw that led to the declaration of war by Chad and the alleged deployment of the Chadian airforce into Sudanese airspace, which the Chadian government denies.\nAn attack on N'Djamena was defeated on April 13, 2006, in the Battle of N'Djamena. The President on national radio stated that the situation was under control, but residents, diplomats and journalists reportedly heard shots of weapons fire.\nOn November 25, 2006, rebels captured the eastern town of Abeche, capital of the Ouadda\u00ef Region and center for humanitarian aid to the Darfur region in Sudan. On the same day, a separate rebel group Rally of Democratic Forces had captured Biltine. On November 26, 2006, the Chadian government claimed to have recaptured both towns, although rebels still claimed control of Biltine. Government buildings and humanitarian aid offices in Abeche were said to have been looted. The Chadian government denied a warning issued by the French Embassy in N'Djamena that a group of rebels was making its way through the Batha Prefecture in central Chad. Chad insists that both rebel groups are supported by the Sudanese government.\nInternational orphanage scandal.\nNearly 100 children at the center of an international scandal that left them stranded at an orphanage in remote eastern Chad returned home after nearly five months March 14, 2008. The 97 children were taken from their homes in October 2007 by a then-obscure French charity, Zo\u00e9's Ark, which claimed they were orphans from Sudan's war-torn Darfur region.\nRebel attack on Ndjamena.\nOn Friday, February 1, 2008, rebels, an opposition alliance of leaders Mahamat Nouri, a former defense minister, and Timane Erdimi, a nephew of Idriss D\u00e9by who was his chief of staff, attacked the Chadian capital of Ndjamena \u2013 even surrounding the Presidential Palace. But Idris Deby with government troops fought back. French forces flew in ammunition for Chadian government troops but took no active part in the fighting. UN has said that up to 20,000 people left the region, taking refuge in nearby Cameroon and Nigeria. Hundreds of people were killed, mostly civilians. The rebels accuse Deby of corruption and embezzling millions in oil revenue. While many Chadians may share that assessment, the uprising appears to be a power struggle within the elite that has long controlled Chad. The French government believes that the opposition has regrouped east of the capital. D\u00e9by has blamed Sudan for the current unrest in Chad.\nRegional interventionism.\nDuring the D\u00e9by era Chad intervened in conflicts in Mali, Central African Republic, Niger and Nigeria.\nIn 2013, Chad sent 2000 men from its military to help France in Operation Serval during the Mali War. Later in the same year Chad sent 850 troops to Central African Republic to help peacekeeping operation MISCA, those troops withdrew in April 2014 after allegations of human rights violations.\nDuring the Boko Haram insurgency, Chad multiple times sent troops to assist the fight against Boko Haram in Niger and Nigeria.\nIn August 2018, rebel fighters of the Military Command Council for the Salvation of the Republic (CCMSR) attacked government forces in northern Chad. Chad experienced threats from jihadists fleeing the Libyan conflict. Chad had been an ally of the West in the fight against Islamist militants in West Africa.\nIn January 2019, after 47 years, Chad restored diplomatic relations with Israel. It was announced during a visit to N\u2019Djamena by Israeli Prime Minister Benjamin Netanyahu.\nMahamat D\u00e9by era (2021\u2013present).\nIn April 2021 Chad's army announced that President Idriss D\u00e9by had died of his injuries following clashes with rebels in the north of the country. Idriss Deby ruled the country for more than 30 years since 1990. It was also announced that a military council led by D\u00e9by's son, Mahamat Idriss D\u00e9by a 37-year-old four star general, will govern for the next 18 months. On May 23, 2024, Mahamat Idriss D\u00e9by was sworn in as President of Chad. He had won the disputed May 6 election outright, with 61 per cent of the vote."}
{"id": "5330", "revid": "1253657836", "url": "https://en.wikipedia.org/wiki?curid=5330", "title": "Geography of Chad", "text": " \nChad is one of the 47 landlocked countries in the world and is located in North Central Africa, measuring , nearly twice the size of France and slightly more than three times the size of California. Most of its ethnically and linguistically diverse population lives in the south, with densities ranging from 54 persons per square kilometer in the Logone River basin to 0.1 persons in the northern B.E.T. (Borkou-Ennedi-Tibesti) desert region, which itself is larger than France. The capital city of N'Djam\u00e9na, situated at the confluence of the Chari and Logone Rivers, is cosmopolitan in nature, with a current population in excess of 700,000 people.\nChad has four climatic zones. The northernmost Saharan zone averages less than of rainfall annually. The sparse human population is largely nomadic, with some livestock, mostly small ruminants and camels. The central Sahelian zone receives between rainfall and has vegetation ranging from grass/shrub steppe to thorny, open savanna. The southern zone, often referred to as the Sudan zone, receives between , with woodland savanna and deciduous forests for vegetation. Rainfall in the Guinea zone, located in Chad's southwestern tip, ranges between . In Chad forest cover is around 3% of the total land area, equivalent to 4,313,000 hectares (ha) of forest in 2020, down from 6,730,000 hectares (ha) in 1990. In 2020, naturally regenerating forest covered 4,293,000 hectares (ha) and planted forest covered 19,800 hectares (ha). For the year 2015, 100% of the forest area was reported to be under public ownership.\nThe country's topography is generally flat, with the elevation gradually rising as one moves north and east away from Lake Chad. The highest point in Chad is Emi Koussi, a mountain that rises in the northern Tibesti Mountains. The Ennedi Plateau and the Ouadda\u00ef highlands in the east complete the image of a gradually sloping basin, which descends towards Lake Chad. There are also central highlands in the Guera region rising to .\nLake Chad is the second largest lake in west Africa and is one of the most important wetlands on the continent. Home to 120 species of fish and at least that many species of birds, the lake has shrunk dramatically in the last four decades due to increased water usage from an expanding population and low rainfall. Bordered by Chad, Niger, Nigeria, and Cameroon, Lake Chad currently covers only 1350 square kilometers, down from 25,000 square kilometers in 1963. The Chari and Logone Rivers, both of which originate in the Central African Republic and flow northward, provide most of the surface water entering Lake Chad. Chad is also next to Niger.\nGeographical placement.\nLocated in north-central Africa, Chad stretches for about 1,800 kilometers from its northernmost point to its southern boundary. Except in the far northwest and south, where its borders converge, Chad's average width is about 800 kilometers. Its area of 1,284,000 square kilometers is roughly equal to the combined areas of Idaho, Wyoming, Utah, Nevada, and Arizona. Chad's neighbors include Libya to the north, Niger and Nigeria to the west, Sudan to the east, Central African Republic to the south, and Cameroon to the southwest.\nChad exhibits two striking geographical characteristics. First, the country is landlocked. N'Djamena, the capital, is located more than 1,100 kilometers northeast of the Atlantic Ocean; Ab\u00e9ch\u00e9, a major city in the east, lies 2,650 kilometers from the Red Sea; and Faya-Largeau, a much smaller but strategically important center in the north, is in the middle of the Sahara Desert, 1,550 kilometers from the Mediterranean Sea. These vast distances from the sea have had a profound impact on Chad's historical and contemporary development.\nThe second noteworthy characteristic is that the country borders on very different parts of the African continent: North Africa, with its Islamic culture and economic orientation toward the Mediterranean Basin; and West Africa, with its diverse religions and cultures and its history of highly developed states and regional economies.\nChad also borders Northeast Africa, oriented toward the Nile Valley and the Red Sea region - and Central or Equatorial Africa, some of whose people have retained classical African religions while others have adopted Christianity, and whose economies were part of the great Congo River system. Although much of Chad's distinctiveness comes from this diversity of influences, since independence the diversity has also been an obstacle to the creation of a national identity.\nLand.\nAlthough Chadian society is economically, socially, and culturally fragmented, the country's geography is unified by the Lake Chad Basin. Once a huge inland sea (the Pale-Chadian Sea) whose only remnant is shallow Lake Chad, this vast depression extends west into Nigeria and Niger. The larger, northern portion of the basin is bounded within Chad by the Tibesti Mountains in the northwest, the Ennedi Plateau in the northeast, the Ouadda\u00ef Highlands in the east along the border with Sudan, the Gu\u00e9ra Massif in central Chad, and the Mandara Mountains along Chad's southwestern border with Cameroon. The smaller, southern part of the basin falls almost exclusively in Chad. It is delimited in the north by the Gu\u00e9ra Massif, in the south by highlands 250 kilometers south of the border with Central African Republic, and in the southwest by the Mandara Mountains.\nLake Chad, located in the southwestern part of the basin at an altitude of 282 meters, surprisingly does not mark the basin's lowest point; instead, this is found in the Bodele and Djourab regions in the north-central and northeastern parts of the country, respectively. This oddity arises because the great stationary dunes (ergs) of the Kanem region create a dam, preventing lake waters from flowing to the basin's lowest point. At various times in the past, and as late as the 1870s, the Bahr el Ghazal Depression, which extends from the northeastern part of the lake to the Djourab, acted as an overflow canal; since independence, climatic conditions have made overflows impossible.\nNorth and northeast of Lake Chad, the basin extends for more than 800 kilometers, passing through regions characterized by great rolling dunes separated by very deep depressions. Although vegetation holds the dunes in place in the Kanem region, farther north they are bare and have a fluid, rippling character. From its low point in the Djourab, the basin then rises to the plateaus and peaks of the Tibesti Mountains in the north. The summit of this formation\u2014as well as the highest point in the Sahara Desert\u2014is Emi Koussi, a dormant volcano that reaches 3,414 meters above sea level.\nThe basin's northeastern limit is the Ennedi Plateau, whose limestone bed rises in steps etched by erosion. East of the lake, the basin rises gradually to the Ouadda\u00ef Highlands, which mark Chad's eastern border and also divide the Chad and Nile watersheds. These highland areas are part of the East Saharan montane xeric woodlands ecoregion.\nSoutheast of Lake Chad, the regular contours of the terrain are broken by the Gu\u00e9ra Massif, which divides the basin into its northern and southern parts. South of the lake lie the floodplains of the Chari and Logone rivers, much of which are inundated during the rainy season. Farther south, the basin floor slopes upward, forming a series of low sand and clay plateaus, called koros, which eventually climb to 615 meters above sea level. South of the Chadian border, the koros divide the Lake Chad Basin from the Ubangi-Zaire river system.\nWater systems.\nPermanent streams do not exist in northern or central Chad. Following infrequent rains in the Ennedi Plateau and Ouadda\u00ef Highlands, water may flow through depressions called enneris and wadis. Often the result of flash floods, such streams usually dry out within a few days as the remaining puddles seep into the sandy clay soil. The most important of these streams is the Batha, which in the rainy season carries water west from the Ouadda\u00ef Highlands and the Gu\u00e9ra Massif to Lake Fitri.\nChad's major rivers are the Chari and the Logone and their tributaries, which flow from the southeast into Lake Chad. Both river systems rise in the highlands of Central African Republic and Cameroon, regions that receive more than 1,250 millimeters of rainfall annually. Fed by rivers of Central African Republic, as well as by the Bahr Salamat, Bahr Aouk, and Bahr Sara rivers of southeastern Chad, the Chari River is about 1,200 kilometers long. From its origins near the city of Sarh, the middle course of the Chari makes its way through swampy terrain; the lower Chari is joined by the Logone River near N'Djamena. The Chari's volume varies greatly, from 17 cubic meters per second during the dry season to 340 cubic meters per second during the wettest part of the year.\nThe Logone River is formed by tributaries flowing from Cameroon and Central African Republic. Both shorter and smaller in volume than the Chari, it flows northeast for 960 kilometers; its volume ranges from five to eighty-five cubic meters per second. At N'Djamena the Logone empties into the Chari, and the combined rivers flow together for thirty kilometers through a large delta and into Lake Chad. At the end of the rainy season in the fall, the river overflows its banks and creates a huge floodplain in the delta.\nThe seventh largest lake in the world (and the fourth largest in Africa), Lake Chad is located in the sahelian zone, a region just south of the Sahara Desert. The Chari River contributes 95 percent of Lake Chad's water, an average annual volume of 40 billion cubic meters, 95% of which is lost to evaporation. The size of the lake is determined by rains in the southern highlands bordering the basin and by temperatures in the Sahel. Fluctuations in both cause the lake to change dramatically in size, from 9,800 square kilometers in the dry season to 25,500 at the end of the rainy season.\nLake Chad also changes greatly in size from one year to another. In 1870 its maximum area was 28,000 square kilometers. The measurement dropped to 12,700 in 1908. In the 1940s and 1950s, the lake remained small, but it grew again to 26,000 square kilometers in 1963. The droughts of the late 1960s, early 1970s, and mid-1980s caused Lake Chad to shrink once again, however. The only other lakes of importance in Chad are Lake Fitri, in Batha Prefecture, and Lake Iro, in the marshy southeast.\nClimate.\nThe Lake Chad Basin embraces a great range of tropical climates from north to south, although most of these climates tend to be dry. Apart from the far north, most regions are characterized by a cycle of alternating rainy and dry seasons. In any given year, the duration of each season is determined largely by the positions of two great air masses\u2014a maritime mass over the Atlantic Ocean to the southwest and a much drier continental mass.\nDuring the rainy season, winds from the southwest push the moister maritime system north over the African continent where it meets and slips under the continental mass along a front called the \"intertropical convergence zone\". At the height of the rainy season, the front may reach as far as Kanem Prefecture. By the middle of the dry season, the intertropical convergence zone moves south of Chad, taking the rain with it. This weather system contributes to the formation of three major regions of climate and vegetation.\nSaharan region.\nThe Saharan region covers roughly the northern half of the country, including Borkou-Ennedi-Tibesti Prefecture along with the northern parts of Kanem, Batha, and Biltine prefectures. Much of this area receives only traces of rain during the entire year; at Faya-Largeau, for example, annual rainfall averages less than , and there are nearly 3800 hours of sunshine. Scattered small oases and occasional wells provide water for a few date palms or small plots of millet and garden crops.\nIn much of the north, the average daily maximum temperature is about during January, the coolest month of the year, and about during May, the hottest month. On occasion, strong winds from the northeast produce violent sandstorms. In northern Biltine Prefecture, a region called the Mortcha plays a major role in animal husbandry. Dry for eight months of the year, it receives or more of rain, mostly during July and August.\nA carpet of green springs from the desert during this brief wet season, attracting herders from throughout the region who come to pasture their cattle and camels. Because very few wells and springs have water throughout the year, the herders leave with the end of the rains, turning over the land to the antelopes, gazelles, and ostriches that can survive with little groundwater. Northern Chad averages over 3500 hours of sunlight per year, the south somewhat less.\nSahelian region.\nThe semiarid sahelian zone, or Sahel, forms a belt about wide that runs from Lac and Chari-Baguirmi prefectures eastward through Gu\u00e9ra, Ouadda\u00ef, and northern Salamat prefectures to the Sudanese frontier. The climate in this transition zone between the desert and the southern sudanian zone is divided into a rainy season (from June to September) and a dry period (from October to May).\nIn the northern Sahel, thorny shrubs and acacia trees grow wild, while date palms, cereals, and garden crops are raised in scattered oases. Outside these settlements, nomads tend their flocks during the rainy season, moving southward as forage and surface water disappear with the onset of the dry part of the year. The central Sahel is characterized by drought-resistant grasses and small woods. Rainfall is more abundant there than in the Saharan region. For example, N'Djamena records a maximum annual average rainfall of , while Ouadda\u00ef Prefecture receives just a bit less.\nDuring the hot season, in April and May, maximum temperatures frequently rise above . In the southern part of the Sahel, rainfall is sufficient to permit crop production on unirrigated land, and millet and sorghum are grown. Agriculture is also common in the marshlands east of Lake Chad and near swamps or wells. Many farmers in the region combine subsistence agriculture with the raising of cattle, sheep, goats, and poultry.\nSudanian region.\nThe humid \"sudanian\" zone includes the Sahel, the southern prefectures of Mayo-Kebbi, Tandjil\u00e9, Logone Occidental, Logone Oriental, Moyen-Chari, and southern Salamat. Between April and October, the rainy season brings between of precipitation. Temperatures are high throughout the year. Daytime readings in Moundou, the major city in the southwest, range from in the middle of the cool season in January to about in the hot months of March, April, and May.\nThe sudanian region is predominantly East Sudanian savanna, or plains covered with a mixture of tropical or subtropical grasses and woodlands. The growth is lush during the rainy season but turns brown and dormant during the five-month dry season between November and March. Over a large part of the region, however, natural vegetation has yielded to agriculture.\n2010 drought.\nOn 22 June, the temperature reached in Faya, breaking a record set in 1961 at the same location. Similar temperature rises were also reported in Niger, which began to enter a famine situation.\nOn 26 July the heat reached near-record levels over Chad and Niger.\nArea.\nArea:\n&lt;br&gt;\"total:\"\n1.284 million km2\n&lt;br&gt;\"land:\"\n1,259,200\u00a0km2\n&lt;br&gt;\"water:\"\n24,800\u00a0km2\nArea \u2014 comparative:\nBoundaries.\nLand boundaries:\n&lt;br&gt;\"total:\"\n6,406\u00a0km\n&lt;br&gt;\"border countries:\"\nCameroon 1,116\u00a0km, Central African Republic 1,556\u00a0km, Libya 1,050\u00a0km, Niger 1,196\u00a0km, Nigeria 85\u00a0km, Sudan 1,403\u00a0km\nCoastline:\n0\u00a0km (landlocked)\nMaritime claims:\nnone (landlocked)\nElevation extremes:\n&lt;br&gt;\"lowest point:\"\nBod\u00e9l\u00e9 Depression 160 m\n&lt;br&gt;\"highest point:\"\nEmi Koussi 3,415 m\nLand use and resources.\nNatural resources:\npetroleum, uranium, natron, kaolin, fish (Chari River, Logone River), gold, limestone, sand and gravel, salt\nLand use:\n&lt;br&gt;\"arable land:\"\n3.89%\n&lt;br&gt;\"permanent crops:\"\n0.03%\n&lt;br&gt;\"other:\"\n96.08% (2012)\nIrrigated land:\n302.7\u00a0km2 (2003)\nTotal renewable water resources:\n43\u00a0km3 (2011)\nFreshwater withdrawal (domestic/industrial/agricultural):\n&lt;br&gt;\"total:\"\n0.88\u00a0km3/yr (12%/12%/76%)\n&lt;br&gt;\"per capita:\"\n84.81 m3/yr (2005)\nEnvironmental issues.\nNatural hazards:\nhot, dry, dusty, Harmattan winds occur in north; periodic droughts; locust plagues\nEnvironment - current issues:\ninadequate supplies of potable water; improper waste disposal in rural areas contributes to soil and water pollution; desertification\nExtreme points.\nThis is a list of the extreme points of Chad, the points that are farther north, south, east or west than any other location.\n\"*Note: technically Chad does not have an easternmost point, the easternmost section of the border being formed by the 24\u00b0 of longitude\""}
{"id": "5331", "revid": "11334803", "url": "https://en.wikipedia.org/wiki?curid=5331", "title": "Demographics of Chad", "text": "The people of Chad speak more than 100 languages and divide themselves into many ethnic groups. However, language and ethnicity are not the same. Moreover, neither element can be tied to a particular physical type.\nAlthough the possession of a common language shows that its speakers have lived together and have a common history, peoples also change languages. This is particularly so in Chad, where the openness of the terrain, marginal rainfall, frequent drought and famine, and low population densities have encouraged physical and linguistic mobility. Slave raids among non-Muslim peoples, internal slave trade, and exports of captives northward from the ninth to the twentieth centuries also have resulted in language changes.\nAnthropologists view ethnicity as being more than genetics. Like language, ethnicity implies a shared heritage, partly economic, where people of the same ethnic group may share a livelihood, and partly social, taking the form of shared ways of doing things and organizing relations among individuals and groups. Ethnicity also involves a cultural component made up of shared values and a common worldview. Like language, ethnicity is not immutable. Shared ways of doing things change over time and alter a group's perception of its own identity.\nNot only do the social aspects of ethnic identity change but the biological composition (or gene pool) also may change over time. Although most ethnic groups emphasize intermarriage, people are often proscribed from seeking partners among close relatives\u2014a prohibition that promotes biological variation. In all groups, the departure of some individuals or groups and the integration of others also changes the biological component.\nThe Chadian government has avoided official recognition of ethnicity. With the exception of a few surveys conducted shortly after independence, little data were available on this important aspect of Chadian society. Nonetheless, ethnic identity was a significant component of life in Chad.\nThe peoples of Chad carry significant ancestry from Eastern, Central, Western, and Northern Africa.\nChad's languages fall into ten major groups, each of which belongs to either the\nNilo-Saharan, Afro-Asiatic, or Niger\u2013Congo language family. These represent three of the four major language families in Africa; only the Khoisan languages of southern Africa are not represented. The presence of such different languages suggests that the Lake Chad Basin may have been an important point of dispersal in ancient times.\nPopulation.\nAccording to the total population was in , compared to only 2 429 000 in 1950. The proportion of children below the age of 15 in 2010 was 45.4%, 51.7% was between 15 and 65 years of age, while 2.9% was 65 years or the country is projected to have a population of 34 millions peoples in 2050 and 61 millions peoples in 2100\nVital statistics.\nRegistration of vital events is in Chad not complete. The Population Departement of the United Nations prepared the following estimates.\nSource: UN DESA, World Population Prospects, 2022\nDemographic and Health Surveys.\nTotal Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):\nFertility data as of 2014-2015 (DHS Program):\nEthnic groups.\nThe peoples of Chad carry significant ancestry from Eastern, Central, Western, and Northern Africa. \nReligion.\nThe separation of religion from social structure in Chad represents a false dichotomy, for they are perceived as two sides of the same coin. Three religious traditions coexist in Chad: traditional African religions, Islam, and Christianity. None is monolithic. The first tradition includes a variety of ancestor and/or place-oriented religions whose expression is highly specific. Islam, although characterized by an orthodox set of beliefs and observances, also is expressed in diverse ways. Christianity arrived in Chad much more recently with the arrival of Europeans. Its followers are divided into Roman Catholics and Protestants (including several denominations); as with Chadian Islam, Chadian Christianity retains aspects of pre-Christian religious belief.\nThe number of followers of each tradition in Chad is unknown. Estimates made in 1962 suggested that 35 percent of Chadians practiced classical African religions, 55 percent were Muslims, and 10 percent were Christians. In the 1970s and 1980s, this distribution undoubtedly changed. Observers report that Islam has spread among the Hadjarai peoples and other non-Muslim populations of the Saharan and sahelian zones. However, the proportion of Muslims may have fallen, because the birthrate among the followers of traditional religions and Christians in southern Chad is thought to be higher than that among Muslims. In addition, the upheavals since the mid-1970s have resulted in the departure of some missionaries; whether or not Chadian Christians have been numerous enough and sufficiently organized to attract more converts since that time is unknown.\nReferences.\nAttribution:"}
{"id": "5332", "revid": "5229428", "url": "https://en.wikipedia.org/wiki?curid=5332", "title": "Politics of Chad", "text": " \nThe politics of Chad take place in a framework of a presidential republic, whereby the President of Chad is both head of state and head of government. Executive power is exercised by the government. Legislative power is vested in both the government and parliament. Chad is one of the most corrupt countries in the world.\nIn May 2013, security forces in Chad foiled a coup against the President Idriss Deby that had been in preparation for several months. In April 2021, President D\u00e9by was injured by the rebel group Front Pour l'Alternance et La Concorde au Tchad (FACT). He succumbed to his injuries on April 20, 2021. His presidency was taken by his family member Mahamat D\u00e9by in April 2021. This resulted in both the National Assembly and Chadian Government being dissolved and replaced with a Transitional Military Council.\nThe National Transitional Council will oversee the transition to democracy. On 23 May 2024, Mahamat Idriss D\u00e9by was sworn in as President of Chad. He had won the disputed 6 May election outright, with 61 per cent of the vote.\nExecutive branch.\nChad's executive branch is headed by the President and dominates the Chadian political system. Following the military overthrow of Hiss\u00e8ne Habr\u00e9 in December 1990, Idriss D\u00e9by won the presidential elections in 1996 and 2001. The constitutional basis for the government is the 1996 constitution, under which the president was limited to two terms of office until D\u00e9by had that provision repealed in 2005. The president has the power to appoint the Council of State (or cabinet), and exercises considerable influence over appointments of judges, generals, provincial officials and heads of Chad's parastatal firms. In cases of grave and immediate threat, the president, in consultation with the National Assembly President and Council of State, may declare a state of emergency. Most of the key advisors for former president D\u00e9by were members of the Zaghawa clan, although some southern and opposition personalities were represented in his government.\nLegislative branch.\nAccording to the 1996 constitution, the National Assembly deputies are elected by universal suffrage for 4-year terms. The Assembly holds regular sessions twice a year, starting in March and October, and can hold special sessions as necessary and called by the prime minister. Deputies elect a president of the National Assembly every 2 years. Assembly deputies or members of the executive branch may introduce legislation; once passed by the Assembly, the president must take action to either sign or reject the law within 15 days. The National Assembly must approve the prime minister's plan of government and may force the prime minister to resign through a majority vote of no-confidence. However, if the National Assembly rejects the executive branch's program twice in one year, the president may disband the Assembly and call for new legislative elections. In practice, the president exercises considerable influence over the National Assembly through the MPS party structure.\nJudicial branch.\nDespite the constitution's guarantee of judicial independence from the executive branch, the president names most key judicial officials. The Supreme Court is made up of a chief justice, named by the president, and 15 councilors chosen by the president and National Assembly; appointments are for life. The Constitutional Council, with nine judges elected to 9-year terms, has the power to review all legislation, treaties and international agreements prior to their adoption. The constitution recognizes customary and traditional law in locales where it is recognized and to the extent it does not interfere with public order or constitutional guarantees of equality for all citizens.\nInternational organization participation.\nACCT, \nACP, \nAfDB, \nAU, \nBDEAC, \nCEMAC, \nFAO, \nFZ, \nG-77, \nIBRD, \nICAO, \nICCt, \nICFTU, \nICRM, \nIDA, \nIDB, \nIFAD, \nIFC, \nIFRCS, \nILO, \nIMF, \nInterpol, \nIOC, \nITU, \nMIGA, \nNAM, \nOIC, \nONUB, \nOPCW, \nUN, \nUNCTAD, \nUNESCO, \nUNIDO, \nUNOCI, \nUPU, \nWCL, \nWHO, \nWIPO, \nWMO, \nWToO, \nWTrO\n2021 government shakeup.\nOn 20 April 2021, following the death of longtime Chad President Idriss D\u00e9by, the Military of Chad released a statement confirming that both the Government of Chad and the nation's National Assembly had been dissolved and that a Transitional Military Council led by D\u00e9by's son Mahamat would lead the nation for at least 18 months. Among the 40-member transitional government were nine women including Lydie Beassemda, Fatime Goukouni Weddeye and Isabelle Housna Kassire.\nFollowing protests on 14 May 2022, the authorities in Chad detained several members of civil society organizations. The protests were organized in N\u2019Djamena, and other cities across the country by Chadian civil society organizations, united under the coalition Wakit Tamma."}
{"id": "5333", "revid": "30645640", "url": "https://en.wikipedia.org/wiki?curid=5333", "title": "Economy of Chad", "text": "The economy of Chad suffers from the landlocked country's geographic remoteness, drought, lack of infrastructure, and political turmoil. About 85% of the population depends on agriculture, including livestock herding. Of Africa's Francophone countries, Chad benefited least from the 50% devaluation of their currencies in January 1994. Financial aid from the World Bank, the African Development Bank, and other sources is directed mainly at improving agriculture, especially livestock production. Because of a lack of financing, the development of oil fields near Doba, originally due to finish in 2000, was delayed until 2003. It was finally developed and is now operated by ExxonMobil. Regarding gross domestic product, Chad ranks 147th globally with $11.051 billion as of 2018.\nAgriculture.\nChad produced in 2018:\nIn addition to smaller productions of other agricultural products.\nMacro-economic trend.\nThe following table shows the leading economic indicators from 1980 to 2024.\nOther statistics.\nGDP:\npurchasing power parity \u2013 $28.62 billion (2017 est.)\nGDP \u2013 real growth rate:\n-3.1% (2017 est.)\nGDP \u2013 per capita:\n$2,300 (2017 est.)\nGross national saving: \n15.5% of GDP (2017 est.)\nGDP \u2013 composition by sector:\n\"agriculture:\"\n52.3% (2017 est.) \n\"industry:\"\n14.7% (2017 est.) \n\"services:\"\n33.1% (2017 est.)\nPopulation below poverty line::\n46.7% (2011 est.)\nDistribution of family income \u2013 Gini index:\n43.3 (2011 est.)\nInflation rate (consumer prices):\n-0.9% (2017 est.)\nLabor force:\n5.654 million (2017 est.)\nLabor force \u2013 by occupation:\nagriculture 80%, industry and services 20% (2006 est.)\nBudget:\n\"revenues:\"\n1.337 billion (2017 est.) \n\"expenditures:\"\n1.481 billion (2017 est.)\nBudget surplus (+) or deficit (-):\n-1.5% (of GDP) (2017 est.)\nPublic debt: \n52.5% of GDP (2017 est.) \nIndustries:\noil, cotton textiles, brewing, natron (sodium carbonate), soap, cigarettes, construction materials\nIndustrial production growth rate:\n-4% (2017 est.)\nelectrification: total population: 4% (2013)\nelectrification: urban areas: 14% (2013)\nelectrification: rural areas: 1% (2013)\nElectricity \u2013 production:\n224.3 million kWh (2016 est.)\nElectricity \u2013 production by source:\n&lt;br&gt;\"fossil fuel:\"\n~98%\n&lt;br&gt;\"hydro:\"\n0%\n&lt;br&gt;\"nuclear:\"\n0%\n&lt;br&gt;\"other renewable:\"\n~3% (2017)\nElectricity \u2013 consumption:\n208.6 million kWh (2016 est.)\nElectricity \u2013 exports:\n0 kWh (2016 est.)\nElectricity \u2013 imports:\n0 kWh (2016 est.)\nAgriculture \u2013 products:\ncotton, sorghum, millet, peanuts, sesame, corn, rice, potatoes, onions, cassava (manioc, tapioca), cattle, sheep, goats, camels\nExports:\n$2.464 billion (2017 est.)\nExports \u2013 commodities:\noil, livestock, cotton, sesame, gum arabic, shea butter\nExports \u2013 partners:\nUS 38.7%, China 16.6%, Netherlands 15.7%, UAE 12.2%, India 6.3% (2017)\nImports:\n$2.16 billion (2017 est.)\nImports \u2013 commodities:\nmachinery and transportation equipment, industrial goods, foodstuffs, textiles\nImports \u2013 partners:\nChina 19.9%, Cameroon 17.2%, France 17%, US 5.4%, India 4.9%, Senegal 4.5% (2017)\nDebt \u2013 external:\n$1.724 billion (31 December 2017 est.)\nReserves of foreign exchange and gold:\n$22.9 million (31 December 2017 est.)"}
{"id": "5334", "revid": "22049567", "url": "https://en.wikipedia.org/wiki?curid=5334", "title": "Telecommunications in Chad", "text": " \nTelecommunications in Chad include radio, television, fixed and mobile telephones, and the Internet.\nRadio and television.\nRadio stations:\nRadios:\n1.7 million (1997).\nTelevision stations:\nTelevision sets:\n10,000 (1997).\nRadio is the most important medium of mass communication. State-run Radiodiffusion Nationale Tchadienne operates national and regional radio stations. Around a dozen private radio stations are on the air, despite high licensing fees, some run by religious or other non-profit groups. The BBC World Service (FM 90.6) and Radio France Internationale (RFI) broadcast in the capital, N'Djamena. The only television station, Tele Tchad, is state-owned.\nState control of many broadcasting outlets allows few dissenting views. Journalists are harassed and attacked. On rare occasions journalists are warned in writing by the High Council for Communication to produce more \"responsible\" journalism or face fines. Some journalists and publishers practice self-censorship. On 10 October 2012, the High Council on Communications issued a formal warning to La Voix du Paysan, claiming that the station's live broadcast on 30 September incited the public to \"insurrection against the government.\" The station had broadcast a sermon by a bishop who criticized the government for allegedly failing to use oil wealth to benefit the region.\nTelephones.\nCalling code: +235\nInternational call prefix: 00\nMain lines:\nMobile cellular:\nTelephone system: inadequate system of radiotelephone communication stations with high costs and low telephone density; fixed-line connections for less than 1 per 100 persons coupled with mobile-cellular subscribership base of only about 35 per 100 persons (2011).\nSatellite earth stations: 1 Intelsat (Atlantic Ocean) (2011).\nInternet.\nTop-level domain: .td\nInternet users:\nFixed broadband: 18,000 subscriptions, 132nd in the world; 0.2% of the population, 161st in the world (2012).\nWireless broadband: Unknown (2012).\nInternet hosts:\nIPv4: 4,096 addresses allocated, less than 0.05% of the world total, 0.4 addresses per 1000 people (2012).\nInternet censorship and surveillance.\nThere are no government restrictions on access to the Internet or credible reports that the government monitors e-mail or Internet chat rooms. However, in September 2023 the government has banned Starlink on the country's territory. Illegal use, sale or purchase is punishable by imprisonment or penalty from 100 000 000 to 200 000 000 francs CFA (around 152 000 - 304 000 \u20ac). \nThe constitution provides for freedom of opinion, expression, and press, but the government does not always respect these rights. Private individuals are generally free to criticize the government without reprisal, but reporters and publishers risk harassment from authorities when publishing critical articles. The 2010 media law abolished prison sentences for defamation and insult, but prohibits \"inciting racial, ethnic, or religious hatred,\" which is punishable by one to two years in prison and a fine of one to three million CFA francs ($2,000 to $6,000)."}
{"id": "5335", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=5335", "title": "Transport in Chad", "text": " \nTransport infrastructure within Chad is generally poor, especially in the north and east of the country. River transport is limited to the south-west corner. As of 2011 Chad had no railways though two lines are planned - from the capital to the Sudanese and Cameroonian borders during the wet season, especially in the southern half of the country. In the north, roads are merely tracks across the desert and land mines continue to present a danger. Draft animals (horses, donkeys and camels) remain important in much of the country.\nFuel supplies can be erratic, even in the south-west of the country, and are expensive. Elsewhere they are practically non-existent.\nHighways.\nThree trans-African automobile routes pass through Chad:\nAs at 2018 Chad had a total of 44,000\u00a0km of roads of which approximately 260\u00a0km are paved. Some, but not all of the roads in the capital N'Djamena are paved. Outside of N'Djamena there is one paved road which runs from Massakory in the north, through N'Djamena and then south, through the cities of Gu\u00e9lengdeng, Bongor, K\u00e9lo and Moundou, with a short spur leading in the direction of Kousseri, Cameroon, near N'Djamena. Expansion of the road towards Cameroon through Pala and L\u00e9r\u00e9 is reportedly in the preparatory stages.\nAirports.\n Chad had an estimated 58 airports, only 9 of which had paved runways. In 2015, scheduled airlines in Chad carried approximately 28,332 passengers.\nAirports with paved runways.\nStatistics on airports with paved runways as of 2017:\nList of airports with paved runways:\nAirports - with unpaved runways.\nStatistics on airports with unpaved runways as of 2013:\nAirline.\nSAGA Airline of Chad - see http://www.airsaga.com\nWaterways.\nAs at 2012, Chari and Logone Rivers were navigable only in wet season (2002). Both flow northwards, from the south of Chad, into Lake Chad.\nPipelines.\nSince 2003, a 1,070\u00a0km pipeline has been used to export crude oil from the oil fields around Doba to offshore oil-loading facilities on Cameroon's Atlantic coast at Kribi.\nThe CIA World Factbook however cites only 582\u00a0km of pipeline in Chad itself as at 2013.\nRailways.\nAs of 2011 Chad had no railways. Two lines were planned to Sudan and Cameroon from the capital, with construction expected to start in 2012. \nNo operative lines were listed as of 2019.\nIn 2021, an ADB study was funded for that rail link from Cameroon to Chad.\nSeaports and harbors.\nNone (landlocked).\nChad's main routes to the sea are:\nIn colonial times, the main access was by road to Bangui, in the Central African Republic, then by river boat to Brazzaville, and onwards by rail from Brazzaville to Pointe Noire, on Congo's Atlantic coast. This route is now little used.\nThere is also a route across Sudan, to the Red Sea, but very little trade goes this way.\nLinks with Niger, north of Lake Chad, are practically nonexistent; it is easier to reach Niger via Cameroon and Nigeria. \nMinistry of Transport.\nThe Ministry is represented at the regional level by the Regional Delegations, which have jurisdiction over a part of the National Territory as defined by Decree No. 003 / PCE / CTPT / 91. Their organization and responsibilities are defined by Order No. 006 / MTPT / SE / DG / 92.\nThe Regional Delegations are:\nEach Regional Delegation is organized into regional services, namely: the Regional Roads Service, the Regional Transport Service, the Civilian Buildings Regional Service and, as needed, other regional services may be established in one or more Delegations ."}
{"id": "5336", "revid": "1812441", "url": "https://en.wikipedia.org/wiki?curid=5336", "title": "Chad National Army", "text": "The Chad National Army (; , ANT) consists of the five Defence and Security Forces listed in Article 185 of the Chadian Constitution that came into effect on 4 May 2018. These are the National Army ((including Ground Forces, and Air Force), the National Gendarmerie, the National Police, the National and Nomadic Guard (GNNT) and the Judicial Police.\nArticle 188 of the Constitution specifies that National Defence is the responsibility of the Army, Gendarmerie and GNNT, whilst the maintenance of public order and security is the responsibility of the Police, Gendarmerie and GNNT.\nHistory.\nFrom independence through the period of the presidency of F\u00e9lix Malloum (1975\u201379), the official national army was known as the Chadian Armed Forces (Forces Arm\u00e9es Tchadiennes\u2014FAT). Composed mainly of soldiers from southern Chad, FAT had its roots in the army recruited by France and had military traditions dating back to World War I. FAT lost its status as the legal state army when Malloum's civil and military administration disintegrated in 1979. Although it remained a distinct military body for several years, FAT was eventually reduced to the status of a regional army representing the south.\nAfter Habr\u00e9 consolidated his authority and assumed the presidency in 1982, his victorious army, the Armed Forces of the North (Forces Arm\u00e9es du Nord\u2014FAN), became the nucleus of a new national army. The force was officially constituted in January 1983, when the various pro-Habr\u00e9 contingents were merged and renamed the Chadian National Armed Forces (Forces Arm\u00e9es Nationales Tchadiennes\u2014FANT).\nThe Military of Chad was dominated by members of Toubou, Zaghawa, Kanembou, Hadjerai, and Massa ethnic groups during the presidency of Hiss\u00e8ne Habr\u00e9. Later Chadian president Idriss D\u00e9by revolted and fled to the Sudan, taking with him many Zaghawa and Hadjerai soldiers in 1989.\nChad's armed forces numbered about 36,000 at the end of the Habr\u00e9 regime, but swelled to an estimated 50,000 in the early days of D\u00e9by's rule. With French support, a reorganization of the armed forces was initiated early in 1991 with the goal of reducing its numbers and making its ethnic composition reflective of the country as a whole. Neither of these goals was achieved, and the military is still dominated by the Zaghawa.\nIn 2004, the government discovered that many of the soldiers it was paying did not exist and that there were only about 19,000 soldiers in the army, as opposed to the 24,000 that had been previously believed. Government crackdowns against the practice are thought to have been a factor in a failed military mutiny in May 2004.\nRenewed conflict, in which the Chadian military is involved, came in the form of a civil war against Sudanese-backed rebels. Chad successfully managed to repel many rebel movements, albeit with some losses (see Battle of N'Djamena (2008)). The army used its artillery systems and tanks, but well-equipped insurgents probably managed to destroy over 20 of Chad's 60 T-55 tanks, and probably shot down a Mi-24 Hind gunship, which bombed enemy positions near the border with Sudan. In November 2006 Libya supplied Chad with four Aermacchi SF.260W light attack planes. They were used to strike enemy positions by the Chadian Air Force, but one was shot down by rebels. During the 2008 battle of N'Djamena, gunships and tanks were put to good use, pushing armed militia forces back from the Presidential palace. The battle impacted the highest levels of the army leadership, as Daoud Soumain, its Chief of Staff, was killed.\nOn March 23, 2020, a Chadian army base was ambushed by fighters of the jihadist insurgent group Boko Haram. The army lost 92 servicemen in one day. In response, President D\u00e9by launched an operation dubbed \"Wrath of Boma\". According to Canadian counter terrorism St-Pierre, numerous external operations and rising insecurity in the neighboring countries had recently overstretched the capacities of the Chadian armed forces.\nAfter the death of President Idriss D\u00e9by on 19 April 2021 in fighting with FACT rebels, his son General Mahamat Idriss D\u00e9by was named interim (and later, permanent) president and head of the armed forces.\nBudget.\nThe CIA World Factbook estimates the military budget of Chad to be 4.2% of GDP as of 2006.. Given the then GDP ($7.095 bln) of the country, military spending was estimated to be about $300 million. This estimate however dropped after the end of the Civil war in Chad (2005\u20132010) to 2.0% as estimated by the World Bank for the year 2011. There aren't any more recent estimates available.\nExternal deployments.\nChad participated in a peace mission under the authority of African Union in the neighboring Central African Republic to try to pacify the recent conflict, but has chosen to withdraw after its soldiers were accused of shooting into a marketplace, unprovoked, according to BBC."}
{"id": "5337", "revid": "42975076", "url": "https://en.wikipedia.org/wiki?curid=5337", "title": "Foreign relations of Chad", "text": " \nThe foreign relations of Chad are significantly influenced by the desire for oil revenue and investment in Chadian oil industry and support for former Chadian President Idriss D\u00e9by. Chad is officially non-aligned but maintains close relations with France, its former colonial power. Relations with neighbouring countries Libya and Sudan vary periodically. Lately, the Idris D\u00e9by regime waged an intermittent proxy war with Sudan. Aside from those two countries, Chad generally enjoys good relations with its neighbouring states.\nDiplomatic relations.\nList of countries which Chad maintains diplomatic relations with:\nBilateral relations.\nAfrica.\nAlthough relations with Libya improved during the presidency of Idriss D\u00e9by, strains persist. Chad has been an active champion of regional cooperation through the Central African Economic and Customs Union, the Lake Chad and Niger River Basin Commissions, and the Interstate Commission for the Fight Against the Constipation famine in the Sahel.\nDelimitation of international boundaries in the vicinity of Lake Chad, the lack of which led to border incidents in the past, has been completed and awaits ratification by Cameroon, Chad, Niger, and Nigeria.\nAsia.\nDespite centuries-old cultural ties to the Arab World, the Chadian Government maintained few significant ties to Arab states in North Africa or West Asia in the 1980s. In September 1972, Chad had broken off relations with the State of Israel under Chadian President Fran\u00e7ois Tombalbaye. President Habr\u00e9 hoped to pursue closer relations with Arab states as a potential opportunity to break out of Chad's post-imperial dependence on France, and to assert Chad's unwillingness to serve as an arena for superpower rivalries. In addition, as a northern Muslim, Habr\u00e9 represented a constituency that favored Afro-Arab solidarity, and hoped Islam would provide a basis for national unity in the long term. For these reasons, he was expected to seize opportunities during the 1990s to pursue closer ties with the Arab World. In 1988, Chad recognized the State of Palestine, which maintains a mission in N'Djamena. In November 2018, President Deby visited Israel and announced his intention to restore diplomatic relations. Chad and Israel re-established diplomatic relations in January 2019. In February 2023, Chad opened an embassy in Israel.\nDuring the 1980s, Arab opinion on the Chadian\u2013Libyan conflict over the Aouzou Strip was divided. Several Arab states supported Libyan territorial claims to the Strip, among the most outspoken of which was Algeria, which provided training for anti-Habr\u00e9 forces, although most recruits for its training programs were from Nigeria or Cameroon, recruited and flown to Algeria by Libya. The Progressive Socialist Party of Lebanon also sent troops to support Muammar Gaddafi's efforts against Chad in 1987. In contrast, numerous other Arab states opposed the Libyan actions, and expressed their desire to see the dispute over the Aouzou Strip settled peacefully. By the end of 1987, Algiers and N'Djamena were negotiating to improve relations and Algeria helped mediate the end of the Aouzou Strip conflict.\nEurope.\nChad is officially non-aligned but maintains close relations with France, its former colonial power, which has about 1,200 troops stationed in the capital N'Djamena. It receives economic aid from countries of the European Community, the United States, and various international organizations. Libya supplies aid and has an ambassador resident in N'Djamena. Traditionally strong ties with the Western community have weakened over the past two years due to a dispute between the Government of Chad and the World Bank over how the profits from Chad's petroleum reserves are allocated. Although oil output to the West has resumed and the dispute has officially been resolved, resentment towards what the D\u00e9by administration considered \"foreign meddling\" lingers.\nMembership of international organizations.\nChad belongs to the following international organizations:"}
{"id": "5341", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=5341", "title": "CountriesX", "text": ""}
{"id": "5342", "revid": "41877120", "url": "https://en.wikipedia.org/wiki?curid=5342", "title": "Commentary", "text": "Commentary or commentaries may refer to:"}
{"id": "5343", "revid": "290432", "url": "https://en.wikipedia.org/wiki?curid=5343", "title": "Canadian Constitution Act", "text": ""}
{"id": "5345", "revid": "788170711", "url": "https://en.wikipedia.org/wiki?curid=5345", "title": "Colloids", "text": ""}
