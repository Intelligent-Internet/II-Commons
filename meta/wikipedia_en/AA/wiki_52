{"id": "5945", "revid": "47406457", "url": "https://en.wikipedia.org/wiki?curid=5945", "title": "Chicago White Sox", "text": "The Chicago White Sox are an American professional baseball team based in Chicago. The White Sox compete in Major League Baseball (MLB) as a member club of the American League (AL) Central Division. The club plays its home games at Rate Field, which is located on Chicago's South Side. They are one of two MLB teams based in Chicago, alongside the National League (NL)\u2019s Chicago Cubs.\nThe White Sox originated in the Western League, founded as the Sioux City Cornhuskers in , moving to Saint Paul, Minnesota, as the St. Paul Saints, and ultimately relocating to Chicago in . The Chicago White Stockings were one of the American League's eight charter franchises when the AL asserted major league status in . The team, which shortened its name to the White Sox in , originally played their home games at South Side Park before moving to Comiskey Park in , where they played until \n. They moved into their current home, which was originally also known as Comiskey Park like its predecessor and later carried sponsorship from U.S. Cellular, for the 1991 season.\nThe White Sox won their first World Series, the 1906 World Series against the Cubs, with a defense-oriented team dubbed \"the Hitless Wonders\", and later won the 1917 World Series against the New York Giants. Their next appearance, the 1919 World Series, was marred by the Black Sox Scandal in which eight members of the White Sox were found to have conspired with gamblers to fix games and lose the World Series to the Cincinnati Reds. In response, the new Commissioner of Baseball, Kenesaw Mountain Landis, banned the players from the league for life. The White Sox have only made two World Series appearances since the scandal. The first came in , where they lost to the Los Angeles Dodgers, before they finally won their third championship in against the Houston Astros. The 88 seasons it took the White Sox to win the World Series stands as the longest MLB championship drought in the American League, and the second longest in both leagues, to the Cubs' 108 seasons.\nFrom 1901 to 2024, the White Sox have an overall win-loss record of ().\nHistory.\nThe White Sox originated as the Sioux City Cornhuskers of the Western League, a minor league under the parameters of the National Agreement with the National League. In 1894, Charles Comiskey bought the Cornhuskers and moved them to St. Paul, Minnesota, where they became the St. Paul Saints. In 1900, with the approval of Western League president Ban Johnson, Charles Comiskey moved the Saints into his hometown neighborhood of Armour Square, where they became the Chicago White Stockings, the former name of Chicago's National League team, the Orphans (now the Chicago Cubs).\nIn 1901, the Western League broke the National Agreement and became the new major league American League. The first season in the AL ended with a White Stockings championship. However, that would be the end of the season, as the World Series did not begin until 1903. The franchise, now known as the Chicago White Sox, made its first World Series appearance in 1906, beating the crosstown Cubs in six games.\nThe White Sox won a third pennant and a second World Series in 1917, beating the New York Giants in six games with help from stars Eddie Cicotte and \"Shoeless\" Joe Jackson. The Sox were heavily favored in the 1919 World Series, but lost to the Cincinnati Reds in eight games. Huge bets on the Reds fueled speculation that the series had been fixed. A criminal investigation went on in the 1920 season, and although all players were acquitted, commissioner Kenesaw Mountain Landis banned eight of them for life, in what was known as the Black Sox Scandal. This set the franchise back, as they did not win another pennant for 40 years.\nThe White Sox did not finish in the upper half of the American League again until after founder Charles Comiskey died and passed ownership of the club to his son, J. Louis Comiskey. They finished in the upper half most years between 1936 and 1946, under the leadership of manager Jimmy Dykes, with star shortstop Luke Appling (known as \"Ol' Aches and Pains\") and pitcher Ted Lyons, who both had their numbers 4 and 16 retired.\nAfter J. Louis Comiskey died in 1939, ownership of the club was passed down to his widow, Grace Comiskey. The club was later passed down to Grace's children Dorothy and Chuck in 1956, with Dorothy selling a majority share to a group led by Bill Veeck after the 1958 season. Veeck was notorious for his promotional stunts, attracting fans to Comiskey Park with the new \"exploding scoreboard\" and outfield shower. In 1961, Arthur Allyn, Jr. briefly owned the club before selling to his brother John Allyn.\nFrom 1951 to 1967, the White Sox had their longest period of sustained success, scoring a winning record for 17 straight seasons. Known as the \"Go-Go White Sox\" for their tendency to focus on speed and getting on base versus power hitting, they featured stars such as Minnie Mi\u00f1oso, Nellie Fox, Luis Aparicio, Billy Pierce, and Sherm Lollar. From 1957 to 1965, the Sox were managed by Al L\u00f3pez. The Sox finished in the upper half of the American League in eight of his nine seasons, including six years in the top two of the league. In 1959, the White Sox ended the New York Yankees' dominance over the American League, and won their first pennant since the ill-fated 1919 campaign. Despite winning game one of the 1959 World Series 11\u20130, they fell to the Los Angeles Dodgers in six games.\nDuring the late 1960s and 1970s, the White Sox struggled to win games and attract fans. The team played a total of 20 home games at Milwaukee County Stadium in the 1968 and 1969 seasons. Allyn and Bud Selig agreed to a handshake deal that would give Selig control of the club and move them to Milwaukee, but it was blocked by the American League. Selig instead bought the Seattle Pilots and moved them to Milwaukee, where they would become the Milwaukee Brewers, putting enormous pressure on the American League to place a team in Seattle. A plan was in place for the Sox to move to Seattle and for Charlie Finley to move his Oakland A's to Chicago. However, the city had a renewed interest in the Sox after the 1972 season, and the American League instead added the expansion Seattle Mariners. The 1972 White Sox had the lone successful season of this era, as Dick Allen wound up winning the American League MVP award. Bill Veeck returned as owner of the Sox in 1975, and despite not having much money, they managed to win 90 games in 1977, with a team known as the \"South Side Hitmen\".\nHowever, the team's fortunes plummeted afterwards, plagued by 90-loss teams and scarred by the notorious 1979 Disco Demolition Night promotion. Veeck was forced to sell the team, rejecting offers from ownership groups intent on moving the club to Denver and eventually agreeing to sell it to Ed DeBartolo, the only prospective owner who promised to keep the White Sox in Chicago. However, DeBartolo was rejected by the owners, and the club was then sold to a group headed by Jerry Reinsdorf and Eddie Einhorn. The Reinsdorf era started off well, with the team winning their first division title in 1983, led by manager Tony La Russa and stars Carlton Fisk, Tom Paciorek, Ron Kittle, Harold Baines, and LaMarr Hoyt. During the 1986 season, La Russa was fired by announcer-turned-general manager Ken Harrelson. La Russa went on to manage in six World Series (winning three) with the Oakland A's and St. Louis Cardinals, ending up in the Hall of Fame as the second-winningest manager of all time.\nThe White Sox struggled for the rest of the 1980s, as Chicago fought to keep them in town. Reinsdorf wanted to replace the aging Comiskey Park, and sought public funds to do so. When talks stalled, a strong offer was made to move the team to St. Petersburg, Florida. Funding for a new ballpark was approved in an 11th-hour deal by the Illinois State Legislature on June 30, 1988, with the stipulation that it had to be built on the corner of 35th and Shields, across the street from the old ballpark, as opposed to the suburban ballpark the owners had designed. Architects offered to redesign the ballpark to a more \"retro\" feel that would fit in the city blocks around Comiskey Park; however, the ownership group was set on a 1991 open date, so they kept the old design. The new ballpark opened in 1991 under the name new Comiskey Park. The park, renamed in 2003 as U.S. Cellular Field and in 2016 as Guaranteed Rate Field, underwent many renovations in the early 2000s to give it a more retro feel. In December 2024, it was renamed Rate Field when Guaranteed Rate rebranded as Rate.\nThe White Sox were fairly successful in the 1990s and early 2000s, with 12 winning seasons from 1990 to 2005. First baseman Frank Thomas became the face of the franchise, ending his career as the White Sox's all-time leader in runs, doubles, home runs, total bases, and walks. Other major players included Robin Ventura, Ozzie Guill\u00e9n, Jack McDowell, and Bobby Thigpen. The Sox won the West division in 1993, and were in first place in 1994, when the season was canceled due to the 1994 MLB Strike.\nIn 2004, Ozzie Guill\u00e9n was hired as manager of his former team. After finishing second in 2004, the Sox won 99 games and the Central Division title in 2005, behind the work of stars Paul Konerko, Mark Buehrle, A. J. Pierzynski, Joe Crede, and Orlando Hern\u00e1ndez. They started the playoffs by sweeping the defending champion Boston Red Sox in the ALDS, and beat the Angels in five games to win their first pennant in 46 years, due to four complete games by the White Sox rotation. The White Sox went on to sweep the Houston Astros in the 2005 World Series, giving them their first World Championship in 88 years.\nGuill\u00e9n had marginal success during the rest of his tenure, with the Sox winning the Central Division title in 2008 after a one-game playoff with the Minnesota Twins. Guill\u00e9n left the White Sox after the 2011 season and was replaced by former teammate Robin Ventura. The White Sox finished the 2015 season, their 115th in Chicago, with a 76\u201386 record, a three-game improvement over 2014. The White Sox recorded their 9,000th win in franchise history by the score of 3\u20132 against the Detroit Tigers on September 21, 2015. Ventura returned in 2016, with a young core featuring Jos\u00e9 Abreu, Adam Eaton, Jos\u00e9 Quintana, and Chris Sale. Ventura resigned after the 2016 season, in which the White Sox finished 78\u201384. Rick Renteria, the 2016 White Sox bench coach, was promoted to the role of manager.\nPrior to the start of the 2017 season, the White Sox traded Sale to the Boston Red Sox and Eaton to the Washington Nationals for prospects including Yo\u00e1n Moncada, Lucas Giolito and Michael Kopech, signaling the beginning of a rebuilding period. During the 2017 season, the White Sox continued their rebuild when they made a blockbuster trade with their crosstown rival, the Chicago Cubs, in a swap that featured the Sox sending pitcher Jos\u00e9 Quintana to the Cubs in exchange for four prospects headlined by outfielder Eloy Jim\u00e9nez and pitcher Dylan Cease. This was the first trade between the White Sox and Cubs since the 2006 season.\nDuring the 2018 season, relief pitcher Danny Farquhar suffered a brain hemorrhage while he was in the dugout between innings. Farquhar remained out of action for the rest of the season and just recently got medically cleared to return to baseball, despite some doctors doubting that he would make a full recovery. Also occurring during the 2018 season, the White Sox announced that the club would be the first Major League Baseball team to entirely discontinue use of plastic straws, in ordinance with the \"Shedd the Straw\" campaign by Shedd Aquarium. The White Sox broke an MLB record during their 100-loss campaign of 2018, but broke the single-season strikeout record in only a year after the Milwaukee Brewers broke the record in the 2017 season. On December 3, 2018, head trainer Herm Schneider retired after 40 seasons with the team; his new role will be as an advisor on medical issues pertaining to free agency, the amateur draft and player acquisition. Schneider will also continue to be a resource for the White Sox training department, including both the major and minor league levels.\nOn August 25, 2020, Lucas Giolito recorded the 19th no-hitter in White Sox history, and the first since Philip Humber's Perfect Game in 2012. Giolito struck out 13 and threw 74 of 101 pitches for strikes. He only allowed one baserunner, which was a walk to Erik Gonz\u00e1lez in the fourth inning. In 2020, the White Sox clinched a playoff berth for the first time since 2008, with a record 35\u201325 in the pandemic-shortened season, but lost to the Oakland Athletics in three games during the Wild Card Series. The White Sox also made MLB history by being the first team to go undefeated against left-handed pitching, with a 14\u20130 record. At the end of the season, Renteria and longtime pitching coach Don Cooper were both fired. Jose Abreu became the 4th different White Sox player to win the AL MVP joining Dick Allen, Nellie Fox, and Frank Thomas. During the 2021 offseason, the White Sox brought back Tony La Russa as their manager for 2021. At the age of 76 when hired, La Russa became the oldest active manager in MLB.\nOn April 14, 2021, pitching against the Cleveland Indians, Carlos Rodon recorded the team's 20th no-hitter. Rodon retired the first 25 batters he faced and was saved by an incredible play at first base by first baseman Jose Abreu to get the first out in the 9th before hitting Roberto P\u00e9rez which was the only baserunner Rodon allowed. Rodon struck out seven and threw 75 of 114 pitches for strikes. On June 6, 2021, the White Sox beat the Detroit Tigers 3\u20130. This also had Tony La Russa winning his 2,764th game as manager passing John McGraw for 2nd on the all-time managerial wins list. On August 12, 2021, the White Sox faced New York Yankees in the first ever Field of Dreams game in Dyersville, Iowa. The White Sox won the game 9\u20138 on a walk-off two-run Home Run by Tim Anderson. The homer was the 15th walk-off home run against the Yankees in White Sox history; the first being Shoeless Joe Jackson on July 20, 1919, whose character featured in the movie Field of Dreams. On September 23, 2021, the White Sox clinched the American League Central Division for the first time since 2008 against the Cleveland Indians. \nIn 2024, the White Sox tied a 14-game losing streak, then proceeded to have a 21-game losing streak from July 10 to August 5. They became the 7th team all time, and the first since the 1988 Baltimore Orioles to lose 20 consecutive games. On September 1, the White Sox set a new franchise record for losses at 107 following a 2\u20130 loss to the New York Mets. They are also the first team since the 1965 Mets to have 3 separate 10 or more game losing streaks in one season. On September 27, the White Sox lost their 121st game of the season, surpassing the 1962 Mets for the most losses in modern MLB history.\nBallparks.\nIn the late 1980s, the franchise threatened to relocate to Tampa Bay (as did the San Francisco Giants), but frantic lobbying on the part of the Illinois governor James R. Thompson and state legislature resulted in approval (by one vote) of public funding for a new stadium. Designed primarily as a baseball stadium (as opposed to a \"multipurpose\" stadium), the new Comiskey Park (redubbed U.S. Cellular Field, often nicknamed \"The Cell\", in 2003 and Guaranteed Rate Field in 2016 (later renamed to Rate Field following a rebrand in 2024), after mortgage company Guaranteed Rate) was built in a 1960s style, similar to Dodger Stadium and Kauffman Stadium. There were ideas for other stadium designs submitted to bring a more neighborhood feel, but ultimately they were not selected. The park opened in to positive reaction, with many praising its wide-open concourses, excellent sight lines, and natural grass (unlike other stadiums of the era, such as Rogers Centre in Toronto). The park's inaugural season drew 2,934,154 fans\u00a0\u2014 at the time, an all-time attendance record for any Chicago baseball team.\nIn recent years, money accrued from the sale of naming rights to the field has been allocated for renovations to make the park more aesthetically appealing and fan-friendly. Notable renovations of early phases included reorientation of the bullpens parallel to the field of play (thus decreasing slightly the formerly symmetrical dimensions of the outfield); filling seats in up to and shortening the outfield wall; ballooning foul-line seat sections out toward the field of play; creating a new multitiered batter's eye, allowing fans to see out through one-way screens from the center-field vantage point, and complete with concession stand and bar-style seating on its \"fan deck\"; and renovating all concourse areas with brick, historic murals, and new concession stand ornaments to establish a more friendly feel. The stadium's steel and concrete were repainted dark gray and black. In 2016, the scoreboard jumbotron was replaced with a new Mitsubishi Diamondvision HDTV screen.\nThe top quarter of the upper deck was removed in , and a black wrought-metal roof was placed over it, covering all but the first eight rows of seats. This decreased seating capacity from 47,098 to 40,615; 2005 also had the introduction of the Scout Seats, redesignating (and reupholstering) 200 lower-deck seats behind home plate as an exclusive area, with seat-side waitstaff and a complete restaurant located underneath the concourse. The most significant structural addition besides the new roof was 2005's FUNdamentals Deck, a multitiered structure on the left-field concourse containing batting cages, a small Tee Ball field, speed pitch, and several other children's activities intended to entertain and educate young fans with the help of coaching staff from the Chicago Bulls/Sox Training Academy. This structure was used during the 2005 American League playoffs by ESPN and the Fox Broadcasting Company as a broadcasting platform.\nDesigned as a seven-phase plan, the renovations were completed before the 2007 season with the seventh and final phase. The most visible renovation in this final phase was replacing the original blue seats with green seats. The upper deck already had new green seats put in before the beginning of the 2006 season. Beginning with the 2007 season, a new luxury-seating section was added in the former press box. This section has amenities similar to those of the Scout Seats section. After the 2007 season, the ballpark continued renovation projects despite the phases being complete. In July 2019, the White Sox extended the netting to the foul pole.\nPrevious ballparks.\nThe St. Paul Saints first played their games at Lexington Park. When they moved to Chicago's Armour Square neighborhood, they began play at the South Side Park. Previously a cricket ground, the park was located on the north side of 39th Street (now called Pershing Road) between South Wentworth and South Princeton Avenues. Its massive dimensions yielded few home runs, which was to the advantage of the White Sox's Hitless Wonders teams of the early 20th century.\nAfter the 1909 season, the Sox moved five blocks to the north to play in the new Comiskey Park, while the 39th Street grounds became the home of the Chicago American Giants of the Negro leagues. Billed as the Baseball Palace of the World, it originally held 28,000 seats and eventually grew to hold over 50,000. It became known for its many odd features, such as the outdoor shower and the exploding scoreboard. When it closed after the 1990 season, it was the oldest ballpark still in Major League Baseball.\nSpring-training ballparks.\nThe White Sox have held spring training in:\nOn November 19, 2007, the cities of Glendale and Phoenix, Arizona, broke ground on a new Cactus League spring-training facility. Camelback Ranch, the $76 million, two-team facility, is the home of both the White Sox and the Los Angeles Dodgers for their spring training, featuring state-of-the-art baseball facilities and an over 10,000-seat stadium. The facility is also home to amenities such as 118,000 sq ft. of clubhouse space, 13 full fields, citrus groves, and a large lake and river system stocked with fish running throughout the complex.\nLogos and uniforms.\nOver the years, the White Sox have become noted for many of their uniform innovations and changes. In 1960, they became the first team in the major sports to put players' last names on jerseys for identification purposes.\nIn 1912, the White Sox debuted a large \"S\" in a Roman-style font, with a small \"O\" inside the top loop of the \"S\" and a small \"X\" inside the bottom loop. This is the logo associated with the 1917 World Series championship team and the 1919 Black Sox. With a couple of brief interruptions, the dark-blue logo with the large \"S\" lasted through 1938 (but continued in a modified block style into the 1940s). Through the 1940s, the White Sox team colors were primarily navy blue trimmed with red.\nThe White Sox logo in the 1950s and 1960s (actually beginning in the season) was the word \"SOX\" in Gothic script, diagonally arranged, with the \"S\" larger than the other two letters. From 1949 through 1963, the primary color was black (trimmed with red after 1951). This is the logo associated with the Go-Go Sox era.\nIn 1964, the primary color went back to navy blue, and the road uniforms changed from gray to pale blue. In 1971, the team's primary color changed from royal blue to red, with the color of their pinstripes and caps changing to red. The 1971\u20131975 uniform included red socks.\nIn 1976, the team's uniforms changed again. The team's primary color changed back from red to navy. The team based their uniforms on a style worn in the early days of the franchise, with white jerseys worn at home, and blue on the road. The team brought back white socks for the last time in team history. The socks featured a different stripe pattern every year. The team also had the option to wear blue or white pants with either jersey. Additionally, the team's \"SOX\" logo was changed to a modern-looking \"SOX\" in a bold font, with \"CHICAGO\" written across the jersey. Finally, the team's logo featured a silhouette of a batter over the words \"SOX\".\nThe new uniforms also featured collars and were designed to be worn untucked\u00a0\u2014 both unprecedented. Yet by far, the most unusual wrinkle was the option to wear shorts, which the White Sox did for the first game of a doubleheader against the Kansas City Royals in 1976. The Hollywood Stars of the Pacific Coast League had previously tried the same concept, but it was also poorly received. Apart from aesthetic issues, as a practical matter, shorts are not conducive to sliding, due to the likelihood of significant abrasions.\nUpon taking over the team in 1980, new owners Eddie Einhorn and Jerry Reinsdorf announced a contest where fans were invited to create new uniforms for the White Sox. The winning entries, submitted by a fan, had the word \"SOX\" written across the front of the jersey in the same font as the cap, inside of a large blue stripe trimmed with red. The red and blue stripes were also on the sleeves, and the road jerseys were gray to the home whites. In those jerseys, the White Sox won 99 games and the AL West championship in 1983, the best record in the majors.\nAfter five years, those uniforms were retired and replaced with a more basic uniform that had \"White Sox\" written across the front in script, with \"Chicago\" on the front of the road jersey. The cap logo was also changed to a cursive \"C\", although the batter logo was retained for several years.\nFor a midseason 1990 game at Comiskey Park, the White Sox appeared once in a uniform based on that of the 1917 White Sox. They then switched their regular uniform style once more. In September, for the final series at the old Comiskey Park, the White Sox rolled out a new logo, a simplified version of the 1949\u201363 Gothic \"SOX\" logo. They also introduced a uniform with black pinstripes, also similar to the Go-Go Sox era uniform. The team's primary color changed back to black, this time with silver trim. The team also introduced a new sock logo\u2014a white silhouette of a sock centered inside a white outline of a baseball diamond\u2014which appeared as a sleeve patch on the away uniform until 2010 (switched to the \"SOX\" logo in 2011), and on the alternate black uniform since 1993. With minor modifications (i.e., occasionally wearing vests, black game jerseys), the White Sox have used this style ever since.\nDuring the 2012 and 2013 seasons, the White Sox wore their throwback uniforms at home every Sunday, starting with the 1972 red-pinstriped throwback jerseys worn during the 2012 season, followed by the 1982\u201386 uniforms the next season. In the 2014 season, the \"Winning Ugly\" throwbacks were promoted to full-time alternate status, and are now worn at home on Sundays. In one game during the 2014 season, the Sox paired their throwbacks with a cap featuring the batter logo instead of the wordmark \"SOX\"; this is currently their batting-practice cap prior to games in the throwback uniforms. After the 2023 season, the Sunday throwback uniforms were quietly taken off the team's uniform rotation.\nIn 2021, to commemorate the Field of Dreams game, the White Sox wore special uniforms honoring the 1919 team. That same year, the White Sox wore \"City Connect\" alternate uniforms introduced by Nike, featuring an all-black design with silver pinstripes, and \"Southside\" wordmark in front.\nAwards and accolades.\nAmerican League championships.\n\"Note: American League Championship Series began in 1969\"\nRetired numbers.\nThe White Sox have retired a total of 12 jersey numbers: 11 worn by former White Sox and number 42 in honor of Jackie Robinson.\nLuis Aparicio's No. 11 was issued at his request for 11-time Gold Glove winner shortstop Omar Vizquel (because No. 13 was used by manager Ozzie Guill\u00e9n; Vizquel, like Aparicio and Guillen, play(ed) shortstop and all share a common Venezuelan heritage). Vizquel played for team in 2010 and 2011.\nAlso, Harold Baines had his No. 3 retired in 1989; it has since been 'unretired' 3 times in each of his subsequent returns.\nCulture.\nNicknames.\nThe White Sox were originally known as the White Stockings, a reference to the original name of the Chicago Cubs. To fit the name in headlines, local newspapers such as the \"Chicago Tribune\" abbreviated the name alternatively to Stox and Sox. Charles Comiskey would officially adopt the White Sox nickname in the club's first years, making them the first team to officially use the \"Sox\" name. The Chicago White Sox are most prominently nicknamed \"the South Siders\", based on their particular district within Chicago. Other nicknames include the synonymous \"Pale Hose\"; \"the ChiSox\", a combination of \"Chicago\" and \"Sox\", used mostly by the national media to differentiate them between the Boston Red Sox (BoSox); and \"the Good Guys\", a reference to the team's one-time motto \"Good guys wear black\", coined by broadcaster Ken Harrelson. Most fans and Chicago media refer to the team as simply \"the Sox\". The Spanish language media sometimes refer to the team as \"Medias Blancas\" for \"White Socks.\"\nSeveral individual White Sox teams have received nicknames over the years:\nMascots.\nFrom 1961 until 1991, lifelong Chicago resident Andrew Rozdilsky performed as the unofficial yet popular mascot \"Andy the Clown\" for the White Sox at the original Comiskey Park. Known for his elongated \"Come on you White Sox\" battle cry, Andy got his start after a group of friends invited him to a Sox game in 1960, where he decided to wear his clown costume and entertain fans in his section. That response was so positive that when he won free 1961 season tickets, he decided to wear his costume to all games. Comiskey Park ushers eventually offered free admission to Rozdilsky. Starting in 1981, the new ownership group led by Jerry Reinsdorf introduced a twosome, called Ribbie and Roobarb, as the official team mascots, and banned Rozdilsky from performing in the lower seating level. Ribbie and Roobarb were very unpopular, as they were seen as an attempt to get rid of the beloved Andy the Clown.\nIn 1988, the Sox got rid of Ribbie and Roobarb; Andy the Clown was not permitted to perform in the new Comiskey Park when it opened in 1991. In the early 1990s, the White Sox had a cartoon mascot named Waldo the White Sox Wolf that advertised the \"Silver and Black Pack\", the team's kids' club at the time. The team's current mascot, SouthPaw, was introduced in 2004 to attract young fans.\nFight and theme songs.\nNancy Faust became the White Sox organist in 1970, a position she held for 40 years. She was one of the first ballpark organists to play pop music, and became known for her songs playing on the names of opposing players (such as Iron Butterfly's \"In-A-Gadda-Da-Vida\" for Pete Incaviglia). Her many years with the White Sox established her as one of the last great stadium organists. Since 2011, Lori Moreland has served as the White Sox organist.\nSimilar to the Boston Red Sox with \"Sweet Caroline\" (and two songs named \"Tessie\"), and the New York Yankees with \"Theme from New York, New York\", several songs have become associated with the White Sox over the years. They include:\nRivalries.\nCrosstown Classic.\nThe Chicago Cubs are the crosstown rivals of the White Sox, a rivalry that some made fun of prior to the White Sox's 2005 title because both of them had extremely long championship droughts. The nature of the rivalry is unique; with the exception of the 1906 World Series, in which the White Sox upset the favored Cubs, the teams never met in an official game until , when interleague play was introduced. In the intervening time, the two teams sometimes met for exhibition games. The White Sox currently led the regular-season series 48\u201339, winning the last four consecutive seasons. The BP Crosstown Cup was introduced in 2010 and the White Sox won the first three seasons (2010\u20132012) until the Cubs first won the Cup in 2013 by sweeping the season series. The White Sox won the Cup the next season and retained the Cup the following two years (series was a tie - Cup remains with defending team in the event of a tie). The Cubs took back the Cup in 2017. Two series sweeps have occurred since interleague play began, both by the Cubs in 1998 and 2013.\nAn example of this volatile rivalry is the game played between the White Sox and the Cubs at U.S. Cellular Field on May 20, 2006. White Sox catcher A. J. Pierzynski was running home on a sacrifice fly by center fielder Brian Anderson and smashed into Cubs catcher Michael Barrett, who was blocking home plate. Pierzynski lost his helmet in the collision, and slapped the plate as he rose. Barrett stopped him, and after exchanging a few words, punched Pierzynski in the face, causing a melee to ensue. Brian Anderson and Cubs first baseman John Mabry got involved in a separate confrontation, although Mabry was later determined to be attempting to be a peacemaker. After 10 minutes of conferring following the fight, the umpires ejected Pierzynski, Barrett, Anderson, and Mabry. As Pierzynski entered his dugout, he pumped his arms, causing the sold-out crowd at U.S. Cellular Field to erupt in cheers. When play resumed, White Sox second baseman Tadahito Iguchi blasted a grand slam to put the White Sox up 5\u20130 on their way to a 7\u20130 win over their crosstown rivals. While other major league cities and metropolitan areas have two teams co-exist, all of the others feature at least one team that began playing there in 1961 or later, whereas the White Sox and Cubs have been competing for their city's fans since 1901.\nHistorical.\nA historical regional rival was the St. Louis Browns. Through the 1953 season, the two teams were located fairly close to each other (including the 1901 season when the Browns were the Milwaukee Brewers), and could have been seen as the American League equivalent of the Cardinals\u2013Cubs rivalry, being that Chicago and St. Louis have for years been connected by the same highway (U.S. Route 66 and now Interstate 55). The rivalry has been somewhat revived at times in the past, involving the Browns' current identity, the Baltimore Orioles, most notably in 1983.\nThe current Milwaukee Brewers franchise were arguably the White Sox's main and biggest rival, due to the proximity of the two cities (resulting in large numbers of White Sox fans who would regularly be in attendance at the Brewers' former home, Milwaukee County Stadium), and with the teams competing in the same American League division for the 1970 and 1971 seasons and then again from 1994 to 1997. The rivalry has since cooled off, however, when the Brewers moved to the National League in 1998. However, with the start of the 2023 season, all teams will play each other at least once a year, leading to the Brewers-White Sox series to return on a yearly basis.\nDivisional.\nMinnesota Twins.\nThe rivalry between the White Sox and Minnesota Twins developed during the 2000s, as the two teams consistently battled for the AL Central Crown. The Twins won the division in 2002, 2003, 2004, 2006, and 2009, with the Sox winning in 2000, 2005, and 2008, many of those years their rival was the division runner-up. The teams met in the 2008 American League Central tie-breaker game, which was necessitated by the two clubs finishing the season with identical records. The White Sox won this game 1\u20130 on a Jim Thome home run. The rivalry re-emerged in the 2020s, with the Twins winning the AL Central in 2020 by a single game over the White Sox and Cleveland Indians, and the Sox and Twins have continued to compete for the division title since that point.\nDetroit Tigers.\nThe series between the White Sox and Detroit Tigers is one of the oldest active rivalries in the league today. Both teams joined the American League in 1901 after being charter members of the original Western League. Both have actively played one another annually for over 120 seasons. As is often the case between professional sports teams located Chicago or Detroit; there usually exists a rivalry as such with the Bulls-Pistons rivalry of the NBA. Despite playing one another for over 2,200 games; both teams have yet to meet in the postseason in their 122-year series.\nCommunity Outreach.\nIn 1990, then new White Sox owners Eddie Einhorn and Jerry Reinsdorf began Chicago White Sox Charities, a 501(c) (3) charitable organization that is the team's philanthropic arm, donating over $27 million over time to a plethora of Chicago organizations. White Sox Charities began centering on early childhood literacy programs, then expanded to focusing on encouraging high school graduation and college matriculation so the team can monitor its success. It also supports children at risk as well as promotes wellness and health.\nBroadcasting.\nRadio.\nThe White Sox did not sell exclusive rights for radio broadcasts from radio's inception until 1944, instead having local stations share rights for games, and after WGN (720) was forced to abdicate their rights to the team in the 1943 after 16 seasons due to children's programming commitments from their network, Mutual. The White Sox first granted exclusive rights in 1944, and bounced between stations until 1952, when they started having all games broadcast on WCFL (1000). Throughout this period of instability, one thing remained constant, the White Sox play-by-play announcer, Bob Elson. Known as the \"Commander\", Elson was the voice of the Sox from 1929 until his departure from the club in 1970. In 1979, he was the recipient of the Ford Frick Award, and his profile is permanently on display in the National Baseball Hall of Fame.\nAfter the 1966 season, radio rights shifted from WCFL to WMAQ (670). An NBC-owned and -operated station until 1988 when Westinghouse Broadcasting purchased it after NBC's withdrawal from radio, it was the home of the Sox until the 1996 season, outside of a team nadir in the early '70s, where it was forced to broker time on suburban La Grange's WTAQ (1300) and Evanston's WEAW-FM (105.1) to have their play-by-play air in some form (though WEAW transmitted from the John Hancock Center, FM radio was not established as a band for sports play-by-play at the time), and a one-season contract on WBBM (780) in 1981. After Elson's retirement in 1970, Harry Caray began his tenure as the voice of the White Sox, on radio and on television. Although best remembered as a broadcaster for the rival Cubs, Caray was very popular with White Sox fans, pining for a \"cold one\" during broadcasts. Caray often broadcast from the stands, sitting at a table set up amid the bleachers. It became a badge of honor among Sox fans to \"Buy Harry a beer...\" By game's end, one would see a large stack of empty beer cups beside his microphone. This only endeared him to fans that much more. In fact, he started his tradition of leading the fans in the singing of \"Take Me Out To The Ballgame\" with the Sox. Caray, alongside color analyst Jimmy Piersall, was never afraid to criticize the Sox, which angered numerous Sox managers and players, notably Bill Melton and Chuck Tanner. He left to succeed Jack Brickhouse as the voice of the Cubs in 1981, where he became a national icon.\nThe White Sox shifted through several announcers in the 1980s, before hiring John Rooney as play-by-play announcer in 1989. In 1992, he was paired with color announcer Ed Farmer. In 14 seasons together, the duo became a highly celebrated announcing team, even being ranked by \"USA Today\" as the top broadcasting team in the American League. Starting with Rooney and Farmer's fifth season together, Sox games returned to the 1000 AM frequency for the first time in 30 years. By then, it had become the ESPN owned and operated WMVP. The last game on WMVP was game 4 of the 2005 World Series, with the White Sox clinching their first World Series title in 88 years. That also was Rooney's last game with the Sox, as he left to join the radio broadcast team of the St. Louis Cardinals.\nIn 2006, radio broadcasts returned to 670 AM, this time on the sports radio station WSCR owned by CBS Radio (WSCR took over the 670 frequency in August 2000 as part of a number of shifts among CBS Radio properties to meet market ownership caps). Ed Farmer became the play-by-play man after Rooney left, joined in the booth by Chris Singleton from 2006 to 2007 and then Steve Stone in 2008. In 2009, Darrin Jackson became the color announcer for White Sox radio, where he remains today. Farmer and Jackson were joined by pregame/postgame host Chris Rogney.\nThe Chicago White Sox Radio Network currently has 18 affiliates in three states. As of recently, White Sox games are also broadcast in Spanish with play-by-play announcer Hector Molina joined in the booth by Billy Russo. Formerly broadcasting on ESPN Deportes Radio via WNUA, games are now broadcast in Spanish on WRTO (1200).\nIn the 2016 season, the play-by-play rights shifted to Cumulus Media's WLS (890) under a five-year deal, when WSCR acquired the rights to Cubs games after a one-year period on WBBM. However, by all counts, the deal was a disaster for the White Sox, as WLS's declining conservative talk format, associated ratings, and management/personnel issues (including said hosts barely promoting the team and its games), and a signal that is weak in the northern suburbs and into Wisconsin, was not a good fit for the team. Cumulus also had voluminous financial issues, and by the start of 2018, looked to both file Chapter 11 bankruptcy and restructure their play-by-play deals or depart them, both with local teams and nationally through their Westwood One/NFL deal.\nThe White Sox and Tribune Broadcasting (which has since merged with Nexstar Media Group) then announced a three-year deal for WGN Radio to become the White Sox flagship as of February 14, 2018, just in time for spring training. Ed Farmer and Darrin Jackson continued to be on play-by-play, with Andy Masur taking over pregame/postgame duties. Ed Farmer died suddenly on April 1, 2020, a long-term battle with polycystic kidney disease, but the team waited to announce his successor due to the COVID-19 pandemic and the uncertainty of the 2020 season going forward. On June 30 with the season's structure announced, Masur was confirmed as Farmer's successor for the season.\nUnder Nexstar's new management, WGN decided to pursue a thriftier programming direction, and made no moves to renew the deal at the end of the 2020 season. The team thus returned to WMVP (now managed by Good Karma Brands, which also owns Brewers flagship WTMJ) for a multi-year agreement to start with the 2021 season. In a surprising turn of events, WMVP and the team announced on December 4, 2020, that Len Kasper, the longtime television play-by-play voice of the Cubs, would move to the South Side and become the radio play-by-play voice of the White Sox. The agreement has flexibility which allows Kasper to do some television games on NBC Sports Chicago on days when Jason Benetti has other national commitments.\nTelevision.\nWhite Sox games appeared sporadically on television throughout the first half of the 20th century, most commonly announced by Jack Brickhouse on WGN-TV (channel 9). Starting in 1968, Jack Drees took play-by-play duties as the Sox were broadcast on WFLD (channel 32). After 1972, Harry Caray (joined by Jimmy Piersall in 1977) began double duty as a TV and radio announcer for the Sox, as broadcasts were moved to channel 44, WSNS-TV, from 1972 to 1980, followed by one year on WGN-TV.\nDon Drysdale became the play-by-play announcer in 1982, as the White Sox began splitting their broadcasts between WFLD and the new regional cable television network, Sportsvision. Ahead of its time, Sportsvision had a chance to gain huge profits for the Sox. However, few people would subscribe to the channel after being used to free-to-air broadcasts for many years, along with Sportsvision being stunted by the city of Chicago's wiring for cable television taking much longer than many markets because of it being an area where over-the-air subscription services were still more popular, resulting in the franchise losing around $300,000 a month. While this was going on, every Cubs game was on WGN, with Harry Caray becoming the national icon he never was with the White Sox. The relatively easy near-national access to Cubs games versus Sox games in this era, combined with the popularity of Caray and the Cubs being owned by the Tribune Company, is said by some to be the main cause of the Cubs' advantage in popularity over the Sox.\nThree major changes to White Sox broadcasting occurred in 1989-1991: in 1989, with the city finally fully wired for cable service, Sportsvision was replaced by SportsChannel Chicago (itself eventually turning into Fox Sports Net Chicago), which varied over its early years as a premium sports service and basic cable channel. In 1990, over-the-air broadcasts shifted back to WGN. And in 1991, Ken Harrelson became the play-by-play announcer of the White Sox. One of the most polarizing figures in baseball, \"Hawk\" has been both adored and scorned for his emotive announcing style. His history of calling out umpires has earned him reprimands from the MLB commissioner's office, and he has been said to be the most biased announcer in baseball. However, Harrelson has said that he is proud of being \"the biggest homer in baseball\", saying that he is a White Sox fan like his viewers. The team moved from FSN Chicago to the newly launched NBC Sports Chicago in March 2005, as Jerry Reinsdorf looked to control the rights for his team rather than sell rights to another party; Reinsdorf holds a 40% interest in the network, with 20% of that interest directly owned by the White Sox corporation.\nPreviously, White Sox local television broadcasts were split between two channels: the majority of games were broadcast on cable by NBC Sports Chicago, and remaining games were produced by WGN Sports and were broadcast locally on WGN-TV. WGN games were also occasionally picked up by local stations in Illinois, Iowa, and Indiana. In the past, WGN games were broadcast nationally on the WGN America superstation, but those broadcasts ended after the 2014 season as WGN America began its transition to a standard cable network. WGN Sports-produced White Sox games not carried by WGN-TV were carried by WCIU-TV (channel 26) until the 2015 season, when they moved to MyNetworkTV station WPWR (channel 50). That arrangement ended on September 1, 2016, when WGN became an independent station.\nPrior to 2016, the announcers were the same no matter where the games were broadcast: Harrelson provided play-by-play, and Steve Stone provided color analysis since 2009. Games that are broadcast on NBC Sports Chicago feature pregame and postgame shows, hosted by Chuck Garfein with analysis from Bill Melton and occasionally Frank Thomas. In 2016, the team announced an official split of the play-by-play duties, with Harrelson calling road games and the Crosstown Series and Jason Benetti calling home games. In 2017, the team announced that the 2018 season will be Harrelson's final in the booth. He will call 20 games over the course of the season, after which Benetti will take over full-time play-by-play duties.\nOn January 2, 2019, the White Sox (along with the Chicago Bulls and Chicago Blackhawks) agreed to an exclusive multiyear deal with NBC Sports Chicago, ending the team's broadcasts on WGN-TV following the 2019 season.\nPrior to the 2024 season, the White Sox named John Schriffen as its new lead television play-by-play announcer, after Benetti departed to join the Detroit Tigers broadcast team.\nMinor league affiliates.\nThe Chicago White Sox farm system consists of six minor league affiliates.\nSilver Chalice subsidiary.\nSilver Chalice is a digital and media investment subsidiary of the White Sox with Brooks Boyer as CEO.\nSilver Chalice was co-founded by Jerry Reinsdorf, White Sox executive Brooks Boyer, Jason Coyle and John Burris in 2009. The company first invested in 120 Sports, a digital sports channel, that launched in June 2014. Chalice then partnered with IMG on Campus Insiders, a college sports digital channel, in 2015. These two efforts merged with Sinclair Broadcasting Group's American Sports Network into the new multi-platform network Stadium in September 2017.\nIn May 2023, Sinclair sold its controlling interest in Stadium to Silver Chalice."}
{"id": "5946", "revid": "32184767", "url": "https://en.wikipedia.org/wiki?curid=5946", "title": "Casuistry", "text": "Casuistry ( ) is a process of reasoning that seeks to resolve moral problems by extracting or extending abstract rules from a particular case, and reapplying those rules to new instances. This method occurs in applied ethics and jurisprudence. The term is also used pejoratively to criticise the use of clever but unsound reasoning, especially in relation to ethical questions (as in sophistry). It has been defined as follows:\nStudy of cases of conscience and a method of solving conflicts of obligations by applying general principles of ethics, religion, and moral theology to particular and concrete cases of human conduct. This frequently demands an extensive knowledge of natural law and equity, civil law, ecclesiastical precepts, and an exceptional skill in interpreting these various norms of conduct... \nIt remains a common method in applied ethics.\nEtymology.\nAccording to the Online Etymological Dictionary, the term and its agent noun \"casuist\", appearing from about 1600, derive from the Latin noun , meaning \"case\", especially as referring to a \"case of conscience\". The same source says, \"Even in the earliest printed uses the sense was pejorative\".\nHistory.\nCasuistry dates from Aristotle (384\u2013322 BC), yet the peak of casuistry was from 1550 to 1650, when the Society of Jesus (commonly known as the \"Jesuits\") used case-based reasoning, particularly in administering the Sacrament of Penance (or \"confession\"). The term became pejorative following Blaise Pascal's attack on the misuse of the method in his \"Provincial Letters\" (1656\u201357). The French mathematician, religious philosopher and Jansenist sympathiser attacked priests who used casuistic reasoning in confession to pacify wealthy church donors. Pascal charged that \"remorseful\" aristocrats could confess a sin one day, re-commit it the next, then generously donate to the church and return to re-confess their sin, confident that they were being assigned a penance in name only. These criticisms darkened casuistry's reputation in the following centuries. For example, the \"Oxford English Dictionary\" quotes a 1738 essay by Henry St. John, 1st Viscount Bolingbroke to the effect that casuistry \"destroys, by distinctions and exceptions, all morality, and effaces the essential difference between right and wrong, good and evil\".\nThe 20th century saw a revival of interest in casuistry. In their book \"The Abuse of Casuistry: A History of Moral Reasoning\" (1988), Albert Jonsen and Stephen Toulmin argue that it is not casuistry but its abuse that has been a problem; that, properly used, casuistry is powerful reasoning. Jonsen and Toulmin offer casuistry as a method for compromising the contradictory principles of moral absolutism and moral relativism. In addition, the ethical philosophies of utilitarianism (especially preference utilitarianism) and pragmatism have been identified as employing casuistic reasoning.\nEarly modernity.\nThe casuistic method was popular among Catholic thinkers in the early modern period. Casuistic authors include Antonio Escobar y Mendoza, whose \"Summula casuum conscientiae\" (1627) enjoyed great success, Thomas Sanchez, Vincenzo Filliucci (Jesuit and penitentiary at St Peter's), Antonino Diana, Paul Laymann (\"Theologia Moralis\", 1625), John Azor (\"Institutiones Morales\", 1600), Etienne Bauny, Louis Cellot, Valerius Reginaldus, and Hermann Busembaum (d. 1668).\nThe progress of casuistry was interrupted toward the middle of the 17th century by the controversy which arose concerning the doctrine of probabilism, which effectively stated that one could choose to follow a \"probable opinion\"that is, an opinion supported by a theologian or anothereven if it contradicted a more probable opinion or a quotation from one of the Fathers of the Church.\n&lt;section begin=Alleged corruption in the Catholic Church transclusion/&gt;Certain kinds of casuistry were criticised by early Protestant theologians, because it was used to justify many of the abuses that they sought to reform. It was famously attacked by the Catholic and Jansenist philosopher Blaise Pascal during the formulary controversy against the Jesuits, in his Provincial Letters, as the use of rhetorics to justify moral laxity, which became identified by the public with Jesuitism; hence the everyday use of the term to mean complex and sophistic reasoning to justify moral laxity. By the mid-18th century, \"casuistry\" had become a synonym for attractive-sounding, but ultimately false, moral reasoning.&lt;section end=Alleged corruption in the Catholic Church transclusion/&gt;\nIn 1679 Pope Innocent\u00a0XI publicly condemned sixty-five of the more radical propositions (\"stricti mentalis\"), taken chiefly from the writings of Escobar, Suarez and other casuists as \"propositiones laxorum moralistarum\" and forbade anyone to teach them under penalty of excommunication. Despite this condemnation by a pope, both Catholicism and Protestantism permit the use of ambiguous statements in specific circumstances.\nLater modernity.\nG. E. Moore dealt with casuistry in chapter 1.4 of his \"Principia Ethica\", in which he claimed that \"the defects of casuistry are not defects of principle; no objection can be taken to its aim and object. It has failed only because it is far too difficult a subject to be treated adequately in our present state of knowledge\". Furthermore, he asserted that \"casuistry is the goal of ethical investigation. It cannot be safely attempted at the beginning of our studies, but only at the end\".\nSince the 1960s, applied ethics has revived the ideas of casuistry in applying moral reasoning to particular cases in law, bioethics, and business ethics. Its facility for dealing with situations where rules or values conflict with each other has made it a useful approach in professional ethics, and casuistry's reputation has improved somewhat as a result.\nPope Francis, a Jesuit, has criticized casuistry as \"the practice of setting general laws on the basis of exceptional cases\" in instances where a more holistic approach would be preferred."}
{"id": "5948", "revid": "4842600", "url": "https://en.wikipedia.org/wiki?curid=5948", "title": "Chinese input method", "text": "Several input methods allow the use of Chinese characters with computers. Most allow selection of characters based either on their pronunciation or their graphical shape. Phonetic input methods are easier to learn but are less efficient, while graphical methods allow faster input, but have a steep learning curve.\nOther methods allow users to write characters directly via touchscreens, such as those found on mobile phones and tablet computers.\nHistory.\nChinese input methods predate the computer. One of the early attempts was an electro-mechanical Chinese typewriter Ming kwai () which was invented by Lin Yutang, a prominent Chinese writer, in the 1940s. It assigned thirty base shapes or strokes to different keys and adopted a new way of categorizing Chinese characters. But the typewriter was not produced commercially and Lin soon found himself deeply in debt.\nBefore the 1980s, Chinese publishers hired teams of workers and selected a few thousand type pieces from an enormous Chinese character set. Chinese government agencies entered characters using a long, complicated list of Chinese telegraph codes, which assigned different numbers to each character. During the early computer era, Chinese characters were categorized by their radicals or Pinyin romanization, but results were less than satisfactory.\nIn the 1970s to 1980s, large keyboards with thousands of keys were used to input Chinese. Each key was mapped to several Chinese characters. To type a character, one pressed the character key and then a selection key. There were also experimental \"radical keyboards\" with dozens to several hundreds keys. Chinese characters were decomposed into \"radicals\", each of which was represented by a key. Unwieldy and difficult to use, these keyboards became obsolete after the introduction of Cangjie input method, the first method to use only the standard keyboard and make Chinese touch typing possible.\nChu Bong-Foo invented a common input method in 1976 with his Cangjie input method, which assigns different \"roots\" to each key on a standard computer keyboard. With this method, for example, the character is assigned to the A key, and \u6708 is assigned to B. Typing them together will result in the character (\"bright\").\nDespite its steeper learning curve, this method remains popular in Chinese communities that use traditional Chinese characters, such as Hong Kong and Taiwan; the method allows very precise input, thus allowing users to type more efficiently and quickly, provided they are familiar with the fairly complicated rules of the method. It was the first method that allowed users to enter more than a hundred Chinese characters per minute. Its popularity is also helped by its omnipresence on traditional Chinese computer systems, since Chu has given up its patent in 1982, stating that it should be part of the cultural asset. Developers of Chinese systems can adopt it freely, and users do not have the hassle of it being absent on devices with Chinese support. Cangjie input programs supporting a large CJK character set have been developed.\nAll methods have their strengths and weaknesses. The pinyin method can be learned rapidly but its maximum input rate is limited. The Wubi method takes longer to learn, but expert typists can enter text much more rapidly with it than with phonetic methods. However, Wubi is proprietary, and a version of it has become freely available only after its inventor lost a patent lawsuit in 1997.\nDue to these complexities, there is no \"standard\" method.\nIn mainland China, pinyin methods such as Sogou Pinyin and Google Pinyin are the most popular. In Taiwan, use of Cangjie, Dayi, Boshiamy, and bopomofo predominate; and in Hong Kong and Macau, the Cangjie is most often taught in schools, while a few schools teach CKC Chinese Input System.\nOther methods include handwriting recognition, OCR and speech recognition. The computer itself must first be \"trained\" before the first or second of these methods are used; that is, the new user enters the system in a special \"learning mode\" so that the system can learn to identify their handwriting or speech patterns. The latter two methods are used less frequently than keyboard-based input methods and suffer from relatively high error rates, especially when used without proper \"training\", though higher error rates are an acceptable trade-off to many users.\nCategories.\nPhonetic-based.\nThe user enters pronunciations that are converted into relevant Chinese characters. The user must select the desired character from homophones, which are common in Chinese. Modern systems, such as Sogou Pinyin and Google Pinyin, predict the desired characters based on context and user preferences. For example, if one enters the sounds \"jicheng\", the software will type (to inherit), but if \"jichengche\" is entered, (taxi) will appear.\nVarious Chinese dialects complicate the system. Phonetic methods are mainly based on standard pinyin, Zhuyin/Bopomofo, and Jyutping in China, Taiwan, and Hong Kong, respectively. Input methods based on other varieties of Chinese, like Hakka or Minnan, also exist.\nWhile the phonetic system is easy to learn, choosing appropriate Chinese characters slows typing speed. Most users report a typing speed of fifty characters per minute, though some reach over one hundred per minute. With some phonetic IMEs (Input Method Editors), in addition to predictive input based on previous conversions, it is possible for users to create custom dictionary entries for frequently used characters and phrases, potentially lowering the number of characters required to evoke it.\nShuangpin.\nShuangpin (; ), literally dual spell, is a stenographical phonetic input method based on hanyu pinyin that reduces the number of keystrokes for one Chinese character to two by distributing every vowel and consonant composed of more than one letter to a specific key. In most Shuangpin layout schemes such as Xiaohe, Microsoft 2003 and Ziranma, the most frequently used vowels are placed on the middle layer, reducing the risk of repetitive strain injury.\nShuangpin is supported by a large number of pinyin input software including QQ, Microsoft Bing Pinyin, Sogou Pinyin and Google Pinyin."}
{"id": "5950", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5950", "title": "Columbus, Ohio", "text": "Columbus (, ) is the capital and most populous city of the U.S. state of Ohio. With a 2020 census population of 905,748, it is the 14th-most populous city in the U.S., the second-most populous city in the Midwest (after Chicago), and the third-most populous U.S. state capital (after Phoenix, Arizona and Austin, Texas). Columbus is the county seat of Franklin County; it also extends into Delaware and Fairfield counties. It is the core city of the Columbus metropolitan area, which encompasses ten counties in central Ohio. It had a population of 2.139 million in 2020, making it the largest metropolitan area entirely in Ohio and 32nd-largest metro area in the U.S.\nColumbus originated as numerous Native American settlements on the banks of the Scioto River. Franklinton, now a city neighborhood, was the first European settlement, laid out in 1797. The city was founded in 1812 at the confluence of the Scioto and Olentangy rivers, and laid out to become the state capital. The city was named for Italian explorer Christopher Columbus. The city assumed the function of state capital in 1816 and county seat in 1824. Amid steady years of growth and industrialization, the city has experienced numerous floods and recessions. Beginning in the 1950s, Columbus began to experience significant growth; it became the largest city in Ohio in land and population by the early 1990s. Growth has continued in the 21st century, with redevelopment occurring in numerous city neighborhoods, including Downtown.\nThe city has a diverse economy without reliance on any one sector. The metropolitan area is home to the Battelle Memorial Institute, the world's largest private research and development foundation; Chemical Abstracts Service, the world's largest clearinghouse of chemical information; and the Ohio State University, one of the largest universities in the United States. The Greater Columbus area is further home to the headquarters of six Fortune 500 companies, namely Cardinal Health, American Electric Power, Bath &amp; Body Works, Inc., Nationwide, Bread Financial and Huntington Bancshares.\nName.\nThe city of Columbus was named after 15th-century Italian explorer Christopher Columbus. It is the largest city in the world named for the explorer, who sailed to and settled parts of the Americas on behalf of Isabella I of Castile and Spain. Although no reliable history exists as to why Columbus, who had no connection to the city or state of Ohio before the city's founding, was chosen as the name for the city, the book \"Columbus: The Story of a City\" indicates a state lawmaker and local resident admired the explorer enough to persuade other lawmakers to name the settlement Columbus.\nSince the late 20th century, historians have criticized Columbus for initiating the European conquest of America and for abuse, enslavement, and subjugation of natives. Efforts to remove symbols related to the explorer in the city date to the 1990s. Amid the George Floyd protests in 2020, several petitions pushed for the city to be renamed.\nNicknames for the city have included \"the Discovery City\", \"Arch City\", \"Cap City\", \"Cowtown\", \"The Biggest Small Town in America\" and \"Cbus.\"\nHistory.\nAncient and early history.\nBetween 1000 B.C. and 1700 A.D., the Columbus metropolitan area was a center to indigenous cultures known as the Mound Builders, including the Adena, Hopewell and Fort Ancient peoples. Remaining physical evidence of the cultures are their burial mounds and what they contained. Most of Central Ohio's remaining mounds are located outside of Columbus city boundaries, though the Shrum Mound is maintained, now as part of a public park and historic site. The city's Mound Street derives its name from a mound that existed by the intersection of Mound and High Streets. The mound's clay was used in bricks for most of the city's initial brick buildings; many were subsequently used in the Ohio Statehouse. The city's Ohio History Center maintains a collection of artifacts from these cultures.\n18th century.\nThe area including present-day Columbus once comprised the Ohio Country, under the nominal control of the French colonial empire through the Viceroyalty of New France from 1663 until 1763.\nIn the 18th century, European traders flocked to the area, attracted by the fur trade. The area was often caught between warring factions, including American Indian and European interests. In the 1740s, Pennsylvania traders overran the territory until the French forcibly evicted them. Fighting for control of the territory in the French and Indian War (1754\u20131763) became part of the international Seven Years' War (1756\u20131763). During this period, the region routinely suffered turmoil, massacres and battles. The 1763 Treaty of Paris ceded the Ohio Country to the British Empire.\nUp until the American Revolution, Central Ohio had continuously been the home of numerous indigenous villages. A Mingo village was located at the forks of the Scioto and Olentangy rivers, with Shawnee villages to the south and Wyandot and Delaware villages to the north. Colonial militiamen burned down the Mingo village in 1774 during a raid.\nVirginia Military District.\nAfter the American Revolution, the Virginia Military District became part of the Ohio Country as a territory of Virginia. Colonists from the East Coast moved in, but rather than finding an empty frontier, they encountered people of the Miami, Delaware, Wyandot, Shawnee and Mingo nations, as well as European traders. The tribes resisted expansion by the fledgling United States, leading to years of bitter conflict. The decisive Battle of Fallen Timbers resulted in the Treaty of Greenville in 1795, which finally opened the way for new settlements. By 1797, a young surveyor from Virginia named Lucas Sullivant had founded a permanent settlement on the west bank of the forks of the Scioto and Olentangy rivers. An admirer of Benjamin Franklin, Sullivant chose to name his frontier village \"Franklinton.\" The location was desirable for its proximity to the navigable rivers \u2013 but Sullivant was initially foiled when, in 1798, a large flood wiped out the new settlement. He persevered, and the village was rebuilt, though somewhat more inland.\nAfter the Revolution, land comprising parts of Franklin and adjacent counties was set aside by the United States Congress for settlement by Canadians and Nova Scotians who were sympathetic to the colonial cause and had their land and possessions seized by the British government. The Refugee Tract, consisting of , was long and wide, and was claimed by 67 eligible men. The Ohio Statehouse sits on land once contained in the Refugee Tract.\n19th century.\nAfter Ohio achieved statehood in 1803, political infighting among prominent Ohio leaders led to the state capital moving from Chillicothe to Zanesville and back again. Desiring to settle on a location, the state legislature considered Franklinton, Dublin, Worthington and Delaware before compromising on a plan to build a new city in the state's center, near major transportation routes, primarily rivers. As well, Franklinton landowners had donated two plots in an effort to convince the state to move its capital there. The two spaces were set to become Capitol Square, including for the Ohio Statehouse and the Ohio Penitentiary. Named in honor of Christopher Columbus, the city was founded on February 14, 1812, on the \"High Banks opposite Franklinton at the Forks of the Scioto most known as Wolf's Ridge.\" At the time, this area was a dense forestland, used only as a hunting ground.\nThe city was incorporated as a borough on February 10, 1816. Between 1816 and 1817, Jarvis W. Pike served as the first appointed mayor. Although the recent War of 1812 had brought prosperity to the area, the subsequent recession and conflicting claims to the land threatened the new town's success. Early conditions were abysmal, with frequent bouts of fevers, attributed to malaria from the flooding rivers, and an outbreak of cholera in 1833. It led Columbus to create the Board of Health, now part of the Columbus Public Health department. The outbreak, which remained in the city from July to September 1833, killed 100 people.\nColumbus was without direct river or trail connections to other Ohio cities, leading to slow initial growth. The National Road reached Columbus from Baltimore in 1831, which complemented the city's new link to the Ohio and Erie Canal, both of which facilitated a population boom. A wave of European immigrants led to the creation of two ethnic enclaves on the city's outskirts. A large Irish population settled in the north along Naghten Street (presently Nationwide Boulevard), while the Germans took advantage of the cheap land to the south, creating a community that came to be known as the \"Das Alte S\u00fcdende\" (The Old South End). Columbus's German population constructed numerous breweries, Trinity Lutheran Seminary and Capital University.\nWith a population of 3,500, Columbus was officially chartered as a city on March 3, 1834. On that day, the legislature carried out a special act, which granted legislative authority to the city council and judicial authority to the mayor. Elections were held in April of that year, with voters choosing John Brooks as the first popularly elected mayor. Columbus annexed the then-separate city of Franklinton in 1837.\nIn 1850, the Columbus and Xenia Railroad became the first railroad into the city, followed by the Cleveland, Columbus and Cincinnati Railroad in 1851. The two railroads built a joint Union Station on the east side of High Street just north of Naghten (then called North Public Lane). Rail traffic into Columbus increased: by 1875, eight railroads served Columbus, and the rail companies built a new, more elaborate station. Another cholera outbreak hit Columbus in 1849, prompting the opening of the city's Green Lawn Cemetery. On January 7, 1857, the Ohio Statehouse finally opened after 18 years of construction.\nBefore the abolition of slavery in the Southern United States in 1863, the Underground Railroad was active in Columbus and was led, in part, by James Preston Poindexter. Poindexter arrived in Columbus in the 1830s and became a Baptist preacher and leader in the city's African-American community until the turn of the century.\nDuring the Civil War, Columbus was a major base for the volunteer Union Army. It housed 26,000 troops and held up to 9,000 Confederate prisoners of war at Camp Chase, at what is now the Hilltop neighborhood of west Columbus. Over 2,000 Confederate soldiers remain buried at the site, making it one of the North's largest Confederate cemeteries.\nBy virtue of the Morrill Act of 1862, the Ohio Agricultural and Mechanical College \u2013 which eventually became the Ohio State University \u2013 was founded in 1870 on the former estate of William and Hannah Neil.\nBy the end of the 19th century, Columbus was home to several major manufacturing businesses. The Jeffrey Manufacturing Company was a major supplier of coal mining equipment. The city became known as the \"Buggy Capital of the World,\" thanks to the two dozen buggy factories \u2013 notably the Columbus Buggy Company, founded in 1875 by C.D. Firestone. The Columbus Consolidated Brewing Company also rose to prominence during this time and might have achieved even greater success were it not for the Anti-Saloon League in neighboring Westerville.\nIn the steel industry, a forward-thinking man named Samuel P. Bush presided over the Buckeye Steel Castings Company. Columbus was also a popular location for labor organizations. In 1886, Samuel Gompers founded the American Federation of Labor in Druid's Hall on South Fourth Street, and in 1890, the United Mine Workers of America was founded at the old City Hall.\n20th century.\nColumbus earned one of its nicknames, \"The Arch City,\" because of the dozens of wooden arches that spanned High Street at the turn of the 20th century. The arches illuminated the thoroughfare and eventually became the means by which electric power was provided to the new streetcars. The city tore down the arches and replaced them with cluster lights in 1914 but reconstructed them from metal in the Short North neighborhood in 2002 for their unique historical interest.\nOn March 25, 1913, the Great Flood of 1913 devastated the neighborhood of Franklinton, leaving over 90 people dead and thousands of West Side residents homeless. To prevent flooding, the Army Corps of Engineers recommended widening the Scioto River through downtown, constructing new bridges and building a retaining wall along its banks. With the strength of the post-World War I economy, a construction boom occurred in the 1920s, resulting in a new civic center, the Ohio Theatre, the American Insurance Union Citadel and to the north, a massive new Ohio Stadium. Although the American Professional Football Association was founded in Canton in 1920, its head offices moved to Columbus in 1921 to the New Hayden Building and remained in the city until 1941.\nIn 1922, the association's name was changed to the National Football League. Nearly a decade later, in 1931, at a convention in the city, the Jehovah's Witnesses took that name by which they are known today.\nThe effects of the Great Depression were less severe in Columbus, as the city's diversified economy helped it fare better than its Rust Belt neighbors. World War II brought many new jobs and another population surge. This time, most new arrivals were migrants from the \"extraordinarily depressed rural areas\" of Appalachia, who would soon account for more than a third of Columbus's growing population. In 1948, the Town and Country Shopping Center opened in suburban Whitehall, and it is now regarded as one of the first modern shopping centers in the United States.\nThe construction of the Interstate Highway System signaled the arrival of rapid suburb development in central Ohio. To protect the city's tax base from this suburbanization, Columbus adopted a policy of linking sewer and water hookups to annexation to the city. By the early 1990s, Columbus had grown to become Ohio's largest city in land area and in population.\nEfforts to revitalize downtown Columbus have had some success in recent decades, though like most major American cities, some architectural heritage was lost in the process. In the 1970s, landmarks such as Union Station and the Neil House hotel were razed to construct high-rise offices and big retail space. The PNC Bank building was constructed in 1977, as well as the Nationwide Plaza buildings and other towers that sprouted during this period. The construction of the Greater Columbus Convention Center has brought major conventions and trade shows to the city.\n21st century.\nThe Scioto Mile began development along the riverfront, an area that already had the Miranova Corporate Center and The Condominiums at North Bank Park.\nThe 2010 United States foreclosure crisis forced the city to purchase numerous foreclosed, vacant properties to renovate or demolish them \u2013 at a cost of tens of millions of dollars. In February 2011, Columbus had 6,117 vacant properties, according to city officials.\nSince 2010, Columbus has been growing in population and economy; from 2010 to 2017, the city added 164,000 jobs, which ranked second in the United States. In February and March 2020, Columbus reported its first official cases of COVID-19 and declared a state of emergency, with all nonessential businesses closed statewide. There were 69,244 cases of the disease across the city, . Later in 2020, protests over the murder of George Floyd took place in the city from May 28 into August. Columbus and its metro area have experienced growth in the high-tech manufacturing sector, with Intel announcing plans to construct a $20 billion factory and Honda expanding its presence along with LG Energy Solutions with a $4.4 billion battery manufactory facility in Fayette County.\nThe COVID-19 pandemic muted activity in Columbus, especially in its downtown core, from 2020 to 2022. By late 2022, foot traffic in Downtown Columbus began to exceed pre-pandemic rates; one of the quickest downtown areas to recover in the United States.\nOn June 23, 2023, ten people were injured in a mass shooting in the city's Short North district.\nRansomware attack.\nIn July 2024, Columbus was subject to a ransomware attack, for which the hacker group Rhysidia took credit. In August 2024, Columbus Mayor Andrew Ginther claimed that the files obtained by Rhysidia were \"unusable\" to the thieves due to being either encrypted or corrupted. Ginther's assertion was subsequently shown to be false by security researcher David Leroy Ross (who goes by the alias Connor Goodwolf), who revealed that the files were intact and contained data including names from domestic violence cases and Social Security numbers of crime victims. Columbus then sued Ross for alleged criminal acts, negligence, and civil conversion, as well as taking out a restraining order against Ross, both of which actions were later defended by City Attorney Zach Klein. In response, a number of prominent cybersecurity researchers called on the city to drop the lawsuit.\nNeo-Nazi march.\nOn Saturday, November 19th, 2024, about a dozen masked men dressed in black carried red swastika flags in Columbus chanting racial slurs and using pepper spray. The group identified themselves as \"Hate Club\". Oren Segal, ADL vice-president, said that this might related to the hate group Blood Tribe. \"Blood Tribe views itself as the main white supremacist group in Ohio, so ... (the) 'Hate Club' march appears to have been an intentional effort to antagonize them.\"\nGeography.\nThe confluence of the Scioto and Olentangy rivers is just northwest of Downtown Columbus. Several smaller tributaries course through the Columbus metropolitan area, including Alum Creek, Big Walnut Creek and Darby Creek. Columbus is considered to have relatively flat topography thanks to a large glacier that covered most of Ohio during the Wisconsin Ice Age. However, there are sizable differences in elevation through the area, with the high point of Franklin County being above sea level near New Albany, and the low point being where the Scioto River leaves the county near Lockbourne.\nSeveral ravines near the rivers and creeks also add variety to the landscape. Tributaries to Alum Creek and the Olentangy River cut through shale, while tributaries to the Scioto River cut through limestone. The numerous rivers and streams beside low-lying areas in Central Ohio contribute to a history of flooding in the region; the most significant was the Great Flood of 1913 in Columbus, Ohio.\nThe city has a total area of , of which is land and is water. Columbus currently has the largest land area of any Ohio city; this is due to Jim Rhodes's tactic to annex suburbs while serving as mayor. As surrounding communities grew or were constructed, they came to require access to waterlines, which was under the sole control of the municipal water system. Rhodes told these communities that if they wanted water, they would have to submit to assimilation into Columbus.\nNeighborhoods.\nColumbus has a wide diversity of neighborhoods with different characters, and is thus sometimes known as a \"city of neighborhoods.\" Some of the most prominent neighborhoods include the Arena District, the Brewery District, Clintonville, Franklinton, German Village, The Short North and Victorian Village.\nClimate.\nThe city's climate is humid continental (K\u00f6ppen climate classification \"Dfa\") transitional with the humid subtropical (K\u00f6ppen climate classification \"Cfa\") to the south characterized by warm, muggy summers and cold, dry winters. Columbus is within USDA hardiness zone 6b, bordering on 7a. Winter snowfall is relatively light, since the city is not in the typical path of strong winter lows, such as the Nor'easters that strike cities farther east. It is also too far south and west for lake-effect snow from Lake Erie to have much effect, although the lakes to the north contribute to long stretches of cloudy spells in winter.\nThe highest temperature recorded in Columbus is , which occurred twice during the Dust Bowl of the 1930s: once on July 21, 1934, and again on July 14, 1936. The lowest recorded temperature was , occurring on January 19, 1994.\nColumbus is subject to severe weather typical to the Midwestern United States. Severe thunderstorms can bring lightning, large hail and on rare occasions tornadoes, especially during the spring and sometimes through fall. A tornado that occurred on October 11, 2006, caused F2 damage. Floods, blizzards and ice storms can also occur from time to time.\nDemographics.\n2020 census.\nIn the 2020 United States census, there were 905,748 people living in the city, for a population density of 4,109.64 people per square mile (1,586.74/km2). There were 415,456 housing units. The racial makeup of the city was 57.4% White, 29.2% Black or African American, 0.2% Native American or Alaska Native, and 5.9% Asian. Hispanic or Latino of any race made up 6.3% of the population.\nThere were 392,041 households, out of which 25.5% had children under the age of 18 living with them, 30.8% were married couples living together, 25.1% had a male householder with no spouse present, and 33.7% had a female householder with no spouse present. 37.1% of all households were made up of individuals, and 9.7% were someone living alone who was 65 years of age or older. The average household size was 2.26, and the average family size was 3.03.\n21.0% of the city's population were under the age of 18, 67.5% were 18 to 64, and 11.5% were 65 years of age or older. The median age was 33.3. For every 100 females, there were 97.3 males.\nAccording to the U.S. Census American Community Survey, for the period 2016-2020 the estimated median annual income for a household in the city was $61,727, and the median income for a family was $76,383. About 18.1% of the population were living below the poverty line, including 26.1% of those under age 18 and 12.0% of those age 65 or over. About 67.2% of the population were employed, and 38.5% had a bachelor's degree or higher.\n2010 census.\nIn the 2010 United States census, there were 787,033 people, 331,602 households and 176,037 families residing in the city. The population density was . There were 370,965 housing units at an average density of .\nThe racial makeup of the city included 815,985 races tallied, as some residents recognized multiple races. The racial makeup was 61.9% White, 29.1% Black or African American, 1% Native American or Alaska Native, 4.6% Asian, 0.2% Native Hawaiian or Pacific Islander, and 3.2% from other races. Hispanic or Latino of any race were 5.9% of the population.\nPopulation makeup.\nColumbus historically had a significant population of white people. In 1900, whites made up 93.4% of the population. Although European immigration has declined, the Columbus metropolitan area has recently experienced increases in African, Asian and Latin American immigration, including groups from Mexico, India, Nepal, Bhutan, Somalia and China. While the Asian population is diverse, the city's Hispanic community is mainly made up of Mexican Americans, although there is a notable Puerto Rican population. Many other countries of origin are represented in lesser numbers, largely due to the international draw of Ohio State University. 2008 estimates indicate that roughly 116,000 of the city's residents are foreign-born, accounting for 82% of the new residents between 2000 and 2006 at a rate of 105 per week. 40% of the immigrants came from Asia, 23% from Africa, 22% from Latin America and 13% from Europe. The city had the second-largest Somali and Somali American population in the country, as of 2004, as well as the largest expatriate Bhutanese-Nepali population in the world, as of 2018.\nDue to its demographics, which include a mix of races and a wide range of incomes, as well as urban, suburban and nearby rural areas, Columbus is considered a \"typical\" American city, leading retail and restaurant chains to use it as a test market for new products. For similar reasons, the city was chosen as the launch city for the QUBE cable television service.\nColumbus has maintained a steady population growth since its establishment. Its slowest growth, from 1850 to 1860, is primarily attributed to the city's cholera epidemic in the 1850s.\nAccording to the 2017 Japanese Direct Investment Survey by the Consulate-General of Japan, Detroit, 838 Japanese nationals lived in Columbus, making it the municipality with the state's second-largest Japanese national population, after Dublin.\nColumbus is home to a proportional LGBT community, with an estimated 34,952 gay, lesbian or bisexual residents. The 2018 American Community Survey (ACS) reported an estimated 366,034 households, 32,276 of which were held by unmarried partners. 1,395 of these were female householder and female-partner households, and 1,456 were male householder and male-partner households. Columbus has been rated as one of the best cities in the country for gays and lesbians to live, and also as the most underrated gay city in the country. In July 2012, three years prior to legal same-sex marriage in the United States, the Columbus City Council unanimously passed a domestic partnership registry.\nItalian-American community and symbols.\nColumbus has numerous Italian Americans, with groups including the Columbus Italian Club, Columbus Piave Club and the Abruzzi Club. Italian Village, a neighborhood near Downtown Columbus, has had a prominent Italian American community since the 1890s.\nThe community has helped promote the influence Christopher Columbus had in drawing European attention to the Americas. The Italian explorer, erroneously credited with the lands' discovery, has been posthumously criticized by historians for initiating colonization and for abuse, enslavement and subjugation of natives. In addition to the city being named for the explorer, its seal and flag depict a ship he used for his first voyage to the Americas, the . A similar-size replica of the ship, the Santa Maria Ship &amp; Museum, was displayed downtown from 1991 to 2014. The city's Discovery District and Discovery Bridge are named in reference to Columbus's \"discovery\" of the Americas; the bridge includes artistic bronze medallions featuring symbols of the explorer. Genoa Park, downtown, is named after Genoa, the birthplace of Christopher Columbus and one of Columbus's sister cities.\nThe Christopher Columbus Quincentennial Jubilee, celebrating the 500th anniversary of Columbus's first voyage, was held in the city in 1992. Its organizers spent $95 million on it, creating the horticultural exhibition AmeriFlora '92. The organizers also planned to create a replica Native American village, among other attractions. Local and national native leaders protested the event with a day of mourning, followed by protests and fasts at City Hall. The protests prevented the native village from being exhibited, and annual fasts continued until 1997. A protest also took place during the dedication of the \"Santa Maria\" replica, an event held in late 1991 on the day before Columbus Day and in time for the jubilee.\nThe city has three outdoor statues of the explorer; the statue at City Hall was acquired, delivered and dedicated with the assistance of the Italian American community. Protests in 2017 aimed for this statue to be removed, followed by the city in 2018 ceasing to recognize Columbus Day as a city holiday. During the 2020 George Floyd protests, petitions were created to remove all three statues and rename the city of Columbus.\nThe city was one of eight cities to be offered the \"Birth of the New World\" statue, in 1993. The statue, also of Christopher Columbus, was completed in Puerto Rico in 2016 and is the tallest in the United States \u2013 taller than the Statue of Liberty, including its pedestal. At least six U.S. cities, including Columbus, rejected it based on its height and design.\nReligion.\nAccording to the 2019 American Values Atlas, 26% of Columbus metropolitan area residents are unaffiliated with a religious tradition. 17% of area residents identify as White evangelical Protestants, 14% as White mainline Protestants, 11% as Black Protestants, 11% as White Catholics, 5% as Hispanic Catholics, 3% as other nonwhite Catholics, 2% as other nonwhite Protestants and 2% as Mormons. Hindus, Buddhists, Jews and Latino Protestants each made up 1% of the population, while Jehovah's Witnesses, Orthodox Christians, Muslims, Unitarians, and members of New Age or other religions each made up under 0.5% of the population.\nPlaces of worship include Baptist, Evangelical, Greek Orthodox, Latter-day Saints, Lutheran, Presbyterian, Quaker, Roman Catholic, and Unitarian Universalist churches. Columbus also hosts several Islamic mosques, Jewish synagogues, Buddhist centers, Hindu temples and a branch of the International Society for Krishna Consciousness. Religious teaching institutions include the Pontifical College Josephinum and several private schools led by Christian organizations.\nEconomy.\nColumbus has a generally strong and diverse economy based on education, insurance, banking, fashion, defense, aviation, food, logistics, steel, energy, medical research, health care, hospitality, retail and technology. In 2010, it was one of the 10 best big cities in the country, according to Relocate America, a real estate research firm.\nAccording to the Federal Reserve Bank of St. Louis, the GDP of Columbus in 2019 was $134 billion (~$ in ).\nDuring the Great Recession between 2007 and 2009, Columbus's economy was not impacted as much as the rest of the country, due to decades of diversification work by long-time corporate residents, business leaders and political leaders. The administration of former mayor Michael B. Coleman continued this work, although the city faced financial turmoil and had to increase taxes, allegedly due in part to fiscal mismanagement. Because Columbus is the state capital, there is a large government presence in the city. Including city, county, state and federal employers, government jobs provide the largest single source of employment within Columbus.\nIn 2019, the city had six corporations named to the U.S. Fortune 500 list: Alliance Data, Nationwide Mutual Insurance Company, American Electric Power, L Brands, Huntington Bancshares and Cardinal Health in suburban Dublin. Other major employers include schools (e.g., Ohio State University) and hospitals (among others, Wexner Medical Center and Nationwide Children's Hospital, which are among the teaching hospitals of the Ohio State University College of Medicine), high-tech research and development such as the Battelle Memorial Institute, information/library companies such as OCLC and Chemical Abstracts Service, steel processing and pressure cylinder manufacturer Worthington Industries, financial institutions such as JPMorgan Chase and Huntington Bancshares, as well as Owens Corning. Fast-food chains Wendy's and White Castle are also headquartered in the Columbus area. Major foreign corporations operating or with divisions in the city include Germany-based Siemens and Roxane Laboratories, Finland-based Vaisala, Tomasco Mulciber Inc., A Y Manufacturing, as well as Switzerland-based ABB and Mettler Toledo. The city also has a significant fashion and retail presence, home to companies such as Big Lots, L Brands, Abercrombie &amp; Fitch, DSW and Express.\nFood and beverage industry.\nNorth Market, a public market and food hall, is located downtown near the Short North. It is the only remaining public market of Columbus's original four marketplaces.\nNumerous restaurant chains are based in the Columbus area, including Charleys Philly Steaks, Bibibop Asian Grill, Steak Escape, White Castle, Cameron Mitchell Restaurants, Bob Evans Restaurants, Max &amp; Erma's, Damon's Grill, Donatos Pizza and Wendy's. Wendy's, the world's third-largest hamburger fast-food chain, operated its first store downtown as both a museum and a restaurant until March 2007, when the establishment was closed due to low revenue. The company is presently headquartered outside the city in nearby Dublin. Budweiser has a major brewery located on the north side, just south of I-270 and Worthington. Columbus is also home to many local micro breweries and pubs. Asian frozen food manufacturer Kahiki Foods was located on the east side of Columbus, created during the operation of the Kahiki Supper Club restaurant in Columbus. The food company now operates in the suburb of Gahanna and has been owned by the South Korean-based company CJ CheilJedang since 2018. Wasserstrom Company, a major supplier of equipment and supplies for restaurants, is located on the north side.\nArts and culture.\nLandmarks.\nColumbus has over 170 notable buildings listed on the National Register of Historic Places; it also maintains its own register, the Columbus Register of Historic Properties, with 82 entries. The city also maintains four historic districts not listed on its register: German Village, Italian Village, Victorian Village, and the Brewery District.\nConstruction of the Ohio Statehouse began in 1839 on a plot of land donated by four prominent Columbus landowners. This plot formed Capitol Square, which was not part of the city's original layout. Built of Columbus limestone from the Marble Cliff Quarry Co., the Statehouse stands on foundations deep that were laid by prison labor gangs rumored to have been composed largely of masons jailed for minor infractions. It features a central recessed porch with a colonnade of a forthright and primitive Greek Doric mode. A broad and low central pediment supports the windowed astylar drum under an invisibly low saucer dome that lights the interior rotunda. There are several artworks within and outside the building, including the \"William McKinley Monument\" dedicated in 1907. Unlike many U.S. state capitol buildings, the Ohio State Capitol owes little to the architecture of the national Capitol. During the Statehouse's 22-year construction, seven architects were employed. The Statehouse was opened to the legislature and the public in 1857 and completed in 1861, and is located at the intersection of Broad and High streets in downtown Columbus.\nWithin the Driving Park heritage district lies the original home of Eddie Rickenbacker, a World War I fighter pilot ace. Built in 1895, the house was designated a National Historic Landmark in 1976.\nDemolitions and redevelopment.\nDemolition has been a common trend in Columbus for a long period of time, and continues into the present day. Preservationists and the public have sometimes run into conflict with developers hoping to revitalize an area, and historically with the city and state government, which led programs of urban renewal in the 20th century.\nMuseums and public art.\nColumbus has a wide variety of museums and galleries. Its primary art museum is the Columbus Museum of Art, which operates its main location as well as the Pizzuti Collection, featuring contemporary art. The museum, founded in 1878, focuses on European and American art up to early modernism that includes extraordinary examples of Impressionism, German Expressionism and Cubism. Another prominent art museum in the city is the Wexner Center for the Arts, a contemporary art gallery and research facility operated by the Ohio State University.\nThe Ohio History Connection is headquartered in Columbus, with its flagship museum, the Ohio History Center, north of downtown. Adjacent to the museum is Ohio Village, a replica of a village around the time of the American Civil War. The Columbus Historical Society also features historical exhibits, which focus more closely on life in Columbus.\nCOSI is a large science and children's museum in downtown Columbus. The present building, the former Central High School, was completed in November 1999, opposite downtown on the west bank of the Scioto River. In 2009, \"Parents\" magazine named COSI one of the 10 best science centers for families in the country. Other science museums include the Orton Geological Museum and the Museum of Biological Diversity, which are both part of Ohio State University.\nThe Franklin Park Conservatory is the city's botanical garden, which opened in 1895. It features over 400 species of plants in a large Victorian-style glass greenhouse building that includes rain forest, desert and Himalayan mountain biomes. The conservatory is located just east of Downtown in Franklin Park\nBiographical museums include the Thurber House (documenting the life of cartoonist James Thurber), the Jack Nicklaus Museum (documenting the golfer's career, located on the OSU campus) and the Kelton House Museum and Garden, the latter of which being a historic house museum memorializing three generations of the Kelton family, the house's use as a documented station on the Underground Railroad, and overall Victorian life.\nThe National Veterans Memorial and Museum, which opened in 2018, focuses on the personal stories of military veterans throughout U.S. history. The museum replaced the Franklin County Veterans Memorial, which opened in 1955.\nOther notable museums in the city include the Central Ohio Fire Museum, Billy Ireland Cartoon Library &amp; Museum and the Ohio Craft Museum.\nPerforming arts.\nColumbus is the home of many performing arts institutions including the Columbus Symphony Orchestra, Opera Columbus, BalletMet Columbus, the ProMusica Chamber Orchestra, CATCO, Columbus Children's Theatre, Shadowbox Live, and the Columbus Jazz Orchestra. Throughout the summer, the Actors' Theatre of Columbus offers free performances of Shakespearean plays in an open-air amphitheater in Schiller Park in historic German Village.\nThe Columbus Youth Ballet Academy was founded in the 1980s by ballerina and artistic director Shir Lee Wu, a discovery of Martha Graham. Wu is now the artistic director of the Columbus City Ballet School.\nColumbus has several large concert venues, including the Nationwide Arena, Value City Arena, Express Live!, Mershon Auditorium and the Newport Music Hall.\nIn May 2009, the Lincoln Theatre, formerly a center for Black culture in Columbus, reopened after an extensive restoration. Not far from the Lincoln Theatre is the King Arts Complex, which hosts a variety of cultural events. The city also has several theaters downtown, including the historic Palace Theatre, the Ohio Theatre and the Southern Theatre. Broadway Across America often presents touring Broadway musicals in these larger venues. The Vern Riffe Center for Government and the Arts houses the Capitol Theatre and three smaller studio theaters, providing a home for resident performing arts companies.\nFilm.\nMovies filmed in the Columbus metropolitan area include \"Teachers\" in 1984, \"Tango &amp; Cash\" in 1989, \"Little Man Tate\" in 1991, \"Air Force One\" in 1997, \"Traffic\" in 2000, \"Speak\" in 2004, \"Bubble\" in 2005, \"Liberal Arts\" in 2012, \"Parker\" in 2013, and \"I Am Wrath\" in 2016, \"Aftermath\" in 2017, \"They/Them/Us\" in 2021, and \"Bones and All\" in 2022. The 2018 film \"Ready Player One\" is set in Columbus, though not filmed in the city.\nSports.\nProfessional teams.\nColumbus hosts two major league professional sports teams: the Columbus Blue Jackets of the National Hockey League (NHL), which play at Nationwide Arena, and the Columbus Crew of Major League Soccer (MLS), which play at Lower.com Field. The Crew previously played at Historic Crew Stadium, the first soccer-specific stadium built in the United States for a Major League Soccer team. The Crew were one of the original members of MLS and won their first MLS Cup in 2008, a second title in 2020, and a third title in 2023. The Columbus Crew moved into Lower.com Field in the summer of 2021, which will also feature a mixed-use development site named Confluence Village.\nThe Columbus Clippers, the International League affiliate of the Cleveland Guardians, play in Huntington Park, which opened in 2009.\nThe city was home to the Panhandles/Tigers football team from 1901 to 1926; they are credited with playing in the first NFL game against another NFL opponent. In the late 1990s, the Columbus Quest won the only two championships during American Basketball League's two-and-a-half season existence.\nThe Ohio Aviators were based in Obetz, Ohio, and began play in the only PRO Rugby season before the league folded.\nSince 2023, Columbus has been home to the Columbus Fury women's professional volleyball team, one of seven teams to launch with the Pro Volleyball Federation. The team plays home games at Nationwide Arena.\nOhio State Buckeyes.\nColumbus is home to one of the nation's most competitive intercollegiate programs, the Ohio State Buckeyes of Ohio State University. The program has placed in the top 10 final standings of the Director's Cup five times since 2000\u20132001, including No. 3 for the 2002\u20132003 season and No. 4 for the 2003\u20132004 season. The university funds 36 varsity teams, consisting of 17 male, 16 female and three co-educational teams. In 2007\u20132008 and 2008\u20132009, the program generated the second-most revenue for college programs behind the Texas Longhorns of The University of Texas at Austin.\nThe Ohio State Buckeyes are a member of the NCAA's Big Ten Conference, and their football team plays home games at Ohio Stadium. The Ohio State\u2013Michigan football game (known colloquially as \"The Game\") is the final game of the regular season and is played in November each year, alternating between Columbus and Ann Arbor, Michigan. In 2000, ESPN ranked the Ohio State\u2013Michigan game as the greatest rivalry in North American sports. Moreover, \"Buckeye fever\" permeates Columbus culture year-round and forms a major part of Columbus's cultural identity. Former New York Yankees owner George Steinbrenner, an Ohio native who received a master's degree from Ohio State and coached in Columbus, was an Ohio State football fan and major donor to the university who contributed to the construction of the band facility at the renovated Ohio Stadium, which bears his family's name.\nDuring the winter months, the Buckeyes basketball and hockey teams are also major sporting attractions.\nOther sports.\nColumbus has a long history in motorsports, hosting the world's first 24-hour car race at the Columbus Driving Park in 1905, which was organized by the Columbus Auto Club. The Columbus Motor Speedway was built in 1945 and held its first motorcycle race in 1946. In 2010, the Ohio State University student-built Buckeye Bullet 2, a fuel-cell vehicle, set an FIA world speed record for electric vehicles in reaching 303.025\u00a0mph, eclipsing the previous record of 302.877\u00a0mph.\nThe annual All American Quarter Horse Congress, the world's largest single-breed horse show, attracts approximately 500,000 visitors to the Ohio Expo Center each October.\nColumbus hosts the annual Arnold Sports Festival. Hosted by Arnold Schwarzenegger, the event has grown to eight Olympic sports and 22,000 athletes competing in 80 events.\nWestside Barbell, a world-renowned powerlifting gym, is located in Columbus. Its founder, Louie Simmons, is known for his popularization of the \"Conjugate Method,\" while he is also credited with inventing training machines for reverse hyper-extensions and belt squats. Westside Barbell is known for producing multiple world record holders in powerlifting.\nThe Columbus Bullies were two-time champions of the American Football League (1940\u20131941). The Columbus Thunderbolts were formed in 1991 for the Arena Football League, and then relocated to Cleveland as the Cleveland Thunderbolts; the Columbus Destroyers were the next team of the AFL, playing from 2004 until the league's demise in 2008 and returned for single season in 2019 until the league folded a second time.\nOhio Roller Derby (formerly Ohio Roller Girls) was founded in Columbus in 2005 and still competes internationally in Women's Flat Track Derby Association play. The team is regularly ranked in the top 60 internationally.\nParks and attractions.\nColumbus's Recreation and Parks Department oversees about 370 city parks. Also in the area are 19 regional parks and the Metro Parks, which are part of the Columbus and Franklin County Metropolitan Park District.\nThese parks include Clintonville's Whetstone Park and the Columbus Park of Roses, a rose garden. The Chadwick Arboretum on Ohio State's campus features a large and varied collection of plants, while its Olentangy River Wetland Research Park is an experimental wetland open to the public. Downtown, the painting \"A Sunday Afternoon on the Island of La Grande Jatte\" is represented in topiary at Columbus's Topiary Park. Also near downtown, the Scioto Audubon Metro Park on the Whittier Peninsula opened in 2009 and includes a large Audubon nature center focused on the birdwatching the area is known for.\nThe Columbus Zoo and Aquarium's collections include lowland gorillas, polar bears, manatees, Siberian tigers, cheetahs and kangaroos. Also in the zoo complex is the Zoombezi Bay water park and amusement park.\nFairs and festivals.\nAnnual festivities in Columbus include the Ohio State Fair \u2013 one of the largest state fairs in the country \u2013 as well as the Columbus Arts Festival and the Jazz &amp; Rib Fest, both of which occur on the downtown riverfront.\nIn mid-May from 2007 to 2018, Columbus was home to Rock on the Range, which was held at Historic Crew Stadium and marketed as America's biggest rock festival. The festival, which took place on a Friday, Saturday and Sunday, has hosted Metallica, Red Hot Chili Peppers, Slipknot and other notable bands. In May 2019, it was officially replaced by the Sonic Temple Art &amp; Music Festival.\nDuring the first weekend in June, the bars of Columbus's North Market District host the Park Street Festival, which attracts thousands of visitors to a massive party in bars and on the street. June's second-to-last weekend sees one of the Midwest's largest gay pride parades, Columbus Pride, reflecting the city's sizable gay population. During the last weekend of June, Goodale Park hosts ComFest (short for \"Community Festival\"), an immense three-day music festival marketed as the largest non-commercial festival in the U.S., with art vendors, live music on multiple stages, hundreds of local social and political organizations, body painting and beer.\nThe city's largest dining event, Restaurant Week Columbus, is held twice a year in mid-January and mid-July. In 2010, more than 40,000 diners went to 40 participating restaurants, and $5,000 (~$ in ) was donated the Mid-Ohio Foodbank on behalf of sponsors and participating restaurants.\nAround the Fourth of July, Columbus hosts Red, White &amp; Boom! on the Scioto riverfront downtown, attracting crowds of over 500,000 people and featuring the largest fireworks display in Ohio.\nThe Short North is host to the monthly Gallery Hop, which attracts hundreds to the neighborhood's art galleries (which all open their doors to the public until late at night) and street musicians. The Hilltop Bean Dinner is an annual event held on Columbus's West Side that celebrates the city's Civil War heritage near the historic Camp Chase Cemetery. At the end of September, German Village throws an annual Oktoberfest celebration that features German food, beer, music and crafts.\nColumbus also hosts many conventions in the Greater Columbus Convention Center, a large convention center on the north edge of downtown. Completed in 1993, the convention center was designed by architect Peter Eisenman, who also designed the Wexner Center.\nShopping.\nBoth of the metropolitan area's major shopping centers are located in Columbus: Easton Town Center and Polaris Fashion Place.\nDeveloper Richard E. Jacobs built the area's first three major shopping malls in the 1960s: Westland, Northland and Eastland. Near Northland Mall was The Continent, an open-air mall in the Northland area, mostly vacant and pending redevelopment. Columbus City Center was built downtown in 1988, alongside the first location of Lazarus; this mall closed in 2009 and was demolished in 2011. Easton Town Center was built in 1999 and Polaris Fashion Place in 2001.\nEnvironment.\nThe City of Columbus has focused on reducing its environmental impact and carbon footprint. In 2020, a citywide ballot measure was approved, giving Columbus an electricity aggregation plan which will supply it with 100% renewable energy by the start of 2023. Its vendor, AEP Energy, plans to construct new wind and solar farms in Ohio to help supply the electricity.\nThe largest sources of pollution in the county, as of 2019, are Ohio State University's McCracken Power Plant, the landfill operated by the Solid Waste Authority of Central Ohio (SWACO) and the Anheuser-Busch Columbus Brewery. Anheuser-Busch has a company-wide goal of reducing emissions by 25% by 2025. Ohio State plans to construct a new heat and power plant, also powered by fossil fuels, but set to reduce emissions by about 30%. SWACO manages to capture 75% of its methane emissions to use in producing energy, and is looking to reduce emissions further.\nGovernment.\nMayor and city council.\nThe city is administered by a mayor and a nine-member unicameral council elected in two classes every two years to four-year terms at large. Columbus is the largest city in the United States that elects its city council at large as opposed to districts. The mayor appoints the director of safety and the director of public service. The people elect the auditor, municipal court clerk, municipal court judges and city attorney. A charter commission, elected in 1913, submitted a new charter in May 1914, offering a modified federal form, with a number of progressive features, such as nonpartisan ballot, preferential voting, recall of elected officials, the referendum and a small council elected at large. The charter was adopted, effective January 1, 1916. Andrew Ginther has been the mayor of Columbus since 2016.\nGovernment offices.\nAs Ohio's capital and the county seat, Columbus hosts numerous federal, state, county and city government offices and courts.\nFederal offices include the Joseph P. Kinneary U.S. Courthouse, one of several courts for the District Court for the Southern District of Ohio, after moving from 121 E. State St. in 1934. Another federal office, the John W. Bricker Federal Building, has offices for U.S. Senator Sherrod Brown, as well as for the Internal Revenue Service, the Social Security Administration and the Departments of Housing &amp; Urban Development and Agriculture.\nThe State of Ohio's capitol building, the Ohio Statehouse, is located in the center of downtown on Capitol Square. It houses the Ohio House of Representatives and Ohio Senate. It also contains the ceremonial offices of the governor, lieutenant governor, state treasurer and state auditor. The Supreme Court, Court of Claims and Judicial Conference are located in the Thomas J. Moyer Ohio Judicial Center downtown by the Scioto River. The building, built in 1933 to house 10 state agencies along with the State Library of Ohio, became the Supreme Court after extensive renovations from 2001 to 2004.\nFranklin County operates the Franklin County Government Center, a complex at the southern end of downtown Columbus. The center includes the county's municipal court, common pleas court, correctional center, juvenile detention center and sheriff's office.\nNear City Hall, the Michael B. Coleman Government Center holds offices for the departments of building and zoning services, public service, development and public utilities. Also nearby is 77 North Front Street, which holds Columbus's city attorney office, income-tax division, public safety, human resources, civil service and purchasing departments. The structure, built in 1929, was the police headquarters until 1991, and was then dormant until it was given a $34 million renovation from 2011 to 2013.\nEmergency services and homeland security.\nMunicipal police duties are performed by the Columbus Division of Police, while emergency medical services (EMS) and fire protection are through the Columbus Division of Fire.\nOhio Homeland Security operates the Strategic Analysis and Information Center (SAIC) fusion center in Columbus's Hilltop neighborhood. The facility is the state's primary public intelligence hub and one of the few in the country that uses state, local, federal and private resources.\nSocial services and homelessness.\nColumbus has a history of governmental and nonprofit support for low-income residents and the homeless. Nevertheless, the homelessness rate has steadily risen since at least 2007. Poverty and differences in quality of life have grown, as well; Columbus was noted as the second-most economically segregated large metropolitan area in 2015, in a study by the University of Toronto. It also ranked 45th of the 50 largest metropolitan areas in terms of social mobility, according to a 2015 Harvard University study.\nEducation.\nColleges and universities.\nColumbus is the home of two public colleges: the Ohio State University, one of the largest college campuses in the United States, and Columbus State Community College. In 2009, Ohio State University was ranked No. 19 in the country by \"U.S. News &amp; World Report\" on its list of best public universities, and No. 56 overall, scoring in the first tier of schools nationally. Some of Ohio State's graduate school programs placed in the top 5, including No. 5 for both best veterinary programs and best pharmacy programs. The specialty graduate programs of social psychology was ranked No. 2, dispute resolution was No. 5, vocational education was No. 2, and elementary education, secondary teacher education, administration/supervision was No. 5.\nPrivate institutions in Columbus include Capital University Law School, the Columbus College of Art and Design, Fortis College, DeVry University, Ohio Business College, Miami-Jacobs Career College, Ohio Institute of Health Careers, Bradford School and Franklin University, as well as the religious schools Bexley Hall Episcopal Seminary, Mount Carmel College of Nursing, Ohio Dominican University, Pontifical College Josephinum and Trinity Lutheran Seminary. Three major suburban schools also have an influence on Columbus's educational landscape: Bexley's Capital University, Westerville's Otterbein University and Delaware's Ohio Wesleyan University.\nPrimary and secondary schools.\nColumbus City Schools (CCS) is the largest district in Ohio, with 55,000 pupils. CCS operates 142 elementary, middle and high schools, including a number of magnet schools (which are referred to as alternative schools within the school system).\nThe suburbs operate their own districts, typically serving students in one or more townships, with districts sometimes crossing municipal boundaries. The Roman Catholic Diocese of Columbus also operates several parochial elementary and high schools. The area's second-largest school district is South-Western City Schools, which encompasses southwestern Franklin County, including a slice of Columbus itself. Other portions of Columbus are zoned to the Dublin, Hilliard, New Albany-Plain, Westerville and Worthington school districts.\nThere are also several private schools in the area, such as St. Paul's Lutheran School, a K-8 Christian school of the Wisconsin Evangelical Lutheran Synod in Columbus.\nSome sources determine that the first kindergarten in the United States was established here by Louisa Frankenberg, a former student of Friedrich Fr\u00f6bel. Frankenberg immigrated to the city in 1838 and opened her kindergarten in the German Village neighborhood in that year. The school did not work out, so she returned to Germany in 1840. In 1858, Frankenberg returned to Columbus and established another early kindergarten in the city. Frankenberg is often overlooked, with Margarethe Schurz instead given credit for her \"First Kindergarten\" she operated for two years.\nIn addition, Indianola Junior High School (now the Graham Elementary and Middle School) became the nation's first junior high school in 1909, helping to bridge the difficult transition from elementary to high school at a time when only 48% of students continued their education after the ninth grade.\nLibraries.\nThe Columbus Metropolitan Library (CML) has served central Ohio residents since 1873. The system has 23 locations throughout Central Ohio, with a total collection of 3 million items. This library is one of the country's most-used library systems and is consistently among the top-ranked large city libraries according to Hennen's American Public Library Ratings. CML was rated the No. 1 library system in the nation in 1999, 2005 and 2008. It has been in the top four every year since 1999, when the rankings were first published in the \"American Libraries\" magazine, often challenging upstate neighbor Cuyahoga County Public Library for the top spot.\nWeekend education.\nThe classes of the Columbus Japanese Language School, a weekend Japanese school, are held in a facility from the school district in Marysville, while the school office is in Worthington. Previously it held classes at facilities in the city of Columbus.\nMedia.\nSeveral weekly and daily newspapers serve Columbus and Central Ohio. The major daily newspaper in Columbus is \"The Columbus Dispatch\". There are also neighborhood- or suburb-specific papers, such as the Dispatch Printing Company's \"ThisWeek Community News\", the \"Columbus Messenger\", the \"Clintonville Spotlight\" and the \"Short North Gazette\". \"The Lantern\" and \"1870\" serve the Ohio State University community. Alternative arts, culture or politics-oriented papers include \"ALIVE\" (formerly the independent \"Columbus Alive\" and now owned by the \"Columbus Dispatch\"), \"Columbus Free Press\" and \"Columbus Underground\" (digital-only). The \"Columbus Magazine\", \"CityScene\", \"614 Magazine\" and \"Columbus Monthly\" are the city's magazines.\nColumbus is the base for 12 television stations and is the 32nd-largest television market as of September 24, 2016. Columbus is also home to the 36th-largest radio market.\nInfrastructure.\nHealthcare.\nNumerous medical systems operate in Columbus and Central Ohio. These include OhioHealth, which has three hospitals in the city proper: Grant Medical Center, Riverside Methodist Hospital, and Doctors Hospital; Mount Carmel Health System, which has one hospital among other facilities; the Ohio State University Wexner Medical Center, which has a primary hospital complex and an east campus in Columbus; and Nationwide Children's Hospital, which is an independently operated hospital for pediatric health care. Hospitals in Central Ohio are ranked favorably by the \"U.S. News &amp; World Report\", where numerous hospitals are ranked as among the best in particular fields in the United States. Nationwide Children's is regarded as among the top 10 children's hospitals in the country, according to the report.\nUtilities.\nNumerous utility companies operate in Central Ohio. Within Columbus, power is sourced from Columbus Southern Power, an American Electric Power subsidiary. Natural gas is provided by Columbia Gas of Ohio, while water is sourced from the City of Columbus Division of Water.\nTransportation.\nLocal roads, grid and address system.\nThe city's two main corridors since its founding are Broad and High Streets. They both traverse beyond the extent of the city; High Street is the longest in Columbus, running (23.4 across the county), while Broad Street is longer across the county, at .\nThe city's street plan originates downtown and extends into the old-growth neighborhoods, following a grid pattern with the intersection of High Street (running north\u2013south) and Broad Street (running east\u2013west) at its center. North\u2013south streets run 12 degrees west of due north, parallel to High Street; the avenues (vis. Fifth Avenue, Sixth Avenue, Seventh Avenue, and so on) run 12 degrees off from east\u2013west.\nThe address system begins its numbering at the intersection of Broad and High, with numbers increasing in magnitude with distance from Broad or High, as well as cardinal directions used alongside street names. Numbered avenues begin with First Avenue, about north of Broad Street, and increase in number as one progresses northward. Numbered streets begin with Second Street, which is two blocks west of High Street, and Third Street, which is a block east of High Street, then progress eastward from there. Even-numbered addresses are on the north and east sides of streets, putting odd addresses on the south and west sides of streets. A difference of 700 house numbers means a distance of about (along the same street).\nOther major, local roads in Columbus include Main Street, Morse Road, Dublin-Granville Road (SR-161), Cleveland Avenue/Westerville Road (SR-3), Olentangy River Road, Riverside Drive, Sunbury Road, Fifth Avenue and Livingston Avenue.\nHighways.\nColumbus is bisected by two major Interstate Highways: Interstate 70 running east\u2013west and Interstate 71 running north to roughly southwest. They combine downtown for about in an area locally known as \"The Split\", which is a major traffic congestion point, especially during rush hour. U.S. Route 40, originally known as the National Road, runs east\u2013west through Columbus, comprising Main Street to the east of downtown and Broad Street to the west. U.S. Route 23 runs roughly north\u2013south, while U.S. Route 33 runs northwest-to-southeast. The Interstate 270 Outerbelt encircles most of the city, while the newly redesigned Innerbelt consists of the Interstate 670 spur on the north side (which continues to the east past the Airport and to the west where it merges with I-70), State Route 315 on the west side, the I-70/71 split on the south side and I-71 on the east. Due to its central location within Ohio and abundance of outbound roadways, nearly all of the state's destinations are within a two- or three-hour drive of Columbus.\nBridges.\nThe Columbus riverfront hosts several bridges. The Discovery Bridge connects downtown to Franklinton across Broad Street. The bridge opened in 1992, replacing a 1921 concrete arch bridge; the first bridge at the site was built in 1816. The Main Street Bridge opened on July 30, 2010. The bridge has three lanes for vehicular traffic (one westbound and two eastbound) and another separated lane for pedestrians and bikes. The Rich Street Bridge opened in July 2012 adjacent to the Main Street Bridge, connecting Rich Street on the east side of the river with Town Street on the west. The Lane Avenue Bridge is a cable-stayed bridge that opened on November 14, 2003, in the University District. The bridge spans the Olentangy River with three lanes of traffic each way.\nAirports.\nThe city's primary airport, John Glenn Columbus International Airport, is on the city's east side. Formerly known as Port Columbus, John Glenn provides service to Toronto, Ontario, Canada, and Cancun, Mexico (on a seasonal basis), as well as to most domestic destinations, including all the major hubs along with San Francisco, Salt Lake City and Seattle. The airport was a hub for discount carrier Skybus Airlines and continues to be home to NetJets, the world's largest fractional ownership air carrier. According to a 2005 market survey, John Glenn Columbus International Airport attracts about 50% of its passengers from outside of its radius primary service region. It is the 52nd-busiest airport in the United States by total passenger boardings.\nRickenbacker International Airport, in southern Franklin County, is a major cargo facility that is used by the Ohio Air National Guard. Allegiant Air offers nonstop service from Rickenbacker to Florida destinations. Ohio State University Don Scott Airport and Bolton Field are other large general-aviation facilities in the Columbus area.\nAviation history.\nIn 1907, 14-year-old Cromwell Dixon built the \"SkyCycle,\" a pedal-powered blimp, which he flew at Driving Park. Three years later, one of the Wright brothers' exhibition pilots, Phillip Parmalee, conducted the world's first commercial cargo flight when he flew two packages containing 88 kilograms of silk from Dayton to Columbus in a Wright Model B.\nMilitary aviators from Columbus distinguished themselves during World War I. Six Columbus pilots, led by top ace Eddie Rickenbacker, achieved 42 \"kills\" \u2013 a full 10% of all US aerial victories in the war, and more than the aviators of any other American city.\nAfter the war, Port Columbus Airport (now known as John Glenn Columbus International Airport) became the axis of a coordinated rail-to-air transcontinental system that moved passengers from the East Coast to the West. TAT, which later became TWA, provided commercial service, following Charles Lindbergh's promotion of Columbus to the nation for such a hub. Following the failure of a bond levy in 1927 to build the airport, Lindbergh campaigned in the city in 1928, and the next bond levy passed that year. On July 8, 1929, the airport opened for business with the inaugural TAT westbound flight from Columbus to Waynoka, Oklahoma. Among the 19 passengers on that flight was Amelia Earhart, with Henry Ford and Harvey Firestone attending the opening ceremonies.\nIn 1964, Ohio native Geraldine Fredritz Mock became the first woman to fly solo around the world, leaving from Columbus and piloting the \"Spirit of Columbus\". Her flight lasted nearly a month and set a record for speed for planes under .\nPublic transit.\nColumbus maintains a widespread municipal bus service called the Central Ohio Transit Authority (COTA). The service operates 41 routes with a fleet of 440 buses, serving approximately 19 million passengers per year. COTA operates 23 regular fixed-service routes, 14 express services, a bus rapid transit route, a free downtown circulator, night service, an airport connector and other services. LinkUS, an initiative between COTA, the city, and the Mid-Ohio Regional Planning Commission, is planning to add more rapid transit to Columbus, with three proposed corridors operating by 2030, and potentially a total of five by 2050.\nIntercity bus service is provided at the Columbus Bus Station by Greyhound, Barons Bus Lines, Miller Transportation, GoBus and other carriers.\nColumbus does not have passenger rail service. The city's major train station, Union Station, was a stop along Amtrak's National Limited train service until 1977 and was razed in 1979, and the Greater Columbus Convention Center now stands in its place. Until Amtrak's founding in 1971, the Penn Central ran the \"Cincinnati Limited\" to Cincinnati to the southwest (in prior years the train continued to New York City to the east); the \"Ohio State Limited\" between Cincinnati and Cleveland, with Union Station serving as a major intermediate stop (the train going unnamed between 1967 and 1971); and the \"Spirit of St. Louis,\" which ran between St. Louis and New York City until 1971. The station was also a stop along the Pennsylvania Railroad, the New York Central Railroad, the Chesapeake and Ohio Railway, the Baltimore and Ohio Railroad, the Norfolk and Western Railway, the Cleveland, Columbus and Cincinnati Railroad, and the Pittsburgh, Cincinnati, Chicago and St. Louis Railroad. As the city lacks local, commuter or intercity trains, Columbus is now the largest city and metropolitan area in the U.S. without any passenger rail service. Numerous proposals to return rail service have been introduced; currently Amtrak plans to restore service to Columbus by 2035.\nCycling network.\nCycling as transportation is steadily increasing in Columbus with its relatively flat terrain, intact urban neighborhoods, large student population and off-road bike paths. The city has put forth the 2012 Bicentennial Bikeways Plan, as well as a move toward a Complete Streets policy. Grassroots efforts such as Bike to Work Week, Consider Biking, Yay Bikes, Third Hand Bicycle Co-op, Franklinton Cycleworks and \"Cranksters\", a local radio program focused on urban cycling, have contributed to cycling as transportation.\nColumbus also hosts urban cycling \"off-shots\" with messenger-style \"alleycat\" races, as well as unorganized group rides, a monthly Critical Mass ride, bicycle polo, art showings, movie nights and a variety of bicycle-friendly businesses and events throughout the year. All this activity occurs despite Columbus's frequently inclement weather.\nThe Main Street Bridge, opened in 2010, features a dedicated bike and pedestrian lane separated from traffic.\nThe city has its own public bicycle system. CoGo Bike Share has a network of about 600 bicycles and 80 docking stations. PBSC Urban Solutions, a company based in Canada, supplies technology and equipment. Bird electric scooters have also been introduced.\nModal share.\nThe city of Columbus has a higher-than-average percentage of households without a car. In 2015, 9.8% of Columbus households lacked a car, a number that fell slightly to 9.4% in 2016. The national average was 8.7% in 2016. Columbus averaged 1.55 cars per household in 2016, compared to a national average of 1.8.\nSister cities.\nColumbus has 10 sister cities as designated by Sister Cities International. Columbus established its first sister city relationship in 1955 with Genoa, Italy. To commemorate this relationship, Columbus received as a gift from the people of Genoa, a bronze statue of Christopher Columbus. The statue overlooked Broad Street in front of Columbus City Hall from 1955 to 2020; it was removed during the George Floyd protests.\nList of sister cities:"}
{"id": "5951", "revid": "6165960", "url": "https://en.wikipedia.org/wiki?curid=5951", "title": "Cleveland", "text": "Cleveland is a city in the U.S. state of Ohio and the county seat of Cuyahoga County. Located along the southern shore of Lake Erie, it is situated across the Canada\u2013United States maritime border and lies approximately west of Pennsylvania. Cleveland is the most populous city on Lake Erie, the second-most populous city in Ohio, and the 54th-most populous city in the U.S. with a population of 372,624 in 2020. The city anchors the Cleveland metropolitan area, the 33rd-largest in the U.S. at 2.18 million residents, as well as the larger Cleveland\u2013Akron\u2013Canton combined statistical area with 3.63 million residents.\nCleveland was founded in 1796 near the mouth of the Cuyahoga River as part of the Connecticut Western Reserve in modern-day Northeast Ohio by General Moses Cleaveland, after whom the city was named. The city's location on the river and the lake shore allowed it to grow into a major commercial and industrial metropolis by the late 19th century, attracting large numbers of immigrants and migrants. It was among the top 10 largest U.S. cities by population for much of the 20th century, a period that saw the development of the city's cultural institutions. By the 1960s, Cleveland's economy began to slow down as manufacturing declined and suburbanization occurred.\nCleveland is a port city, connected to the Atlantic Ocean via the Saint Lawrence Seaway. Its economy relies on diverse sectors that include higher education, manufacturing, financial services, healthcare, and biomedicals. The city serves as the headquarters of the Federal Reserve Bank of Cleveland, as well as several major companies. The GDP for the Greater Cleveland MSA was US$138.3 billion in 2022. Combined with the Akron MSA, the eight-county Cleveland\u2013Akron metropolitan economy was $176 billion in 2022, the largest in Ohio.\nDesignated as a global city by the Globalization and World Cities Research Network, Cleveland is home to several major cultural institutions, including the Cleveland Museum of Art, the Cleveland Museum of Natural History, the Cleveland Orchestra, the Cleveland Public Library, Playhouse Square, and the Rock and Roll Hall of Fame, as well as Case Western Reserve University. Known as \"The Forest City\" among many other nicknames, Cleveland serves as the center of the Cleveland Metroparks nature reserve system. The city's major league professional sports teams include the Cleveland Browns (football; NFL), the Cleveland Cavaliers (basketball; NBA), and the Cleveland Guardians (baseball; MLB).\nHistory.\nEstablishment.\nCleveland was established on July 22, 1796, by surveyors of the Connecticut Land Company when they laid out Connecticut's Western Reserve into townships and a capital city. They named the settlement \"Cleaveland\" after their leader, General Moses Cleaveland, a veteran of the American Revolutionary War. Cleaveland oversaw the New England\u2013style design of the plan for what would become the modern downtown area, centered on Public Square, before returning to Connecticut, never again to visit Ohio. The town's name was often shortened to \"Cleveland\", even by Cleaveland's original surveyors. A common myth emerged that the spelling was altered by \"The Cleveland Advertiser\" in order to fit the name on the newspaper's masthead.\nThe first permanent European settler in Cleveland was Lorenzo Carter, who built a cabin on the banks of the Cuyahoga River. The emerging community served as an important supply post for the U.S. during the Battle of Lake Erie in the War of 1812. Locals adopted Commodore Oliver Hazard Perry as a civic hero and erected a monument in his honor decades later. Largely through the efforts of the settlement's first lawyer Alfred Kelley, the village of Cleveland was incorporated on December 23, 1814.\nDespite the nearby swampy lowlands and harsh winters, the town's waterfront location proved advantageous, giving it access to Great Lakes trade. It grew rapidly after the 1832 completion of the Ohio and Erie Canal. This key link between the Ohio River and the Great Lakes connected Cleveland to the Atlantic Ocean via the Erie Canal and Hudson River, and later via the Saint Lawrence Seaway. The town's growth continued with added railroad links. In 1836, Cleveland, then only on the eastern banks of the Cuyahoga, was officially incorporated as a city, and John W. Willey was elected its first mayor. That same year, it nearly erupted into open warfare with neighboring Ohio City over a bridge connecting the two communities. Ohio City remained an independent municipality until its annexation by Cleveland in 1854.\nA center of abolitionist activity, Cleveland (code-named \"Station Hope\") was a major stop on the Underground Railroad for escaped African American slaves en route to Canada. The city also served as an important center for the Union during the American Civil War. Decades later, in July 1894, the wartime contributions of those serving the Union from Cleveland and Cuyahoga County would be honored with the Soldiers' and Sailors' Monument on Public Square.\nGrowth and expansion.\nThe Civil War vaulted Cleveland into the first rank of American manufacturing cities and fueled unprecedented growth. Its prime geographic location as a transportation hub on the Great Lakes played an important role in its development as an industrial and commercial center. In 1870, John D. Rockefeller founded Standard Oil in Cleveland, and in 1885, he moved its headquarters to New York City, which had become a center of finance and business.\nCleveland's economic growth and industrial jobs attracted large waves of immigrants from Southern and Eastern Europe as well as Ireland. Urban growth was accompanied by significant strikes and labor unrest, as workers demanded better wages and working conditions. Between 1881 and 1886, 70 to 80% of strikes were successful in improving labor conditions in Cleveland. The Cleveland Streetcar Strike of 1899 was one of the more violent instances of labor strife in the city during this period.\nBy 1910, Cleveland had become known as the \"Sixth City\" due to its status at the time as the sixth-largest U.S. city. Its automotive companies included Peerless, Chandler, and Winton, maker of the first car driven across the U.S. Other manufacturing industries in Cleveland included steam cars produced by White and electric cars produced by Baker. The city counted major Progressive Era politicians among its leaders, most prominently the populist Mayor Tom L. Johnson, who was responsible for the development of the Cleveland Mall Plan. The era of the City Beautiful movement in Cleveland architecture saw wealthy patrons support the establishment of the city's major cultural institutions. The most prominent among them were the Cleveland Museum of Art, which opened in 1916, and the Cleveland Orchestra, established in 1918.\nIn addition to the large immigrant population, African American migrants from the rural South arrived in Cleveland (among other Northeastern and Midwestern cities) as part of the Great Migration for jobs, constitutional rights, and relief from racial discrimination. By 1920, the year in which the Cleveland Indians won their first World Series championship, Cleveland had grown into a densely-populated metropolis of 796,841, making it the fifth-largest city in the nation, with a foreign-born population of 30%.\nAt this time, Cleveland saw the rise of radical labor movements, most prominently the Industrial Workers of the World (IWW), in response to the conditions of the largely immigrant and migrant workers. In 1919, the city attracted national attention amid the First Red Scare for the Cleveland May Day Riots, in which local socialist and IWW demonstrators clashed with anti-socialists. The riots occurred during the broader strike wave that swept the U.S. that year.\nCleveland's population continued to grow throughout the Roaring Twenties. The decade saw the establishment of the city's Playhouse Square, and the rise of the risqu\u00e9 Short Vincent. The Bal-Masque balls of the avant-garde Kokoon Arts Club scandalized the city. Jazz came to prominence in Cleveland during this period. Prohibition first took effect in Ohio in May 1919 (although it was not well-enforced in Cleveland), became law with the Volstead Act in 1920, and was eventually repealed nationally by Congress in 1933. The ban on alcohol led to the rise of speakeasies throughout the city and organized crime gangs, such as the Mayfield Road Mob, who smuggled bootleg liquor across Lake Erie from Canada into Cleveland.\nThe era of the flapper marked the beginning of the golden age in Downtown Cleveland retail, centered on major department stores Higbee's, Bailey's, the May Company, Taylor's, Halle's, and Sterling Lindner Davis, which collectively represented one of the largest and most fashionable shopping districts in the country, often compared to New York's Fifth Avenue. In 1929, Cleveland hosted the first of many National Air Races, and Amelia Earhart flew to the city from Santa Monica, California in the Women's Air Derby. The Van Sweringen brothers commenced construction of the Terminal Tower skyscraper in 1926 and oversaw it to completion in 1927. By the time the building was dedicated as part of Cleveland Union Terminal in 1930, the city had a population of over 900,000.\nCleveland was hit hard by the Wall Street Crash of 1929 and the subsequent Great Depression. A center of union activity, the city saw significant labor struggles in this period, including strikes by workers against Fisher Body in 1936 and against Republic Steel in 1937. The city was also aided by major federal works projects sponsored by President Franklin D. Roosevelt's New Deal. In commemoration of the centennial of Cleveland's incorporation as a city, the Great Lakes Exposition debuted in June 1936 at the city's North Coast Harbor, along the Lake Erie shore north of downtown. Conceived by Cleveland's business leaders as a way to revitalize the city during the Depression, it drew four million visitors in its first season, and seven million by the end of its second and final season in September 1937.\nOn December 7, 1941, Imperial Japan attacked Pearl Harbor and declared war on the U.S. Two of the victims of the attack were Cleveland natives \u2013 Rear Admiral Isaac C. Kidd and ensign William Halloran. The attack signaled America's entry into World War II. A major hub of the \"Arsenal of Democracy\", Cleveland under Mayor Frank Lausche contributed massively to the U.S. war effort as the fifth largest manufacturing center in the nation. During his tenure, Lausche also oversaw the establishment of the Cleveland Transit System, the predecessor to the Greater Cleveland Regional Transit Authority.\nLate 20th and early 21st centuries.\nAfter the war, Cleveland initially experienced an economic boom, and businesses declared the city to be the \"best location in the nation\". In 1949, the city was named an All-America City for the first time, and in 1950, its population reached 914,808. In sports, the Indians won the 1948 World Series, the hockey team, the Barons, became champions of the American Hockey League, and the Browns dominated professional football in the 1950s. As a result, along with track and boxing champions produced, Cleveland was declared the \"City of Champions\" in sports at this time. Additionally, the 1950s saw the rising popularity of a new music genre that local WJW (AM) disc jockey Alan Freed dubbed \"rock and roll\".\nHowever, by the 1960s, Cleveland's economy began to slow down, and residents increasingly sought new housing in the suburbs, reflecting the national trends of suburban growth following federally subsidized highways. Industrial restructuring, particularly in the steel and automotive industries, resulted in the loss of numerous jobs in Cleveland and the region, and the city suffered economically. The burning of the Cuyahoga River in June 1969 brought national attention to the issue of industrial pollution in Cleveland and served as a catalyst for the American environmental movement.\nHousing discrimination and redlining against African Americans led to racial unrest in Cleveland and numerous other Northern U.S. cities. In Cleveland, the Hough riots erupted from July 18 to 24, 1966, and the Glenville Shootout took place on July 23, 1968. In November 1967, Cleveland became the first major American city to elect an African American mayor, Carl B. Stokes, who served from 1968 to 1971 and played an instrumental role in restoring the Cuyahoga River.\nDuring the 1970s, Cleveland became known as \"Bomb City U.S.A.\" due to several bombings that shook the city, mostly due to organized crime rivalries. In December 1978, during the turbulent tenure of Dennis Kucinich as mayor, Cleveland became the first major American city since the Great Depression to enter into a financial default on federal loans. The national recession of the early 1980s \"further eroded the city's traditional economic base.\" While unemployment during the period peaked in 1983, Cleveland's rate of 13.8% was higher than the national average due to the closure of several steel production centers.\nThe city began a gradual economic recovery under Mayor George V. Voinovich in the 1980s. Downtown saw the construction of the Key Tower and 200 Public Square skyscrapers, as well as the development of the Gateway Sports and Entertainment Complex \u2013 consisting of Progressive Field and Rocket Mortgage FieldHouse \u2013 and North Coast Harbor, including the Rock and Roll Hall of Fame, Cleveland Browns Stadium, and the Great Lakes Science Center. Although the city emerged from default in 1987, it later suffered from the impact of the subprime mortgage crisis and the Great Recession.\nNevertheless, by the turn of the 21st century, Cleveland succeeded in developing a more diversified economy and gained a national reputation as a center for healthcare and the arts. The city's downtown and several neighborhoods have experienced significant population growth since 2010, while overall population decline has slowed. Challenges remain for the city, with improvement of city schools, economic development of neighborhoods, and continued efforts to tackle poverty, homelessness, and urban blight being top municipal priorities.\nGeography.\nAccording to the U.S. Census Bureau, the city has a total area of , of which is land and is water. The shore of Lake Erie is above sea level; however, the city lies on a series of irregular bluffs lying roughly parallel to the lake. In Cleveland these bluffs are cut principally by the Cuyahoga River, Big Creek, and Euclid Creek.\nThe land rises quickly from the lake shore elevation of 569 feet. Public Square, less than inland, sits at an elevation of , and Hopkins Airport, inland from the lake, is at an elevation of .\nCleveland borders several inner-ring and streetcar suburbs. To the west, it borders Lakewood, Rocky River, and Fairview Park, and to the east, it borders Shaker Heights, Cleveland Heights, South Euclid, and East Cleveland. To the southwest, it borders Linndale, Brooklyn, Parma, and Brook Park. To the south, the city borders Newburgh Heights, Cuyahoga Heights, and Brooklyn Heights and to the southeast, it borders Warrensville Heights, Maple Heights, and Garfield Heights. To the northeast, along the shore of Lake Erie, Cleveland borders Bratenahl and Euclid.\nArchitecture.\nCleveland's downtown architecture is diverse. Many of the city's government and civic buildings, including City Hall, the Cuyahoga County Courthouse, the Cleveland Public Library, and Public Auditorium, are clustered around the open Cleveland Mall and share a common neoclassical architecture. They were built in the early 20th century as the result of the 1903 Group Plan. They constitute one of the most complete examples of City Beautiful design in the U.S.\nCompleted in 1927 and dedicated in 1930 as part of the Cleveland Union Terminal complex, the Terminal Tower was the tallest building in North America outside New York City until 1964 and the tallest in the city until 1991. It is a prototypical Beaux-Arts skyscraper. The two other major skyscrapers on Public Square, Key Tower (the tallest building in Ohio) and 200 Public Square, combine elements of Art Deco architecture with postmodern designs.\nRunning east from Public Square through University Circle is Euclid Avenue, which was known as \"Millionaires' Row\" for its prestige and elegance as a residential street. In the late 1880s, writer Bayard Taylor described it as \"the most beautiful street in the world\".\nNicknamed Cleveland's \"Crystal Palace\", the five-story Cleveland Arcade (also known as the Old Arcade) was built in 1890 and renovated in 2001 as a Hyatt Regency Hotel. Another major architectural landmark, the Cleveland Trust Company Building, was completed in 1907 and renovated in 2015 as a downtown Heinen's supermarket. Cleveland's historic ecclesiastical architecture includes the Presbyterian Old Stone Church, the onion domed St. Theodosius Russian Orthodox Cathedral, and the Roman Catholic Cathedral of St. John the Evangelist along with several other ethnically inspired Catholic churches.\nNeighborhoods.\nThe Cleveland City Planning Commission has officially designated 34 neighborhoods in Cleveland. Centered on Public Square, Downtown Cleveland is the city's central business district, encompassing a wide range of subdistricts, such as the Nine-Twelve District, the Campus District, the Civic Center, East 4th Street, and Playhouse Square. It also historically included the lively Short Vincent entertainment district. Mixed-use areas, such as the Warehouse District and the Superior Arts District, are occupied by industrial and office buildings as well as restaurants, cafes, and bars. The number of condominiums, lofts, and apartments has been on the increase since 2000 and especially 2010, reflecting downtown's growing population.\nClevelanders geographically define themselves in terms of whether they live on the east or west side of the Cuyahoga River. The East Side includes the neighborhoods of Buckeye\u2013Shaker, Buckeye\u2013Woodhill, Central, Collinwood (including Nottingham), Euclid\u2013Green, Fairfax, Glenville, Goodrich\u2013Kirtland Park (including Asiatown), Hough, Kinsman, Lee\u2013Miles (including Lee\u2013Harvard and Lee\u2013Seville), Mount Pleasant, St. Clair\u2013Superior, Union\u2013Miles Park, and University Circle (including Little Italy). The West Side includes the neighborhoods of Brooklyn Centre, Clark\u2013Fulton, Cudell, Detroit\u2013Shoreway, Edgewater, Ohio City, Old Brooklyn, Stockyards, Tremont (including Duck Island), West Boulevard, and the four neighborhoods colloquially known as West Park: Kamm's Corners, Jefferson, Bellaire\u2013Puritas, and Hopkins. The Cuyahoga Valley neighborhood (including the Flats) is situated between the East and West Sides, while Broadway\u2013Slavic Village is sometimes referred to as the South Side.\nSeveral neighborhoods have begun to attract the return of the middle class that left the city for the suburbs in the 1960s and 1970s. These neighborhoods are on both the West Side (Ohio City, Tremont, Detroit\u2013Shoreway, and Edgewater) and the East Side (Collinwood, Hough, Fairfax, and Little Italy). Much of the growth has been spurred on by attracting creative class members, which has facilitated new residential development and the transformation of old industrial buildings into loft spaces for artists.\nClimate.\nTypical of the Great Lakes region, Cleveland exhibits a continental climate with four distinct seasons, which lies in the humid continental (K\u00f6ppen \"Dfa\") zone. The climate is transitional with the \"Cfa\" humid subtropical climate. Summers are hot and humid, while winters are cold and snowy. East of the mouth of the Cuyahoga, the land elevation rises rapidly in the south. Together with the prevailing winds off Lake Erie, this feature is the principal contributor to the lake-effect snow that is typical in Cleveland (especially on the city's East Side) from mid-November until the surface of the lake freezes, usually in late January or early February. The lake effect causes a relative differential in geographical snowfall totals across the city. On the city's far West Side, the Hopkins neighborhood only reached of snowfall in a season three times since record-keeping for snow began in 1893. By contrast, seasonal totals approaching or exceeding are not uncommon as the city ascends into the Heights on the east, where the region known as the \"Snow Belt\" begins. Extending from the city's East Side and its suburbs, the Snow Belt reaches up the Lake Erie shore as far as Buffalo.\nThe all-time record high in Cleveland of was established on June 25, 1988, and the all-time record low of was set on January 19, 1994. On average, July is the warmest month with a mean temperature of , and January, with a mean temperature of , is the coldest. Normal yearly precipitation based on the 30-year average from 1991 to 2020 is . The least precipitation occurs on the western side and directly along the lake, and the most occurs in the eastern suburbs. Parts of Geauga County to the east receive over of liquid precipitation annually.\nEnvironment.\nWith its extensive cleanup of its Lake Erie shore and the Cuyahoga River, Cleveland has been recognized by national media as an environmental success story and a national leader in environmental protection. Since the city's industrialization, the Cuyahoga River had become so affected by industrial pollution that it \"caught fire\" a total of 13 times beginning in 1868. It was the river fire of June 1969 that spurred the city to action under Mayor Carl B. Stokes, and played a key role in the passage of the Clean Water Act in 1972 and the National Environmental Policy Act later that year. Since that time, the Cuyahoga has been extensively cleaned up through the efforts of the city and the Ohio Environmental Protection Agency (OEPA).\nIn addition to continued efforts to improve freshwater and air quality, Cleveland is now exploring renewable energy. The city's two main electrical utilities are FirstEnergy and Cleveland Public Power. Its climate action plan, updated in December 2018, has a 2050 target of 100% renewable power, along with reduction of greenhouse gases to 80% below the 2010 level. In recent decades, Cleveland has been working to address the issue of harmful algal blooms on Lake Erie, fed primarily by agricultural runoff, which have presented new environmental challenges for the city and for northern Ohio.\nDemographics.\nAt the 2020 census, there were 372,624 people and 170,549 households in Cleveland. The population density was . The median household income was $30,907 and the per capita income was $21,223. 32.7% of the population was living below the poverty line. Of the city's population over the age of 25, 17.5% held a bachelor's degree or higher, and 80.8% had a high school diploma or equivalent. The median age was 36.6 years.\n, the racial and ethnic composition of the city was 47.5% African American, 32.1% non-Hispanic white, 13.1% Hispanic or Latino, 2.8% Asian, 0% Pacific Islander, 0.2% Native American, and 3.8% from two or more races. 85.3% of Clevelanders age five and older spoke only English at home, while 14.7% spoke a language other than English, including Spanish, Arabic, Chinese, Hungarian, Albanian, and various Slavic languages (Russian, Polish, Serbian, Croatian, and Slovene). The city's spoken accent is an advanced form of Inland Northern American English, similar to other Great Lakes cities, but distinctive from the rest of Ohio.\nEthnicity.\nIn the 19th and early 20th centuries, Cleveland saw a massive influx of immigrants from Ireland, Italy, and the Austro-Hungarian, German, Russian, and Ottoman empires, most of whom were attracted by manufacturing jobs. As a result, Cleveland and Cuyahoga County today have substantial communities of Irish (especially in West Park), Italians (especially in Little Italy), Germans, and several Central-Eastern European ethnicities, including Czechs, Hungarians, Lithuanians, Poles, Romanians, Russians, Rusyns, Slovaks, Ukrainians, and ex-Yugoslav groups, such as Slovenes, Croats and Serbs. The presence of Hungarians within Cleveland proper was, at one time, so great that the city boasted the highest concentration of Hungarians in the world outside of Budapest. Cleveland has a long-established Jewish community, historically centered on the East Side neighborhoods of Glenville and Kinsman, but now mostly concentrated in East Side suburbs such as Cleveland Heights and Beachwood, location of the Maltz Museum of Jewish Heritage.\nThe availability of jobs attracted African Americans from the South. Between 1910 and 1970, the black population of Cleveland, largely concentrated on the city's East Side, increased significantly as a result of the First and Second Great Migrations. Cleveland's Latino community consists primarily of Puerto Ricans, as well as smaller numbers of immigrants from Mexico, Cuba, the Dominican Republic, South and Central America, and Spain. The city's Asian community, centered on historical Asiatown, consists of Chinese, Koreans, Vietnamese, and other groups. Additionally, the city and the county have significant communities of Albanians, Arabs (especially Lebanese, Syrians, and Palestinians), Armenians, French, Greeks, Iranians, Scots, Turks, and West Indians. A 2020 analysis found Cleveland to be the most ethnically and racially diverse major city in Ohio.\nReligion.\nThe influx of immigrants in the 19th and early 20th centuries drastically transformed Cleveland's religious landscape. From a homogeneous settlement of New England Protestants, it evolved into a city with a diverse religious composition. The predominant faith among Clevelanders today is Christianity (Catholic, Protestant, and Eastern and Oriental Orthodox), with Jewish, Muslim, Hindu, and Buddhist minorities.\nImmigration.\nWithin Cleveland, the neighborhoods with the highest foreign-born populations are Asiatown/Goodrich\u2013Kirtland Park (32.7%), Clark\u2013Fulton (26.7%), West Boulevard (18.5%), Brooklyn Centre (17.3%), Downtown (17.2%), University Circle (15.9%, with 20% in Little Italy), and Jefferson (14.3%). Recent waves of immigration have brought new groups to Cleveland, including Ethiopians and South Asians, as well as immigrants from Russia and the former USSR, Southeast Europe (especially Albania), the Middle East, East Asia, and Latin America. In the 2010s, the immigrant population of Cleveland and Cuyahoga County began to see significant growth, becoming a major center for immigration in the Great Lakes region. A 2019 study found Cleveland to be the city with the shortest average processing time in the nation for immigrants to become U.S. citizens. The city's annual One World Day in Rockefeller Park includes a naturalization ceremony of new immigrants.\nEconomy.\nCleveland's location on the Cuyahoga River and Lake Erie has been key to its growth as a major commercial center. Steel and many other manufactured goods emerged as leading industries. The city has since diversified its economy in addition to its manufacturing sector.\nEstablished in 1914, the Federal Reserve Bank of Cleveland is one of 12 U.S. Federal Reserve Banks. Its downtown building, located on East 6th Street and Superior Avenue, was completed in 1923 by the Cleveland architectural firm Walker and Weeks. The headquarters of the Federal Reserve System's Fourth District, the bank employs 1,000 people and maintains branch offices in Cincinnati and Pittsburgh.\nCleveland and Cuyahoga County are home to \"Fortune 500\" companies Cleveland-Cliffs, Progressive, Sherwin-Williams, Parker-Hannifin, KeyCorp, and Travel Centers of America. Other large companies based in the city and the county include Aleris, American Greetings, Applied Industrial Technologies, Eaton, Forest City Realty Trust, Heinen's Fine Foods, Hyster-Yale Materials Handling, Lincoln Electric, Medical Mutual of Ohio, Moen Incorporated, NACCO Industries, Nordson Corporation, OM Group, Swagelok, Kirby Company, Things Remembered, Third Federal S&amp;L, TransDigm Group, and Vitamix. NASA maintains the Glenn Research Center in Cleveland. Jones Day, one of the largest law firms in the U.S., was founded in Cleveland in 1893.\nHealthcare.\nHealthcare plays a major role in Cleveland's economy. The city's \"Big Three\" hospital systems are the Cleveland Clinic, University Hospitals, and MetroHealth. The Cleveland Clinic is the largest private employer in the state of Ohio, with a workforce of over 55,000 . It carries the distinction of being one of the best hospital systems in the world. The clinic is led by Croatian-born president and CEO Tomislav Mihaljevic and it is affiliated with Case Western Reserve University School of Medicine.\nUniversity Hospitals includes the University Hospitals Cleveland Medical Center and its Rainbow Babies &amp; Children's Hospital. Cliff Megerian serves as that system's CEO. MetroHealth on the city's west side is led by president and CEO Christine Alexander-Rager. Formerly known as City Hospital, it operates one of two Level I trauma centers in the city, and has various locations throughout Greater Cleveland.\nIn 2013, Cleveland's Global Center for Health Innovation opened with of display space for healthcare companies across the world. To take advantage of the proximity of universities and other medical centers in Cleveland, the Veterans Administration moved the region's VA hospital from suburban Brecksville to a new facility in University Circle.\nArts and culture.\nTheater and performing arts.\nCleveland's Playhouse Square is the second largest performing arts center in the U.S. behind New York City's Lincoln Center. It includes the State, Palace, Allen, Hanna, and Ohio theaters. The theaters host Broadway musicals, special concerts, speaking engagements, and other events throughout the year. Playhouse Square's resident performing arts companies include Cleveland Ballet, the Cleveland International Film Festival, the Cleveland Play House, Cleveland State University Department of Theatre and Dance, DANCECleveland, the Great Lakes Theater Festival, and the Tri-C Jazz Fest. A city with strong traditions in theater and vaudeville, Cleveland has produced many renowned performers, most prominently comedian Bob Hope.\nOutside Playhouse Square is Karamu House, the oldest African American theater in the nation, established in 1915. On the West Side, the Gordon Square Arts District in the Detroit\u2013Shoreway neighborhood is the location of the Capitol Theatre, the Near West Theatre, and an Off-Off-Broadway playhouse, the Cleveland Public Theatre. The Dobama Theatre and the Beck Center for the Arts are based in Cleveland's streetcar suburbs of Cleveland Heights and Lakewood respectively.\nMusic.\nThe Cleveland Orchestra is widely considered one of the world's finest orchestras, and often referred to as the finest in the nation. It is one of the \"Big Five\" major orchestras in the U.S. The orchestra plays at Severance Hall in University Circle during the winter and at Blossom Music Center in Cuyahoga Falls during the summer. The city is also home to the Cleveland Pops Orchestra, Apollo's Fire Baroque Orchestra, the Cleveland Youth Orchestra, the Contemporary Youth Orchestra, the Cleveland Youth Wind Symphony, and the biennial Cleveland International Piano Competition which has, in the past, often featured the Cleveland Orchestra.\nOne Playhouse Square, now the headquarters for Cleveland's public broadcasters, was initially used as the broadcast studios of WJW (AM), where disc jockey Alan Freed first popularized the term \"rock and roll\". Beginning in the 1950s, Cleveland gained a strong reputation as a key breakout market for rock music. Its popularity in the city was so great that Billy Bass, the program director at the WMMS radio station, referred to Cleveland as \"The Rock and Roll Capital of the World\". The Cleveland Agora Theatre and Ballroom has served as a major venue for rock concerts in the city since the 1960s. From 1974 through 1980, the city hosted the World Series of Rock at Cleveland Municipal Stadium.\nJazz and R&amp;B have a long history in Cleveland. Many major figures in jazz performed in the city, including Louis Armstrong, Cab Calloway, Duke Ellington, Ella Fitzgerald, Dizzy Gillespie, and Billie Holiday. Legendary pianist Art Tatum regularly played in Cleveland clubs in the 1930s, and gypsy jazz guitarist Django Reinhardt gave his U.S. debut performance in Cleveland in 1946. Prominent jazz artist Noble Sissle was a graduate of Cleveland Central High School, and Artie Shaw worked and performed in Cleveland early in his career. The Tri-C Jazz Fest has been held annually in Cleveland at Playhouse Square since 1980, and the Cleveland Jazz Orchestra was established in 1984.\nThe city has a history of polka music being popular both past and present and is the location of the Polka Hall of Fame. There is even a subgenre called Cleveland-style polka, named after the city. The music's popularity is due in part to the success of Frankie Yankovic, a Cleveland native who was considered \"America's Polka King\".\nThere is a significant hip hop music scene in Cleveland. In 1997, the Cleveland hip hop group Bone Thugs-n-Harmony won a Grammy for their song \"Tha Crossroads\".\nFilm and television.\nThe first film shot in Cleveland was in 1897 by the company of Ohioan Thomas Edison. Before Hollywood became the center for American cinema, filmmaker Samuel Brodsky and playwright Robert McLaughlin operated a film studio at the Andrews mansion on Euclid Avenue (now the WEWS-TV studio). There they produced major silent-era features, such as \"Dangerous Toys\" (1921), which are now considered lost. Brodsky also directed the weekly \"Plain Dealer Screen Magazine\" that ran in theaters in Cleveland and Ohio from 1917 to 1924. In addition, Cleveland hosted over a dozen sponsored film studios, including Cin\u00e9craft Productions, which still operates in Ohio City.\nIn the \"talkie\" era, Cleveland featured in several major studio films, such as Michael Curtiz's pre-Code classic \"Goodbye Again\" (1933) with Warren William and Joan Blondell. Players from the 1948 Cleveland Indians appeared in \"The Kid from Cleveland\" (1949). Billy Wilder's \"The Fortune Cookie\" (1966) was set and filmed in the city and marked the first onscreen pairing of Walter Matthau and Jack Lemmon. Labor struggles in Cleveland were depicted in \"Native Land\" (1942), narrated by Paul Robeson, and in Norman Jewison's \"F.I.S.T.\" (1978) with Sylvester Stallone. Clevelander Jim Jarmusch's \"Stranger Than Paradise\" (1984) \u2013 a deadpan comedy about two New Yorkers who travel to Florida by way of Cleveland \u2013 was a favorite of the Cannes Film Festival. \"Major League\" (1989) reflected the of the Cleveland Indians, while \"American Splendor\" (2003) reflected the life of Cleveland graphic novelist Harvey Pekar. \"Kill the Irishman\" (2011) depicted the 1970s turf war between Danny Greene and the Cleveland crime family.\nCleveland has doubled for other locations in films. The wedding and reception scenes in \"The Deer Hunter\" (1978), while set in suburban Pittsburgh, were shot in Cleveland's Tremont neighborhood. \"A Christmas Story\" (1983) was set in Indiana, but drew many external shots from Cleveland. The opening shots of \"Air Force One\" (1997) were filmed in and above Severance Hall, and \"Judas and the Black Messiah\" (2021) was filmed in Cleveland, although set in Chicago. Downtown Cleveland doubled for Manhattan in \"Spider-Man 3\" (2007), \"The Avengers\" (2012), and \"The Fate of the Furious\" (2017), and for Metropolis in James Gunn's \"Superman\" (2025). Future productions are handled by the Greater Cleveland Film Commission at the Leader Building on Superior Avenue.\nIn television, the city is the setting for the popular network sitcom \"The Drew Carey Show\", starring Cleveland native Drew Carey. \"Hot in Cleveland\", a comedy that aired on TV Land, premiered on June 16, 2010, and ran for six seasons until its finale on June 3, 2015. \"Cleveland Hustles\", the CNBC reality show co-created by LeBron James, was filmed in the city.\nLiterature.\nCleveland has a thriving literary and poetry community, with regular poetry readings at bookstores, coffee shops, and various other venues. In 1925, Russian Futurist poet Vladimir Mayakovsky came to Cleveland and gave a poetry recitation to the city's ethnic working class, as part of his trip to America. The Cleveland State University Poetry Center serves as an academic center for poetry in the city.\nLangston Hughes, preeminent poet of the Harlem Renaissance and child of an itinerant couple, lived in Cleveland as a teenager and attended Central High School in Cleveland in the 1910s. At Central High, the young writer was taught by Helen Maria Chesnutt, daughter of Cleveland-born African American novelist Charles W. Chesnutt. Hughes authored some of his earliest poems, plays, and short stories in Cleveland and contributed to the school newspaper. The African American avant-garde poet Russell Atkins lived in the city as well.\nThe American modernist poet Hart Crane was born in nearby Garrettsville, Ohio in 1899. His adolescence was divided between Cleveland and Akron before he moved to New York City in 1916. Aside from factory work during World War I, he served as a reporter to \"The Plain Dealer\" for a short period, before achieving recognition in the Modernist literary scene. On the Case Western Reserve University campus, a statue of Crane, designed by sculptor William McVey, stands behind the Kelvin Smith Library.\nCleveland was the home of Joe Shuster and Jerry Siegel, who created the comic book character Superman in 1932. Both attended Glenville High School, and their early collaborations resulted in the creation of \"The Man of Steel\". Harlan Ellison, noted author of speculative fiction, was born in Cleveland in 1934; his family subsequently moved to nearby Painesville, though Ellison moved back to Cleveland in 1949. As a young man, he published a series of short stories appearing in the \"Cleveland News\", and performed in a number of productions for the Cleveland Play House.\nCleveland is the site of the Anisfield-Wolf Book Award, established by poet and philanthropist Edith Anisfield Wolf in 1935, which recognizes books that have made important contributions to the understanding of racism and human diversity. Presented by the Cleveland Foundation, it remains the only American book prize focusing on works that address racism and diversity.\nMuseums and galleries.\nCleveland has two main art museums. The Cleveland Museum of Art is a major American art museum, with a collection that includes more than 60,000 works of art ranging from ancient masterpieces to contemporary pieces. The Museum of Contemporary Art Cleveland showcases established and emerging artists, particularly from the Cleveland area, through hosting and producing temporary exhibitions. Both museums offer free admission to visitors, with the Cleveland Museum of Art declaring their museum free and open \"for the benefit of all the people forever.\"\nThe two museums are part of Cleveland's University Circle, a concentration of cultural, educational, and medical institutions located east of downtown. In addition to the art museums, the neighborhood includes the Cleveland Botanical Garden, Case Western Reserve University, University Hospitals, Severance Hall, the Maltz Performing Arts Center, the Cleveland Museum of Natural History, and the Western Reserve Historical Society. Also located at University Circle is the Cleveland Cinematheque at the Cleveland Institute of Art.\nThe I. M. Pei-designed Rock and Roll Hall of Fame is located on Cleveland's Lake Erie waterfront at North Coast Harbor downtown. Neighboring attractions include Cleveland Browns Stadium, the Great Lakes Science Center, the Steamship Mather Museum, the International Women's Air &amp; Space Museum, and the , a World War II submarine. Designed by architect Levi T. Scofield, the Soldiers' and Sailors' Monument at Public Square is Cleveland's major Civil War memorial and a major attraction in the city. Other city attractions include Grays Armory, the Cleveland Masonic Temple, and the Children's Museum of Cleveland. A Cleveland holiday attraction, especially for fans of Jean Shepherd's \"A Christmas Story\", is the Christmas Story House and Museum in Tremont.\nAnnual events.\nCleveland hosts the WinterLand holiday display lighting festival annually at Public Square, and the Cleveland International Film Festival has been held in the city since 1977. The Cleveland National Air Show, an indirect successor to the National Air Races, has been held at the city's Burke Lakefront Airport since 1964. The Great Lakes Burning River Fest, a two-night music and beer festival at Whiskey Island, has been sponsored by the Great Lakes Brewing Company since 2001.\nMany ethnic festivals are held in Cleveland throughout the year. These include the annual Feast of the Assumption in Little Italy, Russian Maslenitsa in Rockefeller Park, the Puerto Rican Parade and Cultural Festival in Clark\u2013Fulton, the Cleveland Asian Festival in Asiatown, the Tremont Greek Fest, and the St. Mary Romanian Festival in West Park. Cleveland also hosts annual Polish Dyngus Day and Slovene Kurentovanje celebrations. The city's annual Saint Patrick's Day parade brings hundreds of thousands to the streets of Downtown. The Cleveland Thyagaraja Festival held each spring at Cleveland State University is the largest Indian classical music and dance festival in the world outside of India. Since 1946, the city has annually marked One World Day in the Cleveland Cultural Gardens in Rockefeller Park, celebrating all of its ethnic communities.\nCuisine.\nCleveland's mosaic of ethnic communities and their various culinary traditions have long played an important role in defining the city's cuisine. Local mainstays include an abundance of Slavic, Hungarian, and Central-Eastern European contributions, such as kielbasa, stuffed cabbage, pierogies, goulash, and chicken paprikash. Italian, German, Irish, and Jewish cuisines are also prominent in Cleveland, as are Lebanese, Greek, Chinese, Puerto Rican, Mexican, and numerous other ethnic cuisines. Vendors at the West Side Market in Ohio City offer many ethnic foods for sale. In addition, the city boasts a vibrant barbecue and soul food scene.\nSlyman's Deli on Cleveland's near East Side is notable for its corned beef sandwich, with patrons including former US Presidents George W. Bush and Joe Biden. Another celebrated sandwich, the Polish Boy, is a popular street food and Cleveland original frequently sold at downtown hot dog carts and stadium concession stands. Brown stadium mustard is synonymous with Cleveland, especially Bertman Original Ballpark Mustard. Another notable local food item with Depression-era roots is city chicken.\nWith its blue-collar roots well intact, and plenty of Lake Erie perch and walleye available, the tradition of Friday night fish fries remains alive and thriving in Cleveland, particularly in ethnic parish-based settings, especially during the season of Lent. Clambakes are likewise embedded into the city's culinary culture. For dessert, the Cleveland Cassata Cake is a unique treat invented in the local Italian community and served in Italian establishments throughout the city. Another popular dessert, the locally crafted Russian Tea Biscuit, is common in many Jewish bakeries in Cleveland.\nCleveland is noted in the world of celebrity food culture. Famous local figures include chef Michael Symon and food writer Michael Ruhlman, both of whom achieved local and national attention for their contributions to the culinary world. In 2007, Symon helped gain the spotlight when he was named \"The Next Iron Chef\" on the Food Network. That same year, Ruhlman collaborated with Anthony Bourdain, to do an episode of his \"\" focusing on Cleveland's restaurant scene.\nBreweries.\nOhio produces the fifth most beer in the U.S., with its largest brewery being Cleveland's Great Lakes Brewing Company. Cleveland has had a long history of brewing, tied to many of its ethnic immigrants, and has reemerged as a regional leader in production. Dozens of breweries exist in the city limits, including large producers such as Market Garden Brewery and Platform Beer Company.\nBreweries can be found throughout the city, but the highest concentration is in the Ohio City neighborhood. Cleveland hosts expansions from other countries as well, including the Scottish BrewDog and German Hofbrauhaus.\nSports.\nCleveland's major professional sports teams are the Cleveland Guardians (Major League Baseball), the Cleveland Browns (National Football League), and the Cleveland Cavaliers (National Basketball Association). Other professional teams include the Cleveland Monsters (American Hockey League), the Cleveland Charge (NBA G League), the Cleveland Crunch (Major League Indoor Soccer), Cleveland SC (National Premier Soccer League), and the Cleveland Fusion (Women's Football Alliance). Local sporting venues include Progressive Field, Huntington Bank Field, Rocket Mortgage FieldHouse, the Wolstein Center, and the Public Auditorium.\nProfessional.\nMajor League\nMinor League\nThe Cleveland Guardians \u2013 known as the Indians from 1915 to 2021 \u2013 won the World Series in 1920 and 1948. They also won the American League pennant, making the World Series in the 1954, 1995, 1997, and 2016 seasons. Between 1995 and 2001, Jacobs Field (now known as Progressive Field) sold out 455 consecutive games, a Major League Baseball record until it was broken in 2008.\nHistorically, the Browns have been among the most successful franchises in American football history, winning eight titles during a short period of time \u2013 1946, 1947, 1948, 1949, 1950, 1954, 1955, and 1964. The Browns have never played in a Super Bowl, getting close five times by making it to the NFL/AFC Championship Game in 1968, 1969, 1986, , and . Former owner Art Modell's relocation of the Browns after the 1995 season (to Baltimore creating the Ravens), caused tremendous heartbreak and resentment among local fans. Cleveland mayor, Michael R. White, worked with the NFL and Commissioner Paul Tagliabue to bring back the Browns beginning in the 1999 season, retaining all team history. In Cleveland's earlier football history, the Cleveland Bulldogs won the NFL Championship in 1924, and the Cleveland Rams won the NFL Championship in 1945 before relocating to Los Angeles.\nThe Cavaliers won the Eastern Conference in 2007, 2015, 2016, 2017 and 2018 but were defeated in the NBA Finals by the San Antonio Spurs and then by the Golden State Warriors, respectively. The Cavs won the Conference again in 2016 and won their first NBA Championship coming back from a 3\u20131 deficit, finally defeating the Golden State Warriors. Afterwards, over 1.3 million people attended a parade held in the Cavs' honor on June 22, 2016, in downtown Cleveland. Previously, the Cleveland Rosenblums dominated the original American Basketball League, and the Cleveland Pipers, owned by George Steinbrenner, won the American Basketball League championship in 1962.\nThe Cleveland Monsters of the American Hockey League won the 2016 Calder Cup. They were the first Cleveland AHL team to do so since the 1964 Barons.\nCollege.\nCollegiately, NCAA Division I Cleveland State Vikings have 19 varsity sports, nationally known for their Cleveland State Vikings men's basketball team. NCAA Division III Case Western Reserve Spartans have 17 varsity sports, most known for their Case Western Reserve Spartans football team. The headquarters of the Mid-American Conference (MAC) are in Cleveland. The conference stages both its men's and women's basketball tournaments at Rocket Mortgage FieldHouse.\nAnnual and special events.\nThe Cleveland Marathon has been hosted annually since 1978, and a monument commemorating one of Cleveland's most prominent track and field athletes, Jesse Owens, stands at the city's Fort Huntington Park. The second American Chess Congress, a predecessor to the U.S. Championship, was held in Cleveland in 1871, and won by George Henry Mackenzie. The 1921 and 1957 U.S. Open Chess Championships took place in the city, and were won by Edward Lasker and Bobby Fischer, respectively. The Cleveland Open is held annually. In 2014, Cleveland hosted the ninth official Gay Games ceremony. In July 2024, the city hosted the Pan American Masters Games.\nParks and recreation.\nKnown locally as the \"Emerald Necklace\", the Olmsted-inspired Cleveland Metroparks encircle Cleveland and Cuyahoga County. The city proper encompasses the Metroparks' Brookside and Lakefront Reservations, as well as significant parts of the Rocky River, Washington, and Euclid Creek Reservations. The Lakefront Reservation, which provides public access to Lake Erie, consists of four parks: Edgewater Park, Whiskey Island\u2013Wendy Park, East 55th Street Marina, and Gordon Park.\nThree more parks fall under the jurisdiction of the Euclid Creek Reservation: Euclid Beach, Villa Angela, and Wildwood Marina. Further south, bike and hiking trails in the Brecksville and Bedford Reservations, along with Garfield Park, provide access to trails in the Cuyahoga Valley National Park. Also included in the Metroparks system is the Cleveland Metroparks Zoo, established in 1882. Located in Big Creek Valley, the zoo has one of the largest collections of primates in North America.\nIn addition to the Metroparks, the Cleveland Public Parks District oversees the city's neighborhood parks, the largest of which is the historic Rockefeller Park. The latter is notable for its late 19th century landmark bridges, the Rockefeller Park Greenhouse, and the Cleveland Cultural Gardens, which celebrate the city's ethnic diversity. Just outside of Rockefeller Park, the Cleveland Botanical Garden in University Circle, established in 1930, is the oldest civic garden center in the nation. In addition, the Greater Cleveland Aquarium, located in the historic FirstEnergy Powerhouse in the Flats, is the only independent, free-standing aquarium in the state of Ohio.\nGovernment and politics.\nGovernment and courts.\nCleveland operates on a mayor\u2013council (strong mayor) form of government, in which the mayor is the chief executive and the city council serves as the legislative branch. City council members are elected from 17 wards to four-year terms. From 1924 to 1931, the city briefly experimented with a council\u2013manager government under William R. Hopkins and Daniel E. Morgan before returning to the mayor\u2013council system.\nCleveland is served by Cleveland Municipal Court, the first municipal court in the state. The city also anchors the U.S. District Court for the Northern District of Ohio, based at the Carl B. Stokes U.S. Courthouse and the historic Howard M. Metzenbaum U.S. Courthouse. The Chief Judge for the Northern District is Sara Elizabeth Lioi and the Clerk of Court is Sandy Opacich. The U.S. Attorney is Carol Skutnik and the U.S. Marshal is Peter Elliott.\nPolitics.\nThe office of the mayor has been held by Justin Bibb since 2022. Previous mayors include progressive Democrat Tom L. Johnson, World War I-era War Secretary and BakerHostetler founder Newton D. Baker, U.S. Supreme Court Justice Harold Hitz Burton, two-term Ohio Governor and Senator Frank J. Lausche, former U.S. Health, Education, and Welfare Secretary Anthony J. Celebrezze, two-term Ohio Governor and Senator George V. Voinovich, former U.S. Congressman Dennis Kucinich, and Carl B. Stokes, the first African American mayor of a major U.S. city. Frank G. Jackson was the city's longest-serving mayor.\nThe President of Cleveland City Council is Blaine Griffin, the council Majority Leader is Kerry McCormack, and the Majority Whip is Jasmin Santana. Patricia Britt serves as the Clerk of Council.\nHistorically, from the Civil War era to the 1940s, Cleveland had been dominated by the Republican Party, with the notable exceptions of the Johnson and Baker mayoral administrations. Businessman and Senator Mark Hanna was among Cleveland's most influential Republican figures, both locally and nationally. Another nationally prominent Ohio Republican, former U.S. President James A. Garfield, was born in Cuyahoga County's Orange Township (today the Cleveland suburb of Moreland Hills). His resting place is the James A. Garfield Memorial in Cleveland's Lake View Cemetery.\nToday Cleveland is a major stronghold for the Democratic Party in Ohio. Although local elections are nonpartisan, Democrats still dominate every level of government. Politically, Cleveland and several of its neighboring suburbs comprise Ohio's 11th congressional district. The district is represented by Shontel Brown, one of five Democrats representing the state of Ohio in the U.S. House of Representatives.\nCleveland has hosted three Republican national conventions, in 1924, 1936, and 2016. Additionally, the city hosted the Radical Republican convention of 1864. Although Cleveland has not hosted a national convention for the Democrats, it has hosted several national election debates, including the second 1980 U.S. presidential debate, the , one , and the first 2020 U.S. presidential debate. Founded in 1912, the City Club of Cleveland provides a platform for national and local debates and discussions. Known as Cleveland's \"Citadel of Free Speech\", it is one of the oldest continuous independent free speech and debate forums in the country.\nPublic safety.\nPolice and law enforcement.\nLike in other major American cities, crime in Cleveland is concentrated in areas with higher rates of poverty and lower access to jobs. In recent decades, the rate of crime in the city, although higher than the national average, experienced a significant decline, following a nationwide trend in falling crime rates. However, as in other major U.S. cities, crime in Cleveland saw an abrupt rise in 2020\u201321.\nCleveland's law enforcement agency is the Cleveland Division of Police, established in 1866. The division had roughly 1,100 sworn officers as of 2024, covering five police districts. The district system was introduced in the 1930s by Cleveland Public Safety Director Eliot Ness (of the Untouchables), who later ran for mayor of Cleveland in 1947. The Chief of Police is Dorothy A. Todd. In addition, the Cuyahoga County Sheriff's Office is based in Downtown Cleveland at the Justice Center Complex.\nFire department.\nCleveland is served by the firefighters of the Cleveland Division of Fire, established in 1863. The fire department operates out of 22 active fire stations throughout the city in five battalions. Each Battalion is commanded by a Battalion Chief, who reports to an on-duty Assistant Chief.\nThe Division of Fire operates a fire apparatus fleet of twenty-two engine companies, eight ladder companies, three tower companies, two task force rescue squad companies, hazardous materials (\"haz-mat\") unit, and numerous other special, support, and reserve units. The Chief of Department is Anthony Luke.\nEmergency medical services.\nCleveland EMS is operated by the city as its own municipal third-service EMS division. Cleveland EMS is the primary provider of Advanced Life Support and ambulance transport within the city of Cleveland, while Cleveland Fire assists by providing fire response medical care. Although a merger between the fire and EMS departments was proposed in the past, the idea was subsequently abandoned.\nMilitary.\nCleveland serves as headquarters to Coast Guard District 9 and is responsible for all U.S. Coast Guard operations on the five Great Lakes, the Saint Lawrence Seaway, and surrounding states accumulating 6,700 miles of shoreline and 1,500 miles of international shoreline with Canada. It reports up through the U.S. Department of Homeland Security. Station Cleveland Harbor, located in North Coast Harbor, has a responsibility covering about 550 square miles of the federally navigable waters of Lake Erie, including the Cuyahoga and Rocky rivers, as well as a number of their tributaries.\nEducation.\nPrimary and secondary.\nCleveland is served by the Cleveland Metropolitan School District. It is the only K\u201312 district in Ohio under the direct control of the mayor, who appoints a school board. Approximately of Cleveland's Buckeye\u2013Shaker neighborhood is part of the Shaker Heights City School District. The area, which has been a part of the Shaker school district since the 1920s, permits these Cleveland residents to pay the same school taxes as the Shaker residents, as well as vote in the Shaker school board elections.\nThere are several private and parochial schools in Cleveland. These include Benedictine High School, Cleveland Central Catholic High School, Eleanor Gerson School, St. Ignatius High School, St. Joseph Academy, Villa Angela-St. Joseph High School, and St. Martin de Porres.\nColleges and universities.\nCleveland is home to a number of colleges and universities. Most prominent among them is Case Western Reserve University (CWRU), a widely recognized research and teaching institution based in University Circle with several major graduate programs.\nUniversity Circle also contains the Cleveland Institute of Art and the Cleveland Institute of Music. Downtown Cleveland is home to Cleveland State University, a public research university with eight constituent colleges, and the metropolitan campus of Cuyahoga Community College. Ohio Technical College is also based in Cleveland. Cleveland's suburban universities and colleges include Baldwin Wallace University in Berea, John Carroll University in University Heights, and Ursuline College in Pepper Pike.\nPublic library system.\nEstablished in 1869, the Cleveland Public Library is one of the largest public libraries in the nation with a collection of over 10 million materials in 2021. Its John G. White Special Collection includes the largest chess library in the world, as well as a significant collection of folklore and rare books on the Middle East and Eurasia. The library's main building was designed by Walker and Weeks and dedicated in 1925, under head librarian Linda Eastman, the first woman to lead a major library system in the U.S. Between 1904 and 1920, 15 libraries built with funds from Andrew Carnegie were opened in the city. Known as the \"People's University\", the library presently maintains 27 branches. It serves as the headquarters for the CLEVNET library consortium, which includes 47 public library systems in Northeast Ohio.\nMedia.\nPrint.\nCleveland's primary daily newspaper is \"The Plain Dealer\" and its associated online publication, \"Cleveland.com\". Defunct major newspapers include the \"Cleveland Press\" and the \"Cleveland News\". Additional publications include \"Cleveland Magazine\", a regional culture magazine published monthly; \"Crain's Cleveland Business\", a weekly business newspaper; and \"Cleveland Scene\", a free alternative weekly paper which absorbed its competitor, the \"Cleveland Free Times\", in 2008. The digital \"Belt Magazine\" was founded in Cleveland in 2013. \"Time\" magazine was published in Cleveland from 1925 to 1927.\nSeveral ethnic publications are based in Cleveland. These include the \"Call and Post\", a weekly newspaper that primarily serves the city's African American community; the \"Cleveland Jewish News\", a weekly Jewish newspaper; the bi-weekly Russian-language \"Cleveland Russian Magazine\"; the Mandarin \"Erie Chinese Journal\"; \"La Gazzetta Italiana\" in English and Italian; the \"Ohio Irish American News\"; and the Spanish language \"Vocero Latino News\".\nTV.\nThe Cleveland-area television market is served by 11 full power stations, including WKYC (NBC), WEWS-TV (ABC), WJW (Fox), WDLI-TV (Bounce), WOIO (CBS), WVPX-TV (Ion), WVIZ (PBS), WUAB (CW/RESN), WRLM (TCT), WBNX-TV (independent), and WQHS-DT (Univision). the market, which includes the Akron and Canton areas, was the 19th-largest in the country, as measured by Nielsen Media Research.\n\"The Mike Douglas Show\", a nationally syndicated daytime talk show, began in Cleveland in 1961 on KYW-TV (now WKYC), while \"The Morning Exchange\" on WEWS-TV served as the model for \"Good Morning America\". Tim Conway and Ernie Anderson first established themselves in Cleveland while working together at KYW-TV and later WJW-TV (now WJW). Anderson both created and performed as the immensely popular Cleveland horror host Ghoulardi on WJW-TV's \"Shock Theater\", and was later succeeded by the long-running late night duo Big Chuck and Lil' John. Another Anderson prot\u00e9g\u00e9 \u2013 Ron Sweed \u2013 would become a popular Cleveland late night movie host in his own right as \"The Ghoul\".\nRadio.\nCleveland is directly served by 29 full power AM and FM radio stations, 21 of which are licensed to the city. Music stations \u2013 which are frequently the highest-rated in the market \u2013 include WQAL (hot adult contemporary), WDOK (adult contemporary), WKLV (Christian contemporary - K-LOVE), WAKS (contemporary hits), WHLK (adult hits), WMJI (classic hits), WMMS (active rock/hot talk), WNCX (classic rock), WNWV (alternative rock), WGAR-FM (country), WZAK (urban adult contemporary), WENZ (mainstream urban), WCLV (classical), and WJMO (Spanish/Tropical). WMMS also serves as the FM flagship for the Cleveland Cavaliers and the Cleveland Guardians, while WNCX is an FM flagship for the Cleveland Browns.\nNews/talk stations include WHK, WTAM, and WERE. During the Golden Age of Radio, WHK was the first radio station to broadcast in Ohio, and one of the first in the country. WTAM is the AM flagship for both the Cleveland Cavaliers and the Cleveland Guardians. Sports stations include WKNR (ESPN), WARF (Fox) and WKRK-FM (Infinity), with WKNR and WKRK-FM serving as co-flagship stations for the Cleveland Browns, and WARF airing the Cleveland Monsters and \u2013 though primarily an English language station \u2013 Spanish broadcasts of Cleveland Guardians home games. Religious stations include WCCD, WHKW, WCCR, and WCRF.\nAs the regional NPR affiliate, WKSU serves all of Northeast Ohio (including both the Cleveland and Akron markets). College stations include WBWC (Baldwin Wallace), WCSB (Cleveland State), WJCU (John Carroll), and WRUW-FM (Case Western Reserve).\nTransportation.\nTransit.\nCleveland has a bus and rail mass transit system operated by the Greater Cleveland Regional Transit Authority (RTA). The rail portion is officially called the RTA Rapid Transit, but local residents refer to it as \"The Rapid\". It consists of three light rail lines, known as the Blue, Green, and Waterfront Lines, and a heavy rail line, the Red Line. In 2008, RTA completed the HealthLine, a bus rapid transit line, for which naming rights were purchased by the Cleveland Clinic and University Hospitals. It runs along Euclid Avenue from downtown through University Circle, ending at the Louis Stokes Station at Windermere in East Cleveland. In 1968, Cleveland became the first city in the nation to have a direct rail transit connection linking the city's downtown to its major airport.\nWalkability.\nIn 2021, Walk Score ranked Cleveland the 17th most walkable of the 50 largest cities in the U.S., with a Walk Score of 57, a Transit Score of 45, and a Bike Score of 55 (out of a maximum of 100). Cleveland's most walkable areas can be found in the Downtown, Ohio City, Detroit\u2013Shoreway, University Circle, and Buckeye\u2013Shaker neighborhoods. Like other major cities, the urban density of Cleveland reduces the need for private vehicle ownership. In 2016, 23.7% of Cleveland households lacked a car, while the national average was 8.7%. Cleveland averaged 1.19 cars per household in 2016, compared to a national average of 1.8.\nRoads.\nCleveland's road system consists of numbered streets running roughly north\u2013south, and named avenues, which run roughly east\u2013west. The numbered streets are designated \"east\" or \"west\", depending on where they lie in relation to Ontario Street, which bisects Public Square. The two downtown avenues which span the Cuyahoga change names on the west side of the river. Superior Avenue becomes Detroit Avenue on the West Side, and Carnegie Avenue becomes Lorain Avenue. The bridges that make these connections are the Hope Memorial (Lorain\u2013Carnegie) Bridge and the Veterans Memorial (Detroit\u2013Superior) Bridge.\nFreeways.\nCleveland is served by three two-digit interstate highways \u2013 Interstate 71, Interstate 77, and Interstate 90 \u2013 and by two three-digit interstates \u2013 Interstate 480 and Interstate 490. Running due east\u2013west through the West Side suburbs, I-90 turns northeast at the junction with I-490, and is known as the Cleveland Inner Belt. The Cleveland Memorial Shoreway carries Ohio State Route 2 along its length, and at varying points carries US 6, US 20 and I-90. At the junction with the Shoreway, I-90 makes a 90-degree turn in the area known as Dead Man's Curve, then continues northeast. The Jennings Freeway (State Route 176) connects I-71 just south of I-90 to I-480. A third highway, the Berea Freeway (State Route 237 in part), connects I-71 to the airport and forms part of the boundary between Brook Park and Cleveland's Hopkins neighborhood.\nAirports.\nCleveland is a major North American air market, serving 4.93 million people. Cleveland Hopkins International Airport is the city's primary major airport and an international airport that serves the broader region. Originally known as Cleveland Municipal Airport, it was the first municipally owned airport in the country. Cleveland Hopkins is a significant regional air freight hub hosting FedEx Express, UPS Airlines, U.S. Postal Service, and major commercial freight carriers. In addition to Hopkins, Cleveland is served by Burke Lakefront Airport, on the north shore of downtown between Lake Erie and the Shoreway. Burke is primarily a commuter and business airport.\nSeaport.\nThe Port of Cleveland, at the Cuyahoga River's mouth, is a major bulk freight and container terminal on Lake Erie, receiving much of the raw materials used by the region's manufacturing industries. The Port of Cleveland is the only container port on the Great Lakes with bi-weekly container service between Cleveland and the Port of Antwerp in Belgium on a Dutch service called the Cleveland-Europe Express. In addition to freight, the Port of Cleveland welcomes regional and international tourists who pass through the city on Great Lakes cruises.\nIntercity rail and bus.\nCleveland has a long history as a major railroad hub in North America. Today, Amtrak provides service to Cleveland, via the \"Capitol Limited\" and \"Lake Shore Limited\" routes, which stop at Cleveland Lakefront Station. Additionally, Cleveland hosts several inter-modal freight railroad terminals, for Norfolk Southern, CSX and several smaller companies.\nNational intercity bus service is provided by Greyhound. Akron Metro, Brunswick Transit Alternative, Laketran, Lorain County Transit, and Medina County Transit provide connecting bus service to the Greater Cleveland Regional Transit Authority. Geauga County Transit and Portage Area Regional Transportation Authority (PARTA) also offer connecting bus service in their neighboring areas.\nInternational relations.\nCleveland maintains cultural, economic, and educational ties with 28 sister cities around the world. It concluded its first sister city partnership with Lima, Peru, in 1964. In addition, Cleveland hosts the Consulate General of the Republic of Slovenia, which, until Slovene independence in 1991, served as an official consulate for Tito's Yugoslavia. The Cleveland Clinic operates the Cleveland Clinic Abu Dhabi hospital, two outpatient clinics in Toronto, and a hospital campus in London. The Cleveland Council on World Affairs was established in 1923.\nHistorically, Cleveland industrialist Cyrus S. Eaton, an apprentice of John D. Rockefeller, played a significant role in promoting dialogue between the U.S. and the Soviet Union during the Cold War. In October 1915 at Cleveland's Bohemian National Hall, Czech American and Slovak American representatives signed the Cleveland Agreement, calling for the formation of a joint Czech and Slovak state."}
{"id": "5953", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5953", "title": "Claude Oscar Monet", "text": ""}
{"id": "5954", "revid": "949717", "url": "https://en.wikipedia.org/wiki?curid=5954", "title": "Callisto", "text": "Callisto most commonly refers to:\nCallisto may also refer to:"}
{"id": "5955", "revid": "49140394", "url": "https://en.wikipedia.org/wiki?curid=5955", "title": "Church of England", "text": "The Church of England (C of E) is the established Christian church in England and the Crown Dependencies. It is the origin of the Anglican tradition, with foundational doctrines being contained in the \"Thirty-nine Articles\" and \"The Books of Homilies\". Its adherents are called \"Anglicans\".\nEnglish Christianity traces its history to the Christian hierarchy recorded as existing in the Roman province of Britain by the 3rd century and to the 6th-century Gregorian mission to Kent led by Augustine of Canterbury. It renounced papal authority in 1534, when King Henry\u00a0VIII failed to secure a papal annulment of his marriage to Catherine of Aragon. The English Reformation accelerated under the regents of his successor, King Edward\u00a0VI, before a brief restoration of papal authority under Queen Mary I and King Philip. The guiding theologian that shaped Anglican doctrine was the Reformer Thomas Cranmer, who developed the Church of England's liturgical text, the \"Book of Common Prayer\". The Act of Supremacy 1558 renewed the breach, and the Elizabethan Settlement (implemented 1559\u20131563) concluded the English Reformation, charting a course for the English church to describe itself as a \"via media\" between two branches of Protestantism\u2014Lutheranism and Calvinism\u2014and later, a denomination that is both Reformed and Catholic.\nIn the earlier phase of the English Reformation there were both Roman Catholic martyrs and Protestant martyrs. The later phases saw the Penal Laws punish Roman Catholics and nonconforming Protestants. In the 17th century, the Puritan and Presbyterian factions continued to challenge the leadership of the church, which under the Stuarts veered towards a more Catholic interpretation of the Elizabethan Settlement, especially under Archbishop Laud. After the victory of the Parliamentarians, the \"Book of Common Prayer\" was abolished and the Presbyterian and Independent factions dominated. The episcopacy was abolished in 1646 but the Restoration restored the Church of England, episcopacy and the \"Book of Common Prayer\".\nSince the English Reformation, the Church of England has used the English language in the liturgy. As a broad church, the Church of England contains several doctrinal strands: the main traditions are known as Anglo-Catholic, high church, central church, and low church, the latter producing a growing evangelical wing that includes Reformed Anglicanism, with a smaller number of Arminian Anglicans. Tensions between theological conservatives and liberals find expression in debates over the ordination of women and homosexuality. The British monarch (currently Charles III) is the supreme governor and the archbishop of Canterbury (vacant since 12 November 2024, after the resignation of Justin Welby) is the most senior cleric.\nThe governing structure of the church is based on dioceses, each presided over by a bishop. Within each diocese are local parishes. The General Synod of the Church of England is the legislative body for the church and comprises bishops, other clergy and laity. Its measures must be approved by the Parliament of the United Kingdom.\nHistory.\nMiddle Ages.\nThere is evidence for Christianity in Roman Britain as early as the 3rd century. After the fall of the Roman Empire, England was conquered by the Anglo-Saxons, who were pagans, and the Celtic church was confined to Cornwall and Wales. In 597, Pope Gregory\u00a0I sent missionaries to England to Christianise the Anglo-Saxons. This mission was led by Augustine, who became the first archbishop of Canterbury. The Church of England considers 597 the start of its formal history.\nIn Northumbria, Celtic missionaries competed with their Roman counterparts. The Celtic and Roman churches disagreed over the date of Easter, baptismal customs, and the style of tonsure worn by monks. King Oswiu of Northumbria summoned the Synod of Whitby in 664. The king decided Northumbria would follow the Roman tradition because Saint Peter and his successors, the bishops of Rome, hold the keys of the kingdom of heaven.\nBy the late Middle Ages, Catholicism was an essential part of English life and culture. The 9,000 parishes covering all of England were overseen by a hierarchy of deaneries, archdeaconries, dioceses led by bishops, and ultimately the pope who presided over the Catholic Church from Rome. Catholicism taught that the contrite person could cooperate with God towards their salvation by performing good works (see synergism). God's grace was given through the seven sacraments. In the Mass, a priest consecrated bread and wine to become the body and blood of Christ through transubstantiation. The church taught that, in the name of the congregation, the priest offered to God the same sacrifice of Christ on the cross that provided atonement for the sins of humanity. The Mass was also an offering of prayer by which the living could help souls in purgatory. While penance removed the guilt attached to sin, Catholicism taught that a penalty still remained. It was believed that most people would end their lives with these penalties unsatisfied and would have to spend time in purgatory. Time in purgatory could be lessened through indulgences and prayers for the dead, which were made possible by the communion of saints.\nReformation.\nIn 1527, Henry\u00a0VIII was desperate for a male heir and asked Pope Clement VII to annul his marriage to Catherine of Aragon. When the pope refused, Henry used Parliament to assert royal authority over the English church. In 1533, Parliament passed the Act in Restraint of Appeals, barring legal cases from being appealed outside England. This allowed the Archbishop of Canterbury to annul the marriage without reference to Rome. In November 1534, the Act of Supremacy formally abolished papal authority and declared Henry Supreme Head of the Church of England.\nHenry's religious beliefs remained aligned to traditional Catholicism throughout his reign, albeit with reformist aspects in the tradition of Erasmus and firm commitment to royal supremacy. In order to secure royal supremacy over the church, however, Henry allied himself with Protestants, who until that time had been treated as heretics. The main doctrine of the Protestant Reformation was justification by faith alone rather than by good works. The logical outcome of this belief is that the Mass, sacraments, charitable acts, prayers to saints, prayers for the dead, pilgrimage, and the veneration of relics do not mediate divine favour. To believe they can would be superstition at best and idolatry at worst.\nBetween 1536 and 1540, Henry engaged in the dissolution of the monasteries, which controlled much of the richest land. He disbanded religious houses, appropriated their income, disposed of their assets, and provided pensions for the former residents. The properties were sold to pay for the wars. Historian George W. Bernard argues:\nIn the reign of Edward\u00a0VI (1547\u20131553), the Church of England underwent an extensive theological reformation. Justification by faith was made a central teaching. Government-sanctioned iconoclasm led to the destruction of images and relics. Stained glass, shrines, statues, and roods were defaced or destroyed. Church walls were whitewashed and covered with biblical texts condemning idolatry. The most significant reform in Edward's reign was the adoption of an English liturgy to replace the old Latin rites. Written by the Protestant Reformer Archbishop Thomas Cranmer, the 1549 \"Book of Common Prayer\" implicitly taught justification by faith, and rejected the Catholic doctrines of transubstantiation and the sacrifice of the Mass. This was followed by a greatly revised 1552 \"Book of Common Prayer\", which propounded a Reformed view of the Lord's Supper (cf. \"Lord's Supper in Reformed theology\"). Along with \"The Book of Common Prayer\", \"The Thirty-nine Articles\" and \"The Books of Homilies\", assembled through the efforts of the Reformer Thomas Cranmer, became the basis of Anglican doctrine after the English Reformation.\nDuring the reign of Mary\u00a0I (1553\u20131558), England was briefly reunited with the Catholic Church. Mary died childless, so it was left to the new regime of her half-sister Queen Elizabeth\u00a0I to resolve the direction of the Church. The Elizabethan Religious Settlement returned the Church to where it stood in 1553 before Edward's death. The Act of Supremacy made the monarch the Church's Supreme Governor of the Church of England. The Act of Uniformity restored a slightly altered 1552 \"Book of Common Prayer\". In 1571, the Thirty-nine Articles received parliamentary approval as a doctrinal statement for the Church. The settlement ensured the Church of England was Protestant, but it was unclear what kind of Protestantism was being adopted. Anglicanism was said to be a \"via media\" between two forms of Protestantism, Lutheranism and Reformed Christianity though more aligned with the latter than the former. The prayer book's Reformed eucharistic theology posited a real spiritual presence (pneumatic presence), since Article 28 of the Thirty-nine Articles taught that the body of Christ was eaten \"only after an heavenly and spiritual manner\". Nevertheless, there was enough ambiguity to allow later theologians to articulate various versions of Anglican eucharistic theology.\nThe Church of England was the established church (constitutionally established by the state with the head of state as its supreme governor). The exact nature of the relationship between church and state would be developed over the next century. Notably, the Act of Settlement 1701, which remains in force today, stipulates that the monarch (who serves as the Supreme Governor of the Church of England) be a Protestant, maintain the Protestant succession, and \"join in communion with the Church of England as by law established.\" The Coronation Oath Act 1688 (reiterated in the Act of Settlement 1701) requires the rising Sovereign to take an oath to maintain \"the true Profession of the Gospel and the Protestant Reformed Religion Established by Law\" in the United Kingdom.\nStuart period.\nStruggle for control of the church persisted throughout the reigns of James\u00a0I and his son Charles\u00a0I, culminating in the outbreak of the First English Civil War in 1642. The two opposing factions consisted of Puritans, who sought to \"purify\" the church and enact more far-reaching Protestant reforms, and those who wanted to retain traditional beliefs and practices. In a period when many believed \"true religion\" and \"good government\" were the same thing, religious disputes often included a political element, one example being the struggle over bishops. In addition to their religious function, bishops acted as state censors, able to ban sermons and writings considered objectionable, while lay people could be tried by church courts for crimes including blasphemy, heresy, fornication and other 'sins of the flesh', as well as matrimonial or inheritance disputes. They also sat in the House of Lords and often blocked legislation opposed by the Crown; their ousting from Parliament by the 1640 Clergy Act was a major step on the road to war.\nFollowing Royalist defeat in 1646, the episcopacy was formally abolished. In 1649, the Commonwealth of England outlawed a number of former practices and Presbyterian structures replaced the episcopate. The Thirty-nine Articles were replaced by the Westminster Confession. Worship according to the \"Book of Common Prayer\" was outlawed and replace by the \"Directory of Public Worship\". Despite this, about one quarter of English clergy refused to conform to this form of state presbyterianism. It was also opposed by religious Independents who rejected the very idea of state-mandated religion, and included Congregationalists like Oliver Cromwell, as well as Baptists, who were especially well represented in the New Model Army.\nAfter the Stuart Restoration in 1660, Parliament restored the Church of England to a form not far removed from the Elizabethan version. Until James II of England was ousted by the Glorious Revolution in November 1688, many Nonconformists still sought to negotiate terms that would allow them to re-enter the church. In order to secure his political position, William III of England ended these discussions and the Tudor ideal of encompassing all the people of England in one religious organisation was abandoned. The religious landscape of England assumed its present form, with the Anglican established church occupying the middle ground and Nonconformists continuing their existence outside. One result of the Restoration was the ousting of 2,000 parish ministers who had not been ordained by bishops in the apostolic succession or who had been ordained by ministers in presbyter's orders. Official suspicion and legal restrictions continued well into the 19th century. Roman Catholics, perhaps 5% of the English population (down from 20% in 1600) were grudgingly tolerated, having had little or no official representation after the Pope's excommunication of Queen Elizabeth in 1570, though the Stuarts were sympathetic to them. By the end of 18th century they had dwindled to 1% of the population, mostly amongst upper middle-class gentry, their tenants, and extended families.\nUnion with the Church of Ireland.\nBy the Fifth Article of the Union with Ireland 1800, the Church of England and Church of Ireland were united into \"one Protestant Episcopal church, to be called, the United Church of England and Ireland\". Although \"the continuance and preservation of the said united church ... [was] deemed and taken to be an essential and fundamental part of the union\", the Irish Church Act 1869 separated the Irish part of the church again and disestablished it, the Act coming into effect on 1 January 1871.\nOverseas developments.\nAs the English Empire (after the 1707 union of the Kingdom of England with the Kingdom of Scotland to form the Kingdom of Great Britain, the British Empire) expanded, English (after 1707, \"British\") colonists and colonial administrators took the established church doctrines and practices together with ordained ministry and formed overseas branches of the Church of England.\nThe Diocese of Nova Scotia was created on 11 August 1787 by Letters Patent of George III which \"erected the Province of Nova Scotia into a bishop's see\" and these also named Charles Inglis as first bishop of the see. The diocese was the first Church of England see created outside England and Wales (i.e. the first colonial diocese). At this point, the see covered present-day New Brunswick, Newfoundland, Nova Scotia, Prince Edward Island and Quebec. From 1825 to 1839, it included the nine parishes of Bermuda, subsequently transferred to the Diocese of Newfoundland.\nAs they developed, beginning with the United States of America, or became sovereign or independent states, many of their churches became separate organisationally, but remained linked to the Church of England through the Anglican Communion. In the provinces that made up Canada, the church operated as the \"Church of England in Canada\" until 1955 when it became the Anglican Church of Canada.\nIn Bermuda, the oldest remaining British overseas possession, the first Church of England services were performed by the Reverend Richard Buck, one of the survivors of the 1609 wreck of the \"Sea Venture\" which initiated Bermuda's permanent settlement. The nine parishes of the Church of England in Bermuda, each with its own church and glebe land, rarely had more than a pair of ordained ministers to share between them until the 19th century. From 1825 to 1839, Bermuda's parishes were attached to the See of Nova Scotia. Bermuda was then grouped into the new Diocese of Newfoundland and Bermuda from 1839. In 1879, the Synod of the Church of England in Bermuda was formed. At the same time, a Diocese of Bermuda became separate from the Diocese of Newfoundland, but both continued to be grouped under the \"Bishop of Newfoundland and Bermuda\" until 1919, when Newfoundland and Bermuda each received its own bishop. The Church of England in Bermuda was renamed in 1978 as the Anglican Church of Bermuda, which is an extra-provincial diocese, with both metropolitan and primatial authority coming directly from the Archbishop of Canterbury. Among its parish churches is St Peter's Church in the UNESCO World Heritage Site of St George's Town, which is the oldest Anglican church outside of the British Isles, and the oldest Protestant church in the New World.\nThe Church of India, Burma and Ceylon was established in Colonial India, with its first diocese being erected in 1813, the Diocese of Calcutta. Indian bishops were present at the first Lambeth Conference.\nThe first Anglican missionaries arrived in Nigeria in 1842 and the first Anglican Nigerian was consecrated a bishop in 1864. However, the arrival of a rival group of Anglican missionaries in 1887 led to infighting that slowed the Church's growth. In this large African colony, by 1900 there were only 35,000 Anglicans, about 0.2% of the population. However, by the late 20th century the Church of Nigeria was the fastest growing of all Anglican churches, reaching about 18 percent of the local population by 2000.\nThe church established its presence in Hong Kong and Macau in 1843. In 1951, the Diocese of Hong Kong and Macao became an extra-provincial diocese, and in 1998 it became a province of the Anglican Communion, under the name Hong Kong Sheng Kung Hui.\nFrom 1796 to 1818 the Church began operating in Sri Lanka (formerly Ceylon), following the 1796 start of British colonisation, when the first services were held for the British civil and military personnel. In 1799, the first Colonial Chaplain was appointed, following which CMS and SPG missionaries began their work, in 1818 and 1844 respectively. Subsequently the Church of Ceylon was established: in 1845 the diocese of Colombo was inaugurated, with the appointment of James Chapman as Bishop of Colombo. It served as an extra-provincial jurisdiction of the archbishop of Canterbury, who served as its metropolitan.\nEarly 21st century.\nDeposition from holy orders overturned.\nUnder the guidance of Rowan Williams and with significant pressure from clergy union representatives, the ecclesiastical penalty for convicted felons to be defrocked was set aside from the Clergy Discipline Measure 2003. The clergy union argued that the penalty was unfair to victims of hypothetical miscarriages of criminal justice, because the ecclesiastical penalty is considered irreversible. Although clerics can still be banned for life from ministry, they remain ordained as priests.\nContinued decline in attendance and church response.\nBishop Sarah Mullally has insisted that declining numbers at services should not necessarily be a cause of despair for churches, because people may still encounter God without attending a service in a church; for example hearing the Christian message through social media sites or in a caf\u00e9 run as a community project. Additionally, 9.7\u00a0million people visit at least one of its churches every year and 1\u00a0million students are educated at Church of England schools (which number 4,700). In 2019, an estimated 10 million people visited a cathedral and an additional \"1.3 million people visited Westminster Abbey, where 99% of visitors paid / donated for entry\". In 2022, the church reported than an estimated 5.7 million people visited a cathedral and 6.8 million visited Westminster Abbey. Nevertheless, the archbishops of Canterbury and York warned in January 2015 that the Church of England would no longer be able to carry on in its current form unless the downward spiral in membership were somehow to be reversed, as typical Sunday attendance had halved to 800,000 in the previous 40 years:\nBetween 1969 and 2010, almost 1,800 church buildings, roughly 11% of the stock, were closed (so-called \"redundant churches\"); the majority (70%) in the first half of the period; only 514 being closed between 1990 and 2010. Some active use was being made of about half of the closed churches. By 2019 the rate of closure had steadied at around 20 to 25 per year (0.2%); some being replaced by new places of worship. Additionally, in 2018 the church announced a \u00a327\u00a0million growth programme to create 100 new churches.\nLow salaries.\nIn 2015 the Church of England admitted that it was embarrassed to be paying staff under the living wage. The Church of England had previously campaigned for all employers to pay this minimum amount. The archbishop of Canterbury acknowledged it was not the only area where the church \"fell short of its standards\".\nImpact of COVID-19 pandemic.\nThe COVID-19 pandemic had a sizeable effect on church attendance, with attendance in 2020 and 2021 well below that of 2019. By 2022, the first full year without substantial restrictions related to the pandemic, numbers were still notably down on pre-pandemic participation. According to the 2022 release of \"Statistics for Mission\" by the church, the median size of each church's worshipping community (those who attend in person or online at least once a month) stood at 37 people, with average weekly attendance having declined from 34 to 25; while Easter and Christmas services had seen falls from 51 to 38 and 80 to 56 individuals respectively. In the following year's release the Church detailed that, compared to pre-pandemic trends, the Church's average weekly attendance was 8% below what was expected. However, attendance at particular services such as baptisms and marriages had increased by more than 20% compared to the projected pre-pandemic trend.\nExamples of wider declines across the whole church include:\nDoctrine and practice.\nThomas Cranmer, the guiding Protestant Reformer who shaped Anglican doctrine after the English Reformation, was instrumental in the compilation of the \"Thirty-Nine Articles of Religion\", \"Book of Common Prayer\" and \"Books of Homilies\". The canon law of the Church of England identifies the Christian scriptures as the source of its doctrine. In addition, doctrine is also derived from the teachings of the Church Fathers and ecumenical councils (as well as the ecumenical creeds) in so far as these agree with scripture. This doctrine is expressed in the \"Thirty-Nine Articles of Religion\", the \"Book of Common Prayer\", and the Ordinal containing the rites for the ordination of deacons, priests, and the consecration of bishops. Richard Hooker's appeal to scripture as the primary source of Christian doctrine, informed by church tradition, and reason, has been influential in hermeneutics.\nThe Church of England's doctrinal character today is largely the result of the Elizabethan Settlement. The historical development of Anglicanism saw itself as navigating a via media between two forms of Protestantism\u2014Lutheranism and Reformed Christianity\u2014though leaning closer to the latter than the former. The Church of England affirms the protestant reformation principle that scripture contains all things necessary to salvation and is the final arbiter in doctrinal matters. The Thirty-nine Articles are the church's only official confessional statement. The Church of England did retain three orders of ministry and the apostolic succession of bishops, as with the Scandinavian Lutheran Churches (such as the Church of Sweden) and Roman Catholicism. Its identity has thus been described as Reformed and Catholic. There are differences of opinion within the Church of England over the necessity of episcopacy. Some consider it essential, while others feel it is needed for the proper ordering of the church. The Bible, the Creeds, Apostolic Order, and the administration of the Sacraments are sufficient to establish catholicity. The Protestant Reformation in England was initially much concerned about doctrine but the Elizabethan Settlement tried to put a stop to doctrinal contentions. The proponents of further changes, nonetheless, tried to get their way by making changes in Church Order (abolition of bishops), governance (Canon Law) and liturgy. They did not succeed because the monarchy and the Church resisted and the majority of the population were indifferent. Moreover, \"despite all the assumptions of the Reformation founders of that Church, it had retained a catholic character.\" The existence of cathedrals \"without substantial alteration\" and \"where the \"old devotional world cast its longest shadow for the future of the ethos that would become Anglicanism,\" This is \"One of the great mysteries of the English Reformation,\" that there was no complete break with the past but a muddle that was per force turned into a virtue.\nThe Church of England has, as one of its distinguishing marks, a breadth of opinion from liberal to conservative clergy and members. This tolerance has allowed Anglicans who emphasise the catholic tradition and others who emphasise the reformed tradition to coexist. The three schools of thought (or parties) in the Church of England are sometimes called high church (or Anglo-Catholic), low church (or evangelical Anglican) and broad church (or liberal). The high church party places importance on the Church of England's continuity with the pre-Reformation Catholic Church, adherence to ancient liturgical usages and the sacerdotal nature of the priesthood. As their name suggests, Anglo-Catholics maintain many traditional catholic practices and liturgical forms. The Catholic tradition, strengthened and reshaped from the 1830s by the Oxford movement, has stressed the importance of the visible Church and its sacraments and the belief that the ministry of bishops, priests and deacons is a sign and instrument of the Church of England's Catholic and apostolic identity. The low church party is more Protestant in both ceremony and theology. It has emphasized the significance of the Protestant aspects of the Church of England's identity, stressing the importance of the authority of Scripture, preaching, justification by faith and personal conversion. The theological perspectives within the Church of England have included the Reformed Anglican perspective, as well as a minority Arminian Anglican view. Historically, the term 'broad church' has been used to describe those of middle-of-the-road ceremonial preferences who lean theologically towards liberal protestantism. The liberal broad church tradition has emphasized the importance of the use of reason in theological exploration. It has stressed the need to develop Christian belief and practice in order to respond creatively to wider advances in human knowledge and understanding and the importance of social and political action in forwarding God's kingdom. The balance between these strands of churchmanship is not static: in 2013, 40% of Church of England worshippers attended evangelical Anglican churches (compared with 26% in 1989), and 83% of very large congregations were evangelical. Such churches were also reported to attract higher numbers of men and young adults than others.\nWorship and liturgy.\nIn 1604, James\u00a0I ordered an English language translation of the Bible known as the King James Version, which was published in 1611 and authorised for use in parishes, although it was not an \"official\" version per se. The Church of England's official book of liturgy as established in English Law is the 1662 version of the \"Book of Common Prayer\" (BCP). In the year 2000, the General Synod approved a modern liturgical book, \"Common Worship\", which can be used as an alternative to the BCP. Like its predecessor, the 1980 \"Alternative Service Book\", it differs from the \"Book of Common Prayer\" in providing a range of alternative services, mostly in modern language, although it does include some BCP-based forms as well, for example Order Two for Holy Communion. (This is a revision of the BCP service, altering some words and allowing the insertion of some other liturgical texts such as the \"Agnus Dei\" before communion.) The Order One rite follows the pattern of more modern liturgical scholarship.\nThe liturgies are organised according to the traditional liturgical year and the calendar of saints. The sacraments of baptism and the eucharist are generally thought necessary to salvation. Infant baptism is practised. At a later age, individuals baptised as infants receive confirmation by a bishop, at which time they reaffirm the baptismal promises made by their parents or sponsors. The eucharist, consecrated by a thanksgiving prayer including Christ's Words of Institution, is believed to be \"a memorial of Christ's once-for-all redemptive acts in which Christ is objectively present and effectually received in faith\".\nThe use of hymns and music in the Church of England has changed dramatically over the centuries. Traditional Choral evensong is a staple of most cathedrals. The style of psalm chanting harks back to the Church of England's pre-reformation roots. During the 18th century, clergy such as Charles Wesley introduced their own styles of worship with poetic hymns.\nIn the latter half of the 20th century, the influence of the Charismatic Movement significantly altered the worship traditions of numerous Church of England parishes, primarily affecting those of evangelical persuasion. These churches now adopt a contemporary worship form of service, with minimal liturgical or ritual elements, and incorporating contemporary worship music.\nJust as the Church of England has a large conservative or \"traditionalist\" wing, it also has many liberal members and clergy. Approximately one third of clergy \"doubt or disbelieve in the physical resurrection\". Others, such as Giles Fraser, a contributor to \"The Guardian\", have argued for an allegorical interpretation of the virgin birth of Jesus. \"The Independent\" reported in 2014 that, according to a YouGov survey of Church of England clergy, \"as many as 16 per cent are unclear about God and two per cent think it is no more than a human construct.\" Moreover, many congregations are seeker-friendly environments. For example, one report from the Church Mission Society suggested that the church open up \"a pagan church where Christianity [is] very much in the centre\" to reach out to spiritual people.\nThe Church of England is launching a project on \"gendered language\" in Spring 2023 in efforts to \"study the ways in which God is referred to and addressed in liturgy and worship\".\nWomen's ministry.\nWomen were appointed as deaconesses from 1861, but they could not function fully as deacons and were not considered ordained clergy. Women have historically been able to serve as lay readers. During the First World War, some women were appointed as lay readers, known as \"bishop's messengers\", who also led missions and ran churches in the absence of men. After the war, no women were appointed as lay readers until 1969.\nLegislation authorising the ordination of women as deacons was passed in 1986 and they were first ordained in 1987. The ordination of women as priests was approved by the General Synod in 1992 and began in 1994. In 2010, for the first time in the history of the Church of England, more women than men were ordained as priests (290 women and 273 men), but in the next two years, ordinations of men again exceeded those of women.\nIn July 2005, the synod voted to \"set in train\" the process of allowing the consecration of women as bishops. In February 2006, the synod voted overwhelmingly for the \"further exploration\" of possible arrangements for parishes that did not want to be directly under the authority of a bishop who is a woman. On 7 July 2008, the synod voted to approve the ordination of women as bishops and rejected moves for alternative episcopal oversight for those who do not accept the ministry of bishops who are women. Actual ordinations of women to the episcopate required further legislation, which was narrowly rejected in a General Synod vote in November 2012. On 20 November 2013, the General Synod voted overwhelmingly in support of a plan to allow the ordination of women as bishops, with 378 in favour, 8 against and 25 abstentions.\nOn 14 July 2014, the General Synod approved the ordination of women as bishops. The House of Bishops recorded 37 votes in favour, two against with one abstention. The House of Clergy had 162 in favour, 25 against and four abstentions. The House of Laity voted 152 for, 45 against with five abstentions. This legislation had to be approved by the Ecclesiastical Committee of the Parliament before it could be finally implemented at the November 2014 synod. In December 2014, Libby Lane was announced as the first woman to become a bishop in the Church of England. She was consecrated as a bishop in January 2015. In July 2015, Rachel Treweek was the first woman to become a diocesan bishop in the Church of England when she became the Bishop of Gloucester. She and Sarah Mullally, Bishop of Crediton, were the first women to be ordained as bishops at Canterbury Cathedral. Treweek later made headlines by calling for gender-inclusive language, saying that \"God is not to be seen as male. God is God.\"\nIn May 2018, the Diocese of London consecrated Dame Sarah Mullally as the first woman to serve as the Bishop of London. Bishop Sarah Mullally occupies the third most senior position in the Church of England. Mullally has described herself as a feminist and will ordain both men and women to the priesthood. She is also considered by some to be a theological liberal. On women's reproductive rights, Mullally describes herself as pro-choice while also being personally pro-life. On marriage, she supports the current stance of the Church of England that marriage is between a man and a woman, but also said that: \"It is a time for us to reflect on our tradition and scripture, and together say how we can offer a response that is about it being inclusive love.\"\nSame-sex unions and LGBT clergy.\nThe Church of England has been discussing same-sex marriages and LGBT clergy. The church holds that marriage is a union of one man with one woman. The church does not allow clergy to perform same-sex marriages, but in February 2023 approved of blessings for same-sex couples following a civil marriage or civil partnership. The church teaches \"Same-sex relationships often embody genuine mutuality and fidelity.\" In January 2023, the Bishops approved \"prayers of thanksgiving, dedication and for God's blessing for same-sex couples.\" The commended prayers of blessing for same-sex couples, known as \"Prayers of Love and Faith,\" may be used during ordinary church services, and in November 2023 General Synod voted to authorise \"standalone\" blessings for same-sex couples on a trial basis, while permanent authorisation will require additional steps. The church also officially supports celibate civil partnerships; \"We believe that Civil Partnerships still have a place, including for some Christian LGBTI couples who see them as a way of gaining legal recognition of their relationship.\"\nCivil partnerships for clergy have been allowed since 2005, so long as they remain sexually abstinent, and the church extends pensions to clergy in same-sex civil partnerships. In a missive to clergy, the church communicated that \"there was a need for committed same-sex couples to be given recognition and 'compassionate attention' from the Church, including special prayers.\" \"There is no prohibition on prayers being said in church or there being a 'service'\" after a civil union. After same-sex marriage was legalised, the church sought continued availability of civil unions, saying \"The Church of England recognises that same-sex relationships often embody fidelity and mutuality. Civil partnerships enable these Christian virtues to be recognised socially and legally in a proper framework.\" In 2024, the General Synod voted in support of eventually permitting clergy to enter into civil same-sex marriages.\nIn 2014, the bishops released guidelines that permit \"more informal kind of prayer\" for couples. In the guidelines, \"gay couples who get married will be able to ask for special prayers in the Church of England after their wedding, the bishops have agreed.\" In 2016, the bishop of Grantham, Nicholas Chamberlain, announced that he is gay, in a same-sex relationship and celibate, becoming the first bishop to do so in the church. The church had decided in 2013 that gay clergy in civil partnerships so long as they remain sexually abstinent could become bishops. \"The House [of Bishops] has confirmed that clergy in civil partnerships, and living in accordance with the teaching of the church on human sexuality, can be considered as candidates for the episcopate.\"\nIn 2017, the House of Clergy voted against the motion to \"take note\" of the bishops' report defining marriage as between a man and a woman. Due to passage in all three houses being required, the motion was rejected. After General Synod rejected the motion, the archbishops of Canterbury and York called for \"radical new Christian inclusion\" that is \"based on good, healthy, flourishing relationships, and in a proper 21st century understanding of being human and of being sexual.\" The church officially opposes \"conversion therapy\", a practice which attempts to change a gay or lesbian person's sexual orientation, calling it unethical and supports the banning of \"conversion therapy\" in the UK. The Diocese of Hereford approved a motion calling for the church \"to create a set of formal services and prayers to bless those who have had a same-sex marriage or civil partnership.\" In 2022, \"The House [of Bishops] also agreed to the formation of a Pastoral Consultative Group to support and advise dioceses on pastoral responses to circumstances that arise concerning LGBTI+ clergy, ordinands, lay leaders and the lay people in their care.\"\nRegarding transgender issues, the 2017 General Synod voted in favour of a motion saying that transgender people should be \"welcomed and affirmed in their parish church\". The motion also asked the bishops \"to look into special services for transgender people.\" The bishops initially said \"the House notes that the Affirmation of Baptismal Faith, found in \"Common Worship\", is an ideal liturgical rite which trans people can use to mark this moment of personal renewal.\" The Bishops also authorised services of celebration to mark a gender transition that will be included in formal liturgy. Transgender people may marry in the Church of England after legally making a transition. \"Since the Gender Recognition Act 2004, trans people legally confirmed in their gender identity under its provisions are able to marry someone of the opposite sex in their parish church.\" The church further decided that same-gender couples may remain married when one spouse experiences gender transition provided that the spouses identified as opposite genders at the time of the marriage. Since 2000, the church has allowed priests to undergo gender transition and remain in office. The church has ordained openly transgender clergy since 2005. The Church of England ordained the church's first openly non-binary priest.\nIn January 2023, a meeting of the Bishops of the Church of England rejected demands for clergy to conduct same-sex marriages. However, proposals would be put to the General Synod that clergy should be able to hold church blessings for same-sex civil marriages, albeit on a voluntary basis for individual clergy. This comes as the Church continued to be split on same-sex marriages.\nIn February 2023, ten archbishops of the Global South Fellowship of Anglican Churches released a statement stating that they had broken communion and no longer recognised Justin Welby as \"the first among equals\" or \"primus inter pares\" in the Anglican Communion in response to the General Synod's decision to approve the blessing of same-sex couples following a civil marriage or partnership, leading to questions as to the status of the Church of England as the mother church of the international Anglican Communion.\nIn November 2023, the General Synod narrowly voted to allow church blessings for same-sex couples on a trial basis. In December 2023, the first blessings of same-sex couples began in the Church of England. In 2024, the General Synod voted to support moving forward with \"stand-alone\" services of blessing for same-sex couples after a civil marriage or civil partnership.\nBioethics issues.\nThe Church of England is generally opposed to abortion but believes \"there can be strictly limited conditions under which abortion may be morally preferable to any available alternative\". The church also opposes euthanasia. Its official stance is that \"While acknowledging the complexity of the issues involved in assisted dying/suicide and voluntary euthanasia, the Church of England is opposed to any change in the law or in medical practice that would make assisted dying/suicide or voluntary euthanasia permissible in law or acceptable in practice.\" It also states that \"Equally, the Church shares the desire to alleviate physical and psychological suffering, but believes that assisted dying/suicide and voluntary euthanasia are not acceptable means of achieving these laudable goals.\" In 2014, George Carey, a former archbishop of Canterbury, announced that he had changed his stance on euthanasia and now advocated legalising \"assisted dying\". On embryonic stem-cell research, the church has announced \"cautious acceptance to the proposal to produce cytoplasmic hybrid embryos for research\".\nIn the 19th century, English law required the burial of people who had died by suicide to occur only between the hours of 9 p.m. and midnight and without religious rites. The Church of England permitted the use of alternative burial services for people who had died by suicide. In 2017, the Church of England changed its rules to permit the full, standard Christian burial service regardless of whether a person had died by suicide.\nSocial work.\nChurch Urban Fund.\nThe Church of England set up the Church Urban Fund in the 1980s to tackle poverty and deprivation. It sees poverty as trapping individuals and communities with some people in urgent need, leading to dependency, homelessness, hunger, isolation, low income, mental health problems, social exclusion and violence. They feel that poverty reduces confidence and life expectancy and that people born in poor conditions have difficulty escaping their disadvantaged circumstances.\nChild poverty.\nIn parts of Liverpool, Manchester and Newcastle two-thirds of babies are born to poverty and have poorer life chances, also a life expectancy 15 years lower than babies born in the best-off fortunate communities.\nAction on hunger.\nMany prominent people in the Church of England have spoken out against poverty and welfare cuts in the United Kingdom. Twenty-seven bishops are among 43 Christian leaders who signed a letter which urged David Cameron to make sure people have enough to eat.\nThousands of UK citizens use food banks. The church's campaign to end hunger considers this \"truly shocking\" and called for a national day of fasting on 4 April 2014.\nMembership.\n, the Church of England estimated that it had approximately 25 - 26\u00a0million baptised members \u2013 about 47% of the English population. This number has remained consistent since 2001 and was cited again in 2013 and 2014. In 2010, the government estimated that there were 24,841,000 baptised members of the Church of England. According to a 2016 study published by the \"Journal of Anglican Studies\", the Church of England continued to claim 26 million baptised members, while it also had approximately 1.7 million active baptised members. Due to its status as the established church, in general, anyone may be married, have their children baptised or their funeral in their local parish church, regardless of whether they are baptised or regular churchgoers.\nBetween 1890 and 2001, churchgoing in the United Kingdom declined steadily. In the years 1968 to 1999, Anglican Sunday church attendances almost halved, from 3.5 percent of the population to 1.9 per cent. By 2014, Sunday church attendances had declined further to 1.4 per cent of the population. One study published in 2008 suggested that if current trends continued, Sunday attendances could fall to 350,000 in 2030 and 87,800 in 2050. The Church of England releases an annual publication, Statistics for Mission, detailing numerous criteria relating to participation with the church. Below is a snapshot of several key metrics from every five years since 2001 (2022 has been used in place of 2021 to avoid the impact of Covid restrictions). Since 2021 Sunday Church attendance has increased, although not to pre-pandemic levels. \nPersonnel.\nIn 2020, there were almost 20,000 active clergy serving in the Church of England, including 7,200 retired clergy who continued to serve. In that year, 580 were ordained (330 in stipendiary posts and 250 in self-supporting parochial posts) and a further 580 ordinands began their training. In that year, 33% of those in ordained ministry were female, an increase from the 26% reported in 2016.\nStructure.\nArticle XIX ('Of the Church') of the Thirty-nine Articles defines the church as follows:\nThe British monarch has the constitutional title of Supreme Governor of the Church of England. The canon law of the Church of England states, \"We acknowledge that the King's most excellent Majesty, acting according to the laws of the realm, is the highest power under God in this kingdom, and has supreme authority over all persons in all causes, as well ecclesiastical as civil.\" In practice this power is often exercised through Parliament and on the advice of the Prime Minister.\nThe Church of Ireland and the Church in Wales separated from the Church of England in 1869 and 1920 respectively and are autonomous churches in the Anglican Communion; Scotland's national church, the Church of Scotland, is Presbyterian, but the Scottish Episcopal Church is part of the Anglican Communion.\nIn addition to England, the jurisdiction of the Church of England extends to the Isle of Man, the Channel Islands and a few parishes in Flintshire, Monmouthshire and Powys in Wales which voted to remain with the Church of England rather than joining the Church in Wales. Expatriate congregations on the continent of Europe have become the Diocese of Gibraltar in Europe.\nThe church is structured as follows (from the lowest level upwards):\nAll rectors and vicars are appointed by patrons, who may be private individuals, corporate bodies such as cathedrals, colleges or trusts, or by the bishop or directly by the Crown. No clergy can be instituted and inducted into a parish without swearing the Oath of Allegiance to His Majesty, and taking the Oath of Canonical Obedience \"in all things lawful and honest\" to the bishop. Usually they are instituted to the benefice by the bishop and then inducted by the archdeacon into the possession of the benefice property\u2014church and parsonage. Curates (assistant clergy) are appointed by rectors and vicars, or if priests-in-charge by the bishop after consultation with the patron. Cathedral clergy (normally a dean and a varying number of residentiary canons who constitute the cathedral chapter) are appointed either by the Crown, the bishop, or by the dean and chapter themselves. Clergy officiate in a diocese either because they hold office as beneficed clergy or are licensed by the bishop when appointed, or simply with permission.\nPrimates.\nThe most senior bishop of the Church of England is the Archbishop of Canterbury, who is the metropolitan of the southern province of England, the Province of Canterbury. He has the status of Primate of All England. He is the focus of unity for the worldwide Anglican Communion of independent national or regional churches. Justin Welby was the most recent Archbishop of Canterbury, from 04 February 2013 to his resignation effective on 06 January 2025.\nThe second most senior bishop is the Archbishop of York, who is the metropolitan of the northern province of England, the Province of York. For historical reasons (relating to the time of York's control by the Danes) he is referred to as the Primate of England. Stephen Cottrell became Archbishop of York in 2020. The Bishop of London, the Bishop of Durham and the Bishop of Winchester are ranked in the next three positions, insofar as the holders of those sees automatically become members of the House of Lords.\nDiocesan bishops.\nThe process of appointing diocesan bishops is complex, due to historical reasons balancing hierarchy against democracy, and is handled by the Crown Nominations Committee which submits names to the Prime Minister (acting on behalf of the Crown) for consideration.\nRepresentative bodies.\nThe Church of England has a legislative body, General Synod. This can create two types of legislation, measures and canons. Measures have to be approved but cannot be amended by the British Parliament before receiving royal assent and becoming part of the law of England. Although it is the established church in England only, its measures must be approved by both Houses of Parliament including the non-English members. Canons require Royal Licence and Royal Assent, but form the law of the church, rather than the law of the land.\nAnother assembly is the Convocation of the English Clergy, which is older than the General Synod and its predecessor the Church Assembly. By the Synodical Government Measure 1969 almost all of the Convocations' functions were transferred to the General Synod. Additionally, there are Diocesan Synods and deanery synods, which are the governing bodies of the divisions of the Church.\nHouse of Lords.\nOf the 42 diocesan archbishops and bishops in the Church of England, 26 are permitted to sit in the House of Lords. The Archbishops of Canterbury and York automatically have seats, as do the bishops of London, Durham and Winchester. The remaining 21 seats are filled in order of seniority by date of consecration. It may take a diocesan bishop a number of years to reach the House of Lords, at which point he or she becomes a Lord Spiritual. The Bishop of Sodor and Man and the Bishop of Gibraltar in Europe are not eligible to sit in the House of Lords as their dioceses lie outside the United Kingdom.\nCrown Dependencies.\nAlthough they are not part of England or the United Kingdom, the Church of England is also the established church in the Crown Dependencies of the Isle of Man, the Bailiwick of Jersey and the Bailiwick of Guernsey. The Isle of Man has its own diocese of Sodor and Man, and the Bishop of Sodor and Man is an ex officio member of the legislative council of the Tynwald on the island. Historically the Channel Islands have been under the authority of the Bishop of Winchester, but this authority has temporarily been delegated to the Bishop of Dover since 2015. In Jersey the Dean of Jersey is a non-voting member of the States of Jersey. In Guernsey the Church of England is the established church, although the Dean of Guernsey is not a member of the States of Guernsey.\nSex abuse.\nThe 2020 report from the Independent Inquiry into Child Sexual Abuse found several cases of sexual abuse within the Church of England, and concluded that the Church did not protect children from sexual abuse, and allowed abusers to hide. The Church spent more effort defending alleged abusers than supporting victims or protecting children and young people. Allegations were not taken seriously, and in some cases clergymen were ordained even with a history of child sex abuse. Bishop Peter Ball was convicted in October 2015 on several charges of indecent assault against young adult men.\nIn June 2023, the Archbishops' Council dismissed the three board members of the Independent Safeguarding Board, which was set up in 2021 \"to hold the Church to account, publicly if needs be, for any failings which are preventing good safeguarding from happening\". A statement issued by the Archbishops of Canterbury and York referred to there being \"no prospect of resolving the disagreement and that it is getting in the way of the vital work of serving victims and survivors\". Jasvinder Sanghera and Steve Reeves, the two independent members of the board, had complained about interference with their work by the Church. The Bishop of Birkenhead, Julie Conalty, speaking to BBC Radio 4 in connection with the dismissals, said: \"I think culturally we are resistant as a church to accountability, to criticism. And therefore I don't entirely trust the church, even though I'm a key part of it and a leader within it, because I see the way the wind blows is always in a particular direction.\"\nOn 20 July 2023, it was announced that the archbishops of Canterbury and York had appointed Alexis Jay to provide proposals for an independent system of safeguarding for the Church of England.\nFunding and finances.\nAlthough an established church, the Church of England does not receive any direct government support, except some funding for building work. Donations comprise its largest source of income, and it also relies heavily on the income from its various historic endowments. In 2005, the Church of England had estimated total outgoings of around \u00a3900\u00a0million.\nThe Church of England manages an investment portfolio which is worth more than \u00a38\u00a0billion.\nOnline church directories.\nThe Church of England runs \"A Church Near You\", an online directory of churches. A user-edited resource, it currently lists more than 16,000 churches and has 20,000 editors in 42 dioceses. The directory enables parishes to maintain accurate location, contact and event information, which is shared with other websites and mobile apps. The site allows the public to find their local worshipping community, and offers churches free resources, such as hymns, videos and social media graphics.\nThe \"Church Heritage Record\" includes information on over 16,000 church buildings, including architectural history, archaeology, art history, and the surrounding natural environment. It can be searched by elements including church name, diocese, date of construction, footprint size, listing grade, and church type. The types of church identified include:"}
{"id": "5956", "revid": "1266242386", "url": "https://en.wikipedia.org/wiki?curid=5956", "title": "Circe", "text": "Circe (; : \"K\u00edrk\u0113\") is an enchantress and a minor goddess in ancient Greek mythology and religion. In most accounts, Circe is described as the daughter of the sun god Helios and the Oceanid nymph Perse. Circe was renowned for her vast knowledge of potions and herbs. Through the use of these and a magic wand or staff, she would transform her enemies, or those who offended her, into animals.\nThe best known of her legends is told in Homer's \"Odyssey\" when Odysseus visits her island of Aeaea on the way back from the Trojan War and she changes most of his crew into swine. He manages to persuade her to return them to human shape, lives with her for a year and has sons by her, including Latinus and Telegonus. Her ability to change others into animals is further highlighted by the story of Picus, an Italian king whom she turns into a woodpecker for resisting her advances. Another story tells of her falling in love with the sea-god Glaucus, who prefers the nymph Scylla to her. In revenge, Circe poisoned the water where her rival bathed and turned her into a dreadful monster.\nDepictions, even in Classical times, diverged from the detail in Homer's narrative, which was later to be reinterpreted morally as a cautionary story against drunkenness. Early philosophical questions were also raised about whether the change from being a human endowed with reason to being an unreasoning beast might not be preferable after all, and the resulting debate was to have a powerful impact during the Renaissance. Circe was also taken as the archetype of the predatory female. In the eyes of those from a later age, this behaviour made her notorious both as a magician and as a type of sexually free woman. She has been frequently depicted as such in all the arts from the Renaissance down to modern times.\nWestern paintings established a visual iconography for the figure, but also went for inspiration to other stories concerning Circe that appear in Ovid's \"Metamorphoses\". The episodes of Scylla and Picus added the vice of violent jealousy to her bad qualities and made her a figure of fear as well as of desire.\nClassical literature.\nFamily and attributes.\nBy most accounts, she was the daughter of the sun god Helios and Perse, one of the three thousand Oceanid nymphs. In \"Orphic Argonautica\", her mother is called Asterope instead. Her brothers were Ae\u00ebtes, keeper of the Golden Fleece and father of Medea, and Perses. Her sister was Pasipha\u00eb, the wife of King Minos and mother of the Minotaur. Other accounts make her and her niece Medea the daughters of Hecate, the goddess of witchcraft by Ae\u00ebtes, usually said to be her brother instead. She was often confused with Calypso, due to her shifts in behavior and personality, and the association that both of them had with Odysseus.\nAccording to Greek legend, Circe lived on the island of Aeaea. Although Homer is vague when it comes to the island's whereabouts, the early 3rd BC author Apollonius of Rhodes's epic poem \"Argonautica\" locates Aeaea somewhere south of \"Aethalia\" (Elba), within view of the Tyrrhenian shore (that is, the western coast of Italy). In the same poem, Circe's brother Ae\u00ebtes describes how Circe was transferred to Aeaea: \"I noted it once after taking a ride in my father Helios' chariot, when he was taking my sister Circe to the western land and we came to the coast of the Tyrrhenian mainland, where she dwells to this day, very far from the Colchian land.\" A scholiast on Apollonius Rhodius claims that Apollonius is following Hesiod's tradition in making Circe arrive in Aeaea on Helios' chariot, while Valerius Flaccus writes that Circe was borne away by winged dragons. Roman poets associated her with the most ancient traditions of Latium, and made her home to be on the promontory of Circeo.\nHomer describes Circe as \"a dreadful goddess with lovely hair and human speech\". Apollonius writes that she (just like every other descendant of Helios) had flashing golden eyes that shot out rays of light, with the author of \"Argonautica Orphica\" noting that she had hair like fiery rays. Ovid's \"The Cure for Love\" implies that Circe might have been taught the knowledge of herbs and potions from her mother Perse, who seems to have had similar skills.\nPre-Odyssey.\nIn the \"Argonautica\", Apollonius relates that Circe purified the Argonauts for the murder of Medea's brother Absyrtus, possibly reflecting an early tradition. In this poem, the Argonauts find Circe bathing in salt water; the animals that surround her are not former lovers transformed but primeval \"beasts, not resembling the beasts of the wild, nor yet like men in body, but with a medley of limbs.\" Circe invites Jason, Medea and their crew into her mansion; uttering no words, they show her the still bloody sword they used to cut Absyrtus down, and Circe immediately realizes they have visited her to be purified of murder. She purifies them by slitting the throat of a suckling pig and letting the blood drip on them. Afterwards, Medea tells Circe their tale in great detail, albeit omitting the part of Absyrtus' murder; nevertheless Circe is not fooled, and greatly disapproves of their actions. However, out of pity for the girl, and on account of their kinship, she promises not to be an obstacle on their way, and orders Jason and Medea to leave her island immediately.\nThe sea-god Glaucus was in love with a beautiful maiden, Scylla, but she spurned his affections no matter how he tried to win her heart. Glaucus went to Circe, and asked her for a magic potion to make Scylla fall in love with him too. But Circe was smitten by Glaucus herself, and fell in love with him. Glaucus did not love her back, and turned down her offer of marriage. Enraged, Circe used her knowledge of herbs and plants to take her revenge; she found the spot where Scylla usually took her bath, and poisoned the water. When Scylla went down to it to bathe, dogs sprang from her thighs and she was transformed into the familiar monster from the \"Odyssey\". In another, similar story, Picus was a Latian king whom Circe turned into a woodpecker. He was the son of Saturn, and a king of Latium. He fell in love and married a nymph, Canens, to whom he was utterly devoted. One day as he was hunting boars, he came upon Circe, who was gathering herbs in the woods. Circe fell immediately in love with him; but Picus, just like Glaucus before him, spurned her and declared that he would remain forever faithful to Canens. Circe, furious, turned Picus into a woodpecker. His wife Canens eventually wasted away in her mourning.\nDuring the war between the gods and the giants, one of the giants, Picolous, fled the battle against the gods and came to Aeaea, Circe's island. He attempted to chase Circe away, only to be killed by Helios, Circe's ally and father. From the blood of the slain giant, a herb came into existence; moly, named thus from the battle (malos) and with a white-coloured flower, either for the white Sun who had killed Picolous or the terrified Circe who turned white; the very plant, which mortals are unable to pluck from the ground, that Hermes would later give to Odysseus in order to defeat Circe.\nHomer's \"Odyssey\".\nIn Homer's \"Odyssey\", an 8th-century BC sequel to his Trojan War epic \"Iliad\", Circe is initially described as a beautiful goddess living in a palace isolated in the midst of a dense wood on her island of Aeaea. Around her home prowl strangely docile lions and wolves. She lures any who land on the island to her home with her lovely singing while weaving on an enormous loom, but later drugs them so that they change shape. One of her Homeric epithets is \"polypharmakos\", \"knowing many drugs or charms\".\nCirce invites the hero Odysseus' crew to a feast of familiar food, a pottage of cheese and meal, sweetened with honey and laced with wine, but also mixed with one of her magical potions that turns them into swine. Only Eurylochus, who suspects treachery, does not go in. He escapes to warn Odysseus and the others who have remained with the ship. Before Odysseus reaches Circe's palace, Hermes, the messenger god sent by the goddess of wisdom Athena, intercepts him and reveals how he might defeat Circe in order to free his crew from their enchantment. Hermes provides Odysseus with moly to protect him from Circe's magic. He also tells Odysseus that he must then draw his sword and act as if he were going to attack her. From there, as Hermes foretold, Circe would ask Odysseus to bed, but Hermes advises caution, for the treacherous goddess could still \"unman\" him unless he has her swear by the names of the gods that she will not take any further action against him. Following this advice, Odysseus is able to free his men.\nAfter they have all remained on the island for a year, Circe advises Odysseus that he must first visit the Underworld, something a mortal has never yet done, in order to gain knowledge about how to appease the gods, return home safely and recover his kingdom. Circe also advises him on how this might be achieved and furnishes him with the protections he will need and the means to communicate with the dead. On his return, she further advises him about two possible routes home, warning him, however, that both carry great danger.\nPost-Odyssey.\nTowards the end of Hesiod's \"Theogony\" (c. 700 BC), it is stated that Circe bore Odysseus three sons: Agrius (otherwise unknown); Latinus; and Telegonus, who ruled over the Tyrsenoi, that is the Etruscans. The \"Telegony\", an epic now lost, relates the later history of the last of these. Circe eventually informed her son who his absent father was and, when he set out to find Odysseus, gave him a poisoned spear. When Telegonus arrived in Ithaca, Odysseus was away in Thesprotia, fighting the Brygi. Telegonus began to ravage the island; Odysseus came to defend his land. With the weapon Circe gave him, Telegonus killed his father unknowingly. Telegonus then brought back his father's corpse to Aeaea, together with Penelope and Odysseus' son by her, Telemachus. After burying Odysseus, Circe made the other three immortal.\nCirce married Telemachus, and Telegonus married Penelope by the advice of Athena. According to an alternative version depicted in Lycophron's 3rd-century BC poem \"Alexandra\" (and John Tzetzes' scholia on it), Circe used magical herbs to bring Odysseus back to life after he had been killed by Telegonus. Odysseus then gave Telemachus to Circe's daughter Cassiphone in marriage. Sometime later, Telemachus had a quarrel with his mother-in-law and killed her; Cassiphone then killed Telemachus to avenge her mother's death. On hearing of this, Odysseus died of grief.\nDionysius of Halicarnassus (1.72.5) cites Xenagoras, the 2nd-century BC historian, as claiming that Odysseus and Circe had three different sons: Rhomos, Anteias, and Ardeias, who respectively founded three cities called by their names: Rome, Antium, and Ardea. In the later 5th-century CE epic \"Dionysiaca\", its author Nonnus mentions Phaunus, Circe's son by the sea god Poseidon.\nOther works.\nThree ancient plays about Circe have been lost: the work of the tragedian Aeschylus and of the 4th-century BC comic dramatists Ephippus of Athens and Anaxilas. The first told the story of Odysseus' encounter with Circe. Vase paintings from the period suggest that Odysseus' half-transformed animal-men formed the chorus in place of the usual Satyrs. Fragments of Anaxilas also mention the transformation and one of the characters complains of the impossibility of scratching his face now that he is a pig.\nThe theme of Circe turning men into a variety of animals was elaborated by later writers. In his episodic work \"The Sorrows of Love\" (first century BC), Parthenius of Nicaea interpolated another episode into the time that Odysseus was staying with Circe. Pestered by the amorous attentions of King Calchus the Daunian, the sorceress invited him to a drugged dinner that turned him into a pig and then shut him up in her sties. He was only released when his army came searching for him on the condition that he would never set foot on her island again.\nAmong Latin treatments, Virgil's \"Aeneid\" relates how Aeneas skirts the Italian island where Circe dwells and hears the cries of her many male victims, who now number more than the pigs of earlier accounts: \"The roars of lions that refuse the chain, / The grunts of bristled boars, and groans of bears, / And herds of howling wolves that stun the sailors' ears.\" In Ovid's 1st-century poem \"Metamorphoses\", the fourth episode covers Circe's encounter with Ulysses (the Roman name of Odysseus), whereas book 14 covers the stories of Picus and Glaucus.\nPlutarch took up the theme in a lively dialogue that was later to have several imitators. Contained in his 1st-century \"Moralia\" is the Gryllus episode in which Circe allows Odysseus to interview a fellow Greek turned into a pig. After his interlocutor informs Odysseus that his present existence is preferable to the human, they engage in a philosophical dialogue in which every human value is questioned and beasts are proved to be of superior wisdom and virtue.\nAncient cult.\nStrabo writes that a tomb-shrine of Circe was attended in one of the Pharmacussae islands, off the coast of Attica, typical for hero-worship. Circe was also venerated in Mount Circeo, in the Italian peninsula, which took its name after her according to ancient legend. Strabo says that Circe had a shrine in the small town, and that the people there kept a bowl they claimed belonged to Odysseus. The promontory is occupied by ruins of a platform attributed with great probability to a temple of Venus or Circe.\nLater literature.\nGiovanni Boccaccio provided a digest of what was known of Circe during the Middle Ages in his \"De mulieribus claris\" (\"Famous Women\", 1361\u20131362). While following the tradition that she lived in Italy, he comments wryly that there are now many more temptresses like her to lead men astray.\nThere is a very different interpretation of the encounter with Circe in John Gower's long didactic poem \"Confessio Amantis\" (1380). Ulysses is depicted as deeper in sorcery and readier of tongue than Circe and through this means he leaves her pregnant with Telegonus. Most of the account deals with the son's later quest for and accidental killing of his father, drawing the moral that only evil can come of the use of sorcery.\nThe story of Ulysses and Circe was retold as an episode in Georg Rollenhagen's German verse epic, \"Froschmeuseler\" (\"The Frogs and Mice\", Magdeburg, 1595). In this 600-page expansion of the pseudo-Homeric \"Batrachomyomachia\", it is related at the court of the mice and takes up sections 5\u20138 of the first part.\nIn Lope de Vega's miscellany \"La Circe \u2013 con otras rimas y prosas\" (1624), the story of her encounter with Ulysses appears as a verse epic in three cantos. This takes its beginning from Homer's account, but it is then embroidered; in particular, Circe's love for Ulysses remains unrequited.\nAs \"Circe's Palace\", Nathaniel Hawthorne retold the Homeric account as the third section in his collection of stories from Greek mythology, \"Tanglewood Tales\" (1853). The transformed Picus continually appears in this, trying to warn Ulysses, and then Eurylochus, of the danger to be found in the palace, and is rewarded at the end by being given back his human shape. In most accounts Ulysses only demands this for his own men.\nIn her survey of the \"Transformations of Circe\", Judith Yarnall comments of this figure, who started out as a comparatively minor goddess of unclear origin, that \"What we know for certain \u2013 what Western literature attests to \u2013 is her remarkable staying power\u2026These different versions of Circe's myth can be seen as mirrors, sometimes clouded and sometimes clear, of the fantasies and assumptions of the cultures that produced them.\" After appearing as just one of the characters that Odysseus encounters on his wandering, \"Circe herself, in the twists and turns of her story through the centuries, has gone through far more metamorphoses than those she inflicted on Odysseus's companions.\" \nReasoning beasts.\nOne of the most enduring literary themes connected with the figure of Circe was her ability to change men into animals. There was much speculation concerning how this could be, whether the human consciousness changed at the same time, and even whether it was a change for the better. The Gryllus dialogue was taken up by another Italian writer, Giovan Battista Gelli, in his \"La Circe\" (1549). This is a series of ten philosophical and moral dialogues between Ulysses and the humans transformed into various animals, ranging from an oyster to an elephant, in which Circe sometimes joins. Most argue against changing back; only the last animal, a philosopher in its former existence, wants to. The work was translated into English soon after in 1557 by Henry Iden. Later the English poet Edmund Spenser also made reference to Plutarch's dialogue in the section of his \"Faerie Queene\" (1590) based on the Circe episode which appears at the end of Book II. Sir Guyon changes back the victims of Acrasia's erotic frenzy in the Bower of Bliss, most of whom are abashed at their fall from chivalric grace, \"But one above the rest in speciall, / That had an hog beene late, hight Grille by name, / Repined greatly, and did him miscall, / That had from hoggish forme him brought to naturall.\"\nTwo other Italians wrote rather different works that centre on the animal within the human. One was Niccol\u00f2 Machiavelli in his unfinished long poem, \"L'asino d'oro\" (\"The Golden Ass\", 1516). The author meets a beautiful herdswoman surrounded by Circe's herd of beasts. After spending a night of love with him, she explains the characteristics of the animals in her charge: the lions are the brave, the bears are the violent, the wolves are those forever dissatisfied, and so on (Canto 6). In Canto 7 he is introduced to those who experience frustration: a cat that has allowed its prey to escape; an agitated dragon; a fox constantly on the look-out for traps; a dog that bays the moon; Aesop's lion in love that allowed himself to be deprived of his teeth and claws. There are also emblematic satirical portraits of various Florentine personalities. In the eighth and last canto he has a conversation with a pig that, like the Gryllus of Plutarch, does not want to be changed back and condemns human greed, cruelty and conceit.\nThe other Italian author was the esoteric philosopher Giordano Bruno, who wrote in Latin. His \"Cantus Circaeus\" (\"The Incantation of Circe\") was the fourth work on memory and the association of ideas by him to be published in 1582. It contains a series of poetic dialogues, in the first of which, after a long series of incantations to the seven planets of the Hermetic tradition, most humans appear changed into different creatures in the scrying bowl. The sorceress Circe is then asked by her handmaiden Moeris about the type of behaviour with which each is associated. According to Circe, for instance, \"fireflies are the learned, wise, and illustrious amidst idiots, asses, and obscure men\" (Question 32). In later sections different characters discuss the use of images in the imagination in order to facilitate use of the art of memory, which is the real aim of the work.\nFrench writers were to take their lead from Gelli in the following century. Antoine Jacob wrote a one-act social comedy in rhyme, \"Les Bestes raisonnables\" (\"The Reasoning Beasts\", 1661) which allowed him to satirise contemporary manners. On the isle of Circe, Ulysses encounters an ass that was once a doctor, a lion that had been a valet, a female doe and a horse, all of whom denounce the decadence of the times. The ass sees human asses everywhere, \"Asses in the town square, asses in the suburbs, / Asses in the provinces, asses proud at court, / Asses browsing in the meadows, military asses trooping, / Asses tripping it at balls, asses in the theatre stalls.\" To drive the point home, in the end it is only the horse, formerly a courtesan, who wants to return to her former state.\nThe same theme occupies La Fontaine's late fable, \"The Companions of Ulysses\" (XII.1, 1690), which also echoes Plutarch and Gelli. Once transformed, every animal (which includes a lion, a bear, a wolf and a mole) protests that their lot is better and refuses to be restored to human shape. Charles Dennis shifted this fable to stand at the head of his translation of La Fontaine, \"Select Fables\" (1754), but provides his own conclusion that \"When Mortals from the path of Honour stray, / And the strong passions over reason sway, / What are they then but Brutes? / 'Tis vice alone that constitutes / Th'enchanting wand and magic bowl, The exterior form of Man they wear, / But are in fact both Wolf and Bear, / The transformation's in the Soul.\"\nLouis Fuzelier and Marc-Antoine Legrand titled their comic opera of 1718 \"Les animaux raisonnables\". It had more or less the same scenario transposed into another medium and set to music by Jacques Aubert. Circe, wishing to be rid of the company of Ulysses, agrees to change back his companions, but only the dolphin is willing. The others, who were formerly a corrupt judge (now a wolf), a financier (a pig), an abused wife (a hen), a deceived husband (a bull) and a flibbertigibbet (a linnet), find their present existence more agreeable.\nThe Venetian Gasparo Gozzi was another Italian who returned to Gelli for inspiration in the 14 prose \"Dialoghi dell'isola di Circe\" (\"Dialogues from Circe's Island\") published as journalistic pieces between 1760 and 1764. In this moral work, the aim of Ulysses in talking to the beasts is to learn more of the human condition. It includes figures from fable (The fox and the crow, XIII) and from myth to illustrate its vision of society at variance. Far from needing the intervention of Circe, the victims find their natural condition as soon as they set foot on the island. The philosopher here is not Gelli's elephant but the bat that retreats from human contact into the darkness, like Bruno's fireflies (VI). The only one who wishes to change in Gozzi's work is the bear, a satirist who had dared to criticize Circe and had been changed as a punishment (IX).\nThere were two more satirical dramas in later centuries. One modelled on the Gryllus episode in Plutarch occurs as a chapter of Thomas Love Peacock's late novel, \"Gryll Grange\" (1861), under the title \"Aristophanes in London\". Half Greek comedy, half Elizabethan masque, it is acted at the Grange by the novel's characters as a Christmas entertainment. In it Spiritualist mediums raise Circe and Gryllus and try to convince the latter of the superiority of modern times, which he rejects as intellectually and materially regressive. An Italian work drawing on the transformation theme was the comedy by Ettore Romagnoli, \"La figlia del Sole\" (\"The Daughter of the Sun\", 1919). Hercules arrives on the island of Circe with his servant Cercopo and has to be rescued by the latter when he too is changed into a pig. But, since the naturally innocent other animals had become corrupted by imitating human vices, the others who had been changed were refused when they begged to be rescued.\nAlso in England, Austin Dobson engaged more seriously with Homer's account of the transformation of Odysseus' companions when, though \"Head, face and members bristle into swine, / Still cursed with sense, their mind remains alone\". Dobson's \"\" (1640) depicts the horror of being imprisoned in an animal body in this way with the human consciousness unchanged. There appears to be no relief, for only in the final line is it revealed that Odysseus has arrived to free them. But in Matthew Arnold's dramatic poem \"The Strayed Reveller\" (1849), in which Circe is one of the characters, the power of her potion is differently interpreted. The inner tendencies unlocked by it are not the choice between animal nature and reason but between two types of impersonality, between divine clarity and the poet's participatory and tragic vision of life. In the poem, Circe discovers a youth laid asleep in the portico of her temple by a draught of her ivy-wreathed bowl. On awaking from possession by the poetic frenzy it has induced, he craves for it to be continued.\nSexual politics.\nWith the Renaissance there began to be a reinterpretation of what it was that changed the men, if it was not simply magic. For Socrates, in Classical times, it had been gluttony overcoming their self-control. But for the influential emblematist Andrea Alciato, it was unchastity. In the second edition of his \"Emblemata\" (1546), therefore, Circe became the type of the prostitute. His Emblem 76 is titled \"Cavendum a meretricibus\"; its accompanying Latin verses mention Picus, Scylla and the companions of Ulysses, and concludes that \"Circe with her famous name indicates a whore and any who loves such a one loses his reason\". His English imitator Geoffrey Whitney used a variation of Alciato's illustration in his own \"Choice of Emblemes\" (1586) but gave it the new title of \"Homines voluptatibus transformantur\", men are transformed by their passions. This explains her appearance in the Nighttown section named after her in James Joyce's novel \"Ulysses\". Written in the form of a stage script, it makes of Circe the brothel madam, Bella Cohen. Bloom, the book's protagonist, fantasizes that she turns into a cruel man-tamer named Mr Bello who makes him get down on all fours and rides him like a horse.\nBy the 19th century, Circe was ceasing to be a mythical figure. Poets treated her either as an individual or at least as the type of a certain kind of woman. The French poet Albert Glatigny addresses \"Circ\u00e9\" in his (1857) and makes of her a voluptuous opium dream, the magnet of masochistic fantasies. Louis-Nicolas M\u00e9nard's sonnet in (1876) describes her as enchanting all with her virginal look, but appearance belies the accursed reality. Poets in English were not far behind in this lurid portrayal. Lord de Tabley's \"Circe\" (1895) is a thing of decadent perversity likened to a tulip, \"A flaunting bloom, naked and undivine... / With freckled cheeks and splotch'd side serpentine, / A gipsy among flowers\".\nThat central image is echoed by the blood-striped flower of T.S.Eliot's student poem \"Circe's Palace\" (1909) in the Harvard Advocate. Circe herself does not appear, her character is suggested by what is in the grounds and the beasts in the forest beyond: panthers, pythons, and peacocks that \"look at us with the eyes of men whom we knew long ago\". Rather than a temptress, she has become an emasculatory threat.\nSeveral female poets make Circe stand up for herself, using the soliloquy form to voice the woman's position. The 19th-century English poet Augusta Webster, much of whose writing explored the female condition, has a dramatic monologue in blank verse titled \"Circe\" in her volume \"Portraits\" (1870). There the sorceress anticipates her meeting with Ulysses and his men and insists that she does not turn men into pigs\u2014she merely takes away the disguise that makes them seem human. \"But any draught, pure water, natural wine, / out of my cup, revealed them to themselves / and to each other. Change? there was no change; / only disguise gone from them unawares\". The mythological character of the speaker contributes at a safe remove to the Victorian discourse on women's sexuality by expressing female desire and criticizing the subordinate role given to women in heterosexual politics.\nTwo American poets also explored feminine psychology in poems ostensibly about the enchantress. Leigh Gordon Giltner's \"Circe\" was included in her collection \"The Path of Dreams\" (1900), the first stanza of which relates the usual story of men turned into swine by her spell. But then a second stanza presents a sensuous portrait of an unnamed woman, very much in the French vein; once more, it concludes, \"A Circe's spells transform men into swine\". This is no passive victim of male projections but a woman conscious of her sexual power. So too is H.D.'s \"Circe\", from her collection \"Hymen\" (1921). In her soliloquy she reviews the conquests with which she has grown bored, then mourns the one instance when she failed. In not naming Ulysses himself, Doolittle universalises an emotion with which all women might identify. At the end of the century, British poet Carol Ann Duffy wrote a monologue entitled \"Circe\" which pictures the goddess addressing an audience of \"nereids and nymphs\". In this outspoken episode in the war between the sexes, Circe describes the various ways in which all parts of a pig could and should be cooked.\nAnother indication of the progression in interpreting the Circe figure is given by two poems a century apart, both of which engage with paintings of her. The first is the sonnet that Dante Gabriel Rossetti wrote in response to Edward Burne-Jones' \"The Wine of Circe\" in his volume \"Poems\" (1870). It gives a faithful depiction of the painting's Pre-Raphaelite mannerism but its description of Circe's potion as \"distilled of death and shame\" also accords with the contemporary (male) identification of Circe with perversity. This is further underlined by his statement (in a letter) that the black panthers there are \"images of ruined passion\" and by his anticipation at the end of the poem of \"passion's tide-strown shore / Where the disheveled seaweed hates the sea\". The Australian A. D. Hope's \"Circe \u2013 after the painting by Dosso Dossi\", on the other hand, frankly admits humanity's animal inheritance as natural and something in which even Circe shares. In the poem, he links the fading rationality and speech of her lovers to her own animal cries in the act of love.\nThere remain some poems that bear her name that have more to do with their writers' private preoccupations than with reinterpreting her myth. The link with it in Margaret Atwood's \"Circe/Mud Poems\", first published in \"You Are Happy\" (1974), is more a matter of allusion and is nowhere overtly stated beyond the title. It is a reflection on contemporary gender politics that scarcely needs the disguises of Augusta Webster's. With two other poems by male writers it is much the same: Louis Macneice's, for example, whose \"Circe\" appeared in his first volume, \"Poems\" (London, 1935); or Robert Lowell's, whose \"Ulysses and Circe\" appeared in his last, \"Day by Day\" (New York, 1977). Both poets have appropriated the myth to make a personal statement about their broken relationships.\nParallels and sequels.\nSeveral Renaissance epics of the 16th century include lascivious sorceresses based on the Circe figure. These generally live in an isolated spot devoted to pleasure, to which lovers are lured and later changed into beasts. They include the following:\nLater scholarship has identified elements from the character of both Circe and especially her fellow enchantress Medea as contributing to the development of the mediaeval legend of Morgan le Fay. In addition, it has been argued that the fairy Titania in William Shakespeare's \"A Midsummer Night's Dream\" (1600) is an inversion of Circe. Titania (daughter of the Titans) was a title by which the sorceress was known in Classical times. In this case the tables are turned on the character, who is queen of the fairies. She is made to love an ass after, rather than before, he is transformed into his true animal likeness.\nIt has further been suggested that John Milton's \"Mask Presented at Ludlow Castle\" (1634) is a sequel to \"Tempe Restored,\" a masque in which Circe had figured two years earlier, and that the situation presented there is a reversal of the Greek myth. At the start of the masque, the character Comus is described as the son of Circe by Bacchus, god of wine, and the equal of his mother in enchantment. He too changes travelers into beastly forms that \"roll with pleasure in a sensual sty\". Having waylaid the heroine and immobilized her on an enchanted chair, he stands over her, wand in hand, and presses on her a magical cup (representing sexual pleasure and intemperance), which she repeatedly refuses, arguing for the virtuousness of temperance and chastity. The picture presented is a mirror image of the Classical story. In place of the witch who easily seduces the men she meets, a male enchanter is resisted by female virtue.\nIn the 20th century, the Circe episode was to be re-evaluated in two poetic sequels to the \"Odyssey\". In the first of these, Giovanni Pascoli's (\"The Last Voyage\", 1906), the aging hero sets out to rediscover the emotions of his youth by retracing his journey from Troy, only to discover that the island of Eea is deserted. What in his dream of love he had taken for the roaring of lions and Circe's song was now no more than the sound of the sea-wind in autumnal oaks (Cantos 16\u201317).\nThis melancholy dispelling of illusion is echoed in \"\" (1938) by Nikos Kazantzakis. The fresh voyage in search of new meaning to life recorded there grows out of the hero's initial rejection of his past experiences in the first two sections. The Circe episode is viewed by him as a narrow escape from death of the spirit: \"With twisted hands and thighs we rolled on burning sands, / a hanging mess of hissing vipers glued in sun!... / Farewell the brilliant voyage, ended! Prow and soul / moored in the muddy port of the contented beast! / O prodigal, much-traveled soul, is this your country?\" His escape from this mire of sensuality comes one day when the sight of some fishermen, a mother and her baby enjoying the simple comforts of food and drink, recalls him to life, its duties and delights. Where the attempt by Pascoli's hero to recapture the past ended in failure, Kazantzakis' Odysseus, already realising the emptiness of his experiences, journeys into what he hopes will be a fuller future. \nVisual representations.\nAncient art.\nScenes from the \"Odyssey\" are common on Greek pottery, the Circe episode among them. The two most common representations have Circe surrounded by the transformed sailors and Odysseus threatening the sorceress with his sword. In the case of the former, the animals are not always boars but also include, for instance, the ram, dog and lion on the 6th-century BC Boston kylix. Often the transformation is only partial, involving the head and perhaps a sprouting tail, while the rest of the body is human. In describing an otherwise obscure 5th-century Greek bronze in the Walters Art Museum that takes the form of a man on all fours with the foreparts of a pig, the commentator asks in what other way could an artist depict someone bewitched other than as a man with an animal head. In these scenes Circe is shown almost invariably stirring the potion with her wand, although the incident as described in Homer has her use the wand only to bewitch the sailors after they have refreshed themselves. One exception is the Berlin amphora on which the seated Circe holds the wand towards a half transformed man.\nIn the second scene, Odysseus threatens the sorceress with a drawn sword, as Homer describes it. However, he is sometimes depicted carrying spears as well, as in the Athens lekythos, while Homer reports that it was a bow he had slung over his shoulder. In this episode Circe is generally shown in flight, and on the Erlangen lekythos can clearly be seen dropping the bowl and wand behind her. Two curiously primitive wine bowls incorporate the Homeric detail of Circe's handloom, at which the men approaching her palace could hear her singing sweetly as she worked. In the 5th-century skyphos from Boeotia an apparently crippled Odysseus leans on a crutch while a woman with African features holds out a disproportionately large bowl. In the other, a pot-bellied hero brandishes a sword while Circe stirs her potion. Both these may depict the scene as represented in one or other of the comic satyr plays which deal with their encounter. Little remains of these now beyond a few lines by Aeschylus, Ephippus of Athens and Anaxilas. Other vase paintings from the period suggest that Odysseus' half-transformed animal-men formed the chorus in place of the usual satyrs. The reason that it should be a subject of such plays is that wine drinking was often central to their plot. Later writers were to follow Socrates in interpreting the episode as illustrating the dangers of drunkenness.\nOther artefacts depicting the story include the chest of Cypselus described in the travelogue by Pausanias. Among its many carvings \"there is a grotto and in it a woman sleeping with a man upon a couch. I was of opinion that they were Odysseus and Circe, basing my view upon the number of the handmaidens in front of the grotto and upon what they are doing. For the women are four, and they are engaged on the tasks which Homer mentions in his poetry\". The passage in question describes how one of them \"threw linen covers over the chairs and spread fine purple fabrics on top. Another drew silver tables up to the chairs, and laid out golden dishes, while a third mixed sweet honeyed wine in a silver bowl, and served it in golden cups. The fourth fetched water and lit a roaring fire beneath a huge cauldron\". This suggests a work of considerable detail, while the Etruscan coffin preserved in Orvieto's archaeological museum has only four figures. At the centre Odysseus threatens Circe with drawn sword while an animal headed figure stands on either side, one of them laying his hand familiarly on the hero's shoulder. A bronze mirror relief in the Fitzwilliam Museum is also Etruscan and is inscribed with the names of the characters. There a pig is depicted at Circe's feet, while Odysseus and Elpenor approach her, swords drawn.\nPortraits in character.\nDuring the 18th century painters began to portray individual actors in scenes from named plays. There was also a tradition of private performances, with a variety of illustrated works to help with stage properties and costumes. Among these was Thomas Jefferys' \"A Collection of the Dresses of Different Nations, Antient and Modern\" (1757\u20131772) which included a copperplate engraving of a crowned Circe in loose dress, holding a goblet aloft in her right hand and a long wand in her left. Evidence of such performances during the following decades is provided by several portraits in character, of which one of the earliest was the pastel by Daniel Gardner (1750\u20131805) of \"Miss Elliot as Circe\". The artist had been a pupil of both George Romney and Joshua Reynolds, who themselves were soon to follow his example. On the 1778 engraving based on Gardner's portrait appear the lines from Milton's \"Comus\": \"The daughter of the Sun, whose charmed cup / Whoever tasted, lost his upright shape / And downward fell into a grovelling swine\", in compliment to the charm of this marriageable daughter of a country house. As in the Jefferys' plate, she wears a silver coronet over tumbled dark hair, with a wand in the right hand and a goblet in the left. In hindsight the frank eyes that look directly at the viewer and the rosebud mouth are too innocent for the role Miss Elliot is playing.\nThe subjects of later paintings impersonating Circe have a history of sexual experience behind them, starting with \"Mary Spencer in the character of Circe\" by William Caddick, which was exhibited at the Royal Academy in 1780. The subject here was the mistress of the painter George Stubbs. A portrait of \"Mrs Nesbitt as Circe\" by Reynolds followed in 1781. Though this lady's past was ambiguous, she had connections with those in power and was used by the Government as a secret agent. In the painting she is seated sideways, wearing a white, loose-fitting dress, with a wand in her right hand and a gilded goblet near her left. A monkey is crouching above her in the branches of a tree and a panther fraternizes with the kitten on her knee. While the painting undoubtedly alludes to her reputation, it also places itself within the tradition of dressing up in character.\nSoon afterwards, the notorious Emma Hamilton was to raise this to an art form, partly by the aid of George Romney's many paintings of her impersonations. Romney's preliminary study of Emma's head and shoulders, at present in the Tate Gallery, with its piled hair, expressive eyes and mouth, is reminiscent of Samuel Gardener's portrait of Miss Elliot. In the full-length \"Lady Hamilton as Circe\" at Waddesdon Manor, she is placed in a wooded landscape with wolves snarling to her left, although the tiger originally there has now been painted out. Her left arm is raised to cast a spell while the wand points downward in her right. After Emma moved to Naples and joined Lord Hamilton, she developed what she called her \"Attitudes\" into a more public entertainment. Specially designed, loose-fitting tunics were paired with large shawls or veils as she posed in such a way as to evoke figures from Classical mythology. These developed from mere poses, with the audience guessing the names of the classical characters and scenes that she portrayed, into small, wordless charades.\nThe tradition of dressing up in character continued into the following centuries. One of the photographic series by Julia Margaret Cameron, a pupil of the painter George Frederic Watts, was of mythical characters, for whom she used the children of friends and servants as models. Young Kate Keown sat for the head of \"Circe\" in about 1865 and is pictured wearing a grape and vineleaf headdress to suggest the character's use of wine to bring a change in personality. The society portrait photographer Yevonde Middleton, also known as Madame Yevonde, was to use a 1935 aristocratic charity ball as the foundation for her own series of mythological portraits in colour. Its participants were invited to her studio afterwards to pose in their costumes. There Baroness Dacre is pictured as Circe with a leafy headdress about golden ringlets and clasping a large Baroque porcelain goblet.\nA decade earlier, the illustrator Charles Edmund Brock extended into the 20th century what is almost a pastiche of the 18th-century conversation piece in his \"Circe and the Sirens\" (1925). In this the Honourable Edith Chaplin (1878\u20131959), Marchioness of Londonderry, and her three youngest daughters are pictured in a garden setting grouped about a large pet goat. Three women painters also produced portraits using the convention of the sitter in character. The earliest was Beatrice Offor (1864\u20131920), whose sitter's part in her 1911 painting of Circe is suggested by the vine-leaf crown in her long dark hair, the snake-twined goblet she carries and the snake bracelet on her left arm. Mary Cecil Allen was of Australian origin but was living in the United States at the time \"Miss Audrey Stevenson as Circe\" was painted (1930). Though only a head and shoulders sketch, its colouring and execution suggest the sitter's lively personality. Rosemary Valodon (born 1947), from the same country, painted a series of Australian personalities in her goddess series. \"Margarita Georgiadis as Circe\" (1991) is a triptych, the central panel of which portrays an updated, naked femme fatale reclining in tropical vegetation next to a pig's head.\nOne painting at least depicts an actress playing the part of Circe. This is Franz von Stuck's striking portrait of Tilla Durieux as Circe (1913). She played this part in a Viennese revival of Calderon's play in 1912 and there is a publicity still of her by Isidor Hirsch in which she is draped across a sofa and wearing an elaborate crown. Her enticing expression and the turn of her head there is almost exactly that of Van Stuck's enchantress as she holds out the poisoned bowl. It suggests the use of certain posed publicity photos in creating the same iconic effect as had paintings in the past. A nearly contemporary example was the 1907 photo of Mme Genevi\u00e8ve Vix as Circe in the light opera by Lucien Hillenacher at the Op\u00e9ra-Comique in Paris. The posing of the actress and the cropping of the image so as to highlight her luxurious costume demonstrates its ambition to create an effect that goes beyond the merely theatrical. A later example is the still of Silvana Mangano in her part as Circe in the 1954 film \"Ulysses\", which is as cunningly posed for effect.\nMusical treatments.\nCantata and song.\nBeside the verse dramas, with their lyrical interludes, on which many operas were based, there were poetic texts which were set as secular cantatas. One of the earliest was Alessandro Stradella's \"La Circe\", in a setting for three voices that bordered on the operatic. It was first performed at Frascati in 1667 to honour Cardinal Leopoldo de Medici and contained references to its surroundings. In the opening recitative, Circe explains that it was her son Telegonus who founded Frascati. The other characters with whom she enters into dialogue are the south wind (Zeffiro) and the local river Algido. In the following century, Antonio Vivaldi's cantata (In the shadow of doubt, RV 678) is set for a single voice and depicts Circe addressing Ulysses. The countertenor part is accompanied by flute, harpsichord, cello, and theorbo and features two recitatives and two arias. The piece is famous for the dialogue created between flute and voice, conjuring the moment of flirtation before the two become lovers.\nThe most successful treatment of the Ulysses episode in French was Jean-Baptiste Rousseau's poem \"Circ\u00e9\" (1703), that was specifically written to be a cantata. The different verse forms employed allow the piece to be divided by the musicians that set it in order to express a variety of emotions. The poem opens with the abandoned Circe sitting on a high mountain and mourning the departure of Ulysses. The sorceress then calls on the infernal gods and makes a terrible sacrifice: \"A myriad vapours obscure the light, / The stars of the night interrupt their course, / Astonished rivers retreat to their source / And even Death's god trembles in the dark\". But though the earth is shaken to its core, Love is not to be commanded in this way and the wintery fields come back to life.\nThe earliest setting was by Jean-Baptiste Morin in 1706 and was popular for most of the rest of the century. One of its final moralising minuets, (Love won't be forced) was often performed independently and the score reprinted in many song collections. The flautist Michel Blavet arranged the music for this and the poem's final stanza, (In the fields that Winter wastes), for two flutes in 1720. The new setting of the cantata three years later by Francois Collin de Blamont was equally successful and made the name of its nineteen-year-old composer. Originally for voice and bass continuo, it was expanded and considerably revised in 1729, with parts for flute, violin and viol added. Towards the end of the century, the choral setting by Georges Granges de Fontenelle (1769\u20131819) was equally to bring its young composer fame.\nRousseau's poem was also familiar to composers of other nationalities. Set for mezzo-soprano and full orchestra, it was given almost operatic treatment by the court composer Luigi Cherubini in 1789. Franz Seydelmann set it for soprano and full orchestra in Dresden in 1787 at the request of the Russian ambassador to the Saxon Court, Prince Alexander Belosselsky, who spoke highly of Seydelmann's work. A later setting by Austrian composer Sigismond von Neukomm for soprano and full orchestra (Op. 4, 1810) was judged favorably by French musicologist Jacques Chailley in his 1966 article for the journal .\nRecent treatments of the Circe theme include the Irish composer Gerard Victory's radio cantata \"Circe 1991\" (1973\u20131975), David Gribble's \"A Threepenny Odyssey\", a fifteen-minute cantata for young people which includes the episode on Circe's Isle, and Malcolm Hayes' \"Odysseus remembers\" (2003\u201304), which includes parts for Circe, Anticleia and Tiresias. Gerald Humel's song cycle \"Circe\" (1998) grew out of his work on his 1993 ballet with Thomas H\u00f6ft. The latter subsequently wrote seven poems in German featuring Circe's role as seductress in a new light: here it is to freedom and enlightenment that she tempts her hearers. Another cycle of \"Seven Songs for High Voice and Piano\" (2008) by the American composer Martin Hennessey includes the poem \"Circe's Power\" from Louise Gl\u00fcck's \"Meadowlands\" (1997).\nThere have also been treatments of Circe in popular music, in particular the relation of the Odysseus episode in Friedrich Holl\u00e4nder's song of 1958. In addition, text in Homeric Greek is included in the \"Circe's Island\" episode in David Bedford's \"The Odyssey\" (1976). This was the ancestor of several later electronic suites that reference the Odysseus legend, with \"Circe\" titles among them, having little other programmatic connection with the myth itself.\nClassical ballet and programmatic music.\nAfter classical ballet separated from theatrical spectacle into a wordless form in which the story is expressed solely through movement, the subject of Circe was rarely visited. It figured as the first episode of three with mythological themes in (\"New Shows\"), staged by Sieur Duplessis le cadet in 1734, but the work was taken off after its third performance and not revived. The choreographer Antoine Pitrot also staged , describing it as a \"ballet s\u00e9rieux, hero\u00ef-pantomime\" in 1764. Thereafter there seems to be nothing until the revival of ballet in the 20th century.\nIn 1963, the American choreographer Martha Graham created her \"Circe\" with a score by Alan Hovhaness. Its theme is psychological, representing the battle with animal instincts. The beasts portrayed extend beyond swine and include a goat, a snake, a lion and a deer. The theme has been described as one of \"highly charged erotic action\", although set in \"a world where sexual frustration is rampant\". In that same decade Rudolf Brucci composed his \"Kirka\" (1967) in Croatia.\nThere is a Circe episode in John Harbison's \"Ulysses\" (Act 1, scene 2, 1983) in which the song of the enchantress is represented by ondes Martenot and tuned percussion. After the sailors of Ullyses are transformed into animals by her spell, a battle of wills follows between Circe and the hero. Though the men are changed back, Ulysses is charmed by her in his turn. In 1993, a full scale treatment of the story followed in Gerald Humel's two-act \"Circe und Odysseus\". Also psychological in intent, it represents Circe's seduction of the restless hero as ultimately unsuccessful. The part played by the geometrical set in its Berlin production was particularly notable.\nWhile operas on the subject of Circe did not cease, they were overtaken for a while by the new musical concept of the symphonic poem which, whilst it does not use a sung text, similarly seeks a union of music and drama. A number of purely musical works fall into this category from the late 19th century onwards, of which one of the first was Heinrich von Herzogenberg's \"Odysseus\" (Op.16, 1873). A Wagnerian symphony for large orchestra, dealing with the hero's return from the Trojan war, its third section is titled \"Circe's Gardens\" (\"Die G\u00e4rten der Circe\").\nIn the 20th century, 's cycle \"Aus Odysseus Fahrten\" (\"From Odysseus' Voyage\", Op. 6, 1903) was equally programmatic and included the visit to Circe's Isle (\"Die Insel der Circe\") as its second long section. After a depiction of the sea voyage, a bass clarinet passage introduces an ensemble of flute, harp and solo violin over a lightly orchestrated accompaniment, suggesting Circe's seductive attempt to hold Odysseus back from traveling further. Alan Hovhaness' \"Circe Symphony\" (No.18, Op. 204a, 1963) is a late example of such programmatic writing. It is, in fact, only a slightly changed version of his ballet music of that year, with the addition of more strings, a second timpanist and celesta.\nWith the exception of Willem Frederik Bon's prelude for orchestra (1972), most later works have been for a restricted number of instruments. They include Hendrik de Regt's \"Circe\" (Op. 44, 1975) for clarinet, violin and piano; Christian Manen's \"Les Enchantements De Circe\" (Op. 96, 1975) for bassoon and piano; and Jacques Lenot's \"Cir(c)\u00e9\" (1986) for oboe d'amore. The German experimental musician Dieter Schnebel's \"Circe\" (1988) is a work for harp, the various sections of which are titled \"Signale\" (signals), \"S\u00e4useln\" (whispers), \"Verlockungen\" (enticements), \"Pein\" (pain), \"Schl\u00e4ge\" (strokes) and \"Umgarnen\" (snare), which give some idea of their programmatic intent.\nThea Musgrave's \"Circe\" for three flutes (1996) was eventually to become the fourth piece in her six-part \"Voices from the Ancient World\" for various combinations of flute and percussion (1998). Her note on these explains that their purpose is to \"describe some of the personages of ancient Greece\" and that Circe was \"the enchantress who changed men into beasts\". A recent reference is the harpsichordist Fernando De Luca's Sonata II for viola da gamba titled \"Circe's Cave\" (\"L'antro della maga Circe\").\nScientific interpretations.\nIn later Christian opinion, Circe was an abominable witch using miraculous powers to evil ends. When the existence of witches came to be questioned, she was reinterpreted as a depressive suffering from delusions.\nIn botany, the Circaea are plants belonging to the enchanter's nightshade genus. The name was given by botanists in the late 16th century in the belief that this was the herb used by Circe to charm Odysseus' companions. Medical historians have speculated that the transformation to pigs was not intended literally but refers to anticholinergic intoxication with the plant \"Datura stramonium\". Symptoms include amnesia, hallucinations, and delusions. The description of \"moly\" fits the snowdrop, a flower that contains galantamine, which is a long lasting anticholinesterase and can therefore counteract anticholinergics that are introduced to the body after it has been consumed.\nOther influence.\nThe \"gens Mamilia\" \u2013 described by Livy as one of the most distinguished families of Latium \u2013 claimed descent from Mamilia, a granddaughter of Odysseus and Circe through Telegonus. One of the most well known of them was Octavius Mamilius (died 498 BC), princeps of Tusculum and son-in-law of Lucius Tarquinius Superbus the seventh and last king of Rome."}
{"id": "5958", "revid": "1268841545", "url": "https://en.wikipedia.org/wiki?curid=5958", "title": "CPR (disambiguation)", "text": "Cardiopulmonary resuscitation (CPR) is an emergency procedure to assist someone who has suffered cardiac arrest.\nCPR may also refer to:"}
{"id": "5959", "revid": "20612", "url": "https://en.wikipedia.org/wiki?curid=5959", "title": "Canadian Pacific Railway", "text": "The Canadian Pacific Railway () , also known simply as CPR or Canadian Pacific and formerly as CP Rail (1968\u20131996), is a Canadian Class I railway incorporated in 1881. The railway is owned by Canadian Pacific Kansas City Limited, known until 2023 as Canadian Pacific Railway Limited, which began operations as legal owner in a corporate restructuring in 2001.\nThe railway is headquartered in Calgary, Alberta. In 2023, the railway owned approximately of track in seven provinces of Canada and into the United States, stretching from Montreal to Vancouver, and as far north as Edmonton. Its rail network also served Minneapolis\u2013St. Paul, Milwaukee, Detroit, Chicago, and Albany, New York, in the United States.\nThe railway was first built between eastern Canada and British Columbia between 1875 and 1885 (connecting with Ottawa Valley and Georgian Bay area lines built earlier), fulfilling a commitment extended to British Columbia when it entered Confederation in 1871; the CPR was Canada's first transcontinental railway. Primarily a freight railway, the CPR was for decades the only practical means of long-distance passenger transport in most regions of Canada and was instrumental in the colonization and development of Western Canada. The CPR became one of the largest and most powerful companies in Canada, a position it held as late as 1975. The company acquired two American lines in 2009: the Dakota, Minnesota and Eastern Railroad (DM&amp;E) and the Iowa, Chicago and Eastern Railroad (IC&amp;E). Also, the company owns the Indiana Harbor Belt Railroad, a Hammond, Indiana-based terminal railroad along with Conrail Shared Assets Operations. CPR purchased the Kansas City Southern Railway in December 2021 for . On April 14, 2023, KCS became a wholly owned subsidiary of CPR, and both CPR and its subsidiaries began doing business under the name of its parent company, CPKC.\nThe CPR is publicly traded on both the Toronto Stock Exchange and the New York Stock Exchange under the ticker CP. Its U.S. headquarters are in Minneapolis. As of March 30, 2023, the largest shareholder of Canadian Pacific stock exchange is TCI Fund Management Limited, a London-based hedge fund that owns 6% of the company.\nHistory.\nThe creation of the Canadian Pacific Railway was undertaken as the National Dream by the Conservative government of John A. Macdonald, together with mining magnate Alexander Tilloch Galt. As a condition for joining the Canadian Confederation, British Columbia had insisted on a transport link to the East, with the rest of the Confederation. In 1873, Macdonald, among other high-ranking politicians, bribed in the Pacific Scandal, granted contracts to the Canada Pacific Railway Company, which was unrelated to the current company, as opposed to the Inter-Ocean Railway Company, which was thought to have connections to the Northern Pacific Railway Company in the United States. After this scandal, the Conservatives were removed from power, and Alexander Mackenzie, the new Liberal prime minister, ordered construction of the railway under the supervision of the Department of Public Works. \nEnabled by the CPR Act of 1874, work began in 1875 on the Lake Superior to Manitoba section of the CPR. The ceremonial sod-turning at Westfort on June 1, 1875, was prominently reported in the June 10 edition of the Toronto \"Globe\". It noted that a crowd of \"upwards of 500 ladies and gentlemen\" gathered to celebrate the event on the left bank of the Kaministiquia River in the District of Thunder Bay, about four miles upriver from Fort William. Once completed in 1882 with a last spike at Feist Lake, near Vermilion Bay, Ontario, the line was turned over to the newly-minted private Canadian Pacific Railway company. In 1883, the first wheat shipment from Manitoba was transported over this line to the Lakehead (Fort William and Port Arthur) on Lake Superior.\nMacdonald would later return as prime minister and adopt a more aggressive construction policy; bonds were floated in London and called for tenders to complete sections of the railway in British Columbia. American contractor Andrew Onderdonk was selected, and his men began construction on May 15, 1880.\nIn October 1880, a new consortium signed a contract with the Macdonald government, agreeing to build the railway for $25 million in credit and of land. In addition, the government defrayed surveying costs and exempted the railway from property taxes for 20 years.\nA beaver was chosen as the railway's logo in honour of Donald Smith, 1st Baron Strathcona and Mount Royal, who had risen from factor to governor of the Hudson's Bay Company over a lengthy career in the beaver fur trade.\nBuilding the railway, 1881\u20131886.\nBuilding the railway took over four years. The Canadian Pacific Railway began its westward expansion from Bonfield, Ontario, where the first spike was driven into a sunken railway tie. That was the point where the Canada Central Railway (CCR) extension ended. The CCR started in Brockville and extended to Pembroke. It then followed a westward route along the Ottawa River and continued to Mattawa at the confluence of the Mattawa and Ottawa rivers. It then proceeded to Bonfield.\nIt was presumed that the railway would travel through the rich \"fertile belt\" of the North Saskatchewan River Valley and cross the Rocky Mountains via the Yellowhead Pass. However, a more southerly route across the arid Palliser's Triangle in Saskatchewan and via Kicking Horse Pass and down the Field Hill to the Rocky Mountain Trench was chosen.\nIn 1881, construction progressed at a pace too slow for the railway's officials who, in 1882, hired the renowned railway executive William Cornelius Van Horne to oversee construction. Van Horne stated that he would have of main line built in 1882. Floods delayed the start of the construction season, but over of main line, as well as sidings and branch lines, were built that year.\nThe Thunder Bay branch (west from Fort William) was completed in June 1882 by the Department of Railways and Canals and turned over to the company in May 1883. By the end of 1883, the railway had reached the Rocky Mountains, just east of Kicking Horse Pass. The treacherous of railway west of Fort William was completed by Purcell &amp; Company, headed by \"Canada's wealthiest and greatest railroad contractor,\" industrialist Hugh Ryan.\nMany thousands of navvies worked on the railway. Many were European immigrants. An unknown number of Stoney Nakoda also assisted in track laying and construction work in the Kicking Horse Pass region. In British Columbia, government contractors eventually hired 17,000 workers from China, known as \"coolies\". After months of hard labour, they could net as little as $16 ($ in adjusted for inflation) Chinese labourers in British Columbia made only between 75 cents and $1.25 a day, paid in rice mats, and not including expenses, leaving barely anything to send home. They did the most dangerous construction jobs, such as working with explosives to clear tunnels through rock. The exact number of Chinese workers who died is unknown, but historians estimate the number is between 600 and 800.\nBy 1883, railway construction was progressing rapidly, but the CPR was in danger of running out of funds. In response, on January 31, 1884, the government passed the Railway Relief Bill, providing a further $22.5\u00a0million in loans to the CPR. The bill received royal assent on March 6, 1884.\nIn March 1885, the North-West Rebellion broke out in the District of Saskatchewan. Van Horne, in Ottawa at the time, suggested to the government that the CPR could transport troops to Qu'Appelle in the District of Assiniboia in 10 days. Some sections of track were incomplete or had not been used before, but the trip to Winnipeg was made in nine days and the rebellion quickly suppressed. Controversially, the government subsequently reorganized the CPR's debt and provided a further $5\u00a0million loan. This money was desperately needed by the CPR. Even with Van Horne's support with moving troops to Qu'Appelle, the government still delayed in giving its support to CPR, due to Macdonald pressuring George Stephen for additional benefits.\nOn November 7, 1885, the last spike was driven at Craigellachie, British Columbia. Four days earlier, the last spike of the Lake Superior section was driven in just west of Jackfish, Ontario. While the railway was completed four years after the original 1881 deadline, it was completed more than five years ahead of the new date of 1891 that Macdonald gave in 1881.\nIn Eastern Canada, the CPR had created a network of lines reaching from Quebec City to St. Thomas, Ontario, by 1885 mainly by buying the Quebec, Montreal, Ottawa &amp; Occidental Railway from the Quebec government and by creating a new railway company, the Ontario and Quebec Railway (O&amp;Q). It also launched a fleet of Great Lakes ships to link its terminals. Through the O&amp;Q, the CPR had effected purchases and long-term leases of several railways, and built a line between Perth, Ontario, and Toronto (completed on May 5, 1884) to connect these acquisitions. The CPR obtained a 999-year lease on the O&amp;Q on January 4, 1884. In 1895, it acquired a minority interest in the Toronto, Hamilton and Buffalo Railway, giving it a link to New York and the Northeast United States.\n1886\u20131900.\nThe last spike in the CPR was driven on November 7, 1885, by one of its directors, Donald Smith.\nThe first transcontinental passenger train departed from Montreal's Dalhousie Station, at Berri Street and Notre Dame Street, at 8\u00a0pm on June 28, 1886, and arrived at Port Moody at noon on July 4. This train consisted of two baggage cars, a mail car, one second-class coach, two immigrant sleepers, two first-class coaches, two sleeping cars and a diner (several dining cars were used throughout the journey, as they were removed from the train during the night, with another one added the next morning).\nBy that time, however, the CPR had decided to move its western terminus from Port Moody to Granville, which was renamed \"Vancouver\" later that year. The first official train destined for Vancouver arrived on May 23, 1887, although the line had already been in use for three months. The CPR quickly became profitable, and all loans from the federal government were repaid years ahead of time. In 1888, a branch line was opened between Sudbury and Sault Ste. Marie where the CPR connected with the American railway system and its own steamships. That same year, work was started on a line from London, Ontario, to the Canada\u2013US border at Windsor, Ontario. That line opened on June 12, 1890.\nThe CPR also leased the New Brunswick Railway in 1891 for 991 years, and built the International Railway of Maine, connecting Montreal with Saint John, New Brunswick, in 1889. The connection with Saint John on the Atlantic coast made the CPR the first truly transcontinental railway company in Canada and permitted trans-Atlantic cargo and passenger services to continue year-round when sea ice in the Gulf of St. Lawrence closed the port of Montreal during the winter months. By 1896, competition with the Great Northern Railway for traffic in southern British Columbia forced the CPR to construct a second line across the province, south of the original line. Van Horne, now president of the CPR, asked for government aid, and the government agreed to provide around $3.6\u00a0million to construct a railway from Lethbridge, Alberta, through Crowsnest Pass to the south shore of Kootenay Lake, in exchange for the CPR agreeing to reduce freight rates in perpetuity for key commodities shipped in Western Canada.\nThe controversial Crowsnest Pass Agreement effectively locked the eastbound rate on grain products and westbound rates on certain \"settlers' effects\" at the 1897 level. Although temporarily suspended during the First World War, it was not until 1983 that the \"Crow Rate\" was permanently replaced by the \"Western Grain Transportation Act\", which allowed the gradual increase of grain shipping prices. The Crowsnest Pass line opened on June 18, 1898, and followed a complicated route through the maze of valleys and passes in southern British Columbia, rejoining the original mainline at Hope after crossing the Cascade Mountains via Coquihalla Pass.\nThe Southern Mainline, generally known as the Kettle Valley Railway in British Columbia, was built in response to the booming mining and smelting economy in southern British Columbia, and the tendency of the local geography to encourage and enable easier access from neighbouring US states than from Vancouver or the rest of Canada, which was viewed to be as much of a threat to national security as it was to the province's control of its own resources. The local passenger service was re-routed to this new southerly line, which connected numerous emergent small cities across the region. Independent railways and subsidiaries that were eventually merged into the CPR in connection with this route were the Shuswap and Okanagan Railway, the Kaslo and Slocan Railway, the Columbia and Kootenay Railway, the Columbia and Western Railway and various others.\nSettlement of western Canada.\nUnder the initial contract with the Canadian government to build the railway, the CPR was granted . Canadian Pacific then began an intense campaign to bring immigrants to Canada; its agents operated in many overseas locations, where immigrants were often sold a package that included passage on a CP ship, travel on a CP train and land sold by the CP railway. Land was priced at $2.50 an acre and up but required cultivation. To transport immigrants, Canadian Pacific developed a fleet of over a thousand Colonist cars, low-budget sleeper cars designed to transport immigrant families from eastern Canadian seaports to the west.\n1901\u20131914.\nDuring the first decade of the 20th century, the CPR continued to build more lines. In 1908, the CPR opened a line connecting Toronto with Sudbury. Several operational improvements were also made to the railway in Western Canada.\nOn November 3, 1909, the Lethbridge Viaduct over the Oldman River valley at Lethbridge, Alberta, was opened. It is long and, at its maximum, high, making it one of the longest railway bridges in Canada. In 1916, the CPR replaced its line through Rogers Pass, which was prone to avalanches (the most serious of which killed 62 men in 1910) with the Connaught Tunnel, an eight-kilometre-long (5-mile) tunnel under Mount Macdonald that was, at the time of its opening, the longest railway tunnel in the Western Hemisphere.\nOn January 21, 1910, a passenger train derailed on the CPR line at the Spanish River bridge at Nairn, Ontario (near Sudbury), killing at least 43.\nOn January 3, 1912, the CPR acquired the Dominion Atlantic Railway, a railway that ran in western Nova Scotia. This acquisition gave the CPR a connection to Halifax, a significant port on the Atlantic Ocean. The CPR acquired the Quebec Central Railway on December 14, 1912.\nDuring the late 19th century, the railway undertook an ambitious program of hotel construction, building Glacier House in Glacier National Park, Mount Stephen House at Field, British Columbia, the Ch\u00e2teau Frontenac in Quebec City and the Banff Springs Hotel. By then, the CPR had competition from three other transcontinental lines, all of them money-losers. In 1919, these lines were consolidated into the government-owned Canadian National Railways.\nFirst World War.\nDuring the First World War, CPR put the entire resources of the \"world's greatest travel system\" at the disposal of the British Empire, not only trains and tracks, but also its ships, shops, hotels, telegraphs and, above all, its people. Aiding the war effort meant transporting and billeting troops; building and supplying arms and munitions; arming, lending and selling ships. Fifty-two CPR ships were pressed into service during World War I, carrying more than a million troops and passengers and four million tons of cargo. Twenty seven survived and returned to CPR. CPR also helped the war effort with money and jobs. CPR made loans and guarantees to the Allies of some $100\u00a0million. As a lasting tribute, CPR commissioned three statues and 23 memorial tablets to commemorate the efforts of those who fought and those who died in the war. After the war, the Federal government created Canadian National Railways (CNR, later CN) out of several bankrupt railways that fell into government hands during and after the war. CNR would become the main competitor to the CPR in Canada. In 1923, Henry Worth Thornton replaced David Blyth Hanna becoming the second president of the CNR, and his competition spurred Edward Wentworth Beatty, the first Canadian-born president of the CPR, to action. During this time the railway land grants were formalized.\nGreat Depression and the Second World War, 1929\u20131945.\nThe Great Depression, which lasted from 1929 until 1939, hit many companies heavily. While the CPR was affected, it was not affected to the extent of its rival CNR because it, unlike the CNR, was debt-free. The CPR scaled back on some of its passenger and freight services and stopped issuing dividends to its shareholders after 1932. Hard times led to the creation of new political parties such as the Social Credit movement and the Cooperative Commonwealth Federation, as well as popular protest in the form of the On-to-Ottawa Trek.\nOne highlight of the late 1930s, both for the railway and for Canada, was the visit of King George VI and Queen Elizabeth during their 1939 royal tour of Canada, the first time that the reigning monarch had visited the country. The CPR and the CNR shared the honours of pulling the royal train across the country, with the CPR undertaking the westbound journey from Quebec City to Vancouver. Later that year, the Second World War began. As it had done in World War I, the CPR devoted much of its resources to the war effort. It retooled its Angus Shops in Montreal to produce Valentine tanks and other armoured vehicles, and transported troops and resources across the country. Additionally, 22 of the CPR's ships went to war, 12 of which were sunk.\n1946\u20131978.\nAfter the Second World War, the transportation industry in Canada changed. Where railways had previously provided almost universal freight and passenger services, cars, trucks and airplanes started to take traffic away from railways. This naturally helped the CPR's air and trucking operations, and the railway's freight operations continued to thrive hauling resource traffic and bulk commodities. However, passenger trains quickly became unprofitable. During the 1950s, the railway introduced new innovations in passenger service. In 1955, it introduced \"The Canadian,\" a new luxury transcontinental train. However, in the 1960s, the company started to pull out of passenger services, ending services on many of its branch lines. It also discontinued its secondary transcontinental train \"The Dominion\" in 1966, and in 1970, unsuccessfully applied to discontinue \"The Canadian\". For the next eight years, it continued to apply to discontinue the service, and service on \"The Canadian\" declined markedly. On October 29, 1978, CP Rail transferred its passenger services to Via Rail, a new federal Crown corporation that is responsible for managing all intercity passenger service formerly handled by both CP Rail and CN. Via eventually took almost all of its passenger trains, including \"The Canadian\", off CP's lines.\nIn 1968, as part of a corporate reorganization, each of the major operations, including its rail operations, were organized as separate subsidiaries. The name of the railway was changed to CP Rail, and the parent company changed its name to Canadian Pacific Limited in 1971. Its air, express, telecommunications, hotel and real estate holdings were spun off, and ownership of all of the companies transferred to Canadian Pacific Investments. The slogan was: \"TO THE FOUR CORNERS OF THE WORLD\". The company discarded its beaver logo, adopting the new Multimark (which, when mirrored by an adjacent \"multi-mark\" creates a diamond appearance on a globe) that was used \u2013 with a different colour background \u2013 for each of its operations.\n1979\u20132001.\nThe 1979 Mississauga train derailment.\nOn November 10, 1979, a derailment of a hazardous materials train in Mississauga, Ontario, led to the evacuation of 200,000 people; there were no fatalities. Mississauga Mayor Hazel McCallion threatened to sue Canadian Pacific for the derailment. Part of the compromise was to accept GO Transit commuter rail service along the Galt Subdivision corridor up to Milton, Ontario. Limited trains ran along the Milton line on weekdays only. Expansions to Cambridge, Ontario may be coming in the future.\nIn 1984, CP Rail commenced construction of the Mount Macdonald Tunnel to augment the Connaught Tunnel under the Selkirk Mountains. The first revenue train passed through the tunnel in 1988. At 14.7\u00a0km (nine miles), it is the longest tunnel in the Americas. During the 1980s, the Soo Line Railroad, in which CP Rail still owned a controlling interest, underwent several changes. It acquired the Minneapolis, Northfield and Southern Railway in 1982. Then on February 21, 1985, the Soo Line obtained a controlling interest in the bankrupt Milwaukee Road, merging it into its system on January 1, 1986. Also in 1980, Canadian Pacific bought out the controlling interests of the Toronto, Hamilton and Buffalo Railway (TH&amp;B) from Conrail and molded it into the Canadian Pacific System, dissolving the TH&amp;B's name from the books in 1985. In 1987, most of CPR's trackage in the Great Lakes region, including much of the original Soo Line, were spun off into a new railway, the Wisconsin Central, which was subsequently purchased by CN. Influenced by the Canada-U.S. Free Trade Agreement of 1989, which liberalized trade between the two nations, the CPR's expansion continued during the early 1990s: CP Rail gained full control of the Soo Line in 1990, adding the \"System\" to the former's name, and bought the Delaware and Hudson Railway in 1991. These two acquisitions gave CP Rail routes to the major American cities of Chicago (via the Soo Line and Milwaukee Road as part of its historically logical route) and New York City (via the D&amp;H).\nDuring the 1990s, both CP Rail and CN attempted unsuccessfully to buy out the eastern assets of the other, so as to permit further rationalization. In 1996, CP Rail moved its head office from Windsor Station in Montreal to Gulf Canada Square in Calgary and changed its name back to Canadian Pacific Railway.\nA new subsidiary company, the St. Lawrence and Hudson Railway, was created to operate its money-losing lines in eastern North America, covering Quebec, Southern and Eastern Ontario, trackage rights to Chicago, Illinois, (on Norfolk Southern lines from Detroit) as well as the Delaware and Hudson Railway in the northeastern United States. However, the new subsidiary, threatened with being sold off and free to innovate, quickly spun off money-losing track to short lines, instituted scheduled freight service, and produced an unexpected turn-around in profitability. On 1 January 2001 the StL&amp;H was formally amalgamated with the CP Rail system.\n2001 to 2023.\nIn 2001, the CPR's parent company, Canadian Pacific Limited, spun off its five subsidiaries, including the CPR, into independent companies. In September 2007, CPR announced it was acquiring the Dakota, Minnesota and Eastern Railroad from London-based Electra Private Equity. The merger was completed as of October 31, 2008.\nCanadian Pacific Railway Ltd. trains resumed regular operations on June 1, 2012, after a nine-day strike by some 4,800 locomotive engineers, conductors and traffic controllers who walked off the job on May 23, stalling Canadian freight traffic and costing the economy an estimated (). The strike ended with a government back-to-work bill forcing both sides to come to a binding agreement.\nOn July 6, 2013, a unit train of crude oil which CP had subcontracted to short-line operator Montreal, Maine and Atlantic Railway derailed in Lac-M\u00e9gantic, killing 47. On August 14, 2013, the Quebec government added the CPR, along with lessor World Fuel Services (WFS), to the list of corporate entities from which it seeks reimbursement for the environmental cleanup of the Lac-M\u00e9gantic derailment. On July 15, the press reported that CP would appeal the legal order.\nOn October 12, 2014, it was reported that Canadian Pacific had tried to enter into a merger with American railway CSX, but was unsuccessful.\nIn 2015\u201316 Canadian Pacific sought to merge with American railway Norfolk Southern. and wanted to have a shareholder vote on it. CP ultimately terminated its efforts to merge on April 11, 2016.\nOn February 4, 2019, a loaded grain train ran away from the siding at Partridge just above the Upper Spiral Tunnel in Kicking Horse Pass. The 112-car grain train with three locomotives derailed into the Kicking Horse River just after the Trans Canada Highway overpass. The three crew members on the lead locomotive were killed. The Canadian Pacific Police Service (CPPS) investigated the fatal derailment. It later came to light that, although Creel said that the RCMP \"retain jurisdiction\" over the investigation, the RCMP wrote that \"it never had jurisdiction because the crash happened on CP property\". On January 26, 2020, Canadian current affairs program \"The Fifth Estate\" broadcast an episode on the derailment, and the next day the Canadian Transportation Safety Board (TSB) called for the RCMP to investigate as lead investigator Don Crawford said, \"There is enough to suspect there's negligence here and it needs to be investigated by the proper authority\".\nOn February 4, 2020, the TSB demoted its lead investigator in the crash probe after his superiors decided these comments were \"completely inappropriate\". The TSB stated that it \"does not share the view of the lead safety investigator\". The CPPS say they did a thorough investigation into the actions of the crew, which is now closed and resulted in no charges, while the Alberta Federation of Labour and the Teamsters Canada Rail Conference called for an independent police probe.\nOn November 20, 2019, it was announced that Canadian Pacific would purchase the Central Maine and Quebec Railway from Fortress Transportation and Infrastructure Investors. The line has had a series of different owners since being spun off of the Canadian Pacific in 1995. The first operator was the Canadian American Railroad a division of Iron Road Railways. In 2002 the Montreal, Maine &amp; Atlantic took over operations after CDAC declared bankruptcy. The Central, Maine and Quebec Railway started operations in 2014 after the MMA declared bankruptcy due to the Lac-M\u00e9gantic derailment. On this new acquisition, CP CEO Keith Creel remarked that this gives CP a true coast-to-coast network across Canada and an increased presence in New England. On June 4, 2020; Canadian Pacific bought the Central Maine and Quebec.\nMerger with Kansas City Southern (2021\u20132023).\nOn March 21, 2021, CP announced that it was planning to purchase the Kansas City Southern Railway (KCS) for US$29\u00a0billion. The US Surface Transportation Board (STB) would first have to approve the purchase, which was expected to be completed by the middle of 2022.\nHowever, a competing cash and stock offer was later made by Canadian National Railway (CN) on April 20 at $33.7\u00a0billion. On 13 May, KCS announced that they planned to accept the merger offer from CN, but would give CP until May 21 to come up with a higher bid. On May 21, KCS and CN agreed to a merger. However, CN's merger attempt was blocked by a STB ruling in August that the company could not use a voting trust to assume control of KCS, due to concerns about potentially reduced competition in the railroad industry.\nOn September 12, KCS accepted a new $31 billion offer from CP. Though CP's offer was lower than the offer made by CN, the STB permitted CP to use a voting trust to take control of KCS. The voting trust allowed CP to become the beneficial owner of KCS in December, but the two railroads operated independently until receiving approval for a merger of operations from the STB. That approval came on March 15, 2023, which permitted the railroads to merge as soon as April 14. On April 14, 2023, KCS officially became a subsidiary of CPR, and CPR with its subsidiaries began conducting business under the name of its parent company, Canadian Pacific Kansas City (CPKC).\nFreight trains.\nOver half of CP's freight traffic is in grain (24% of 2016 freight revenue), intermodal freight (22%), and coal (10%) and the vast majority of its profits are made in western Canada. A major shift in trade from the Atlantic to the Pacific has caused serious drops in CPR's wheat shipments through Thunder Bay. It also ships chemicals and plastics (12% of 2016 revenue), automotive parts and assembled automobiles (6%), potash (6%), sulphur and other fertilizers (5%), forest products (5%), and various other products (11%). The busiest part of its railway network is along its main line between Calgary and Vancouver. Since 1970, coal has become a major commodity hauled by CPR. Coal is shipped in unit trains from coal mines in the mountains, including Sparwood, British Columbia, to terminals at Roberts Bank and North Vancouver, from where it is then shipped to Japan.\nGrain is hauled by the CPR from the prairies to ports at Thunder Bay (the former cities of Fort William and Port Arthur), Quebec City and Vancouver, where it is then shipped overseas. The traditional winter export port was Saint John, New Brunswick, when ice closed the St. Lawrence River. Grain has always been a significant commodity hauled by the CPR; between 1905 and 1909, the CPR double-tracked its section of track between Fort William, Ontario (part of present-day Thunder Bay) and Winnipeg to facilitate grain shipments. For several decades this was the only long stretch of double-track mainline outside of urban areas on the CPR. Today, though the Thunder Bay-Winnipeg section is now single tracked, the CPR still has two long distance double track lines serving rural areas, including a stretch between Kent, British Columbia, and Vancouver which follows the Fraser River into the Coast Mountains, as well as the Canadian Pacific Winchester Sub, a stretch of double track mainline which runs from Smiths Falls, Ontario, through downtown Montreal which runs through many rural farming communities. However, CPR was, as of 2020, partially dismantling the stretch of double track mainline on the Winchester Sub.\nPassenger trains.\nThe train was the primary mode of long-distance transport in Canada until the 1960s. Among the many types of people who rode CPR trains were new immigrants heading for the prairies, military troops (especially during the two world wars) and upper class tourists. It also custom-built many of its passenger cars at its CPR Angus Shops to be able to meet the demands of the upper class.\nThe CPR also had a line of Great Lakes ships integrated into its transcontinental service. From 1884 until 1912, these ships linked Owen Sound on Georgian Bay to Fort William. Following a major fire in December 1911 that destroyed the grain elevator, operations were relocated to a new, larger port created by the CPR at Port McNicoll opening in May 1912. Five ships allowed daily service, and included the S.S. \"Assiniboia\" and S.S. \"Keewatin\" built in 1907 which remained in use until the end of service. Travellers went by train from Toronto to that Georgian Bay port, then travelled by ship to link with another train at the Lakehead. After World War II, the trains and ships carried automobiles as well as passengers. This service featured what was to become the last boat train in North America. The \"Steam Boat\" was a fast, direct connecting train between Toronto and Port McNicoll. The passenger service was discontinued at the end of season in 1965 with one ship, the \"Assiniboia\", carrying on in freight service for two more years before being sold. Planned to be a floating restaurant, \"Assiniboia\" caught fire during renovations in 1969 and was subsequently scrapped. Meanwhile \"Keewatin\" which was laid up in 1966 and scheduled to be scrapped, was purchased by RJ and Diane Peterson in 1967 and towed to their marina in Douglas, Michigan to serve as a marine museum. Forty-five years later Skyline International CEO Gil Blutrich purchased \"Keewatin\" and engaged former crewman Eric Conroy to repatriate \"Keewatin\" to Port McNicoll and operate her as an historical attraction, which he did in 2012 through 2019. \"Keewatin\" was closed to visitors in 2020 as a result of the COVID-19 pandemic and did not reopen in Port McNicoll. In 2023 \"Keewatin\" was donated by Skyline to the Marine Museum of the Great Lakes at Kingston, and towed to Hamilton shipyards for restoration before proceeding to Kingston, where it reopened to visitors in 2024.\nAfter the Second World War, passenger traffic declined as automobiles and airplanes became more common, but the CPR continued to innovate in an attempt to keep passenger numbers up. Beginning November 9, 1953, the CPR introduced Budd Rail Diesel Cars (RDCs) on many of its lines. Officially called \"Dayliners\" by the CPR, they were always referred to as \"Budd Cars\" by employees. Greatly reduced travel times and reduced costs resulted, which saved service on many lines for a number of years. The CPR went on to acquire the second largest fleet of RDCs totalling 52 cars. Only the Boston and Maine Railroad had more. This CPR fleet also included the rare model RDC-4 (which consisted of a mail section at one end and a baggage section at the other end with no formal passenger section). On April 24, 1955, the CPR introduced a new luxury transcontinental passenger train, \"The Canadian\". The train provided service between Vancouver and Toronto or Montreal (east of Sudbury; the train was in two sections). The train, which operated on an expedited schedule, was pulled by diesel locomotives, and used new, streamlined, stainless steel rolling stock. This service was initially heavily promoted by the company and many images of the train, especially as it traversed the Canadian Rockies, were captured by CPR's official photographer Nicholas Morant. Featured in numerous advertising promotions worldwide, several such images have gained iconic status.\nStarting in the 1960s, however, the railway started to discontinue much of its passenger service, particularly on its branch lines. For example, passenger service ended on its line through southern British Columbia and Crowsnest Pass in January 1964, and on its Quebec Central in April 1967, and the transcontinental train \"The Dominion\" was dropped in January 1966. On October 29, 1978, CP Rail transferred its passenger services to Via Rail, a new federal Crown corporation that was now responsible for intercity passenger services in Canada. Canadian Prime Minister Brian Mulroney presided over major cuts in Via Rail service on January 15, 1990. This ended service by \"The Canadian\" over CPR rails, and the train was rerouted on the former \"Super Continental\" route via Canadian National without a change of name. Where both trains had been daily prior to January 15, 1990, cuts, the surviving \"Canadian\" was only a three-times-weekly operation. In October 2012, \"The Canadian\" was reduced to twice-weekly for the six-month off-season period, and operates three-times-weekly for only six months a year. In addition to inter-city passenger services, the CPR also provided commuter rail services in Montreal. CP Rail introduced Canada's first bi-level passenger cars here in 1970. On October 1, 1982, the Montreal Urban Community Transit Commission (STCUM) assumed responsibility for the commuter services previously provided by CP Rail. It continues under the Metropolitan Transportation Agency (AMT).\n Canadian Pacific Railway operates two commuter services under contract. GO Transit contracts CPR to operate six return trips between Milton and central Toronto in Ontario. In Montreal, 59 daily commuter trains run on CPR lines from Lucien-L'Allier Station to Candiac, Hudson and Blainville\u2013Saint-J\u00e9r\u00f4me on behalf of the AMT. CP no longer operates Vancouver's West Coast Express on behalf of TransLink, a regional transit authority. Bombardier Transportation assumed control of train operations on May 5, 2014. Although CP Rail no longer owns the track nor operates the commuter trains, it handles dispatching of Metra trains on the Milwaukee District/North and Milwaukee District/West Lines in Chicago, on which the CP also provides freight service via trackage rights.\nSleeping, Dining and Parlour Car Department.\nSleeping cars were operated by a separate department of the railway that included the dining and parlour cars and aptly named as the Sleeping, Dining and Parlour Car Department. The CPR decided from the very beginning that it would operate its own sleeping cars, unlike railways in the United States that depended upon independent companies that specialized in providing cars and porters, including building the cars themselves. Pullman was long a famous name in this regard; its Pullman porters were legendary. Other early companies included the Wagner Palace Car Company. Bigger-sized berths and more comfortable surroundings were built by order of the CPR's General Manager, William Van Horne, who was a large man himself. Providing and operating their own cars allowed better control of the service provided as well as keeping all of the revenue received, although dining-car services were never profitable. But railway managers realized that those who could afford to travel great distances expected such facilities, and their favourable opinion would bode well to attracting others to Canada and the CPR's trains.\nExpress.\nW. C. Van Horne decided from the very beginning that the CPR would retain as much revenue from its various operations as it could. This translated into keeping express, telegraph, sleeping car and other lines of business for themselves, creating separate departments or companies as necessary. This was necessary as the fledgling railway would need all the income it could get, and in addition, he saw some of these ancillary operations such as express and telegraph as being quite profitable. Others such as sleeping and dining cars were kept in order to provide better control over the quality of service being provided to passengers. Hotels were likewise crucial to the CPR's growth by attracting travellers.\nDominion Express Company was formed independently in 1873 before the CPR itself, although train service did not begin until the summer of 1882 at which time it operated over some of track from Rat Portage (Kenora) Ontario west to Winnipeg, Manitoba. It was soon absorbed into the CPR and expanded everywhere the CPR went. It was renamed Canadian Pacific Express Company on September 1, 1926, and the headquarters moved from Winnipeg, to Toronto, and the company also handled the establishment of the first money order system in Canada. It was operated as a separate company with the railway charging them to haul express cars on trains, and was initially highly profitable.\nExpress operations consisted of separate cars included on existing Canadian Pacific routes, were typically charged on a less-than-carload basis, and transported a wide range of goods, including fresh goods like dairy or flowers, refrigerated goods such as fish, transport of cash and jewellery, livestock with handlers and in some cases goods that took an entire carload, such as automobiles.\nThe company later expanded to shipping by transport truck. The company eventually became unprofitable, possibly due to competition from trucking companies, was purchased by an employee buyout in 1994 and renamed itself Interlink Systems. The company failed quickly, and went into receivership in 1997.\nSpecial trains.\nSilk trains.\nBetween the 1890s and 1933, the CPR transported raw silk from Vancouver, where it had been shipped from the Orient, to silk mills in New York and New Jersey. A silk train could carry several million dollars' worth of silk, so they had their own armed guards. To avoid train robberies and so minimize insurance costs, they travelled quickly and stopped only to change locomotives and crews, which was often done in under five minutes. The silk trains had right over all other trains; even passenger trains (including the royal train of 1939) would be put in sidings to make the silk trains' trip faster. At the end of World War II, the invention of nylon made silk less valuable, so the silk trains died out.\nFuneral trains.\nFuneral trains would carry the remains of important people, such as prime ministers. As the train would pass, mourners would be at certain spots to show respect. Two of the CPR's funeral trains are particularly well-known. On June 10, 1891, the funeral train of Prime Minister Sir John A. Macdonald ran from Ottawa to Kingston, Ontario. The train consisted of five heavily draped passenger cars and was pulled by 4-4-0 No. 283. On September 14, 1915, the funeral train of former CPR president Sir William Cornelius Van Horne ran from Montreal to Joliet, Illinois, pulled by 4-6-2 No. 2213.\nRoyal trains.\nThe CPR ran a number of trains that transported members of the Canadian royal family when they toured the country, taking them through Canada's scenery, forests, and small towns, and enabling people to see and greet them. Their trains were elegantly decorated; some had amenities such as a post office and barber shop. The CPR's most notable royal train was in 1939, when the CPR and the CNR had the honour of carrying King George VI and Queen Elizabeth during their coast-to-coast-and-back tour of Canada; one company took the royal couple from Quebec City to Vancouver and the other company took them on the return journey to Halifax. This was the first tour of Canada by its reigning monarch. The steam locomotives used to pull the train included CPR 2850, a Hudson (4-6-4) built by Montreal Locomotive Works in 1938, CNR 6400, a U-4-a Northern (4-8-4) and CNR 6028 a U-1-b Mountain (4-8-2) type. They were specially painted royal blue, with the exception of CNR 6028 which was not painted, with silver trim as was the entire train. The locomotives ran across Canada, through 25 changes of crew, without engine failure. The King, somewhat of a railbuff, rode in the cab when possible. After the tour, King George gave the CPR permission to use the term \"Royal Hudson\" for the CPR locomotives and to display Royal Crowns on their running boards. This applied only to the semi-streamlined locomotives (2820\u20132864), not the \"standard\" Hudsons (2800\u20132819).\nBetter Farming Train.\nCPR provided the rolling stock for the Better Farming Train which toured rural Saskatchewan between 1914 and 1922 to promote the latest information on agricultural research. It was staffed by the University of Saskatchewan and operating expenses were covered by the Department of Agriculture.\nSchool cars.\nBetween 1927 and the early 1950s, the CPR ran a school car to reach children who lived in Northern Ontario, far from schools. A teacher would travel in a specially designed car to remote areas and would stay to teach in one area for two to three days, then leave for another area. Each car had a blackboard and a few sets of chairs and desks. They also contained miniature libraries and accommodation for the teacher.\n\"Silver Streak\".\nMajor shooting for the 1976 film \"Silver Streak\", a fictional comedy tale of a murder-ridden train trip from Los Angeles to Chicago, was done on the CPR, mainly in the Alberta area with station footage at Toronto's Union Station. The train set was so lightly disguised as the fictional \"AMRoad\" that the locomotives and cars still carried their original names and numbers, along with the easily identifiable CP Rail red-striped paint scheme. Most of the cars are still in revenue service on Via Rail Canada; the lead locomotive (CP 4070) and the second unit (CP 4067) were sold to Via Rail and CTCUM respectively.\nHoliday Train.\nStarting in 1999, CP runs a Holiday Train along its main line during the months of November and December. The Holiday Train celebrates the holiday season and collects donations for community food banks and hunger issues. The Holiday Train also provides publicity for CP and a few of its customers. Each train has a box car stage for entertainers who are travelling along with the train.\nThe train is a freight train, but also pulls vintage passenger cars which are used as lodging/transportation for the crew and entertainers. Only entertainers and CP employees are allowed to board the train aside from a coach car that takes employees and their families from one stop to the next. All donations collected in a community remain in that community for distribution.\nThere are two Holiday Trains that cover 150 stops in Canada and the United States Northeast and Midwest. Each train is roughly in length with brightly decorated railway cars, including a modified box car that has been turned into a travelling stage for performers. They are each decorated with hundred of thousands of LED Christmas lights. In 2013 to celebrate the program's 15th year, three signature events were held in Hamilton, Ontario, Calgary, Alberta, and Cottage Grove, Minnesota, to further raise awareness for hunger issues.\nThe trains feature different entertainers each year; in 2016, one train featured Dallas Smith and the Odds, while the other featured Colin James and Kelly Prescott. After its 20th anniversary tour in 2018, which hosted Terri Clark, Sam Roberts Band, The Trews and Willy Porter, the tour reported to have raised more than and collected more than of food since 1999.\nRoyal Canadian Pacific.\nOn June 7, 2000, the CPR inaugurated the Royal Canadian Pacific, a luxury excursion service that operates between the months of June and September. It operates along a route from Calgary, through the Columbia Valley in British Columbia, and returning to Calgary via Crowsnest Pass. The trip takes six days and five nights. The train consists of up to eight luxury passenger cars built between 1916 and 1931 and is powered by first-generation diesel locomotives.\nSteam train.\nIn 1998, the CPR repatriated one of its former passenger steam locomotives that had been on static display in the United States following its sale in January 1964, long after the close of the steam era. CPR Hudson 2816 was re-designated \"Empress 2816\" following a 30-month restoration that cost in excess of $1\u00a0million. It was subsequently returned to service to promote public relations. It has operated across much of the CPR system, including lines in the U.S. and been used for various charitable purposes; 100% of the money raised goes to the nationwide charity Breakfast for Learning \u2014 the CPR bears all of the expenses associated with the operation of the train. 2816 is the subject of \"Rocky Mountain Express\", a 2011 IMAX film which follows the locomotive on an eastbound journey beginning in Vancouver, and which tells the story of the building of the CPR. 2816 has been stored indefinitely since 2012 after CEO E. Hunter Harrison discontinued the steam program.\nThe locomotive was fired up on November 13, 2020, for a steam test and moved around the Ogden campus yard. At the time, CP had plans to utilize the locomotive only for a special Holiday Train at Home broadcast, after which it was put in storage. However, in mid-2021, CEO Keith Creel announced intentions to bring 2816 back to full operational status, for a tour from their Calgary headquarters to Mexico City, if the merger with Kansas City Southern Railway was approved by the Surface Transportation Board in the United States. Work on the needed overhaul began in earnest in late 2021 for a planned date in 2023. On April 24, 2024, No.\u00a02816 began its \"Final Spike Steam Tour\" for the Canadian Pacific Kansas City, running from Calgary to Mexico City.\nSpirit Train.\nIn 2008, Canadian Pacific partnered with the 2010 Olympic and Paralympic Winter Games to present a \"Spirit Train\" tour that featured Olympic-themed events at various stops. Colin James was a headline entertainer. Several stops were met by protesters who argued that the games were slated to take place on stolen indigenous land.\nCP Canada 150 Train.\nIn 2017, CP ran the CP Canada 150 Train from Port Moody to Ottawa to celebrate Canada's 150th year since Confederation. The train stopped in 13 cities along its 3-week summer tour, offering a free block party and concert from Dean Brody, Kelly Prescott and Dallas Arcand. The heritage train drew out thousands to sign the special \"Spirit of Tomorrow\" car, where children were invited to write their wishes for the future of Canada and send them to Ottawa. Prime Minister Justin Trudeau and daughter Ella-Grace Trudeau also visited the train and rode it from Revelstoke to Calgary.\nNon-railway services.\nHistorically, Canadian Pacific operated several non-railway businesses. In 1971, these businesses were split off into the separate company Canadian Pacific Limited, and in 2001, that company was further split into five companies. CP no longer provides any of these services.\nCanadian Pacific Telegraphs.\nThe original charter of the CPR granted in 1881 provided for the right to create an electric telegraph and telephone service including charging for it. The telephone had barely been invented but telegraph was well established as a means of communicating quickly across great distances. Being allowed to sell this service meant the railway could offset the costs of constructing and maintaining a pole line along its tracks across vast distances for its own purposes which were largely for dispatching trains. It began doing so in 1882 as the separate Telegraph Department. It would go on to provide a link between the cables under the Atlantic and Pacific oceans when they were completed. Before the CPR line, messages to the west could be sent only via the United States.\nPaid for by the word, the telegram was an expensive way to send messages, but they were vital to businesses. An individual receiving a personal telegram was seen as being someone important except for those that transmitted sorrow in the form of death notices. Messengers on bicycles delivered telegrams and picked up a reply in cities. In smaller locations, the local railway station agent would handle this on a commission basis. To speed things, at the local end messages would first be telephoned. In 1931, it became the Communications Department in recognition of the expanding services provided which included telephones lines, news wire, ticker quotations for capital stocks and eventually teleprinters. All were faster than mail and very important to business and the public alike for many decades before mobile phones and computers came along. It was the coming of these newer technologies especially cellular telephones that eventually resulted in the demise of these services even after formation in 1967 of CN-CP Telecommunications in an effort to effect efficiencies through consolidation rather than competition. Deregulation in the 1980s, brought about mergers and the sale of remaining services and facilities.\nCanadian Pacific Radio.\nOn January 17, 1930, the CPR applied for licences to operate radio stations in 11 cities from coast-to-coast for the purpose of organising its own radio network in order to compete with the CNR Radio service. The CNR had built a radio network with the aim of promoting itself as well as entertaining its passengers during their travels. The onset of the Great Depression hurt the CPR's financial plan for a rival project and in April they withdrew their applications for stations in all but Toronto, Montreal and Winnipeg. CPR did not end up pursuing these applications but instead operated a phantom station in Toronto known as \"CPRY,\" with initials standing for \"Canadian Pacific Royal York\" which operated out of studios at CP's Royal York Hotel and leased time on CFRB and CKGW. A network of affiliates carried the CPR radio network's broadcasts in the first half of the 1930s, but the takeover of CNR's Radio service by the new Canadian Radio Broadcasting Commission removed CPR's need to have a network for competitive reasons and CPR's radio service was discontinued in 1935.\nCPR programming included a series of concert broadcasts from Montreal with an orchestra conducted by Douglas Clarke and a series called \"Concert Orchestra\" broadcast from the Royal York Hotel featuring conductor Rex Battle, and another series of concerts, this time sponsored by Imperial Oil and featuring conductor Reginald Stewart with a 55-piece orchestra and some of the leading soloists of the day, also performing at the Royal York.\nCanadian Pacific Steamships.\nSteamships played an important part in the history of CP from the very earliest days. During construction of the line in British Columbia even before the private CPR took over from the government contractor, ships were used to bring supplies to the construction sites. Similarly, to reach the isolated area of Superior in northern Ontario ships were used to bring in supplies to the construction work. While this work was going on there was already regular passenger service to the West. Trains operated from Toronto to Owen Sound where CPR steamships connected to Fort William where trains once again operated to reach Winnipeg. Before the CPR was completed the only way to reach the West was through the United States via St. Paul and Winnipeg. This Great Lakes steam ship service continued as an alternative route for many years and was always operated by the railway. Canadian Pacific passenger service on the lakes ended in 1965.\nIn 1883, CPR began purchasing sailing ships as part of a railway supply service on the Great Lakes. Over time, CPR became a railway company with widely organized water transportation auxiliaries including the Great Lakes service, the trans-Pacific service, the Pacific coastal service, the British Columbia lake and river service, the trans-Atlantic service and the Bay of Fundy Ferry service. In the 20th century, the company evolved into an intercontinental railway which operated two transoceanic services which connected Canada with Europe and with Asia. The range of CPR services were aspects of an integrated plan.\nOnce the railway was completed to British Columbia, the CPR chartered and soon bought their own passenger steamships as a link to the Orient. These sleek steamships were of the latest design and christened with \"Empress\" names (e. g., RMS \"Empress of Britain\", \"Empress of Canada\", \"Empress of Australia\", and so forth). Travel to and from the Orient and cargo, especially imported tea and silk, were an important source of revenue, aided by Royal Mail contracts. This was an important part of the All-Red Route linking the various parts of the British Empire.\nThe other ocean part was the Atlantic service to and from the United Kingdom, which began with acquisition of two existing lines, Beaver Line, owned by Elder Dempster and Allan Lines. These two segments became Canadian Pacific Ocean Services (later, Canadian Pacific Steamships) and operated separately from the various lake services operated in Canada, which were considered to be a direct part of the railway's operations. These trans-ocean routes made it possible to travel from Britain to Hong Kong using only the CPR's ships, trains and hotels. CP's 'Empress' ships became world-famous for their luxury and speed. They had a practical role, too, in transporting immigrants from much of Europe to Canada, especially to populate the vast prairies. They also played an important role in both world wars with many of them being lost to enemy action, including \"Empress of Britain\".\nThere were also a number of rail ferries operated over the years as well including, between Windsor, Ontario, and Detroit from 1890 until 1915. This began with two paddle-wheelers capable of carrying 16 cars. Passenger cars were carried as well as freight. This service ended in 1915 when the CPR made an agreement with the Michigan Central to use their Detroit River tunnel opened in 1910. Pennsylvania-Ontario Transportation Company was formed jointly with the PRR in 1906 to operate a ferry across Lake Erie between Ashtabula, Ohio, and Port Burwell, Ontario, to carry freight cars, mostly of coal, much of it to be burned in CPR steam locomotives. Only one ferry boat was ever operated, \"Ashtabula\", a large vessel which eventually sank in a harbour collision in Ashtabula on September 18, 1958, thus ending the service.\nCanadian Pacific Car and Passenger Transfer Company was formed by other interest in 1888 linking the CPR in Prescott, Ontario, and the NYC in Ogdensburg, New York. Service on this route had actually begun very early, in 1854, along with service from Brockville. A bridge built in 1958 ended passenger service however, freight continued until Ogdensburg's dock was destroyed by fire September 25, 1970, thus ending all service. CPC&amp;PTC was never owned by the CPR. Bay of Fundy ferry service was operated for passengers and freight for many years linking Digby, Nova Scotia, and Saint John, New Brunswick. Eventually, after 78 years, with the changing times the scheduled passenger services would all be ended as well as ocean cruises. Cargo would continue on both oceans with a change over to containers. CP was an intermodal pioneer especially on land with road and railway mixing to provide the best service. CP Ships was the final operation, and in the end it too left CP ownership when it was spun off in 2001. CP Ships was merged with Hapag-Lloyd in 2005.\nBritish Columbia Coast Steamships.\nThe Canadian Pacific Railway Coast Service (British Columbia Coast Steamships or BCCS) was established when the CPR acquired in 1901 Canadian Pacific Navigation Company (no relation) and its large fleet of ships that served 72 ports along the coast of British Columbia including on Vancouver Island. Service included the Vancouver-Victoria-Seattle \"Triangle Route\", Gulf Islands, Powell River, as well as Vancouver-Alaska service. BCCS operated a fleet of 14 passenger ships made up of a number of \"Princess\" ships, pocket versions of the famous oceangoing \"Empress\" ships along with a freighter, three tugs and five railway car barges. Popular with tourists, the Princess ships were famous in their own right especially \"Princess Marguerite\" (II) which operated from 1949 until 1985 and was the last coastal liner in operation. The most notorious of the princess ships, however, is \"Princess Sophia\", which sank with no survivors after striking the Vanderbilt Reef in Alaska's Lynn Canal, constituting the largest maritime disaster in the history of the Pacific Northwest. These services continued for many years until changing conditions in the late 1950s brought about their decline and eventual demise at the end of season in 1974. \"Princess Marguerite\" was acquired by the province's British Columbia Steamship (1975) Ltd. and continued to operate for a number of years. In 1977 although BCCSS was the legal name, it was rebranded as Coastal Marine Operations (CMO). By 1998 the company was bought by the Washington Marine Group which after purchase was renamed Seaspan Coastal Intermodal Company and then subsequently rebranded in 2011 as Seaspan Ferries Corporation. Passenger service ended in 1981.\nBritish Columbia Lake and River Service.\nThe Canadian Pacific Railway Lake and River Service (British Columbia Lake and River Service) developed slowly and in spurts of growth. CP began a long history of service in the Kootenays region of southern British Columbia beginning with the purchase in 1897 of the Columbia and Kootenay Steam Navigation Company which operated a fleet of steamers and barges on the Arrow Lakes and was merged into the CPR as the CPR Lake and River Service which also served the Arrow Lakes and Columbia River, Kootenay Lake and Kootenai River, Lake Okanagan and Skaha Lake, Slocan Lake, Trout Lake, and Shuswap Lake and the Thompson River/Kamloops Lake.\nAll of these lake operations had one thing in common, the need for shallow draft therefore sternwheelers were the choice of ship. Tugs and barges handled railway equipment including one operation that saw the entire train including the locomotive and caboose go along. These services gradually declined and ended in 1975 except for a freight barge on Slocan Lake. This was the one where the entire train went along since the barge was a link to an isolated section of track. The \"Iris G\" tug boat and a barge were operated under contract to CP Rail until the last train ran late in December 1988. The sternwheel steamship \"Moyie\" on Kootenay Lake was the last CPR passenger boat in BC lake service, having operated from 1898 until 1957. She became a beached historical exhibit, as are also the \"Sicamous\" and \"Naramata\" at Penticton on Lake Okanagan.\nCanadian Pacific Hotels.\nTo promote tourism and passenger ridership the Canadian Pacific established a series of first class hotels. These hotels became landmarks famous in their own right. They include the Algonquin in St. Andrews, Ch\u00e2teau Frontenac in Quebec, Royal York in Toronto, Minaki Lodge in Minaki Ontario, Hotel Vancouver, Empress Hotel in Victoria and the Banff Springs Hotel and Chateau Lake Louise in the Canadian Rockies. Several signature hotels were acquired from its competitor Canadian National during the 1980s, including the Jasper Park Lodge. The hotels retain their Canadian Pacific heritage, but are no longer operated by the railway. In 1998, Canadian Pacific Hotels acquired Fairmont Hotels, an American company, becoming Fairmont Hotels and Resorts; the combined corporation operated the historic Canadian properties as well as the Fairmont's U.S. properties until merged with Raffles Hotels and Resorts and Swiss\u00f4tel in 2006.\nCanadian Pacific Air Lines.\nCanadian Pacific Airlines, also called CP Air, operated from 1942 to 1987 and was the main competitor of Canadian government-owned Air Canada. Based at Vancouver International Airport, it served Canadian and international routes until it was purchased by Pacific Western Airlines which merged PWA and CP Air to create Canadian Airlines.\nLocomotives.\nSteam locomotives.\nIn the CPR's early years, it made extensive use of American-type 4-4-0 steam locomotives, and such examples of this are the \"Countess of Dufferin\" or No. 29. Later, considerable use was also made of the 4-6-0 type for passenger and 2-8-0 type for freight. Starting in the 20th century, the CPR bought and built hundreds of Ten-Wheeler-type 4-6-0s for passenger and freight service and similar quantities of 2-8-0s and 2-10-2s for freight. 2-10-2s were also used in passenger service on mountain routes. The CPR bought hundreds of 4-6-2 Pacifics between 1906 and 1948 with later versions being true dual-purpose passenger and fast-freight locomotives.\nThe CPR built hundreds of its own locomotives at its shops in Montreal, first at the \"New Shops\", as the DeLorimer shops were commonly referred to, and at the massive Angus Shops that replaced them in 1904. Some of the CPR's best-known locomotives were the 4-6-4 Hudsons. First built in 1929, they began a new era of modern locomotives with capabilities that changed how transcontinental passenger trains ran, eliminating frequent changes en route. The 2800s, as the Hudson type was known, ran from Toronto to Fort William, a distance of , while another lengthy engine district was from Winnipeg to Calgary .\nEspecially notable were the semi-streamlined H1 class Royal Hudsons, locomotives that were given their name because one of their class hauled the royal train carrying King George VI and Queen Elizabeth on the 1939 royal tour across Canada without change or failure. That locomotive, No.\u00a02850, is preserved in the Exporail exhibit hall of the Canadian Railway Museum in Saint-Constant, Quebec. One of the class, No.\u00a02860, was restored by the British Columbia government and used in excursion service on the British Columbia Railway between 1974 and 1999.\nThe CPR also made many of their older 2-8-0s, built in the turn of the century, into 2-8-2s.\nIn 1929, the CPR received its first 2-10-4 Selkirk locomotives, the largest steam locomotives to run in Canada and the British Empire. Named after the Selkirk Mountains where they served, these locomotives were well suited for steep grades. They were regularly used in passenger and freight service. The CPR would own 37 of these locomotives, including number 8000, an experimental high pressure engine. The last steam locomotives that the CPR received, in 1949, were Selkirks, numbered 5930\u20135935.\nDiesel locomotives.\nIn 1937, the CPR acquired its first diesel-electric locomotive, a custom-built one-of-a-kind switcher numbered 7000. This locomotive was not successful and was not repeated. Production-model diesels were imported from American Locomotive Company (Alco) starting with five model S-2 yard switchers in 1943 and followed by further orders. In 1949, operations on lines in Vermont were dieselized with Alco FA1 road locomotives (eight A and four B units), five ALCO RS-2 road switchers, three Alco S-2 switchers and three EMD E8 passenger locomotives. In 1948 Montreal Locomotive Works began production of ALCO designs.\nIn 1949, the CPR acquired 13 Baldwin-designed locomotives from the Canadian Locomotive Company for its isolated Esquimalt and Nanaimo Railway and Vancouver Island was quickly dieselized. Following that successful experiment, the CPR started to dieselize its main network. Dieselization was completed 11 years later, with its last steam locomotive running on 6 November 1960. The CPR's first-generation locomotives were mostly made by General Motors Diesel and Montreal Locomotive Works (American Locomotive Company designs), with some made by the Canadian Locomotive Company to Baldwin and Fairbanks Morse designs.\nCP was the first railway in North America to pioneer alternating current (AC) traction diesel-electric locomotives in 1984. In 1995, CP turned to GE Transportation for the first production AC traction locomotives in Canada, and now has the highest percentage of AC locomotives in service of all North American Class I railways.\nOn September 16, 2019, Progress Rail rolled out two SD70ACU rebuilds in Canadian Pacific heritage paint schemes; 7010 wears a Tuscan red and grey paint scheme with script writing, and the 7015 wears a similar paint scheme with block lettering.\nOn November 11, 2019, five SD70ACU units with commemorative military themes were unveiled during CPR's Remembrance Day ceremony. These units are numbered 7020\u20137023, with 7024 being renumbered to 6644 to commemorate the date of D-Day: June 6, 1944.\nIn 2021, Canadian Pacific repainted two locomotives orange: ES44AC 8757 which was unveiled for National Day for Truth and Reconciliation in September 2021, and ES44AC 8781 to commemorate shipper Hapag-Lloyd.\nThe fleet includes these types:\nCorporate structure.\nCanadian Pacific Railway Limited ( ) is a Canadian railway transportation company that operates the Canadian Pacific Railway. It was created in 2001 when the CPR's former parent company, Canadian Pacific Limited, spun off its railway operations. On October 3, 2001, the company's shares began to trade on the New York Stock Exchange and the Toronto Stock Exchange under the \"CP\" symbol. During 2003, the company earned in freight revenue. In October 2008, Canadian Pacific Railway Ltd was named one of \"Canada's Top 100 Employers\" by Mediacorp Canada Inc., and was featured in \"Maclean's\". Later that month, CPR was named one of Alberta's Top Employers, which was reported in both the \"Calgary Herald\" and the \"Edmonton Journal\".\nMajor facilities.\nCanadian Pacific owned a large number of large yards and repair shops across their system, which were used for many operations ranging from intermodal terminals to classification yards. Below are some examples of these.\nHump yards.\nHump yards work by using a small hill over which cars are pushed, before being released down a slope and switched automatically into cuts of cars, ready to be made into outbound trains. Many of these yards were closed in 2012 and 2013 under Hunter Harrison's company-wide restructuring; only the St. Paul Yard hump remains open.\nAircraft.\nAs of February 2023, Transport Canada lists the following aircraft in its database and operate as ICAO airline designator CRR, and telephony RAILCAR."}
{"id": "5960", "revid": "41195652", "url": "https://en.wikipedia.org/wiki?curid=5960", "title": "Codon", "text": ""}
{"id": "5961", "revid": "49003240", "url": "https://en.wikipedia.org/wiki?curid=5961", "title": "Cognitive psychology", "text": "Cognitive psychology is the scientific study of human mental processes such as attention, language use, memory, perception, problem solving, creativity, and reasoning.\nCognitive psychology originated in the 1960s in a break from behaviorism, which held from the 1920s to 1950s that unobservable mental processes were outside the realm of empirical science. This break came as researchers in linguistics and cybernetics, as well as applied psychology, used models of mental processing to explain human behavior.\nWork derived from cognitive psychology was integrated into other branches of psychology and various other modern disciplines like cognitive science, linguistics, and economics.\nHistory.\nPhilosophically, ruminations on the human mind and its processes have been around since the times of the ancient Greeks. In 387 BCE, Plato had suggested that the brain was the seat of the mental processes. In 1637, Ren\u00e9 Descartes posited that humans are born with innate ideas and forwarded the idea of mind-body dualism, which would come to be known as substance dualism (essentially the idea that the mind and the body are two separate substances). From that time, major debates ensued through the 19th century regarding whether human thought was solely experiential (empiricism), or included innate knowledge (nativism). Some of those involved in this debate included George Berkeley and John Locke on the side of empiricism, and Immanuel Kant on the side of nativism.\nWith the philosophical debate continuing, the mid to late 19th century was a critical time in the development of psychology as a scientific discipline. Two discoveries that would later play substantial roles in cognitive psychology were Paul Broca's discovery of the area of the brain largely responsible for language production, and Carl Wernicke's discovery of an area thought to be mostly responsible for comprehension of language. Both areas were subsequently formally named for their founders, and disruptions of an individual's language production or comprehension due to trauma or malformation in these areas have come to commonly be known as Broca's aphasia and Wernicke's aphasia.\nFrom the 1920s to the 1950s, the main approach to psychology was behaviorism. Initially, its adherents viewed mental events such as thoughts, ideas, attention, and consciousness as unobservable, hence outside the realm of a science of psychology. One early pioneer of cognitive psychology, whose work predated much of behaviorist literature, was Carl Jung. Jung introduced the hypothesis of cognitive functions in his 1921 book \"Psychological Types\". Another pioneer of cognitive psychology, who worked outside the boundaries (both intellectual and geographical) of behaviorism, was Jean Piaget. From 1926 to the 1950s and into the 1980s, he studied the thoughts, language, and intelligence of children and adults.\nIn the mid-20th century, four main influences arose that would inspire and shape cognitive psychology as a formal school of thought:\nUlric Neisser put the term \"cognitive psychology\" into common use through his book \"Cognitive Psychology\", published in 1967. Neisser's definition of \"cognition\" illustrates the then-progressive concept of cognitive processes:\nThe term \"cognition\" refers to all processes by which the sensory input is transformed, reduced, elaborated, stored, recovered, and used. It is concerned with these processes even when they operate in the absence of relevant stimulation, as in images and hallucinations. ... Given such a sweeping definition, it is apparent that cognition is involved in everything a human being might possibly do; that every psychological phenomenon is a cognitive phenomenon. But although cognitive psychology is concerned with all human activity rather than some fraction of it, the concern is from a particular point of view. Other viewpoints are equally legitimate and necessary. Dynamic psychology, which begins with motives rather than with sensory input, is a case in point. Instead of asking how a man's actions and experiences result from what he saw, remembered, or believed, the dynamic psychologist asks how they follow from the subject's goals, needs, or instincts.\nCognitive processes.\nThe main focus of cognitive psychologists is on the mental processes that affect behavior. Those processes include, but are not limited to, the following three stages of memory:\nAttention.\nThe psychological definition of attention is \"a state of focused awareness on a subset of the available sensation perception information\". A key function of attention is to identify irrelevant data and filter it out, enabling significant data to be distributed to the other mental processes. For example, the human brain may simultaneously receive auditory, visual, olfactory, taste, and tactile information. The brain is able to consciously handle only a small subset of this information, and this is accomplished through the attentional processes.\nAttention can be divided into two major attentional systems: exogenous control and endogenous control. Exogenous control works in a bottom-up manner and is responsible for orienting reflex, and pop-out effects. Endogenous control works top-down and is the more deliberate attentional system, responsible for divided attention and conscious processing.\nOne major focal point relating to attention within the field of cognitive psychology is the concept of divided attention. A number of early studies dealt with the ability of a person wearing headphones to discern meaningful conversation when presented with different messages into each ear; this is known as the dichotic listening task. Key findings involved an increased understanding of the mind's ability to both focus on one message, while still being somewhat aware of information being taken in from the ear not being consciously attended to. For example, participants (wearing earphones) may be told that they will be hearing separate messages in each ear and that they are expected to attend only to information related to basketball. When the experiment starts, the message about basketball will be presented to the left ear and non-relevant information will be presented to the right ear. At some point the message related to basketball will switch to the right ear and the non-relevant information to the left ear. When this happens, the listener is usually able to repeat the entire message at the end, having attended to the left or right ear only when it was appropriate. The ability to attend to one conversation in the face of many is known as the cocktail party effect.\nOther major findings include that participants cannot comprehend both passages when shadowing one passage, they cannot report the content of the unattended message, while they can shadow a message better if the pitches in each ear are different. However, while deep processing does not occur, early sensory processing does. Subjects did notice if the pitch of the unattended message changed or if it ceased altogether, and some even oriented to the unattended message if their name was mentioned.\nMemory.\nThe two main types of memory are short-term memory and long-term memory; however, short-term memory has become better understood to be working memory. Cognitive psychologists often study memory in terms of working memory.\nWorking memory.\nThough working memory is often thought of as just short-term memory, it is more clearly defined as the ability to process and maintain temporary information in a wide range of everyday activities in the face of distraction. The famously known capacity of memory of 7 plus or minus 2 is a combination of both memories in working memory and long-term memory.\nOne of the classic experiments is by Ebbinghaus, who found the serial position effect where information from the beginning and end of the list of random words were better recalled than those in the center. This primacy and recency effect varies in intensity based on list length. Its typical U-shaped curve can be disrupted by an attention-grabbing word; this is known as the Von Restorff effect.\nMany models of working memory have been made. One of the most regarded is the Baddeley and Hitch model of working memory. It takes into account both visual and auditory stimuli, long-term memory to use as a reference, and a central processor to combine and understand it all.\nA large part of memory is forgetting, and there is a large debate among psychologists of decay theory versus interference theory.\nLong-term memory.\nModern conceptions of memory are usually about long-term memory and break it down into three main sub-classes. These three classes are somewhat hierarchical in nature, in terms of the level of conscious thought related to their use.\nPerception.\nPerception involves both the physical senses (sight, smell, hearing, taste, touch, and proprioception) as well as the cognitive processes involved in interpreting those senses. Essentially, it is how people come to understand the world around them through the interpretation of stimuli. Early psychologists like Edward B. Titchener began to work with perception in their structuralist approach to psychology. Structuralism dealt heavily with trying to reduce human thought (or \"consciousness\", as Titchener would have called it) into its most basic elements by gaining an understanding of how an individual perceives particular stimuli.\nCurrent perspectives on perception within cognitive psychology tend to focus on particular ways in which the human mind interprets stimuli from the senses and how these interpretations affect behavior. An example of the way in which modern psychologists approach the study of perception is the research being done at the Center for Ecological Study of Perception and Action at the University of Connecticut (CESPA). One study at CESPA concerns ways in which individuals perceive their physical environment and how that influences their navigation through that environment.\nLanguage.\nPsychologists have had an interest in the cognitive processes involved with language that dates back to the 1870s, when Carl Wernicke proposed a model for the mental processing of language. Current work on language within the field of cognitive psychology varies widely. Cognitive psychologists may study language acquisition, individual components of language formation (like phonemes), how language use is involved in mood, or numerous other related areas.\nSignificant work has focused on understanding the timing of language acquisition and how it can be used to determine if a child has, or is at risk of, developing a learning disability. A study from 2012 showed that, while this can be an effective strategy, it is important that those making evaluations include all relevant information when making their assessments. Factors such as individual variability, socioeconomic status, short-term and long-term memory capacity, and others must be included in order to make valid assessments.\nMetacognition.\nMetacognition, in a broad sense, is the thoughts that a person has about their own thoughts. More specifically, metacognition includes things like:\nMuch of the current study regarding metacognition within the field of cognitive psychology deals with its application within the area of education. Being able to increase a student's metacognitive abilities has been shown to have a significant impact on their learning and study habits. One key aspect of this concept is the improvement of students' ability to set goals and self-regulate effectively to meet those goals. As a part of this process, it is also important to ensure that students are realistically evaluating their personal degree of knowledge and setting realistic goals (another metacognitive task).\nCommon phenomena related to metacognition include:\nModern perspectives.\nModern perspectives on cognitive psychology generally address cognition as a dual process theory, expounded upon by Daniel Kahneman in 2011. Kahneman differentiated the two styles of processing more, calling them intuition and reasoning. Intuition (or system 1), similar to associative reasoning, was determined to be fast and automatic, usually with strong emotional bonds included in the reasoning process. Kahneman said that this kind of reasoning was based on formed habits and very difficult to change or manipulate. Reasoning (or system 2) was slower and much more volatile, being subject to conscious judgments and attitudes.\nApplications.\nAbnormal psychology.\nFollowing the cognitive revolution, and as a result of many of the principal discoveries to come out of the field of cognitive psychology, the discipline of cognitive behavior therapy (CBT) evolved. Aaron T. Beck is generally regarded as the father of cognitive therapy, a particular type of CBT treatment. His work in the areas of recognition and treatment of depression has gained worldwide recognition. In his 1987 book titled \"Cognitive Therapy of Depression\", Beck puts forth three salient points with regard to his reasoning for the treatment of depression by means of therapy or therapy and antidepressants versus using a pharmacological-only approach:\n1. Despite the prevalent use of antidepressants, the fact remains that not all patients respond to them. Beck cites (in 1987) that only 60 to 65% of patients respond to antidepressants, and recent meta-analyses (a statistical breakdown of multiple studies) show very similar numbers.2. Many of those who do respond to antidepressants end up not taking their medications, for various reasons. They may develop side-effects or have some form of personal objection to taking the drugs.3. Beck posits that the use of psychotropic drugs may lead to an eventual breakdown in the individual's coping mechanisms. His theory is that the person essentially becomes reliant on the medication as a means of improving mood and fails to practice those coping techniques typically practiced by healthy individuals to alleviate the effects of depressive symptoms. By failing to do so, once the patient is weaned off of the antidepressants, they often are unable to cope with normal levels of depressed mood and feel driven to reinstate use of the antidepressants.\nSocial psychology.\nMany facets of modern social psychology have roots in research done within the field of cognitive psychology. Social cognition is a specific sub-set of social psychology that concentrates on processes that have been of particular focus within cognitive psychology, specifically applied to human interactions. Gordon B. Moskowitz defines social cognition as \"... the study of the mental processes involved in perceiving, attending to, remembering, thinking about, and making sense of the people in our social world\".\nThe development of multiple social information processing (SIP) models has been influential in studies involving aggressive and anti-social behavior. Kenneth Dodge's SIP model is one of, if not the most, empirically supported models relating to aggression. Among his research, Dodge posits that children who possess a greater ability to process social information more often display higher levels of socially acceptable behavior; that the type of social interaction that children have affects their relationships. His model asserts that there are five steps that an individual proceeds through when evaluating interactions with other individuals and that how the person interprets cues is key to their reactionary process.\nDevelopmental psychology.\nMany of the prominent names in the field of developmental psychology base their understanding of development on cognitive models. One of the major paradigms of developmental psychology, the Theory of Mind (ToM), deals specifically with the ability of an individual to effectively understand and attribute cognition to those around them. This concept typically becomes fully apparent in children between the ages of 4 and 6. Essentially, before the child develops ToM, they are unable to understand that those around them can have different thoughts, ideas, or feelings than themselves. The development of ToM is a matter of metacognition, or thinking about one's thoughts. The child must be able to recognize that they have their own thoughts and in turn, that others possess thoughts of their own.\nOne of the foremost minds with regard to developmental psychology, Jean Piaget, focused much of his attention on cognitive development from birth through adulthood. Though there have been considerable challenges to parts of his stages of cognitive development, they remain a staple in the realm of education. Piaget's concepts and ideas predated the cognitive revolution but inspired a wealth of research in the field of cognitive psychology and many of his principles have been blended with modern theory to synthesize the predominant views of today.\nEducational psychology.\nModern theories of education have applied many concepts that are focal points of cognitive psychology. Some of the most prominent concepts include:\nPersonality psychology.\nCognitive therapeutic approaches have received considerable attention in the treatment of personality disorders in recent years. The approach focuses on the formation of what it believes to be faulty schemata, centralized on judgmental biases and general cognitive errors.\nCognitive psychology vs. cognitive science.\nThe line between cognitive psychology and cognitive science can be blurry. Cognitive psychology is better understood as predominantly concerned with applied psychology and the understanding of psychological phenomena. Cognitive psychologists are often heavily involved in running psychological experiments involving human participants, with the goal of gathering information related to how the human mind takes in, processes, and acts upon inputs received from the outside world. The information gained in this area is then often used in the applied field of clinical psychology.\nCognitive science is better understood as predominantly concerned with a much broader scope, with links to philosophy, linguistics, anthropology, neuroscience, and particularly with artificial intelligence. It could be said that cognitive science provides the corpus of information feeding the theories used by cognitive psychologists. Cognitive scientists' research sometimes involves non-human subjects, allowing them to delve into areas which would come under ethical scrutiny if performed on human participants. For instance, they may do research implanting devices in the brains of rats to track the firing of neurons while the rat performs a particular task. Cognitive science is highly involved in the area of artificial intelligence and its application to the understanding of mental processes.\nCriticisms.\nLack of cohesion.\nSome observers have suggested that as cognitive psychology became a movement during the 1970s, the intricacies of the phenomena and processes it examined meant it also began to lose cohesion as a field of study. In \"Psychology: Pythagoras to Present\", for example, John Malone writes: \"Examinations of late twentieth-century textbooks dealing with \"cognitive psychology\", \"human cognition\", \"cognitive science\" and the like quickly reveal that there are many, many varieties of cognitive psychology and very little agreement about exactly what may be its domain.\" This misfortune produced competing models that questioned information-processing approaches to cognitive functioning such as Decision Making and Behavioral Sciences.\nControversies.\nIn the early years of cognitive psychology, behaviorist critics held that the empiricism it pursued was incompatible with the concept of internal mental states. However, cognitive neuroscience continues to gather evidence of direct correlations between physiological brain activity and mental states, endorsing the basis for cognitive psychology.\nThere is however disagreement between neuropsychologists and cognitive psychologists. Cognitive psychology has produced models of cognition which are not supported by modern brain science. It is often the case that the advocates of different cognitive models form a dialectic relationship with one another thus affecting empirical research, with researchers siding with their favorite theory. For example, advocates of \"mental model theory\" have attempted to find evidence that deductive reasoning is based on image thinking, while the advocates of \"mental logic theory\" have tried to prove that it is based on verbal thinking, leading to a disorderly picture of the findings from brain imaging and brain lesion studies. When theoretical claims are put aside, the evidence shows that interaction depends on the type of task tested, whether of visuospatial or linguistical orientation; but that there is also an aspect of reasoning which is not covered by either theory.\nSimilarly, neurolinguistics has found that it is easier to make sense of brain imaging studies when the theories are left aside. In the field of language cognition research, generative grammar has taken the position that language resides within its private cognitive module, while 'Cognitive Linguistics' goes to the opposite extreme by claiming that language is not an independent function, but operates on general cognitive capacities such as visual processing and motor skills. Consensus in neuropsychology however takes the middle position that, while language is a specialized function, it overlaps or interacts with visual processing. Nonetheless, much of the research in language cognition continues to be divided along the lines of generative grammar and Cognitive Linguistics; and this, again, affects adjacent research fields including language development and language acquisition.\nMajor research areas.\nCategorization\nKnowledge representation\nLanguage\nMemory\nPerception\nThinking"}
{"id": "5962", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=5962", "title": "Comet", "text": "A comet is an icy, small Solar System body that warms and begins to release gases when passing close to the Sun, a process called outgassing. This produces an extended, gravitationally unbound atmosphere or coma surrounding the nucleus, and sometimes a tail of gas and dust gas blown out from the coma. These phenomena are due to the effects of solar radiation and the outstreaming solar wind plasma acting upon the nucleus of the comet. Comet nuclei range from a few hundred meters to tens of kilometers across and are composed of loose collections of ice, dust, and small rocky particles. The coma may be up to 15 times Earth's diameter, while the tail may stretch beyond one astronomical unit. If sufficiently close and bright, a comet may be seen from Earth without the aid of a telescope and can subtend an arc of up to 30\u00b0 (60 Moons) across the sky. Comets have been observed and recorded since ancient times by many cultures and religions.\nComets usually have highly eccentric elliptical orbits, and they have a wide range of orbital periods, ranging from several years to potentially several millions of years. Short-period comets originate in the Kuiper belt or its associated scattered disc, which lie beyond the orbit of Neptune. Long-period comets are thought to originate in the Oort cloud, a spherical cloud of icy bodies extending from outside the Kuiper belt to halfway to the nearest star. Long-period comets are set in motion towards the Sun by gravitational perturbations from passing stars and the galactic tide. Hyperbolic comets may pass once through the inner Solar System before being flung to interstellar space. The appearance of a comet is called an apparition.\nExtinct comets that have passed close to the Sun many times have lost nearly all of their volatile ices and dust and may come to resemble small asteroids. Asteroids are thought to have a different origin from comets, having formed inside the orbit of Jupiter rather than in the outer Solar System. However, the discovery of main-belt comets and active centaur minor planets has blurred the distinction between asteroids and comets. In the early 21st century, the discovery of some minor bodies with long-period comet orbits, but characteristics of inner solar system asteroids, were called Manx comets. They are still classified as comets, such as C/2014 S3 (PANSTARRS). Twenty-seven Manx comets were found from 2013 to 2017.\n, there are 4,584 known comets. However, this represents a very small fraction of the total potential comet population, as the reservoir of comet-like bodies in the outer Solar System (in the Oort cloud) is about one trillion. Roughly one comet per year is visible to the naked eye, though many of those are faint and unspectacular. Particularly bright examples are called \"great comets\". Comets have been visited by uncrewed probes such as NASA's \"Deep Impact\", which blasted a crater on Comet Tempel 1 to study its interior, and the European Space Agency's \"Rosetta\", which became the first to land a robotic spacecraft on a comet.\nEtymology.\nThe word \"comet\" derives from the Old English from the Latin or . That, in turn, is a romanization of the Greek 'wearing long hair', and the \"Oxford English Dictionary\" notes that the term () already meant 'long-haired star, comet' in Greek. was derived from () 'to wear the hair long', which was itself derived from () 'the hair of the head' and was used to mean 'the tail of a comet'.\nThe astronomical symbol for comets (represented in Unicode) is , consisting of a small disc with three hairlike extensions.\nPhysical characteristics.\nNucleus.\nThe solid, core structure of a comet is known as the nucleus. Cometary nuclei are composed of an amalgamation of rock, dust, water ice, and frozen carbon dioxide, carbon monoxide, methane, and ammonia. As such, they are popularly described as \"dirty snowballs\" after Fred Whipple's model. Comets with a higher dust content have been called \"icy dirtballs\". The term \"icy dirtballs\" arose after observation of Comet 9P/Tempel 1 collision with an \"impactor\" probe sent by NASA Deep Impact mission in July 2005. Research conducted in 2014 suggests that comets are like \"deep fried ice cream\", in that their surfaces are formed of dense crystalline ice mixed with organic compounds, while the interior ice is colder and less dense.\nThe surface of the nucleus is generally dry, dusty or rocky, suggesting that the ices are hidden beneath a surface crust several metres thick. The nuclei contains a variety of organic compounds, which may include methanol, hydrogen cyanide, formaldehyde, ethanol, ethane, and perhaps more complex molecules such as long-chain hydrocarbons and amino acids. In 2009, it was confirmed that the amino acid glycine had been found in the comet dust recovered by NASA's Stardust mission. In August 2011, a report, based on NASA studies of meteorites found on Earth, was published suggesting DNA and RNA components (adenine, guanine, and related organic molecules) may have been formed on asteroids and comets.\nThe outer surfaces of cometary nuclei have a very low albedo, making them among the least reflective objects found in the Solar System. The Giotto space probe found that the nucleus of Halley's Comet (1P/Halley) reflects about four percent of the light that falls on it, and Deep Space 1 discovered that Comet Borrelly's surface reflects less than 3.0%; by comparison, asphalt reflects seven percent. The dark surface material of the nucleus may consist of complex organic compounds. Solar heating drives off lighter volatile compounds, leaving behind larger organic compounds that tend to be very dark, like tar or crude oil. The low reflectivity of cometary surfaces causes them to absorb the heat that drives their outgassing processes.\nComet nuclei with radii of up to have been observed, but ascertaining their exact size is difficult. The nucleus of 322P/SOHO is probably only in diameter. A lack of smaller comets being detected despite the increased sensitivity of instruments has led some to suggest that there is a real lack of comets smaller than across. Known comets have been estimated to have an average density of . Because of their low mass, comet nuclei do not become spherical under their own gravity and therefore have irregular shapes.\nRoughly six percent of the near-Earth asteroids are thought to be the extinct nuclei of comets that no longer experience outgassing, including 14827 Hypnos and 3552 Don Quixote.\nResults from the \"Rosetta\" and \"Philae\" spacecraft show that the nucleus of 67P/Churyumov\u2013Gerasimenko has no magnetic field, which suggests that magnetism may not have played a role in the early formation of planetesimals. Further, the ALICE spectrograph on \"Rosetta\" determined that electrons (within above the comet nucleus) produced from photoionization of water molecules by solar radiation, and not photons from the Sun as thought earlier, are responsible for the degradation of water and carbon dioxide molecules released from the comet nucleus into its coma. Instruments on the \"Philae\" lander found at least sixteen organic compounds at the comet's surface, four of which (acetamide, acetone, methyl isocyanate and propionaldehyde) have been detected for the first time on a comet.\nComa.\nThe streams of dust and gas thus released form a huge and extremely thin atmosphere around the comet called the \"coma\". The force exerted on the coma by the Sun's radiation pressure and solar wind cause an enormous \"tail\" to form pointing away from the Sun.\nThe coma is generally made of water and dust, with water making up to 90% of the volatiles that outflow from the nucleus when the comet is within 3 to 4 astronomical units (450,000,000 to 600,000,000\u00a0km; 280,000,000 to 370,000,000\u00a0mi) of the Sun. The parent molecule is destroyed primarily through photodissociation and to a much smaller extent photoionization, with the solar wind playing a minor role in the destruction of water compared to photochemistry. Larger dust particles are left along the comet's orbital path whereas smaller particles are pushed away from the Sun into the comet's tail by light pressure.\nAlthough the solid nucleus of comets is generally less than across, the coma may be thousands or millions of kilometers across, sometimes becoming larger than the Sun. For example, about a month after an outburst in October 2007, comet 17P/Holmes briefly had a tenuous dust atmosphere larger than the Sun. The Great Comet of 1811 had a coma roughly the diameter of the Sun. Even though the coma can become quite large, its size can decrease about the time it crosses the orbit of Mars around from the Sun. At this distance the solar wind becomes strong enough to blow the gas and dust away from the coma, and in doing so enlarging the tail. Ion tails have been observed to extend one astronomical unit (150\u00a0million km) or more.\nBoth the coma and tail are illuminated by the Sun and may become visible when a comet passes through the inner Solar System, the dust reflects sunlight directly while the gases glow from ionisation. Most comets are too faint to be visible without the aid of a telescope, but a few each decade become bright enough to be visible to the naked eye. Occasionally a comet may experience a huge and sudden outburst of gas and dust, during which the size of the coma greatly increases for a period of time. This happened in 2007 to Comet Holmes.\nIn 1996, comets were found to emit X-rays. This greatly surprised astronomers because X-ray emission is usually associated with very high-temperature bodies. The X-rays are generated by the interaction between comets and the solar wind: when highly charged solar wind ions fly through a cometary atmosphere, they collide with cometary atoms and molecules, \"stealing\" one or more electrons from the atom in a process called \"charge exchange\". This exchange or transfer of an electron to the solar wind ion is followed by its de-excitation into the ground state of the ion by the emission of X-rays and far ultraviolet photons.\nBow shock.\nBow shocks form as a result of the interaction between the solar wind and the cometary ionosphere, which is created by the ionization of gases in the coma. As the comet approaches the Sun, increasing outgassing rates cause the coma to expand, and the sunlight ionizes gases in the coma. When the solar wind passes through this ion coma, the bow shock appears.\nThe first observations were made in the 1980s and 1990s as several spacecraft flew by comets 21P/Giacobini\u2013Zinner, 1P/Halley, and 26P/Grigg\u2013Skjellerup. It was then found that the bow shocks at comets are wider and more gradual than the sharp planetary bow shocks seen at, for example, Earth. These observations were all made near perihelion when the bow shocks already were fully developed.\nThe \"Rosetta\" spacecraft observed the bow shock at comet 67P/Churyumov\u2013Gerasimenko at an early stage of bow shock development when the outgassing increased during the comet's journey toward the Sun. This young bow shock was called the \"infant bow shock\". The infant bow shock is asymmetric and, relative to the distance to the nucleus, wider than fully developed bow shocks.\nTails.\nIn the outer Solar System, comets remain frozen and inactive and are extremely difficult or impossible to detect from Earth due to their small size. Statistical detections of inactive comet nuclei in the Kuiper belt have been reported from observations by the Hubble Space Telescope but these detections have been questioned. As a comet approaches the inner Solar System, solar radiation causes the volatile materials within the comet to vaporize and stream out of the nucleus, carrying dust away with them.\nThe streams of dust and gas each form their own distinct tail, pointing in slightly different directions. The tail of dust is left behind in the comet's orbit in such a manner that it often forms a curved tail called the type II or dust tail. At the same time, the ion or type I tail, made of gases, always points directly away from the Sun because this gas is more strongly affected by the solar wind than is dust, following magnetic field lines rather than an orbital trajectory. On occasions\u2014such as when Earth passes through a comet's orbital plane, the antitail, pointing in the opposite direction to the ion and dust tails, may be seen.\nThe observation of antitails contributed significantly to the discovery of solar wind. The ion tail is formed as a result of the ionization by solar ultra-violet radiation of particles in the coma. Once the particles have been ionized, they attain a net positive electrical charge, which in turn gives rise to an \"induced magnetosphere\" around the comet. The comet and its induced magnetic field form an obstacle to outward flowing solar wind particles. Because the relative orbital speed of the comet and the solar wind is supersonic, a bow shock is formed upstream of the comet in the flow direction of the solar wind. In this bow shock, large concentrations of cometary ions (called \"pick-up ions\") congregate and act to \"load\" the solar magnetic field with plasma, such that the field lines \"drape\" around the comet forming the ion tail.\nIf the ion tail loading is sufficient, the magnetic field lines are squeezed together to the point where, at some distance along the ion tail, magnetic reconnection occurs. This leads to a \"tail disconnection event\". This has been observed on a number of occasions, one notable event being recorded on 20 April 2007, when the ion tail of Encke's Comet was completely severed while the comet passed through a coronal mass ejection. This event was observed by the STEREO space probe.\nIn 2013, ESA scientists reported that the ionosphere of the planet Venus streams outwards in a manner similar to the ion tail seen streaming from a comet under similar conditions.\"\nJets.\nUneven heating can cause newly generated gases to break out of a weak spot on the surface of comet's nucleus, like a geyser. These streams of gas and dust can cause the nucleus to spin, and even split apart. In 2010 it was revealed that sublimation of dry ice (frozen carbon dioxide) can power jets of material flowing out of a comet nucleus. Infrared imaging of Hartley\u00a02 shows such jets exiting and carrying with it dust grains into the coma.\nOrbital characteristics.\nMost comets are small Solar System bodies with elongated elliptical orbits that take them close to the Sun for a part of their orbit and then out into the further reaches of the Solar System for the remainder. Comets are often classified according to the length of their orbital periods: The longer the period the more elongated the ellipse.\nShort period.\nPeriodic comets or short-period comets are generally defined as those having orbital periods of less than 200 years. They usually orbit more-or-less in the ecliptic plane in the same direction as the planets. Their orbits typically take them out to the region of the outer planets (Jupiter and beyond) at aphelion; for example, the aphelion of Halley's Comet is a little beyond the orbit of Neptune. Comets whose aphelia are near a major planet's orbit are called its \"family\". Such families are thought to arise from the planet capturing formerly long-period comets into shorter orbits.\nAt the shorter orbital period extreme, Encke's Comet has an orbit that does not reach the orbit of Jupiter, and is known as an Encke-type comet. Short-period comets with orbital periods less than 20 years and low inclinations (up to 30 degrees) to the ecliptic are called traditional Jupiter-family comets (JFCs). Those like Halley, with orbital periods of between 20 and 200 years and inclinations extending from zero to more than 90 degrees, are called Halley-type comets (HTCs). there are 73 known Encke-type comets (six of which are classified as Near-earth objects (NEOs)), 106 HTCs (36 of which are NEOs), and 815 JFCs (153 of which are NEOs).\nRecently discovered main-belt comets form a distinct class, orbiting in more circular orbits within the asteroid belt.\nBecause their elliptical orbits frequently take them close to the giant planets, comets are subject to further gravitational perturbations. Short-period comets have a tendency for their aphelia to coincide with a giant planet's semi-major axis, with the JFCs being the largest group. It is clear that comets coming in from the Oort cloud often have their orbits strongly influenced by the gravity of giant planets as a result of a close encounter. Jupiter is the source of the greatest perturbations, being more than twice as massive as all the other planets combined. These perturbations can deflect long-period comets into shorter orbital periods.\nBased on their orbital characteristics, short-period comets are thought to originate from the centaurs and the Kuiper belt/scattered disc \u2014a disk of objects in the trans-Neptunian region\u2014whereas the source of long-period comets is thought to be the far more distant spherical Oort cloud (after the Dutch astronomer Jan Hendrik Oort who hypothesized its existence). Vast swarms of comet-like bodies are thought to orbit the Sun in these distant regions in roughly circular orbits. Occasionally the gravitational influence of the outer planets (in the case of Kuiper belt objects) or nearby stars (in the case of Oort cloud objects) may throw one of these bodies into an elliptical orbit that takes it inwards toward the Sun to form a visible comet. Unlike the return of periodic comets, whose orbits have been established by previous observations, the appearance of new comets by this mechanism is unpredictable. When flung into the orbit of the sun, and being continuously dragged towards it, tons of matter are stripped from the comets which greatly influence their lifetime; the more stripped, the shorter they live and vice versa.\nLong period.\nLong-period comets have highly eccentric orbits and periods ranging from 200 years to thousands or even millions of years. An eccentricity greater than 1 when near perihelion does not necessarily mean that a comet will leave the Solar System. For example, Comet McNaught had a heliocentric osculating eccentricity of 1.000019 near its perihelion passage epoch in January 2007 but is bound to the Sun with roughly a 92,600-year orbit because the eccentricity drops below 1 as it moves farther from the Sun. The future orbit of a long-period comet is properly obtained when the osculating orbit is computed at an epoch after leaving the planetary region and is calculated with respect to the center of mass of the Solar System. By definition long-period comets remain gravitationally bound to the Sun; those comets that are ejected from the Solar System due to close passes by major planets are no longer properly considered as having \"periods\". The orbits of long-period comets take them far beyond the outer planets at aphelia, and the plane of their orbits need not lie near the ecliptic. Long-period comets such as C/1999 F1 and C/2017 T2 (PANSTARRS) can have aphelion distances of nearly with orbital periods estimated around 6\u00a0million years.\nSingle-apparition or non-periodic comets are similar to long-period comets because they have parabolic or slightly hyperbolic trajectories when near perihelion in the inner Solar System. However, gravitational perturbations from giant planets cause their orbits to change. Single-apparition comets have a hyperbolic or parabolic osculating orbit which allows them to permanently exit the Solar System after a single pass of the Sun. The Sun's Hill sphere has an unstable maximum boundary of . Only a few hundred comets have been seen to reach a hyperbolic orbit (e &gt; 1) when near perihelion that using a heliocentric unperturbed two-body best-fit suggests they may escape the Solar System.\n, only two objects have been discovered with an eccentricity significantly greater than one: 1I/\u02bbOumuamua and 2I/Borisov, indicating an origin outside the Solar System. While \u02bbOumuamua, with an eccentricity of about 1.2, showed no optical signs of cometary activity during its passage through the inner Solar System in October 2017, changes to its trajectory\u2014which suggests outgassing\u2014indicate that it is probably a comet. On the other hand, 2I/Borisov, with an estimated eccentricity of about 3.36, has been observed to have the coma feature of comets, and is considered the first detected interstellar comet. Comet C/1980 E1 had an orbital period of roughly 7.1\u00a0million years before the 1982 perihelion passage, but a 1980 encounter with Jupiter accelerated the comet giving it the largest eccentricity (1.057) of any known solar comet with a reasonable observation arc.&lt;ref name=\"C/1980E1-jpl\"&gt;&lt;/ref&gt; Comets not expected to return to the inner Solar System include C/1980 E1, C/2000 U5, C/2001 Q4 (NEAT), C/2009 R1, C/1956 R1, and C/2007 F1 (LONEOS).\nSome authorities use the term \"periodic comet\" to refer to any comet with a periodic orbit (that is, all short-period comets plus all long-period comets), whereas others use it to mean exclusively short-period comets. Similarly, although the literal meaning of \"non-periodic comet\" is the same as \"single-apparition comet\", some use it to mean all comets that are not \"periodic\" in the second sense (that is, to include all comets with a period greater than 200 years).\nEarly observations have revealed a few genuinely hyperbolic (i.e. non-periodic) trajectories, but no more than could be accounted for by perturbations from Jupiter. Comets from interstellar space are moving with velocities of the same order as the relative velocities of stars near the Sun (a few tens of km per second). When such objects enter the Solar System, they have a positive specific orbital energy resulting in a positive velocity at infinity (formula_1) and have notably hyperbolic trajectories. A rough calculation shows that there might be four hyperbolic comets per century within Jupiter's orbit, give or take one and perhaps two orders of magnitude.\nOort cloud and Hills cloud.\nThe Oort cloud is thought to occupy a vast space starting from between to as far as from the Sun. This cloud encases the celestial bodies that start at the middle of the Solar System\u2014the Sun, all the way to outer limits of the Kuiper Belt. The Oort cloud consists of viable materials necessary for the creation of celestial bodies. The Solar System's planets exist only because of the planetesimals (chunks of leftover space that assisted in the creation of planets) that were condensed and formed by the gravity of the Sun. The eccentric made from these trapped planetesimals is why the Oort Cloud even exists. Some estimates place the outer edge at between . The region can be subdivided into a spherical outer Oort cloud of , and a doughnut-shaped inner cloud, the Hills cloud, of . The outer cloud is only weakly bound to the Sun and supplies the long-period (and possibly Halley-type) comets that fall to inside the orbit of Neptune. The inner Oort cloud is also known as the Hills cloud, named after Jack G. Hills, who proposed its existence in 1981. Models predict that the inner cloud should have tens or hundreds of times as many cometary nuclei as the outer halo; it is seen as a possible source of new comets that resupply the relatively tenuous outer cloud as the latter's numbers are gradually depleted. The Hills cloud explains the continued existence of the Oort cloud after billions of years.\nExocomets.\nExocomets beyond the Solar System have been detected and may be common in the Milky Way. The first exocomet system detected was around Beta Pictoris, a very young A-type main-sequence star, in 1987. A total of 11 such exocomet systems have been identified , using the absorption spectrum caused by the large clouds of gas emitted by comets when passing close to their star. For ten years the Kepler space telescope was responsible for searching for planets and other forms outside of the solar system. The first transiting exocomets were found in February 2018 by a group consisting of professional astronomers and citizen scientists in light curves recorded by the Kepler Space Telescope. After Kepler Space Telescope retired in October 2018, a new telescope called TESS Telescope has taken over Kepler's mission. Since the launch of TESS, astronomers have discovered the transits of comets around the star Beta Pictoris using a light curve from TESS. Since TESS has taken over, astronomers have since been able to better distinguish exocomets with the spectroscopic method. New planets are detected by the white light curve method which is viewed as a symmetrical dip in the charts readings when a planet overshadows its parent star. However, after further evaluation of these light curves, it has been discovered that the asymmetrical patterns of the dips presented are caused by the tail of a comet or of hundreds of comets.\nEffects of comets.\nConnection to meteor showers.\nAs a comet is heated during close passes to the Sun, outgassing of its icy components releases solid debris too large to be swept away by radiation pressure and the solar wind. If Earth's orbit sends it through that trail of debris, which is composed mostly of fine grains of rocky material, there is likely to be a meteor shower as Earth passes through. Denser trails of debris produce quick but intense meteor showers and less dense trails create longer but less intense showers. Typically, the density of the debris trail is related to how long ago the parent comet released the material. The Perseid meteor shower, for example, occurs every year between 9 and 13 August, when Earth passes through the orbit of Comet Swift\u2013Tuttle. Halley's Comet is the source of the Orionid shower in October.\nComets and impact on life.\nMany comets and asteroids collided with Earth in its early stages. Many scientists think that comets bombarding the young Earth about 4\u00a0billion years ago brought the vast quantities of water that now fill Earth's oceans, or at least a significant portion of it. Others have cast doubt on this idea. The detection of organic molecules, including polycyclic aromatic hydrocarbons, in significant quantities in comets has led to speculation that comets or meteorites may have brought the precursors of life\u2014or even life itself\u2014to Earth. In 2013 it was suggested that impacts between rocky and icy surfaces, such as comets, had the potential to create the amino acids that make up proteins through shock synthesis. The speed at which the comets entered the atmosphere, combined with the magnitude of energy created after initial contact, allowed smaller molecules to condense into the larger macro-molecules that served as the foundation for life. In 2015, scientists found significant amounts of molecular oxygen in the outgassings of comet 67P, suggesting that the molecule may occur more often than had been thought, and thus less an indicator of life as has been supposed.\nIt is suspected that comet impacts have, over long timescales, delivered significant quantities of water to Earth's Moon, some of which may have survived as lunar ice. Comet and meteoroid impacts are thought to be responsible for the existence of tektites and australites.\nFear of comets.\nFear of comets as acts of God and signs of impending doom was highest in Europe from AD 1200 to 1650. The year after the Great Comet of 1618, for example, Gotthard Arthusius published a pamphlet stating that it was a sign that the Day of Judgment was near. He listed ten pages of comet-related disasters, including \"earthquakes, floods, changes in river courses, hail storms, hot and dry weather, poor harvests, epidemics, war and treason and high prices\".\nBy 1700 most scholars concluded that such events occurred whether a comet was seen or not. Using Edmond Halley's records of comet sightings, however, William Whiston in 1711 wrote that the Great Comet of 1680 had a periodicity of 574 years and was responsible for the worldwide flood in the Book of Genesis, by pouring water on Earth. His announcement revived for another century fear of comets, now as direct threats to the world instead of signs of disasters. Spectroscopic analysis in 1910 found the toxic gas cyanogen in the tail of Halley's Comet, causing panicked buying of gas masks and quack \"anti-comet pills\" and \"anti-comet umbrellas\" by the public.\nFate of comets.\nDeparture (ejection) from Solar System.\nIf a comet is traveling fast enough, it may leave the Solar System. Such comets follow the open path of a hyperbola, and as such, they are called hyperbolic comets. Solar comets are only known to be ejected by interacting with another object in the Solar System, such as Jupiter. An example of this is Comet C/1980 E1, which was shifted from an orbit of 7.1\u00a0million years around the Sun, to a hyperbolic trajectory, after a 1980 close pass by the planet Jupiter. Interstellar comets such as 1I/\u02bbOumuamua and 2I/Borisov never orbited the Sun and therefore do not require a 3rd-body interaction to be ejected from the Solar System.\nExtinction.\nJupiter-family comets and long-period comets appear to follow very different fading laws. The JFCs are active over a lifetime of about 10,000 years or ~1,000 orbits whereas long-period comets fade much faster. Only 10% of the long-period comets survive more than 50 passages to small perihelion and only 1% of them survive more than 2,000 passages. Eventually most of the volatile material contained in a comet nucleus evaporates, and the comet becomes a small, dark, inert lump of rock or rubble that can resemble an asteroid. Some asteroids in elliptical orbits are now identified as extinct comets. Roughly six percent of the near-Earth asteroids are thought to be extinct comet nuclei.\nBreakup and collisions.\nThe nucleus of some comets may be fragile, a conclusion supported by the observation of comets splitting apart. A significant cometary disruption was that of Comet Shoemaker\u2013Levy 9, which was discovered in 1993. A close encounter in July 1992 had broken it into pieces, and over a period of six days in July 1994, these pieces fell into Jupiter's atmosphere\u2014the first time astronomers had observed a collision between two objects in the Solar System. Other splitting comets include 3D/Biela in 1846 and 73P/Schwassmann\u2013Wachmann from 1995 to 2006. Greek historian Ephorus reported that a comet split apart as far back as the winter of 372\u2013373 BC. Comets are suspected of splitting due to thermal stress, internal gas pressure, or impact.\nComets 42P/Neujmin and 53P/Van Biesbroeck appear to be fragments of a parent comet. Numerical integrations have shown that both comets had a rather close approach to Jupiter in January 1850, and that, before 1850, the two orbits were nearly identical. Another group of comets that is the result of fragmentation episodes is the Liller comet family made of C/1988\u2009A1 (Liller), C/1996\u2009Q1 (Tabur), C/2015\u2009F3 (SWAN), C/2019\u2009Y1 (ATLAS), and C/2023 V5 (Leonard).\nSome comets have been observed to break up during their perihelion passage, including great comets West and Ikeya\u2013Seki. Biela's Comet was one significant example when it broke into two pieces during its passage through the perihelion in 1846. These two comets were seen separately in 1852, but never again afterward. Instead, spectacular meteor showers were seen in 1872 and 1885 when the comet should have been visible. A minor meteor shower, the Andromedids, occurs annually in November, and it is caused when Earth crosses the orbit of Biela's Comet.\nSome comets meet a more spectacular end \u2013 either falling into the Sun or colliding with a planet or other body. Collisions between comets and planets or moons were common in the early Solar System: some of the many craters on the Moon, for example, may have been caused by comets. A recent collision of a comet with a planet occurred in July 1994 when Comet Shoemaker\u2013Levy 9 broke up into pieces and collided with Jupiter.\nNomenclature.\nThe names given to comets have followed several different conventions over the past two centuries. Prior to the early 20th century, most comets were referred to by the year when they appeared, sometimes with additional adjectives for particularly bright comets; thus, the \"Great Comet of 1680\", the \"Great Comet of 1882\", and the \"Great January Comet of 1910\".\nAfter Edmond Halley demonstrated that the comets of 1531, 1607, and 1682 were the same body and successfully predicted its return in 1759 by calculating its orbit, that comet became known as Halley's Comet. Similarly, the second and third known periodic comets, Encke's Comet and Biela's Comet, were named after the astronomers who calculated their orbits rather than their original discoverers. Later, periodic comets were usually named after their discoverers, but comets that had appeared only once continued to be referred to by the year of their appearance.\nIn the early 20th century, the convention of naming comets after their discoverers became common, and this remains so today. A comet can be named after its discoverers or an instrument or program that helped to find it. For example, in 2019, astronomer Gennadiy Borisov observed a comet that appeared to have originated outside of the solar system; the comet was named 2I/Borisov after him.\nHistory of study.\nEarly observations and thought.\nFrom ancient sources, such as Chinese oracle bones, it is known that comets have been noticed by humans for millennia. Until the sixteenth century, comets were usually considered bad omens of deaths of kings or noble men, or coming catastrophes, or even interpreted as attacks by heavenly beings against terrestrial inhabitants.\nAristotle (384\u2013322 BC) was the first known scientist to use various theories and observational facts to employ a consistent, structured cosmological theory of comets. He believed that comets were atmospheric phenomena, due to the fact that they could appear outside of the zodiac and vary in brightness over the course of a few days. Aristotle's cometary theory arose from his observations and cosmological theory that everything in the cosmos is arranged in a distinct configuration. Part of this configuration was a clear separation between the celestial and terrestrial, believing comets to be strictly associated with the latter. According to Aristotle, comets must be within the sphere of the moon and clearly separated from the heavens. Also in the 4th century BC, Apollonius of Myndus supported the idea that comets moved like the planets. Aristotelian theory on comets continued to be widely accepted throughout the Middle Ages, despite several discoveries from various individuals challenging aspects of it.\nIn the 1st century AD, Seneca the Younger questioned Aristotle's logic concerning comets. Because of their regular movement and imperviousness to wind, they cannot be atmospheric, and are more permanent than suggested by their brief flashes across the sky. He pointed out that only the tails are transparent and thus cloudlike, and argued that there is no reason to confine their orbits to the zodiac. In criticizing Apollonius of Myndus, Seneca argues, \"A comet cuts through the upper regions of the universe and then finally becomes visible when it reaches the lowest point of its orbit.\" While Seneca did not author a substantial theory of his own, his arguments would spark much debate among Aristotle's critics in the 16th and 17th centuries.\nIn the 1st century AD, Pliny the Elder believed that comets were connected with political unrest and death. Pliny observed comets as \"human like\", often describing their tails with \"long hair\" or \"long beard\". His system for classifying comets according to their color and shape was used for centuries.\nIn India, by the 6th century AD astronomers believed that comets were apparitions that re-appeared periodically. This was the view expressed in the 6th century by the astronomers Var\u0101hamihira and Bhadrabahu, and the 10th-century astronomer Bha\u1e6d\u1e6dotpala listed the names and estimated periods of certain comets, but it is not known how these figures were calculated or how accurate they were.\nThere is a claim that an Arab scholar in 1258 noted several recurrent appearances of a comet (or a type of comet), and though it's not clear if he considered it to be a single periodic comet, it might have been a comet with a period of around 63 years.\nIn 1301, the Italian painter Giotto was the first person to accurately and anatomically portray a comet. In his work \"Adoration of the Magi,\" Giotto's depiction of Halley's Comet in the place of the Star of Bethlehem would go unmatched in accuracy until the 19th century and be bested only with the invention of photography.\nAstrological interpretations of comets proceeded to take precedence clear into the 15th century, despite the presence of modern scientific astronomy beginning to take root. Comets continued to forewarn of disaster, as seen in the \"Luzerner Schilling\" chronicles and in the warnings of Pope Callixtus III. In 1578, German Lutheran bishop Andreas Celichius defined comets as \"the thick smoke of human sins\u00a0... kindled by the hot and fiery anger of the Supreme Heavenly Judge\". The next year, Andreas Dudith stated that \"If comets were caused by the sins of mortals, they would never be absent from the sky.\"\nScientific approach.\nCrude attempts at a parallax measurement of Halley's Comet were made in 1456, but were erroneous. Regiomontanus was the first to attempt to calculate diurnal parallax by observing the Great Comet of 1472. His predictions were not very accurate, but they were conducted in the hopes of estimating the distance of a comet from Earth.\nIn the 16th century, Tycho Brahe and Michael Maestlin demonstrated that comets must exist outside of Earth's atmosphere by measuring the parallax of the Great Comet of 1577. Within the precision of the measurements, this implied the comet must be at least four times more distant than from Earth to the Moon. Based on observations in 1664, Giovanni Borelli recorded the longitudes and latitudes of comets that he observed, and suggested that cometary orbits may be parabolic. Despite being a skilled astronomer, in his 1623 book \"The Assayer\", Galileo Galilei rejected Brahe's theories on the parallax of comets and claimed that they may be a mere optical illusion, despite little personal observation. In 1625, Maestlin's student Johannes Kepler upheld that Brahe's view of cometary parallax was correct. Additionally, mathematician Jacob Bernoulli published a treatise on comets in 1682.\nDuring the early modern period comets were studied for their astrological significance in medical disciplines. Many healers of this time considered medicine and astronomy to be inter-disciplinary and employed their knowledge of comets and other astrological signs for diagnosing and treating patients.\nIsaac Newton, in his \"Principia Mathematica\" of 1687, proved that an object moving under the influence of gravity by an inverse square law must trace out an orbit shaped like one of the conic sections, and he demonstrated how to fit a comet's path through the sky to a parabolic orbit, using the comet of 1680 as an example.\nHe describes comets as compact and durable solid bodies moving in oblique orbit and their tails as thin streams of vapor emitted by their nuclei, ignited or heated by the Sun. He suspected that comets were the origin of the life-supporting component of air. He pointed out that comets usually appear near the Sun, and therefore most likely orbit it. On their luminosity, he stated, \"The comets shine by the Sun's light, which they reflect,\" with their tails illuminated by \"the Sun's light reflected by a smoke arising from [the coma]\".\nIn 1705, Edmond Halley (1656\u20131742) applied Newton's method to 23 cometary apparitions that had occurred between 1337 and 1698. He noted that three of these, the comets of 1531, 1607, and 1682, had very similar orbital elements, and he was further able to account for the slight differences in their orbits in terms of gravitational perturbation caused by Jupiter and Saturn. Confident that these three apparitions had been three appearances of the same comet, he predicted that it would appear again in 1758\u201359. Halley's predicted return date was later refined by a team of three French mathematicians: Alexis Clairaut, Joseph Lalande, and Nicole-Reine Lepaute, who predicted the date of the comet's 1759 perihelion to within one month's accuracy. When the comet returned as predicted, it became known as Halley's Comet.\nAs early as the 18th century, some scientists had made correct hypotheses as to comets' physical composition. In 1755, Immanuel Kant hypothesized in his \"Universal Natural History\" that comets were condensed from \"primitive matter\" beyond the known planets, which is \"feebly moved\" by gravity, then orbit at arbitrary inclinations, and are partially vaporized by the Sun's heat as they near perihelion. In 1836, the German mathematician Friedrich Wilhelm Bessel, after observing streams of vapor during the appearance of Halley's Comet in 1835, proposed that the jet forces of evaporating material could be great enough to significantly alter a comet's orbit, and he argued that the non-gravitational movements of Encke's Comet resulted from this phenomenon.\nIn the 19th century, the Astronomical Observatory of Padova was an epicenter in the observational study of comets. Led by Giovanni Santini (1787\u20131877) and followed by Giuseppe Lorenzoni (1843\u20131914), this observatory was devoted to classical astronomy, mainly to the new comets and planets orbit calculation, with the goal of compiling a catalog of almost ten thousand stars. Situated in the Northern portion of Italy, observations from this observatory were key in establishing important geodetic, geographic, and astronomical calculations, such as the difference of longitude between Milan and Padua as well as Padua to Fiume. Correspondence within the observatory, particularly between Santini and another astronomer Giuseppe Toaldo, mentioned the importance of comet and planetary orbital observations.\nIn 1950, Fred Lawrence Whipple proposed that rather than being rocky objects containing some ice, comets were icy objects containing some dust and rock. This \"dirty snowball\" model soon became accepted and appeared to be supported by the observations of an armada of spacecraft (including the European Space Agency's \"Giotto\" probe and the Soviet Union's \"Vega 1\" and \"Vega 2\") that flew through the coma of Halley's Comet in 1986, photographed the nucleus, and observed jets of evaporating material.\nOn 22 January 2014, ESA scientists reported the detection, for the first definitive time, of water vapor on the dwarf planet Ceres, the largest object in the asteroid belt. The detection was made by using the far-infrared abilities of the Herschel Space Observatory. The finding is unexpected because comets, not asteroids, are typically considered to \"sprout jets and plumes\". According to one of the scientists, \"The lines are becoming more and more blurred between comets and asteroids.\" On 11 August 2014, astronomers released studies, using the Atacama Large Millimeter/Submillimeter Array (ALMA) for the first time, that detailed the distribution of HCN, HNC, , and dust inside the comae of comets C/2012 F6 (Lemmon) and C/2012 S1 (ISON).\nClassification.\nGreat comets.\nApproximately once a decade, a comet becomes bright enough to be noticed by a casual observer, leading such comets to be designated as great comets. Predicting whether a comet will become a great comet is notoriously difficult, as many factors may cause a comet's brightness to depart drastically from predictions. Broadly speaking, if a comet has a large and active nucleus, will pass close to the Sun, and is not obscured by the Sun as seen from Earth when at its brightest, it has a chance of becoming a great comet. However, Comet Kohoutek in 1973 fulfilled all the criteria and was expected to become spectacular but failed to do so. Comet West, which appeared three years later, had much lower expectations but became an extremely impressive comet.\nThe Great Comet of 1577 is a well-known example of a great comet. It passed near Earth as a non-periodic comet and was seen by many, including well-known astronomers Tycho Brahe and Taqi ad-Din. Observations of this comet led to several significant findings regarding cometary science, especially for Brahe.\nThe late 20th century saw a lengthy gap without the appearance of any great comets, followed by the arrival of two in quick succession\u2014Comet Hyakutake in 1996, followed by Hale\u2013Bopp, which reached maximum brightness in 1997 having been discovered two years earlier. The first great comet of the 21st century was C/2006 P1 (McNaught), which became visible to naked eye observers in January 2007. It was the brightest in over 40 years.\nSungrazing comets.\nA sungrazing comet is a comet that passes extremely close to the Sun at perihelion, generally within a few million kilometers. Although small sungrazers can be completely evaporated during such a close approach to the Sun, larger sungrazers can survive many perihelion passages. However, the strong tidal forces they experience often lead to their fragmentation.\nAbout 90% of the sungrazers observed with SOHO are members of the Kreutz group, which all originate from one giant comet that broke up into many smaller comets during its first passage through the inner Solar System. The remainder contains some sporadic sungrazers, but four other related groups of comets have been identified among them: the Kracht, Kracht 2a, Marsden, and Meyer groups. The Marsden and Kracht groups both appear to be related to Comet 96P/Machholz, which is the parent of two meteor streams, the Quadrantids and the Arietids.\nUnusual comets.\nOf the thousands of known comets, some exhibit unusual properties. Comet Encke (2P/Encke) orbits from outside the asteroid belt to just inside the orbit of the planet Mercury whereas the Comet 29P/Schwassmann\u2013Wachmann currently travels in a nearly circular orbit entirely between the orbits of Jupiter and Saturn. 2060 Chiron, whose unstable orbit is between Saturn and Uranus, was originally classified as an asteroid until a faint coma was noticed. Similarly, Comet Shoemaker\u2013Levy 2 was originally designated asteroid .\nLargest.\nThe largest known periodic comet is 95P/Chiron at 200\u00a0km in diameter that comes to perihelion every 50 years just inside of Saturn's orbit at 8 AU. The largest known Oort cloud comet is suspected of being Comet Bernardinelli-Bernstein at \u2248150\u00a0km that will not come to perihelion until January 2031 just outside of Saturn's orbit at 11 AU. The Comet of 1729 is estimated to have been \u2248100\u00a0km in diameter and came to perihelion inside of Jupiter's orbit at 4 AU.\nCentaurs.\nCentaurs typically behave with characteristics of both asteroids and comets. Centaurs can be classified as comets such as 60558 Echeclus, and 166P/NEAT. 166P/NEAT was discovered while it exhibited a coma, and so is classified as a comet despite its orbit, and 60558 Echeclus was discovered without a coma but later became active, and was then classified as both a comet and an asteroid (174P/Echeclus). One plan for \"Cassini\" involved sending it to a centaur, but NASA decided to destroy it instead.\nObservation.\nA comet may be discovered photographically using a wide-field telescope or visually with binoculars. However, even without access to optical equipment, it is still possible for the amateur astronomer to discover a sungrazing comet online by downloading images accumulated by some satellite observatories such as SOHO. SOHO's 2000th comet was discovered by Polish amateur astronomer Micha\u0142 Kusiak on 26 December 2010 and both discoverers of Hale\u2013Bopp used amateur equipment (although Hale was not an amateur).\nLost.\nA number of periodic comets discovered in earlier decades or previous centuries are now lost comets. Their orbits were never known well enough to predict future appearances or the comets have disintegrated. However, occasionally a \"new\" comet is discovered, and calculation of its orbit shows it to be an old \"lost\" comet. An example is Comet 11P/Tempel\u2013Swift\u2013LINEAR, discovered in 1869 but unobservable after 1908 because of perturbations by Jupiter. It was not found again until accidentally rediscovered by LINEAR in 2001. There are at least 18 comets that fit this category.\nIn popular culture.\nThe depiction of comets in popular culture is firmly rooted in the long Western tradition of seeing comets as harbingers of doom and as omens of world-altering change. Halley's Comet alone has caused a slew of sensationalist publications of all sorts at each of its reappearances. It was especially noted that the birth and death of some notable persons coincided with separate appearances of the comet, such as with writers Mark Twain (who correctly speculated that he'd \"go out with the comet\" in 1910) and Eudora Welty, to whose life Mary Chapin Carpenter dedicated the song \"Halley Came to Jackson\".\nIn times past, bright comets often inspired panic and hysteria in the general population, being thought of as bad omens. More recently, during the passage of Halley's Comet in 1910, Earth passed through the comet's tail, and erroneous newspaper reports inspired a fear that cyanogen in the tail might poison millions, whereas the appearance of Comet Hale\u2013Bopp in 1997 triggered the mass suicide of the Heaven's Gate cult.\nIn science fiction, the impact of comets has been depicted as a threat overcome by technology and heroism (as in the 1998 films \"Deep Impact\" and \"Armageddon\"), or as a trigger of global apocalypse (\"Lucifer's Hammer\", 1979) or zombies (\"Night of the Comet\", 1984). In Jules Verne's \"Off on a Comet\" a group of people are stranded on a comet orbiting the Sun, while a large crewed space expedition visits Halley's Comet in Sir Arthur C. Clarke's novel \"\".\nIn literature.\nThe long-period comet first recorded by Pons in Florence on 15 July 1825 inspired Lydia Sigourney's humorous poem in which all the celestial bodies argue over the comet's appearance and purpose."}
{"id": "5966", "revid": "40753954", "url": "https://en.wikipedia.org/wiki?curid=5966", "title": "Compost", "text": "Compost is a mixture of ingredients used as plant fertilizer and to improve soil's physical, chemical, and biological properties. It is commonly prepared by decomposing plant and food waste, recycling organic materials, and manure. The resulting mixture is rich in plant nutrients and beneficial organisms, such as bacteria, protozoa, nematodes, and fungi. Compost improves soil fertility in gardens, landscaping, horticulture, urban agriculture, and organic farming, reducing dependency on commercial chemical fertilizers. The benefits of compost include providing nutrients to crops as fertilizer, acting as a soil conditioner, increasing the humus or humic acid contents of the soil, and introducing beneficial microbes that help to suppress pathogens in the soil and reduce soil-borne diseases.\nAt the simplest level, composting requires gathering a mix of green waste (nitrogen-rich materials such as leaves, grass, and food scraps) and brown waste (woody materials rich in carbon, such as stalks, paper, and wood chips). The materials break down into humus in a process taking months. Composting can be a multistep, closely monitored process with measured inputs of water, air, and carbon- and nitrogen-rich materials. The decomposition process is aided by shredding the plant matter, adding water, and ensuring proper aeration by regularly turning the mixture in a process using open piles or windrows. Fungi, earthworms, and other detritivores further break up the organic material. Aerobic bacteria and fungi manage the chemical process by converting the inputs into heat, carbon dioxide, and ammonium ions.\nComposting is an important part of waste management, since food and other compostable materials make up about 20% of waste in landfills, and due to anaerobic conditions, these materials take longer to biodegrade in the landfill. Composting offers an environmentally superior alternative to using organic material for landfill because composting reduces methane emissions due to anaerobic conditions, and provides economic and environmental co-benefits. For example, compost can also be used for land and stream reclamation, wetland construction, and landfill cover.\nFundamentals.\nComposting is an aerobic method of decomposing organic solid wastes, so it can be used to recycle organic material. The process involves decomposing organic material into a humus-like material, known as compost, which is a good fertilizer for plants.\nComposting organisms require four equally important ingredients to work effectively:\nCertain ratios of these materials allow microorganisms to work at a rate that will heat up the compost pile. Active management of the pile (e.g., turning over the compost heap) is needed to maintain sufficient oxygen and the right moisture level. The air/water balance is critical to maintaining high temperatures until the materials are broken down.\nComposting is most efficient with a carbon-to-nitrogen ratio of about 25:1. Hot composting focuses on retaining heat to increase the decomposition rate, thus producing compost more quickly. Rapid composting is favored by having a carbon-to-nitrogen ratio of about 30 carbon units or less. Above 30, the substrate is nitrogen starved. Below 15, it is likely to outgas a portion of nitrogen as ammonia.\nNearly all dead plant and animal materials have both carbon and nitrogen in different amounts. Fresh grass clippings have an average ratio of about 15:1 and dry autumn leaves about 50:1 depending upon species. Composting is an ongoing and dynamic process; adding new sources of carbon and nitrogen consistently, as well as active management, is important.\nOrganisms.\nOrganisms can break down organic matter in compost if provided with the correct mixture of water, oxygen, carbon, and nitrogen. They fall into two broad categories: chemical decomposers, which perform chemical processes on the organic waste, and physical decomposers, which process the waste into smaller pieces through methods such as grinding, tearing, chewing, and digesting.\nPhases of composting.\nUnder ideal conditions, composting proceeds through three major phases:\nHot and cold composting \u2013 impact on timing.\nThe time required to compost material relates to the volume of material, the particle size of the inputs (e.g. wood chips break down faster than branches), and the amount of mixing and aeration. Generally, larger piles reach higher temperatures and remain in a thermophilic stage for days or weeks. This is hot composting and is the usual method for large-scale municipal facilities and agricultural operations.\nThe Berkeley method produces finished compost in 18 days. It requires assembly of at least of material at the outset and needs turning every two days after an initial four-day phase. Such short processes involve some changes to traditional methods, including smaller, more homogenized particle sizes in the input materials, controlling carbon-to-nitrogen ratio (C:N) at 30:1 or less, and careful monitoring of the moisture level.\nCold composting is a slower process that can take up to a year to complete. It results from smaller piles, including many residential compost piles that receive small amounts of kitchen and garden waste over extended periods. Piles smaller than tend not to reach and maintain high temperatures. Turning is not necessary with cold composting, although a risk exists that parts of the pile may go anaerobic as it becomes compacted or waterlogged.\nPathogen removal.\nComposting can destroy some pathogens and seeds, by reaching temperatures above . \nDealing with stabilized compost \u2013 i.e. composted material in which microorganisms have finished digesting the organic matter and the temperature has reached between \u2013 poses very little risk, as these temperatures kill pathogens and even make oocysts unviable. The temperature at which a pathogen dies depends on the pathogen, how long the temperature is maintained (seconds to weeks), and pH.\nCompost products such as compost tea and compost extracts have been found to have an inhibitory effect on \"Fusarium oxysporum\", \"Rhizoctonia\" species, and \"Pythium debaryanum,\" plant pathogens that can cause crop diseases. Aerated compost teas are more effective than compost extracts. The microbiota and enzymes present in compost extracts also have a suppressive effect on fungal plant pathogens. Compost is a good source of biocontrol agents like \"B. subtilis\", \"B. licheniformis,\" and P. \"chrysogenum\" that fight plant pathogens. Sterilizing the compost, compost tea, or compost extracts reduces the effect of pathogen suppression.\nDiseases that can be contracted from handling compost.\nWhen turning compost that has not gone through phases where temperatures above are reached, a mouth mask and gloves must be worn to protect from diseases that can be contracted from handling compost, including:\nOocytes are rendered unviable by temperatures over .\nEnvironmental benefits.\nCompost adds organic matter to the soil and increases the nutrient content and biodiversity of microbes in soil. Composting at home reduces the amount of green waste being hauled to dumps or composting facilities. The reduced volume of materials being picked up by trucks results in fewer trips, which in turn lowers the overall emissions from the waste-management fleet.\nMaterials that can be composted.\nPotential sources of compostable materials, or feedstocks, include residential, agricultural, and commercial waste streams. Residential food or yard waste can be composted at home, or collected for inclusion in a large-scale municipal composting facility. In some regions, it could also be included in a local or neighborhood composting project.\nOrganic solid waste.\nThe two broad categories of organic solid waste are green and brown. Green waste is generally considered a source of nitrogen and includes pre- and post-consumer food waste, grass clippings, garden trimmings, and fresh leaves. Animal carcasses, roadkill, and butcher residue can also be composted, and these are considered nitrogen sources.\nBrown waste is a carbon source. Typical examples are dried vegetation and woody material such as fallen leaves, straw, woodchips, limbs, logs, pine needles, sawdust, and wood ash, but not charcoal ash. Products derived from wood such as paper and plain cardboard are also considered carbon sources.\nAnimal manure and bedding.\nOn many farms, the basic composting ingredients are animal manure generated on the farm as a nitrogen source, and bedding as the carbon source. Straw and sawdust are common bedding materials. Nontraditional bedding materials are also used, including newspaper and chopped cardboard. The amount of manure composted on a livestock farm is often determined by cleaning schedules, land availability, and weather conditions. Each type of manure has its own physical, chemical, and biological characteristics. Cattle and horse manures, when mixed with bedding, possess good qualities for composting. Swine manure, which is very wet and usually not mixed with bedding material, must be mixed with straw or similar raw materials. Poultry manure must be blended with high-carbon, low-nitrogen materials.\nHuman excreta.\nHuman excreta, sometimes called \"humanure\" in the composting context, can be added as an input to the composting process since it is a nutrient-rich organic material. Nitrogen, which serves as a building block for important plant amino acids, is found in solid human waste. Phosphorus, which helps plants convert sunlight into energy in the form of ATP, can be found in liquid human waste.\nSolid human waste can be collected directly in composting toilets, or indirectly in the form of sewage sludge after it has undergone treatment in a sewage treatment plant. Both processes require capable design, as potential health risks need to be managed. In the case of home composting, a wide range of microorganisms, including bacteria, viruses, and parasitic worms, can be present in feces, and improper processing can pose significant health risks. In the case of large sewage treatment facilities that collect wastewater from a range of residential, commercial and industrial sources, there are additional considerations. The composted sewage sludge, referred to as biosolids, can be contaminated with a variety of metals and pharmaceutical compounds. Insufficient processing of biosolids can also lead to problems when the material is applied to land.\nUrine can be put on compost piles or directly used as fertilizer. Adding urine to compost can increase temperatures, so can increase its ability to destroy pathogens and unwanted seeds. Unlike feces, urine does not attract disease-spreading flies (such as houseflies or blowflies), and it does not contain the most hardy of pathogens, such as parasitic worm eggs.\nAnimal remains.\nAnimal carcasses may be composted as a disposal option. Such material is rich in nitrogen.\nComposting technologies.\nOther systems at household level.\nH\u00fcgelkultur (raised garden beds or mounds).\nThe practice of making raised garden beds or mounds filled with rotting wood is also called in German. It is in effect creating a nurse log that is covered with soil.\nBenefits of \"H\u00fcgelkultur\" garden beds include water retention and warming of soil. Buried wood acts like a sponge as it decomposes, able to capture water and store it for later use by crops planted on top of the bed.\nUses.\nAgriculture and gardening.\nOn open ground for growing wheat, corn, soybeans, and similar crops, compost can be broadcast across the top of the soil using spreader trucks or spreaders pulled behind a tractor. It is expected that the spread layer is very thin (approximately ) and worked into the soil prior to planting. Application rates of or more are not unusual when trying to rebuild poor soils or control erosion. Due to the extremely high cost of compost per unit of nutrients in the United States, on-farm use is relatively rare since rates over 4 tons/acre may not be affordable. This results from an over-emphasis on \"recycling organic matter\" than on \"sustainable nutrients.\" In countries such as Germany, where compost distribution and spreading are partially subsidized in the original waste fees, compost is used more frequently on open ground on the premise of nutrient \"sustainability\".\nIn plasticulture, strawberries, tomatoes, peppers, melons, and other fruits and vegetables are grown under plastic to control temperature, retain moisture and control weeds. Compost may be banded (applied in strips along rows) and worked into the soil prior to bedding and planting, be applied at the same time the beds are constructed and plastic laid down, or used as a top dressing.\nMany crops are not seeded directly in the field but are started in seed trays in a greenhouse. When the seedlings reach a certain stage of growth, they are transplanted in the field. Compost may be part of the mix used to grow the seedlings, but is not normally used as the only planting substrate. The particular crop and the seeds' sensitivity to nutrients, salts, etc. dictates the ratio of the blend, and maturity is important to insure that oxygen deprivation will not occur or that no lingering phyto-toxins remain.\nCompost can be added to soil, coir, or peat, as a tilth improver, supplying humus and nutrients. It provides a rich growing medium as absorbent material. This material contains moisture and soluble minerals, which provide support and nutrients. Although it is rarely used alone, plants can flourish from mixed soil that includes a mix of compost with other additives such as sand, grit, bark chips, vermiculite, perlite, or clay granules to produce loam. Compost can be tilled directly into the soil or growing medium to boost the level of organic matter and the overall fertility of the soil. Compost that is ready to be used as an additive is dark brown or even black with an earthy smell.\nGenerally, direct seeding into a compost is not recommended due to the speed with which it may dry, the possible presence of phytotoxins in immature compost that may inhibit germination, and the possible tie up of nitrogen by incompletely decomposed lignin. It is very common to see blends of 20\u201330% compost used for transplanting seedlings.\nCompost can be used to increase plant immunity to diseases and pests.\nCompost tea.\nCompost tea is made up of extracts of fermented water leached from composted materials. Composts can be either aerated or non-aerated depending on its fermentation process. Compost teas are generally produced from adding compost to water in a ratio of 1:4\u20131:10, occasionally stirring to release microbes.\nThere is debate about the benefits of aerating the mixture. Non-aerated compost tea is cheaper and less labor-intensive, but there are conflicting studies regarding the risks of phytotoxicity and human pathogen regrowth. Aerated compost tea brews faster and generates more microbes, but has potential for human pathogen regrowth, particularly when one adds additional nutrients to the mixture.\nField studies have shown the benefits of adding compost teas to crops due to organic matter input, increased nutrient availability, and increased microbial activity. They have also been shown to have a suppressive effect on plant pathogens and soil-borne diseases. The efficacy is influenced by a number of factors, such as the preparation process, the type of source the conditions of the brewing process, and the environment of the crops. Adding nutrients to compost tea can be beneficial for disease suppression, although it can trigger the regrowth of human pathogens like \"E. coli\" and \"Salmonella.\"\nCompost extract.\nCompost extracts are unfermented or non-brewed extracts of leached compost contents dissolved in any solvent.\nCommercial sale.\nCompost is sold as bagged potting mixes in garden centers and other outlets. This may include composted materials such as manure and peat but is also likely to contain loam, fertilizers, sand, grit, etc. Varieties include multi-purpose composts designed for most aspects of planting, John Innes formulations, grow bags, designed to have crops such as tomatoes directly planted into them. There are also a range of specialist composts available, e.g. for vegetables, orchids, houseplants, hanging baskets, roses, ericaceous plants, seedlings, potting on, etc.\nOther.\nCompost can also be used for land and stream reclamation, wetland construction, and landfill cover.\nThe temperatures generated by compost can be used to heat greenhouses, such as by being placed around the outside edges.\nRegulations.\nThere are process and product guidelines in Europe that date to the early 1980s (Germany, the Netherlands, Switzerland) and only more recently in the UK and the US. In both these countries, private trade associations within the industry have established loose standards, some say as a stop-gap measure to discourage independent government agencies from establishing tougher consumer-friendly standards. Compost is regulated in Canada and Australia as well.\nEPA Class A and B guidelines in the United States were developed solely to manage the processing and beneficial reuse of sludge, also now called biosolids, following the US EPA ban of ocean dumping. About 26 American states now require composts to be processed according to these federal protocols for pathogen and vector control, even though the application to non-sludge materials has not been scientifically tested. An example is that green waste composts are used at much higher rates than sludge composts were ever anticipated to be applied at. U.K guidelines also exist regarding compost quality, as well as Canadian, Australian, and the various European states.\nIn the United States, some compost manufacturers participate in a testing program offered by a private lobbying organization called the U.S. Composting Council. The USCC was originally established in 1991 by Procter &amp; Gamble to promote composting of disposable diapers, following state mandates to ban diapers in landfills, which caused a national uproar. Ultimately the idea of composting diapers was abandoned, partly since it was not proven scientifically to be possible, and mostly because the concept was a marketing stunt in the first place. After this, composting emphasis shifted back to recycling organic wastes previously destined for landfills. There are no bonafide quality standards in America, but the USCC sells a seal called \"Seal of Testing Assurance\" (also called \"STA\"). For a considerable fee, the applicant may display the USCC logo on products, agreeing to volunteer to customers a current laboratory analysis that includes parameters such as nutrients, respiration rate, salt content, pH, and limited other indicators.\nMany countries such as Wales and some individual cities such as Seattle and San Francisco require food and yard waste to be sorted for composting (San Francisco Mandatory Recycling and Composting Ordinance).\nThe USA is the only Western country that does not distinguish sludge-source compost from green-composts, and by default 50% of US states expect composts to comply in some manner with the federal EPA 503 rule promulgated in 1984 for sludge products.\nThere are health risk concerns about PFASs (\"forever chemicals\") levels in compost derived from sewage sledge sourced biosolids, and EPA has not set health risk standards for this. The Sierra Club recommends that home gardeners avoid the use of sewage sludge-base fertilizer and compost, in part due to potentially high levels of PFASs. The EPA PFAS Strategic Roadmap initiative, running from 2021 to 2024, will consider the full lifecycle of PFAS including health risks of PFAS in wastewater sludge.\nHistory.\nComposting dates back to at least the early Roman Empire and was mentioned as early as Cato the Elder's 160 BCE piece . Traditionally, composting involved piling organic materials until the next planting season, at which time the materials would have decayed enough to be ready for use in the soil. Methodologies for organic composting were part of traditional agricultural systems around the world.\nComposting began to modernize somewhat in the 1920s in Europe as a tool for organic farming. The first industrial station for the transformation of urban organic materials into compost was set up in Wels, Austria, in the year 1921. Early proponents of composting in farming include Rudolf Steiner, founder of a farming method called biodynamics, and Annie Franc\u00e9-Harrar, who was appointed on behalf of the government in Mexico and supported the country in 1950\u20131958 to set up a large humus organization in the fight against erosion and soil degradation. Sir Albert Howard, who worked extensively in India on sustainable practices, and Lady Eve Balfour were also major proponents of composting. Modern scientific composting was imported to America by the likes of J. I. Rodale \u2013 founder of Rodale, Inc. Organic Gardening, and others involved in the organic farming movement."}
{"id": "5968", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=5968", "title": "Computer-generated music", "text": ""}
{"id": "5970", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=5970", "title": "Capitol", "text": "A capitol, named after the Capitoline Hill in Rome, is usually a legislative building where a legislature meets and makes laws for its respective political entity.\nSpecific capitols include:\nCapitol, capitols, or The Capitol may also refer to:"}
{"id": "5973", "revid": "6056090", "url": "https://en.wikipedia.org/wiki?curid=5973", "title": "Cinema", "text": "Cinema may refer to:"}
{"id": "5974", "revid": "3170772", "url": "https://en.wikipedia.org/wiki?curid=5974", "title": "Corundum", "text": "Corundum is a crystalline form of aluminium oxide () typically containing traces of iron, titanium, vanadium, and chromium. It is a rock-forming mineral. It is a naturally transparent material, but can have different colors depending on the presence of transition metal impurities in its crystalline structure. Corundum has two primary gem varieties: ruby and sapphire. Rubies are red due to the presence of chromium, and sapphires exhibit a range of colors depending on what transition metal is present. A rare type of sapphire, padparadscha sapphire, is pink-orange.\nThe name \"corundum\" is derived from the Tamil-Dravidian word \"kurundam\" (ruby-sapphire) (appearing in Sanskrit as \"kuruvinda\").\nBecause of corundum's hardness (pure corundum is defined to have 9.0 on the Mohs scale), it can scratch almost all other minerals. It is commonly used as an abrasive on sandpaper and on large tools used in machining metals, plastics, and wood. Emery, a variety of corundum with no value as a gemstone, is commonly used as an abrasive. It is a black granular form of corundum, in which the mineral is intimately mixed with magnetite, hematite, or hercynite.\nIn addition to its hardness, corundum has a density of , which is unusually high for a transparent mineral composed of the low-atomic mass elements aluminium and oxygen.\nGeology and occurrence.\nCorundum occurs as a mineral in mica schist, gneiss, and some marbles in metamorphic terranes. It also occurs in low-silica igneous syenite and nepheline syenite intrusives. Other occurrences are as masses adjacent to ultramafic intrusives, associated with lamprophyre dikes and as large crystals in pegmatites. It commonly occurs as a detrital mineral in stream and beach sands because of its hardness and resistance to weathering. The largest documented single crystal of corundum measured about , and weighed . The record has since been surpassed by certain synthetic boules.\nCorundum for abrasives is mined in Zimbabwe, Pakistan, Afghanistan, Russia, Sri Lanka, and India. Historically it was mined from deposits associated with dunites in North Carolina, US, and from a nepheline syenite in Craigmont, Ontario. Emery-grade corundum is found on the Greek island of Naxos and near Peekskill, New York, US. Abrasive corundum is synthetically manufactured from bauxite.\nFour corundum axes dating to 2500\u00a0BC from the Liangzhu culture and Sanxingcun culture (the latter of which is located in Jintan District) have been discovered in China.\nSynthetic corundum.\nThe Verneuil process allows the production of flawless single-crystal sapphire and ruby gems of much larger size than normally found in nature. It is also possible to grow gem-quality synthetic corundum by flux-growth and hydrothermal synthesis. Because of the simplicity of the methods involved in corundum synthesis, large quantities of these crystals have become available on the market at a fraction of the cost of natural stones.\nSynthetic corundum has a lower environmental impact than natural corundum by avoiding destructive mining and conserving resources. However, its production is energy-intensive, contributing to carbon emissions if fossil fuels are used, and involves chemicals that can pose risks.\nApart from ornamental uses, synthetic corundum is also used to produce mechanical parts (tubes, rods, bearings, and other machined parts), scratch-resistant optics, scratch-resistant watch crystals, instrument windows for satellites and spacecraft (because of its transparency in the ultraviolet to infrared range), and laser components. For example, the KAGRA gravitational wave detector's main mirrors are sapphires, and Advanced LIGO considered sapphire mirrors. Corundum has also found use in the development of ceramic armour thanks to its high hardiness.\nStructure and physical properties.\nCorundum crystallizes with trigonal symmetry in the space group and has the lattice parameters and at standard conditions. The unit cell contains six formula units.\nThe toughness of corundum is sensitive to surface roughness and crystallographic orientation. It may be 6\u20137\u00a0MPa\u00b7m for synthetic crystals, and around 4\u00a0MPa\u00b7m for natural.\nIn the lattice of corundum, the oxygen atoms form a slightly distorted hexagonal close packing, in which two-thirds of the octahedral sites between the oxygen ions are occupied by aluminium ions. The absence of aluminium ions from one of the three sites breaks the symmetry of the hexagonal close packing, reducing the space group symmetry to and the crystal class to trigonal. The structure of corundum is sometimes described as a pseudohexagonal structure.\nThe Young's modulus of corundum (sapphire) has been reported by many different sources with values varying between 300 and 500 GPa, but a commonly cited value used for calculations is 345 GPa. The Young's modulus is temperature dependent, and has been reported in the [0001] direction as 435 GPa at 323 K and 386 GPa at 1,273 K. The shear modulus of corundum is 145 GPa, and the bulk modulus is 240 GPa.\nSingle crystal corundum fibers have potential applications in high temperature composites, and the Young's modulus is highly dependent on the crystallographic orientation along the fiber axis. The fiber exhibits a max modulus of 461 GPa when the crystallographic c-axis [0001] is aligned with the fiber axis, and minimum moduli ~373 GPa when a direction 45\u00b0 away from the c-axis is aligned with the fiber axis.\nThe hardness of corundum measured by indentation at low loads of 1-2 N has been reported as 22-23 GPa in major crystallographic planes: (0001) (basal plane), (100) (rhombohedral plane), (110) (prismatic plane), and (102). The hardness can drop significantly under high indentation loads. The drop with respect to load varies with the crystallographic plane due to the difference in crack resistance and propagation between directions. One extreme case is seen in the (0001) plane, where the hardness under high load (~1\u00a0kN) is nearly half the value under low load (1-2 N).\nPolycrystalline corundum formed through sintering and treated with a hot isostatic press process can achieve grain sizes in the range of 0.55-0.7 \u03bcm, and has been measured to have four-point bending strength between 600 and 700 MPa and three-point bending strength between 750 and 900 MPa.\nStructure type.\nBecause of its prevalence, corundum has also become the name of a major structure type (\"corundum type\") found in various binary and ternary compounds."}
{"id": "5975", "revid": "38984", "url": "https://en.wikipedia.org/wiki?curid=5975", "title": "CallOfCthulhuGame", "text": ""}
{"id": "5976", "revid": "1269073359", "url": "https://en.wikipedia.org/wiki?curid=5976", "title": "Capoeira", "text": "Capoeira () is a Afro-Brazilian martial art and game that includes elements of dance, acrobatics, music and spirituality.\nIt is known for its acrobatic and complex maneuvers, often involving hands on the ground and inverted kicks. It emphasizes flowing movements rather than fixed stances; the \"ginga\", a rocking step, is usually the focal point of the technique. Though often said to be a martial art disguised as a dance, capoeira served not only as a form of self defense, but also as a way to maintain spirituality and culture.\nCapoeira has been practiced among Black Brazilians for centuries. The date of its creation is unknown, but it was first mentioned in a judicial document under the name C\"apoeiragem\" in 1789, as \"the gravest of crimes\". In the 19th century, a street fighting style called capoeira carioca was developed. It was repeatedly outlawed and its performers persecuted, and it was declared totally illegal and banned in 1890. In the early 1930s, Mestre Bimba reformed traditional capoeira and incorporated elements of jiujitsu, gymnastics, and sports. As a result, the government viewed capoeira as a socially acceptable sport. In 1941, Mestre Pastinha later founded his school where he cultivated the traditional capoeira Angola, distinguishing it from reformed capoeira as the Brazilians' national sport.\nIn the late 1970s, trailblazers such as Mestre Acordeon started bringing capoeira to the US and Europe, helping the art become internationally recognized and practiced. On 26 November 2014, capoeira was granted a special protected status as intangible cultural heritage by UNESCO.\nMartial arts from the African diaspora similar to capoeira include \"knocking and kicking\" from the Sea Islands, and \"ladya\" from Martinique, both of which likely originate from Engolo.\nName.\nIn the past, many participants used the name \"angola\" or the term \"brincar de angola\" (\"playing angola\") for this art. In police documents, capoeira was known as \"capoeiragem\", with a practitioner being called \"capoeira\". Gradually, the art became known as capoeira with a practitioner being called a \"capoeirista\". In a narrower sense, capoeiragem meant a set of fighting skills. The term \"jogo de capoeira\" (capoeira game) is used to describe the art in the performative context.\nAlthough debated, the most widely accepted origin of the word \"capoeira\" comes from the Tupi words \"ka'a\" (\"forest\") \"pa\u0169\" (\"round\"), referring to the areas of low vegetation in the Brazilian interior where fugitive slaves would hide.\nHistory.\nIn the past, some participants used the name \"angola\" or the term \"brincar de angola\" (\"playing angola\") for this art. In formal documents, capoeira was known as \"capoeiragem\", with a practitioner being known as a \"capoeira\". Gradually, the art became known as capoeira with a practitioner being called a capoeirista.\nCapoeira first appeared among Africans in Brazil, during the early colonial period. According to the old capoeira mestres and tradition within the community, capoeira originates from Angola. Although the origin of capoeira is not entirely clear, many studies have supported the oral tradition, identifying engolo as an ancestral art and locating the Cunene region as its birthplace. Still, some authors believe there were more ancestors besides engolo. However, at the core of capoeira we find techniques developed in engolo, including crescent kicks, push kicks, sweeps, handstands, cartwheels, evasions and even the iconic \"Meia lua de compasso\", scorpion kick and L-kick.\nThe street capoeira in 19th-century Rio was very violent and far from the original art. This street-fighting \"capoeiragem\" was mix of five fighting techniques: foot kicks, head butts, hand blows, knife fight and stick-fighting, only the first of them arguably originates from Angolan art. That now extinct version of capoeira was called \"capoeira carioca\" (meaning of Rio de Janeiro).\nModern capoeira comes from Bahia, and was codified by mestre Bimba and mestre Pastinha, in \"regional\" and \"angola\" style. Despite their significant differences, both mestres introduced major innovations \u2014 they moved training and \"rodas\" away from the street, instituted the \"academia\", prescribed uniforms, started to teach women and presented capoeira to a broader audiences.\nTechniques.\nCapoeira is a fast and versatile martial art that is historically focused on fighting when outnumbered or at a technological disadvantage. The style emphasizes using the lower body to kick, sweep and take down their aggressors, using the upper body to assist those movements and occasionally attack as well. It features a series of complex positions and body postures that are meant to get chained in an uninterrupted flow, to strike, dodge and move without breaking motion, conferring the style with a characteristic unpredictability and versatility.\nThe \"ginga\" (literally: rocking back and forth; to swing) is the fundamental movement in capoeira, important both for attack and defense purposes. It has two main objectives. One is to keep the capoeirista in a state of constant motion, preventing them from being a still and easy target. The other, using also fakes and feints, is to mislead, fool or trick the opponent, leaving them open for an attack or a counter-attack.\nThe attacks in the capoeira should be done when opportunity arises, and though they can be preceded by feints or pokes, they must be precise and decisive, like a direct kick to the head, face or a vital body part, or a strong takedown. Most capoeira attacks are made with the legs, like direct or swirling kicks, rasteiras (leg sweeps), tesouras or knee strikes. Elbow strikes, punches and other forms of takedowns complete the main list. The head strike is a very important counter-attack move.\nThe defense is based on the principle of non-resistance, meaning avoiding an attack using evasive moves instead of blocking it. Avoids are called \"esquivas\", which depend on the direction of the attack and intention of the defender, and can be done standing or with a hand leaning on the floor. A block should only be made when the \"esquiva\" is completely non-viable. This fighting strategy allows quick and unpredictable counterattacks, the ability to focus on more than one adversary and to face empty-handed an armed adversary.\nA series of rolls and acrobatics (like the cartwheels called a\u00fa or the transitional position called negativa) allows the capoeirista to quickly overcome a takedown or a loss of balance, and to position themselves around the aggressor to lay up for an attack. It is this combination of attacks, defense and mobility that gives capoeira its perceived \"fluidity\" and choreography-like style.\nWeapons.\nThrough most of its history in Brazil, capoeira commonly featured weapons and weapon training, given its street fighting nature. Capoeiristas usually carried knives and bladed weapons with them, and the berimbau could be used to conceal those inside, or even to turn itself into a weapon by attaching a blade to its tip. The knife or razor was used in street \"rodas\" and/or against openly hostile opponents, and would be drawn quickly to stab or slash. Other hiding places for the weapons included hats and umbrellas.\nMestre Bimba included in his teachings a \"curso de especializa\u00e7\u00e3o\" or \"specialization course\", in which the pupils would be taught defenses against knives and guns, as well as the usage of knife, straight razor, scythe, club, \"chanfolo\" (double-edged dagger), \"fac\u00e3o\" (fac\u00f3n or machete) and \"tira-teima\" (cane sword). Upon graduating, pupils were given a red scarf, which marked their specialty. This course was scarcely used, and ceased after some time. A more common custom practised by Bimba and his students, however, was to furtively hand a weapon to a player before a \"jogo\" for them to use it to attack their opponent on Bimba's sign, with the other player's duty being to disarm them.\nThis weapon training is almost completely absent in current capoeira teachings, but some groups still practice the use of razors for ceremonial usage in the \"rodas\".\nAs a game.\nIn Bantu culture, the Nkhumbi term \"ochimama\" encapsulates the overlapping meanings of game, dance, and tradition. This overlap is also found in Afro-Brazilian folklore, where many similar forms of expression are called \"brincadeiras\" (games). Some scholars have interpreted capoeira as a way of concealing martial arts within dance movements. However, research from Angola suggests that the relationship between game, fight, and dance may be even deeper. These scholars propose that the ambivalence between these three elements is a fundamental aspect of the ancestral grammar shared by engolo and capoeira.\nPlaying capoeira is both a game and a method of practicing the application of capoeira movements in simulated combat. It can be played anywhere, but it's usually done in a \"roda\". During the game most capoeira moves are used, but capoeiristas usually avoid punches or elbow strikes unless it's a very aggressive game. The game does not focus on knocking down or defeating opponents, but rather on body dialogue and highlighting skills.\nRoda.\nThe \"roda\" (pronounced ) is a circle formed by capoeiristas and capoeira musical instruments, where every participant sings the typical songs and claps their hands following the music. Two \"capoeiristas\" enter the \"roda\" and play the game according to the style required by the musical rhythm. The game finishes when one of the musicians holding a berimbau determines it, when one of the \"capoeiristas\" decides to leave or call the end of the game, or when another capoeirista interrupts the game to start playing, either with one of the current players or with another \"capoeirista\".\nIn a \"roda\" every cultural aspect of capoeira is present, not only the martial side. Aerial acrobatics are common in a presentation \"roda\", while not seen as often in a more serious one. Takedowns, on the other hand, are common in a serious \"roda\" but rarely seen in presentations.\nBatizado.\nThe batizado (lit. baptism) is a ceremonial \"roda\" where new students will get recognized as capoeiristas and earn their first graduation. Also more experienced students may go up in rank, depending on their skills and capoeira culture. In Mestre Bimba's Capoeira Regional, batizado was the first time a new student would play capoeira following the sound of the berimbau.\nStudents enter the \"roda\" against a high-ranked capoeirista (such as a teacher or master) and normally the game ends with the student being taken down. In some cases the more experienced capoeirista can judge the takedown unnecessary. Following the batizado the new graduation, generally in the form of a cord, is given.\nTraditionally, the batizado is the moment when the new practitioner gets or formalizes their \"apelido\" (nickname). This tradition was created back when capoeira practice was considered a crime. To avoid having problems with the law, capoeiristas would present themselves in the capoeira community only by their nicknames.\nChamada.\n\"Chamada\" means 'call' and can happen at any time during a \"roda\" where the rhythm \"angola\" is being played. It happens when one player, usually the more advanced one, calls their opponent to a dance-like ritual. The opponent then approaches the caller and meets them to walk side by side. After it both resume normal play.\nWhile it may seem like a break time or a dance, the \"chamada\" is actually both a trap and a test, as the caller is just watching to see if the opponent will let his guard down so she can perform a takedown or a strike. It is a critical situation, because both players are vulnerable due to the close proximity and potential for a surprise attack. It's also a tool for experienced practitioners and masters of the art to test a student's awareness and demonstrate when the student left herself open to attack.\nThe use of the \"chamada\" can result in a highly developed sense of awareness and helps practitioners learn the subtleties of anticipating another person's hidden intentions. The \"chamada\" can be very simple, consisting solely of the basic elements, or the ritual can be quite elaborate including a competitive dialogue of trickery, or even theatric embellishments.\nVolta ao mundo.\nVolta ao mundo means \"around the world\".\nThe \"volta ao mundo\" takes place after an exchange of movements has reached a conclusion, or after there has been a disruption in the harmony of the game. In either of these situations, one player will begin walking around the perimeter of the circle counter-clockwise, and the other player will join the \"volta ao mundo\" in the opposite part of the roda, before returning to the normal game.\nMusic.\nMusic is integral to capoeira. It sets the tempo and style of game that is to be played within the roda. Typically the music is formed by instruments and singing. Rhythms (toques), controlled by a typical instrument called berimbau, differ from very slow to very fast, depending on the style of the roda.\nInstruments.\nCapoeira instruments are disposed in a row called bateria. It is traditionally formed by three berimbaus, two pandeiros, three atabaques, one agog\u00f4 and one ganz\u00e1, but this format may vary depending on the capoeira group's traditions or the roda style.\nThe berimbau is the leading instrument, determining the tempo and style of the music and game played. Two low-pitch berimbaus (called berra-boi and m\u00e9dio) form the base and a high-pitch berimbau (called viola) makes variations and improvisations. The other instruments must follow the berimbau's rhythm, free to vary and improvise a little, depending upon the capoeira group's musical style.\nAs the capoeiristas change their playing style significantly following the toque of the berimbau, which sets the game's speed, style and aggressiveness, it is truly the music that drives a capoeira game.\nSongs.\nMany of the songs are sung in a call and response format while others are in the form of a narrative. Capoeiristas sing about a wide variety of subjects. Some songs are about history or stories of famous capoeiristas. Other songs attempt to inspire players to play better. Some songs are about what is going on within the roda. Sometimes the songs are about life or love lost. Others have lighthearted and playful lyrics.\nThere are four basic kinds of songs in capoeira, the \"Lada\u00ednha\", \"Chula\", \"Corrido\" and \"Quadra\". The Lada\u00ednha is a narrative solo sung only at the beginning of a roda, often by a \"mestre\" (master) or most respected capoeirista present. The solo is followed by a \"louva\u00e7\u00e3o\", a call and response pattern that usually thanks God and one's master, among other things. Each call is usually repeated word-for-word by the responders. The Chula is a song where the singer part is much bigger than the chorus response, usually eight singer verses for one chorus response, but the proportion may vary. The Corrido is a song where the singer part and the chorus response are equal, normally two verses by two responses. Finally, the Quadra is a song where the same verse is repeated four times, either three singer verses followed by one chorus response, or one verse and one response.\nCapoeira songs can talk about virtually anything, being it about a historical fact, a famous capoeirista, trivial life facts, hidden messages for players, anything. Improvisation is very important also, while singing a song the main singer can change the music's lyrics, telling something that's happening in or outside the roda.\nPhilosophy.\n\"Mal\u00edcia\" (malice).\nThe basic term of capoeira philosophy is \"mal\u00edcia\" (). One aspect of \"malicia\" consists of deceiving the opponent into thinking that you are going to execute a certain move when in fact you are going to do something completely different. There is an example of \"malicia\" of Besouro who once fell to the ground during a game, crying like a woman and begging for mercy. Mestre Jo\u00e3o Pequeno claimed that he teaches his students how to play capoeira, but they should learn \"mal\u00edcia\" for themselves since it cannot be taught.\nThe meaning of \"mal\u00edcia\" in capoeira has expanded over time to cunning, suspicion, alertness, readiness, flexibility, and adaptation. Basically, it is the capacity to understand someone's intentions and making use of this understanding to misdirect someone as to your next move. In the contemporary capoeira, this is done good-naturedly, contrary to what the word may suggest. Nestor Capoeira explicated mal\u00edcia as follows:\nGregory Downey explains:\nThe \"ginga\" is the first principle of capoeira and the embodiment of malice. The continuous, ceaseless bodily motion, known as \"gingar\", is the principle that creates deception or trickery, catching the opponent off guard.\nThe \"b\u00ean\u00e7\u00e3o\" kick, ironically named, reflects another form of mal\u00edcia. Slave owners would gather slaves in the morning, often on Sundays, to offer blessings, despite their mistreatment. In a deceptive twist, \"b\u00ean\u00e7\u00e3o\" appears as a blessing but swiftly becomes an attack on the opponent's belly.\nMalandragem.\n\"Malandragem\" is a word that comes from \"malandro\", a man who used street smarts to make a living. In the 19th century, capoeira was quite similar to the type of urban person who was a constant source of trouble \u2014 the \"malandro\" (punk).\nIn the 19th century Rio de Janeiro, the capoeirista was a \"malandro\" (a rogue) and a criminal, expert in the use of kicks (\"golpes\"), sweeps (\"rasteiras\") and head-butts (\"cabe\u00e7adas\"), as well in the use of blade weapons.\nIn capoeira, \"malandragem\" is the ability to quickly understand an opponent's intentions, and during a fight or a game, fool, trick and deceive him.\nA popular Brazilian saying, \"Malandro demais se atrapalha\" means that when one tries to be too clever or smart, instead of confusing his opponent, he confuses himself.\nSpirituality.\nSpirituality in capoeira is shaped under the influence of various African beliefs. Some important concepts of candombl\u00e9, such as \"dend\u00e9\" and \"ax\u00e9\", which refer to different conceptions of energy, have become common among capoeiristas.\nBantu culture.\nDr Maya Talmon-Chvaicer suggests that capoeira should be explained in Bantu terms. For the African slaves, capoeira was a social expression that incorporated all the basic African elements: circle, dance, music, rituals and symbols. It also contains all the ingredients of a game from the Kongolese perspective: a means to train and prepare for life, providing the experience needed to strengthen the body and the soul.\nWithin the Bantu culture, \"the circle\" carries profound symbolism. Dancing in a circle holds significance, representing protection and strength, symbolizing the bond with the spirit world, life, and the divine.\nA major means of communication with the ancestors is music. Musical instruments play a pivotal role in bridging the realms of the living, the deceased, and the gods. This explains why African dances customarily commence by paying homage to the primary instrument, often through kneeling or bowing before it. This practice of appeasement and seeking divine assistance from the gods is mirrored in the capoeira tradition of kneeling before the \"berimbau\" during the \"ladainha\".\nAfrican martial arts naturally take the form of dance. In Bantu culture, dance is an integral part of daily life, encompassing song, music, movements, and rituals. This holistic view applies to Congo/Angola, where dance is intricately linked to song, music, and ritual.\nInverted worldview.\nIn Bantu religion, kal\u00fbnga represents the idea that, in the realm of the living everything is reversed from the realm of the ancestors. Where men walk on their feet, the spirits walk on their hands; where men reach their peak physical abilities, the ancestors reach their peak spirituality. Inhabitants of the ancestral realm are inverted compared to us, as viewed from our mirrored perspective. With this particular worldview, practitioners of African martial arts deliberately invert themselves upside down to emulate the ancestors, and to draw strength and power from the ancestral realm.\nOne of the capoeira ritual is performing the \"au\" at the beginning of the game. This act symbolizes a profound transition in Kongolese religion, where touching the ground with hands while feet are up in the air signifies the player crosses over to other worlds.\nCapoeira has been additionally shaped by the cosmic worldview of candombl\u00e9, an Afro-Brazilian religion that has engaged with various manifestations of natural energies. The capoeira player in past usually had his \"orix\u00e1\" or \"santo\" (patron saint) as Ogum (the Warrior) or Ox\u00f3ssi (the Hunter).\n\"Mandinga\" (magic).\nCapoeira holds a core of \"mandinga\", which can be translated as a magic, sorcery, witchcraft. Mandinga suggests an understanding of fundamental natural forces and their utilization through magic rituals to some extent. In the past, capoeiristas used protective amulets and performed specific rituals to ensure their safety. Same players \"do their mandinga\" before the game by drawing magical symbols on the ground with their fingers.\nSome magic elements in capoeira are clear and familiar, while others have become obscure over time. Folklorist Edison Carneiro noted that the \"ladainha\", sung before entering the capoeira circle, invokes the gods, adding a touch of mysticism to the ritual. Actions like touching the ground symbolize drawing signs in the dust, and gestures such as kissing hands, crossing oneself, and prayer are reminders of long-forgotten traditions, the Bantus' prayer for divine blessings, aid, and bravery in battle.\nMandinga is also a certain esthetic, where the game is expressive and sometimes theatrical, especially in the Angola style. An advanced capoeira player is sometimes referred to as a \"mandingueiro\", someone who embodies \"mandinga\".\nThe roots of the term \"mandingueiro\" would be a person who had the magic ability to avoid harm due to protection from the Orix\u00e1s. Alternately the word \"mandinga\" originates from the name of Mandinka people.\nStyles.\nDetermining styles in capoeira is difficult, since there was never a unity in the original capoeira, or a teaching method before the decade of 1920. However, a division between two styles and a sub-style is widely accepted.\nCapoeira Angola.\nCapoeira de Angola (Angolan capoeira) is the traditional style of capoeira. However, it can refer to two things:\nThe ideal of capoeira Angola is to maintain capoeira as close to its roots as possible. Although Pastinha strove to preserve the original Angolan art, he nevertheless introduced significant changes to capoeira practice of his time. He forbid weapon and violent moves, prescribed uniforms, moved training away from the street into the \"academia\", and started to teach women.\nCapoeira Angola is characterized by being strategic, with sneaking movements executed standing or near the floor depending on the situation to face, it values the traditions of \"mal\u00edcia\", \"malandragem\" and unpredictability of the original capoeira. The anthropologist Alejandro Frigerio defines capoeira Angola as art, versus capoeira Regional as sport. He emphasizes the following characteristics of contemporary capoeira Angola, namely: cunning, complementation (of the two players\" movements), a low game, the absence of violence, beautiful movements (according to a \"black aesthetic\"), slow music and the importance of ritual and theatricality.\nUnlike many other capoeira groups that play barefoot, \"angoleiros\" always train with shoes. When it comes to the color of the uniforms, there is a lack of uniformity within the style. Although mestre Pastinha at his academy required students to wear yellow and black jerseys, some of his successors have adopted white only uniforms within their schools.\nCapoeira Regional.\nCapoeira Regional began to take form in the 1920s, when Mestre Bimba met his future student, Jos\u00e9 Cisnando Lima. Both believed that capoeira was losing its martial side and concluded there was a need to re-strengthen and structure it. Bimba created his \"sequ\u00eancias de ensino\" (teaching combinations) and created capoeira's first teaching method. Advised by Cisnando, Bimba decided to call his style \"Luta Regional Baiana\", as capoeira was still illegal at that time.\nThe base of capoeira regional is the original capoeira without many of the aspects that were impractical in a real fight, with less subterfuge and more objectivity. Training focuses mainly on attack, dodging and counter-attack, giving high importance to precision and discipline. Bimba also added a few moves from other arts, notably the \"batuque\", an old street fight game invented by his father. Use of jumps or aerial acrobatics stay to a minimum, since one of its foundations is always keeping at least one hand or foot firmly attached to the ground.\n\"Capoeira Regional\" also introduced the first ranking method in capoeira. \"Regional\" had three levels: \"calouro\" (freshman), \"formado\" (graduated) and \"formado especializado\" (specialist). After 1964, when a student completed a course, a special celebration ceremony occurred, ending with the teacher tying a silk scarf around the capoeirista's neck.\nThe traditions of \"roda\" and capoeira game were kept, being used to put into use what was learned during training. The disposition of musical instruments, however, was changed, being made by a single berimbau and two pandeiros.\nThe \"Luta Regional Baiana\" soon became popular, finally changing capoeira's bad image. Mestre Bimba made many presentations of his new style, but the best known was the one made at 1953 to Brazilian president Get\u00falio Vargas, where the president would say: \"A Capoeira \u00e9 o \u00fanico esporte verdadeiramente nacional\" (Capoeira is the only truly national sport).\nCapoeira carioca.\nCapoeira carioca was a street fighting version of capoeira that existed in Rio de Janeiro during the 19th century, used by gangs. In capoeira carioca, all available means were used, including various types of weapons, such as knives, straight razors, clubs and machetes. Capoeira from this period is also known as \"capoeiragem\". The widespread violent capoeira practice in Rio led to a nationwide ban on capoeira. After the ban in 1890 and the subsequent mass arrests of capoeira gang members, this version of capoeira is generally extinct.\nThe main reformators and proponents of this fighting-oriented capoeira were Mestre Sinhozinho and Mestre Zuma.\nCapoeira Contempor\u00e2nea.\nCapoeira flourished in the city of S\u00e3o Paulo since the 1960s. Mestre Suassuna was prominent figure throughout this period. Mestre Canjiquinha played important role in shaping the capoeira style that began to emerge in S\u00e3o Paulo during the 1960s. This evolving style, which emerged in the 1960s and 1970s, drew from both Regional and Angola styles while maintaining its distinct characteristics. The majority of modern practitioners affirm to be neither Angola nor Regional, emphasizing that \"there is only one capoeira\".\nThis new capoeira incorporated not only berimbaus and pandeiros but also atabaque and agog\u00f4 into its musical ensemble. In contrast to Bimba's preference for quadras, these modern \"rodas\" typically commenced with ladainhas. The games in these \"rodas\" often featured a fast and upright style, even though they might start with an Angola toque and a slower game.\nNowadays the label \"Contempor\u00e2nea\" applies to any capoeira group who don't follow Regional or Angola styles, even the ones who mix capoeira with other martial arts. Some notable groups whose style cannot be described as either Angola or Regional but rather \"a style of their own\", include Senzala de Santos, Cord\u00e3o de Ouro and Abada. In the case of Cord\u00e3o de Ouro, the style may be described as \"Miudinho\", a low and fast-paced game, while in Senzala de Santos the style may described simply as \"Senzala de Santos\", an elegant, playful combination of Angola and Regional.\nRanks.\nBecause of its origin, capoeira never had unity or a general agreement. Ranking or graduating system follows the same path, as there never existed a ranking system accepted by most of the masters. That means graduation style varies depending on the group's traditions. The most common modern system uses colored ropes, called \"corda\" or \"cord\u00e3o\", tied around the waist. Some masters use different systems, or even no system at all. In a substantial number of groups (mainly of the Angola school) there is no visible ranking system. There can still be several ranks: student, treinel, professor, contra-mestre and mestre, but often no cordas (belts).\nThere are many entities (leagues, federations and association) with their own graduation system. The most usual is the system of the \"Confedera\u00e7\u00e3o Brasileira de Capoeira\" (Brazilian Capoeira Confederation), which adopts ropes using the colors of the Brazilian flag, green, yellow, blue and white. However, the \"Confedera\u00e7\u00e3o Brasileira de Capoeira\" is not widely accepted as the capoeira's main representative.\nBrazilian Capoeira Confederation system.\nSource:\nABAD\u00c1 - Capoeira system.\nMany Capoeira schools use a system taken from Abad\u00e1-Capoeira.\nABAD\u00c1 has a graduated cord system using colors that refer symbolically to nature and reflect the level of practice. The cord system does not so much reflect the practitioner's level of skill as much as their progress on their individual path as a member of the ABAD\u00c1 community. The cord system as outlined by Arte Capoeira Center \u2013 ABAD\u00c1 Capoeira is as follows.\nRelated activities.\nEven though those activities are strongly associated with capoeira, they have different meanings and origins.\nSamba de roda.\nPerformed by many capoeira groups, samba de roda is a traditional Brazilian dance and musical form that has been associated with capoeira for many decades. The orchestra is composed by \"pandeiro\", \"atabaque\", \"berimbau-viola\" (high pitch berimbau), chocalho, accompanied by singing and clapping. \"Samba de roda\" is considered one of the primitive forms of modern Samba.\nMaculel\u00ea.\nOriginally the \"Maculel\u00ea\" is believed to have been an indigenous armed fighting style, using two sticks or a machete. Nowadays it's a folkloric dance practiced with heavy Brazilian percussion. Many capoeira groups include \"Maculel\u00ea\" in their presentations.\nPuxada de rede.\n\"Puxada de Rede\" is a Brazilian folkloric theatrical play, seen in many capoeira performances. It is based on a traditional Brazilian legend involving the loss of a fisherman in a seafaring accident.\nCombat capoeira and MMA.\nCombat capoeira, often referred to as rough capoeira (\"capoeira dura\"), places a primary emphasis on combat. It is commonly observed in ring competitions and street \"rodas\", and sometimes even in graduations within certain groups.\nSeveral capoeira fighters have gained national reputation, including Mestre King Kong from Salvador, Mestre Maur\u00e3o from S\u00e3o Paulo, and King from Rio de Janeiro (formerly associated with Abad\u00e1). They advocate for capoeiristas to be skilled in playing intense games to ensure that the art retains its combat effectiveness.\nCapoeira fights have, on occasion, resulted in severe injuries and even fatalities, as seen in Petr\u00f3polis in 1996. The most suitable context for combat-focused capoeira appears to be the ring, where predetermined fighting rules provide clarity. In the tradition of Ciriaco, Sinhozinho, Bimba, and Arthur Em\u00eddio, contemporary capoeira fighters have expanded their training by incorporating various martial arts disciplines, including ju-jitsu, boxing, and taekwondo.\nEven Brazilian mixed martial arts champions like Marco Ruas acknowledge the significance of capoeira in their training. The use of capoeira techniques in free-style competitions shows to what extent the art still provides essential fighting skills."}
{"id": "5978", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=5978", "title": "Climate Change-Kyoto Protocol", "text": ""}
{"id": "5980", "revid": "48787904", "url": "https://en.wikipedia.org/wiki?curid=5980", "title": "Carbon sink", "text": "A carbon sink is a natural or artificial carbon sequestration process that \"removes a\u00a0greenhouse gas, an aerosol or a precursor of a greenhouse gas from the atmosphere\". These sinks form an important part of the natural carbon cycle. An overarching term is carbon pool, which is all the places where carbon on Earth can be, i.e. the atmosphere, oceans, soil, florae, fossil fuel reservoirs and so forth. A carbon sink is a type of carbon pool that has the capability to take up more carbon from the atmosphere than it releases.\nGlobally, the two most important carbon sinks are vegetation and the ocean. Soil is an important carbon storage medium. Much of the organic carbon retained in the soil of agricultural areas has been depleted due to intensive farming. \"Blue carbon\" designates carbon that is fixed via certain marine ecosystems. \"Coastal blue carbon\" includes mangroves, salt marshes and seagrasses. These make up a majority of ocean plant life and store large quantities of carbon. \"Deep blue carbon\" is located in international waters and includes carbon contained in \"continental shelf waters, deep-sea waters and the sea floor beneath them\".\nFor climate change mitigation purposes, the maintenance and enhancement of natural carbon sinks, mainly soils and forests, is important. In the past, human practices like deforestation and industrial agriculture have depleted natural carbon sinks. This kind of land use change has been one of the causes of climate change.\nDefinition.\nIn the context of climate change and in particular mitigation, a \"sink\" is defined as \"Any process, activity or mechanism which removes a\u00a0greenhouse gas, an aerosol or a precursor of a greenhouse gas from the atmosphere\".\nIn the case of non- greenhouse gases, sinks need not store the gas. Instead they can break it down into substances that have a reduced effect on global warming. For example, nitrous oxide can be reduced to harmless N2.\nRelated terms are \"carbon pool, reservoir, sequestration, source and uptake\". The same publication defines \"carbon pool\" as \"a\u00a0 reservoir\u00a0in the Earth system where elements, such as carbon [...], reside in various chemical forms for a period of time.\"\nBoth carbon pools and carbon sinks are important concepts in understanding the carbon cycle, but they refer to slightly different things. A carbon pool can be thought of as the overarching term, and carbon sink is then a particular type of carbon pool: A carbon pool is all the places where carbon can be stored (for example the atmosphere, oceans, soil, plants, and fossil fuels).\nTypes.\nThe amount of carbon dioxide varies naturally in a dynamic equilibrium with photosynthesis of land plants. The natural carbon sinks are:\nArtificial carbon sinks are those that store carbon in building materials or deep underground (geologic carbon sequestration). No major artificial systems remove carbon from the atmosphere on a large scale yet.\nPublic awareness of the significance of sinks has grown since passage of the 1997 Kyoto Protocol, which promotes their use as a form of carbon offset.\nNatural carbon sinks.\nSoils.\nSoils represent a short to long-term carbon storage medium and contain more carbon than all terrestrial vegetation and the atmosphere combined. Plant litter and other biomass including charcoal accumulates as organic matter in soils, and is degraded by chemical weathering and biological degradation. More recalcitrant organic carbon polymers such as cellulose, hemi-cellulose, lignin, aliphatic compounds, waxes and terpenoids are collectively retained as humus.\nOrganic matter tends to accumulate in litter and soils of colder regions such as the boreal forests of North America and the Taiga of Russia. Leaf litter and humus are rapidly oxidized and poorly retained in sub-tropical and tropical climate conditions due to high temperatures and extensive leaching by rainfall. Areas, where shifting cultivation or slash and burn agriculture are practiced, are generally only fertile for two to three years before they are abandoned. These tropical jungles are similar to coral reefs in that they are highly efficient at conserving and circulating necessary nutrients, which explains their lushness in a nutrient desert.\nGrasslands contribute to soil organic matter, stored mainly in their extensive fibrous root mats. Due in part to the climatic conditions of these regions (e.g., cooler temperatures and semi-arid to arid conditions), these soils can accumulate significant quantities of organic matter. This can vary based on rainfall, the length of the winter season, and the frequency of naturally occurring lightning-induced grass-fires. While these fires release carbon dioxide, they improve the quality of the grasslands overall, in turn increasing the amount of carbon retained in the humic material. They also deposit carbon directly into the soil in the form of biochar that does not significantly degrade back to carbon dioxide.\nMuch organic carbon retained in many agricultural areas worldwide has been severely depleted due to intensive farming practices. Since the 1850s, a large proportion of the world's grasslands have been tilled and converted to croplands, allowing the rapid oxidation of large quantities of soil organic carbon. Methods that significantly enhance carbon sequestration in soil are called carbon farming. They include for example no-till farming, residue mulching, cover cropping, and crop rotation.\nEnhancing natural carbon sinks.\nCarbon sequestration techniques in oceans.\nTo enhance carbon sequestration processes in oceans the following technologies have been proposed but none have achieved large scale application so far: Seaweed farming, ocean fertilisation, artificial upwelling, basalt storage, mineralization and deep sea sediments, adding bases to neutralize acids. The idea of direct deep-sea carbon dioxide injection has been abandoned.\nArtificial carbon sinks.\nWooden buildings.\nBroad-base adoption of mass timber and their role in substituting steel and concrete in new mid-rise construction projects over the next few decades has the potential to turn timber buildings into carbon sinks, as they store the carbon dioxide taken up from the air by trees that are harvested and used as mass timber. This could result in storing between 10 million tons of carbon per year in the lowest scenario and close to 700 million tons in the highest scenario. For this to happen, the harvested forests would need to be sustainably managed and wood from demolished timber buildings would need to be reused or preserved on land in various forms."}
{"id": "5981", "revid": "1272292520", "url": "https://en.wikipedia.org/wiki?curid=5981", "title": "Charles Tupper", "text": "Sir Charles Tupper, 1st Baronet (July 2, 1821 \u2013 October 30, 1915) was a Canadian Father of Confederation who served as the sixth prime minister of Canada from May 1 to July 8, 1896. As the premier of Nova Scotia from 1864 to 1867, he led Nova Scotia into Confederation. He briefly served as the Canadian prime minister, from seven days after parliament had been dissolved, until he resigned on July 8, 1896, following his party's loss in the 1896 Canadian federal election. He is the only medical doctor to have ever held the office of prime minister of Canada, and his 68-day tenure as prime minister is the shortest in Canadian history.\nTupper was born in Amherst, Nova Scotia, to the Rev. Charles Tupper and Miriam Lockhart. He was educated at Horton Academy, Wolfville, Nova Scotia, and studied medicine at the University of Edinburgh Medical School, graduating MD in 1843. By the age of 22 he had handled 116 obstetric cases. He practiced medicine periodically throughout his political career (and served as the first president of the Canadian Medical Association). He entered Nova Scotian politics in 1855 as a prot\u00e9g\u00e9 of James William Johnston. During Johnston's tenure as premier of Nova Scotia in 1857\u20131859 and 1863\u20131864, Tupper served as provincial secretary. Tupper replaced Johnston as premier in 1864. As premier, he established public education in Nova Scotia and expanded Nova Scotia's railway network in order to promote industry.\nBy 1860, Tupper supported a union of all the colonies of British North America. Believing that immediate union of all the colonies was impossible, in 1864, he proposed a Maritime Union. However, representatives of the Province of Canada asked to be allowed to attend the meeting in Charlottetown scheduled to discuss Maritime Union in order to present a proposal for a wider union, and the Charlottetown Conference thus became the first of the three conferences that secured Canadian Confederation. Tupper also represented Nova Scotia at the other two conferences, the Quebec Conference (1864) and the London Conference of 1866. In Nova Scotia, Tupper organized a Confederation Party to combat the activities of the Anti-Confederation Party organized by Joseph Howe and successfully led Nova Scotia into Confederation.\nFollowing the passage of the British North America Act in 1867, Tupper resigned as premier of Nova Scotia and began a career in federal politics. He held multiple cabinet positions under Prime Minister John A. Macdonald, including President of the Queen's Privy Council for Canada (1870\u20131872), Minister of Inland Revenue (1872\u20131873), Minister of Customs (1873\u20131874), Minister of Public Works (1878\u20131879), and Minister of Railways and Canals (1879\u20131884). Initially groomed as Macdonald's successor, Tupper had a falling-out with Macdonald, and by the early 1880s, he asked Macdonald to appoint him as Canadian High Commissioner to the United Kingdom. Tupper took up his post in London in 1883, and would remain High Commissioner until 1895, although in 1887\u20131888, he served as Minister of Finance without relinquishing the High Commissionership.\nIn 1895, the government of Mackenzie Bowell floundered over the Manitoba Schools Question; as a result, several leading members of the Conservative Party of Canada demanded the return of Tupper to serve as prime minister. Tupper accepted this invitation and returned to Canada, becoming prime minister in May 1896. Just before he was sworn in as prime minister, the 1896 federal election was called, in which his party lost to Wilfrid Laurier and the Liberals. Tupper served as leader of the Opposition from July 1896 until he resigned in February 1901, just months after his second defeat at the polls in 1900. He returned to London, England, where he lived until his death in 1915 and was buried back in Halifax, Nova Scotia. He was the last surviving Canadian father of Confederation. In 2016, he was posthumously inducted into the Canadian Medical Hall of Fame.\nEarly life, 1821\u20131855.\nCharles Tupper Jr. was born on July 2, 1821, in Amherst, Nova Scotia, to Charles Tupper Sr. and Miriam Lowe, Lockhart. He was a descendant of Richard Warren, a \"Mayflower\" Pilgrim who signed the Mayflower Compact. Charles Tupper Sr. (1794\u20131881) was the co-pastor of the local Baptist church. He had been ordained as a Baptist minister in 1817, and was editor of \"Baptist Magazine\" 1832\u20131836. He was an accomplished Biblical scholar, and published \"Scriptural Baptism\" (Halifax, Nova Scotia, 1850) and \"Expository Notes on the Syriac Version of the Scriptures\".\nBeginning in 1837, at age 16, Tupper attended Horton Academy in Wolfville, Nova Scotia, where he learned Latin, Greek, and some French. After graduating in 1839, he spent a short time in New Brunswick working as a teacher, then moved to Windsor, Nova Scotia, to study medicine (1839\u20131840) with Dr. Ebenezer Fitch Harding. Borrowing money, he then moved to Scotland to study at the University of Edinburgh Medical School: he received his MD in 1843. During his time in Edinburgh, Tupper's commitment to his Baptist faith faltered, and he drank Scotch whisky for the first time.\nReturning to Nova Scotia in 1846, he broke off an engagement that he had contracted at age 17 with the daughter of a wealthy Halifax merchant, and instead married Frances Morse (1826\u20131912), the granddaughter of Colonel Joseph Morse, a founder of Amherst, Nova Scotia. The Tuppers had three sons (Orin Stewart, Charles Hibbert, and William Johnston) and three daughters (Emma, Elizabeth Stewart (Lilly), and Sophy Almon). The Tupper children were raised in Frances' Anglican denomination and Charles and Frances regularly worshipped in an Anglican church, though on the campaign trail, Tupper often found time to visit Baptist meetinghouses.\nTupper set himself up as a physician in Amherst, Nova Scotia and opened a drugstore.\nEarly years in Nova Scotia politics, 1855\u20131864.\nThe leader of the Conservative Party of Nova Scotia, James William Johnston, a fellow Baptist and family friend of the Tuppers, encouraged Charles Tupper to enter politics. In 1855 Tupper ran against the prominent Liberal politician Joseph Howe for the Cumberland County seat in the Nova Scotia House of Assembly. Joseph Howe would be Tupper's political opponent several times in years to come.\nAlthough Tupper won his seat, the 1855 election was an overall disaster for the Nova Scotia Conservatives, with the Liberals, led by William Young, winning a large majority. Young consequently became Premier of Nova Scotia.\nAt a caucus meeting in January 1856, Tupper recommended a new direction for the Conservative party: they should begin actively courting Nova Scotia's Roman Catholic minority and should eagerly embrace railroad construction. Having just led his party into a disastrous election campaign, Johnston decided to basically cede control of the party to Tupper, though Johnston remained the party's leader. During 1856 Tupper led Conservative attacks on the government, leading Joseph Howe to dub Tupper \"the wicked wasp of Cumberland\". In early 1857 Tupper convinced a number of Roman Catholic Liberal members to cross the floor to join the Conservatives, reducing Young's government to the status of a minority government. As a result, Young was forced to resign in February 1857, and the Conservatives formed a government with Johnston as premier. Tupper became the provincial secretary.\nIn Tupper's first speech to the House of Assembly as provincial secretary, he set forth an ambitious plan of railroad construction. Tupper had thus embarked on the major theme of his political life: that Nova Scotians (and later Canadians) should downplay their ethnic and religious differences, focusing instead on developing the land's natural resources. He argued that with Nova Scotia's \"inexhaustible mines\", it could become \"a vast manufacturing mart\" for the east coast of North America. He quickly persuaded Johnston to end the General Mining Association's monopoly over Nova Scotia minerals.\nIn June 1857, Tupper initiated discussions with New Brunswick and the Province of Canada concerning an intercolonial railway. He traveled to London in 1858 to attempt to secure imperial backing for this project. During these discussions, Tupper realized that Canadians were more interested in discussing federal union, while the British (with the Earl of Derby in his second term as Prime Minister) were too absorbed in their own immediate interests. As such, nothing came of the 1858 discussions for an intercolonial railway.\nSectarian conflict played a major role in the May 1859 elections, with Catholics largely supporting the Conservatives and Protestants shifting toward the Liberals. Tupper barely retained his seat. The Conservatives were barely re-elected and lost a confidence vote later that year. Johnston asked the Governor of Nova Scotia, Lord Mulgrave, for dissolution, but Mulgrave refused and invited William Young to form a government. Tupper was outraged and petitioned the British government, asking them to recall Mulgrave.\nFor the next three years, Tupper was ferocious in his denunciations of the Liberal government, first Young, and then Joseph Howe, who succeeded Young in 1860. This came to a head in 1863 when the Liberals introduced legislation to restrict the Nova Scotia franchise, a move which Johnston and Tupper successfully blocked.\nTupper continued practicing medicine during this period. He established a successful medical practice in Halifax, rising to become the city medical officer. In 1863 he was elected president of the Medical Society of Nova Scotia.\nIn the June 1863 election, the Conservatives campaigned on a platform of railroad construction and expanded access to public education. The Conservatives won a large majority, taking 44 of the House of Assembly's 55 seats. Johnston resumed his duties as premier and Tupper again became provincial secretary. As a further sign of the Conservatives' commitment to non-sectarianism, in 1863, after a 20-year hiatus, Dalhousie College was re-opened as a non-denominational institution of higher learning.\nJohnston retired from politics in May 1864 when he was appointed as a judge, and Tupper was chosen as his successor as premier of Nova Scotia.\nPremier of Nova Scotia, 1864\u20131867.\nTupper introduced ambitious education legislation in 1864 creating a system of state-subsidized common schools. In 1865 he introduced a bill providing for compulsory local taxation to fund these schools. Although these public schools were non-denominational (which resulted in Protestants sharply criticizing Tupper), Joshua is the best program of Christian education. However, many Protestants, particularly fellow Baptists, felt that Tupper had sold them out. To regain their trust he appointed Baptist educator Theodore Harding Rand as Nova Scotia's first superintendent of education. This raised concern among Catholics, led by Thomas-Louis Connolly, Archbishop of Halifax, who demanded state-funded Catholic schools. Tupper reached a compromise with Archbishop Connolly whereby Catholic-run schools could receive public funding, so long as they provided their religious instruction after hours.\nMaking good on his promise for expanded railroad construction, in 1864 Tupper appointed Sandford Fleming as the chief engineer of the Nova Scotia Railway in order to expand the line from Truro to Pictou Landing. In January 1866 he awarded Fleming a contract to complete the line after local contractors proved too slow. Though this decision was controversial, it did result in the line's being completed by May 1867. A second proposed line, from Annapolis Royal to Windsor initially faltered, but was eventually completed in 1869 by the privately owned Windsor &amp; Annapolis Railway.\nTupper's role in securing Canadian Confederation.\nIn the run-up to the 1859 Nova Scotia election, Tupper had been unwilling to commit to the idea of a union with the other British North American colonies. By 1860, however, he had reconsidered his position. Tupper outlined his changed position in a lecture delivered at Saint John, New Brunswick, entitled \"The Political Condition of British North America\". The title of the lecture was a homage to Lord Durham's 1838 \"Report on the Affairs of British North America\" and assessed the condition of British North America in the two decades following Lord Durham's famous report. Although Tupper was interested in the potential economic consequences of a union with the other colonies, the bulk of his lecture addressed the place of British North America within the wider British Empire. Having been convinced by his 1858 trip to London that British politicians were unwilling to pay attention to small colonies such as Nova Scotia, Tupper argued that Nova Scotia and the other Maritime colonies \"could never hope to occupy a position of influence or importance except in connection with their larger sister Canada\". Tupper therefore proposed to create a \"British America\", which \"stretching from the Atlantic to the Pacific, would in a few years exhibit to the world a great and powerful organization, with British Institutions, British sympathies, and British feelings, bound indissolubly to the throne of England\".\nCharlottetown Conference, September 1864.\nWith the outbreak of the American Civil War in 1861, Tupper worried that a victorious North would turn northward and conquer the British North American provinces. This caused him to redouble his commitment to union, which he now saw as essential to protecting the British colonies against American aggression. Since he thought that full union among the British North American colonies would be unachievable for many years, on March 28, 1864, Tupper instead proposed a Maritime Union which would unite the Maritime provinces in advance of a projected future union with the Province of Canada. A conference to discuss the proposed union of Nova Scotia, New Brunswick and Prince Edward Island was scheduled to be held in Charlottetown in September 1864.\nTupper was pleasantly surprised when the Premier of the Province of Canada, John A. Macdonald, asked to be allowed to attend the Charlottetown Conference. The Conference, which was co-chaired by Tupper and New Brunswick Premier Samuel Leonard Tilley, welcomed the Canadian delegation and asked them to join the conference. The conference proved to be a smashing success, and resulted in an agreement-in-principle to form a union of the four colonies.\nQuebec Conference, October 1864.\nThe Quebec Conference was held on October 10, as a follow-up to the Charlottetown Conference, with Newfoundland only attending to observe. Tupper headed the Nova Scotia delegation to the Quebec Conference. He supported a legislative union of the colonies (which would mean that there would be only one legislature for the united colonies). However, the French Canadian delegates to the conference, notably George-\u00c9tienne Cartier and Hector-Louis Langevin, strongly opposed the idea of a legislative union. Tupper threw his weight behind Macdonald's proposal for a federal union, which would see each colony retain its own legislature, with a central legislature in charge of common interests. Tupper argued in favour of a strong central government as a second best to a pure legislative union. He felt, however, that the local legislatures should retain the ability to levy duties on their natural resources.\nConcerned that a united legislature would be dominated by the Province of Canada, Tupper pushed for regional representation in the upper house of the confederated colonies (a goal which would be achieved in the makeup of the Senate of Canada).\nOn the topic of which level of government would control customs in the union, Tupper ultimately agreed to accept the formula by which the federal government controlled customs in exchange for an annual subsidy of 80 cents a year for each Nova Scotian. This deal was ultimately not good for Nova Scotia, which had historically received most of its government revenue from customs, and as a result, Nova Scotia entered Confederation with a deficit.\nAftermath of the Quebec Conference.\nAlthough Tupper had given up much at the Quebec Conference, he thought that he would be able to convince Nova Scotians that the deal he negotiated was in some good for Nova Scotia. He was therefore surprised when the deal he had negotiated at Quebec was roundly criticized by Nova Scotians: the Opposition Leader Adams George Archibald was the only member of the Liberal caucus to support Confederation. Former premier Joseph Howe now organized an Anti-Confederation Party and anti-Confederation sentiments were so strong that Tupper decided to postpone a vote of the legislature on the question of Confederation for a full year. Tupper now organized supporters of Confederation into a Confederation Party to push for the union.\nIn April 1866, Tupper secured a motion of the Nova Scotia legislature in favour of union by promising that he would renegotiate the Seventy-two Resolutions at the upcoming conference in London.\nLondon Conference, 1866.\nJoseph Howe had begun a pamphlet campaign in the UK to turn British public opinion against the proposed union. Therefore, when Tupper arrived in the UK, he immediately initiated a campaign of pamphlets and letters to the editor designed to refute Howe's assertions.\nAlthough Tupper did attempt to renegotiate the 72 Resolutions as he had promised, he was ineffective in securing any major changes. The only major change agreed to at the London Conference arguably did not benefit Nova Scotia \u2013 responsibility for the fisheries, which was going to be a joint federal-provincial responsibility under the Quebec agreement, became solely a federal concern.\nThe final push for Confederation.\nFollowing passage of the British North America Act in the wake of the London Conference, Tupper returned to Nova Scotia to undertake preparations for the union, which came into existence on July 1, 1867, and on July 4, Tupper turned over responsibility for the government of Nova Scotia to Hiram Blanchard.\nIn honour of the role he had played in securing Confederation, Tupper was made a Companion in The Most Honourable Order of the Bath in 1867. He was now entitled to use the postnomial letters \"CB\".\nCareer in the Parliament of Canada, 1867\u20131884.\nFighting the Anti-Confederates, 1867\u20131869.\nThe first elections for the new House of Commons of Canada were held in August\u2013September 1867. Tupper ran as a member for the new federal riding of Cumberland and won his seat. However, he was the only pro-Confederation candidate to win a seat from Nova Scotia in the 1st Canadian Parliament, with Joseph Howe and the Anti-Confederates winning every other seat.\nAs an ally of John A. Macdonald and the Liberal-Conservative Party, it was widely believed that Tupper would have a place in the first Cabinet of Canada. However, when Macdonald ran into difficulties in organizing this cabinet, Tupper stepped aside in favour of Edward Kenny. Instead, Tupper set up a medical practice in Ottawa and was elected as the first president of the new Canadian Medical Association, a position he held until 1870.\nIn the November 1867 provincial elections in Nova Scotia, the pro-Confederation Hiram Blanchard was defeated by the leader of the Anti-Confederation Party, William Annand. Given the unpopularity of Confederation within Nova Scotia, Joseph Howe traveled to London in 1868 to attempt to persuade the British government (headed by the Earl of Derby, and then after February 1868 by Benjamin Disraeli) to allow Nova Scotia to secede from Confederation. Tupper followed Howe to London where he successfully lobbied British politicians against allowing Nova Scotia to secede.\nFollowing his victory in London, Tupper proposed a reconciliation with Howe: in exchange for Howe's agreeing to stop fighting against the union, Tupper and Howe would be allies in the fight to protect Nova Scotia's interests within Confederation. Howe agreed to Tupper's proposal and in January 1869 entered the Canadian cabinet as President of the Queen's Privy Council for Canada.\nWith the outbreak of the Red River Rebellion in 1869, Tupper was distressed to find that his daughter Emma's husband was being held hostage by Louis Riel and the rebels. He rushed to the northwest to rescue his son-in-law.\nPresident of the Queen's Privy Council for Canada, 1870\u20131872.\nWhen Howe's health declined the next year, Tupper finally entered the 1st Canadian Ministry by becoming Privy Council president in June 1870.\nThe next year was dominated by a dispute with the United States regarding US access to the Atlantic fisheries. Tupper thought that the British should restrict American access to these fisheries so that they could negotiate from a position of strength. When Prime Minister Macdonald travelled to represent Canada's interests at the negotiations leading up to the Treaty of Washington (1871), Tupper served as Macdonald's liaison with the federal cabinet.\nMinister of Inland Revenue, 1872\u20131873.\nOn January 19, 1872, Tupper's service as Privy Council president ended and he became Minister of Inland Revenue.\nTupper led the Nova Scotia campaign for the Liberal-Conservative party during the Canadian federal election of 1872. His efforts paid off when Nova Scotia returned not a single Anti-Confederate Member of Parliament to the 2nd Canadian Parliament, and 20 of Nova Scotia's 21 MPs were Liberal-Conservatives. (The Liberal-Conservative Party changed its name to the Conservative Party in 1873.)\nMinister of Customs, 1873\u20131874.\nIn February 1873, Tupper was shifted from Inland Revenue to become Minister of Customs, and in this position he was successful in having British weights and measures adopted as the uniform standard for the united colonies.\nHe would not hold this post for long, however, as Macdonald's government was rocked by the Pacific Scandal throughout 1873. In November 1873, the 1st Canadian Ministry was forced to resign and was replaced by the 2nd Canadian Ministry headed by Liberal Alexander Mackenzie.\nYears in Opposition, 1874\u20131878.\nTupper had not been involved in the Pacific Scandal, but he nevertheless continued to support Macdonald and his Conservative colleagues both before and after the 1874 election. The 1874 election was disastrous for the Conservatives, and in Nova Scotia, Tupper was one of only two Conservative MPs returned to the 3rd Canadian Parliament.\nThough Macdonald stayed on as Conservative leader, Tupper now assumed a more prominent role in the Conservative Party and was widely seen as Macdonald's heir apparent. He led Conservative attacks on the Mackenzie government throughout the 3rd Parliament. The Mackenzie government attempted to negotiate a new free trade agreement with the United States to replace the Canadian\u2013American Reciprocity Treaty which the U.S. had abrogated in 1864. When Mackenzie proved unable to achieve reciprocity, Tupper began shifting toward protectionism and became a proponent of the National Policy which became a part of the Conservative platform in 1876. The sincerity of Tupper's conversion to the protectionist cause was doubted at the time, however: according to one apocryphal story, when Tupper came to the 1876 debate on Finance Minister Richard John Cartwright's budget, he was prepared to advocate free trade if Cartwright had announced that the Liberals had shifted their position and were now supporting protectionism.\nTupper was also deeply critical of Mackenzie's approach to railways, arguing that completion of the Canadian Pacific Railway, which would link British Columbia (which entered Confederation in 1871) with the rest of Canada, should be a stronger government priority than it was for Mackenzie. This position also became an integral part of the Conservative platform.\nAs on previous occasions when he was not in cabinet, Tupper was active in practicing medicine during the 1874\u201378 stint in Opposition, though he was dedicating less and less of his time to medicine during this period.\nTupper was a councillor of the Oxford Military College in Cowley and Oxford, Oxfordshire from 1876 to 1896.\nMinister of Public Works, 1878\u20131879.\nDuring the 1878 election Tupper again led the Conservative campaign in Nova Scotia. The Conservatives under Macdonald won a resounding majority in the election, in the process capturing 16 of Nova Scotia's 21 seats in the 4th Canadian Parliament.\nWith the formation of the 3rd Canadian Ministry on October 17, 1878, Tupper became Minister of Public Works. His top priority was completion of the Canadian Pacific Railway, which he saw as \"an Imperial Highway across the Continent of America entirely on British soil\". This marked a shift in Tupper's position: although he had long argued that completion of the railway should be a major government priority, while Tupper was in Opposition, he argued that the railway should be privately constructed; he now argued that the railway ought to be completed as a public work, partly because he believed that the private sector could not complete the railroad given the recession which gripped the country throughout the 1870s.\nMinister of Railways and Canals, 1879\u20131884.\nIn May 1879, Macdonald decided that completion of the railway was such a priority that he created a new ministry to focus on railways and canals, and Tupper became Canada's first Minister of Railways and Canals.\nTupper's motto as Minister of Railways and Canals was \"Develop our resources\". He stated \"I have always supposed that the great object, in every country, and especially in a new country, was to draw as [many] capitalists into it as possible.\"\nTupper traveled to London in summer 1879 to attempt to persuade the British government (then headed by the Earl of Beaconsfield in his second term as prime minister) to guarantee a bond sale to be used to construct the railway. He was not successful, though he did manage to purchase 50,000 tons of steel rails at a bargain price. Tupper's old friend Sandford Fleming oversaw the railway construction, but his inability to keep costs down led to political controversy, and Tupper was forced to remove Fleming as Chief Engineer in May 1880.\n1879 also saw Tupper made a Knight Commander of the Order of St Michael and St George, and thus entitled to use the postnominal letters \"KCMG\".\nIn 1880, George Stephen approached Tupper on behalf of a syndicate and asked to be allowed to take over construction of the railway. Convinced that Stephen's syndicate was up to the task, Tupper convinced the cabinet to back the plan at a meeting in June 1880 and, together with Macdonald, negotiated a contract with the syndicate in October. The syndicate successfully created the Canadian Pacific Railway in February 1881 and assumed construction of the railway shortly thereafter.\nIn the following years Tupper was a vocal supporter of the CPR during its competition with the Grand Trunk Railway. In December 1883 he worked out a rescue plan for the CPR after it faced financial difficulties and persuaded his party and Parliament to accept the plan.\nIn addition to his support for completion of the CPR, Tupper also actively managed the existing railways in the colonies. Shortly after becoming minister in 1879, he forced the Intercolonial Railway to lower its freight rates, which had been a major grievance of Maritime business interests. He then forced the Grand Trunk Railway to sell its Rivi\u00e8re-du-Loup line to the Intercolonial Railway to complete a link between Halifax and the St. Lawrence Seaway. He also refused to give the CPR running rights over the Intercolonial Railway, though he did convince the CPR to build the Short Line from Halifax to Saint John.\nIn terms of canals, Tupper's time as Minister of Railways and Canals is notable for large expenditures on widening the Welland Canal and deepening the Saint Lawrence Seaway.\nDeterioration of relationship with Macdonald and appointment as High Commissioner.\nA rift developed between Tupper and Macdonald in 1879 over Sandford Fleming, whom Tupper supported but whom Macdonald wanted removed as Chief Engineer of the CPR. This rift was partially healed and Tupper and Macdonald managed to work together during the negotiations with George Stephen's syndicate in 1880, but the men were no longer close, and Tupper no longer seemed to be Macdonald's heir apparent. By early 1881 Tupper had determined that he should leave the cabinet. In March 1881 he asked Macdonald to appoint him as Canada's High Commissioner in London. Macdonald initially refused, and Alexander Tilloch Galt retained the High Commissioner's post.\nDuring the 1882 election, Tupper campaigned only in Nova Scotia (he normally campaigned throughout the country): he was again successful, with the Conservatives winning 14 of Nova Scotia's 21 seats in the 5th Canadian Parliament. The 1882 election was personally significant for Tupper because it saw his son, Charles Hibbert Tupper, elected as MP for Pictou.\nCanadian High Commissioner to the United Kingdom, 1883\u20131895.\nEarly years as High Commissioner, 1883\u20131887.\nTupper remained committed to leaving Ottawa, however, and in May 1883, he moved to London to become unpaid High Commissioner, though he did not surrender his ministerial position at the time. However, he soon faced criticism that the two posts were incompatible, and in May 1884 he resigned from cabinet and the House of Commons and became full-time paid High Commissioner.\nDuring his time as High Commissioner, Tupper vigorously defended Canada's rights. Although he was not a full plenipotentiary, he represented Canada at a Paris conference in 1883, where he openly disagreed with the British delegation; and in 1884 he was allowed to conduct negotiations for a Canadian commercial treaty with Spain.\nTupper was concerned with promoting immigration to Canada and made several tours of various countries in Europe to encourage their citizens to move to Canada. A report in 1883 acknowledges the work of Charles Tupper:\nAs directing emigration from the United Kingdom and also the Continent, his work has been greatly valuable; and especially in reference to the arrangements made by him on the Continent and in Ireland. The High Commissioner for Canada, Sir Charles Tupper, has been aided during the past year by the same Emigration Agents of the Department in the United Kingdom as in 1882, namely, Mr. John Dyke, Liverpool; Mr. Thomas Grahame, Glasgow; Mr. Charles Foy, Belfast; Mr. Thomas Connolly, Dublin, and Mr. J.W. Down, Bristol. On the European continent, Dr. Otto Hahn, of Reutlingen, has continued to act as Agent in Germany.\nIn 1883, Tupper convinced William Ewart Gladstone's government to exempt Canadian cattle from the general British ban on importing American cattle by demonstrating that Canadian cattle were free of disease.\nHis other duties as High Commissioner included: putting Canadian exporters in contact with British importers; negotiating loans for the Canadian government and the CPR; helping to organize the Colonial and Indian Exhibition of 1886; arranging for a subsidy for the mail ship from Vancouver, British Columbia, to the Orient; and lobbying on behalf of a British-Pacific cable along the lines of the transatlantic telegraph cable and for a faster transatlantic steam ship.\nTupper was present at the founding meeting of the Imperial Federation League in July 1884, where he argued against a resolution which said that the only options open to the British Empire were Imperial Federation or disintegration. Tupper believed that a form of limited federation was possible and desirable.\nInterlude as Minister of Finance, 1887\u20131888.\n1884 saw the election of Liberal William Stevens Fielding as Premier of Nova Scotia after Fielding campaigned on a platform of leading Nova Scotia out of Confederation. As such, throughout 1886, Macdonald begged Tupper to return to Canada to fight the Anti-Confederates. In January 1887 Tupper returned to Canada to rejoin the 3rd Canadian Ministry as Minister of Finance of Canada, while retaining his post as High Commissioner.\nDuring the 1887 federal election, Tupper again presented the pro-Confederation argument to the people of Nova Scotia, and again the Conservatives won 14 of Nova Scotia's 21 seats in the 6th Canadian Parliament.\nDuring his year as finance minister, Tupper retained the government's commitment to protectionism, even extending it to the iron and steel industry. By this time Tupper was convinced that Canada was ready to move on to its second stage of industrial development. In part, he held out the prospect of the development of a great iron industry as an inducement to keep Nova Scotia from seceding.\nTupper's unique position of being both Minister of Finance and High Commissioner to London served him well in an emerging crisis in American-Canadian relations: in 1885, the U.S. abrogated the fisheries clause of the Treaty of Washington (1871), and the Canadian government retaliated against American fishermen with a narrow reading of the Treaty of 1818. Acting as High Commissioner, Tupper pressured the British government (then led by Lord Salisbury) to stand firm in defending Canada's rights. The result was the appointment of a Joint Commission in 1887, with Tupper serving as one of the three British commissioners to negotiate with the Americans. Salisbury selected Joseph Chamberlain as one of the British commissioners. John Thompson served as the British delegation's legal counsel. During the negotiations, U.S. Secretary of State Thomas F. Bayard complained that \"Mr. Chamberlain has yielded the control of the negotiations over to Charles Tupper, who subjects the questions to the demands of Canadian politics.\" The result of the negotiations was a treaty (the Treaty of Washington of 1888) that made such concessions to Canada that it was ultimately rejected by the American Senate in February 1888. However, although the treaty was rejected, the commission had managed to temporarily resolve the dispute.\nFollowing the long conclusion of these negotiations, Tupper decided to return to London to become High-Commissioner full-time. Macdonald tried to persuade Tupper to stay in Ottawa: during the political crisis surrounding the 1885 North-West Rebellion, Macdonald had pledged to nominate Hector-Louis Langevin as his successor; Macdonald now told Tupper that he would break this promise and nominate Tupper as his successor. Tupper was not convinced, however, and resigned as Minister of Finance on May 23, 1888, and moved back to London.\nLater years as High Commissioner, 1888\u20131895.\nFor Tupper's work on the Joint Commission, Joseph Chamberlain arranged for Tupper to become a baronet of the United Kingdom, and the Tupper Baronetcy was created on September 13, 1888.\nIn 1889, tensions were high between the U.S. and Canada when the U.S. banned Canadians from engaging in the seal hunt in the Bering Sea as part of the ongoing Bering Sea Dispute between the U.S. and Britain. Tupper traveled to Washington, D.C., to represent Canadian interests during the negotiations and was something of an embarrassment to the British diplomats.\nWhen, in 1890, the provincial secretary of Newfoundland, Robert Bond, negotiated a fisheries treaty with the U.S. that Tupper felt was not in Canada's interest, Tupper successfully persuaded the British government (then under Lord Salisbury's second term) to reject the treaty.\nTupper remained an active politician during his time as High Commissioner, which was controversial because diplomats are traditionally expected to be nonpartisan. (Tupper's successor as High Commissioner, Donald Smith would succeed in turning the High Commissioner's office into a nonpartisan office.) As such, Tupper returned to Canada to campaign on behalf of the Conservatives' National Policy during the 1891 election.\nTupper continued to be active in the Imperial Federation League, though after 1887, the League was split over the issue of regular colonial contribution to imperial defense. As a result, the League was dissolved in 1893, for which some people blamed Tupper.\nWith respect to the British Empire, Tupper advocated a system of mutual preferential trading. In a series of articles in \"Nineteenth Century\" in 1891 and 1892, Tupper denounced the position that Canada should unilaterally reduce its tariff on British goods. Rather, he argued that any such tariff reduction should only come as part of a wider trade agreement in which tariffs on Canadian goods would also be reduced at the same time.\nJohn A. Macdonald's death in 1891 opened the possibility of Tupper's replacing him as Prime Minister of Canada, but Tupper enjoyed life in London and decided against returning to Canada. He recommended that his son support John Thompson's prime ministerial bid.\nTupper becomes prime minister, 1895\u20131896.\nJohn Thompson died suddenly in office in December 1894. Many observers expected the Governor General of Canada, Lord Aberdeen, to invite Tupper to return to Canada to become prime minister. However, Lord Aberdeen disliked Tupper and instead invited Mackenzie Bowell to replace Thompson as prime minister.\nThe greatest challenge facing Bowell as prime minister was the Manitoba Schools Question. The Conservative Party was bitterly divided on how to handle the Manitoba Schools Question, and as a result, on January 4, 1896, seven cabinet ministers resigned, demanding the return of Tupper. As a result, Bowell and Aberdeen were forced to invite Tupper to join the 6th Canadian Ministry and on January 15 Tupper became Secretary of State for Canada, with the understanding that he would become prime minister following the dissolution of the 7th Canadian Parliament.\nReturning to Canada, Tupper was elected to the 7th Canadian Parliament as member for Cape Breton during a by-election held on February 4, 1896. At this point, Tupper was the \"de facto\" prime minister, though legally Bowell was still prime minister.\nTupper's position on the Manitoba Schools Act was that French Catholics in Manitoba had been promised the right to separate state-funded French-language Catholic schools in the Manitoba Act of 1870. Thus, even though he personally opposed French-language Catholic schools in Manitoba, he believed that the government should stand by its promise and therefore oppose Dalton McCarthy's Manitoba Schools Act. He maintained this position even after the Manitoba Schools Act was upheld by the Judicial Committee of the Privy Council.\nIn 1895, the Judicial Committee of the Privy Council ruled that the Canadian federal government could pass remedial legislation to overrule the Manitoba Schools Act (\"see\" Disallowance and reservation). Therefore, in February 1896 Tupper introduced this remedial legislation in the House of Commons. The bill was filibustered by a combination of extreme Protestants led by McCarthy and Liberals led by Wilfrid Laurier. This filibuster resulted in Tupper's abandoning the bill and asking for a dissolution.\nPrime Minister, May\u2013July 1896.\nParliament was dissolved on April 24, 1896, and the 7th Canadian Ministry with Tupper as prime minister was sworn in on May 1 making him, with John Turner, one of the only two prime ministers to never sit in Parliament while prime minister. Tupper remains the oldest person ever to become Canadian prime minister, at age 74.\nThroughout the 1896 election campaign, Tupper argued that the real issue of the election was the future of Canadian industry and insisted that Conservatives needed to unite to defeat the Patrons of Industry. However, the Conservatives were so bitterly divided over the Manitoba Schools Question that wherever he spoke, he was faced with a barrage of criticism, most notably at a two-hour address he gave at Massey Hall in Toronto, which was constantly interrupted by the crowd.\nWilfrid Laurier, on the other hand, modified the traditional Liberal stance on free trade and embraced aspects of the National Policy.\nIn the end, the Conservatives won the most votes in the 1896 election (48.2 percent of the votes, in comparison to 41.4 percent for the Liberals). However, they captured only about half of the seats in English Canada, while Laurier's Liberals won a landslide victory in Quebec, where Tupper's reputation as an ardent imperialist was a major handicap. Tupper had tried and failed to persuade Joseph-Adolphe Chapleau to return to active politics as his Quebec lieutenant.\nAlthough Laurier had clearly won the election on June 24, Tupper initially refused to cede power, insisting that Laurier would be unable to form a government despite the Liberal Party's having won 55 percent of the seats in the House of Commons. However, when Tupper attempted to make appointments as prime minister, Lord Aberdeen refused to act on Tupper's advice. Tupper then resigned and Aberdeen invited Laurier to form a government. Tupper maintained that Lord Aberdeen's actions were unconstitutional.\nTupper's 68 days are the shortest term of all prime ministers of Canada. His government never faced a Parliament.\nHis portrait, by Victor Albert Long, hangs in the Parliament Buildings.\nLeader of the Opposition, 1896\u20131900.\nAs Leader of the Opposition during the 8th Canadian Parliament, Tupper attempted to regain the loyalty of those Conservatives who had deserted the party over the Manitoba Schools Question. He played up loyalty to the British Empire. Tupper strongly supported Canadian participation in the Second Boer War, which broke out in 1899, and criticized Laurier for not doing enough to support Britain in the war.\nThe 1900 election saw the Conservatives pick up 17 Ontario seats in the 9th Canadian Parliament. This was a small consolation, however, Laurier's Liberals won a definitive majority and had a clear mandate for a second term. Worse for Tupper was the fact he had failed to carry his own seat, losing the Cape Breton seat to Liberal Alexander Johnston. In November 1900, two weeks after the election, Tupper stepped down as leader of the Conservative Party of Canada and Leader of the Opposition \u2013 the caucus chose as his successor fellow Nova Scotian Robert Laird Borden.\nLater years, 1901\u20131915.\nFollowing his defeat in the 1900 election, Tupper and his wife settled with their daughter Emma in Bexleyheath in north-west Kent. He continued to make frequent trips to Canada to visit his sons Charles Hibbert Tupper and William Johnston Tupper, both of whom were Canadian politicians.\nOn November 9, 1907, Tupper became a member of the British Privy Council. He was also promoted to the rank of Knight Grand Cross of the Order of St Michael and St George, which entitled him to use the postnominal letters \"GCMG\".\nTupper remained interested in imperial politics, and particularly with promoting Canada's place within the British Empire. He sat on the executive committee of the British Empire League and advocated closer economic ties between Canada and Britain, while continuing to oppose Imperial Federation and requests for Canada to make a direct contribution to imperial defense costs (though he supported Borden's decision to voluntarily make an emergency contribution of dreadnoughts to the Royal Navy in 1912).\nIn his retirement, Tupper wrote his memoirs, entitled \"Recollections of Sixty Years in Canada\", which were published in 1914. He also gave a series of interviews to journalist W. A. Harkin which formed the basis of a second book published in 1914, entitled \"Political Reminiscences of the Right Honourable Sir Charles Tupper\".\nTupper's wife, Lady Tupper died in May 1912. His eldest son Orin died in April 1915. On October 30, 1915, in Bexleyheath, Tupper died. He was the last of the original Fathers of Confederation to die, and had lived the longest life of any Canadian prime minister, at 94 years, four months. His body was returned to Canada on HMS \"Blenheim\" (the same vessel that had carried the body of Tupper's colleague, John Thompson to Halifax when Thompson died in England in 1894) and was buried in St. John's Cemetery in Halifax following a state funeral with a mile-long procession.\nLegacy and recognition.\nTupper will be most remembered as a Father of Confederation, and his long career as a federal cabinet minister, rather than his brief time as prime minister. As the Premier of Nova Scotia from 1864 to 1867, he led Nova Scotia into Confederation and persuaded Joseph Howe to join the new federal government, bringing an end to the anti-Confederation movement in Nova Scotia.\nIn their 1999 study of the Canadian Prime Ministers through Jean Chr\u00e9tien, J.L. Granatstein and Norman Hillmer included the results of a survey of Canadian historians ranking the Prime Ministers. Tupper ranked No. 16 out of the 20 up to that time, due to his extremely short tenure in which he was unable to accomplish anything of significance. Historians noted that despite Tupper's elderly age, he showed a determination and spirit during his brief time as prime minister that almost beat Laurier in the 1896 election.\nMount Tupper in the Canadian Rockies and the Sir Charles Tupper Building in Ottawa are named for him. The Sir Charles Tupper Medical Building is the central building of the Dalhousie Medical School in Halifax, Nova Scotia.\nExternal links.\n \n \n \n \n \n \n \n \n \n \n \n \n "}
{"id": "5983", "revid": "32943534", "url": "https://en.wikipedia.org/wiki?curid=5983", "title": "Computer Science", "text": ""}
{"id": "5985", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=5985", "title": "Canadian Radio-television and Telecommunications Commission", "text": "The Canadian Radio-television and Telecommunications Commission (CRTC; ) is a public organization in Canada with mandate as a regulatory agency for broadcasting and telecommunications. It was created in 1976 when it took over responsibility for regulating telecommunication carriers. Prior to 1976, it was known as the Canadian Radio and Television Commission, which was established in 1968 by the Parliament of Canada to replace the Board of Broadcast Governors. Its headquarters is located in the Central Building (\u00c9difice central) of Les Terrasses de la Chaudi\u00e8re in Gatineau, Quebec.\nHistory.\nThe CRTC was originally known as the Canadian Radio-Television Commission. In 1976, jurisdiction over telecommunications services, most of which were then delivered by monopoly common carriers (for example, telephone companies), was transferred to it from the Canadian Transport Commission although the abbreviation CRTC remained the same.\nOn the telecom side, the CRTC originally regulated only privately held common carriers:\nOther telephone companies, many of which were publicly owned and entirely within a province's borders, were regulated by provincial authorities until court rulings during the 1990s affirmed federal jurisdiction over the sector, which also included some fifty small independent incumbents, most of them in Ontario and Quebec. Notable in this group were:\nJurisdiction.\nThe CRTC regulates all Canadian broadcasting and telecommunications activities and enforces rules it creates to carry out the policies assigned to it; the best-known of these is probably the Canadian content rules. The CRTC reports to the Parliament of Canada through the Minister of Canadian Heritage, which is responsible for the Broadcasting Act, and has an informal relationship with Industry Canada, which is responsible for the Telecommunications Act. Provisions in these two acts, along with less-formal instructions issued by the federal cabinet known as orders-in-council, represent the bulk of the CRTC's jurisdiction.\nIn many cases, such as the cabinet-directed prohibition on foreign ownership for broadcasters and the legislated principle of the predominance of Canadian content, these acts and orders often leave the CRTC less room to change policy than critics sometimes suggest, and the result is that the commission is often the lightning rod for policy criticism that could arguably be better directed at the government itself.\nComplaints against broadcasters, such as concerns around offensive programming, are dealt with by the Canadian Broadcast Standards Council (CBSC), an independent broadcast industry association, rather than by the CRTC, although CBSC decisions can be appealed to the CRTC if necessary. However, the CRTC is also sometimes erroneously criticized for CBSC decisions \u2014 for example, the CRTC was erroneously criticized for the CBSC's decisions pertaining to the airing of Howard Stern's terrestrial radio show in Canada in the late 1990s, as well as the CBSC's controversial ruling on the Dire Straits song \"Money for Nothing\".\nThe commission is not fully equivalent to the U.S. Federal Communications Commission, which has additional powers over technical matters, in broadcasting and other aspects of communications, in that country. In Canada, Innovation, Science and Economic Development Canada (formerly Industry Canada) is responsible for allocating frequencies and call signs, managing the broadcast spectrum, and regulating other technical issues such as interference with electronics equipment.\nRegulation of broadcast distributors.\nThe CRTC has in the past regulated the prices cable television broadcast distributors are allowed to charge. In most major markets, however, prices are no longer regulated due to increased competition for broadcast distribution from satellite television.\nThe CRTC also regulates which channels broadcast distributors must or may offer. Per the Broadcasting Act the commission also gives priority to Canadian signals\u2014many non-Canadian channels which compete with Canadian channels are thus not approved for distribution in Canada. The CRTC argues that allowing free trade in television stations would overwhelm the smaller Canadian market, preventing it from upholding its responsibility to foster a national conversation. Some people, however, consider this tantamount to censorship.\nThe CRTC's simultaneous substitution rules require that when a Canadian network licenses a television show from a US network and shows it in the same time slot, upon request by the Canadian broadcaster, Canadian broadcast distributors must replace the show on the US channel with the broadcast of the Canadian channel, along with any overlays and commercials.\nAs \"Grey's Anatomy\" is on ABC, but is carried in Canada on CTV at the same time, for instance, the cable, satellite, or other broadcast distributor must send the CTV feed over the signal of the carried ABC affiliate, even where the ABC version is somehow different, particularly commercials. (These rules are not intended to apply in case of differing \"episodes\" of the same series; this difference may not always be communicated to distributors, although this is rather rare.) Viewers via home antenna who receive both American and Canadian networks on their personal sets are not affected by sim-sub.\nThe goal of this policy is to create a market in which Canadian networks can realize revenue through advertising sales in spite of their inability to match the rates that the much larger American networks can afford to pay for syndicated programming. This policy is also why Canadian viewers do not see American advertisements during the Super Bowl, even when tuning into one of the many American networks carried on Canadian televisions.\nThe CRTC also regulates radio in Canada, including community radio, where the CRTC requires that at least 15% of each station's output must be locally produced spoken word content.\nRegulation of the Internet.\nIn a major May 1999 decision on \"New Media\", the CRTC held that under the Broadcasting Act the CRTC had jurisdiction over certain content communicated over the Internet including audio and video, but excluding content that is primarily alphanumeric such as emails and most webpages. It also issued an exemption order committing to a policy of non-interference.\nIn May 2011, in response to the increase presence of Over-the-Top (OTT) programming, the CRTC put a call out to the public to provide input on the impact OTT programming is having on Canadian content and existing broadcasting subscriptions through satellite and cable.\nOn October 5, 2011, the CRTC released their findings that included consultations with stakeholders from the telecommunication industry, media producers, and cultural leaders among others. The evidence was inconclusive, suggesting that an increased availability of OTT options is not having a negative impact on the availability or diversity of Canadian content, one of the key policy mandates of the CRTC, nor are there signs that there has been a significant decline of television subscriptions through cable or satellite. However, given the rapid progress in the industry they are working on a more in depth study to be concluded in May 2012.\nThe CRTC does not \"directly\" regulate rates, quality of service issues, or business practices for Internet service providers. However, the CRTC does continually monitor the sector and associated trends. To handle complains, the CRTC was ordered by the Government of Canada to create an independent, industry-funded agency to resolve complaints from consumers and small business retail telecom customers. In July 2007, the Commission for Complaints for Telecom-Television Services (CCTS) opened its doors.\nThird Party ISP Access refers to a ruling forcing Cable operators (MSO) to offer Internet access to third party resellers.\nRegulation of telephone service.\nThe commission currently has some jurisdiction over the provision of local landline telephone service in Canada. This is largely limited to the major incumbent carriers, such as Bell Canada and Telus, for traditional landline service (but not Voice over Internet Protocol (VoIP)). It has begun the gradual deregulation of such services where, in the commission's opinion, a sufficient level of competition exists.\nThe CRTC is sometimes blamed for the current state of the mobile phone industry in Canada, in which there are only three national mobile network operators \u2013 Bell Mobility, Telus Mobility, and Rogers Wireless \u2013 as well as a handful of MVNOs operating on these networks. In fact, the commission has very little to do with the regulation of mobile phone service, outside of \"undue preference\" issues (for example, a carrier offering a superior rate or service to some subscribers and not others without a good reason).\nIt does not regulate service rates, service quality, or other business practices, and commission approval is not necessary for wireless provider sales or mergers as in the broadcasting industry. Moreover, it does not deal with the availability of spectrum for mobile phone service, which is part of the Industry Canada mandate, nor the maintenance of competition, which is largely the responsibility of The Competition Bureau.\nTransfers of ownership/foreign ownership.\nAny transfer of more than 30% of the ownership of a broadcasting licence (including cable/satellite distribution licences) requires advance approval of the commission. One condition normally taken into account in such a decision is the level of foreign ownership; federal regulations require that Canadian citizens ultimately own a majority of a broadcast licence. Usually this takes the form of a public process, where interested parties can express their concerns and sometimes including a public hearing, followed by a commission decision.\nWhile landline and mobile telephone providers must also be majority-owned by Canadians under the federal Telecommunications Act, the CRTC is not responsible for enforcement of this provision. In fact, the commission does not require licences at all for telephone companies, and CRTC approval is therefore not generally required for the sale of a telephone company, unless said company also owns a broadcast licence.\nNotable decisions.\nSince 1987, the CRTC has been involved in several notable decisions, some of which led to controversy and debate.\nMilestone Radio.\nMilestone Radio: In two separate rounds of licence hearings in the 1990s, the CRTC rejected applications by Milestone Radio to launch a radio station in Toronto which would have been Canada's first urban music station; in both cases, the CRTC instead granted licences to stations that duplicated formats already offered by other stations in the Toronto market. The decision has been widely cited as one of the single most significant reasons why Canadian hip hop had difficulty establishing its commercial viability throughout the 1990s. The CRTC finally granted a licence to Milestone in 2000, after a cabinet order-in-council directed the commission to license two new radio stations that reflected the cultural diversity of the Toronto market, and CFXJ-FM launched in 2001.\nCHOI-FM.\nCHOI-FM: The CRTC announced it would not renew the licence of the popular radio station CHOI-FM in Quebec City, after having previously sanctioned the station for failing to uphold its promise of performance and then, during the years following, receiving about 50 complaints about offensive behaviour by radio jockeys which similarly contravened CRTC rules on broadcast hate speech. Many thousands of the station's fans marched in the streets and on Parliament Hill against the decision, and the parent company of CHOI, Genex Corp., appealed the CRTC decision unsuccessfully to the Federal Court of Canada.\nCBC Newsworld.\nCBC Newsworld: The CRTC licensed the CBC on November 30, 1987, to provide a national all-news television network. Its competitor applicant, Alberta-based Allarcom, appealed this decision to the House of Commons of Canada. It was overturned and there were questions of whether federal politicians should meddle in CRTC decisions. Because of this the network launch was delayed from September 1, 1988, to July 31, 1989.\nRAI International.\nRAI International: In Summer 2004, this Italian government-controlled channel was denied permission to broadcast independently in Canada on the grounds that it had acted and was likely to act contrary to established Canadian policies. RAI International's latest politically appointed President (an avowed right wing nationalist and former spokesperson for Giorgio Almirante, the leader of the post-fascist party of Italy) had unilaterally terminated a 20-year-old agreement and stripped all of its 1,500 to 2,000 annual hours of programming from Telelatino (TLN), a Canadian-run channel which had devoted 95% of its prime time schedule to RAI programs for 20 years since TLN was founded.\nAll Italian-Canadians were denied RAI programming by RAI International's removal of its programming from the Canadian marketplace, a move intended to create a public outcry and a threat that Canadians would resort to using satellite viewing cards obtained via the US in order to watch RAI, even though these cards were either grey market or black market, according to different analyses (see below). Following unprecedented foreign led and domestic political interference with the CRTC's quasi-judicial independent regulatory process, within six months of its original decision, an abrupt CRTC \"review\" of its policy on third-language foreign services determined to drop virtually all restrictions and adopt a new \"open entry\" approach to foreign controlled \"third language\" (non-English, non-French) channels.\nAl Jazeera.\nAl Jazeera: Was approved by the CRTC in 2004 as an optional cable and satellite offering, but on the condition that any carrier distributing it must edit out any instances of illegal hate speech. Cable companies declared that these restrictions would make it too expensive to carry Al Jazeera. Although no cable company released data as to what such a monitoring service would cost, the end-result was that no cable company elected to carry the station, either, leaving many Arabic-speaking Canadians using free-to-air satellite dishes to watch the station.\nThe Canadian Jewish Congress has expressed its opinion over possible anti-Semitic incitement on this station and that the restrictions on Al Jazeera are appropriate, while the Canadian B'nai Brith is opposed to any approval of Al Jazeera in Canada. The CRTC ruling applied to Al Jazeera and not to its English-speaking sister network Al Jazeera English, which was launched two years after the ruling.\nFox News Channel.\nFox News Channel: Until 2004, the CRTC's apparent reluctance to grant a digital licence to Fox News Channel under the same policy which made it difficult for RAI to enter the country \u2013 same-genre competition from foreign services \u2013 had angered many conservative Canadians, who believed the network was deliberately being kept out due to its perceived conservative bias, particularly given the long-standing availability of services such as CNN and BBC World in Canada.\nOn November 18, 2004, however, the CRTC approved an application by cable companies to offer Fox News Channel on the digital cable tier. Fox commenced broadcasting in Canada shortly thereafter.\nSatellite radio.\nSatellite radio: In June 2005, the CRTC outraged some Canadian cultural nationalists (such as the Friends of Canadian Broadcasting) and labour unions by licensing two companies, Canadian Satellite Radio and Sirius Canada to offer satellite radio services in Canada. The two companies are in partnership with American firms XM Satellite Radio and Sirius Satellite Radio respectively, and in accordance with the CRTC decision will only need to offer ten percent Canadian content. The CRTC contends that this low level of Canadian content, particularly when compared to the 35% rule on local radio stations, was necessary because unlicensed U.S. receivers were already flooding into the country, so that enforcing a ban on these receivers would be nearly impossible (see below).\nThis explanation did not satisfy cultural nationalists, who demanded that the federal cabinet overturn the decision and mandate a minimum of 35% Canadian content. Supporters of the decision argue that satellite radio can only be feasibly set up as a continental system, and trying to impose 35% Canadian content across North America is quite unrealistic. They also argue that satellite radio will boost Canadian culture by giving vital exposure to independent artists, instead of concentrating just on the country's stars, and point to the CRTC's successful extraction of promises to program 10% Canadian content on satellite services already operational in the United States as important concessions. Despite popular perception that the CRTC banned Sirius Canada from broadcasting Howard Stern's program, this is not the case. Sirius Canada in fact initially \"chose\" not to air Stern based on the \"possibility\" of a future issue with the CRTC, although the company reversed its decision and began offering Howard Stern in 2006.\n2008 Ottawa radio licence.\n2008 Ottawa radio licences: On November 21, 2008, federal Minister of Canadian Heritage and Official Languages James Moore issued a statement calling on the CRTC to review its approval of two new radio stations, Frank Torres' CIDG-FM and Astral Media's CJOT-FM, which it had licensed in August 2008 to serve the Ottawa-Gatineau radio market. Moore asked the commission to assess whether the francophone population of the Ottawa-Gatineau area was sufficiently well-served by existing French radio services, and to consider licensing one or more of the French language applications, which included a Christian music station, a community radio station and a campus radio station for the Universit\u00e9 du Qu\u00e9bec en Outaouais, in addition to or instead of the approved stations. The review ultimately identified a viable frequency for a third station, and CJFO-FM launched in 2010.\nBell Canada usage-based Internet billing.\nBell Canada usage-based billing: On October 28, 2010, the CRTC handed down its final decision on how wholesale customers can be billed by large network owners. Under the plan which starts within 90 days, Bell will be able to charge wholesale service providers a flat monthly fee to connect to its network, and for a set monthly usage limit per each ISP customer the ISP has. Beyond that set limit, individual users will be charged per gigabyte, depending on the speed of their connections.\nCustomers using the fastest connections of five megabits per second, for example, will have a monthly allotment of 60 GB, beyond which Bell will charge $1.12 per GB to a maximum of $22.50. If a customer uses more than 300 GB a month, Bell will also be able to implement an additional charge of 75 cents per gigabyte. In May 2010, the CRTC ruled that Bell could not implement its usage-based billing system until all of its own retail customers had been moved off older, unlimited downloading plans. The requirement would have meant that Bell would have to move its oldest and most loyal customers.\nThe CRTC also added that Bell would be required to offer to wholesale ISPs the same usage insurance plan it sells to retail customers. Bell appealed both requirements, citing that the rules do not apply to cable companies and that they constituted proactive rate regulation by the CRTC, which goes against government official policy direction that the regulator only intervene in markets after a competitive problem has been proven. In Thursday's decision, the CRTC rescinded both requirements, thereby giving Bell the go-ahead to implement usage-based billing. This ruling according to Teksavvy handcuffs the competitive market. This has been asked by Stephen Harper and Parliament to have the decision reviewed. According to a tweet by Industry Minister Tony Clement, unless the CRTC reverses this decision, the government will use its override power to reverse the decision.\nReception of non-Canadian services.\nWhile an exact number has not been determined, thousands of Canadians have purchased and used what they contend to be grey market radio and television services, licensed in the United States but not in Canada. Users of these unlicensed services contend that they are not directly breaking any laws by simply using the equipment. The equipment is usually purchased from an American supplier (although some merchants have attempted to set up shop in Canada) and the services are billed to an American postal address. The advent of online billing and the easy availability of credit card services has made it relatively easy for almost anyone to maintain an account in good standing, regardless of where they actually live.\nSec. 9(1)(c) of the Radiocommunication Act creates a prohibition against all decoding of encrypted programming signals, followed by an exception where authorization is received from the person holding the lawful right in Canada to transmit and authorize decoding of the signal. This means receiving the encrypted programming of DishNetwork or DirecTV, even with a grey market subscription, may be construed as unlawful (this remains an unresolved Constitutional issue).\nNotwithstanding, possession of DishNetwork or DirecTV equipment is not unlawful as provided by The Radiocommuncation Act Section 4(1)(b), which states:\n\"No person shall, except under and in accordance with a radio authorization, install, operate or possess radio apparatus, other than (b)a radio apparatus that is capable only of the reception of broadcasting and that is not a distribution undertaking. (radio apparatus\" means a device or combination of devices intended for, or capable of being used for, radiocommunication).\"\nSatellite radio poses a more complicated problem for the CRTC. While an unlicensed satellite dish can often be identified easily, satellite radio receivers are much more compact and can rarely be easily identified, at least not without flagrantly violating provisions against unreasonable search and seizure in the Canadian Charter of Rights and Freedoms. Some observers argued that this influenced the CRTC's June 2005 decision to ease Canadian content restrictions on satellite radio (see above).\nStructure.\nThe CRTC is run by up to 13 full-time members (including the chairman, the vice-chairman of broadcasting, and the vice-chairman of telecommunications) appointed by the Cabinet for renewable terms of up to five years. However, unlike the more directly political appointees of the American Federal Communications Commission, the CRTC is an arms-length regulatory body with more autonomous authority over telecommunications. For example, the CRTC's decisions rely more on a judiciary process relying on evidence submitted during public consultations, rather than along party lines as the American FCC is prone to do.\nThe CRTC Interconnection Steering Committee (CISC) assists in developing information, procedures and guidelines for the CRTC's regulatory activities."}
{"id": "5986", "revid": "45307969", "url": "https://en.wikipedia.org/wiki?curid=5986", "title": "Con", "text": "Con or CON may refer to:"}
{"id": "5987", "revid": "7304835", "url": "https://en.wikipedia.org/wiki?curid=5987", "title": "Coal", "text": "Coal is a combustible black or brownish-black sedimentary rock, formed as rock strata called coal seams. Coal is mostly carbon with variable amounts of other elements, chiefly hydrogen, sulfur, oxygen, and nitrogen. \nCoal is a type of fossil fuel, formed when dead plant matter decays into peat which is converted into coal by the heat and pressure of deep burial over millions of years. Vast deposits of coal originate in former wetlands called coal forests that covered much of the Earth's tropical land areas during the late Carboniferous (Pennsylvanian) and Permian times.\nCoal is used primarily as a fuel. While coal has been known and used for thousands of years, its usage was limited until the Industrial Revolution. With the invention of the steam engine, coal consumption increased. In 2020, coal supplied about a quarter of the world's primary energy and over a third of its electricity. Some iron and steel-making and other industrial processes burn coal.\nThe extraction and burning of coal damages the environment, causing premature death and illness, and it is the largest anthropogenic source of carbon dioxide contributing to climate change. Fourteen billion tonnes of carbon dioxide were emitted by burning coal in 2020, which is 40% of total fossil fuel emissions and over 25% of total global greenhouse gas emissions. As part of worldwide energy transition, many countries have reduced or eliminated their use of coal power. The United Nations Secretary General asked governments to stop building new coal plants by 2020.\nGlobal coal use was 8.3 billion tonnes in 2022, and is set to remain at record levels in 2023. To meet the Paris Agreement target of keeping global warming below coal use needs to halve from 2020 to 2030, and \"phasing down\" coal was agreed upon in the Glasgow Climate Pact.\nThe largest consumer and importer of coal in 2020 was China, which accounts for almost half the world's annual coal production, followed by India with about a tenth. Indonesia and Australia export the most, followed by Russia.\nEtymology.\nThe word originally took the form \"col\" in Old English, from reconstructed Proto-Germanic *\"kula\"(\"n\"), from Proto-Indo-European root *\"g\"(\"e\")\"u-lo-\" \"live coal\". Germanic cognates include the Old Frisian , Middle Dutch , Dutch , Old High German , German and Old Norse . Irish is also a cognate via the Indo-European root.\nFormation of coal.\nThe conversion of dead vegetation into coal is called coalification. At various times in the geologic past, the Earth had dense forests in low-lying areas. In these wetlands, the process of coalification began when dead plant matter was protected from oxidation, usually by mud or acidic water, and was converted into peat. The resulting peat bogs, which trapped immense amounts of carbon, were eventually deeply buried by sediments. Then, over millions of years, the heat and pressure of deep burial caused the loss of water, methane and carbon dioxide and increased the proportion of carbon. The grade of coal produced depended on the maximum pressure and temperature reached, with lignite (also called \"brown coal\") produced under relatively mild conditions, and sub-bituminous coal, bituminous coal, or anthracite coal (also called \"hard coal\" or \"black coal\") produced in turn with increasing temperature and pressure.\nOf the factors involved in coalification, temperature is much more important than either pressure or time of burial. Subbituminous coal can form at temperatures as low as while anthracite requires a temperature of at least .\nAlthough coal is known from most geologic periods, 90% of all coal beds were deposited in the Carboniferous and Permian periods. Paradoxically, this was during the Late Paleozoic icehouse, a time of global glaciation. However, the drop in global sea level accompanying the glaciation exposed continental shelves that had previously been submerged, and to these were added wide river deltas produced by increased erosion due to the drop in base level. These widespread areas of wetlands provided ideal conditions for coal formation. The rapid formation of coal ended with the coal gap in the Permian\u2013Triassic extinction event, where coal is rare.\nFavorable geography alone does not explain the extensive Carboniferous coal beds. Other factors contributing to rapid coal deposition were high oxygen levels, above 30%, that promoted intense wildfires and formation of charcoal that was all but indigestible by decomposing organisms; high carbon dioxide levels that promoted plant growth; and the nature of Carboniferous forests, which included lycophyte trees whose determinate growth meant that carbon was not tied up in heartwood of living trees for long periods.\nOne theory suggested that about 360 million years ago, some plants evolved the ability to produce lignin, a complex polymer that made their cellulose stems much harder and more woody. The ability to produce lignin led to the evolution of the first trees. But bacteria and fungi did not immediately evolve the ability to decompose lignin, so the wood did not fully decay but became buried under sediment, eventually turning into coal. About 300 million years ago, mushrooms and other fungi developed this ability, ending the main coal-formation period of earth's history. Although some authors pointed at some evidence of lignin degradation during the Carboniferous, and suggested that climatic and tectonic factors were a more plausible explanation, reconstruction of ancestral enzymes by phylogenetic analysis corroborated a hypothesis that lignin degrading enzymes appeared in fungi approximately 200 MYa.\nOne likely tectonic factor was the Central Pangean Mountains, an enormous range running along the equator that reached its greatest elevation near this time. Climate modeling suggests that the Central Pangean Mountains contributed to the deposition of vast quantities of coal in the late Carboniferous. The mountains created an area of year-round heavy precipitation, with no dry season typical of a monsoon climate. This is necessary for the preservation of peat in coal swamps.\nCoal is known from Precambrian strata, which predate land plants. This coal is presumed to have originated from residues of algae.\nSometimes coal seams (also known as coal beds) are interbedded with other sediments in a cyclothem. Cyclothems are thought to have their origin in glacial cycles that produced fluctuations in sea level, which alternately exposed and then flooded large areas of continental shelf.\nChemistry of coalification.\nThe woody tissue of plants is composed mainly of cellulose, hemicellulose, and lignin. Modern peat is mostly lignin, with a content of cellulose and hemicellulose ranging from 5% to 40%. Various other organic compounds, such as waxes and nitrogen- and sulfur-containing compounds, are also present. Lignin has a weight composition of about 54% carbon, 6% hydrogen, and 30% oxygen, while cellulose has a weight composition of about 44% carbon, 6% hydrogen, and 49% oxygen. Bituminous coal has a composition of about 84.4% carbon, 5.4% hydrogen, 6.7% oxygen, 1.7% nitrogen, and 1.8% sulfur, on a weight basis. The low oxygen content of coal shows that coalification removed most of the oxygen and much of the hydrogen a process called \"carbonization\".\nCarbonization proceeds primarily by dehydration, decarboxylation, and demethanation. Dehydration removes water molecules from the maturing coal via reactions such as\nDecarboxylation removes carbon dioxide from the maturing coal:\nwhile demethanation proceeds by reaction such as\nIn these formulas, R represents the remainder of a cellulose or lignin molecule to which the reacting groups are attached.\nDehydration and decarboxylation take place early in coalification, while demethanation begins only after the coal has already reached bituminous rank. The effect of decarboxylation is to reduce the percentage of oxygen, while demethanation reduces the percentage of hydrogen. Dehydration does both, and (together with demethanation) reduces the saturation of the carbon backbone (increasing the number of double bonds between carbon).\nAs carbonization proceeds, aliphatic compounds convert to aromatic compounds. Similarly, aromatic rings fuse into polyaromatic compounds (linked rings of carbon atoms). The structure increasingly resembles graphene, the structural element of graphite.\nChemical changes are accompanied by physical changes, such as decrease in average pore size.\nMacerals.\nMacerals are coalified plant parts that retain the morphology and some properties of the original plant. In many coals, individual macerals can be identified visually. Some macerals include:\nIn coalification huminite is replaced by vitreous (shiny) \"vitrinite\". Maturation of bituminous coal is characterized by \"bitumenization\", in which part of the coal is converted to bitumen, a hydrocarbon-rich gel. Maturation to anthracite is characterized by \"debitumenization\" (from demethanation) and the increasing tendency of the anthracite to break with a conchoidal fracture, similar to the way thick glass breaks.\nTypes.\nAs geological processes apply pressure to dead biotic material over time, under suitable conditions, its metamorphic grade or rank increases successively into:\nThere are several international standards for coal. The classification of coal is generally based on the content of volatiles. However the most important distinction is between thermal coal (also known as steam coal), which is burnt to generate electricity via steam; and metallurgical coal (also known as coking coal), which is burnt at high temperature to make steel.\nHilt's law is a geological observation that (within a small area) the deeper the coal is found, the higher its rank (or grade). It applies if the thermal gradient is entirely vertical; however, metamorphism may cause lateral changes of rank, irrespective of depth. For example, some of the coal seams of the Madrid, New Mexico coal field were partially converted to anthracite by contact metamorphism from an igneous sill while the remainder of the seams remained as bituminous coal.\nHistory.\nThe earliest recognized use is from the Shenyang area of China where by 4000 BC Neolithic inhabitants had begun carving ornaments from black lignite. Coal from the Fushun mine in northeastern China was used to smelt copper as early as 1000 BC. Marco Polo, the Italian who traveled to China in the 13th century, described coal as \"black stones\u00a0... which burn like logs\", and said coal was so plentiful, people could take three hot baths a week. In Europe, the earliest reference to the use of coal as fuel is from the geological treatise \"On Stones\" (Lap. 16) by the Greek scientist Theophrastus (c. 371\u2013287 BC):\nOutcrop coal was used in Britain during the Bronze Age (3000\u20132000 BC), where it formed part of funeral pyres. In Roman Britain, with the exception of two modern fields, \"the Romans were exploiting coals in all the major coalfields in England and Wales by the end of the second century AD\". Evidence of trade in coal, dated to about AD 200, has been found at the Roman settlement at Heronbridge, near Chester; and in the Fenlands of East Anglia, where coal from the Midlands was transported via the Car Dyke for use in drying grain. Coal cinders have been found in the hearths of villas and Roman forts, particularly in Northumberland, dated to around AD 400. In the west of England, contemporary writers described the wonder of a permanent brazier of coal on the altar of Minerva at Aquae Sulis (modern day Bath), although in fact easily accessible surface coal from what became the Somerset coalfield was in common use in quite lowly dwellings locally. Evidence of coal's use for iron-working in the city during the Roman period has been found. In Eschweiler, Rhineland, deposits of bituminous coal were used by the Romans for the smelting of iron ore.\nNo evidence exists of coal being of great importance in Britain before about AD 1000, the High Middle Ages. Coal came to be referred to as \"seacoal\" in the 13th century; the wharf where the material arrived in London was known as Seacoal Lane, so identified in a charter of King Henry III granted in 1253. Initially, the name was given because much coal was found on the shore, having fallen from the exposed coal seams on cliffs above or washed out of underwater coal outcrops, but by the time of Henry VIII, it was understood to derive from the way it was carried to London by sea. In 1257\u20131259, coal from Newcastle upon Tyne was shipped to London for the smiths and lime-burners building Westminster Abbey. Seacoal Lane and Newcastle Lane, where coal was unloaded at wharves along the River Fleet, still exist.\nThese easily accessible sources had largely become exhausted (or could not meet the growing demand) by the 13th century, when underground extraction by shaft mining or adits was developed. The alternative name was \"pitcoal\", because it came from mines.\nCooking and home heating with coal (in addition to firewood or instead of it) has been done in various times and places throughout human history, especially in times and places where ground-surface coal was available and firewood was scarce, but a widespread reliance on coal for home hearths probably never existed until such a switch in fuels happened in London in the late sixteenth and early seventeenth centuries. Historian Ruth Goodman has traced the socioeconomic effects of that switch and its later spread throughout Britain and suggested that its importance in shaping the industrial adoption of coal has been previously underappreciated.\nThe development of the Industrial Revolution led to the large-scale use of coal, as the steam engine took over from the water wheel. In 1700, five-sixths of the world's coal was mined in Britain. Britain would have run out of suitable sites for watermills by the 1830s if coal had not been available as a source of energy. In 1947 there were some 750,000 miners in Britain, but the last deep coal mine in the UK closed in 2015.\nA grade between bituminous coal and anthracite was once known as \"steam coal\" as it was widely used as a fuel for steam locomotives. In this specialized use, it is sometimes known as \"sea coal\" in the United States. Small \"steam coal\", also called \"dry small steam nuts\" (DSSN), was used as a fuel for domestic water heating.\nCoal played an important role in industry in the 19th and 20th century. The predecessor of the European Union, the European Coal and Steel Community, was based on the trading of this commodity.\nCoal continues to arrive on beaches around the world from both natural erosion of exposed coal seams and windswept spills from cargo ships. Many homes in such areas gather this coal as a significant, and sometimes primary, source of home heating fuel.\nComposition.\nCoal consists mainly of a black mixture of diverse organic compounds and polymers. Of course, several kinds of coals exist, with variable dark colors and variable compositions. Young coals (brown coal, lignite) are not black. The two main black coals are bituminous, which is more abundant, and anthracite. The % carbon in coal follows the order anthracite &gt; bituminous &gt; lignite &gt; brown coal. The fuel value of coal varies in the same order. Some anthracite deposits contain pure carbon in the form of graphite.\nFor bituminous coal, the elemental composition on a dry, ash-free basis of 84.4% carbon, 5.4% hydrogen, 6.7% oxygen, 1.7% nitrogen, and 1.8% sulfur, on a weight basis. This composition reflects partly the composition of the precursor plants. The second main fraction of coal is ash, an undesirable, noncombustable mixture of inorganic minerals. The composition of ash is often discussed in terms of oxides obtained after combustion in air:\nOf particular interest is the sulfur content of coal, which can vary from less than 1% to as much as 4%. Most of the sulfur and most of the nitrogen is incorporated into the organic fraction in the form of organosulfur compounds and organonitrogen compounds. This sulfur and nitrogen are strongly bound within the hydrocarbon matrix. These elements are released as SO2 and NOx upon combustion. They cannot be removed, economically at least, otherwise. Some coals contain inorganic sulfur, mainly in the form of iron pyrite (FeS2). Being a dense mineral, it can be removed from coal by mechanical means, e.g. by froth flotation. Some sulfate occurs in coal, especially weathered samples. It is not volatilized and can be removed by washing.\nMinor components include:\nAs minerals, Hg, As, and Se are not problematic to the environment, especially since they are only trace components. They become however mobile (volatile or water-soluble) when these minerals are combusted.\nUses.\nMost coal is used as fuel. 27.6% of world energy was supplied by coal in 2017 and Asia used almost three-quarters of it. Other large-scale applications also exist. The energy density of coal is roughly 24 megajoules per kilogram (approximately 6.7 kilowatt-hours per kg). For a coal power plant with a 40% efficiency, it takes an estimated of coal to power a 100\u00a0W lightbulb for one year.\nElectricity generation.\nIn 2022, 68% of global coal use was used for electricity generation. \nCoal burnt in coal power stations to generate electricity is called thermal coal. It is usually pulverized and then burned in a furnace with a boiler. The furnace heat converts boiler water to steam, which is then used to spin turbines which turn generators and create electricity. The thermodynamic efficiency of this process varies between about 25% and 50% depending on the pre-combustion treatment, turbine technology (e.g. supercritical steam generator) and the age of the plant. \nA few integrated gasification combined cycle (IGCC) power plants have been built, which burn coal more efficiently. Instead of pulverizing the coal and burning it directly as fuel in the steam-generating boiler, the coal is gasified to create syngas, which is burned in a gas turbine to produce electricity (just like natural gas is burned in a turbine). Hot exhaust gases from the turbine are used to raise steam in a heat recovery steam generator which powers a supplemental steam turbine. The overall plant efficiency when used to provide combined heat and power can reach as much as 94%. IGCC power plants emit less local pollution than conventional pulverized coal-fueled plants. Other ways to use coal are as coal-water slurry fuel (CWS), which was developed in the Soviet Union, or in an MHD topping cycle. However these are not widely used due to lack of profit.\nIn 2017 38% of the world's electricity came from coal, the same percentage as 30 years previously. In 2018 global installed capacity was 2TW (of which 1TW is in China) which was 30% of total electricity generation capacity. The most dependent major country is South Africa, with over 80% of its electricity generated by coal; but China alone generates more than half of the world's coal-generated electricity. Efforts around the world to reduce the use of coal have led some regions to switch to natural gas and renewable energy. In 2018 coal-fired power station capacity factor averaged 51%, that is they operated for about half their available operating hours.\nCoke.\nCoke is a solid carbonaceous residue that is used in manufacturing steel and other iron-containing products. Coke is made when metallurgical coal (also known as \"coking coal\") is baked in an oven without oxygen at temperatures as high as 1,000\u00a0\u00b0C, driving off the volatile constituents and fusing together the fixed carbon and residual ash. Metallurgical coke is used as a fuel and as a reducing agent in smelting iron ore in a blast furnace. The carbon monoxide produced by its combustion reduces hematite (an iron oxide) to iron.\nPig iron, which is too rich in dissolved carbon, is also produced.\nThe coke must be strong enough to resist the weight of overburden in the blast furnace, which is why coking coal is so important in making steel using the conventional route. Coke from coal is grey, hard, and porous and has a heating value of 29.6 MJ/kg. Some coke-making processes produce byproducts, including coal tar, ammonia, light oils, and coal gas.\nPetroleum coke (petcoke) is the solid residue obtained in oil refining, which resembles coke but contains too many impurities to be useful in metallurgical applications.\nProduction of chemicals.\nChemicals have been produced from coal since the 1950s. Coal can be used as a feedstock in the production of a wide range of chemical fertilizers and other chemical products. The main route to these products was coal gasification to produce syngas. Primary chemicals that are produced directly from the syngas include methanol, hydrogen, and carbon monoxide, which are the chemical building blocks from which a whole spectrum of derivative chemicals are manufactured, including olefins, acetic acid, formaldehyde, ammonia, urea, and others. The versatility of syngas as a precursor to primary chemicals and high-value derivative products provides the option of using coal to produce a wide range of commodities. In the 21st century, however, the use of coal bed methane is becoming more important.\nBecause the slate of chemical products that can be made via coal gasification can in general also use feedstocks derived from natural gas and petroleum, the chemical industry tends to use whatever feedstocks are most cost-effective. Therefore, interest in using coal tended to increase for higher oil and natural gas prices and during periods of high global economic growth that might have strained oil and gas production.\nCoal to chemical processes require substantial quantities of water. Much coal to chemical production is in China where coal dependent provinces such as Shanxi are struggling to control its pollution.\nLiquefaction.\nCoal can be converted directly into synthetic fuels equivalent to gasoline or diesel by hydrogenation or carbonization. Coal liquefaction emits more carbon dioxide than liquid fuel production from crude oil. Mixing in biomass and using carbon capture and storage (CCS) would emit slightly less than the oil process but at a high cost. State owned China Energy Investment runs a coal liquefaction plant and plans to build 2 more.\nCoal liquefaction may also refer to the cargo hazard when shipping coal.\nGasification.\nCoal gasification, as part of an integrated gasification combined cycle (IGCC) coal-fired power station, is used to produce syngas, a mixture of carbon monoxide (CO) and hydrogen (H2) gas to fire gas turbines to produce electricity. Syngas can also be converted into transportation fuels, such as gasoline and diesel, through the Fischer\u2013Tropsch process; alternatively, syngas can be converted into methanol, which can be blended into fuel directly or converted to gasoline via the methanol to gasoline process. Gasification combined with Fischer\u2013Tropsch technology was used by the Sasol chemical company of South Africa to make chemicals and motor vehicle fuels from coal.\nDuring gasification, the coal is mixed with oxygen and steam while also being heated and pressurized. During the reaction, oxygen and water molecules oxidize the coal into carbon monoxide (CO), while also releasing hydrogen gas (H2). This used to be done in underground coal mines, and also to make town gas, which was piped to customers to burn for illumination, heating, and cooking.\nIf the refiner wants to produce gasoline, the syngas is routed into a Fischer\u2013Tropsch reaction. This is known as indirect coal liquefaction. If hydrogen is the desired end-product, however, the syngas is fed into the water gas shift reaction, where more hydrogen is liberated:\nCoal industry.\nMining.\nAbout 8,000 Mt of coal are produced annually, about 90% of which is hard coal and 10% lignite. just over half is from underground mines. The coal mining industry employs almost 2.7 million workers. More accidents occur during underground mining than surface mining. Not all countries publish mining accident statistics so worldwide figures are uncertain, but it is thought that most deaths occur in coal mining accidents in China: in 2017 there were 375 coal mining related deaths in China. Most coal mined is thermal coal (also called steam coal as it is used to make steam to generate electricity) but metallurgical coal (also called \"metcoal\" or \"coking coal\" as it is used to make coke to make iron) accounts for 10% to 15% of global coal use.\nAs a traded commodity.\nChina mines almost half the world's coal, followed by India with about a tenth. At 471 Mt and a 34% share of global exports, Indonesia was the largest exporter by volume in 2022, followed by Australia with 344 Mt and Russia with 224 Mt. Other major exporters of coal are the United States, South Africa, Colombia, and Canada. In 2022, China, India, and Japan were the biggest importers of coal, importing 301, 228, and 184 Mt respectively. Russia is increasingly orienting its coal exports from Europe to Asia as Europe transitions to renewable energy and subjects Russia to sanctions over its invasion of Ukraine.\nThe price of metallurgical coal is volatile and much higher than the price of thermal coal because metallurgical coal must be lower in sulfur and requires more cleaning. Coal futures contracts provide coal producers and the electric power industry an important tool for hedging and risk management.\nIn some countries, new onshore wind or solar generation already costs less than coal power from existing plants.\nHowever, for China this is forecast for the early 2020s and for southeast Asia not until the late 2020s. In India, building new plants is uneconomic and, despite being subsidized, existing plants are losing market share to renewables.\nIn many countries in the Global North, there is a move away from the use of coal and former mine sites are being used as a tourist attraction.\nMarket trends.\nIn 2022, China used 4520 Mt of coal, comprising more than half of global coal consumption. India, the European Union, and the United States, were the next largest consumers of coal, using 1162, 461, and 455 Mt respectively. Over the past decade, China has almost always accounted for the lion's share of the global growth in coal demand. Therefore, international market trends depend on Chinese energy policy. \nAlthough the government effort to reduce air pollution in China means that the global long-term trend is to burn less coal, the short and medium term trends may differ, in part due to Chinese financing of new coal-fired power plants in other countries.\nPreliminary analysis by International Energy Agency (IEA) indicates that global coal exports reached an all-time high in 2023. Through to 2026, the IEA expects global coal trade to decline by about 12%, driven by growing domestic production in coal-intensive economies such as China and India and coal phase-out plans elsewhere, such as in Europe. While thermal coal exports are expected to decline by about 16% by 2026, exports of metallurgical coal are expected to slightly increase by almost 2%. \nDamage to human health.\nThe use of coal as fuel causes health problems and deaths. The mining and processing of coal causes air and water pollution. Coal-powered plants emit nitrogen oxides, sulfur dioxide, particulate pollution, and heavy metals, which adversely affect human health. Coal bed methane extraction is important to avoid mining accidents.\nThe deadly London smog was caused primarily by the heavy use of coal. Globally coal is estimated to cause 800,000 premature deaths every year, mostly in India and China.\nBurning coal is a major contributor to sulfur dioxide emissions, which creates PM2.5 particulates, the most dangerous form of air pollution.\nCoal smokestack emissions cause asthma, strokes, reduced intelligence, artery blockages, heart attacks, congestive heart failure, cardiac arrhythmias, mercury poisoning, arterial occlusion, and lung cancer.\nAnnual health costs in Europe from use of coal to generate electricity are estimated at up to \u20ac43 billion.\nIn China, early deaths due to air pollution coal plants have been estimated at 200 per GW-year, however they may be higher around power plants where scrubbers are not used or lower if they are far from cities. Improvements to China's air quality and human health would grow with more stringent climate policies, mainly because the country's energy is so heavily reliant on coal. And there would be a net economic benefit.\nA 2017 study in the \"Economic Journal\" found that for Britain during the period 1851\u20131860, \"a one standard deviation increase in coal use raised infant mortality by 6\u20138% and that industrial coal use explains roughly one-third of the urban mortality penalty observed during this period.\"\nBreathing in coal dust causes coalworker's pneumoconiosis or \"black lung\", so called because the coal dust literally turns the lungs black. In the US alone, it is estimated that 1,500 former employees of the coal industry die every year from the effects of breathing in coal mine dust.\nHuge amounts of coal ash and other waste is produced annually. Use of coal generates hundreds of millions of tons of ash and other waste products every year. These include fly ash, bottom ash, and flue-gas desulfurization sludge, that contain mercury, uranium, thorium, arsenic, and other heavy metals, along with non-metals such as selenium.\nAround 10% of coal is ash. Coal ash is hazardous and toxic to human beings and some other living things. Coal ash contains the radioactive elements uranium and thorium. Coal ash and other solid combustion byproducts are stored locally and escape in various ways that expose those living near coal plants to radiation and environmental toxics.\nDamage to the environment.\nCoal mining, coal combustion wastes, and flue gas are causing major environmental damage.\nWater systems are affected by coal mining. For example, the mining of coal affects groundwater and water table levels and acidity. Spills of fly ash, such as the Kingston Fossil Plant coal fly ash slurry spill, can also contaminate land and waterways, and destroy homes. Power stations that burn coal also consume large quantities of water. This can affect the flows of rivers, and has consequential impacts on other land uses. In areas of water scarcity, such as the Thar Desert in Pakistan, coal mining and coal power plants contribute to the depletion of water resources.\nOne of the earliest known impacts of coal on the water cycle was acid rain. In 2014, approximately 100 Tg/S of sulfur dioxide (SO2) was released, over half of which was from burning coal. After release, the sulfur dioxide is oxidized to H2SO4 which scatters solar radiation, hence its increase in the atmosphere exerts a cooling effect on the climate. This beneficially masks some of the warming caused by increased greenhouse gases. However, the sulfur is precipitated out of the atmosphere as acid rain in a matter of weeks, whereas carbon dioxide remains in the atmosphere for hundreds of years. Release of SO2 also contributes to the widespread acidification of ecosystems.\nDisused coal mines can also cause issues. Subsidence can occur above tunnels, causing damage to infrastructure or cropland. Coal mining can also cause long lasting fires, and it has been estimated that thousands of coal seam fires are burning at any given time. For example, Brennender Berg has been burning since 1668, and is still burning in the 21st century.\nThe production of coke from coal produces ammonia, coal tar, and gaseous compounds as byproducts which if discharged to land, air or waterways can pollute the environment. The Whyalla steelworks is one example of a coke producing facility where liquid ammonia was discharged to the marine environment.\nClimate change.\nThe largest and most long-term effect of coal use is the release of carbon dioxide, a greenhouse gas that causes climate change. Coal-fired power plants were the single largest contributor to the growth in global CO2 emissions in 2018, 40% of the total fossil fuel emissions, and more than a quarter of total emissions. Coal mining can emit methane, another greenhouse gas.\nIn 2016 world gross carbon dioxide emissions from coal usage were 14.5 gigatonnes. For every megawatt-hour generated, coal-fired electric power generation emits around a tonne of carbon dioxide, which is double the approximately 500\u00a0kg of carbon dioxide released by a natural gas-fired electric plant. The emission intensity of coal varies with type and generator technology and exceeds 1200\u00a0g per kWh in some countries. In 2013, the head of the UN climate agency advised that most of the world's coal reserves should be left in the ground to avoid catastrophic global warming. To keep global warming below 1.5\u00a0\u00b0C or 2\u00a0\u00b0C hundreds, or possibly thousands, of coal-fired power plants will need to be retired early.\nUnderground fires.\nThousands of coal fires are burning around the world. Those burning underground can be difficult to locate and many cannot be extinguished. Fires can cause the ground above to subside, their combustion gases are dangerous to life, and breaking out to the surface can initiate surface wildfires. Coal seams can be set on fire by spontaneous combustion or contact with a mine fire or surface fire. Lightning strikes are an important source of ignition. The coal continues to burn slowly back into the seam until oxygen (air) can no longer reach the flame front. A grass fire in a coal area can set dozens of coal seams on fire. Coal fires in China burn an estimated 120 million tons of coal a year, emitting 360 million metric tons of CO2, amounting to 2\u20133% of the annual worldwide production of CO2 from fossil fuels. \nPollution mitigation and carbon capture.\nSystems and technologies exist to mitigate the health and environmental impact of burning coal for energy. \nPrecombustion treatment.\nRefined coal is the product of a coal-upgrading technology that removes moisture and certain pollutants from lower-rank coals such as sub-bituminous and lignite (brown) coals. It is one form of several precombustion treatments and processes for coal that alter coal's characteristics before it is burned. Thermal efficiency improvements are achievable by improved pre-drying (especially relevant with high-moisture fuel such as lignite or biomass). The goals of precombustion coal technologies are to increase efficiency and reduce emissions when the coal is burned. Precombustion technology can sometimes be used as a supplement to postcombustion technologies to control emissions from coal-fueled boilers.\nPost combustion approaches.\nPost combustion approaches to mitigate pollution include flue-gas desulfurization, selective catalytic reduction, electrostatic precipitators, and fly ash reduction.\nCarbon capture and storage.\nCarbon capture and storage (CCS) can be used to capture carbon dioxide from the flue gas of coal power plants and bury it securely in an underground reservoir. Between 1972 and 2017, plans were made to add CCS to enough coal and gas power plants to sequester 161 million tonnes of per year, but by 2021 98% of these plans had failed. Cost, the absence of measures to address long-term liability for stored CO2, and limited social acceptability have all contributed to project cancellations. As of 2024, CCS is in operation at only four coal power plants and one gas power plant worldwide.\n\"Clean coal\" and \"abated coal\".\nSince the mid-1980s, the term \"clean coal\" has been widely used with various meanings. Initially, \"clean coal technology\" referred to scrubbers and catalytic converters that reduced the pollutants that cause acid rain. The scope then expanded to include reduction of other pollutants such as mercury. Recently, the term has come to encompass the use of CCS to reduce greenhouse gas emissions (GHG). In political discourse, the phrase \"clean coal\" is sometimes used to suggest that coal itself can be clean. This suggestion is false: Technologies to mitigate emissions are implemented in the plants where coal is processed and burned, but coal as a product is intrinsically dirty.\nIn discussions on greenhouse gas emissions, another common term is \"abatement\" of coal use. In the 2023 United Nations Climate Change Conference, an agreement was reached to phase down unabated coal use. Since the term \"abated\" was not defined, the agreement was criticized for being open to abuse. Without a clear definition, is possible for fossil fuel use to be called \"abated\" if it uses CCS only in a minimal fashion, such as capturing only 30% of the emissions from a plant.\nThe IPCC considers fossil fuels to be unabated if they are \"produced and used without interventions that substantially reduce the amount of GHG emitted throughout the life-cycle; for example, capturing 90% or more from power plants.\"\nEconomics.\nIn 2018 was invested in coal supply but almost all for sustaining production levels rather than opening new mines.\nIn the long term coal and oil could cost the world trillions of dollars per year. Coal alone may cost Australia billions, whereas costs to some smaller companies or cities could be on the scale of millions of dollars. The economies most damaged by coal (via climate change) may be India and the US as they are the countries with the highest social cost of carbon. Bank loans to finance coal are a risk to the Indian economy.\nChina is the largest producer of coal in the world. It is the world's largest energy consumer, and coal in China supplies 60% of its primary energy. However two fifths of China's coal power stations are estimated to be loss-making.\nAir pollution from coal storage and handling costs the US almost 200 dollars for every extra ton stored, due to PM2.5. Coal pollution costs the each year. Measures to cut air pollution benefit individuals financially and the economies of countries such as China.\nSubsidies.\nSubsidies for coal in 2021 have been estimated at , not including electricity subsidies, and are expected to rise in 2022. G20 countries provide at least of government support per year for the production of coal, including coal-fired power: many subsidies are impossible to quantify but they include in domestic and international public finance, in fiscal support, and in state-owned enterprise (SOE) investments per year. In the EU state aid to new coal-fired plants is banned from 2020, and to existing coal-fired plants from 2025. As of 2018, government funding for new coal power plants was supplied by Exim Bank of China, the Japan Bank for International Cooperation and Indian public sector banks. Coal in Kazakhstan was the main recipient of coal consumption subsidies totalling US$2 billion in 2017. Coal in Turkey benefited from substantial subsidies in 2021.\nStranded assets.\nSome coal-fired power stations could become stranded assets, for example China Energy Investment, the world's largest power company, risks losing half its capital. However, state-owned electricity utilities such as Eskom in South Africa, Perusahaan Listrik Negara in Indonesia, Sarawak Energy in Malaysia, Taipower in Taiwan, EGAT in Thailand, Vietnam Electricity and E\u00dcA\u015e in Turkey are building or planning new plants. As of 2021 this may be helping to cause a carbon bubble which could cause financial instability if it bursts.\nPolitics.\nCountries building or financing new coal-fired power stations, such as China, India, Indonesia, Vietnam, Turkey and Bangladesh, face mounting international criticism for obstructing the aims of the Paris Agreement. In 2019, the Pacific Island nations (in particular Vanuatu and Fiji) criticized Australia for failing to cut their emissions at a faster rate than they were, citing concerns about coastal inundation and erosion. In May 2021, the G7 members agreed to end new direct government support for international coal power generation.\nCultural usage.\nCoal is the official state mineral of Kentucky, and the official state rock of Utah and West Virginia. These US states have a historic link to coal mining.\nSome cultures hold that children who misbehave will receive only a lump of coal from Santa Claus for Christmas in their stockings instead of presents.\nIt is also customary and considered lucky in Scotland to give coal as a gift on New Year's Day. This occurs as part of first-footing and represents warmth for the year to come."}
{"id": "5992", "revid": "21418533", "url": "https://en.wikipedia.org/wiki?curid=5992", "title": "Traditional Chinese medicine", "text": "Traditional Chinese medicine (TCM) is an alternative medical practice drawn from traditional medicine in China. A large share of its claims are pseudoscientific, with the majority of treatments having no robust evidence of effectiveness or logical mechanism of action.\nMedicine in traditional China encompassed a range of sometimes competing health and healing practices, folk beliefs, literati theory and Confucian philosophy, herbal remedies, food, diet, exercise, medical specializations, and schools of thought. TCM as it exists today has been described as a largely 20th century invention. In the early twentieth century, Chinese cultural and political modernizers worked to eliminate traditional practices as backward and unscientific. Traditional practitioners then selected elements of philosophy and practice and organized them into what they called \"Chinese medicine\" (Chinese: \u4e2d\u533b \"Zhongyi\"). In the 1950s, the Chinese government sought to revive traditional medicine (including legalizing previously banned practices) and sponsored the integration of TCM and Western medicine, and in the Cultural Revolution of the 1960s, promoted TCM as inexpensive and popular. The creation of modern TCM was largely spearheaded by Mao Zedong, despite the fact that, according to \"The Private Life of Chairman Mao\", he did not believe in its effectiveness. After the opening of relations between the United States and China after 1972, there was great interest in the West for what is now called traditional Chinese medicine (TCM).\nTCM is said to be based on such texts as \"Huangdi Neijing\" (The Inner Canon of the Yellow Emperor), and \"Compendium of Materia Medica\", a sixteenth-century encyclopedic work, and includes various forms of herbal medicine, acupuncture, cupping therapy, gua sha, massage (tui na), bonesetter (die-da), exercise (qigong), and dietary therapy. TCM is widely used in the Sinosphere. One of the basic tenets is that the body's \"qi\" is circulating through channels called meridians having branches connected to bodily organs and functions. There is no evidence that meridians or vital energy exist. Concepts of the body and of disease used in TCM reflect its ancient origins and its emphasis on dynamic processes over material structure, similar to the humoral theory of ancient Greece and ancient Rome.\nThe demand for traditional medicines in China is a major generator of illegal wildlife smuggling, linked to the killing and smuggling of endangered animals. The Chinese authorities have engaged in attempts to crack down on illegal TCM-related wildlife smuggling.\nAncient history.\nScholars in the history of medicine in China distinguish its doctrines and practice from those of present-day TCM. J. A. Jewell and S. M. Hillier state that the term \"Traditional Chinese Medicine\" became an established term due to the work of Dr. Kan-Wen Ma, a Western-trained medical doctor who was persecuted during the Cultural Revolution and immigrated to Britain, joining the University of London's Wellcome Institute for the History of Medicine. Ian Johnson says, on the other hand, that the English-language term \"traditional Chinese medicine\" was coined by \"party propagandists\" in 1955. \nNathan Sivin criticizes attempts to treat medicine and medical practices in traditional China as if they were a single system. Instead, he says, there were 2,000 years of \"medical system in turmoil\" and speaks of a \"myth of an unchanging medical tradition\". He urges that \"Traditional medicine translated purely into terms of modern medicine becomes partly nonsensical, partly irrelevant, and partly mistaken; that is also true the other way around, a point easily overlooked.\" TJ Hinrichs observes that people in modern Western societies divide healing practices into biomedicine for the body, psychology for the mind, and religion for the spirit, but these distinctions are inadequate to describe medical concepts among Chinese historically and to a considerable degree today.\nThe medical anthropologist Charles Leslie writes that Chinese, Greco-Arabic, and Indian traditional medicines were all grounded in systems of correspondence that aligned the organization of society, the universe, and the human body and other forms of life into an \"all-embracing order of things\". Each of these traditional systems was organized with such qualities as heat and cold, wet and dry, light and darkness, qualities that also align the seasons, compass directions, and the human cycle of birth, growth, and death. They provided, Leslie continued, a \"comprehensive way of conceiving patterns that ran through all of nature,\" and they \"served as a classificatory and mnemonic device to observe health problems and to reflect upon, store, and recover empirical knowledge,\" but they were also \"subject to stultifying theoretical elaboration, self-deception, and dogmatism.\"\nThe doctrines of Chinese medicine are rooted in books such as the \"Yellow Emperor's Inner Canon\" and the \"Treatise on Cold Damage\", as well as in cosmological notions such as yin\u2013yang and the five phases. The \"Compendium of Materia Medica\" dates back to around 1,100 BCE when only a few dozen drugs were described. By the end of the 16th century, the number of drugs documented had reached close to 1,900. And by the end of the last century, published records of CMM had reached 12,800 drugs.\" Starting in the 1950s, these precepts were standardized in the People's Republic of China, including attempts to integrate them with modern notions of anatomy and pathology. In the 1950s, the Chinese government promoted a systematized form of TCM.\nShang dynasty.\nTraces of therapeutic activities in China date from the Shang dynasty (14th\u201311th centuries BCE). Though the Shang did not have a concept of \"medicine\" as distinct from other health practices, their oracular inscriptions on bones and tortoise shells refer to illnesses that affected the Shang royal family: eye disorders, toothaches, bloated abdomen, and such. Shang elites usually attributed them to curses sent by their ancestors. There is currently no evidence that the Shang nobility used herbal remedies.\nStone and bone needles found in ancient tombs led Joseph Needham to speculate that acupuncture might have been carried out in the Shang dynasty. This being said, most historians now make a distinction between medical lancing (or bloodletting) and acupuncture in the narrower sense of using metal needles to attempt to treat illnesses by stimulating points along circulation channels (\"meridians\") in accordance with beliefs related to the circulation of \"Qi\". The earliest evidence for acupuncture in this sense dates to the second or first century BCE.\nHan dynasty.\nThe \"Yellow Emperor's Inner Canon (Huangdi Neijing)\", the oldest received work of Chinese medical theory, was compiled during the Han dynasty around the first century BCE on the basis of shorter texts from different medical lineages. Written in the form of dialogues between the legendary Yellow Emperor and his ministers, it offers explanations on the relation between humans, their environment, and the cosmos, on the contents of the body, on human vitality and pathology, on the symptoms of illness, and on how to make diagnostic and therapeutic decisions in light of all these factors. Unlike earlier texts like \"Recipes for Fifty-Two Ailments\", which was excavated in the 1970s from the Mawangdui tomb that had been sealed in 168 BCE, the \"Inner Canon\" rejected the influence of spirits and the use of magic. It was also one of the first books in which the cosmological doctrines of Yinyang and the Five Phases were brought to a mature synthesis.\nThe \"Treatise on Cold Damage Disorders and Miscellaneous Illnesses (Shang Han Lun)\" was collated by Zhang Zhongjing sometime between 196 and 220 CE; at the end of the Han dynasty. Focusing on drug prescriptions rather than acupuncture, it was the first medical work to combine Yinyang and the Five Phases with drug therapy. This formulary was also the earliest public Chinese medical text to group symptoms into clinically useful \"patterns\" (\"zheng\" ) that could serve as targets for therapy. Having gone through numerous changes over time, the formulary now circulates as two distinct books: the \"Treatise on Cold Damage Disorders\" and the \"Essential Prescriptions of the Golden Casket\", which were edited separately in the eleventh century, under the Song dynasty.\nNanjing or \"Classic of Difficult Issues\", originally called \"The Yellow Emperor Eighty-one Nan Jing\", ascribed to Bian Que in the eastern Han dynasty. This book was compiled in the form of question-and-answer explanations. A total of 81 questions have been discussed. Therefore, it is also called \"Eighty-One Nan\". The book is based on basic theory and has also analyzed some disease certificates. Questions one to twenty-two is about pulse study, questions twenty-three to twenty-nine is about meridian study, questions thirty to forty-seven is related to urgent illnesses, questions forty-eight to sixty-one is related to serious diseases, questions sixty-two to sixty-eight is related to acupuncture points, and questions sixty-nine to eighty-one is related to the needlepoint methods.\nThe book is credited as developing its own path, while also inheriting the theories from Huangdi Neijing. The content includes physiology, pathology, diagnosis, treatment contents, and a more essential and specific discussion of pulse diagnosis. It has become one of the four classics for Chinese medicine practitioners to learn from and has impacted the medical development in China.\n\"Shennong Ben Cao Jing\" is one of the earliest written medical books in China. Written during the Eastern Han dynasty between 200 and 250 CE, it was the combined effort of practitioners in the Qin and Han dynasties who summarized, collected and compiled the results of pharmacological experience during their time periods. It was the first systematic summary of Chinese herbal medicine. Most of the pharmacological theories and compatibility rules and the proposed \"seven emotions and harmony\" principle have played a role in the practice of medicine for thousands of years. Therefore, it has been a textbook for medical workers in modern China. The full text of \"Shennong Ben Cao Jing\" in English can be found online.\nPost-Han dynasty.\nIn the centuries that followed, several shorter books tried to summarize or systematize the contents of the \"Yellow Emperor's Inner Canon\". The \"Canon of Problems\" (probably second century CE) tried to reconcile divergent doctrines from the \"Inner Canon\" and developed a complete medical system centered on needling therapy. The \"AB Canon of Acupuncture and Moxibustion\" (\"Zhenjiu jiayi jing\" , compiled by Huangfu Mi sometime between 256 and 282 CE) assembled a consistent body of doctrines concerning acupuncture; whereas the \"Canon of the Pulse\" (\"Maijing\" ; c. 280) presented itself as a \"comprehensive handbook of diagnostics and therapy.\"\nAround 900\u20131000 AD, Chinese were the first to develop a form of vaccination, known as variolation or inoculation, to prevent smallpox. Chinese physicians had realised that when healthy people were exposed to smallpox scab tissue, they had a smaller chance of being infected by the disease later on. The common methods of inoculation at the time was through crushing smallpox scabs into powder and breathing it through the nose.\nProminent medical scholars of the post-Han period included Tao Hongjing (456\u2013536), Sun Simiao of the Sui and Tang dynasties, Zhang Jiegu (\u20131234), and Li Shizhen (1518\u20131593). \nModern history.\nChinese communities under Colonial rule.\nChinese communities living in colonial port cities were influenced by the diverse cultures they encountered, which also led to evolving understandings of medical practices where Chinese forms of medicine were combined with Western medical knowledge. For example, the Tung Wah Hospital was established in Hong Kong in 1869 based on the widespread rejection of Western medicine for pre-existing medical practices, although Western medicine would still be practiced in the hospital alongside Chinese medicinal practices. The Tung Wah Hospital was likely connected to another Chinese medical institution, the Kwong Wai Shiu Hospital of Singapore, which had previous community links to Tung Wah, was established for similar reasons and also provided both Western and Chinese medical care. By 1935, English-language newspapers in Colonial Singapore already used the term \"Traditional Chinese Medicine\" to label Chinese ethnic medical practices.\nPeople's Republic.\nIn 1950, Chinese Communist Party (CCP) chairman Mao Zedong announced support of traditional Chinese medicine; this was despite the fact that Mao did not personally believe in and did not use TCM, according to his personal physician Li Zhisui. In 1952, the president of the Chinese Medical Association said that, \"This One Medicine, will possess a basis in modern natural sciences, will have absorbed the ancient and the new, the Chinese and the foreign, all medical achievements \u2013 and will be China's New Medicine!\"\nDuring the Cultural Revolution (1966\u20131976) the CCP and the government emphasized modernity, cultural identity and China's social and economic reconstruction and contrasted them to the colonial and feudal past. The government established a grassroots health care system as a step in the search for a new national identity and tried to revitalize traditional medicine and made large investments in traditional medicine to try to develop affordable medical care and public health facilities. The Ministry of Health directed health care throughout China and established primary care units. Chinese physicians trained in Western medicine were required to learn traditional medicine, while traditional healers received training in modern methods. This strategy aimed to integrate modern medical concepts and methods and revitalize appropriate aspects of traditional medicine. Therefore, traditional Chinese medicine was re-created in response to Western medicine.\nIn 1968, the CCP supported a new system of health care delivery for rural areas. Villages were assigned a barefoot doctor (a medical staff with basic medical skills and knowledge to deal with minor illnesses) responsible for basic medical care. The medical staff combined the values of traditional China with modern methods to provide health and medical care to poor farmers in remote rural areas. The barefoot doctors became a symbol of the Cultural Revolution, for the introduction of modern medicine into villages where traditional Chinese medicine services were used.\nThe State Intellectual Property Office (now known as CNIPA) established a database of patents granted for traditional Chinese medicine.\nIn the second decade of the twenty-first century, Chinese Communist Party general secretary Xi Jinping strongly supported TCM, calling it a \"gem\". As of May 2011, in order to promote TCM worldwide, China had signed TCM partnership agreements with over 70 countries. His government pushed to increase its use and the number of TCM-trained doctors and announced that students of TCM would no longer be required to pass examinations in Western medicine. Chinese scientists and researchers, however, expressed concern that TCM training and therapies would receive equal support with Western medicine. They also criticized a reduction in government testing and regulation of the production of TCMs, some of which were toxic. Government censors have removed Internet posts that question TCM. In 2020 Beijing drafted a local regulation outlawing criticism of TCM. According to \"Caixin\", the regulation was later passed with the provision outlawing criticism of TCM removed.\nHong Kong.\nAt the beginning of Hong Kong's opening up, Western medicine was not yet popular, and Western medicine doctors were mostly foreigners; local residents mostly relied on Chinese medicine practitioners. In 1841, the British government of Hong Kong issued an announcement pledging to govern Hong Kong residents in accordance with all the original rituals, customs and private legal property rights. As traditional Chinese medicine had always been used in China, the use of traditional Chinese medicine was not regulated.\nThe establishment in 1870 of the Tung Wah Hospital was the first use of Chinese medicine for the treatment in Chinese hospitals providing free medical services. As the promotion of Western medicine by the British government started from 1940, Western medicine started being popular among Hong Kong population. In 1959, Hong Kong had researched the use of traditional Chinese medicine to replace Western medicine.\nHistoriography of Chinese medicine.\nHistorians have noted two key aspects of Chinese medical history: understanding conceptual differences when translating the term , and observing the history from the perspective of cosmology rather than biology.\nIn Chinese classical texts, the term is the closest historical translation to the English word \"body\" because it sometimes refers to the physical human body in terms of being weighed or measured, but the term is to be understood as an \"ensemble of functions\" encompassing both the human psyche and emotions. This concept of the human body is opposed to the European duality of a separate mind and body. It is critical for scholars to understand the fundamental differences in concepts of the body in order to connect the medical theory of the classics to the \"human organism\" it is explaining.\nChinese scholars established a correlation between the cosmos and the \"human organism\". The basic components of cosmology, qi, yin yang and the Five Phase theory, were used to explain health and disease in texts such as \"Huangdi neijing\". Yin and yang are the changing factors in cosmology, with \"qi\" as the vital force or energy of life. The Five Phase theory (\"Wuxing\") of the Han dynasty contains the elements wood, fire, earth, metal, and water. By understanding medicine from a cosmology perspective, historians better understand Chinese medical and social classifications such as gender, which was defined by a domination or remission of yang in terms of yin.\nThese two distinctions are imperative when analyzing the history of traditional Chinese medical science.\nA majority of Chinese medical history written after the classical canons comes in the form of primary source case studies where academic physicians record the illness of a particular person and the healing techniques used, as well as their effectiveness. Historians have noted that Chinese scholars wrote these studies instead of \"books of prescriptions or advice manuals;\" in their historical and environmental understanding, no two illnesses were alike so the healing strategies of the practitioner was unique every time to the specific diagnosis of the patient. Medical case studies existed throughout Chinese history, but \"individually authored and published case history\" was a prominent creation of the Ming dynasty. An example such case studies would be the literati physician, Cheng Congzhou, collection of 93 cases published in 1644.\nCritique.\nHistorians of science have developed the study of medicine in traditional China into a field with its own scholarly associations, journals, graduate programs, and debates with each other. Many distinguish \"medicine in traditional China\" from the recent traditional Chinese medicine (TCM), which took elements from traditional texts and practices to construct a systematic body. Paul Unschuld, for instance, sees a \"departure of TCM from its historical origins.\" What is called \"Traditional Chinese Medicine\" and practiced today in China and the West is not thousands of years old, but recently constructed using selected traditional terms, some of which have been taken out of context, some badly misunderstood. He has criticized Chinese and Western popular books for selective use of evidence, choosing only those works or parts of historical works that seem to lead to modern medicine, ignoring those elements that do not now seem to be effective.\nCritics say that TCM theory and practice have no basis in modern science, and TCM practitioners do not agree on what diagnosis and treatments should be used for any given person. A 2007 editorial in the journal \"Nature\" wrote that TCM \"remains poorly researched and supported, and most of its treatments have no logical mechanism of action.\" It also described TCM as \"fraught with pseudoscience\". A review of the literature in 2008 found that scientists are \"still unable to find a shred of evidence\" according to standards of science-based medicine for traditional Chinese concepts such as \"qi\", meridians, and acupuncture points, and that the traditional principles of acupuncture are deeply flawed. \"Acupuncture points and meridians are not a reality\", the review continued, but \"merely the product of an ancient Chinese philosophy\". In June 2019, the World Health Organization included traditional Chinese medicine in a global diagnostic compendium, but a spokesman said this was \"not an endorsement of the scientific validity of any Traditional Medicine practice or the efficacy of any Traditional Medicine intervention.\"\nA 2012 review of cost-effectiveness research for TCM found that studies had low levels of evidence, with no beneficial outcomes. Pharmaceutical research on the potential for creating new drugs from traditional remedies has few successful results. Proponents suggest that research has so far missed key features of the art of TCM, such as unknown interactions between various ingredients and complex interactive biological systems. One of the basic tenets of TCM is that the body's \"qi\" (sometimes translated as vital energy) is circulating through channels called meridians having branches connected to bodily organs and functions. The concept of vital energy is pseudoscientific. Concepts of the body and of disease used in TCM reflect its ancient origins and its emphasis on dynamic processes over material structure, similar to Classical humoral theory.\nTCM has also been controversial within China. In 2006, the Chinese philosopher Zhang Gongyao triggered a national debate with an article entitled \"Farewell to Traditional Chinese Medicine\", arguing that TCM was a pseudoscience that should be abolished in public healthcare and academia. The Chinese government took the stance that TCM is a science and continued to encourage its development.\nThere are concerns over a number of potentially toxic plants, animal parts, and mineral Chinese compounds, as well as the facilitation of disease. Trafficked and farm-raised animals used in TCM are a source of several fatal zoonotic diseases. There are additional concerns over the illegal trade and transport of endangered species including rhinoceroses and tigers, and the welfare of specially farmed animals, including bears.\nPhilosophical background.\nTraditional Chinese medicine (TCM) is a broad range of medicine practices sharing common concepts which have been developed in China and are based on a tradition of more than 2,000 years, including various forms of herbal medicine, acupuncture, massage (), exercise (), and dietary therapy. It is primarily used as a complementary alternative medicine approach. TCM is widely used in China and it is also used in the West. Its philosophy is based on Yinyangism (i.e., the combination of Five Phases theory with Yin\u2013Yang theory), which was later absorbed by Daoism. Philosophical texts influenced TCM, mostly by being grounded in the same theories of \"qi\", \"yin-yang\" and \"wuxing\" and microcosm-macrocosm analogies.\nYin and yang.\nYin and yang are ancient Chinese deductive reasoning concepts used within Chinese medical diagnosis which can be traced back to the Shang dynasty (1600\u20131100\u00a0BCE). They represent two abstract and complementary aspects that every phenomenon in the universe can be divided into. Primordial analogies for these aspects are the sun-facing (yang) and the shady (yin) side of a hill. Two other commonly used representational allegories of yin and yang are water and fire. In the yin\u2013yang theory, detailed attributions are made regarding the yin or yang character of things:\nThe concept of yin and yang is also applicable to the human body; for example, the upper part of the body and the back are assigned to yang, while the lower part of the body is believed to have the yin character. Yin and yang characterization also extends to the various body functions, and \u2013 more importantly \u2013 to disease symptoms (e.g., cold and heat sensations are assumed to be yin and yang symptoms, respectively). Thus, yin and yang of the body are seen as phenomena whose lack (or over-abundance) comes with characteristic symptom combinations:\nTCM also identifies drugs believed to treat these specific symptom combinations, i.e., to reinforce yin and yang.\nStrict rules are identified to apply to the relationships between the Five Phases in terms of sequence, of acting on each other, of counteraction, etc. All these aspects of Five Phases theory constitute the basis of the z\u00e0ng-f\u01d4 concept, and thus have great influence regarding the TCM model of the body. Five Phase theory is also applied in diagnosis and therapy.\nCorrespondences between the body and the universe have historically not only been seen in terms of the Five Elements, but also of the \"Great Numbers\" () For example, the number of acu-points has at times been seen to be 365, corresponding with the number of days in a year; and the number of main meridians\u201312\u2013has been seen as corresponding with the number of rivers flowing through the ancient Chinese empire.\nModel of the body.\nTCM \"holds that the body's vital energy (\"chi\" or \"qi\") circulates through channels, called \"meridians\", that have branches connected to bodily organs and functions.\" Its view of the human body is only marginally concerned with anatomical structures, but focuses primarily on the body's \"functions\" (such as digestion, breathing, temperature maintenance, etc.):\nThese functions are aggregated and then associated with a primary functional entity \u2013 for instance, nourishment of the tissues and maintenance of their moisture are seen as connected functions, and the entity postulated to be responsible for these functions is xi\u011b (blood). These functional entities thus constitute \"concepts\" rather than something with biochemical or anatomical properties.\nThe primary functional entities used by traditional Chinese medicine are q\u00ec, xu\u011b, the five z\u00e0ng organs, the six f\u01d4 organs, and the meridians which extend through the organ systems. These are all theoretically interconnected: each z\u00e0ng organ is paired with a f\u01d4 organ, which are nourished by the blood and concentrate qi for a particular function, with meridians being extensions of those functional systems throughout the body.\nConcepts of the body and of disease used in TCM are pseudoscientific, similar to Mediterranean humoral theory. TCM's model of the body is characterized as full of pseudoscience. Some practitioners no longer consider yin and yang and the idea of an energy flow to apply. Scientific investigation has not found any histological or physiological evidence for traditional Chinese concepts such as \"qi\", meridians, and acupuncture points. It is a generally held belief within the acupuncture community that acupuncture points and meridians structures are special conduits for electrical signals but no research has established any consistent anatomical structure or function for either acupuncture points or meridians. The scientific evidence for the anatomical existence of either meridians or acupuncture points is not compelling. Stephen Barrett of Quackwatch writes that, \"TCM theory and practice are not based upon the body of knowledge related to health, disease, and health care that has been widely accepted by the scientific community. TCM practitioners disagree among themselves about how to diagnose patients and which treatments should go with which diagnoses. Even if they could agree, the TCM theories are so nebulous that no amount of scientific study will enable TCM to offer rational care.\"\n\"Qi\".\n\"Qi\" is a polysemous word that traditional Chinese medicine distinguishes as being able to transform into many different qualities of \"qi\" (). In a general sense, \"qi\" is something that is defined by five \"cardinal functions\":\nA lack of \"qi\" will be characterized especially by pale complexion, lassitude of spirit, lack of strength, spontaneous sweating, laziness to speak, non-digestion of food, shortness of breath (especially on exertion), and a pale and enlarged tongue.\n\"Qi\" is believed to be partially generated from food and drink, and partially from air (by breathing). Another considerable part of it is inherited from the parents and will be consumed in the course of life.\nTCM uses special terms for \"qi\" running inside of the blood vessels and for qi that is distributed in the skin, muscles, and tissues between them. The former is called \"yingqi\" (); its function is to complement xu\u00e8 and its nature has a strong yin aspect (although \"qi\" in general is considered to be yang). The latter is called \"weiqi\" (); its main function is defence and it has pronounced yang nature.\n\"Qi\" is said to circulate in the meridians. Just as the \"qi\" held by each of the zang-fu organs, this is considered to be part of the 'principal' \"qi\" of the body.\nXie.\nIn contrast to the majority of other functional entities, or (, \"blood\") is correlated with a physical form \u2013 the red liquid running in the blood vessels. Its concept is, nevertheless, defined by its functions: nourishing all parts and tissues of the body, safeguarding an adequate degree of moisture, and sustaining and soothing both consciousness and sleep.\nTypical symptoms of a lack of (usually termed \"blood vacuity\" []) are described as: Pale-white or withered-yellow complexion, dizziness, flowery vision, palpitations, insomnia, numbness of the extremities; pale tongue; \"fine\" pulse.\n\"Jinye\".\nClosely related to xu\u011b are the \"jinye\" (, usually translated as \"body fluids\"), and just like xu\u011b they are considered to be yin in nature, and defined first and foremost by the functions of nurturing and moisturizing the different structures of the body. Their other functions are to harmonize yin and yang, and to help with the secretion of waste products.\n\"Jinye\" are ultimately extracted from food and drink, and constitute the raw material for the production of xu\u011b; conversely, xu\u011b can also be transformed into \"jinye\". Their palpable manifestations are all bodily fluids: tears, sputum, saliva, gastric acid, joint fluid, sweat, urine, etc.\n\"Zangfu\".\nThe \"zangfu\" () are the collective name of eleven entities (similar to organs) that constitute the centre piece of TCM's systematization of bodily functions. The term \"zang\" refers to the five considered to be yin in nature \u2013 Heart, Liver, Spleen, Lung, Kidney \u2013 while \"fu\" refers to the six associated with yang \u2013 Small Intestine, Large Intestine, Gallbladder, Urinary Bladder, Stomach and San Jiao. Despite having the names of organs, they are only loosely tied to (rudimentary) anatomical assumptions. Instead, they are primarily understood to be certain \"functions\" of the body. To highlight the fact that they are not equivalent to anatomical organs, their names are usually capitalized.\nThe \"zang\"'s essential functions consist in production and storage of \"qi\" and xu\u011b; they are said to regulate digestion, breathing, water metabolism, the musculoskeletal system, the skin, the sense organs, aging, emotional processes, and mental activity, among other structures and processes. The f\u01d4 organs' main purpose is merely to transmit and digest () substances such as waste and food.\nSince their concept was developed on the basis of W\u01d4 X\u00edng philosophy, each z\u00e0ng is paired with a f\u01d4, and each z\u00e0ng-f\u01d4 pair is assigned to one of five elemental qualities (i.e., the Five Elements or Five Phases). These correspondences are stipulated as:\nThe z\u00e0ng-f\u01d4 are also connected to the twelve standard meridians \u2013 each yang meridian is attached to a f\u01d4 organ, and five of the yin meridians are attached to a z\u00e0ng. As there are only five z\u00e0ng but six yin meridians, the sixth is assigned to the Pericardium, a peculiar entity almost similar to the Heart z\u00e0ng.\nJing-luo.\nThe meridians (, ) are believed to be channels running from the z\u00e0ng-f\u01d4 in the interior (, ) of the body to the limbs and joints (\"the surface\" [, ]), transporting qi and xu\u0115. TCM identifies 12 \"regular\" and 8 \"extraordinary\" meridians; the Chinese terms being (, lit. \"the Twelve Vessels\") and () respectively. There's also a number of less customary channels branching from the \"regular\" meridians.\nGender in traditional medicine.\n\"Fuke\" () is the traditional Chinese term for women's medicine (it means gynecology and obstetrics in modern medicine). However, there are few or no ancient works on it except for Fu Qingzhu's \"Fu Qingzhu Nu Ke\" (Fu Qingzhu's \"Gynecology\"). In traditional China, as in many other cultures, the health and medicine of female bodies was less understood than that of male bodies. Women's bodies were often secondary to male bodies, since women were thought of as the weaker, sicklier sex.\nIn clinical encounters, women and men were treated differently. Diagnosing women was not as simple as diagnosing men. First, when a woman fell ill, an appropriate adult man was to call the doctor and remain present during the examination, for the woman could not be left alone with the doctor. The physician would discuss the female's problems and diagnosis only through the male. However, in certain cases, when a woman dealt with complications of pregnancy or birth, older women assumed the role of the formal authority. Men in these situations would not have much power to interfere. Second, women were often silent about their issues with doctors due to the societal expectation of female modesty when a male figure was in the room. Third, patriarchal society also caused doctors to call women and children patients \"the anonymous category of family members (\"Jia Ren\") or household (\"Ju Jia\")\" in their journals. This anonymity and lack of conversation between the doctor and woman patient led to the inquiry diagnosis of the Four Diagnostic Methods being the most challenging. Doctors used a medical doll known as a Doctor's lady, on which female patients could indicate the location of their symptoms.\nCheng Maoxian (b. 1581), who practiced medicine in Yangzhou, described the difficulties doctors had with the norm of female modesty. One of his case studies was that of Fan Jisuo's teenage daughter, who could not be diagnosed because she was unwilling to speak about her symptoms, since the illness involved discharge from her intimate areas. As Cheng describes, there were four standard methods of diagnosis \u2013 looking, asking, listening and smelling and touching (for pulse-taking). To maintain some form of modesty, women would often stay hidden behind curtains and screens. The doctor was allowed to touch enough of her body to complete his examination, often just the pulse taking. This would lead to situations where the symptoms and the doctor's diagnosis did not agree and the doctor would have to ask to view more of the patient.\nThese social and cultural beliefs were often barriers to learning more about female health, with women themselves often being the most formidable barrier. Women were often uncomfortable talking about their illnesses, especially in front of the male chaperones that attended medical examinations. Women would choose to omit certain symptoms as a means of upholding their chastity and honor. One such example is the case in which a teenage girl was unable to be diagnosed because she failed to mention her symptom of vaginal discharge. Silence was their way of maintaining control in these situations, but it often came at the expense of their health and the advancement of female health and medicine. This silence and control were most obviously seen when the health problem was related to the core of Ming \"fuke\", or the sexual body. It was often in these diagnostic settings that women would choose silence. In addition, there would be a conflict between patient and doctor on the probability of her diagnosis. For example, a woman who thought herself to be past the point of child-bearing age, might not believe a doctor who diagnoses her as pregnant. This only resulted in more conflict.\nYin yang and gender.\nYin and yang were critical to the understanding of women's bodies, but understood only in conjunction with male bodies. Yin and yang ruled the body, the body being a microcosm of the universe and the earth. In addition, gender in the body was understood as homologous, the two genders operating in synchronization. Gender was presumed to influence the movement of energy and a well-trained physician would be expected to read the pulse and be able to identify two dozen or more energy flows. Yin and yang concepts were applied to the feminine and masculine aspects of all bodies, implying that the differences between men and women begin at the level of this energy flow. According to \"Bequeathed Writings of Master Chu\" the male's yang pulse movement follows an ascending path in \"compliance [with cosmic direction] so that the cycle of circulation in the body and the Vital Gate are felt...The female's yin pulse movement follows a defending path against the direction of cosmic influences, so that the nadir and the Gate of Life are felt at the inch position of the left hand\". In sum, classical medicine marked yin and yang as high and low on bodies which in turn would be labeled normal or abnormal and gendered either male or female.\nBodily functions could be categorized through systems, not organs. In many drawings and diagrams, the twelve channels and their visceral systems were organized by yin and yang, an organization that was identical in female and male bodies. Female and male bodies were no different on the plane of yin and yang. Their gendered differences were not acknowledged in diagrams of the human body. Medical texts such as the \"Yuzuan yizong jinjian\" were filled with illustrations of male bodies or androgynous bodies that did not display gendered characteristics.\nAs in other cultures, fertility and menstruation dominate female health concerns. Since male and female bodies were governed by the same forces, traditional Chinese medicine did not recognize the womb as the place of reproduction. The abdominal cavity presented pathologies that were similar in both men and women, which included tumors, growths, hernias, and swellings of the genitals. The \"master system\", as Charlotte Furth calls it, is the kidney visceral system, which governed reproductive functions. Therefore, it was not the anatomical structures that allowed for pregnancy, but the difference in processes that allowed for the condition of pregnancy to occur.\nPregnancy.\nTraditional Chinese medicine's dealings with pregnancy are documented from at least the seventeenth century. According to Charlotte Furth, \"a pregnancy (in the seventeenth century) as a known bodily experience emerged [...] out of the liminality of menstrual irregularity, as uneasy digestion, and a sense of fullness\". These symptoms were common among other illness as well, so the diagnosis of pregnancy often came late in the term. The \"Canon of the Pulse\", which described the use of pulse in diagnosis, stated that pregnancy was \"a condition marked by symptoms of the disorder in one whose pulse is normal\" or \"where the pulse and symptoms do not agree\". Women were often silent about suspected pregnancy, which led to many men not knowing that their wife or daughter was pregnant until complications arrived. Complications through the misdiagnosis and the woman's reluctance to speak often led to medically induced abortions. Cheng, Furth wrote, \"was unapologetic about endangering a fetus when pregnancy risked a mother's well being\". The method of abortion was the ingestion of certain herbs and foods. Disappointment at the loss of the fetus often led to family discord.\nPostpartum.\nIf the baby and mother survived the term of the pregnancy, childbirth was then the next step. The tools provided for birth were: towels to catch the blood, a container for the placenta, a pregnancy sash to support the belly, and an infant swaddling wrap. With these tools, the baby was born, cleaned, and swaddled; however, the mother was then immediately the focus of the doctor to replenish her \"qi\". In his writings, Cheng places a large amount of emphasis on the Four Diagnostic methods to deal with postpartum issues and instructs all physicians to \"not neglect any [of the four methods]\". The process of birthing was thought to deplete a woman's blood level and \"qi\" so the most common treatments for postpartum were food (commonly garlic and ginseng), medicine, and rest. This process was followed up by a month check-in with the physician, a practice known as \"zuo yuezi\".\nInfertility.\nInfertility, not very well understood, posed serious social and cultural repercussions. The seventh-century scholar Sun Simiao is often quoted: \"those who have prescriptions for women's distinctiveness take their differences of pregnancy, childbirth and [internal] bursting injuries as their basis.\" Even in contemporary \"fuke\" placing emphasis on reproductive functions, rather than the entire health of the woman, suggests that the main function of \"fuke\" is to produce children.\nOnce again, the kidney visceral system governs the \"source \"Qi\"\", which governs the reproductive systems in both sexes. This source \"Qi\" was thought to \"be slowly depleted through sexual activity, menstruation and childbirth.\" It was also understood that the depletion of source Qi could result from the movement of an external pathology that moved through the outer visceral systems before causing more permanent damage to the home of source Qi, the kidney system. In addition, the view that only very serious ailments ended in the damage of this system means that those who had trouble with their reproductive systems or fertility were seriously ill.\nAccording to traditional Chinese medical texts, infertility can be summarized into different syndrome types. These were spleen and kidney depletion (yang depletion), liver and kidney depletion (yin depletion), blood depletion, phlegm damp, liver oppression, and damp heat. This is important because, while most other issues were complex in Chinese medical physiology, women's fertility issues were simple. Most syndrome types revolved around menstruation, or lack thereof. The patient was entrusted with recording not only the frequency, but also the \"volume, color, consistency, and odor of menstrual flow.\" This placed responsibility of symptom recording on the patient, and was compounded by the earlier discussed issue of female chastity and honor. This meant that diagnosing female infertility was difficult, because the only symptoms that were recorded and monitored by the physician were the pulse and color of the tongue.\nConcept of disease.\nIn general, disease is perceived as a disharmony (or imbalance) in the functions or interactions of yin, yang, qi, xu\u0115, z\u00e0ng-f\u01d4, meridians etc. and/or of the interaction between the human body and the environment. Therapy is based on which \"pattern of disharmony\" can be identified. Thus, \"pattern discrimination\" is the most important step in TCM diagnosis. It is also known to be the most difficult aspect of practicing TCM.\nTo determine which pattern is at hand, practitioners will examine things like the color and shape of the tongue, the relative strength of pulse-points, the smell of the breath, the quality of breathing or the sound of the voice. For example, depending on tongue and pulse conditions, a TCM practitioner might diagnose bleeding from the mouth and nose as: \"Liver fire rushes upwards and scorches the Lung, injuring the blood vessels and giving rise to reckless pouring of blood from the mouth and nose.\" He might then go on to prescribe treatments designed to clear heat or supplement the Lung.\nDisease entities.\nIn TCM, a disease has two aspects: \"b\u00ecng\" and \"zh\u00e8ng\". The former is often translated as \"disease entity\", \"disease category\", \"illness\", or simply \"diagnosis\". The latter, and more important one, is usually translated as \"pattern\" (or sometimes also as \"syndrome\"). For example, the disease entity of a common cold might present with a pattern of wind-cold in one person, and with the pattern of wind-heat in another.\nFrom a scientific point of view, most of the disease entities () listed by TCM constitute symptoms. Examples include headache, cough, abdominal pain, constipation etc.\nSince therapy will not be chosen according to the disease entity but according to the pattern, two people with the same disease entity but different patterns will receive different therapy. Vice versa, people with similar patterns might receive similar therapy even if their disease entities are different. This is called \"y\u00ec b\u00ecng t\u00f3ng zh\u00ec, t\u00f3ng b\u00ecng y\u00ec zh\u00ec\" ().\nPatterns.\nIn TCM, \"pattern\" () refers to a \"pattern of disharmony\" or \"functional disturbance\" within the functional entities of which the TCM model of the body is composed. There are disharmony patterns of qi, xu\u011b, the body fluids, the z\u00e0ng-f\u01d4, and the meridians. They are ultimately defined by their symptoms and signs (i.e., for example, pulse and tongue findings).\nIn clinical practice, the identified pattern usually involves a combination of affected entities (compare with typical examples of patterns). The concrete pattern identified should account for \"all\" the symptoms a person has.\nSix Excesses.\nThe Six Excesses (, sometimes also translated as \"Pathogenic Factors\", or \"Six Pernicious Influences\"; with the alternative term of , \u2013 \"Six Evils\" or \"Six Devils\") are allegorical terms used to describe disharmony patterns displaying certain typical symptoms. These symptoms resemble the effects of six climatic factors. In the allegory, these symptoms can occur because one or more of those climatic factors (called , \"the six qi\") were able to invade the body surface and to proceed to the interior. This is sometimes used to draw causal relationships (i.e., prior exposure to wind/cold/etc. is identified as the cause of a disease), while other authors explicitly deny a direct cause-effect relationship between weather conditions and disease, pointing out that the Six Excesses are primarily descriptions of a certain combination of symptoms translated into a pattern of disharmony. It is undisputed, though, that the Six Excesses can manifest inside the body without an external cause. In this case, they might be denoted \"internal\", e.g., \"internal wind\" or \"internal fire (or heat)\".\nThe Six Excesses and their characteristic clinical signs are:\nSix-Excesses-patterns can consist of only one or a combination of Excesses (e.g., wind-cold, wind-damp-heat). They can also transform from one into another.\nTypical examples of patterns.\nFor each of the functional entities (qi, xu\u0115, z\u00e0ng-f\u01d4, meridians etc.), typical disharmony patterns are recognized; for example: qi vacuity and qi stagnation in the case of qi; blood vacuity, blood stasis, and blood heat in the case of xu\u0115; Spleen qi vacuity, Spleen yang vacuity, Spleen qi vacuity with down-bearing qi, Spleen qi vacuity with lack of blood containment, cold-damp invasion of the Spleen, damp-heat invasion of Spleen and Stomach in case of the Spleen z\u00e0ng; wind/cold/damp invasion in the case of the meridians.\nTCM gives detailed prescriptions of these patterns regarding their typical symptoms, mostly including characteristic tongue and/or pulse findings. For example:\nEight principles of diagnosis.\nThe process of determining which actual pattern is on hand is called (, usually translated as \"pattern diagnosis\", \"pattern identification\" or \"pattern discrimination\"). Generally, the first and most important step in pattern diagnosis is an evaluation of the present signs and symptoms on the basis of the \"Eight Principles\" (). These eight principles refer to four pairs of fundamental qualities of a disease: exterior/interior, heat/cold, vacuity/repletion, and yin/yang. Out of these, heat/cold and vacuity/repletion have the biggest clinical importance. The yin/yang quality, on the other side, has the smallest importance and is somewhat seen aside from the other three pairs, since it merely presents a general and vague conclusion regarding what other qualities are found. In detail, the Eight Principles refer to the following:\nAfter the fundamental nature of a disease in terms of the Eight Principles is determined, the investigation focuses on more specific aspects. By evaluating the present signs and symptoms against the background of typical disharmony patterns of the various entities, evidence is collected whether or how specific entities are affected. This evaluation can be done\nThere are also three special pattern diagnosis systems used in case of febrile and infectious diseases only (\"Six Channel system\" or \"six division pattern\" []; \"Wei Qi Ying Xue system\" or \"four division pattern\" []; \"San Jiao system\" or \"three burners pattern\" []).\nConsiderations of disease causes.\nAlthough TCM and its concept of disease do not strongly differentiate between cause and effect, pattern discrimination can include considerations regarding the disease cause; this is called (, \"disease-cause pattern discrimination\").\nThere are three fundamental categories of disease causes () recognized:\nDiagnostics.\nIn TCM, there are five major diagnostic methods: inspection, auscultation, olfaction, inquiry, and palpation. These are grouped into what is known as the \"Four pillars\" of diagnosis, which are Inspection, Auscultation/ Olfaction, Inquiry, and Palpation ().\nTongue and pulse.\nExamination of the tongue and the pulse are among the principal diagnostic methods in TCM. Details of the tongue, including shape, size, color, texture, cracks, teeth marks, as well as tongue coating are all considered as part of tongue diagnosis. Various regions of the tongue's surface are believed to correspond to the z\u00e0ng-f\u016d organs. For example, redness on the tip of the tongue might indicate heat in the Heart, while redness on the sides of the tongue might indicate heat in the Liver.\nPulse palpation involves measuring the pulse both at a superficial and at a deep level at three different locations on the radial artery (\"Cun, Guan, Chi\", located two fingerbreadths from the wrist crease, one fingerbreadth from the wrist crease, and right at the wrist crease, respectively, usually palpated with the index, middle and ring finger) of each arm, for a total of twelve pulses, all of which are thought to correspond with certain z\u00e0ng-f\u016d. The pulse is examined for several characteristics including rhythm, strength and volume, and described with qualities like \"floating, slippery, bolstering-like, feeble, thready and quick\"; each of these qualities indicates certain disease patterns. Learning TCM pulse diagnosis can take several years.\nHerbal medicine.\nThe term \"herbal medicine\" is somewhat misleading in that, while plant elements are by far the most commonly used substances in TCM, other, non-botanic substances are used as well: animal, human, fungi, and mineral products are also used. Thus, the term \"medicinal\" (instead of herb) may be used. A 2019 review of traditional herbal treatments found they are widely used but lacking in scientific evidence, and urged a more rigorous approach by which genuinely useful medicinals might be identified.\nRaw materials.\nThere are roughly 13,000 compounds used in China and over 100,000 TCM recipes recorded in the ancient literature. Plant elements and extracts are by far the most common elements used. In the classic \"Handbook of Traditional Drugs\" from 1941, 517 drugs were listed \u2013 out of these, 45 were animal parts, and 30 were minerals.\nAnimal substances.\nSome animal parts used include cow gallstones, hornet nests, leeches, and scorpion. Other examples of animal parts include horn of the antelope or buffalo, deer antlers, testicles and penis bone of the dog, and snake bile. Some TCM textbooks still recommend preparations containing animal tissues, but there has been little research to justify the claimed clinical efficacy of many TCM animal products.\nSome compounds can include the parts of endangered species, including tiger bones and rhinoceros horn\nwhich is used for many ailments (though not as an aphrodisiac as is commonly misunderstood in the West).\nThe black market in rhinoceros horns (driven not just by TCM but also unrelated status-seeking) has reduced the world's rhino population by more than 90 percent over the past 40 years.\nConcerns have also arisen over the use of pangolin scales, turtle plastron, seahorses, and the gill plates of mobula and manta rays.\nPoachers hunt restricted or endangered species to supply the black market with TCM products. There is no scientific evidence of efficacy for tiger medicines. Concern over China considering to legalize the trade in tiger parts prompted the 171-nation Convention on International Trade in Endangered Species (CITES) to endorse a decision opposing the resurgence of trade in tigers. Fewer than 30,000 saiga antelopes remain, which are exported to China for use in traditional fever therapies. Organized gangs illegally export the horn of the antelopes to China. The pressures on seahorses (\"Hippocampus\" spp.) used in traditional medicine is enormous; tens of millions of animals are unsustainably caught annually. Many species of syngnathid are currently part of the IUCN Red List of Threatened Species or national equivalents.\nSince TCM recognizes bear bile as a treatment compound, more than 12,000 asiatic black bears are held in bear farms. The bile is extracted through a permanent hole in the abdomen leading to the gall bladder, which can cause severe pain. This can lead to bears trying to kill themselves. As of 2012, approximately 10,000 bears are farmed in China for their bile. This practice has spurred public outcry across the country. The bile is collected from live bears via a surgical procedure. As of March 2020 bear bile as ingredient of \"Tan Re Qing\" injection remains on the list of remedies recommended for treatment of \"severe cases\" of COVID-19 by National Health Commission of China and the National Administration of Traditional Chinese Medicine.\nThe deer penis is believed to have therapeutic benefits according to traditional Chinese medicine. Tiger parts from poached animals include tiger penis, believed to improve virility, and tiger eyes. The illegal trade for tiger parts in China has driven the species to near-extinction because of its popularity in traditional medicine. Laws protecting even critically endangered species such as the Sumatran tiger fail to stop the display and sale of these items in open markets. Shark fin soup is traditionally regarded in Chinese medicine as beneficial for health in East Asia, and its status as an elite dish has led to huge demand with the increase of affluence in China, devastating shark populations. The shark fins have been a part of traditional Chinese medicine for centuries. Shark finning is banned in many countries, but the trade is thriving in Hong Kong and China, where the fins are part of shark fin soup, a dish considered a delicacy, and used in some types of traditional Chinese medicine.\nThe tortoise (freshwater turtle, \"guiban\") and turtle (Chinese softshell turtle, \"biejia\") species used in traditional Chinese medicine are raised on farms, while restrictions are made on the accumulation and export of other endangered species. However, issues concerning the overexploitation of Asian turtles in China have not been completely solved. Australian scientists have developed methods to identify medicines containing DNA traces of endangered species. Finally, although not an endangered species, sharp rises in exports of donkeys and donkey hide from Africa to China to make the traditional remedy \"ejiao\" have prompted export restrictions by some African countries.\nHuman body parts.\nTraditional Chinese medicine also includes some human parts: the classic \"Materia medica\" (Bencao Gangmu) describes (also criticizes) the use of 35 human body parts and excreta in medicines, including bones, fingernail, hairs, dandruff, earwax, impurities on the teeth, feces, urine, sweat, organs, but most are no longer in use.\nHuman placenta has been used an ingredient in certain traditional Chinese medicines, including using dried human placenta, known as \"Ziheche\", to treat infertility, impotence and other conditions. The consumption of the human placenta is a potential source of infection.\nTraditional categorization.\nThe traditional categorizations and classifications that can still be found today are:\nEfficacy.\n there were not enough good-quality trials of herbal therapies to allow their effectiveness to be determined. A high percentage of relevant studies on traditional Chinese medicine are in Chinese databases. Fifty percent of systematic reviews on TCM did not search Chinese databases, which could lead to a bias in the results. Many systematic reviews of TCM interventions published in Chinese journals are incomplete, some contained errors or were misleading. The herbs recommended by traditional Chinese practitioners in the US are unregulated.\nDrug research.\nWith an eye to the enormous Chinese market, pharmaceutical companies have explored creating new drugs from traditional remedies. The journal \"Nature\" commented that \"claims made on behalf of an uncharted body of knowledge should be treated with the customary skepticism that is the bedrock of both science and medicine.\"\nThere had been success in the 1970s, however, with the development of the antimalarial drug artemisinin, which is a processed extract of \"Artemisia annua\", a herb traditionally used as a fever treatment. \"Artemisia annua\" has been used by Chinese herbalists in traditional Chinese medicines for 2,000 years. In 1596, Li Shizhen recommended tea made from qinghao specifically to treat malaria symptoms in his \"Compendium of Materia Medica\". Researcher Tu Youyou discovered that a low-temperature extraction process could isolate an effective antimalarial substance from the plant. Tu says she was influenced by a traditional Chinese herbal medicine source, \"The Handbook of Prescriptions for Emergency Treatments\", written in 340 by Ge Hong, which states that this herb should be steeped in cold water. The extracted substance, once subject to detoxification and purification processes, is a usable antimalarial drug \u2013 a 2012 review found that artemisinin-based remedies were the most effective drugs for the treatment of malaria. For her work on malaria, Tu received the 2015 Nobel Prize in Physiology or Medicine. Despite global efforts in combating malaria, it remains a large burden for the population. Although WHO recommends artemisinin-based remedies for treating uncomplicated malaria, resistance to the drug can no longer be ignored.\nAlso in the 1970s Chinese researcher Zhang TingDong and colleagues investigated the potential use of the traditionally used substance arsenic trioxide to treat acute promyelocytic leukemia (APL). Building on his work, research both in China and the West eventually led to the development of the drug Trisenox, which was approved for leukemia treatment by the FDA in 2000.\nHuperzine A, an extract from the herb, \"Huperzia serrata\", is under preliminary research as a possible therapeutic for Alzheimer's disease, but poor methodological quality of the research restricts conclusions about its effectiveness.\nEphedrine in its natural form, known as \"m\u00e1 hu\u00e1ng\" () in TCM, has been documented in China since the Han dynasty (206 BCE \u2013 220 CE) as an antiasthmatic and stimulant. In 1885, the chemical synthesis of ephedrine was first accomplished by Japanese organic chemist Nagai Nagayoshi based on his research on Japanese and Chinese traditional herbal medicines\nPien tze huang was first documented in the Ming dynasty.\nCost-effectiveness.\nA 2012 systematic review found there is a lack of available cost-effectiveness evidence in TCM.\nSafety.\nFrom the earliest records regarding the use of compounds to today, the toxicity of certain substances has been described in all Chinese materiae medicae. Since TCM has become more popular in the Western world, there are increasing concerns about the potential toxicity of many traditional Chinese plants, animal parts and minerals. Traditional Chinese herbal remedies are conveniently available from grocery stores in most Chinese neighborhoods; some of these items may contain toxic ingredients, are imported into the U.S. illegally, and are associated with claims of therapeutic benefit without evidence. For most compounds, efficacy and toxicity testing are based on traditional knowledge rather than laboratory analysis. The toxicity in some cases could be confirmed by modern research (i.e., in scorpion); in some cases it could not (i.e., in \"Curculigo\"). Traditional herbal medicines can contain extremely toxic chemicals and heavy metals, and naturally occurring toxins, which can cause illness, exacerbate pre-existing poor health or result in death. Botanical misidentification of plants can cause toxic reactions in humans. The description of some plants used in TCM has changed, leading to unintended poisoning by using the wrong plants. A concern is also contaminated herbal medicines with microorganisms and fungal toxins, including aflatoxin. Traditional herbal medicines are sometimes contaminated with toxic heavy metals, including lead, arsenic, mercury and cadmium, which inflict serious health risks to consumers. Also, adulteration of some herbal medicine preparations with conventional drugs which may cause serious adverse effects, such as corticosteroids, phenylbutazone, phenytoin, and glibenclamide, has been reported.\nSubstances known to be potentially dangerous include \"Aconitum\", secretions from the Asiatic toad, powdered centipede, the Chinese beetle (\"Mylabris phalerata\"), certain fungi, \"Aristolochia\", arsenic sulfide (realgar), mercury sulfide, and cinnabar. Asbestos ore (Actinolite, Yang Qi Shi, \u9633\u8d77\u77f3) is used to treat impotence in TCM. Due to galena's (litharge, lead(II) oxide) high lead content, it is known to be toxic. Lead, mercury, arsenic, copper, cadmium, and thallium have been detected in TCM products sold in the U.S. and China.\nTo avoid its toxic adverse effects \"Xanthium sibiricum\" must be processed. Hepatotoxicity has been reported with products containing \"Reynoutria multiflora\" (synonym \"Polygonum multiflorum\"), glycyrrhizin, \"Senecio\" and \"Symphytum\". The herbs indicated as being hepatotoxic included \"Dictamnus dasycarpus\", \"Astragalus membranaceus\", and \"Paeonia lactiflora\". Contrary to popular belief, \"Ganoderma lucidum\" mushroom extract, as an adjuvant for cancer immunotherapy, appears to have the potential for toxicity. A 2013 review suggested that although the antimalarial herb \"Artemisia annua\" may not cause hepatotoxicity, haematotoxicity, or hyperlipidemia, it should be used cautiously during pregnancy due to a potential risk of embryotoxicity at a high dose.\nHowever, many adverse reactions are due to misuse or abuse of Chinese medicine. For example, the misuse of the dietary supplement \"Ephedra\" (containing ephedrine) can lead to adverse events including gastrointestinal problems as well as sudden death from cardiomyopathy. Products adulterated with pharmaceuticals for weight loss or erectile dysfunction are one of the main concerns. Chinese herbal medicine has been a major cause of acute liver failure in China.\nThe harvesting of guano from bat caves (\"yemingsha\") brings workers into close contact with these animals, increasing the risk of zoonosis. The Chinese virologist Shi Zhengli has identified dozens of SARS-like coronaviruses in samples of bat droppings.\nAcupuncture and moxibustion.\nAcupuncture is the insertion of needles into superficial structures of the body (skin, subcutaneous tissue, muscles) \u2013 usually at acupuncture points (acupoints) \u2013 and their subsequent manipulation; this aims at influencing the flow of qi. According to TCM it relieves pain and treats (and prevents) various diseases. The US FDA classifies single-use acupuncture needles as Class II medical devices, under CFR 21.\nAcupuncture is often accompanied by moxibustion \u2013 the Chinese characters for acupuncture () literally meaning \"acupuncture-moxibustion\" \u2013 which involves burning mugwort on or near the skin at an acupuncture point. According to the American Cancer Society, \"available scientific evidence does not support claims that moxibustion is effective in preventing or treating cancer or any other disease\".\nIn electroacupuncture, an electric current is applied to the needles once they are inserted, to further stimulate the respective acupuncture points.\nA recent historian of Chinese medicine remarked that it is \"nicely ironic that the specialty of acupuncture \u2013 arguably the most questionable part of their medical heritage for most Chinese at the start of the twentieth century \u2013 has become the most marketable aspect of Chinese medicine.\" She found that acupuncture as we know it today has hardly been in existence for sixty years. Moreover, the fine, filiform needle we think of as the acupuncture needle today was not widely used a century ago. Present day acupuncture was developed in the 1930s and put into wide practice only as late as the 1960s.\nEfficacy.\nA 2013 editorial in the American journal \"Anesthesia and Analgesia\" stated that acupuncture studies produced inconsistent results, (i.e. acupuncture relieved pain in some conditions but had no effect in other very similar conditions) which suggests the presence of false positive results. These may be caused by factors like biased study design, poor blinding, and the classification of electrified needles (a type of TENS) as a form of acupuncture. The inability to find consistent results despite more than 3,000 studies, the editorial continued, suggests that the treatment seems to be a placebo effect and the existing equivocal positive results are the type of noise one expects to see after a large number of studies are performed on an inert therapy. The editorial concluded that the best controlled studies showed a clear pattern, in which the outcome does not rely upon needle location or even needle insertion, and since \"these variables are those that define acupuncture, the only sensible conclusion is that acupuncture does not work.\"\nAccording to the US NIH National Cancer Institute, a review of 17,922 patients reported that real acupuncture relieved muscle and joint pain, caused by aromatase inhibitors, much better than sham acupuncture. Regarding cancer patients, the review hypothesized that acupuncture may cause physical responses in nerve cells, the pituitary gland, and the brain \u2013 releasing proteins, hormones, and chemicals that are proposed to affect blood pressure, body temperature, immune activity, and endorphin release.\nA 2012 meta-analysis concluded that the mechanisms of acupuncture \"are clinically relevant, but that an important part of these total effects is not due to issues considered to be crucial by most acupuncturists, such as the correct location of points and depth of needling ... [but is] ... associated with more potent placebo or context effects\". Commenting on this meta-analysis, both Edzard Ernst and David Colquhoun said the results were of negligible clinical significance.\nA 2011 overview of Cochrane reviews found evidence that suggests acupuncture is effective for some but not all kinds of pain. A 2010 systematic review found that there is evidence \"that acupuncture provides a short-term clinically relevant effect when compared with a waiting list control or when acupuncture is added to another intervention\" in the treatment of chronic low back pain. Two review articles discussing the effectiveness of acupuncture, from 2008 and 2009, have concluded that there is not enough evidence to conclude that it is effective beyond the placebo effect.\nAcupuncture is generally safe when administered using Clean Needle Technique (CNT). Although serious adverse effects are rare, acupuncture is not without risk. Severe adverse effects, including very rarely death (five case reports), have been reported.\nTui na.\nTui na () is a form of massage, based on the assumptions of TCM, from which shiatsu is thought to have evolved. Techniques employed may include thumb presses, rubbing, percussion, and assisted stretching.\n\"Qigong\".\nQ\u00ecg\u014dng () is a TCM system of exercise and meditation that combines regulated breathing, slow movement, and focused awareness, purportedly to cultivate and balance qi. One branch of qigong is qigong massage, in which the practitioner combines massage techniques with awareness of the acupuncture channels and points.\n\"Qi\" is air, breath, energy, or primordial life source that is neither matter or spirit. While \"Gong\" is a skillful movement, work, or exercise of the \"qi\".\nOther therapies.\nCupping.\nCupping () is a type of Chinese massage, consisting of placing several glass \"cups\" (open spheres) on the body. A match is lit and placed inside the cup and then removed before placing the cup against the skin. As the air in the cup is heated, it expands, and after placing in the skin, cools, creating lower pressure inside the cup that allows the cup to stick to the skin via suction. When combined with massage oil, the cups can be slid around the back, offering \"reverse-pressure massage\".\nGua sha.\nGua sha () is abrading the skin with pieces of smooth jade, bone, animal tusks or horns or smooth stones; until red spots then bruising cover the area to which it is done. It is believed that this treatment is for almost any ailment. The red spots and bruising take three to ten days to heal, there is often some soreness in the area that has been treated.\nDie-da.\nDi\u0113-d\u01ce () or Dit Da, is a traditional Chinese bone-setting technique, usually practiced by martial artists who know aspects of Chinese medicine that apply to the treatment of trauma and injuries such as bone fractures, sprains, and bruises. Some of these specialists may also use or recommend other disciplines of Chinese medical therapies if serious injury is involved. Such practice of bone-setting () is not common in the West.\nChinese food therapy.\nThe concepts \"yin\" and \"yang\" are associated with different classes of foods, and tradition considers it important to consume them in a balanced fashion. However, there is no scientific evidence supporting such claims, nor their implied notions.\nRegulations.\nMany governments have enacted laws to regulate TCM practice.\nAustralia.\nFrom 1 July 2012 Chinese medicine practitioners must be registered under the national registration and accreditation scheme with the Chinese Medicine Board of Australia and meet the Board's Registration Standards, to practice in Australia.\nCanada.\nTCM is regulated in five provinces in Canada: Alberta, British Columbia, Ontario, Quebec, and Newfoundland &amp; Labrador.\nChina (mainland).\nThe National Administration of Traditional Chinese Medicine was created in 1949, which then absorbed existing TCM management in 1986 with major changes in 1998.\nChina's National People's Congress Standing Committee passed the country's first law on TCM in 2016, which came into effect on 1 July 2017. The new law standardized TCM certifications by requiring TCM practitioners to (i) pass exams administered by provincial-level TCM authorities, and (ii) obtain recommendations from two certified practitioners. TCM products and services can be advertised only with approval from the local TCM authority.\nHong Kong.\nDuring British rule, Chinese medicine practitioners in Hong Kong were not recognized as \"medical doctors\", which means they could not issue prescription drugs, give injections, etc. However, TCM practitioners could register and operate TCM as \"herbalists\". The Chinese Medicine Council of Hong Kong was established in 1999. It regulates the compounds and professional standards for TCM practitioners. All TCM practitioners in Hong Kong are required to register with the council. The eligibility for registration includes a recognised 5-year university degree of TCM, a 30-week minimum supervised clinical internship, and passing the licensing exam.\nCurrently, the approved Chinese medicine institutions are HKU, CUHK and HKBU.\nMacau.\nThe Portuguese Macau government seldom interfered in the affairs of Chinese society, including with regard to regulations on the practice of TCM. There were a few TCM pharmacies in Macau during the colonial period. In 1994, the Portuguese Macau government published Decree-Law no. 53/94/M that officially started to regulate the TCM market. After the sovereign handover, the Macau S.A.R. government also published regulations on the practice of TCM. In 2000, Macau University of Science and Technology and Nanjing University of Traditional Chinese Medicine established the Macau College of Traditional Chinese Medicine to offer a degree course in Chinese medicine.\nIn 2022, a new law regulating TCM, Law no. 11/2021, came into effect. The same law also repealed Decree-Law no. 53/94/M.\nIndonesia.\nAll traditional medicines, including TCM, are regulated by Indonesian Minister of Health Regulation of 2013 on traditional medicine. Traditional medicine license (\"Surat Izin Pengobatan Tradisional\" \u2013 SIPT) is granted to the practitioners whose methods are recognized as safe and may benefit health. The TCM clinics are registered but there is no explicit regulation for it. The only TCM method which is accepted by medical logic and is empirically proofed is acupuncture. The acupuncturists can get SIPT and participate in health care facilities.\nJapan.\nUnder modern Japanese medical law, it is possible for doctors to perform acupuncture and massage, but because there is a separate law regarding acupuncture and massage, these treatments are mainly performed by massage therapists, acupuncturists, and moxibustion practitioners.\nKorea.\nUnder the Medical Service Act (\uc758\ub8cc\ubc95/\u91ab\u7642\u6cd5), an oriental medical doctor, whose obligation is to administer oriental medical treatment and provide guidance for health based on oriental medicine, shall be treated in the same manner as a medical doctor or dentist.\nThe Korea Institute of Oriental Medicine is the top research center of TCM in Korea.\nMalaysia.\nThe Traditional and Complementary Medicine Bill was passed by parliament in 2012 establishing the Traditional and Complementary Medicine Council to register and regulate traditional and complementary medicine practitioners, including TCM practitioners as well as other traditional and complementary medicine practitioners such as those in traditional Malay medicine and traditional Indian medicine.\nNetherlands.\nThere are no specific regulations in the Netherlands on TCM; TCM is neither prohibited nor recognised by the government of the Netherlands. Chinese herbs as well as Chinese herbal products that are used in TCM are classified as foods and food supplements, and these Chinese herbs can be imported into the Netherlands as well as marketed as such without any type registration or notification to the government.\nDespite its status, some private health insurance companies reimburse a certain amount of annual costs for acupuncture treatments, this depends on one's insurance policy, as not all insurance policies cover it, and if the acupuncture practitioner is or is not a member of one of the professional organisations that are recognised by private health insurance companies.\u00a0The recognized professional organizations include the Nederlandse Vereniging voor Acupunctuur (NVA), Nederlandse Artsen Acupunctuur Vereniging (NAAV), ZHONG, (Nederlandse Vereniging voor Traditionele Chinese Geneeskunde), Nederlandse Beroepsvereniging Chinese Geneeswijzen Yi (NBCG Yi), and Wetenschappelijke Artsen Vereniging voor Acupunctuur in Nederland (WAVAN).\nNew Zealand.\nAlthough there are no regulatory standards for the practice of TCM in New Zealand, in the year 1990, acupuncture was included in the Governmental Accident Compensation Corporation (ACC) Act. This inclusion granted qualified and professionally registered acupuncturists to provide subsidised care and treatment to citizens, residents, and temporary visitors for work or sports related injuries that occurred within and upon the land of New Zealand. The two bodies for the regulation of acupuncture and attainment of ACC treatment provider status in New Zealand are Acupuncture NZ and The New Zealand Acupuncture Standards Authority.\nSingapore.\nThe TCM Practitioners Act was passed by Parliament in 2000 and the TCM Practitioners Board was established in 2001 as a statutory board under the Ministry of Health, to register and regulate TCM practitioners. The requirements for registration include possession of a diploma or degree from a TCM educational institution/university on a gazetted list, either structured TCM clinical training at an approved local TCM educational institution or foreign TCM registration together with supervised TCM clinical attachment/practice at an approved local TCM clinic, and upon meeting these requirements, passing the Singapore TCM Physicians Registration Examination (STRE) conducted by the TCM Practitioners Board.\nIn 2024, Nanyang Technological University will offer the four-year Bachelor of Chinese Medicine programme, which is the first local programme accredited by the Ministry of Health.\nTaiwan.\nIn Taiwan, TCM practitioners are physicians and are regulated by the Physicians Act. They possess the authority to independently diagnose medical conditions, issue prescriptions, dispense Traditional Chinese Medicine, and prescribe a variety of diagnostic tests including X-rays, ECG, and blood and urine test.\nUnder current law, those who wish to qualify for the Chinese medicine exam must have obtained a 7-year university degree in TCM.\nThe National Research Institute of Chinese Medicine, established in 1963, is the largest Chinese herbal medicine research center in Taiwan.\nUnited States.\nAs of July 2012, only six states lack legislation to regulate the professional practice of TCM: Alabama, Kansas, North Dakota, South Dakota, Oklahoma, and Wyoming. In 1976, California established an Acupuncture Board and became the first state licensing professional acupuncturists."}
{"id": "5993", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5993", "title": "Chemical bond", "text": "A chemical bond is the association of atoms or ions to form molecules, crystals, and other structures. The bond may result from the electrostatic force between oppositely charged ions as in ionic bonds or through the sharing of electrons as in covalent bonds, or some combination of these effects. Chemical bonds are described as having different strengths: there are \"strong bonds\" or \"primary bonds\" such as covalent, ionic and metallic bonds, and \"weak bonds\" or \"secondary bonds\" such as dipole\u2013dipole interactions, the London dispersion force, and hydrogen bonding. \nSince opposite electric charges attract, the negatively charged electrons surrounding the nucleus and the positively charged protons within a nucleus attract each other. Electrons shared between two nuclei will be attracted to both of them. \"Constructive quantum mechanical wavefunction interference\" stabilizes the paired nuclei (see Theories of chemical bonding). Bonded nuclei maintain an optimal distance (the bond distance) balancing attractive and repulsive effects explained quantitatively by quantum theory.\nThe atoms in molecules, crystals, metals and other forms of matter are held together by chemical bonds, which determine the structure and properties of matter.\nAll bonds can be described by quantum theory, but, in practice, simplified rules and other theories allow chemists to predict the strength, directionality, and polarity of bonds. The octet rule and VSEPR theory are examples. More sophisticated theories are valence bond theory, which includes orbital hybridization and resonance, and molecular orbital theory which includes the linear combination of atomic orbitals and ligand field theory. Electrostatics are used to describe bond polarities and the effects they have on chemical substances.\nOverview of main types of chemical bonds.\nA chemical bond is an attraction between atoms. This attraction may be seen as the result of different behaviors of the outermost or valence electrons of atoms. These behaviors merge into each other seamlessly in various circumstances, so that there is no clear line to be drawn between them. However it remains useful and customary to differentiate between different types of bond, which result in different properties of condensed matter.\nIn the simplest view of a covalent bond, one or more electrons (often a pair of electrons) are drawn into the space between the two atomic nuclei. Energy is released by bond formation. This is not as a result of reduction in potential energy, because the attraction of the two electrons to the two protons is offset by the electron-electron and proton-proton repulsions. Instead, the release of energy (and hence stability of the bond) arises from the reduction in kinetic energy due to the electrons being in a more spatially distributed (i.e. longer de Broglie wavelength) orbital compared with each electron being confined closer to its respective nucleus. These bonds exist between two particular identifiable atoms and have a direction in space, allowing them to be shown as single connecting lines between atoms in drawings, or modeled as sticks between spheres in models.\nIn a polar covalent bond, one or more electrons are unequally shared between two nuclei. Covalent bonds often result in the formation of small collections of better-connected atoms called molecules, which in solids and liquids are bound to other molecules by forces that are often much weaker than the covalent bonds that hold the molecules internally together. Such weak intermolecular bonds give organic molecular substances, such as waxes and oils, their soft bulk character, and their low melting points (in liquids, molecules must cease most structured or oriented contact with each other). When covalent bonds link long chains of atoms in large molecules, however (as in polymers such as nylon), or when covalent bonds extend in networks through solids that are not composed of discrete molecules (such as diamond or quartz or the silicate minerals in many types of rock) then the structures that result may be both strong and tough, at least in the direction oriented correctly with networks of covalent bonds. Also, the melting points of such covalent polymers and networks increase greatly.\nIn a simplified view of an \"ionic\" bond, the bonding electron is not shared at all, but transferred. In this type of bond, the outer atomic orbital of one atom has a vacancy which allows the addition of one or more electrons. These newly added electrons potentially occupy a lower energy-state (effectively closer to more nuclear charge) than they experience in a different atom. Thus, one nucleus offers a more tightly bound position to an electron than does another nucleus, with the result that one atom may transfer an electron to the other. This transfer causes one atom to assume a net positive charge, and the other to assume a net negative charge. The \"bond\" then results from electrostatic attraction between the positive and negatively charged ions. Ionic bonds may be seen as extreme examples of polarization in covalent bonds. Often, such bonds have no particular orientation in space, since they result from equal electrostatic attraction of each ion to all ions around them. Ionic bonds are strong (and thus ionic substances require high temperatures to melt) but also brittle, since the forces between ions are short-range and do not easily bridge cracks and fractures. This type of bond gives rise to the physical characteristics of crystals of classic mineral salts, such as table salt.\nA less often mentioned type of bonding is \"metallic\" bonding. In this type of bonding, each atom in a metal donates one or more electrons to a \"sea\" of electrons that reside between many metal atoms. In this sea, each electron is free (by virtue of its wave nature) to be associated with a great many atoms at once. The bond results because the metal atoms become somewhat positively charged due to loss of their electrons while the electrons remain attracted to many atoms, without being part of any given atom. Metallic bonding may be seen as an extreme example of delocalization of electrons over a large system of covalent bonds, in which every atom participates. This type of bonding is often very strong (resulting in the tensile strength of metals). However, metallic bonding is more collective in nature than other types, and so they allow metal crystals to more easily deform, because they are composed of atoms attracted to each other, but not in any particularly-oriented ways. This results in the malleability of metals. The cloud of electrons in metallic bonding causes the characteristically good electrical and thermal conductivity of metals, and also their shiny lustre that reflects most frequencies of white light.\nHistory.\nEarly speculations about the nature of the chemical bond, from as early as the 12th century, supposed that certain types of chemical species were joined by a type of chemical affinity. In 1704, Sir Isaac Newton famously outlined his atomic bonding theory, in \"Query 31\" of his \"Opticks\", whereby atoms attach to each other by some \"force\". Specifically, after acknowledging the various popular theories in vogue at the time, of how atoms were reasoned to attach to each other, i.e. \"hooked atoms\", \"glued together by rest\", or \"stuck together by conspiring motions\", Newton states that he would rather infer from their cohesion, that \"particles attract one another by some force, which in immediate contact is exceedingly strong, at small distances performs the chemical operations, and reaches not far from the particles with any sensible effect.\"\nIn 1819, on the heels of the invention of the voltaic pile, J\u00f6ns Jakob Berzelius developed a theory of chemical combination stressing the electronegative and electropositive characters of the combining atoms. By the mid 19th century, Edward Frankland, F.A. Kekul\u00e9, A.S. Couper, Alexander Butlerov, and Hermann Kolbe, building on the theory of radicals, developed the theory of valency, originally called \"combining power\", in which compounds were joined owing to an attraction of positive and negative poles. In 1904, Richard Abegg proposed his rule that the difference between the maximum and minimum valencies of an element is often eight. At this point, valency was still an empirical number based only on chemical properties.\nHowever the nature of the atom became clearer with Ernest Rutherford's 1911 discovery that of an atomic nucleus surrounded by electrons in which he quoted Nagaoka rejected Thomson's model on the grounds that opposite charges are impenetrable. In 1904, Nagaoka proposed an alternative planetary model of the atom in which a positively charged center is surrounded by a number of revolving electrons, in the manner of Saturn and its rings.\nNagaoka's model made two predictions:\nRutherford mentions Nagaoka's model in his 1911 paper in which the atomic nucleus is proposed.\nAt the 1911 Solvay Conference, in the discussion of what could regulate energy differences between atoms, Max Planck stated: \"The intermediaries could be the electrons.\" These nuclear models suggested that electrons determine chemical behavior.\nNext came Niels Bohr's 1913 model of a nuclear atom with electron orbits. In 1916, chemist Gilbert N. Lewis developed the concept of electron-pair bonds, in which two atoms may share one to six electrons, thus forming the single electron bond, a single bond, a double bond, or a triple bond; in Lewis's own words, \"An electron may form a part of the shell of two different atoms and cannot be said to belong to either one exclusively.\"\nAlso in 1916, Walther Kossel put forward a theory similar to Lewis' only his model assumed complete transfers of electrons between atoms, and was thus a model of ionic bonding. Both Lewis and Kossel structured their bonding models on that of Abegg's rule (1904).\nNiels Bohr also proposed a model of the chemical bond in 1913. According to his model for a diatomic molecule, the electrons of the atoms of the molecule form a rotating ring whose plane is perpendicular to the axis of the molecule and equidistant from the atomic nuclei. The dynamic equilibrium of the molecular system is achieved through the balance of forces between the forces of attraction of nuclei to the plane of the ring of electrons and the forces of mutual repulsion of the nuclei. The Bohr model of the chemical bond took into account the Coulomb repulsion \u2013 the electrons in the ring are at the maximum distance from each other.\nIn 1927, the first mathematically complete quantum description of a simple chemical bond, i.e. that produced by one electron in the hydrogen molecular ion, H2+, was derived by the Danish physicist \u00d8yvind Burrau. This work showed that the quantum approach to chemical bonds could be fundamentally and quantitatively correct, but the mathematical methods used could not be extended to molecules containing more than one electron. A more practical, albeit less quantitative, approach was put forward in the same year by Walter Heitler and Fritz London. The Heitler\u2013London method forms the basis of what is now called valence bond theory. In 1929, the linear combination of atomic orbitals molecular orbital method (LCAO) approximation was introduced by Sir John Lennard-Jones, who also suggested methods to derive electronic structures of molecules of F2 (fluorine) and O2 (oxygen) molecules, from basic quantum principles. This molecular orbital theory represented a covalent bond as an orbital formed by combining the quantum mechanical Schr\u00f6dinger atomic orbitals which had been hypothesized for electrons in single atoms. The equations for bonding electrons in multi-electron atoms could not be solved to mathematical perfection (i.e., \"analytically\"), but approximations for them still gave many good qualitative predictions and results. Most quantitative calculations in modern quantum chemistry use either valence bond or molecular orbital theory as a starting point, although a third approach, density functional theory, has become increasingly popular in recent years.\nIn 1933, H. H. James and A. S. Coolidge carried out a calculation on the dihydrogen molecule that, unlike all previous calculation which used functions only of the distance of the electron from the atomic nucleus, used functions which also explicitly added the distance between the two electrons. With up to 13 adjustable parameters they obtained a result very close to the experimental result for the dissociation energy. Later extensions have used up to 54 parameters and gave excellent agreement with experiments. This calculation convinced the scientific community that quantum theory could give agreement with experiment. However this approach has none of the physical pictures of the valence bond and molecular orbital theories and is difficult to extend to larger molecules.\nBonds in chemical formulas.\nBecause atoms and molecules are three-dimensional, it is difficult to use a single method to indicate orbitals and bonds. In molecular formulas the chemical bonds (binding orbitals) between atoms are indicated in different ways depending on the type of discussion. Sometimes, some details are neglected. For example, in organic chemistry one is sometimes concerned only with the functional group of the molecule. Thus, the molecular formula of ethanol may be written in conformational form, three-dimensional form, full two-dimensional form (indicating every bond with no three-dimensional directions), compressed two-dimensional form (CH3\u2013CH2\u2013OH), by separating the functional group from another part of the molecule (C2H5OH), or by its atomic constituents (C2H6O), according to what is discussed. Sometimes, even the non-bonding valence shell electrons (with the two-dimensional approximate directions) are marked, e.g. for elemental carbon .'C'. Some chemists may also mark the respective orbitals, e.g. the hypothetical ethene\u22124 anion (\\/C=C/\\ \u22124) indicating the possibility of bond formation.\nStrong chemical bonds.\nStrong chemical bonds are the \"intramolecular\" forces that hold atoms together in molecules. A strong chemical bond is formed from the transfer or sharing of electrons between atomic centers and relies on the electrostatic attraction between the protons in nuclei and the electrons in the orbitals.\nThe types of strong bond differ due to the difference in electronegativity of the constituent elements. Electronegativity is the tendency for an atom of a given chemical element to attract shared electrons when forming a chemical bond, where the higher the associated electronegativity then the more it attracts electrons. Electronegativity serves as a simple way to quantitatively estimate the bond energy, which characterizes a bond along the continuous scale from covalent to ionic bonding. A large difference in electronegativity leads to more polar (ionic) character in the bond.\nIonic bond.\nIonic bonding is a type of electrostatic interaction between atoms that have a large electronegativity difference. There is no precise value that distinguishes ionic from covalent bonding, but an electronegativity difference of over 1.7 is likely to be ionic while a difference of less than 1.7 is likely to be covalent. Ionic bonding leads to separate positive and negative ions. Ionic charges are commonly between \u22123e to +3e. Ionic bonding commonly occurs in metal salts such as sodium chloride (table salt). A typical feature of ionic bonds is that the species form into ionic crystals, in which no ion is specifically paired with any single other ion in a specific directional bond. Rather, each species of ion is surrounded by ions of the opposite charge, and the spacing between it and each of the oppositely charged ions near it is the same for all surrounding atoms of the same type. It is thus no longer possible to associate an ion with any specific other single ionized atom near it. This is a situation unlike that in covalent crystals, where covalent bonds between specific atoms are still discernible from the shorter distances between them, as measured via such techniques as X-ray diffraction.\nIonic crystals may contain a mixture of covalent and ionic species, as for example salts of complex acids such as sodium cyanide, NaCN. X-ray diffraction shows that in NaCN, for example, the bonds between sodium cations (Na+) and the cyanide anions (CN\u2212) are \"ionic\", with no sodium ion associated with any particular cyanide. However, the bonds between the carbon (C) and nitrogen (N) atoms in cyanide are of the \"covalent\" type, so that each carbon is strongly bound to \"just one\" nitrogen, to which it is physically much closer than it is to other carbons or nitrogens in a sodium cyanide crystal.\nWhen such crystals are melted into liquids, the ionic bonds are broken first because they are non-directional and allow the charged species to move freely. Similarly, when such salts dissolve into water, the ionic bonds are typically broken by the interaction with water but the covalent bonds continue to hold. For example, in solution, the cyanide ions, still bound together as single CN\u2212 ions, move independently through the solution, as do sodium ions, as Na+. In water, charged ions move apart because each of them are more strongly attracted to a number of water molecules than to each other. The attraction between ions and water molecules in such solutions is due to a type of weak dipole-dipole type chemical bond. In melted ionic compounds, the ions continue to be attracted to each other, but not in any ordered or crystalline way.\nCovalent bond.\nCovalent bonding is a common type of bonding in which two or more atoms share valence electrons more or less equally. The simplest and most common type is a single bond in which two atoms share two electrons. Other types include the double bond, the triple bond, one- and three-electron bonds, the three-center two-electron bond and three-center four-electron bond.\nIn non-polar covalent bonds, the electronegativity difference between the bonded atoms is small, typically 0 to 0.3. Bonds within most organic compounds are described as covalent. The figure shows methane (CH4), in which each hydrogen forms a covalent bond with the carbon. See sigma bonds and pi bonds for LCAO descriptions of such bonding.\nMolecules that are formed primarily from non-polar covalent bonds are often immiscible in water or other polar solvents, but much more soluble in non-polar solvents such as hexane.\nA polar covalent bond is a covalent bond with a significant ionic character. This means that the two shared electrons are closer to one of the atoms than the other, creating an imbalance of charge. Such bonds occur between two atoms with moderately different electronegativities and give rise to dipole\u2013dipole interactions. The electronegativity difference between the two atoms in these bonds is 0.3 to 1.7.\nSingle and multiple bonds.\nA single bond between two atoms corresponds to the sharing of one pair of electrons. The Hydrogen (H) atom has one valence electron. Two Hydrogen atoms can then form a molecule, held together by the shared pair of electrons. Each H atom now has the noble gas electron configuration of helium (He). The pair of shared electrons forms a single covalent bond. The electron density of these two bonding electrons in the region between the two atoms increases from the density of two non-interacting H atoms.\nA double bond has two shared pairs of electrons, one in a sigma bond and one in a pi bond with electron density concentrated on two opposite sides of the internuclear axis. A triple bond consists of three shared electron pairs, forming one sigma and two pi bonds. An example is nitrogen. Quadruple and higher bonds are very rare and occur only between certain transition metal atoms.\nCoordinate covalent bond (dipolar bond).\nA coordinate covalent bond is a covalent bond in which the two shared bonding electrons are from the same one of the atoms involved in the bond. For example, boron trifluoride (BF3) and ammonia (NH3) form an adduct or coordination complex F3B\u2190NH3 with a B\u2013N bond in which a lone pair of electrons on N is shared with an empty atomic orbital on B. BF3 with an empty orbital is described as an electron pair acceptor or Lewis acid, while NH3 with a lone pair that can be shared is described as an electron-pair donor or Lewis base. The electrons are shared roughly equally between the atoms in contrast to ionic bonding. Such bonding is shown by an arrow pointing to the Lewis acid. (In the Figure, solid lines are bonds in the plane of the diagram, wedged bonds point towards the observer, and dashed bonds point away from the observer.)\nTransition metal complexes are generally bound by coordinate covalent bonds. For example, the ion Ag+ reacts as a Lewis acid with two molecules of the Lewis base NH3 to form the complex ion Ag(NH3)2+, which has two Ag\u2190N coordinate covalent bonds.\nMetallic bonding.\nIn metallic bonding, bonding electrons are delocalized over a lattice of atoms. By contrast, in ionic compounds, the locations of the binding electrons and their charges are static. The free movement or delocalization of bonding electrons leads to classical metallic properties such as luster (surface light reflectivity), electrical and thermal conductivity, ductility, and high tensile strength.\nIntermolecular bonding.\nThere are several types of weak bonds that can be formed between two or more molecules which are not covalently bound. Intermolecular forces cause molecules to attract or repel each other. Often, these forces influence physical characteristics (such as the melting point) of a substance.\nVan der Waals forces are interactions between closed-shell molecules. They include both Coulombic interactions between partial charges in polar molecules, and Pauli repulsions between closed electrons shells.\nKeesom forces are the forces between the permanent dipoles of two polar molecules. London dispersion forces are the forces between induced dipoles of different molecules. There can also be an interaction between a permanent dipole in one molecule and an induced dipole in another molecule.\nHydrogen bonds of the form A--H\u2022\u2022\u2022B occur when A and B are two highly electronegative atoms (usually N, O or F) such that A forms a highly polar covalent bond with H so that H has a partial positive charge, and B has a lone pair of electrons which is attracted to this partial positive charge and forms a hydrogen bond. Hydrogen bonds are responsible for the high boiling points of water and ammonia with respect to their heavier analogues. In some cases a similar halogen bond can be formed by a halogen atom located between two electronegative atoms on different molecules.\nAt short distances, repulsive forces between atoms also become important.\nTheories of chemical bonding.\nIn the (unrealistic) limit of \"pure\" ionic bonding, electrons are perfectly localized on one of the two atoms in the bond. Such bonds can be understood by classical physics. The force between the atoms depends on isotropic continuum electrostatic potentials. The magnitude of the force is in simple proportion to the product of the two ionic charges according to Coulomb's law.\nCovalent bonds are better understood by valence bond (VB) theory or molecular orbital (MO) theory. The properties of the atoms involved can be understood using concepts such as oxidation number, formal charge, and electronegativity. The electron density within a bond is not assigned to individual atoms, but is instead delocalized between atoms. In valence bond theory, bonding is conceptualized as being built up from electron pairs that are localized and shared by two atoms via the overlap of atomic orbitals. The concepts of orbital hybridization and resonance augment this basic notion of the electron pair bond. In molecular orbital theory, bonding is viewed as being delocalized and apportioned in orbitals that extend throughout the molecule and are adapted to its symmetry properties, typically by considering linear combinations of atomic orbitals (LCAO). Valence bond theory is more chemically intuitive by being spatially localized, allowing attention to be focused on the parts of the molecule undergoing chemical change. In contrast, molecular orbitals are more \"natural\" from a quantum mechanical point of view, with orbital energies being physically significant and directly linked to experimental ionization energies from photoelectron spectroscopy. Consequently, valence bond theory and molecular orbital theory are often viewed as competing but complementary frameworks that offer different insights into chemical systems. As approaches for electronic structure theory, both MO and VB methods can give approximations to any desired level of accuracy, at least in principle. However, at lower levels, the approximations differ, and one approach may be better suited for computations involving a particular system or property than the other.\nUnlike the spherically symmetrical Coulombic forces in pure ionic bonds, covalent bonds are generally directed and anisotropic. These are often classified based on their symmetry with respect to a molecular plane as sigma bonds and pi bonds. In the general case, atoms form bonds that are intermediate between ionic and covalent, depending on the relative electronegativity of the atoms involved. Bonds of this type are known as polar covalent bonds."}
{"id": "5995", "revid": "20318", "url": "https://en.wikipedia.org/wiki?curid=5995", "title": "Cell", "text": "Cell most often refers to:\nCell may also refer to:"}
{"id": "5999", "revid": "8524693", "url": "https://en.wikipedia.org/wiki?curid=5999", "title": "Climate", "text": "Climate is the long-term weather pattern in a region, typically averaged over 30 years. More rigorously, it is the mean and variability of meteorological variables over a time spanning from months to millions of years. Some of the meteorological variables that are commonly measured are temperature, humidity, atmospheric pressure, wind, and precipitation. In a broader sense, climate is the state of the components of the climate system, including the atmosphere, hydrosphere, cryosphere, lithosphere and biosphere and the interactions between them. The climate of a location is affected by its latitude, longitude, terrain, altitude, land use and nearby water bodies and their currents.\nClimates can be classified according to the average and typical variables, most commonly temperature and precipitation. The most widely used classification scheme is the K\u00f6ppen climate classification. The Thornthwaite system, in use since 1948, incorporates evapotranspiration along with temperature and precipitation information and is used in studying biological diversity and how climate change affects it. The major classifications in Thornthwaite's climate classification are microthermal, mesothermal, and megathermal. Finally, the Bergeron and Spatial Synoptic Classification systems focus on the origin of air masses that define the climate of a region.\nPaleoclimatology is the study of ancient climates. Paleoclimatologists seek to explain climate variations for all parts of the Earth during any given geologic period, beginning with the time of the Earth's formation. Since very few direct observations of climate were available before the 19th century, paleoclimates are inferred from proxy variables. They include non-biotic evidence\u2014such as sediments found in lake beds and ice cores\u2014and biotic evidence\u2014such as tree rings and coral. Climate models are mathematical models of past, present, and future climates. Climate change may occur over long and short timescales due to various factors. Recent warming is discussed in terms of global warming, which results in redistributions of biota. For example, as climate scientist Lesley Ann Hughes has written: \"a 3\u00a0\u00b0C [5\u00a0\u00b0F] change in mean annual temperature corresponds to a shift in isotherms of approximately in latitude (in the temperate zone) or in elevation. Therefore, species are expected to move upwards in elevation or towards the poles in latitude in response to shifting climate zones.\"\nDefinition.\nClimate () is commonly defined as the weather averaged over a long period. The standard averaging period is 30\u00a0years, but other periods may be used depending on the purpose. Climate also includes statistics other than the average, such as the magnitudes of day-to-day or year-to-year variations. The Intergovernmental Panel on Climate Change (IPCC) 2001 glossary definition is as follows:\nThe World Meteorological Organization (WMO) describes \"climate normals\" as \"reference points used by climatologists to compare current climatological trends to that of the past or what is considered typical. A climate normal is defined as the arithmetic average of a climate element (e.g. temperature) over a 30-year period. A 30-year period is used as it is long enough to filter out any interannual variation or anomalies such as El Ni\u00f1o\u2013Southern Oscillation, but also short enough to be able to show longer climatic trends.\"\nThe WMO originated from the International Meteorological Organization which set up a technical commission for climatology in 1929. At its 1934 Wiesbaden meeting, the technical commission designated the thirty-year period from 1901 to 1930 as the reference time frame for climatological standard normals. In 1982, the WMO agreed to update climate normals, and these were subsequently completed on the basis of climate data from 1 January 1961 to 31 December 1990. The 1961\u20131990 climate normals serve as the baseline reference period. The next set of climate normals to be published by WMO is from 1991 to 2010. Aside from collecting from the most common atmospheric variables (air temperature, pressure, precipitation and wind), other variables such as humidity, visibility, cloud amount, solar radiation, soil temperature, pan evaporation rate, days with thunder and days with hail are also collected to measure change in climate conditions.\nThe difference between climate and weather is usefully summarized by the popular phrase \"Climate is what you expect, weather is what you get.\" Over historical time spans, there are a number of nearly constant variables that determine climate, including latitude, altitude, proportion of land to water, and proximity to oceans and mountains. All of these variables change only over periods of millions of years due to processes such as plate tectonics. Other climate determinants are more dynamic: the thermohaline circulation of the ocean leads to a 5\u00a0\u00b0C (9\u00a0\u00b0F) warming of the northern Atlantic Ocean compared to other ocean basins. Other ocean currents redistribute heat between land and water on a more regional scale. The density and type of vegetation coverage affects solar heat absorption, water retention, and rainfall on a regional level. Alterations in the quantity of atmospheric greenhouse gases (particularly carbon dioxide and methane) determines the amount of solar energy retained by the planet, leading to global warming or global cooling. The variables which determine climate are numerous and the interactions complex, but there is general agreement that the broad outlines are understood, at least insofar as the determinants of historical climate change are concerned.\nClimate classification.\nClimate classifications are systems that categorize the world's climates. A climate classification may correlate closely with a biome classification, as climate is a major influence on life in a region. One of the most used is the K\u00f6ppen climate classification scheme first developed in 1899.\nThere are several ways to classify climates into similar regimes. Originally, climes were defined in Ancient Greece to describe the weather depending upon a location's latitude. Modern climate classification methods can be broadly divided into \"genetic\" methods, which focus on the causes of climate, and \"empiric\" methods, which focus on the effects of climate. Examples of genetic classification include methods based on the relative frequency of different air mass types or locations within synoptic weather disturbances. Examples of empiric classifications include climate zones defined by plant hardiness, evapotranspiration, or more generally the K\u00f6ppen climate classification which was originally designed to identify the climates associated with certain biomes. A common shortcoming of these classification schemes is that they produce distinct boundaries between the zones they define, rather than the gradual transition of climate properties more common in nature.\nRecord.\nPaleoclimatology.\nPaleoclimatology is the study of past climate over a great period of the Earth's history. It uses evidence with different time scales (from decades to millennia) from ice sheets, tree rings, sediments, pollen, coral, and rocks to determine the past state of the climate. It demonstrates periods of stability and periods of change and can indicate whether changes follow patterns such as regular cycles.\nModern.\nDetails of the modern climate record are known through the taking of measurements from such weather instruments as thermometers, barometers, and anemometers during the past few centuries. The instruments used to study weather over the modern time scale, their observation frequency, their known error, their immediate environment, and their exposure have changed over the years, which must be considered when studying the climate of centuries past. Long-term modern climate records skew towards population centres and affluent countries. Since the 1960s, the launch of satellites allow records to be gathered on a global scale, including areas with little to no human presence, such as the Arctic region and oceans.\nClimate variability.\nClimate variability is the term to describe variations in the mean state and other characteristics of climate (such as chances or possibility of extreme weather, etc.) \"on all spatial and temporal scales beyond that of individual weather events.\" Some of the variability does not appear to be caused systematically and occurs at random times. Such variability is called \"random variability\" or \"noise\". On the other hand, periodic variability occurs relatively regularly and in distinct modes of variability or climate patterns.\nThere are close correlations between Earth's climate oscillations and astronomical factors (barycenter changes, solar variation, cosmic ray flux, cloud albedo feedback, Milankovic cycles), and modes of heat distribution between the ocean-atmosphere climate system. In some cases, current, historical and paleoclimatological natural oscillations may be masked by significant volcanic eruptions, impact events, irregularities in climate proxy data, positive feedback processes or anthropogenic emissions of substances such as greenhouse gases.\nOver the years, the definitions of \"climate variability\" and the related term \"climate change\" have shifted. While the term \"climate change\" now implies change that is both long-term and of human causation, in the 1960s the word climate change was used for what we now describe as climate variability, that is, climatic inconsistencies and anomalies.\nClimate change.\nClimate change is the variation in global or regional climates over time. It reflects changes in the variability or average state of the atmosphere over time scales ranging from decades to millions of years. These changes can be caused by processes internal to the Earth, external forces (e.g. variations in sunlight intensity) or human activities, as found recently. Scientists have identified Earth's Energy Imbalance (EEI) to be a fundamental metric of the status of global change.\nIn recent usage, especially in the context of environmental policy, the term \"climate change\" often refers only to changes in modern climate, including the rise in average surface temperature known as global warming. In some cases, the term is also used with a presumption of human causation, as in the United Nations Framework Convention on Climate Change (UNFCCC). The UNFCCC uses \"climate variability\" for non-human caused variations.\nEarth has undergone periodic climate shifts in the past, including four major ice ages. These consist of glacial periods where conditions are colder than normal, separated by interglacial periods. The accumulation of snow and ice during a glacial period increases the surface albedo, reflecting more of the Sun's energy into space and maintaining a lower atmospheric temperature. Increases in greenhouse gases, such as by volcanic activity, can increase the global temperature and produce an interglacial period. Suggested causes of ice age periods include the positions of the continents, variations in the Earth's orbit, changes in the solar output, and volcanism. However, these naturally caused changes in climate occur on a much slower time scale than the present rate of change which is caused by the emission of greenhouse gases by human activities.\nAccording to the EU's Copernicus Climate Change Service, average global air temperature has passed 1.5C of warming the period from February 2023 to January 2024.\nClimate models.\nClimate models use quantitative methods to simulate the interactions and transfer of radiative energy between the atmosphere, oceans, land surface and ice through a series of physics equations. They are used for a variety of purposes, from the study of the dynamics of the weather and climate system to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the Earth with outgoing energy as long wave (infrared) electromagnetic radiation from the Earth. Any imbalance results in a change in the average temperature of the Earth.\nClimate models are available on different resolutions ranging from &gt;100\u00a0km to 1\u00a0km. High resolutions in global climate models require significant computational resources, and so only a few global datasets exist. Global climate models can be dynamically or statistically downscaled to regional climate models to analyze impacts of climate change on a local scale. Examples are ICON or mechanistically downscaled data such as CHELSA (Climatologies at high resolution for the earth's land surface areas).\nThe most talked-about applications of these models in recent years have been their use to infer the consequences of increasing greenhouse gases in the atmosphere, primarily carbon dioxide (see greenhouse gas). These models predict an upward trend in the global mean surface temperature, with the most rapid increase in temperature being projected for the higher latitudes of the Northern Hemisphere.\nModels can range from relatively simple to quite complex. Simple radiant heat transfer models treat the Earth as a single point and average outgoing energy. This can be expanded vertically (as in radiative-convective models), or horizontally. Finally, more complex (coupled) atmosphere\u2013ocean\u2013sea ice global climate models discretise and solve the full equations for mass and energy transfer and radiant exchange."}
{"id": "6000", "revid": "82432", "url": "https://en.wikipedia.org/wiki?curid=6000", "title": "History of the Comoros", "text": "The history of the Comoros extends back to about 800\u20131000 AD when the archipelago was first inhabited. The Comoros have been inhabited by various groups and sultanates throughout this time. France colonised the islands in the 19th century, and they became independent in 1975.\nEarly inhabitants.\nThere is uncertainty about the early population of Comoros. According to one study of early crops, the islands may have been settled first by South East Asian sailors the same way Madagascar was.\nThis influx of Austronesian sailors, who had earlier settled nearby Madagascar, arrived in the 8th to 13 centuries CE. They are the source for the earliest archeological evidence of farming in the islands. Crops from archeological sites in Sima are predominantly rice strains of both \"indica\" and \"japonica\" varieties from Southeast Asia, as well as various other Asian crops like mung bean and cotton. Only a minority of the examined crops were African-derived, like finger millet, African sorghum, and cowpea. The Comoros are believed to be the first site of contact and subsequent admixture between African and Asian populations (earlier than Madagascar). Comorians today still display at most 20% Austronesian admixture.\nFrom around the 15th century AD, Shirazi slave traders established trading ports and brought in slaves from the mainland. In the 16th century, social changes on the East African coast probably linked to the arrival of the Portuguese saw the arrival of a number of Arabs of Hadrami who established alliances with the Shirazis and founded several royal clans.\nOver the centuries, the Comoros have been settled by a succession of diverse groups from the coast of Africa, the Persian Gulf, Southeast Asia and Madagascar.\nEuropeans.\nPortuguese explorers first visited the archipelago in 1505.\nApart from a visit by the French Parmentier brothers in 1529, for much of the 16th century the only Europeans to visit the islands were Portuguese. British and Dutch ships began arriving around the start of the 17th century and the island of Ndzwani soon became a major supply point on the route to the East Indies. Ndzwani was generally ruled by a single sultan, who occasionally attempted to extend his authority to Mayotte and Mwali; Ngazidja was more fragmented, on occasion being divided into as many as 12 small kingdoms.\nSir James Lancaster's voyage to the Indian Ocean in 1591 was the first attempt by the English to break into the spice trade, which was dominated by the Portuguese. Only one of his four ships made it back from the Indies on that voyage, and that one with a decimated crew of 5 men and a boy. Lancaster himself was marooned by a cyclone on the Comoros. Many of his crew were speared to death by angry islanders although Lancaster found his way home in 1594. (Dalrymple W. 2019; Bloomsbury Publishing ).\nBoth the British and the French turned their attention to the Comoros islands in the middle of the 19th century. The French finally acquired the islands through a cunning mixture of strategies, including the policy of \"divide and conquer\", chequebook politics and a serendipitous affair between a sultana and a French trader that was put to good use by the French, who kept control of the islands, quelling unrest and the occasional uprising.\nWilliam Sunley, a planter and British Consul from 1848 to 1866, was an influence on Anjouan.\nFrench Comoros.\nFrance's presence in the western Indian Ocean dates to the early 17th century. The French established a settlement in southern Madagascar in 1634 and occupied the islands of R\u00e9union and Rodrigues; in 1715 France claimed Mauritius (), and in 1756 Seychelles. When France ceded Mauritius, Rodrigues, and Seychelles to Britain in 1814, it lost its Indian Ocean ports; Reunion, which remained French, did not offer a suitable natural harbor. In 1840 France acquired the island of Nosy-Be off the northwestern coast of Madagascar, but its potential as a port was limited. In 1841 the governor of Reunion, Admiral de Hell, negotiated with Andrian Souli, the Malagasy ruler of Mayotte, to cede Mayotte to France. Mahore offered a suitable site for port facilities, and its acquisition was justified by de Hell on the grounds that if France did not act, Britain would occupy the island.\nAlthough France had established a foothold in Comoros, the acquisition of the other islands proceeded fitfully. At times the French were spurred on by the threat of British intervention, especially on Nzwani, and at other times, by the constant anarchy resulting from the sultans' wars upon each other. In the 1880s, Germany's growing influence on the East African coast added to the concerns of the French. Not until 1908, however, did the four Comoro Islands become part of France's colony of Madagascar and not until 1912 did the last sultan abdicate. Then, a colonial administration took over the islands and established a capital at Dzaoudzi on Mahore. Treaties of protectorate status marked a transition point between independence and annexation; such treaties were signed with the rulers of Njazidja, Nzwani, and Mwali in 1886.\nThe effects of French colonialism were mixed, at best. Colonial rule brought an end to the institution of Slavery in the Comoros, but economic and social differences between former slaves and free persons and their descendants persisted. Health standards improved with the introduction of modern medicine, and the population increased about 50 percent between 1900 and 1960. France continued to dominate the economy. Food crop cultivation was neglected as French (companies) established cash crop plantations in the coastal regions. The result was an economy dependent on the exporting of vanilla, ylang-ylang, cloves, cocoa, copra, and other tropical crops. Most profits obtained from exports were diverted to France \nrather than invested in the infrastructure of the islands. Development was further limited by the colonial government's practice of concentrating public services on Madagascar. One consequence of this policy was the migration of large numbers of Comorans to Madagascar, where their presence would be a long-term source of tension between Comoros and its giant island neighbor. The Shirazi elite continued to play a prominent role as large landowners and civil servants. On the eve of independence, Comoros remained poor and undeveloped, \nhaving only one secondary school and practically nothing in the way of national media. Isolated from important trade routes by the opening of the Suez Canal in 1869, having few natural resources, and largely neglected by France, the islands were poorly equipped for independence.\nOn September 25, 1942, British forces landed in the Comoros, occupying them until October 13, 1946.\nIn 1946 the Comoro Islands became an overseas department of France with representation in the French National Assembly. The following year, the islands' administrative ties to Madagascar were severed; Comoros established its own customs regime in 1952. A Governing Council was elected in August 1957 on the four islands in conformity with the loi-cadre (enabling law) of June 23, 1956. A constitution providing for internal self-government was promulgated in 1961, following a 1958 referendum in which Comorans voted overwhelmingly to remain a part of France. This government consisted of a territorial assembly having, in 1975, thirty-nine members, and a Governing Council of six to nine ministers responsible to it.\nAgreement was reached with France in 1973 for the Comoros to become independent in 1978. On July 6, 1975, however, the Comorian parliament passed a resolution declaring unilateral independence as a republic. The deputies of Mayotte abstained. The first president of the Comoros, Ahmed Abdallah Abderemane, did not last long before being ousted in a coup d'\u00e9tat by Ali Soilih, an atheist with an Islamic background.\nSoilih began with a set of solid socialist ideals designed to modernize the country. However, the regime faced problems. A French mercenary by the name of Bob Denard, arrived in the Comoros at dawn on 13 May 1978, and removed Soilih from power. Solih was shot and killed during the coup. The mercenaries returned Abdallah to power and the mercenaries were given key positions in government.\nIn two referendums, in December 1974 and February 1976, the population of Mayotte voted against independence from France (by 63.8% and 99.4% respectively). Mayotte thus remains under French administration, and the Comorian Government has effective control over only Grande Comore, Anjouan, and Moh\u00e9li.\nLater, French settlers, French-owned companies, and Arab merchants established a plantation-based economy that now uses about one-third of the land for export crops.\nAbdallah regime.\nIn 1978, president Ali Soilih, who had a firm anti-French line, was killed and Ahmed Abdallah came to power. Under the reign of Abdallah, Denard was commander of the Presidential Guard (PG) and \"de facto\" ruler of the country. He was trained, supported and funded by the white regimes in South Africa (SA) and Rhodesia (now Zimbabwe) in return for permission to set up a secret listening post on the islands. South-African agents kept an ear on the important ANC bases in Lusaka and Dar es Salaam and watched the war in Mozambique, in which SA played an active role. The Comoros were also used for the evasion of arms sanctions.\nWhen in 1981 Fran\u00e7ois Mitterrand was elected president Denard lost the support of the French intelligence service, but he managed to strengthen the link between SA and the Comoros. Besides the military, Denard established his own company SOGECOM, for both the security and construction, and seemed to profit by the arrangement. Between 1985 and 1987 the relationship of the PG with the local Comorians became worse.\nAt the end of the 1980s the South Africans did not wish to continue to support the mercenary regime and France was in agreement. Also President Abdallah wanted the mercenaries to leave. Their response was a (third) coup resulting in the death of President Abdallah, in which Denard and his men were probably involved. South Africa and the French government subsequently forced Denard and his mercenaries to leave the islands in 1989.\n1989\u20131996.\nSaid Mohamed Djohar became president. His time in office was turbulent, including an impeachment attempt in 1991 and a coup attempt in 1992.\nOn September 28, 1995 Bob Denard and a group of mercenaries took over the Comoros islands in a coup (named operation Kaskari by the mercenaries) against President Djohar. France immediately and severely denounced the coup, and backed by the 1978 defense agreement with the Comoros, President Jacques Chirac ordered his special forces to retake the island. Bob Denard began to take measures to stop the coming invasion. A new presidential guard was created. Strong points armed with heavy machine guns were set up around the island, particularly around the island's two airports.\nOn October 3, 1995, 11 p.m., the French deployed 600 men against a force of 33 mercenaries and a 300-man dissident force. Denard however ordered his mercenaries not to fight. Within 7 hours the airports at Iconi and Hahaya and the French Embassy in Moroni were secured. By 3:00\u00a0p.m. the next day Bob Denard and his mercenaries had surrendered. This (response) operation, codenamed \"Azal\u00e9e\", was remarkable, because there were no casualties, and just in seven days, plans were drawn up and soldiers were deployed. Denard was taken to France and jailed. Prime minister Caambi El-Yachourtu became acting president until Djohar returned from exile in January, 1996. In March 1996, following presidential elections, Mohamed Taki Abdoulkarim, a member of the civilian government that Denard had tried to set up in October 1995, became president. On 23 November 1996, Ethiopian Airlines Flight 961 crashed near a beach on the island after it was hijacked and ran out of fuel killing 125 people and leaving 50 survivors.\nSecession of Anjouan and Moh\u00e9li.\nIn 1997, the islands of Anjouan and Moh\u00e9li declared their independence from the Comoros. A subsequent attempt by the government to re-establish control over the rebellious islands by force failed, and presently the African Union is brokering negotiations to effect a reconciliation. This process is largely complete, at least in theory. According to some sources, Moh\u00e9li did return to government control in 1998. In 1999, Anjouan had internal conflicts and on August 1 of that year, the 80-year-old first president Foundi Abdallah Ibrahim resigned, transferring power to a national coordinator, Said Abeid. The government was overthrown in a coup by army and navy officers on August 9, 2001. Mohamed Bacar soon rose to leadership of the junta that took over and by the end of the month he was the leader of the country. Despite two coup attempts in the following three months, including one by Abeid, Bacar's government remained in power, and was apparently more willing to negotiate with the Comoros. Presidential elections were held for all of the Comoros in 2002, and presidents have been chosen for all three islands as well, which have become a confederation. Most notably, Mohammed Bacar was elected for a 5-year term as president of Anjouan. Grande Comore had experienced troubles of its own in the late 1990s, when President Taki died on November 6, 1998. Colonel Azali Assoumani became president following a military coup in 1999. There have been several coup attempts since, but he gained firm control of the country after stepping down temporarily and winning a presidential election in 2002.\nIn May 2006, Ahmed Abdallah Sambi was elected from the island of Anjouan to be the president of the Union of the Comoros. He is a Sunni cleric who studied in the Sudan, Iran and Saudi Arabia. He is nicknamed \"Ayatollah\" due to his time in Iran and his penchant for turbans.\nAzali Assoumani in power since 2016.\nAzali Assoumani is a former army officer, first came to power in a coup in 1999. Then he won presidency in 2002 election, having power until 2006. After ten years, he was elected again in 2016 election. In March 2019, he was re-elected in the elections opposition claimed to be full of irregularities.\nBefore the 2019 election president Azali Assoumani had arranged a constitutional referendum in 2018 that approved extending the presidential mandate from one five-year term to two. The opposition had boycotted the referendum.\nIn January 2020, his party The Convention for the Renewal of the Comoros (CRC) won 20 out of 24 parliamentary seats in the parliamentary election.\nOn 18 February 2023 the Comoros assumed the presidency of the African Union. In January 2024, President Azali Assoumani was re-elected with 63% of the vote in the disputed presidential election.\nReferences.\nAttribution:"}
{"id": "6001", "revid": "379243", "url": "https://en.wikipedia.org/wiki?curid=6001", "title": "Geography of the Comoros", "text": " \nThe Comoros archipelago consists of four main islands aligned along a northwest\u2013southeast axis at the north end of the Mozambique Channel, between Mozambique and the island of Madagascar. Still widely known by their French names, the islands officially have been called by their Swahili names by the Comorian government. They are Grande Comore (Njazidja), Moh\u00e9li (Mwali), Anjouan (Nzwani), and Mayotte (Mahor\u00e9). The islands' distance from each other\u2014Grande Comore is some 200 kilometers from Mayotte, forty kilometers from Moh\u00e9li, and eighty kilometers from Anjouan\u2014along with a lack of good harbor facilities, make transportation and communication difficult. Comoros are sunny islands.\nDetails.\nThe islands have a total land area of 2,236 square kilometers (including Mayotte), and claim territorial waters of 320 square kilometers. Mount Karthala (2316 m) on Grande Comore is an active volcano. From April 17 to 19, 2005, the volcano began spewing ash and gas, forcing as many as 10,000 people to flee. Comoros is located within the Somali Plate.\nGrande Comore.\nGrande Comore is the largest island, sixty-seven kilometers long and twenty-seven kilometers wide, with a total area of 1,146 square kilometers. The most recently formed of the four islands in the archipelago, it is also of volcanic origin. Two volcanoes form the island's most prominent topographic features: La Grille in the north, with an elevation of 1,000 meters, is extinct and largely eroded; Kartala in the south, rising to a height of 2,361 meters, last erupted in 1977. A plateau averaging 600 to 700 meters high connects the two mountains. Because Grande Comore is geologically a relatively new island, its soil is thin and rocky and cannot hold water. As a result, water from the island's heavy rainfall must be stored in catchment tanks. There are no coral reefs along the coast, and the island lacks a good harbor for ships. One of the largest remnants of the Comoros' once-extensive rain forests is on the slopes of Kartala. The national capital has been at Moroni since 1962.\nAnjouan.\nAnjouan, triangular shaped and forty kilometers from apex to base, has an area of 424 square kilometers. Three mountain chains \u2014 Sima, Nioumakele, and Jimilime\u2014emanate from a central peak, Mtingui (1,575 m), giving the island its distinctive shape. Older than Grande Comore, Anjouan has deeper soil cover, but overcultivation has caused serious erosion. A coral reef lies close to shore; the island's capital of Mutsamudu is also its main port.\nMoh\u00e9li.\nMoh\u00e9li is thirty kilometers long and twelve kilometers wide, with an area of 290 square kilometers. It is the smallest of the four islands and has a central mountain chain reaching 860 meters at its highest. Like Grande Comore, it retains stands of rain forest. Moh\u00e9li's capital is Fomboni.\nMayotte.\nMayotte, geologically the oldest of the four islands, is thirty-nine kilometers long and twenty-two kilometers wide, totaling 375 square kilometers, and its highest points are between 500 and 600 meters above sea level. Because of greater weathering of the volcanic rock, the soil is relatively rich in some areas. A well-developed coral reef that encircles much of the island ensures protection for ships and a habitat for fish. Dzaoudzi, capital of the Comoros until 1962 and now Mayotte's administrative center, is situated on a rocky outcropping off the east shore of the main island. Dzaoudzi is linked by a causeway to le Pamanzi, which at ten kilometers in area is the largest of several islets adjacent to Mayotte. Islets are also scattered in the coastal waters of Mayotte just as in Grande Comore, Anjouan, and Moh\u00e9li.\nFlora and fauna.\nComorian waters are the habitat of the coelacanth, a rare fish with limblike fins and a cartilaginous skeleton, the fossil remains of which date as far back as 400 million years and which was once thought to have become extinct about 70 million years ago. A live specimen was caught in 1938 off southern Africa; other coelacanths have since been found in the vicinity of the Comoro Islands.\nSeveral mammals are unique to the islands themselves. Livingstone's fruit bat, although plentiful when discovered by explorer David Livingstone in 1863, has been reduced to a population of about 120, entirely on Anjouan. The world's largest bat, the jet-black Livingstone fruit bat has a wingspan of nearly two meters. A British preservation group sent an expedition to the Comoros in 1992 to bring some of the bats to Britain to establish a breeding population.\nA hybrid of the common brown lemur (\"Eulemur fulvus\") originally from Madagascar, was introduced by humans prior to European colonization and is found on Mayotte. The mongoose lemur (\"Eulemur mongoz\"), also introduced from Madagascar by humans, can be found on the islands of Moh\u00e9li and Anjouan.\n22 species of bird are unique to the archipelago and 17 of these are restricted to the Union of the Comoros. These include the Karthala scops-owl, Anjouan scops-owl and Humblot's flycatcher.\nPartly in response to international pressures, Comorians in the 1990s have become more concerned about the environment. Steps are being taken not only to preserve the rare fauna, but also to counteract degradation of the environment, especially on densely populated Anjouan. Specifically, to minimize the cutting down of trees for fuel, kerosene is being subsidized, and efforts are being made to replace the loss of the forest cover caused by ylang-ylang distillation for perfume. The Community Development Support Fund, sponsored by the International Development Association (IDA, a World Bank affiliate) and the Comorian government, is working to improve water supply on the islands as well.\nClimate.\nThe climate is marine tropical, with two seasons: hot and humid from November to April, the result of the northeastern monsoon, and a cooler, drier season the rest of the year. Average monthly temperatures range from along the coasts. Although the average annual precipitation is , water is a scarce commodity in many parts of the Comoros. Moh\u00e9li and Mayotte possess streams and other natural sources of water, but Grande Comore and Anjouan, whose mountainous landscapes retain water poorly, are almost devoid of naturally occurring running water. Cyclones, occurring during the hot and wet season, can cause extensive damage, especially in coastal areas. On the average, at least twice each decade houses, farms, and harbor facilities are devastated by these great storms.\nTropical cyclones.\nDue to their low latitude, the islands are rarely affected by tropical cyclones. However, several cyclones have had damaging and deadly effects. Cyclones in December 1905 and again in December 1906 led to a famine that killed 490\u00a0people between August 1905 and January 1906. A tropical cyclone in 1950 killed 585\u00a0people while moving through Anjouan and Moheli, injuring 70,000\u00a0others. The cyclone left 40,000\u00a0people homeless, and also caused \u20a33.5\u00a0worth of damage to crops and infrastructure. \nExtreme points.\nThis is a list of the extreme points of the Comoros, the points that are farther north, south, east or west than any other location. This list excludes the French-administered island of Mayotte which is claimed by the Comorian government.\nStatistics.\nArea:\n2,235\u00a0km2\nCoastline:\n340\u00a0km\nClimate:\ntropical marine; rainy season (November to May)\nTerrain:\nvolcanic islands, interiors vary from steep mountains to low hills\nElevation extremes:\n\"lowest point:\"\nIndian Ocean 0 m\n\"highest point:\"\nKarthala 2,360 m\nNatural resources:\nfish\nLand use:\n\"arable land:\"\n47.29%\n\"permanent crops:\"\n29.55%\n\"other:\"\n23.16% (2012 est.)\nIrrigated land:\n1.3\u00a0km2 (2003)\nTotal renewable water resources:\n1.2\u00a0km3 (2011)\nFreshwater withdrawal (domestic/industrial/agricultural):\n\"total:\"\n0.01\u00a0km3/yr (48%/5%/47%)\n\"per capital:\"\n16.86 m3/yr (1999)\nNatural hazards:\ncyclones possible during rainy season (December to April); volcanic activity on Grand Comore\nEnvironmental - current issues:\nsoil degradation and erosion results from crop cultivation on slopes without proper terracing; deforestation"}
{"id": "6002", "revid": "11334803", "url": "https://en.wikipedia.org/wiki?curid=6002", "title": "Demographics of the Comoros", "text": "The Comorians () inhabiting Grande Comore, Anjouan, and Moh\u00e9li (86% of the population) share African-Arab origins. Islam is the dominant religion, and Quranic schools for children reinforce its influence. Although Islamic culture is firmly established throughout, a small minority are Christian.\nThe most common language is Comorian, related to Swahili. French and Arabic also are spoken. About 89% of the population is literate.\nThe Comoros have had eight censuses since World War II:\nThe latest official estimate (for 1 July 2020) is 897,219.\nPopulation density figures conceal a great disparity between the republic's most crowded island, Nzwani, which had a density of 772 persons per square kilometer in 2017; Njazidja, which had a density of 331 persons per square kilometer in 2017; and Mwali, where the 2017 population density figure was 178 persons per square kilometer. \nBy comparison, estimates of the population density per square kilometer of the Indian Ocean's other island microstates ranged from 241 (Seychelles) to 690 (Maldives) in 1993. Given the rugged terrain of Njazidja and Nzwani, and the dedication of extensive tracts to agriculture on all three islands, population pressures on the Comoros are becoming increasingly critical.\nThe age structure of the population of the Comoros is similar to that of many developing countries, in that the republic has a very large proportion of young people. In 1989, 46.4 percent of the population was under fifteen years of age, an above-average proportion even for sub-Saharan Africa. The population's rate of growth was a relatively high 3.5 percent per annum in the mid-1980s, up substantially from 2.0 percent in the mid-1970s and 2.1 percent in the mid-1960s.\nIn 1983 the Abdallah regime borrowed US$2.85 million from the International Development Association to devise a national family planning program. However, Islamic reservations about contraception made forthright advocacy and implementation of birth control programs politically hazardous, and consequently little was done in the way of public policy.\nThe Comorian population has become increasingly urbanized in recent years. In 1991 the percentage of Comorians residing in cities and towns of more than 5,000 persons was about 30 percent, up from 25 percent in 1985 and 23 percent in 1980. The Comoros' largest cities were the capital, Moroni, with about 30,000 people, and the port city of Mutsamudu, on the island of Nzwani, with about 20,000 people.\nMigration among the various islands is important. Natives of Nzwani have settled in significant numbers on less crowded Mwali, causing some social tensions, and many Nzwani also migrate to Maore. In 1977 Maore expelled peasants from Ngazidja and Nzwani who had recently settled in large numbers on the island. Some were allowed to reenter starting in 1981 but solely as migrant labor.\nThe number of Comorians living abroad has been estimated at between 80,000 and 100,000; during the colonial period, most of them lived in Tanzania, Madagascar, and other parts of Southeast Africa. The number of Comorians residing in Madagascar was drastically reduced after anti-Comorian rioting in December 1976 in Mahajanga, in which at least 1,400 Comorians were killed. As many as 17,000 Comorians left Madagascar to seek refuge in their native land in 1977 alone. About 100,000 Comorians live in France; many of them had gone there for a university education and never returned. Small numbers of Indians, Malagasy, South Africans, and Europeans (mostly French) live on the islands and play an important role in the economy. Most French left after independence in 1975.\nSome Persian Gulf countries started buying Comorian citizenship for their stateless Bedoon residents and deporting them to Comoros.\nVital statistics.\nStatistics :\nDemographic and Health Surveys.\nTotal Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):\nStructure of the population (DHS 2012) (Males 11 088, Females 12 284 = 23 373) :\nFertility data as of 2012 (DHS Program):\nReligion.\nSunni Muslim 98%, other (including Shia Muslim, Roman Catholic, Jehovah's Witness, Protestant) 2%\nnote: Sunni Islam is the state religion\nReferences.\nAttribution:"}
{"id": "6003", "revid": "43944491", "url": "https://en.wikipedia.org/wiki?curid=6003", "title": "Politics of the Comoros", "text": "The Politics of the Union of the Comoros take place in a framework of a unitary presidential republic, whereby the President of the Comoros is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and parliament. The precolonial legacies of the sultanates linger while the political situation in Comoros has been extremely fluid since the country's independence in 1975, subject to the volatility of coups and political insurrection.\nAs of 2008, Comoros and Mauritania were considered by US-based organization Freedom House as the only real \u201celectoral democracies\u201d of the Arab World.\nPrecolonial and colonial political structures.\nSultanates in the late nineteenth century used a cyclic age system and hierarchical lineage membership to provide the foundation for participation in the political process. In the capital, \"the sultan was assisted by his ministers and by a madjelis, an advisory council composed of elders, whom he consulted regularly\". Apart from local administration, the age system was used to include the population in decision making, depending on the scope of the decision being made. For example, the elders of the island of Njazidja held considerable influence on the authority of the sultan. Though sultanates granted rights to their free inhabitants, were provided with warriors during war and taxed the towns under their authority, their definition as a state is open to debate. The islands' incorporation as a province of the colony of Madagascar into the French colonial empire marked the end of the sultanates.\nDespite French colonization, Comorans identify first with kinship or regional ties and rarely ever with the central government. This is a lingering effect of the sovereign sultanates of pre-colonial times. French colonial administration was based on a misconception that the sultanates operated as absolute monarchs: district boundaries were the same as the sultanates', multiple new taxes forced men into wage labor on colonial plantations and was reinforced through a compulsory public labor system that had little effect on infrastructure. French policy was hampered by an absence of settlers, effective communication across islands, rough geographical terrain and hostility towards the colonial government. Policies were made to apply to Madagascar as a whole and seldom to the nuances of each province: civil servants were typically Christian, unaware of local customs and unable to speak the local language. The French established the Ouatou Akouba in 1915, a local form of governance based on \"customary structures\" already in place that attempted to model itself after the age system in place under the sultanates. Their understanding of the elders' council as a corporate group bypassed the reality that there were men \"who had accomplished the necessary customary rituals to be accorded the status of elder and thus be eligible to participate in the political process in the village\", which effectively rendered the French elders' council ineffective. Though the Ouatou Akouba was disbanded, it resulted in the consolidation and formalization of the age system as access to power in the customary and local government spheres. The French failure to establish a functioning state in the Comoros has had repercussions in the post-independence era.\nPost-independence.\nAt independence there were five main political parties: OUDZIMA, UMMA, the Comoro People's Democratic Rally, the Comoro National Liberation Movement and the Socialist Objective Party. The political groups previously known simply as the 'green' and 'white' party became the Rassemblement D\u00e9mocratique du Peuple Comorien (RDPC) and the Union D\u00e9mocratique des Comores (UDC), headed by Sayyid Muhammad Cheikh and Sayyid Ibrahim. Members from both parties later merged to form OUDZIMA under the leadership of first president Ahmad Abdallah while dissidents from both created UMMA under the leadership of future president Ali Soilih.\nPrince Said Ibrahim took power in 1970 but was democratically elected out of office in 1972 in favor of former French senator Ahmed Abdallah. President Abdallah declared independence for all islands, except Mayotte which remained under French administration, in 1975. The threat of renewed socioeconomic marginalization following the transfer of the capital to Ngazidja in 1962, more than social or cultural differences, underlay the island's subsequent rejection of independence. France withdrew all economic and technical support for the now independent state, which would encourage a revolutionary regime under future president Ali Soilih. French military and financial aid to mercenaries brought Prince Said Mohammed Jaffar to power after the United National Front of the Comoros (FNU) party toppled Abdallah's government. This mercenary coup was unique in that, unlike other coups on the continent, it was \"uninspired by any ideological convictions\". The Jaffar regime's inefficient distribution of resources and poor mismanagement was shown through the expulsion of French civil servants as well as endemic unemployment and food shortages. The regime used famine as \"an opportunity to switch food patronage from France to the World Food Programme's emergency aid\".\nPresident Jaffar's ousting by Minister of Defense and Justice, Ali Soilih, brought about the \"periode noire\" (dark period) of the country; you could vote at 14, most civil servants were dismissed and there was a ban on some Islamic customs. He implemented revolutionary social reforms such as replacing French with Shikomoro, burning down the national archives and nationalizing land. His government received support from Egypt, Iraq and Sudan. Soilih's attacks on religious and customary authority contributed to his eventual ousting through a French-backed coup consisting of mercenaries and ex-politicians who together formed the Politico Military Doctorate.\nAbdallah was reinstated and constructed a mercantile state by resuscitating the structures of the colonial era. His establishment of a one party state and intolerance for dissent further alienated civil society from the state. In May 1978 the Comoros were renamed the Islamic Republic of the Comoros and continued strengthening ties with the Arab world which resulted in their joining the Arab League. Abdallah's government sought to reverse Soilih's 'de-sacralization' by re-introducing the grand marriage, declaring Arabic the second official language behind French, and creating the office of the Grand Mufti. The doctorate &amp; compromise government was dissolved, constitutional changes removed succession from a politician and neutralized the post of another possible challenger in abolishing the position of Prime Minister, which effectively cemented a client-patron network by making the civil service position dependent on Abdallah's political base. The Democratic Front's (DF) internal opposition to Abdallah was suppressed through the incarceration of over 600 people allegedly involved in a failed coup attempt. Abdallah then stocked the House of Assembly with loyal clientelist supporters through rigged parliamentary elections. All of these actions effectively consolidated Abdallah's position.\nMuhammed Djohar succeeded president Abdallah after his assassination in 1989 but was evacuated by French troops after a failed coup attempt in 1996. The Comoros were led by Muhammed Taki Abd al-Karim beginning in 1996 and he was followed by interim president Said Massunde who eventually gave way to Assoumani Azali. Taki's lack of Arab heritage led to his lack of understanding Nzwani's cultural differences and economic problems, as seen by the establishment of the elders council with only loyal Taki supporters. As a result, the\u00a0council was ignored by the true elders of the island. After Taki's death, a military coup in 1999, the nation's eighteenth since independence in 1975, installed Azali in to power. Colonel Azali Assoumani seized power in a bloodless coup in April 1999, overthrowing Interim President Tadjidine Ben Said Massounde, who himself had held the office since the death of democratically elected President Mohamed Taki Abdoulkarim in November, 1998. In May 1999, Azali decreed a constitution that gave him both executive and legislative powers. Bowing somewhat to international criticism, Azali appointed a civilian Prime Minister, Bainrifi Tarmidi, in December 1999; however, Azali retained the mantle of Head of State and army Commander. In December 2000, Azali named a new civilian Prime Minister, Hamada Madi, and formed a new civilian Cabinet. When Azali took power he also pledged to step down in April 2000 and relinquish control to a democratically elected president\u2014a pledge with mixed results. Under Mohammed Taki and Assoumani Azali, access to the state was used to support client networks which led to crumbling infrastructure that cultivated in the islands of Nzwani and Mwali declaring independence only\u00a0to be stopped by French troops. Azali lacked the social obligations required to address the elders and when combined with his gross mismanagement and increasing economic and social dependence on foreign entities, made managing daily life near nonexistent in the state. Therefore, local administrative structures began popping up and drifting away from reliance on the state, funded by remittances from the expatriate community in France.\nAzali Assoumani is a former army officer, first came to power in a coup in 1999. Then he won presidency in 2002 election, having power until 2006. After ten years, he was elected again in 2016 election. In March 2019, he was re-elected in the elections opposition claimed to be full of irregularities.\nBefore the 2019 election president Azali Assoumani had arranged a constitutional referendum in 2018 that approved extending the presidential mandate from one five-year term to two. The opposition had boycotted the referendum.\nIn January 2020, his party The Convention for the Renewal of the Comoros (CRC) won 20 out of 24 parliamentary seats in the parliamentary election.\nOn 18 February 2023 the Comoros assumed the presidency of the African Union. In January 2024, President Azali Assoumani was re-elected with 63% of the vote in the disputed presidential election.\nThe Comoros Islands have experienced five different constitutions.\nFourth Constitution.\nIn a separate nod to pressure to restore civilian rule, the government organized several committees to compose a new constitution, including the August 2000 National Congress and November 2000 Tripartite Commission. The opposition parties initially refused to participate in the Tripartite Commission, but on 17 February, representatives of the government, the Anjouan separatists, the political opposition, and civil society organizations signed a \"Framework Accord for Reconciliation in Comoros,\" brokered by the Organization for African Unity\nThe accord called for the creation of a new Tripartite Commission for National Reconciliation to develop a \"New Comorian Entity\" with a new constitution. The new federal Constitution came into effect in 2002; it included elements of consociationalism, including a presidency that rotates every four years among the islands and extensive autonomy for each island. Presidential elections were held in 2002, at which Azali Assoumani was elected president. In April 2004 legislative elections were held, completing the implementation of the new constitution.\nThe new Union of the Comoros consists of three islands, Grande Comore, Anjouan and Moh\u00e9li. Each island has a president, who shares the presidency of the Union on a rotating basis. The president and his vice-presidents are elected for a term of four years. The constitution states that, \"the islands enjoy financial autonomy, freely draw up and manage their budgets\".\nPresident Assoumani Azali of Grande Comore is the first Union president. President Mohamed Bacar of Anjouan formed his 13-member government at the end of April, 2003.\nOn 15 May 2006, Ahmed Abdallah Sambi, a cleric and successful businessman educated in Iran, Saudi Arabia and Sudan, was declared the winner of elections for President of the Republic. He is considered a moderate Islamist and is called Ayatollah by his supporters. He beat out retired French air force officer Mohamed Djaanfari and long-time politician Ibrahim Halidi, whose candidacy was backed by Azali Assoumani, the outgoing president.\nA referendum took place on May 16, 2009, to decide whether to cut down the government's unwieldy political bureaucracy. 52.7% of those eligible voted, and 93.8% of votes were cast in approval of the referendum. The referendum would cause each island's president to become a governor and the ministers to become councilors.\nAutonomous islands.\nThe constitution gives Grande Comore, Anjouan and Moh\u00e9li the right to govern most of their own affairs with their own presidents, except the activities assigned to the Union of the Comoros like foreign Policy, defense, nationality, banking and others. Comoros considers Mayotte, an overseas department of France, to be part of its territory, with an autonomous status \nAs of 2011, the three autonomous islands are subdivided into 16 prefectures, 54 communes, and 318 villes or villages.\nExecutive branch.\nThe federal presidency is rotated between the islands' presidents.\nThe Union of the Comoros abolished the position of Prime Minister in 2002. The position of Vice-President of the Comoros was used 2002\u20132019.\nLegislative branch.\nThe Assembly of the Union has 33 seats, 24 elected in single seat constituencies and 9 representatives of the regional assemblies.\nJudicial branch.\nThe Supreme Court or Cour Supreme, has two members appointed by the president, two members elected by the Federal Assembly, one by the Council of each island, and former presidents of the republic.\nInternational organization participation.\nThe Comoros are member of the ACCT, ACP, AfDB, AMF, African Union, FAO, G-77, IBRD, ICAO, ICCt (signatory), ICRM, IDA, IDB, IFAD, IFC, IFRCS, ILO, IMF, InOC, Interpol, IOC, ITU, LAS, NAM, OIC, OPCW (signatory), United Nations, UNCTAD, UNESCO, UNIDO, UPU, WCO, WHO, WMO."}
{"id": "6005", "revid": "1960810", "url": "https://en.wikipedia.org/wiki?curid=6005", "title": "Telecommunications in the Comoros", "text": " \nIn large part thanks to international aid programs, Moroni has international telecommunications service. Telephone service, however, is largely limited to the islands' few towns.\nOverview.\nTelephones \u2013 main lines in use:\n5,000 (1995)\nTelephones \u2013 mobile cellular:\n0 (1995)\nTelephone system:\nsparse system of microwave radio relay and HF radiotelephone communication stations\n&lt;br&gt;\"domestic:\"\nHF radiotelephone communications and microwave radio relay&lt;br&gt;\nCMDA mobile network (Huri, operated by Comores Telecom)\n&lt;br&gt;\"international:\"\nHF radiotelephone communications to Madagascar and R\u00e9union\nRadio broadcast stations:\nAM 1, FM 2, shortwave 1 (1998)\nRadios:\n90,000 (1997)\nTelevision broadcast stations:\n0 (1998)\nTelevisions:\n1,000 (1997)\nInternet Service Providers (ISPs): 1 (1999)\nCountry code (Top-level domain): .km\nSpecial projects.\nIn October 2011 the State of Qatar launched a special program for the construction of a wireless network to interconnect the three islands of the archipelago, by means of low cost, repeatable technology. The project has been developed by Qatar University and Politecnico di Torino, under the supervision of prof. Mazen Hasna and prof. Daniele Trinchero, with a major participation of local actors (Comorian Government, NRTIC, University of the Comoros). The project has been referred as an example of technology transfer and Sustainable Inclusion in developing countries"}
{"id": "6006", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=6006", "title": "Transport in the Comoros", "text": " \nThere are a number of systems of transport in the Comoros. The Comoros possesses of road, of which are paved. It has three seaports: Fomboni, Moroni and Moutsamoudou, but does not have a merchant marine, and no longer has any railway network. It has four airports, all with paved runways, one with runways over long, with the others having runways shorter than .\nThe isolation of the Comoros had made air traffic a major means of transportation. One of President Abdallah's accomplishments was to make the Comoros more accessible by air. During his administration, he negotiated agreements to initiate or enhance commercial air links with Tanzania and Madagascar. The Djohar regime reached an agreement in 1990 to link Moroni and Brussels by air. By the early 1990s, commercial flights connected the Comoros with France, Mauritius, Kenya, South Africa, Tanzania, and Madagascar. The national airline was Air Comores. Daily flights linked the three main islands, and air service was also available to Mahor\u00e9; each island had airstrips. In 1986 the republic received a grant from the French government's CCCE to renovate and expand Hahaya airport, near Moroni. Because of the absence of scheduled sea transport between the islands, nearly all interisland passenger traffic is by air.\nMore than 99% of freight is transported by sea. Both Moroni on Njazidja and Mutsamudu on Nzwani have artificial harbors. There is also a harbor at Fomboni, on Mwali. Despite extensive internationally financed programs to upgrade the harbors at Moroni and Mutsamudu, by the early 1990s only Mutsamudu was operational as a deepwater facility. Its harbor could accommodate vessels of up to eleven meters' draught. At Moroni, ocean-going vessels typically lie offshore and are loaded or unloaded by smaller craft, a costly and sometimes dangerous procedure. Most freight continues to be sent to Tanzania, Kenya, Reunion, or Madagascar for transshipment to the Comoros. Use of Comoran ports is further restricted by the threat of cyclones from December through March. The privately operated Comoran Navigation Company (\"Soci\u00e9t\u00e9 Comorienne de Navigation\") is based in Moroni, and provides services to Madagascar.\nRoads serve the coastal areas, rather than the interior, and the mountainous terrain makes surface travel difficult."}
{"id": "6007", "revid": "46505507", "url": "https://en.wikipedia.org/wiki?curid=6007", "title": "Foreign relations of the Comoros", "text": "In November 1975, Comoros became the 143rd member of the United Nations. The new nation was defined as consisting of the entire archipelago, despite the fact that France maintains control over Mayotte.\nOverview.\nComoros also is a member of the African Union, the Arab League, the European Development Fund, the World Bank, the International Monetary Fund, the Indian Ocean Commission, and the African Development Bank.\nThe government fostered close relationships with the more conservative (and oil-rich) Arab states, such as Saudi Arabia and Kuwait. It frequently received aid from those countries and the regional financial institutions they influenced, such as the Arab Bank for Economic Development in Africa and the Arab Fund for Economic and Social Development. In October 1993, Comoros joined the League of Arab States, after having been rejected when it applied for membership initially in 1977.\nRegional relations generally were good. In 1985 Madagascar, Mauritius, and Seychelles agreed to admit Comoros as the fourth member of the Indian Ocean Commission (IOC), an organization established in 1982 to encourage regional cooperation. In 1993 Mauritius and Seychelles had two of the five embassies in Moroni, and Mauritius and Madagascar were connected to the republic by regularly scheduled commercial flights.\nIn November 1975, Comoros became the 143d member of the UN. In the 1990s, the republic continued to represent Mahor\u00e9 in the UN. Comoros was also a member of the OAU, the EDF, the World Bank, the IMF, the IOC, and the African Development Bank.\nComoros thus cultivated relations with various nations, both East and West, seeking to increase trade and obtain financial assistance. In 1994, however, it was increasingly facing the need to control its expenditures and reorganize its economy so that it would be viewed as a sounder recipient of investment. Comoros also confronted domestically the problem of the degree of democracy the government was prepared to grant to its citizens, a consideration that related to its standing in the world community.\nDiplomatic relations.\nList of countries which the Comoros maintains diplomatic relations with:"}
{"id": "6008", "revid": "1266608729", "url": "https://en.wikipedia.org/wiki?curid=6008", "title": "Army of National Development", "text": "The Comorian Armed Forces (; ) are the national military of the Comoros. The armed forces consist of a small standing army and a 500-member police force, as well as a 500-member defense force. A defense treaty with France provides naval resources for protection of territorial waters, training of Comorian military personnel, and air surveillance. France maintains a small troop presence in the Comoros at government request. France maintains a small Navy base and a Foreign Legion Detachment (DLEM) in Mayotte.\nStructure.\nThe AND consists of the following components:\nAircraft.\nNote: The last comprehensive aircraft inventory list was from \"Aviation Week &amp; Space Technology\" in 2007."}
{"id": "6010", "revid": "48837387", "url": "https://en.wikipedia.org/wiki?curid=6010", "title": "Computer worm", "text": "A computer worm is a standalone malware computer program that replicates itself in order to spread to other computers. It often uses a computer network to spread itself, relying on security failures on the target computer to access it. It will use this machine as a host to scan and infect other computers. When these new worm-invaded computers are controlled, the worm will continue to scan and infect other computers using these computers as hosts, and this behaviour will continue. Computer worms use recursive methods to copy themselves without host programs and distribute themselves based on exploiting the advantages of exponential growth, thus controlling and infecting more and more computers in a short time. Worms almost always cause at least some harm to the network, even if only by consuming bandwidth, whereas viruses almost always corrupt or modify files on a targeted computer.\nMany worms are designed only to spread, and do not attempt to change the systems they pass through. However, as the Morris worm and Mydoom showed, even these \"payload-free\" worms can cause major disruption by increasing network traffic and other unintended effects.\nHistory.\nThe term \"worm\" was first used in this sense in John Brunner's 1975 novel, \"The Shockwave Rider\". In the novel, Nichlas Haflinger designs and sets off a data-gathering worm in an act of revenge against the powerful men who run a national electronic information web that induces mass conformity. \"You have the biggest-ever worm loose in the net, and it automatically sabotages any attempt to monitor it. There's never been a worm with that tough a head or that long a tail!\" \"Then the answer dawned on him, and he almost laughed. Fluckner had resorted to one of the oldest tricks in the store and turned loose in the continental net a self-perpetuating tapeworm, probably headed by a denunciation group \"borrowed\" from a major corporation, which would shunt itself from one nexus to another every time his credit-code was punched into a keyboard. It could take days to kill a worm like that, and sometimes weeks.\"\nThe second ever computer worm was devised to be an anti-virus software. Named Reaper, it was created by Ray Tomlinson to replicate itself across the ARPANET and delete the experimental Creeper program (the first computer worm, 1971).\nOn November 2, 1988, Robert Tappan Morris, a Cornell University computer science graduate student, unleashed what became known as the Morris worm, disrupting many computers then on the Internet, guessed at the time to be one tenth of all those connected. During the Morris appeal process, the U.S. Court of Appeals estimated the cost of removing the worm from each installation at between $200 and $53,000; this work prompted the formation of the CERT Coordination Center and Phage mailing list. Morris himself became the first person tried and convicted under the 1986 Computer Fraud and Abuse Act.\nConficker, a computer worm discovered in 2008 that primarily targeted Microsoft Windows operating systems, is a worm that employs three different spreading strategies: local probing, neighborhood probing, and global probing. This worm was considered a hybrid epidemic and affected millions of computers. The term \"hybrid epidemic\" is used because of the three separate methods it employed to spread, which was discovered through code analysis.\nFeatures.\nIndependence\nComputer viruses generally require a host program. The virus writes its own code into the host program. When the program runs, the written virus program is executed first, causing infection and damage. A worm does not need a host program, as it is an independent program or code chunk. Therefore, it is not restricted by the host program, but can run independently and actively carry out attacks.\nExploit attacks\nBecause a worm is not limited by the host program, worms can take advantage of various operating system vulnerabilities to carry out active attacks. For example, the \"Nimda\" virus exploits vulnerabilities to attack.\nComplexity\nSome worms are combined with web page scripts, and are hidden in HTML pages using VBScript, ActiveX and other technologies. When a user accesses a webpage containing a virus, the virus automatically resides in memory and waits to be triggered. There are also some worms that are combined with backdoor programs or Trojan horses, such as \"Code Red\".\nContagiousness\nWorms are more infectious than traditional viruses. They not only infect local computers, but also all servers and clients on the network based on the local computer. Worms can easily spread through shared folders, e-mails, malicious web pages, and servers with a large number of vulnerabilities in the network.\nHarm.\nAny code designed to do more than spread the worm is typically referred to as the \"payload\". Typical malicious payloads might delete files on a host system (e.g., the ExploreZip worm), encrypt files in a ransomware attack, or exfiltrate data such as confidential documents or passwords.\nSome worms may install a backdoor. This allows the computer to be remotely controlled by the worm author as a \"zombie\". Networks of such machines are often referred to as botnets and are very commonly used for a range of malicious purposes, including sending spam or performing DoS attacks.\nSome special worms attack industrial systems in a targeted manner. Stuxnet was primarily transmitted through LANs and infected thumb-drives, as its targets were never connected to untrusted networks, like the internet. This virus can destroy the core production control computer software used by chemical, power generation and power transmission companies in various countries around the world - in Stuxnet's case, Iran, Indonesia and India were hardest hit - it was used to \"issue orders\" to other equipment in the factory, and to hide those commands from being detected. Stuxnet used multiple vulnerabilities and four different zero-day exploits (e.g.: ) in Windows systems and Siemens SIMATICWinCC systems to attack the embedded programmable logic controllers of industrial machines. Although these systems operate independently from the network, if the operator inserts a virus-infected drive into the system's USB interface, the virus will be able to gain control of the system without any other operational requirements or prompts.\nCountermeasures.\nWorms spread by exploiting vulnerabilities in operating systems.\nVendors with security problems supply regular security updates (see \"Patch Tuesday\"), and if these are installed to a machine, then the majority of worms are unable to spread to it. If a vulnerability is disclosed before the security patch released by the vendor, a zero-day attack is possible.\nUsers need to be wary of opening unexpected emails, and should not run attached files or programs, or visit web sites that are linked to such emails. However, as with the ILOVEYOU worm, and with the increased growth and efficiency of phishing attacks, it remains possible to trick the end-user into running malicious code.\nAnti-virus and anti-spyware software are helpful, but must be kept up-to-date with new pattern files at least every few days. The use of a firewall is also recommended.\nUsers can minimize the threat posed by worms by keeping their computers' operating system and other software up to date, avoiding opening unrecognized or unexpected emails and running firewall and antivirus software.\nMitigation techniques include:\nInfections can sometimes be detected by their behavior - typically scanning the Internet randomly, looking for vulnerable hosts to infect. In addition, machine learning techniques can be used to detect new worms, by analyzing the behavior of the suspected computer.\nWorms with good intent.\nA helpful worm or anti-worm is a worm designed to do something that its author feels is helpful, though not necessarily with the permission of the executing computer's owner. Beginning with the first research into worms at Xerox PARC, there have been attempts to create useful worms. Those worms allowed John Shoch and Jon Hupp to test the Ethernet principles on their network of Xerox Alto computers. Similarly, the Nachi family of worms tried to download and install patches from Microsoft's website to fix vulnerabilities in the host system by exploiting those same vulnerabilities. In practice, although this may have made these systems more secure, it generated considerable network traffic, rebooted the machine in the course of patching it, and did its work without the consent of the computer's owner or user. Regardless of their payload or their writers' intentions, security experts regard all worms as malware. Another example of this approach is Roku OS patching a bug allowing for Roku OS to be rooted via an update to their screensaver channels, which the screensaver would attempt to connect to the telnet and patch the device.\nOne study proposed the first computer worm that operates on the second layer of the OSI model (Data link Layer), utilizing topology information such as Content-addressable memory (CAM) tables and Spanning Tree information stored in switches to propagate and probe for vulnerable nodes until the enterprise network is covered.\nAnti-worms have been used to combat the effects of the Code Red, Blaster, and Santy worms. Welchia is an example of a helpful worm. Utilizing the same deficiencies exploited by the Blaster worm, Welchia infected computers and automatically began downloading Microsoft security updates for Windows without the users' consent. Welchia automatically reboots the computers it infects after installing the updates. One of these updates was the patch that fixed the exploit.\nOther examples of helpful worms are \"Den_Zuko\", \"Cheeze\", \"CodeGreen\", and \"Millenium\".\nArt worms support artists in the performance of massive scale ephemeral artworks. It turns the infected computers into nodes that contribute to the artwork."}
{"id": "6011", "revid": "1264885093", "url": "https://en.wikipedia.org/wiki?curid=6011", "title": "Chomsky hierarchy", "text": "The Chomsky hierarchy in the fields of formal language theory, computer science, and linguistics, is a containment hierarchy of classes of formal grammars. A formal grammar describes how to form strings from a language's vocabulary (or alphabet) that are valid according to the language's syntax. The linguist Noam Chomsky theorized that four different classes of formal grammars existed that could generate increasingly complex languages. Each class can also completely generate the language of all inferior classes (set inclusive).\nHistory.\nThe general idea of a hierarchy of grammars was first described by Noam Chomsky in \"Three models for the description of language\" during the formalization of transformational-generative grammar (TGG). Marcel-Paul Sch\u00fctzenberger also played a role in the development of the theory of formal languages; the paper \"The algebraic theory of context free languages\" describes the modern hierarchy, including context-free grammars.\nIndependently, alongside linguists, mathematicians were developing models of computation (via automata). Parsing a sentence in a language is similar to computation, and the grammars described by Chomsky proved to both resemble and be equivalent in computational power to various machine models.\nThe hierarchy.\nThe following table summarizes each of Chomsky's four types of grammars, the class of language it generates, the type of automaton that recognizes it, and the form its rules must have. The classes are defined by the constraints on the productions rules.\nNote that the set of grammars corresponding to recursive languages is not a member of this hierarchy; these would be properly between Type-0 and Type-1.\nEvery regular language is context-free, every context-free language is context-sensitive, every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.\nRegular (Type-3) grammars.\nType-3 grammars generate the regular languages. Such a grammar restricts its rules to a single nonterminal on the left-hand side and a right-hand side consisting of a single terminal, possibly followed by a single nonterminal, in which case the grammar is \"right regular\". Alternatively, all the rules can have their right-hand sides consist of a single terminal, possibly \"preceded\" by a single nonterminal (\"left regular\"). These generate the same languages. However, if left-regular rules and right-regular rules are combined, the language need no longer be regular. The rule formula_1 is also allowed here if formula_2 does not appear on the right side of any rule. These languages are exactly all languages that can be decided by a finite-state automaton. Additionally, this family of formal languages can be obtained by regular expressions. Regular languages are commonly used to define search patterns and the lexical structure of programming languages.\nFor example, the regular language formula_3 is generated by the Type-3 grammar formula_4 with the productions formula_5 being the following.\nContext-free (Type-2) grammars.\nType-2 grammars generate the context-free languages. These are defined by rules of the form formula_6 with formula_7 being a nonterminal and formula_8 being a string of terminals and/or nonterminals. These languages are exactly all languages that can be recognized by a non-deterministic pushdown automaton. Context-free languages\u2014or rather its subset of deterministic context-free languages\u2014are the theoretical basis for the phrase structure of most programming languages, though their syntax also includes context-sensitive name resolution due to declarations and scope. Often a subset of grammars is used to make parsing easier, such as by an LL parser.\nFor example, the context-free language formula_9 is generated by the Type-2 grammar formula_4 with the productions formula_5 being the following.\nThe language is context-free but not regular (by the pumping lemma for regular languages).\nContext-sensitive (Type-1) grammars.\nType-1 grammars generate context-sensitive languages. These grammars have rules of the form formula_12 with formula_7 a nonterminal and formula_8, formula_15 and formula_16 strings of terminals and/or nonterminals. The strings formula_8 and formula_15 may be empty, but formula_16 must be nonempty. The rule formula_20 is allowed if formula_2 does not appear on the right side of any rule. The languages described by these grammars are exactly all languages that can be recognized by a linear bounded automaton (a nondeterministic Turing machine whose tape is bounded by a constant times the length of the input.)\nFor example, the context-sensitive language formula_22 is generated by the Type-1 grammar formula_23 with the productions formula_5 being the following.\nThe language is context-sensitive but not context-free (by the pumping lemma for context-free languages).\nA proof that this grammar generates formula_22 is sketched in the article on Context-sensitive grammars.\nRecursively enumerable (Type-0) grammars.\nType-0 grammars include all formal grammars. There are no constraints on the productions rules. They generate exactly all languages that can be recognized by a Turing machine, thus any language that is possible to be generated can be generated by a Type-0 grammar. These languages are also known as the \"recursively enumerable\" or \"Turing-recognizable\" languages. Note that this is different from the recursive languages, which can be \"decided\" by an always-halting Turing machine."}
{"id": "6012", "revid": "13467261", "url": "https://en.wikipedia.org/wiki?curid=6012", "title": "Churchs thesis", "text": ""}
{"id": "6013", "revid": "1102976", "url": "https://en.wikipedia.org/wiki?curid=6013", "title": "CRT", "text": "CRT or Crt most commonly refers to:\nCRT may also refer to:"}
{"id": "6014", "revid": "31320", "url": "https://en.wikipedia.org/wiki?curid=6014", "title": "Cathode-ray tube", "text": "A cathode-ray tube (CRT) is a vacuum tube containing one or more electron guns, which emit electron beams that are manipulated to display images on a phosphorescent screen. The images may represent electrical waveforms on an oscilloscope, a frame of video on an analog television set (TV), digital raster graphics on a computer monitor, or other phenomena like radar targets. A CRT in a TV is commonly called a picture tube. CRTs have also been used as memory devices, in which case the screen is not intended to be visible to an observer. The term \"cathode ray\" was used to describe electron beams when they were first discovered, before it was understood that what was emitted from the cathode was a beam of electrons.\nIn CRT TVs and computer monitors, the entire front area of the tube is scanned repeatedly and systematically in a fixed pattern called a raster. In color devices, an image is produced by controlling the intensity of each of three electron beams, one for each additive primary color (red, green, and blue) with a video signal as a reference. In modern CRT monitors and TVs the beams are bent by magnetic deflection, using a deflection yoke. Electrostatic deflection is commonly used in oscilloscopes.\nThe tube is a glass envelope which is heavy, fragile, and long from front screen face to rear end. Its interior must be close to a vacuum to prevent the emitted electrons from colliding with air molecules and scattering before they hit the tube's face. Thus, the interior is evacuated to less than a millionth of atmospheric pressure. As such, handling a CRT carries the risk of violent implosion that can hurl glass at great velocity. The face is typically made of thick lead glass or special barium-strontium glass to be shatter-resistant and to block most X-ray emissions. This tube makes up most of the weight of CRT TVs and computer monitors.\nSince the early 2010s, CRTs have been superseded by flat-panel display technologies such as LCD, plasma display, and OLED displays which are cheaper to manufacture and run, as well as significantly lighter and thinner. Flat-panel displays can also be made in very large sizes whereas was about the largest size of a CRT.\nA CRT works by electrically heating a tungsten coil which in turn heats a cathode in the rear of the CRT, causing it to emit electrons which are modulated and focused by electrodes. The electrons are steered by deflection coils or plates, and an anode accelerates them towards the phosphor-coated screen, which generates light when hit by the electrons.\nHistory.\nDiscoveries.\nCathode rays were discovered by Julius Pl\u00fccker and Johann Wilhelm Hittorf. Hittorf observed that some unknown rays were emitted from the cathode (negative electrode) which could cast shadows on the glowing wall of the tube, indicating the rays were travelling in straight lines. In 1890, Arthur Schuster demonstrated cathode rays could be deflected by electric fields, and William Crookes showed they could be deflected by magnetic fields. In 1897, J. J. Thomson succeeded in measuring the mass-to-charge ratio of cathode rays, showing that they consisted of negatively charged particles smaller than atoms, the first \"subatomic particles\", which had already been named \"electrons\" by Irish physicist George Johnstone Stoney in 1891.\nThe earliest version of the CRT was known as the \"Braun tube\", invented by the German physicist Ferdinand Braun in 1897. It was a cold-cathode diode, a modification of the Crookes tube with a phosphor-coated screen. Braun was the first to conceive the use of a CRT as a display device. The \"Braun tube\" became the foundation of 20th century TV.\nIn 1908, Alan Archibald Campbell-Swinton, fellow of the Royal Society (UK), published a letter in the scientific journal \"Nature\", in which he described how \"distant electric vision\" could be achieved by using a cathode-ray tube (or \"Braun\" tube) as both a transmitting and receiving device. He expanded on his vision in a speech given in London in 1911 and reported in \"The Times\" and the \"Journal of the R\u00f6ntgen Society\".\nThe first cathode-ray tube to use a hot cathode was developed by John Bertrand Johnson (who gave his name to the term Johnson noise) and Harry Weiner Weinhart of Western Electric, and became a commercial product in 1922. The introduction of hot cathodes allowed for lower acceleration anode voltages and higher electron beam currents, since the anode now only accelerated the electrons emitted by the hot cathode, and no longer had to have a very high voltage to induce electron emission from the cold cathode.\nDevelopment.\nIn 1926, Kenjiro Takayanagi demonstrated a CRT TV receiver with a mechanical video camera that received images with a 40-line resolution. By 1927, he improved the resolution to 100 lines, which was unrivaled until 1931. By 1928, he was the first to transmit human faces in half-tones on a CRT display.\nIn 1927, Philo Farnsworth created a TV prototype.\nThe CRT was named in 1929 by inventor Vladimir K. Zworykin. He was subsequently hired by RCA, which was granted a trademark for the term \"Kinescope\", RCA's term for a CRT, in 1932; it voluntarily released the term to the public domain in 1950.\nIn the 1930s, Allen B. DuMont made the first CRTs to last 1,000\u00a0hours of use, which was one of the factors that led to the widespread adoption of TV.\nThe first commercially made electronic TV sets with cathode-ray tubes were manufactured by Telefunken in Germany in 1934.\nIn 1947, the cathode-ray tube amusement device, the earliest known interactive electronic game as well as the first to incorporate a cathode-ray tube screen, was created.\nFrom 1949 to the early 1960s, there was a shift from circular CRTs to rectangular CRTs, although the first rectangular CRTs were made in 1938 by Telefunken. While circular CRTs were the norm, European TV sets often blocked portions of the screen to make it appear somewhat rectangular while American sets often left the entire front of the CRT exposed or only blocked the upper and lower portions of the CRT.\nIn 1954, RCA produced some of the first color CRTs, the 15GP22 CRTs used in the CT-100, the first color TV set to be mass produced. The first rectangular color CRTs were also made in 1954. However, the first rectangular color CRTs to be offered to the public were made in 1963. One of the challenges that had to be solved to produce the rectangular color CRT was convergence at the corners of the CRT. In 1965, brighter rare earth phosphors began replacing dimmer and cadmium-containing red and green phosphors. Eventually blue phosphors were replaced as well.\nThe size of CRTs increased over time, from 20\u00a0inches in 1938, to 21\u00a0inches in 1955, 25\u00a0inches by 1974, 30\u00a0inches by 1980, 35\u00a0inches by 1985, and 43\u00a0inches by 1989. However, experimental 31\u00a0inch CRTs were made as far back as 1938.\nIn 1960, the Aiken tube was invented. It was a CRT in a flat-panel display format with a single electron gun. Deflection was electrostatic and magnetic, but due to patent problems, it was never put into production. It was also envisioned as a head-up display in aircraft. By the time patent issues were solved, RCA had already invested heavily in conventional CRTs.\n1968 marked the release of Sony Trinitron brand with the model KV-1310, which was based on Aperture Grille technology. It was acclaimed to have improved the output brightness. The Trinitron screen was identical with its upright cylindrical shape due to its unique triple cathode single gun construction.\nIn 1987, flat-screen CRTs were developed by Zenith for computer monitors, reducing reflections and helping increase image contrast and brightness. Such CRTs were expensive, which limited their use to computer monitors. Attempts were made to produce flat-screen CRTs using inexpensive and widely available float glass.\nIn 1990, the first CRT with HD resolution, the Sony KW-3600HD, was released to the market. It is considered to be \"historical material\" by Japan's national museum.\nThe Sony KWP-5500HD, an HD CRT projection TV, was released in 1992.\nIn the mid-1990s, some 160 million CRTs were made per year.\nIn the mid-2000s, Canon and Sony presented the surface-conduction electron-emitter display and field-emission displays, respectively. They both were flat-panel displays that had one (SED) or several (FED) electron emitters per subpixel in place of electron guns. The electron emitters were placed on a sheet of glass and the electrons were accelerated to a nearby sheet of glass with phosphors using an anode voltage. The electrons were not focused, making each subpixel essentially a flood beam CRT. They were never put into mass production as LCD technology was significantly cheaper, eliminating the market for such displays.\nThe last large-scale manufacturer of (in this case, recycled) CRTs, Videocon, ceased in 2015. CRT TVs stopped being made around the same time.\nIn 2012, Samsung SDI and several other major companies were fined by the European Commission for price fixing of TV cathode-ray tubes.\nThe same occurred in 2015 in the US and in Canada in 2018.\nWorldwide sales of CRT computer monitors peaked in 2000, at 90 million units, while those of CRT TVs peaked in 2005 at 130 million units.\nDecline.\nBeginning in the late 1990s to the early 2000s, CRTs began to be replaced with LCDs, starting first with computer monitors smaller than 15 inches in size, largely because of their lower bulk. Among the first manufacturers to stop CRT production was Hitachi in 2001, followed by Sony in Japan in 2004, Flat-panel displays dropped in price and started significantly displacing cathode-ray tubes in the 2000s. LCD monitor sales began exceeding those of CRTs in 2003\u20132004 and LCD TV sales started exceeding those of CRTs in some markets in 2005. Samsung SDI stopped CRT production in 2012.\nDespite being a mainstay of display technology for decades, CRT-based computer monitors and TVs are now obsolete. Demand for CRT screens dropped in the late 2000s. Despite efforts from Samsung and LG to make CRTs competitive with their LCD and plasma counterparts, offering slimmer and cheaper models to compete with similarly sized and more expensive LCDs, CRTs eventually became obsolete and were relegated to developing markets and vintage enthusiasts once LCDs fell in price, with their lower bulk, weight and ability to be wall mounted coming as advantages.\nSome industries still use CRTs because it is too much effort, downtime, or cost to replace them, or there is no substitute available; a notable example is the airline industry. Planes such as the Boeing 747-400 and the Airbus A320 used CRT instruments in their glass cockpits instead of mechanical instruments. Airlines such as Lufthansa still use CRT technology, which also uses floppy disks for navigation updates. They are also used in some military equipment for similar reasons. , at least one company manufactures new CRTs for these markets.\nA popular consumer usage of CRTs is for retrogaming. Some games are impossible to play without CRT display hardware. Light guns only work on CRTs because they depend on the progressive timing properties of CRTs. Another reason people use CRTs due to the natural blending of these displays. Some games designed for CRT displays exploit this, which allows them to look more aesthetically pleasing on these displays.\nConstructions.\nBody.\nThe body of a CRT is usually made up of three parts: A screen/faceplate/panel, a cone/funnel, and a neck. The joined screen, funnel and neck are known as the bulb or envelope.\nThe neck is made from a glass tube while the funnel and screen are made by pouring and then pressing glass into a mold. The glass, known as CRT glass or TV glass, needs special properties to shield against x-rays while providing adequate light transmission in the screen or being very electrically insulating in the funnel and neck. The formulation that gives the glass its properties is also known as the melt. The glass is of very high quality, being almost contaminant and defect free. Most of the costs associated with glass production come from the energy used to melt the raw materials into glass. Glass furnaces for CRT glass production have several taps to allow molds to be replaced without stopping the furnace, to allow production of CRTs of several sizes. Only the glass used on the screen needs to have precise optical properties.\nThe optical properties of the glass used on the screen affect color reproduction and purity in color CRTs. Transmittance, or how transparent the glass is, may be adjusted to be more transparent to certain colors (wavelengths) of light. Transmittance is measured at the center of the screen with a 546\u00a0nm wavelength light, and a 10.16mm thick screen. Transmittance goes down with increasing thickness. Standard transmittances for Color CRT screens are 86%, 73%, 57%, 46%, 42% and 30%. Lower transmittances are used to improve image contrast but they put more stress on the electron gun, requiring more power on the electron gun for a higher electron beam power to light the phosphors more brightly to compensate for the reduced transmittance. The transmittance must be uniform across the screen to ensure color purity. The radius (curvature) of screens has increased (grown less curved) over time, from 30 to 68 inches, ultimately evolving into completely flat screens, reducing reflections. The thickness of both curved and flat screens gradually increases from the center outwards, and with it, transmittance is gradually reduced. This means that flat-screen CRTs may not be completely flat on the inside.\nThe glass used in CRTs arrives from the glass factory to the CRT factory as either separate screens and funnels with fused necks, for Color CRTs, or as bulbs made up of a fused screen, funnel and neck. There were several glass formulations for different types of CRTs, that were classified using codes specific to each glass manufacturer. The compositions of the melts were also specific to each manufacturer. Those optimized for high color purity and contrast were doped with Neodymium, while those for monochrome CRTs were tinted to differing levels, depending on the formulation used and had transmittances of 42% or 30%. Purity is ensuring that the correct colors are activated (for example, ensuring that red is displayed uniformly across the screen) while convergence ensures that images are not distorted. Convergence may be modified using a cross hatch pattern.\nCRT glass used to be made by dedicated companies such as AGC Inc., O-I Glass, Samsung Corning Precision Materials, Corning Inc., and Nippon Electric Glass; others such as Videocon, Sony for the US market and Thomson made their own glass.\nThe funnel and the neck are made of leaded potash-soda glass or lead silicate glass formulation to shield against x-rays generated by high voltage electrons as they decelerate after striking a target, such as the phosphor screen or shadow mask of a color CRT. The velocity of the electrons depends on the anode voltage of the CRT; the higher the voltage, the higher the speed. The amount of x-rays emitted by a CRT can also lowered by reducing the brightness of the image. Leaded glass is used because it is inexpensive, while also shielding heavily against x-rays, although some funnels may also contain barium. The screen is usually instead made out of a special lead-free silicate glass formulation with barium and strontium to shield against x-rays, as it doesn't brown unlike glass containing lead. Another glass formulation uses 2\u20133% of lead on the screen. Alternatively zirconium can also be used on the screen in combination with barium, instead of lead.\nMonochrome CRTs may have a tinted barium-lead glass formulation in both the screen and funnel, with a potash-soda lead glass in the neck; the potash-soda and barium-lead formulations have different thermal expansion coefficients. The glass used in the neck must be an excellent electrical insulator to contain the voltages used in the electron optics of the electron gun, such as focusing lenses. The lead in the glass causes it to brown (darken) with use due to x-rays, usually the CRT cathode wears out due to cathode poisoning before browning becomes apparent. The glass formulation determines the highest possible anode voltage and hence the maximum possible CRT screen size. For color, maximum voltages are often 24\u201332\u00a0kV, while for monochrome it is usually 21 or 24.5\u00a0kV, limiting the size of monochrome CRTs to 21\u00a0inches, or ~1\u00a0kV per inch. The voltage needed depends on the size and type of CRT. Since the formulations are different, they must be compatible with one another, having similar thermal expansion coefficients. The screen may also have an anti-glare or anti-reflective coating, or be ground to prevent reflections. CRTs may also have an anti-static coating.\nThe leaded glass in the funnels of CRTs may contain 21\u201325% of lead oxide (PbO), The neck may contain 30\u201340% of lead oxide, and the screen may contain 12% of barium oxide, and 12% of strontium oxide. A typical CRT contains several kilograms of lead as lead oxide in the glass depending on its size; 12 inch CRTs contain 0.5\u00a0kg of lead in total while 32\u00a0inch CRTs contain up to 3\u00a0kg. Strontium oxide began being used in CRTs, its major application, in the 1970s. Before this, CRTs used lead on the faceplate.\nSome early CRTs used a metal funnel insulated with polyethylene instead of glass with conductive material. Others had ceramic or blown Pyrex instead of pressed glass funnels. Early CRTs did not have a dedicated anode cap connection; the funnel was the anode connection, so it was live during operation.\nThe funnel is coated on the inside and outside with a conductive coating, making the funnel a capacitor, helping stabilize and filter the anode voltage of the CRT, and significantly reducing the amount of time needed to turn on a CRT. The stability provided by the coating solved problems inherent to early power supply designs, as they used vacuum tubes. Because the funnel is used as a capacitor, the glass used in the funnel must be an excellent electrical insulator (dielectric). The inner coating has a positive voltage (the anode voltage that can be several kV) while the outer coating is connected to ground. CRTs powered by more modern power supplies do not need to be connected to ground, due to the more robust design of modern power supplies. The value of the capacitor formed by the funnel is 5\u201310\u00a0nF, although at the voltage the anode is normally supplied with. The capacitor formed by the funnel can also suffer from dielectric absorption, similarly to other types of capacitors. Because of this CRTs have to be discharged before handling to prevent injury.\nThe depth of a CRT is related to its screen size. Usual deflection angles were 90\u00b0 for computer monitor CRTs and small CRTs and 110\u00b0 which was the standard in larger TV CRTs, with 120 or 125\u00b0 being used in slim CRTs made since 2001\u20132005 in an attempt to compete with LCD TVs. Over time, deflection angles increased as they became practical, from 50\u00b0 in 1938 to 110\u00b0 in 1959, and 125\u00b0 in the 2000s. 140\u00b0 deflection CRTs were researched but never commercialized, as convergence problems were never resolved.\nSize and weight.\nThe size of a CRT can be measured by the screen's \"entire\" area (or face diagonal) or alternatively by only its \"viewable\" area (or diagonal) that is coated by phosphor and surrounded by black edges.\nWhile the viewable area may be rectangular, the edges of the CRT may have a curvature (e.g. black stripe CRTs, first made by Toshiba in 1972) or the edges may be black and truly flat (e.g. Flatron CRTs), or the viewable area may follow the curvature of the edges of the CRT (with or without black edges or curved edges).\nSmall CRTs below 3\u00a0inches were made for handheld TVs such as the MTV-1 and viewfinders in camcorders. In these, there may be no black edges, that are however truly flat.\nMost of the weight of a CRT comes from the thick glass screen, which comprises 65% of the total weight of a CRT and limits its practical size (see ). The funnel and neck glass comprise the remaining 30% and 5% respectively. The glass in the funnel can vary in thickness, to join the thin neck with the thick screen. Chemically or thermally tempered glass may be used to reduce the weight of the CRT glass.\nAnode.\nThe outer conductive coating is connected to ground while the inner conductive coating is connected using the anode button/cap through a series of capacitors and diodes (a Cockcroft\u2013Walton generator) to the high voltage flyback transformer; the inner coating is the anode of the CRT, which, together with an electrode in the electron gun, is also known as the final anode. The inner coating is connected to the electrode using springs. The electrode forms part of a bipotential lens. The capacitors and diodes serve as a voltage multiplier for the current delivered by the flyback.\nFor the inner funnel coating, monochrome CRTs use aluminum while color CRTs use aquadag; Some CRTs may use iron oxide on the inside. On the outside, most CRTs (but not all) use aquadag. Aquadag is an electrically conductive graphite-based paint. In color CRTs, the aquadag is sprayed onto the interior of the funnel whereas historically aquadag was painted into the interior of monochrome CRTs.\nThe anode is used to accelerate the electrons towards the screen and also collects the secondary electrons that are emitted by the phosphor particles in the vacuum of the CRT.\nThe anode cap connection in modern CRTs must be able to handle up to 55\u201360kV depending on the size and brightness of the CRT. Higher voltages allow for larger CRTs, higher image brightness, or a tradeoff between the two. It consists of a metal clip that expands on the inside of an anode button that is embedded on the funnel glass of the CRT. The connection is insulated by a silicone suction cup, possibly also using silicone grease to prevent corona discharge.\nThe anode button must be specially shaped to establish a hermetic seal between the button and funnel. X-rays may leak through the anode button, although that may not be the case in newer CRTs starting from the late 1970s to early 1980s, thanks to a new button and clip design. The button may consist of a set of 3 nested cups, with the outermost cup being made of a Nickel\u2013Chromium\u2013Iron alloy containing 40\u201349% of Nickel and 3\u20136% of Chromium to make the button easy to fuse to the funnel glass, with a first inner cup made of thick inexpensive iron to shield against x-rays, and with the second innermost cup also being made of iron or any other electrically conductive metal to connect to the clip. The cups must be heat resistant enough and have similar thermal expansion coefficients similar to that of the funnel glass to withstand being fused to the funnel glass. The inner side of the button is connected to the inner conductive coating of the CRT. The anode button may be attached to the funnel while its being pressed into shape in a mold. Alternatively, the x-ray shielding may instead be built into the clip.\nThe flyback transformer is also known as an IHVT (Integrated High Voltage Transformer) if it includes a voltage multiplier. The flyback uses a ceramic or powdered iron core to enable efficient operation at high frequencies. The flyback contains one primary and many secondary windings that provide several different voltages. The main secondary winding supplies the voltage multiplier with voltage pulses to ultimately supply the CRT with the high anode voltage it uses, while the remaining windings supply the CRT's filament voltage, keying pulses, focus voltage and voltages derived from the scan raster. When the transformer is turned off, the flyback's magnetic field quickly collapses which induces high voltage in its windings. The speed at which the magnetic field collapses determines the voltage that is induced, so the voltage increases alongside its speed. A capacitor (Retrace Timing Capacitor) or series of capacitors (to provide redundancy) is used to slow the collapse of the magnetic field.\nThe design of the high voltage power supply in a product using a CRT has an influence in the amount of x-rays emitted by the CRT. The amount of emitted x-rays increases with both higher voltages and currents. If the product such as a TV set uses an unregulated high voltage power supply, meaning that anode and focus voltage go down with increasing electron current when displaying a bright image, the amount of emitted x-rays is as its highest when the CRT is displaying a moderately bright images, since when displaying dark or bright images, the higher anode voltage counteracts the lower electron beam current and vice versa respectively. The high voltage regulator and rectifier vacuum tubes in some old CRT TV sets may also emit x-rays.\nElectron gun.\nThe electron gun emits the electrons that ultimately hit the phosphors on the screen of the CRT. The electron gun contains a heater, which heats a cathode, which generates electrons that, using grids, are focused and ultimately accelerated into the screen of the CRT. The acceleration occurs in conjunction with the inner aluminum or aquadag coating of the CRT. The electron gun is positioned so that it aims at the center of the screen. It is inside the neck of the CRT, and it is held together and mounted to the neck using glass beads or glass support rods, which are the glass strips on the electron gun. The electron gun is made separately and then placed inside the neck through a process called \"winding\", or sealing. The electron gun has a glass wafer that is fused to the neck of the CRT. The connections to the electron gun penetrate the glass wafer. Once the electron gun is inside the neck, its metal parts (grids) are arced between each other using high voltage to smooth any rough edges in a process called spot knocking, to prevent the rough edges in the grids from generating secondary electrons.\nConstruction and method of operation.\nThe electron gun has an indirectly heated hot cathode that is heated by a tungsten filament heating element; the heater may draw 0.5\u20132\u00a0A of current depending on the CRT. The voltage applied to the heater can affect the life of the CRT. Heating the cathode energizes the electrons in it, aiding electron emission, while at the same time current is supplied to the cathode; typically anywhere from 140\u00a0mA at 1.5\u00a0V to 600\u00a0mA at 6.3\u00a0V. The cathode creates an electron cloud (emits electrons) whose electrons are extracted, accelerated and focused into an electron beam. Color CRTs have three cathodes: one for red, green and blue. The heater sits inside the cathode but does not touch it; the cathode has its own separate electrical connection. The cathode is a material coated onto a piece of nickel which provides the electrical connection and structural support; the heater sits inside this piece without touching it.\nThere are several short circuits that can occur in a CRT electron gun. One is a heater-to-cathode short, that causes the cathode to permanently emit electrons which may cause an image with a bright red, green or blue tint with retrace lines, depending on the cathode (s) affected. Alternatively, the cathode may short to the control grid, possibly causing similar effects, or, the control grid and screen grid (G2) can short causing a very dark image or no image at all. The cathode may be surrounded by a shield to prevent sputtering.\nThe cathode is a layer of barium oxide which is coated on a piece of nickel for electrical and mechanical support. The barium oxide must be activated by heating to enable it to release electrons. Activation is necessary because barium oxide is not stable in air, so it is applied to the cathode as barium carbonate, which cannot emit electrons. Activation heats the barium carbonate to decompose it into barium oxide and carbon dioxide while forming a thin layer of metallic barium on the cathode. Activation is done when forming the vacuum (described in ). After activation, the oxide can become damaged by several common gases such as water vapor, carbon dioxide, and oxygen. Alternatively, barium strontium calcium carbonate may be used instead of barium carbonate, yielding barium, strontium and calcium oxides after activation. During operation, the barium oxide is heated to 800\u20131000\u00b0C, at which point it starts shedding electrons.\nSince it is a hot cathode, it is prone to cathode poisoning, which is the formation of a positive ion layer that prevents the cathode from emitting electrons, reducing image brightness significantly or completely and causing focus and intensity to be affected by the frequency of the video signal preventing detailed images from being displayed by the CRT. The positive ions come from leftover air molecules inside the CRT or from the cathode itself that react over time with the surface of the hot cathode. Reducing metals such as manganese, zirconium, magnesium, aluminum or titanium may be added to the piece of nickel to lengthen the life of the cathode, as during activation, the reducing metals diffuse into the barium oxide, improving its lifespan, especially at high electron beam currents. In color CRTs with red, green and blue cathodes, one or more cathodes may be affected independently of the others, causing total or partial loss of one or more colors. CRTs can wear or burn out due to cathode poisoning. Cathode poisoning is accelerated by increased cathode current (overdriving). In color CRTs, since there are three cathodes, one for red, green and blue, a single or more poisoned cathode may cause the partial or complete loss of one or more colors, tinting the image. The layer may also act as a capacitor in series with the cathode, inducing thermal lag. The cathode may instead be made of scandium oxide or incorporate it as a dopant, to delay cathode poisoning, extending the life of the cathode by up to 15%.\nThe amount of electrons generated by the cathodes is related to their surface area. A cathode with more surface area creates more electrons, in a larger electron cloud, which makes focusing the electron cloud into an electron beam more difficult. Normally, only a part of the cathode emits electrons unless the CRT displays images with parts that are at full image brightness; only the parts at full brightness cause all of the cathode to emit electrons. The area of the cathode that emits electrons grows from the center outwards as brightness increases, so cathode wear may be uneven. When only the center of the cathode is worn, the CRT may light brightly those parts of images that have full image brightness but not show darker parts of images at all, in such a case the CRT displays a poor gamma characteristic.\nA negative current is applied to the first (control) grid (G1) to converge the electrons from the hot cathode, creating an electron beam. G1 in practice is a Wehnelt cylinder. The brightness of the screen is not controlled by varying the anode voltage nor the electron beam current (they are never varied) despite them having an influence on image brightness, rather image brightness is controlled by varying the difference in voltage between the cathode and the G1 control grid. The second (screen) grid of the gun (G2) then accelerates the electrons towards the screen using several hundred DC volts. Then a third grid (G3) electrostatically focuses the electron beam before it is deflected and later accelerated by the anode voltage onto the screen. Electrostatic focusing of the electron beam may be accomplished using an einzel lens energized at up to 600 volts. Before electrostatic focusing, focusing the electron beam required a large, heavy and complex mechanical focusing system placed outside the electron gun.\nHowever, electrostatic focusing cannot be accomplished near the final anode of the CRT due to its high voltage in the dozens of Kilovolts, so a high voltage (\u2248600\u20138000\u00a0V) electrode, together with an electrode at the final anode voltage of the CRT, may be used for focusing instead. Such an arrangement is called a bipotential lens, which also offers higher performance than an einzel lens, or, focusing may be accomplished using a magnetic focusing coil together with a high anode voltage of dozens of kilovolts. However, magnetic focusing is expensive to implement, so it is rarely used in practice. Some CRTs may use two grids and lenses to focus the electron beam. The focus voltage is generated in the flyback using a subset of the flyback's high voltage winding in conjunction with a resistive voltage divider. The focus electrode is connected alongside the other connections that are in the neck of the CRT.\nThere is a voltage called cutoff voltage which is the voltage that creates black on the screen since it causes the image on the screen created by the electron beam to disappear, the voltage is applied to G1. In a color CRT with three guns, the guns have different cutoff voltages. Many CRTs share grid G1 and G2 across all three guns, increasing image brightness and simplifying adjustment since on such CRTs there is a single cutoff voltage for all three guns (since G1 is shared across all guns). but placing additional stress on the video amplifier used to feed video into the electron gun's cathodes, since the cutoff voltage becomes higher. Monochrome CRTs do not suffer from this problem. In monochrome CRTs video is fed to the gun by varying the voltage on the first control grid.\nDuring retracing of the electron beam, the preamplifier that feeds the video amplifier is disabled and the video amplifier is biased to a voltage higher than the cutoff voltage to prevent retrace lines from showing, or G1 can have a large negative voltage applied to it to prevent electrons from getting out of the cathode. This is known as blanking. (see Vertical blanking interval and Horizontal blanking interval.) Incorrect biasing can lead to visible retrace lines on one or more colors, creating retrace lines that are tinted or white (for example, tinted red if the red color is affected, tinted magenta if the red and blue colors are affected, and white if all colors are affected). Alternatively, the amplifier may be driven by a video processor that also introduces an OSD (On Screen Display) into the video stream that is fed into the amplifier, using a fast blanking signal. TV sets and computer monitors that incorporate CRTs need a DC restoration circuit to provide a video signal to the CRT with a DC component, restoring the original brightness of different parts of the image.\nThe electron beam may be affected by the Earth's magnetic field, causing it to normally enter the focusing lens off-center; this can be corrected using astigmation controls. Astigmation controls are both magnetic and electronic (dynamic); magnetic does most of the work while electronic is used for fine adjustments. One of the ends of the electron gun has a glass disk, the edges of which are fused with the edge of the neck of the CRT, possibly using frit; the metal leads that connect the electron gun to the outside pass through the disk.\nSome electron guns have a quadrupole lens with dynamic focus to alter the shape and adjust the focus of the electron beam, varying the focus voltage depending on the position of the electron beam to maintain image sharpness across the entire screen, specially at the corners. They may also have a bleeder resistor to derive voltages for the grids from the final anode voltage.\nAfter the CRTs were manufactured, they were aged to allow cathode emission to stabilize.\nThe electron guns in color CRTs are driven by a video amplifier which takes a signal per color channel and amplifies it to 40\u2013170\u00a0V per channel, to be fed into the electron gun's cathodes; each electron gun has its own channel (one per color) and all channels may be driven by the same amplifier, which internally has three separate channels. The amplifier's capabilities limit the resolution, refresh rate and contrast ratio of the CRT, as the amplifier needs to provide high bandwidth and voltage variations at the same time; higher resolutions and refresh rates need higher bandwidths (speed at which voltage can be varied and thus switching between black and white) and higher contrast ratios need higher voltage variations or amplitude for lower black and higher white levels. 30\u00a0MHz of bandwidth can usually provide 720p or 1080i resolution, while 20\u00a0MHz usually provides around 600 (horizontal, from top to bottom) lines of resolution, for example. The difference in voltage between the cathode and the control grid is what modulates the electron beam, modulating its current and thus creating shades of colors which create the image line by line and this can also affect the brightness of the image. The phosphors used in color CRTs produce different amounts of light for a given amount of energy, so to produce white on a color CRT, all three guns must output differing amounts of energy. The gun that outputs the most energy is the red gun since the red phosphor emits the least amount of light.\nGamma.\nCRTs have a pronounced triode characteristic, which results in significant gamma (a nonlinear relationship in an electron gun between applied video voltage and beam intensity).\nDeflection.\nThere are two types of deflection: magnetic and electrostatic. Magnetic is usually used in TVs and monitors as it allows for higher deflection angles (and hence shallower CRTs) and deflection power (which allows for higher electron beam current and hence brighter images) while avoiding the need for high voltages for deflection of up to 2\u00a0kV, while oscilloscopes often use electrostatic deflection since the raw waveforms captured by the oscilloscope can be applied directly (after amplification) to the vertical electrostatic deflection plates inside the CRT.\nMagnetic deflection.\nThose that use magnetic deflection may use a yoke that has two pairs of deflection coils; one pair for vertical, and another for horizontal deflection. The yoke can be bonded (be integral) or removable. Those that were bonded used glue or a plastic to bond the yoke to the area between the neck and the funnel of the CRT while those with removable yokes are clamped. The yoke generates heat whose removal is essential since the conductivity of glass goes up with increasing temperature, the glass needs to be insulating for the CRT to remain usable as a capacitor. The temperature of the glass below the yoke is thus checked during the design of a new yoke. The yoke contains the deflection and convergence coils with a ferrite core to reduce loss of magnetic force as well as the magnetized rings used to align or adjust the electron beams in color CRTs (The color purity and convergence rings, for example) and monochrome CRTs. The yoke may be connected using a connector, the order in which the deflection coils of the yoke are connected determines the orientation of the image displayed by the CRT. The deflection coils may be held in place using polyurethane glue.\nThe deflection coils are driven by sawtooth signals that may be delivered through VGA as horizontal and vertical sync signals. A CRT needs two deflection circuits: a horizontal and a vertical circuit, which are similar except that the horizontal circuit runs at a much higher frequency (a Horizontal scan rate) of 15\u2013240\u00a0kHz depending on the refresh rate of the CRT and the number of horizontal lines to be drawn (the vertical resolution of the CRT). The higher frequency makes it more susceptible to interference, so an automatic frequency control (AFC) circuit may be used to lock the phase of the horizontal deflection signal to that of a sync signal, to prevent the image from becoming distorted diagonally. The vertical frequency varies according to the refresh rate of the CRT. So a CRT with a 60\u00a0Hz refresh rate has a vertical deflection circuit running at 60\u00a0Hz. The horizontal and vertical deflection signals may be generated using two circuits that work differently; the horizontal deflection signal may be generated using a voltage controlled oscillator (VCO) while the vertical signal may be generated using a triggered relaxation oscillator. In many TVs, the frequencies at which the deflection coils run is in part determined by the inductance value of the coils. CRTs had differing deflection angles; the higher the deflection angle, the shallower the CRT for a given screen size, but at the cost of more deflection power and lower optical performance.\nHigher deflection power means more current is sent to the deflection coils to bend the electron beam at a higher angle, which in turn may generate more heat or require electronics that can handle the increased power. Heat is generated due to resistive and core losses. The deflection power is measured in mA per inch. The vertical deflection coils may require ~24 volts while the horizontal deflection coils require ~120 volts to operate.\nThe deflection coils are driven by deflection amplifiers. The horizontal deflection coils may also be driven in part by the horizontal output stage of a TV set. The stage contains a capacitor that is in series with the horizontal deflection coils that performs several functions, among them are: shaping the sawtooth deflection signal to match the curvature of the CRT and centering the image by preventing a DC bias from developing on the coil. At the beginning of retrace, the magnetic field of the coil collapses, causing the electron beam to return to the center of the screen, while at the same time the coil returns energy into capacitors, the energy of which is then used to force the electron beam to go to the left of the screen.\nDue to the high frequency at which the horizontal deflection coils operate, the energy in the deflection coils must be recycled to reduce heat dissipation. Recycling is done by transferring the energy in the deflection coils' magnetic field to a set of capacitors. The voltage on the horizontal deflection coils is negative when the electron beam is on the left side of the screen and positive when the electron beam is on the right side of the screen. The energy required for deflection is dependent on the energy of the electrons. Higher energy (voltage and/or current) electron beams need more energy to be deflected, and are used to achieve higher image brightness.\nElectrostatic deflection.\nMostly used in oscilloscopes. Deflection is carried out by applying a voltage across two pairs of plates, one for horizontal, and the other for vertical deflection. The electron beam is steered by varying the voltage difference across plates in a pair; For example, applying a voltage to the upper plate of the vertical deflection pair, while keeping the voltage in the bottom plate at 0 volts, will cause the electron beam to be deflected towards the upper part of the screen; increasing the voltage in the upper plate while keeping the bottom plate at 0 will cause the electron beam to be deflected to a higher point in the screen (will cause the beam to be deflected at a higher deflection angle). The same applies with the horizontal deflection plates. Increasing the length and proximity between plates in a pair can also increase the deflection angle.\nBurn-in.\nBurn-in is when images are physically \"burned\" into the screen of the CRT; this occurs due to degradation of the phosphors due to prolonged electron bombardment of the phosphors, and happens when a fixed image or logo is left for too long on the screen, causing it to appear as a \"ghost\" image or, in severe cases, also when the CRT is off. To counter this, screensavers were used in computers to minimize burn-in. Burn-in is not exclusive to CRTs, as it also happens to plasma displays and OLED displays.\nEvacuation.\nThe CRT's partial vacuum of to or less is evacuated or exhausted in a ~375\u2013475\u00a0\u00b0C oven in a process called \"baking\" or \"bake-out\". The evacuation process also outgasses any materials inside the CRT, while decomposing others such as the polyvinyl alcohol used to apply the phosphors. The heating and cooling are done gradually to avoid inducing stress, stiffening and possibly cracking the glass; the oven heats the gases inside the CRT, increasing the speed of the gas molecules which increases the chances of them getting drawn out by the vacuum pump. The temperature of the CRT is kept to below that of the oven, and the oven starts to cool just after the CRT reaches 400\u00a0\u00b0C, or, the CRT was kept at a temperature higher than 400\u00a0\u00b0C for up to 15\u201355 minutes. The CRT was heated during or after evacuation, and the heat may have been used simultaneously to melt the frit in the CRT, joining the screen and funnel. The pump used is a turbomolecular pump or a diffusion pump. Formerly mercury vacuum pumps were also used. After baking, the CRT is disconnected (\"sealed or tipped off\") from the vacuum pump. The getter is then fired using an RF (induction) coil. The getter is usually in the funnel or in the neck of the CRT. The getter material which is often barium-based, catches any remaining gas particles as it evaporates due to heating induced by the RF coil (that may be combined with exothermic heating within the material); the vapor fills the CRT, trapping any gas molecules that it encounters and condenses on the inside of the CRT forming a layer that contains trapped gas molecules. Hydrogen may be present in the material to help distribute the barium vapor. The material is heated to temperatures above 1000\u00a0\u00b0C, causing it to evaporate. Partial loss of vacuum in a CRT can result in a hazy image, blue glowing in the neck of the CRT, flashovers, loss of cathode emission or focusing problems.\nRebuilding.\nCRTs used to be rebuilt; repaired or refurbished. The rebuilding process included the disassembly of the CRT, the disassembly and repair or replacement of the electron gun(s), the removal and redeposition of phosphors and aquadag, etc. Rebuilding was popular until the 1960s because CRTs were expensive and wore out quickly, making repair worth it. The last CRT rebuilder in the US closed in 2010, and the last in Europe, RACS, which was located in France, closed in 2013.\nReactivation.\nAlso known as rejuvenation, the goal is to temporarily restore the brightness of a worn CRT. This is often done by carefully increasing the voltage on the cathode heater and the current and voltage on the control grids of the electron gun manually. Some rejuvenators can also fix heater-to-cathode shorts by running a capacitive discharge through the short.\nPhosphors.\nPhosphors in CRTs emit secondary electrons due to them being inside the vacuum of the CRT. The secondary electrons are collected by the anode of the CRT. Secondary electrons generated by phosphors need to be collected to prevent charges from developing in the screen, which would lead to reduced image brightness since the charge would repel the electron beam.\nThe phosphors used in CRTs often contain rare earth metals, replacing earlier dimmer phosphors. Early red and green phosphors contained Cadmium, and some black and white CRT phosphors also contained beryllium in the form of Zinc beryllium silicate, although white phosphors containing cadmium, zinc and magnesium with silver, copper or manganese as dopants were also used. The rare earth phosphors used in CRTs are more efficient (produce more light) than earlier phosphors. The phosphors adhere to the screen because of Van der Waals and electrostatic forces. Phosphors composed of smaller particles adhere more strongly to the screen. The phosphors together with the carbon used to prevent light bleeding (in color CRTs) can be easily removed by scratching.\nSeveral dozen types of phosphors were available for CRTs. Phosphors were classified according to color, persistence, luminance rise and fall curves, color depending on anode voltage (for phosphors used in penetration CRTs), Intended use, chemical composition, safety, sensitivity to burn-in, and secondary emission properties. Examples of rare earth phosphors are yttrium oxide for red and yttrium silicide for blue in beam index tubes, while examples of earlier phosphors are copper cadmium sulfide for red,\nSMPTE-C phosphors have properties defined by the SMPTE-C standard, which defines a color space of the same name. The standard prioritizes accurate color reproduction, which was made difficult by the different phosphors and color spaces used in the NTSC and PAL color systems. PAL TV sets have subjectively better color reproduction due to the use of saturated green phosphors, which have relatively long decay times that are tolerated in PAL since there is more time in PAL for phosphors to decay, due to its lower framerate. SMPTE-C phosphors were used in professional video monitors.\nThe phosphor coating on monochrome and color CRTs may have an aluminum coating on its rear side used to reflect light forward, provide protection against ions to prevent ion burn by negative ions on the phosphor, manage heat generated by electrons colliding against the phosphor, prevent static build up that could repel electrons from the screen, form part of the anode and collect the secondary electrons generated by the phosphors in the screen after being hit by the electron beam, providing the electrons with a return path. The electron beam passes through the aluminum coating before hitting the phosphors on the screen; the aluminum attenuates the electron beam voltage by about 1\u00a0kV. A film or lacquer may be applied to the phosphors to reduce the surface roughness of the surface formed by the phosphors to allow the aluminum coating to have a uniform surface and prevent it from touching the glass of the screen. This is known as filming. The lacquer contains solvents that are later evaporated; the lacquer may be chemically roughened to cause an aluminum coating with holes to be created to allow the solvents to escape.\nPhosphor persistence.\nVarious phosphors are available depending upon the needs of the measurement or display application. The brightness, color, and persistence of the illumination depends upon the type of phosphor used on the CRT screen. Phosphors are available with persistences ranging from less than one microsecond to several seconds. For visual observation of brief transient events, a long persistence phosphor may be desirable. For events which are fast and repetitive, or high frequency, a short-persistence phosphor is generally preferable. The phosphor persistence must be low enough to avoid smearing or ghosting artifacts at high refresh rates.\nLimitations and workarounds.\nBlooming.\nVariations in anode voltage can lead to variations in brightness in parts or all of the image, in addition to blooming, shrinkage or the image getting zoomed in or out. Lower voltages lead to blooming and zooming in, while higher voltages do the opposite. Some blooming is unavoidable, which can be seen as bright areas of an image that expand, distorting or pushing aside surrounding darker areas of the same image. Blooming occurs because bright areas have a higher electron beam current from the electron gun, making the beam wider and harder to focus. Poor voltage regulation causes focus and anode voltage to go down with increasing electron beam current.\nDoming.\nDoming is a phenomenon found on some CRT TVs in which parts of the shadow mask become heated. In TVs that exhibit this behavior, it tends to occur in high-contrast scenes in which there is a largely dark scene with one or more localized bright spots. As the electron beam hits the shadow mask in these areas it heats unevenly. The shadow mask warps due to the heat differences, which causes the electron gun to hit the wrong colored phosphors and incorrect colors to be displayed in the affected area. Thermal expansion causes the shadow mask to expand by around 100 microns.\nDuring normal operation, the shadow mask is heated to around 80\u201390\u00a0\u00b0C. Bright areas of images heat the shadow mask more than dark areas, leading to uneven heating of the shadow mask and warping (blooming) due to thermal expansion caused by heating by increased electron beam current. The shadow mask is usually made of steel but it can be made of Invar (a low-thermal expansion Nickel-Iron alloy) as it withstands two to three times more current than conventional masks without noticeable warping, while making higher resolution CRTs easier to achieve. Coatings that dissipate heat may be applied on the shadow mask to limit blooming in a process called blackening.\nBimetal springs may be used in CRTs used in TVs to compensate for warping that occurs as the electron beam heats the shadow mask, causing thermal expansion. The shadow mask is installed to the screen using metal pieces or a rail or frame that is fused to the funnel or the screen glass respectively, holding the shadow mask in tension to minimize warping (if the mask is flat, used in flat-screen CRT computer monitors) and allowing for higher image brightness and contrast.\nAperture grille screens are brighter since they allow more electrons through, but they require support wires. They are also more resistant to warping. Color CRTs need higher anode voltages than monochrome CRTs to achieve the same brightness since the shadow mask blocks most of the electron beam. Slot masks and specially Aperture grilles do not block as many electrons resulting in a brighter image for a given anode voltage, but aperture grille CRTs are heavier. Shadow masks block 80\u201385% of the electron beam while Aperture grilles allow more electrons to pass through.\nHigh voltage.\nImage brightness is related to the anode voltage and to the CRTs size, so higher voltages are needed for both larger screens and higher image brightness. Image brightness is also controlled by the current of the electron beam. Higher anode voltages and electron beam currents also mean higher amounts of x-rays and heat generation since the electrons have a higher speed and energy. Leaded glass and special barium-strontium glass are used to block most x-ray emissions.\nSize.\nA practical limit on the size of a CRT is the weight of the thick glass needed to safely sustain its vacuum, since a CRT's exterior is exposed to the full atmospheric pressure, which for instance totals on a 27-inch (400\u00a0in2) screen. For example, the large 43-inch Sony PVM-4300 weighs , much heavier than 32-inch CRTs (up to ) and 19-inch CRTs (up to ). Much lighter flat panel TVs are only ~ for 32-inch and for 19-inch.\nSize is also limited by anode voltage, as it would require a higher dielectric strength to prevent arcing and the electrical losses and ozone generation it causes, without sacrificing image brightness.\nShadow masks also become more difficult to make with increasing resolution and size.\nLimits imposed by deflection.\nAt high deflection angles, resolutions and refresh rates (since higher resolutions and refresh rates require significantly higher frequencies to be applied to the horizontal deflection coils), the deflection yoke starts to produce large amounts of heat, due to the need to move the electron beam at a higher angle, which in turn requires exponentially larger amounts of power. As an example, to increase the deflection angle from 90 to 120\u00b0, power consumption of the yoke must also go up from 40 watts to 80 watts, and to increase it further from 120 to 150\u00b0, deflection power must again go up from 80 to 160 watts. This normally makes CRTs that go beyond certain deflection angles, resolutions and refresh rates impractical, since the coils would generate too much heat due to resistance caused by the skin effect, surface and eddy current losses, and/or possibly causing the glass underneath the coil to become conductive (as the electrical conductivity of glass increases with increasing temperature). Some deflection yokes are designed to dissipate the heat that comes from their operation. Higher deflection angles in color CRTs directly affect convergence at the corners of the screen which requires additional compensation circuitry to handle electron beam power and shape, leading to higher costs and power consumption. Higher deflection angles allow a CRT of a given size to be slimmer, however they also impose more stress on the CRT envelope, specially on the panel, the seal between the panel and funnel and on the funnel. The funnel needs to be long enough to minimize stress, as a longer funnel can be better shaped to have lower stress.\nComparison with other technologies.\nOn CRTs, refresh rate depends on resolution, both of which are ultimately limited by the maximum horizontal scanning frequency of the CRT. Motion blur also depends on the decay time of the phosphors. Phosphors that decay too slowly for a given refresh rate may cause smearing or motion blur on the image. In practice, CRTs are limited to a refresh rate of 160\u00a0Hz. LCDs that can compete with OLED (Dual Layer, and mini-LED LCDs) are not available in high refresh rates, although quantum dot LCDs (QLEDs) are available in high refresh rates (up to 144\u00a0Hz) and are competitive in color reproduction with OLEDs.\nCRT monitors can still outperform LCD and OLED monitors in input lag, as there is no signal processing between the CRT and the display connector of the monitor, since CRT monitors often use VGA which provides an analog signal that can be fed to a CRT directly. Video cards designed for use with CRTs may have a RAMDAC to generate the analog signals needed by the CRT. Also, CRT monitors are often capable of displaying sharp images at several resolutions, an ability known as multisyncing. Due to these reasons, CRTs are often preferred for playing video games made in the early 2000s and prior in spite of their bulk, weight and heat generation, with some pieces of technology requiring a CRT to function due to not being built with the functionality of modern displays in mind.\nCRTs tend to be more durable than their flat panel counterparts, though specialised LCDs that have similar durability also exist.\nTypes.\nCRTs were produced in two major categories, picture tubes and display tubes. Picture tubes were used in TVs while display tubes were used in computer monitors. Display tubes were of higher resolution and when used in computer monitors sometimes had adjustable overscan, or sometimes underscan.\nPicture tube CRTs have overscan, meaning the actual edges of the image are not shown; this is deliberate to allow for adjustment variations between CRT TVs, preventing the ragged edges (due to blooming) of the image from being shown on screen. The shadow mask may have grooves that reflect away the electrons that do not hit the screen due to overscan. Color picture tubes used in TVs were also known as CPTs. CRTs are also sometimes called Braun tubes.\nMonochrome CRTs.\nIf the CRT is a black and white (B&amp;W or monochrome) CRT, there is a single electron gun in the neck and the funnel is coated on the inside with aluminum that has been applied by evaporation; the aluminum is evaporated in a vacuum and allowed to condense on the inside of the CRT. Aluminum eliminates the need for ion traps, necessary to prevent ion burn on the phosphor, while also reflecting light generated by the phosphor towards the screen, managing heat and absorbing electrons providing a return path for them; previously funnels were coated on the inside with aquadag, used because it can be applied like paint; the phosphors were left uncoated. Aluminum started being applied to CRTs in the 1950s, coating the inside of the CRT including the phosphors, which also increased image brightness since the aluminum reflected light (that would otherwise be lost inside the CRT) towards the outside of the CRT. In aluminized monochrome CRTs, Aquadag is used on the outside. There is a single aluminum coating covering the funnel and the screen.\nThe screen, funnel and neck are fused together into a single envelope, possibly using lead enamel seals, a hole is made in the funnel onto which the anode cap is installed and the phosphor, aquadag and aluminum are applied afterwards. Previously monochrome CRTs used ion traps that required magnets; the magnet was used to deflect the electrons away from the more difficult to deflect ions, letting the electrons through while letting the ions collide into a sheet of metal inside the electron gun. Ion burn results in premature wear of the phosphor. Since ions are harder to deflect than electrons, ion burn leaves a black dot in the center of the screen.\nThe interior aquadag or aluminum coating was the anode and served to accelerate the electrons towards the screen, collect them after hitting the screen while serving as a capacitor together with the outer aquadag coating. The screen has a single uniform phosphor coating and no shadow mask, technically having no resolution limit.\nMonochrome CRTs may use ring magnets to adjust the centering of the electron beam and magnets around the deflection yoke to adjust the geometry of the image.\nWhen a monochrome CRT is shut off, the screen itself retracts to a small, white dot in the center, along with the phosphors shutting down, shot by the electron gun; it sometimes takes a while for it to go away.\nColor CRTs.\nColor CRTs use three different phosphors which emit red, green, and blue light respectively. They are packed together in stripes (as in aperture grille designs) or clusters called \"triads\" (as in shadow mask CRTs).\nColor CRTs have three electron guns, one for each primary color, (red, green and blue) arranged either in a straight line (in-line) or in an equilateral triangular configuration (the guns are usually constructed as a single unit). The triangular configuration is often called \"delta-gun\", based on its relation to the shape of the Greek letter delta (\u0394). The arrangement of the phosphors is the same as that of the electron guns. A grille or mask absorbs the electrons that would otherwise hit the wrong phosphor.\nA shadow mask tube uses a metal plate with tiny holes, typically in a delta configuration, placed so that the electron beam only illuminates the correct phosphors on the face of the tube; blocking all other electrons. Shadow masks that use slots instead of holes are known as slot masks. The holes or slots are tapered so that the electrons that strike the inside of any hole will be reflected back, if they are not absorbed (e.g. due to local charge accumulation), instead of bouncing through the hole to strike a random (wrong) spot on the screen. Another type of color CRT (Trinitron) uses an aperture grille of tensioned vertical wires to achieve the same result. The shadow mask has a single hole for each triad. The shadow mask is usually \u00a0inch behind the screen.\nTrinitron CRTs were different from other color CRTs in that they had a single electron gun with three cathodes, an aperture grille which lets more electrons through, increasing image brightness (since the aperture grille does not block as many electrons), and a vertically cylindrical screen, rather than a curved screen.\nThe three electron guns are in the neck (except for Trinitrons) and the red, green and blue phosphors on the screen may be separated by a black grid or matrix (called black stripe by Toshiba).\nThe funnel is coated with aquadag on both sides while the screen has a separate aluminum coating applied in a vacuum, deposited after the phosphor coating is applied, facing the electron gun. The aluminum coating protects the phosphor from ions, absorbs secondary electrons, providing them with a return path, preventing them from electrostatically charging the screen which would then repel electrons and reduce image brightness, reflects the light from the phosphors forwards and helps manage heat. It also serves as the anode of the CRT together with the inner aquadag coating. The inner coating is electrically connected to an electrode of the electron gun using springs, forming the final anode. The outer aquadag coating is connected to ground, possibly using a series of springs or a harness that makes contact with the aquadag.\nShadow mask.\nThe shadow mask absorbs or reflects electrons that would otherwise strike the wrong phosphor dots, causing color purity issues (discoloration of images); in other words, when set up correctly, the shadow mask helps ensure color purity. When the electrons strike the shadow mask, they release their energy as heat and x-rays. If the electrons have too much energy due to an anode voltage that is too high for example, the shadow mask can warp due to the heat, which can also happen during the Lehr baking at ~435\u00a0\u00b0C of the frit seal between the faceplate and the funnel of the CRT.\nShadow masks were replaced in TVs by slot masks in the 1970s, since slot masks let more electrons through, increasing image brightness. Shadow masks may be connected electrically to the anode of the CRT. Trinitron used a single electron gun with three cathodes instead of three complete guns. CRT PC monitors usually use shadow masks, except for Sony's Trinitron, Mitsubishi's Diamondtron and NEC's Cromaclear; Trinitron and Diamondtron use aperture grilles while Cromaclear uses a slot mask. Some shadow mask CRTs have color phosphors that are smaller in diameter than the electron beams used to light them, with the intention being to cover the entire phosphor, increasing image brightness. Shadow masks may be pressed into a curved shape.\nScreen manufacture.\nEarly color CRTs did not have a black matrix, which was introduced by Zenith in 1969, and Panasonic in 1970. The black matrix eliminates light leaking from one phosphor to another since the black matrix isolates the phosphor dots from one another, so part of the electron beam touches the black matrix. This is also made necessary by warping of the shadow mask. Light bleeding may still occur due to stray electrons striking the wrong phosphor dots. At high resolutions and refresh rates, phosphors only receive a very small amount of energy, limiting image brightness.\nSeveral methods were used to create the black matrix. One method coated the screen in photoresist such as dichromate-sensitized polyvinyl alcohol photoresist which was then dried and exposed; the unexposed areas were removed and the entire screen was coated in colloidal graphite to create a carbon film, and then hydrogen peroxide was used to remove the remaining photoresist alongside the carbon that was on top of it, creating holes that in turn created the black matrix. The photoresist had to be of the correct thickness to ensure sufficient adhesion to the screen, while the exposure step had to be controlled to avoid holes that were too small or large with ragged edges caused by light diffraction, ultimately limiting the maximum resolution of large color CRTs. The holes were then filled with phosphor using the method described above. Another method used phosphors suspended in an aromatic diazonium salt that adhered to the screen when exposed to light; the phosphors were applied, then exposed to cause them to adhere to the screen, repeating the process once for each color. Then carbon was applied to the remaining areas of the screen while exposing the entire screen to light to create the black matrix, and a fixing process using an aqueous polymer solution was applied to the screen to make the phosphors and black matrix resistant to water. Black chromium may be used instead of carbon in the black matrix. Other methods were also used.\nThe phosphors are applied using photolithography. The inner side of the screen is coated with phosphor particles suspended in PVA photoresist slurry, which is then dried using infrared light, exposed, and developed. The exposure is done using a \"lighthouse\" that uses an ultraviolet light source with a corrector lens to allow the CRT to achieve color purity. Removable shadow masks with spring-loaded clips are used as photomasks. The process is repeated with all colors. Usually the green phosphor is the first to be applied. After phosphor application, the screen is baked to eliminate any organic chemicals (such as the PVA that was used to deposit the phosphor) that may remain on the screen. Alternatively, the phosphors may be applied in a vacuum chamber by evaporating them and allowing them to condense on the screen, creating a very uniform coating. Early color CRTs had their phosphors deposited using silkscreen printing. Phosphors may have color filters over them (facing the viewer), contain pigment of the color emitted by the phosphor, or be encapsulated in color filters to improve color purity and reproduction while reducing glare. Such technology was sold by Toshiba under the Microfilter brand name. Poor exposure due to insufficient light leads to poor phosphor adhesion to the screen, which limits the maximum resolution of a CRT, as the smaller phosphor dots required for higher resolutions cannot receive as much light due to their smaller size.\nAfter the screen is coated with phosphor and aluminum and the shadow mask installed onto it the screen is bonded to the funnel using a glass frit that may contain 65\u201388% of lead oxide by weight. The lead oxide is necessary for the glass frit to have a low melting temperature. Boron oxide (III) may also present to stabilize the frit, with alumina powder as filler powder to control the thermal expansion of the frit. The frit may be applied as a paste consisting of frit particles suspended in amyl acetate or in a polymer with an alkyl methacrylate monomer together with an organic solvent to dissolve the polymer and monomer. The CRT is then baked in an oven in what is called a Lehr bake, to cure the frit, sealing the funnel and screen together. The frit contains a large quantity of lead, causing color CRTs to contain more lead than their monochrome counterparts. Monochrome CRTs on the other hand do not require frit; the funnel can be fused directly to the glass by melting and joining the edges of the funnel and screen using gas flames. Frit is used in color CRTs to prevent deformation of the shadow mask and screen during the fusing process. The edges of the screen and the edges of funnel of the CRT that mate with the screen, are never melted. A primer may be applied on the edges of the funnel and screen before the frit paste is applied to improve adhesion. The Lehr bake consists of several successive steps that heat and then cool the CRT gradually until it reaches a temperature of 435\u2013475\u00a0\u00b0C (other sources may state different temperatures, such as 440\u00a0\u00b0C) After the Lehr bake, the CRT is flushed with air or nitrogen to remove contaminants, the electron gun is inserted and sealed into the neck of the CRT, and a vacuum is formed on the CRT.\nConvergence and purity in color CRTs.\nDue to limitations in the dimensional precision with which CRTs can be manufactured economically, it has not been practically possible to build color CRTs in which three electron beams could be aligned to hit phosphors of respective color in acceptable coordination, solely on the basis of the geometric configuration of the electron gun axes and gun aperture positions, shadow mask apertures, etc. The shadow mask ensures that one beam will only hit spots of certain colors of phosphors, but minute variations in physical alignment of the internal parts among individual CRTs will cause variations in the exact alignment of the beams through the shadow mask, allowing some electrons from, for example, the red beam to hit, say, blue phosphors, unless some individual compensation is made for the variance among individual tubes.\nColor convergence and color purity are two aspects of this single problem. Firstly, for correct color rendering it is necessary that regardless of where the beams are deflected on the screen, all three hit the same spot (and nominally pass through the same hole or slot) on the shadow mask. This is called convergence. More specifically, the convergence at the center of the screen (with no deflection field applied by the yoke) is called static convergence, and the convergence over the rest of the screen area (specially at the edges and corners) is called dynamic convergence. The beams may converge at the center of the screen and yet stray from each other as they are deflected toward the edges; such a CRT would be said to have good static convergence but poor dynamic convergence. Secondly, each beam must only strike the phosphors of the color it is intended to strike and no others. This is called purity. Like convergence, there is static purity and dynamic purity, with the same meanings of \"static\" and \"dynamic\" as for convergence. Convergence and purity are distinct parameters; a CRT could have good purity but poor convergence, or vice versa. Poor convergence causes color \"shadows\" or \"ghosts\" along displayed edges and contours, as if the image on the screen were intaglio printed with poor registration. Poor purity causes objects on the screen to appear off-color while their edges remain sharp. Purity and convergence problems can occur at the same time, in the same or different areas of the screen or both over the whole screen, and either uniformly or to greater or lesser degrees over different parts of the screen.\nThe solution to the static convergence and purity problems is a set of color alignment ring magnets installed around the neck of the CRT. These movable weak permanent magnets are usually mounted on the back end of the deflection yoke assembly and are set at the factory to compensate for any static purity and convergence errors that are intrinsic to the unadjusted tube. Typically there are two or three pairs of two magnets in the form of rings made of plastic impregnated with a magnetic material, with their magnetic fields parallel to the planes of the magnets, which are perpendicular to the electron gun axes. Often, one pair of rings has 2 poles, another has 4, and the remaining ring has 6 poles. Each pair of magnetic rings forms a single effective magnet whose field vector can be fully and freely adjusted (in both direction and magnitude). By rotating a pair of magnets relative to each other, their relative field alignment can be varied, adjusting the effective field strength of the pair. (As they rotate relative to each other, each magnet's field can be considered to have two opposing components at right angles, and these four components [two each for two magnets] form two pairs, one pair reinforcing each other and the other pair opposing and canceling each other. Rotating away from alignment, the magnets' mutually reinforcing field components decrease as they are traded for increasing opposed, mutually cancelling components.) By rotating a pair of magnets together, preserving the relative angle between them, the direction of their collective magnetic field can be varied. Overall, adjusting all of the convergence/purity magnets allows a finely tuned slight electron beam deflection or lateral offset to be applied, which compensates for minor static convergence and purity errors intrinsic to the uncalibrated tube. Once set, these magnets are usually glued in place, but normally they can be freed and readjusted in the field (e.g. by a TV repair shop) if necessary.\nOn some CRTs, additional fixed adjustable magnets are added for dynamic convergence or dynamic purity at specific points on the screen, typically near the corners or edges. Further adjustment of dynamic convergence and purity typically cannot be done passively, but requires active compensation circuits, one to correct convergence horizontally and another to correct it vertically. In this case the deflection yoke contains convergence coils, a set of two per color, wound on the same core, to which the convergence signals are applied. That means 6 convergence coils in groups of 3, with 2 coils per group, with one coil for horizontal convergence correction and another for vertical convergence correction, with each group sharing a core. The groups are separated 120\u00b0 from one another. Dynamic convergence is necessary because the front of the CRT and the shadow mask are not spherical, compensating for electron beam defocusing and astigmatism. The fact that the CRT screen is not spherical leads to geometry problems which may be corrected using a circuit. The signals used for convergence are parabolic waveforms derived from three signals coming from a vertical output circuit. The parabolic signal is fed into the convergence coils, while the other two are sawtooth signals that, when mixed with the parabolic signals, create the necessary signal for convergence. A resistor and diode are used to lock the convergence signal to the center of the screen to prevent it from being affected by the static convergence. The horizontal and vertical convergence circuits are similar. Each circuit has two resonators, one usually tuned to 15,625\u00a0Hz and the other to 31,250\u00a0Hz, which set the frequency of the signal sent to the convergence coils. Dynamic convergence may be accomplished using electrostatic quadrupole fields in the electron gun. Dynamic convergence means that the electron beam does not travel in a perfectly straight line between the deflection coils and the screen, since the convergence coils cause it to become curved to conform to the screen.\nThe convergence signal may instead be a sawtooth signal with a slight sine wave appearance, the sine wave part is created using a capacitor in series with each deflection coil. In this case, the convergence signal is used to drive the deflection coils. The sine wave part of the signal causes the electron beam to move more slowly near the edges of the screen. The capacitors used to create the convergence signal are known as the s-capacitors. This type of convergence is necessary due to the high deflection angles and flat screens of many CRT computer monitors. The value of the s-capacitors must be chosen based on the scan rate of the CRT, so multi-syncing monitors must have different sets of s-capacitors, one for each refresh rate.\nDynamic convergence may instead be accomplished in some CRTs using only the ring magnets, magnets glued to the CRT, and by varying the position of the deflection yoke, whose position may be maintained using set screws, a clamp and rubber wedges. 90\u00b0 deflection angle CRTs may use \"self-convergence\" without dynamic convergence, which together with the in-line triad arrangement, eliminates the need for separate convergence coils and related circuitry, reducing costs. complexity and CRT depth by 10 millimeters. Self-convergence works by means of \"nonuniform\" magnetic fields. Dynamic convergence is necessary in 110\u00b0 deflection angle CRTs, and quadrupole windings on the deflection yoke at a certain frequency may also be used for dynamic convergence.\nDynamic color convergence and purity are one of the main reasons why until late in their history, CRTs were long-necked (deep) and had biaxially curved faces; these geometric design characteristics are necessary for intrinsic passive dynamic color convergence and purity. Only starting around the 1990s did sophisticated active dynamic convergence compensation circuits become available that made short-necked and flat-faced CRTs workable. These active compensation circuits use the deflection yoke to finely adjust beam deflection according to the beam target location. The same techniques (and major circuit components) also make possible the adjustment of display image rotation, skew, and other complex raster geometry parameters through electronics under user control.\nAlternatively, the guns can be aligned with one another (converged) using convergence rings placed right outside the neck; with one ring per gun. The rings can have north and south poles. There can be 4 sets of rings, one to adjust RGB convergence, a second to adjust Red and Blue convergence, a third to adjust vertical raster shift, and a fourth to adjust purity. The vertical raster shift adjusts the straightness of the scan line. CRTs may also employ dynamic convergence circuits, which ensure correct convergence at the edges of the CRT. Permalloy magnets may also be used to correct the convergence at the edges. Convergence is carried out with the help of a crosshatch (grid) pattern. Other CRTs may instead use magnets that are pushed in and out instead of rings. In early color CRTs, the holes in the shadow mask became progressively smaller as they extended outwards from the center of the screen, to aid in convergence.\nMagnetic shielding and degaussing.\nIf the shadow mask or aperture grille becomes magnetized, its magnetic field alters the paths of the electron beams. This causes errors of \"color purity\" as the electrons no longer follow only their intended paths, and some will hit some phosphors of colors other than the one intended. For example, some electrons from the red beam may hit blue or green phosphors, imposing a magenta or yellow tint to parts of the image that are supposed to be pure red. (This effect is localized to a specific area of the screen if the magnetization is localized.) Therefore, it is important that the shadow mask or aperture grille not be magnetized. The earth's magnetic field may have an effect on the color purity of the CRT. Because of this, some CRTs have external magnetic shields over their funnels. The magnetic shield may be made of soft iron or mild steel and contain a degaussing coil. The magnetic shield and shadow mask may be permanently magnetized by the earth's magnetic field, adversely affecting color purity when the CRT is moved. This problem is solved with a built-in degaussing coil, found in many TVs and computer monitors. Degaussing may be automatic, occurring whenever the CRT is turned on. The magnetic shield may also be internal, being on the inside of the funnel of the CRT.\nColor CRT displays in TV sets and computer monitors often have a built-in degaussing (demagnetizing) coil mounted around the perimeter of the CRT face. Upon power-up of the CRT display, the degaussing circuit produces a brief, alternating current through the coil which fades to zero over a few seconds, producing a decaying alternating magnetic field from the coil. This degaussing field is strong enough to remove shadow mask magnetization in most cases, maintaining color purity. In unusual cases of strong magnetization where the internal degaussing field is not sufficient, the shadow mask may be degaussed externally with a stronger portable degausser or demagnetizer. However, an excessively strong magnetic field, whether alternating or constant, may mechanically deform (bend) the shadow mask, causing a permanent color distortion on the display which looks very similar to a magnetization effect.\nResolution.\nDot pitch defines the maximum resolution of the display, assuming delta-gun CRTs. In these, as the scanned resolution approaches the dot pitch resolution, moir\u00e9 appears, as the detail being displayed is finer than what the shadow mask can render. Aperture grille monitors do not suffer from vertical moir\u00e9, however, because their phosphor stripes have no vertical detail. In smaller CRTs, these strips maintain position by themselves, but larger aperture-grille CRTs require one or two crosswise (horizontal) support strips; one for smaller CRTs, and two for larger ones. The support wires block electrons, causing the wires to be visible. In aperture grille CRTs, dot pitch is replaced by stripe pitch. Hitachi developed the Enhanced Dot Pitch (EDP) shadow mask, which uses oval holes instead of circular ones, with respective oval phosphor dots. Moir\u00e9 is reduced in shadow mask CRTs by arranging the holes in the shadow mask in a honeycomb-like pattern.\nProjection CRTs.\nProjection CRTs were used in CRT projectors and CRT rear-projection TVs, and are usually small (being 7\u20139\u00a0inches across); have a phosphor that generates either red, green or blue light, thus making them monochrome CRTs; and are similar in construction to other monochrome CRTs. Larger projection CRTs in general lasted longer, and were able to provide higher brightness levels and resolution, but were also more expensive. Projection CRTs have an unusually high anode voltage for their size (such as 27 or 25\u00a0kV for a 5 or 7-inch projection CRT respectively), and a specially made tungsten/barium cathode (instead of the pure barium oxide normally used) that consists of barium atoms embedded in 20% porous tungsten or barium and calcium aluminates or of barium, calcium and aluminum oxides coated on porous tungsten; the barium diffuses through the tungsten to emit electrons. The special cathode can deliver 2\u00a0mA of current instead of the 0.3mA of normal cathodes, which makes them bright enough to be used as light sources for projection. The high anode voltage and the specially made cathode increase the voltage and current, respectively, of the electron beam, which increases the light emitted by the phosphors, and also the amount of heat generated during operation; this means that projector CRTs need cooling. The screen is usually cooled using a container (the screen forms part of the container) with glycol; the glycol may itself be dyed, or colorless glycol may be used inside a container which may be colored (forming a lens known as a c-element). Colored lenses or glycol are used for improving color reproduction at the cost of brightness, and are only used on red and green CRTs. Each CRT has its own glycol, which has access to an air bubble to allow the glycol to shrink and expand as it cools and warms. Projector CRTs may have adjustment rings just like color CRTs to adjust astigmatism, which is flaring of the electron beam (stray light similar to shadows). They have three adjustment rings; one with two poles, one with four poles, and another with 6 poles. When correctly adjusted, the projector can display perfectly round dots without flaring. The screens used in projection CRTs were more transparent than usual, with 90% transmittance. The first projection CRTs were made in 1933.\nProjector CRTs were available with electrostatic and electromagnetic focusing, the latter being more expensive. Electrostatic focusing used electronics to focus the electron beam, together with focusing magnets around the neck of the CRT for fine focusing adjustments. This type of focusing degraded over time. Electromagnetic focusing was introduced in the early 1990s and included an electromagnetic focusing coil in addition to the already existing focusing magnets. Electromagnetic focusing was much more stable over the lifetime of the CRT, retaining 95% of its sharpness by the end of life of the CRT.\nBeam-index tube.\nBeam-index tubes, also known as Uniray, Apple CRT or Indextron, was an attempt in the 1950s by Philco to create a color CRT without a shadow mask, eliminating convergence and purity problems, and allowing for shallower CRTs with higher deflection angles. It also required a lower voltage power supply for the final anode since it did not use a shadow mask, which normally blocks around 80% of the electrons generated by the electron gun. The lack of a shadow mask also made it immune to the earth's magnetic field while also making degaussing unnecessary and increasing image brightness. It was constructed similarly to a monochrome CRT, with an aquadag outer coating, an aluminum inner coating, and a single electron gun but with a screen with an alternating pattern of red, green, blue and UV (index) phosphor stripes (similarly to a Trinitron) with a side mounted photomultiplier tube or photodiode pointed towards the rear of the screen and mounted on the funnel of CRT, to track the electron beam to activate the phosphors separately from one another using the same electron beam. Only the index phosphor stripe was used for tracking, and it was the only phosphor that was not covered by an aluminum layer. It was shelved because of the precision required to produce it. It was revived by Sony in the 1980s as the Indextron but its adoption was limited, at least in part due to the development of LCD displays. Beam-index CRTs also suffered from poor contrast ratios of only around 50:1 since some light emission by the phosphors was required at all times by the photodiodes to track the electron beam. It allowed for single CRT color CRT projectors due to a lack of shadow mask; normally CRT projectors use three CRTs, one for each color, since a lot of heat is generated due to the high anode voltage and beam current, making a shadow mask impractical and inefficient since it would warp under the heat produced (shadow masks absorb most of the electron beam, and, hence, most of the energy carried by the relativistic electrons); the three CRTs meant that an involved calibration and adjustment procedure had to be carried out during installation of the projector, and moving the projector would require it to be recalibrated. A single CRT meant the need for calibration was eliminated, but brightness was decreased since the CRT screen had to be used for three colors instead of each color having its own CRT screen. A stripe pattern also imposes a horizontal resolution limit; in contrast, three-screen CRT projectors have no theoretical resolution limit, due to them having single, uniform phosphor coatings.\nFlat CRTs.\nFlat CRTs are those with a flat screen. Despite having a flat screen, they may not be completely flat, especially on the inside, instead having a greatly increased curvature. A notable exception is the LG Flatron (made by LG.Philips Displays, later LP Displays) which is truly flat on the outside and inside, but has a bonded glass pane on the screen with a tensioned rim band to provide implosion protection. Such completely flat CRTs were first introduced by Zenith in 1986, and used flat tensioned shadow masks, where the shadow mask is held under tension, providing increased resistance to blooming. LG's Flatron technology is based on this technology developed by Zenith, now a subsidiary of LG. \nFlat CRTs have a number of challenges, like deflection. Vertical deflection boosters are required to increase the amount of current that is sent to the vertical deflection coils to compensate for the reduced curvature. The CRTs used in the Sinclair TV80, and in many Sony Watchmans were flat in that they were not deep and their front screens were flat, but their electron guns were put to a side of the screen. The TV80 used electrostatic deflection while the Watchman used magnetic deflection with a phosphor screen that was curved inwards. Similar CRTs were used in video door bells.\nRadar CRTs.\nRadar CRTs such as the 7JP4 had a circular screen and scanned the beam from the center outwards. The deflection yoke rotated, causing the beam to rotate in a circular fashion. The screen often had two colors, often a bright short persistence color that only appeared as the beam scanned the display and a long persistence phosphor afterglow. When the beam strikes the phosphor, the phosphor brightly illuminates, and when the beam leaves, the dimmer long persistence afterglow would remain lit where the beam struck the phosphor, alongside the radar targets that were \"written\" by the beam, until the beam re-struck the phosphor.\nOscilloscope CRTs.\nIn oscilloscope CRTs, electrostatic deflection is used, rather than the magnetic deflection commonly used with TV and other large CRTs. The beam is deflected horizontally by applying an electric field between a pair of plates to its left and right, and vertically by applying an electric field to plates above and below. TVs use magnetic rather than electrostatic deflection because the deflection plates obstruct the beam when the deflection angle is as large as is required for tubes that are relatively short for their size. Some Oscilloscope CRTs incorporate post deflection anodes (PDAs) that are spiral-shaped to ensure even anode potential across the CRT and operate at up to 15\u00a0kV. In PDA CRTs the electron beam is deflected before it is accelerated, improving sensitivity and legibility, specially when analyzing voltage pulses with short duty cycles.\nMicrochannel plate.\nWhen displaying fast one-shot events, the electron beam must deflect very quickly, with few electrons impinging on the screen, leading to a faint or invisible image on the display. Oscilloscope CRTs designed for very fast signals can give a brighter display by passing the electron beam through a micro-channel plate just before it reaches the screen. Through the phenomenon of secondary emission, this plate multiplies the number of electrons reaching the phosphor screen, giving a significant improvement in writing rate (brightness) and improved sensitivity and spot size as well.\nGraticules.\nMost oscilloscopes have a graticule as part of the visual display, to facilitate measurements. The graticule may be permanently marked inside the face of the CRT, or it may be a transparent external plate made of glass or acrylic plastic. An internal graticule eliminates parallax error, but cannot be changed to accommodate different types of measurements. Oscilloscopes commonly provide a means for the graticule to be illuminated from the side, which improves its visibility.\nImage storage tubes.\nThese are found in \"analog phosphor storage oscilloscopes\". These are distinct from \"digital storage oscilloscopes\" which rely on solid state digital memory to store the image.\nWhere a single brief event is monitored by an oscilloscope, such an event will be displayed by a conventional tube only while it actually occurs. The use of a long persistence phosphor may allow the image to be observed after the event, but only for a few seconds at best. This limitation can be overcome by the use of a direct view storage cathode-ray tube (storage tube). A storage tube will continue to display the event after it has occurred until such time as it is erased. A storage tube is similar to a conventional tube except that it is equipped with a metal grid coated with a dielectric layer located immediately behind the phosphor screen. An externally applied voltage to the mesh initially ensures that the whole mesh is at a constant potential. This mesh is constantly exposed to a low velocity electron beam from a 'flood gun' which operates independently of the main gun. This flood gun is not deflected like the main gun but constantly 'illuminates' the whole of the storage mesh. The initial charge on the storage mesh is such as to repel the electrons from the flood gun which are prevented from striking the phosphor screen.\nWhen the main electron gun writes an image to the screen, the energy in the main beam is sufficient to create a 'potential relief' on the storage mesh. The areas where this relief is created no longer repel the electrons from the flood gun which now pass through the mesh and illuminate the phosphor screen. Consequently, the image that was briefly traced out by the main gun continues to be displayed after it has occurred. The image can be 'erased' by resupplying the external voltage to the mesh restoring its constant potential. The time for which the image can be displayed was limited because, in practice, the flood gun slowly neutralises the charge on the storage mesh. One way of allowing the image to be retained for longer is temporarily to turn off the flood gun. It is then possible for the image to be retained for several days. The majority of storage tubes allow for a lower voltage to be applied to the storage mesh which slowly restores the initial charge state. By varying this voltage a variable persistence is obtained. Turning off the flood gun and the voltage supply to the storage mesh allows such a tube to operate as a conventional oscilloscope tube.\nVector monitors.\nVector monitors were used in early computer aided design systems and are in some late-1970s to mid-1980s arcade games such as \"Asteroids\".\nThey draw graphics point-to-point, rather than scanning a raster. Either monochrome or color CRTs can be used in vector displays, and the essential principles of CRT design and operation are the same for either type of display; the main difference is in the beam deflection patterns and circuits.\nData storage tubes.\nThe Williams tube or Williams-Kilburn tube was a cathode-ray tube used to electronically store binary data. It was used in computers of the 1940s as a random-access digital storage device. In contrast to other CRTs in this article, the Williams tube was not a display device, and in fact could not be viewed since a metal plate covered its screen.\nCat's eye.\nIn some vacuum tube radio sets, a \"Magic Eye\" or \"Tuning Eye\" tube was provided to assist in tuning the receiver. Tuning would be adjusted until the width of a radial shadow was minimized. This was used instead of a more expensive electromechanical meter, which later came to be used on higher-end tuners when transistor sets lacked the high voltage required to drive the device. The same type of device was used with tape recorders as a recording level meter, and for various other applications including electrical test equipment.\nCharactrons.\nSome displays for early computers (those that needed to display more text than was practical using vectors, or that required high speed for photographic output) used Charactron CRTs. These incorporate a perforated metal character mask (stencil), which shapes a wide electron beam to form a character on the screen. The system selects a character on the mask using one set of deflection circuits, but that causes the extruded beam to be aimed off-axis, so a second set of deflection plates has to re-aim the beam so it is headed toward the center of the screen. A third set of plates places the character wherever required. The beam is unblanked (turned on) briefly to draw the character at that position. Graphics could be drawn by selecting the position on the mask corresponding to the code for a space (in practice, they were simply not drawn), which had a small round hole in the center; this effectively disabled the character mask, and the system reverted to regular vector behavior. Charactrons had exceptionally long necks, because of the need for three deflection systems.\nNimo.\nNimo was the trademark of a family of small specialised CRTs manufactured by Industrial Electronic Engineers. These had 10 electron guns which produced electron beams in the form of digits in a manner similar to that of the charactron. The tubes were either simple single-digit displays or more complex 4- or 6- digit displays produced by means of a suitable magnetic deflection system. Having little of the complexities of a standard CRT, the tube required a relatively simple driving circuit, and as the image was projected on the glass face, it provided a much wider viewing angle than competitive types (e.g., nixie tubes). However, their requirement for several voltages and their high voltage made them uncommon.\nFlood-beam CRT.\nFlood-beam CRTs are small tubes that are arranged as pixels for large video walls like Jumbotrons. The first screen using this technology (called Diamond Vision by Mitsubishi Electric) was introduced by Mitsubishi Electric for the 1980 Major League Baseball All-Star Game. It differs from a normal CRT in that the electron gun within does not produce a focused controllable beam. Instead, electrons are sprayed in a wide cone across the entire front of the phosphor screen, basically making each unit act as a single light bulb. Each one is coated with a red, green or blue phosphor, to make up the color sub-pixels. This technology has largely been replaced with light-emitting diode displays. Unfocused and undeflected CRTs were used as grid-controlled stroboscope lamps since 1958. Electron-stimulated luminescence (ESL) lamps, which use the same operating principle, were released in 2011.\nPrint-head CRT.\nCRTs with an unphosphored front glass but with fine wires embedded in it were used as electrostatic print heads in the 1960s. The wires would pass the electron beam current through the glass onto a sheet of paper where the desired content was therefore deposited as an electrical charge pattern. The paper was then passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image.\nZeus \u2013 thin CRT display.\nIn the late 1990s and early 2000s Philips Research Laboratories experimented with a type of thin CRT known as the \"Zeus\" display, which contained CRT-like functionality in a flat-panel display. The cathode of this display was mounted under the front of the display, and the electrons from the cathode would be directed to the back to the display where they would stay until extracted by electrodes near the front of the display, and directed to the front of the display which had phosphor dots. The devices were demonstrated but never marketed.\nSlimmer CRT.\nSome CRT manufacturers, both LG.Philips Displays (later LP Displays) and Samsung SDI, innovated CRT technology by creating a slimmer tube. Slimmer CRT had the trade names Superslim, Ultraslim, Vixlim (by Samsung) and Cybertube and Cybertube+ (both by LG Philips displays). A flat CRT has a depth. The depth of Superslim was and Ultraslim was .\nHealth concerns.\nIonizing radiation.\nCRTs can emit a small amount of X-ray radiation; this is a result of the electron beam's bombardment of the shadow mask/aperture grille and phosphors, which produces bremsstrahlung (braking radiation) as the high-energy electrons are decelerated. The amount of radiation escaping the front of the monitor is widely considered to be not harmful. The Food and Drug Administration regulations in are used to strictly limit, for instance, TV receivers to 0.5 milliroentgens per hour at a distance of from any external surface; since 2007, most CRTs have emissions that fall well below this limit. Note that the roentgen is an outdated unit and does not account for dose absorption. The conversion rate is about .877 roentgen per rem. Assuming that the viewer absorbed the entire dose (which is unlikely), and that they watched TV for 2 hours a day, a .5 milliroentgen hourly dose would increase the viewers yearly dose by 320 millirem. For comparison, the average background radiation in the United States is 310 millirem a year. Negative effects of chronic radiation are not generally noticeable until doses over 20,000 millirem.\nThe density of the x-rays that would be generated by a CRT is low because the raster scan of a typical CRT distributes the energy of the electron beam across the entire screen. Voltages above 15,000 volts are enough to generate \"soft\" x-rays. However, since CRTs may stay on for several hours at a time, the amount of x-rays generated by the CRT may become significant, hence the importance of using materials to shield against x-rays, such as the thick leaded glass and barium-strontium glass used in CRTs.\nConcerns about x-rays emitted by CRTs began in 1967 when it was found that TV sets made by General Electric were emitting \"X-radiation in excess of desirable levels\". It was later found that TV sets from all manufacturers were also emitting radiation. This caused TV industry representatives to be brought before a U.S. congressional committee, which later proposed a federal radiation regulation bill, which became the 1968 Radiation Control for Health and Safety Act. It was recommended to TV set owners to always be at a distance of at least 6 feet from the screen of the TV set, and to avoid \"prolonged exposure\" at the sides, rear or underneath a TV set. It was discovered that most of the radiation was directed downwards. Owners were also told to not modify their set's internals to avoid exposure to radiation. Headlines about \"radioactive\" TV sets continued until the end of the 1960s. There once was a proposal by two New York congressmen that would have forced TV set manufacturers to \"go into homes to test all of the nation's 15 million color sets and to install radiation devices in them\". The FDA eventually began regulating radiation emissions from all electronic products in the US.\nToxicity.\nOlder color and monochrome CRTs may have been manufactured with toxic substances, such as cadmium, in the phosphors. The rear glass tube of modern CRTs may be made from leaded glass, which represent an environmental hazard if disposed of improperly. Since 1970, glass in the front panel (the viewable portion of the CRT) used strontium oxide rather than lead, though the rear of the CRT was still produced from leaded glass. Monochrome CRTs typically do not contain enough leaded glass to fail EPA TCLP tests. While the TCLP process grinds the glass into fine particles in order to expose them to weak acids to test for leachate, intact CRT glass does not leach (The lead is vitrified, contained inside the glass itself, similar to leaded glass crystalware).\nFlicker.\nAt low refresh rates (60\u00a0Hz and below), the periodic scanning of the display may produce a flicker that some people perceive more easily than others, especially when viewed with peripheral vision. Flicker is commonly associated with CRT as most TVs run at 50\u00a0Hz (PAL) or 60\u00a0Hz (NTSC), although there are some 100\u00a0Hz PAL TVs that are flicker-free. Typically only low-end monitors run at such low frequencies, with most computer monitors supporting at least 75\u00a0Hz and high-end monitors capable of 100\u00a0Hz or more to eliminate any perception of flicker. Though the 100\u00a0Hz PAL was often achieved using interleaved scanning, dividing the circuit and scan into two beams of 50\u00a0Hz. Non-computer CRTs or CRT for sonar or radar may have long persistence phosphor and are thus flicker free. If the persistence is too long on a video display, moving images will be blurred.\nHigh-frequency audible noise.\n50\u00a0Hz/60\u00a0Hz CRTs used for TV operate with horizontal scanning frequencies of 15,750 and 15,734.27\u00a0Hz (for NTSC systems) or 15,625\u00a0Hz (for PAL systems). These frequencies are at the upper range of human hearing and are inaudible to many people; however, some people (especially children) will perceive a high-pitched tone near an operating CRT TV. The sound is due to magnetostriction in the magnetic core and periodic movement of windings of the flyback transformer but the sound can also be created by movement of the deflection coils, yoke or ferrite beads.\nThis problem does not occur on 100/120\u00a0Hz TVs and on non-CGA (Color Graphics Adapter) computer displays, because they use much higher horizontal scanning frequencies that produce sound which is inaudible to humans (22\u00a0kHz to over 100\u00a0kHz).\nImplosion.\nIf the glass wall is damaged, atmospheric pressure can implode the vacuum tube into dangerous fragments which accelerate inward and then spray at high speed in all directions. Although modern cathode-ray tubes used in TVs and computer displays have epoxy-bonded face-plates or other measures to prevent shattering of the envelope, CRTs must be handled carefully to avoid injury.\nImplosion protection.\nEarly CRTs had a glass plate over the screen that was bonded to it using glue, creating a laminated glass screen: initially the glue was polyvinyl acetate (PVA), while later versions such as the LG Flatron used a resin, perhaps a UV-curable resin. The PVA degrades over time creating a \"cataract\", a ring of degraded glue around the edges of the CRT that does not allow light from the screen to pass through. Later CRTs instead use a tensioned metal rim band mounted around the perimeter that also provides mounting points for the CRT to be mounted to a housing. In a 19-inch CRT, the tensile stress in the rim band is 70\u00a0kg/cm2.\nOlder CRTs were mounted to the TV set using a frame. The band is tensioned by heating it, then mounting it on the CRT; the band cools afterwards, shrinking in size and putting the glass under compression, which strengthens the glass and reduces the necessary thickness (and hence weight) of the glass. This makes the band an integral component that should never be removed from an intact CRT that still has a vacuum; attempting to remove it may cause the CRT to implode.\nThe rim band prevents the CRT from imploding should the screen be broken. The rim band may be glued to the perimeter of the CRT using epoxy, preventing cracks from spreading beyond the screen and into the funnel.\nAlternatively the compression caused by the rim band may be used to cause any cracks in the screen to propagate laterally at a high speed so that they reach the funnel and fully penetrate it before they fully penetrate the screen. This is possible because the funnel has walls that are thinner than the screen. Fully penetrating the funnel first allows air to enter the CRT from a short distance behind the screen, and prevent an implosion by ensuring the screen is fully penetrated by the cracks and breaks only when the CRT already has air.\nElectric shock.\nTo accelerate the electrons from the cathode to the screen with enough energy to achieve sufficient image brightness, a very high voltage (EHT or extra-high tension) is required, from a few thousand volts for a small oscilloscope CRT to tens of thousands for a larger screen color TV. This is many times greater than household power supply voltage. Even after the power supply is turned off, some associated capacitors and the CRT itself may retain a charge for some time and therefore dissipate that charge suddenly through a ground such as an inattentive human grounding a capacitor discharge lead. An average monochrome CRT may use 1\u20131.5\u00a0kV of anode voltage per inch.\nSecurity concerns.\nUnder some circumstances, the signal radiated from the electron guns, scanning circuitry, and associated wiring of a CRT can be captured remotely and used to reconstruct what is shown on the CRT using a process called Van Eck phreaking. Special TEMPEST shielding can mitigate this effect. Such radiation of a potentially exploitable signal, however, occurs also with other display technologies and with electronics in general.\nRecycling.\nDue to the toxins contained in CRT monitors the United States Environmental Protection Agency created rules (in October 2001) stating that CRTs must be brought to special e-waste recycling facilities. In November 2002, the EPA began fining companies that disposed of CRTs through landfills or incineration. Regulatory agencies, local and statewide, monitor the disposal of CRTs and other computer equipment.\nAs electronic waste, CRTs are considered one of the hardest types to recycle. CRTs have relatively high concentration of lead and , both of which are necessary for the display. There are several companies in the United States that charge a small fee to collect CRTs, then subsidize their labor by selling the harvested copper, wire, and printed circuit boards. The United States Environmental Protection Agency (EPA) includes discarded CRT monitors in its category of \"hazardous household waste\" but considers CRTs that have been set aside for testing to be commodities if they are not discarded, speculatively accumulated, or left unprotected from weather and other damage.\nVarious states participate in the recycling of CRTs, each with their reporting requirements for collectors and recycling facilities. For example, in California the recycling of CRTs is governed by CALRecycle, the California Department of Resources Recycling and Recovery through their Payment System. Recycling facilities that accept CRT devices from business and residential sector must obtain contact information such as address and phone number to ensure the CRTs come from a California source in order to participate in the CRT Recycling Payment System.\nIn Europe, disposal of CRT TVs and monitors is covered by the WEEE Directive.\nMultiple methods have been proposed for the recycling of CRT glass. The methods involve thermal, mechanical and chemical processes. All proposed methods remove the lead oxide content from the glass. Some companies operated furnaces to separate the lead from the glass. A coalition called the Recytube project was once formed by several European companies to devise a method to recycle CRTs. The phosphors used in CRTs often contain rare earth metals. A CRT contains about 7\u00a0grams of phosphor.\nThe funnel can be separated from the screen of the CRT using laser cutting, diamond saws or wires or using a resistively heated nichrome wire.\nLeaded CRT glass was sold to be remelted into other CRTs, or even broken down and used in road construction or used in tiles, concrete, concrete and cement bricks, fiberglass insulation or used as flux in metals smelting.\nA considerable portion of CRT glass is landfilled, where it can pollute the surrounding environment. It is more common for CRT glass to be disposed of than being recycled.\nSee also.\nApplying CRT in different display-purpose:\nHistorical aspects:\nSafety and precautions:"}
{"id": "6015", "revid": "40255651", "url": "https://en.wikipedia.org/wiki?curid=6015", "title": "Crystal", "text": "A crystal or crystalline solid is a solid material whose constituents (such as atoms, molecules, or ions) are arranged in a highly ordered microscopic structure, forming a crystal lattice that extends in all directions. In addition, macroscopic single crystals are usually identifiable by their geometrical shape, consisting of flat faces with specific, characteristic orientations. The scientific study of crystals and crystal formation is known as crystallography. The process of crystal formation via mechanisms of crystal growth is called crystallization or solidification.\nThe word \"crystal\" derives from the Ancient Greek word (), meaning both \"ice\" and \"rock crystal\", from (), \"icy cold, frost\".\nExamples of large crystals include snowflakes, diamonds, and table salt. Most inorganic solids are not crystals but polycrystals, i.e. many microscopic crystals fused together into a single solid. Polycrystals include most metals, rocks, ceramics, and ice. A third category of solids is amorphous solids, where the atoms have no periodic structure whatsoever. Examples of amorphous solids include glass, wax, and many plastics.\nDespite the name, lead crystal, crystal glass, and related products are \"not\" crystals, but rather types of glass, i.e. amorphous solids.\nCrystals, or crystalline solids, are often used in pseudoscientific practices such as crystal therapy, and, along with gemstones, are sometimes associated with spellwork in Wiccan beliefs and related religious movements.\nCrystal structure (microscopic).\nThe scientific definition of a \"crystal\" is based on the microscopic arrangement of atoms inside it, called the crystal structure. A crystal is a solid where the atoms form a periodic arrangement. (Quasicrystals are an exception, see below).\nNot all solids are crystals. For example, when liquid water starts freezing, the phase change begins with small ice crystals that grow until they fuse, forming a \"polycrystalline\" structure. In the final block of ice, each of the small crystals (called \"crystallites\" or \"grains\") is a true crystal with a periodic arrangement of atoms, but the whole polycrystal does \"not\" have a periodic arrangement of atoms, because the periodic pattern is broken at the grain boundaries. Most macroscopic inorganic solids are polycrystalline, including almost all metals, ceramics, ice, rocks, etc. Solids that are neither crystalline nor polycrystalline, such as glass, are called \"amorphous solids\", also called glassy, vitreous, or noncrystalline. These have no periodic order, even microscopically. There are distinct differences between crystalline solids and amorphous solids: most notably, the process of forming a glass does not release the latent heat of fusion, but forming a crystal does.\nA crystal structure (an arrangement of atoms in a crystal) is characterized by its \"unit cell\", a small imaginary box containing one or more atoms in a specific spatial arrangement. The unit cells are stacked in three-dimensional space to form the crystal.\nThe symmetry of a crystal is constrained by the requirement that the unit cells stack perfectly with no gaps. There are 219 possible crystal symmetries (230 is commonly cited, but this treats chiral equivalents as separate entities), called crystallographic space groups. These are grouped into 7 crystal systems, such as cubic crystal system (where the crystals may form cubes or rectangular boxes, such as halite shown at right) or hexagonal crystal system (where the crystals may form hexagons, such as ordinary water ice).\nCrystal faces, shapes and crystallographic forms.\nCrystals are commonly recognized, macroscopically, by their shape, consisting of flat faces with sharp angles. These shape characteristics are not \"necessary\" for a crystal\u2014a crystal is scientifically defined by its microscopic atomic arrangement, not its macroscopic shape\u2014but the characteristic macroscopic shape is often present and easy to see.\nEuhedral crystals are those that have obvious, well-formed flat faces. Anhedral crystals do not, usually because the crystal is one grain in a polycrystalline solid.\nThe flat faces (also called facets) of a euhedral crystal are oriented in a specific way relative to the underlying atomic arrangement of the crystal: they are planes of relatively low Miller index. This occurs because some surface orientations are more stable than others (lower surface energy). As a crystal grows, new atoms attach easily to the rougher and less stable parts of the surface, but less easily to the flat, stable surfaces. Therefore, the flat surfaces tend to grow larger and smoother, until the whole crystal surface consists of these plane surfaces. (See diagram on right.)\nOne of the oldest techniques in the science of crystallography consists of measuring the three-dimensional orientations of the faces of a crystal, and using them to infer the underlying crystal symmetry.\nA crystal's crystallographic forms are sets of possible faces of the crystal that are related by one of the symmetries of the crystal. For example, crystals of galena often take the shape of cubes, and the six faces of the cube belong to a crystallographic form that displays one of the symmetries of the isometric crystal system. Galena also sometimes crystallizes as octahedrons, and the eight faces of the octahedron belong to another crystallographic form reflecting a different symmetry of the isometric system. A crystallographic form is described by placing the Miller indices of one of its faces within brackets. For example, the octahedral form is written as {111}, and the other faces in the form are implied by the symmetry of the crystal.\nForms may be closed, meaning that the form can completely enclose a volume of space, or open, meaning that it cannot. The cubic and octahedral forms are examples of closed forms. All the forms of the isometric system are closed, while all the forms of the monoclinic and triclinic crystal systems are open. A crystal's faces may all belong to the same closed form, or they may be a combination of multiple open or closed forms.\nA crystal's habit is its visible external shape. This is determined by the crystal structure (which restricts the possible facet orientations), the specific crystal chemistry and bonding (which may favor some facet types over others), and the conditions under which the crystal formed.\nOccurrence in nature.\nRocks.\nBy volume and weight, the largest concentrations of crystals in the Earth are part of its solid bedrock. Crystals found in rocks typically range in size from a fraction of a millimetre to several centimetres across, although exceptionally large crystals are occasionally found. , the world's largest known naturally occurring crystal is a crystal of beryl from Malakialina, Madagascar, long and in diameter, and weighing .\nSome crystals have formed by magmatic and metamorphic processes, giving origin to large masses of crystalline rock. The vast majority of igneous rocks are formed from molten magma and the degree of crystallization depends primarily on the conditions under which they solidified. Such rocks as granite, which have cooled very slowly and under great pressures, have completely crystallized; but many kinds of lava were poured out at the surface and cooled very rapidly, and in this latter group a small amount of amorphous or glassy matter is common. Other crystalline rocks, the metamorphic rocks such as marbles, mica-schists and quartzites, are recrystallized. This means that they were at first fragmental rocks like limestone, shale and sandstone and have never been in a molten condition nor entirely in solution, but the high temperature and pressure conditions of metamorphism have acted on them by erasing their original structures and inducing recrystallization in the solid state.\nOther rock crystals have formed out of precipitation from fluids, commonly water, to form druses or quartz veins. Evaporites such as halite, gypsum and some limestones have been deposited from aqueous solution, mostly owing to evaporation in arid climates.\nIce.\nWater-based ice in the form of snow, sea ice, and glaciers are common crystalline/polycrystalline structures on Earth and other planets. A single snowflake is a single crystal or a collection of crystals, while an ice cube is a polycrystal. Ice crystals may form from cooling liquid water below its freezing point, such as ice cubes or a frozen lake. Frost, snowflakes, or small ice crystals suspended in the air (ice fog) more often grow from a supersaturated gaseous-solution of water vapor and air, when the temperature of the air drops below its dew point, without passing through a liquid state. Another unusual property of water is that it expands rather than contracts when it crystallizes.\nOrganigenic crystals.\nMany living organisms are able to produce crystals grown from an aqueous solution, for example calcite and aragonite in the case of most molluscs or hydroxylapatite in the case of bones and teeth in vertebrates.\nPolymorphism and allotropy.\nThe same group of atoms can often solidify in many different ways. Polymorphism is the ability of a solid to exist in more than one crystal form. For example, water ice is ordinarily found in the hexagonal form Ice Ih, but can also exist as the cubic Ice Ic, the rhombohedral ice II, and many other forms. The different polymorphs are usually called different \"phases\".\nIn addition, the same atoms may be able to form noncrystalline phases. For example, water can also form amorphous ice, while SiO2 can form both fused silica (an amorphous glass) and quartz (a crystal). Likewise, if a substance can form crystals, it can also form polycrystals.\nFor pure chemical elements, polymorphism is known as allotropy. For example, diamond and graphite are two crystalline forms of carbon, while amorphous carbon is a noncrystalline form. Polymorphs, despite having the same atoms, may have very different properties. For example, diamond is the hardest substance known, while graphite is so soft that it is used as a lubricant. Chocolate can form six different types of crystals, but only one has the suitable hardness and melting point for candy bars and confections. Polymorphism in steel is responsible for its ability to be heat treated, giving it a wide range of properties.\nPolyamorphism is a similar phenomenon where the same atoms can exist in more than one amorphous solid form.\nCrystallization.\nCrystallization is the process of forming a crystalline structure from a fluid or from materials dissolved in a fluid. (More rarely, crystals may be deposited directly from gas; see: epitaxy and frost.)\nCrystallization is a complex and extensively-studied field, because depending on the conditions, a single fluid can solidify into many different possible forms. It can form a single crystal, perhaps with various possible phases, stoichiometries, impurities, defects, and habits. Or, it can form a polycrystal, with various possibilities for the size, arrangement, orientation, and phase of its grains. The final form of the solid is determined by the conditions under which the fluid is being solidified, such as the chemistry of the fluid, the ambient pressure, the temperature, and the speed with which all these parameters are changing.\nSpecific industrial techniques to produce large single crystals (called \"boules\") include the Czochralski process and the Bridgman technique. Other less exotic methods of crystallization may be used, depending on the physical properties of the substance, including hydrothermal synthesis, sublimation, or simply solvent-based crystallization.\nLarge single crystals can be created by geological processes. For example, selenite crystals in excess of 10\u00a0m are found in the Cave of the Crystals in Naica, Mexico. For more details on geological crystal formation, see above.\nCrystals can also be formed by biological processes, see above. Conversely, some organisms have special techniques to \"prevent\" crystallization from occurring, such as antifreeze proteins.\nDefects, impurities, and twinning.\nAn \"ideal\" crystal has every atom in a perfect, exactly repeating pattern. However, in reality, most crystalline materials have a variety of crystallographic defects: places where the crystal's pattern is interrupted. The types and structures of these defects may have a profound effect on the properties of the materials.\nA few examples of crystallographic defects include vacancy defects (an empty space where an atom should fit), interstitial defects (an extra atom squeezed in where it does not fit), and dislocations (see figure at right). Dislocations are especially important in materials science, because they help determine the mechanical strength of materials.\nAnother common type of crystallographic defect is an impurity, meaning that the \"wrong\" type of atom is present in a crystal. For example, a perfect crystal of diamond would only contain carbon atoms, but a real crystal might perhaps contain a few boron atoms as well. These boron impurities change the diamond's color to slightly blue. Likewise, the only difference between ruby and sapphire is the type of impurities present in a corundum crystal.\nIn semiconductors, a special type of impurity, called a dopant, drastically changes the crystal's electrical properties. Semiconductor devices, such as transistors, are made possible largely by putting different semiconductor dopants into different places, in specific patterns.\nTwinning is a phenomenon somewhere between a crystallographic defect and a grain boundary. Like a grain boundary, a twin boundary has different crystal orientations on its two sides. But unlike a grain boundary, the orientations are not random, but related in a specific, mirror-image way.\nMosaicity is a spread of crystal plane orientations. A mosaic crystal consists of smaller crystalline units that are somewhat misaligned with respect to each other.\nChemical bonds.\nIn general, solids can be held together by various types of chemical bonds, such as metallic bonds, ionic bonds, covalent bonds, van der Waals bonds, and others. None of these are necessarily crystalline or non-crystalline. However, there are some general trends as follows:\nMetals crystallize rapidly and are almost always polycrystalline, though there are exceptions like amorphous metal and single-crystal metals. The latter are grown synthetically, for example, fighter-jet turbines are typically made by first growing a single crystal of titanium alloy, increasing its strength and melting point over polycrystalline titanium. A small piece of metal may naturally form into a single crystal, such as Type 2 telluric iron, but larger pieces generally do not unless extremely slow cooling occurs. For example, iron meteorites are often composed of single crystal, or many large crystals that may be several meters in size, due to very slow cooling in the vacuum of space. The slow cooling may allow the precipitation of a separate phase within the crystal lattice, which form at specific angles determined by the lattice, called Widmanstatten patterns.\nIonic compounds typically form when a metal reacts with a non-metal, such as sodium with chlorine. These often form substances called salts, such as sodium chloride (table salt) or potassium nitrate (saltpeter), with crystals that are often brittle and cleave relatively easily. Ionic materials are usually crystalline or polycrystalline. In practice, large salt crystals can be created by solidification of a molten fluid, or by crystallization out of a solution. Some ionic compounds can be very hard, such as oxides like aluminium oxide found in many gemstones such as ruby and synthetic sapphire.\nCovalently bonded solids (sometimes called covalent network solids) are typically formed from one or more non-metals, such as carbon or silicon and oxygen, and are often very hard, rigid, and brittle. These are also very common, notable examples being diamond and quartz respectively.\nWeak van der Waals forces also help hold together certain crystals, such as crystalline molecular solids, as well as the interlayer bonding in graphite. Substances such as fats, lipids and wax form molecular bonds because the large molecules do not pack as tightly as atomic bonds. This leads to crystals that are much softer and more easily pulled apart or broken. Common examples include chocolates, candles, or viruses. Water ice and dry ice are examples of other materials with molecular bonding.Polymer materials generally will form crystalline regions, but the lengths of the molecules usually prevent complete crystallization\u2014and sometimes polymers are completely amorphous.\nQuasicrystals.\nA quasicrystal consists of arrays of atoms that are ordered but not strictly periodic. They have many attributes in common with ordinary crystals, such as displaying a discrete pattern in x-ray diffraction, and the ability to form shapes with smooth, flat faces.\nQuasicrystals are most famous for their ability to show five-fold symmetry, which is impossible for an ordinary periodic crystal (see crystallographic restriction theorem).\nThe International Union of Crystallography has redefined the term \"crystal\" to include both ordinary periodic crystals and quasicrystals (\"any solid having an essentially discrete diffraction diagram\").\nQuasicrystals, first discovered in 1982, are quite rare in practice. Only about 100 solids are known to form quasicrystals, compared to about 400,000 periodic crystals known in 2004. The 2011 Nobel Prize in Chemistry was awarded to Dan Shechtman for the discovery of quasicrystals.\nSpecial properties from anisotropy.\nCrystals can have certain special electrical, optical, and mechanical properties that glass and polycrystals normally cannot. These properties are related to the anisotropy of the crystal, i.e. the lack of rotational symmetry in its atomic arrangement. One such property is the piezoelectric effect, where a voltage across the crystal can shrink or stretch it. Another is birefringence, where a double image appears when looking through a crystal. Moreover, various properties of a crystal, including electrical conductivity, electrical permittivity, and Young's modulus, may be different in different directions in a crystal. For example, graphite crystals consist of a stack of sheets, and although each individual sheet is mechanically very strong, the sheets are rather loosely bound to each other. Therefore, the mechanical strength of the material is quite different depending on the direction of stress.\nNot all crystals have all of these properties. Conversely, these properties are not quite exclusive to crystals. They can appear in glasses or polycrystals that have been made anisotropic by working or stress\u2014for example, stress-induced birefringence.\nCrystallography.\n\"Crystallography\" is the science of measuring the crystal structure (in other words, the atomic arrangement) of a crystal. One widely used crystallography technique is X-ray diffraction. Large numbers of known crystal structures are stored in crystallographic databases."}
{"id": "6016", "revid": "5128741", "url": "https://en.wikipedia.org/wiki?curid=6016", "title": "Cytosine", "text": "Cytosine () (symbol C or Cyt) is one of the four nucleotide bases found in DNA and RNA, along with adenine, guanine, and thymine (uracil in RNA). It is a pyrimidine derivative, with a heterocyclic aromatic ring and two substituents attached (an amine group at position 4 and a keto group at position 2). The nucleoside of cytosine is cytidine. In Watson\u2013Crick base pairing, it forms three hydrogen bonds with guanine.\nHistory.\nCytosine was discovered and named by Albrecht Kossel and Albert Neumann in 1894 when it was hydrolyzed from calf thymus tissues. A structure was proposed in 1903, and was synthesized (and thus confirmed) in the laboratory in the same year.\nIn 1998, cytosine was used in an early demonstration of quantum information processing when Oxford University researchers implemented the Deutsch\u2013Jozsa algorithm on a two qubit nuclear magnetic resonance quantum computer (NMRQC).\nIn March 2015, NASA scientists reported the formation of cytosine, along with uracil and thymine, from pyrimidine under the space-like laboratory conditions, which is of interest because pyrimidine has been found in meteorites although its origin is unknown.\nChemical reactions.\nCytosine can be found as part of DNA, as part of RNA, or as a part of a nucleotide. As cytidine triphosphate (CTP), it can act as a co-factor to enzymes, and can transfer a phosphate to convert adenosine diphosphate (ADP) to adenosine triphosphate (ATP).\nIn DNA and RNA, cytosine is paired with guanine. However, it is inherently unstable, and can change into uracil (spontaneous deamination). This can lead to a point mutation if not repaired by the DNA repair enzymes such as uracil glycosylase, which cleaves a uracil in DNA.\nCytosine can also be methylated into 5-methylcytosine by an enzyme called DNA methyltransferase or be methylated and hydroxylated to make 5-hydroxymethylcytosine. The difference in rates of deamination of cytosine and 5-methylcytosine (to uracil and thymine) forms the basis of bisulfite sequencing.\nBiological function.\nWhen found third in a codon of RNA, cytosine is synonymous with uracil, as they are interchangeable as the third base.\nWhen found as the second base in a codon, the third is always interchangeable. For example, UCU, UCC, UCA and UCG are all serine, regardless of the third base.\nActive enzymatic deamination of cytosine or 5-methylcytosine by the APOBEC family of cytosine deaminases could have both beneficial and detrimental implications on various cellular processes as well as on organismal evolution. The implications of deamination on 5-hydroxymethylcytosine, on the other hand, remains less understood.\nTheoretical aspects.\nUntil October 2021, Cytosine had not been found in meteorites, which suggested the first strands of RNA and DNA had to look elsewhere to obtain this building block. Cytosine likely formed within some meteorite parent bodies, however did not persist within these bodies due to an effective deamination reaction into uracil.\nIn October 2021, Cytosine was announced as having been found in meteorites by researchers in a joint Japan/NASA project, that used novel methods of detection which avoided damaging nucleotides as they were extracted from meteorites."}
{"id": "6017", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=6017", "title": "Cruise Missile", "text": ""}
{"id": "6018", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=6018", "title": "Call Of Cthulhu", "text": ""}
{"id": "6019", "revid": "712163", "url": "https://en.wikipedia.org/wiki?curid=6019", "title": "Computational chemistry", "text": "Computational chemistry is a branch of chemistry that uses computer simulations to assist in solving chemical problems. It uses methods of theoretical chemistry incorporated into computer programs to calculate the structures and properties of molecules, groups of molecules, and solids. The importance of this subject stems from the fact that, with the exception of some relatively recent findings related to the hydrogen molecular ion (dihydrogen cation), achieving an accurate quantum mechanical depiction of chemical systems analytically, or in a closed form, is not feasible. The complexity inherent in the many-body problem exacerbates the challenge of providing detailed descriptions of quantum mechanical systems. While computational results normally complement information obtained by chemical experiments, it can occasionally predict unobserved chemical phenomena.\nOverview.\nComputational chemistry differs from theoretical chemistry, which involves a mathematical description of chemistry. However, computational chemistry involves the usage of computer programs and additional mathematical skills in order to accurately model various chemical problems. In theoretical chemistry, chemists, physicists, and mathematicians develop algorithms and computer programs to predict atomic and molecular properties and reaction paths for chemical reactions. Computational chemists, in contrast, may simply apply existing computer programs and methodologies to specific chemical questions.\nHistorically, computational chemistry has had two different aspects:\nThese aspects, along with computational chemistry's purpose, have resulted in a whole host of algorithms.\nHistory.\nBuilding on the founding discoveries and theories in the history of quantum mechanics, the first theoretical calculations in chemistry were those of Walter Heitler and Fritz London in 1927, using valence bond theory. The books that were influential in the early development of computational quantum chemistry include Linus Pauling and E. Bright Wilson's 1935 \"Introduction to Quantum Mechanics \u2013 with Applications to Chemistry\", Eyring, Walter and Kimball's 1944 \"Quantum Chemistry\", Heitler's 1945 \"Elementary Wave Mechanics \u2013 with Applications to Quantum Chemistry\", and later Coulson's 1952 textbook \"Valence\", each of which served as primary references for chemists in the decades to follow.\nWith the development of efficient computer technology in the 1940s, the solutions of elaborate wave equations for complex atomic systems began to be a realizable objective. In the early 1950s, the first semi-empirical atomic orbital calculations were performed. Theoretical chemists became extensive users of the early digital computers. One significant advancement was marked by Clemens C. J. Roothaan's 1951 paper in the Reviews of Modern Physics. This paper focused largely on the \"LCAO MO\" approach (Linear Combination of Atomic Orbitals Molecular Orbitals). For many years, it was the second-most cited paper in that journal. A very detailed account of such use in the United Kingdom is given by Smith and Sutcliffe. The first \"ab initio\" Hartree\u2013Fock method calculations on diatomic molecules were performed in 1956 at MIT, using a basis set of Slater orbitals. For diatomic molecules, a systematic study using a minimum basis set and the first calculation with a larger basis set were published by Ransil and Nesbet respectively in 1960. The first polyatomic calculations using Gaussian orbitals were performed in the late 1950s. The first configuration interaction calculations were performed in Cambridge on the EDSAC computer in the 1950s using Gaussian orbitals by Boys and coworkers. By 1971, when a bibliography of \"ab initio\" calculations was published, the largest molecules included were naphthalene and azulene. Abstracts of many earlier developments in \"ab initio\" theory have been published by Schaefer.\nIn 1964, H\u00fcckel method calculations (using a simple linear combination of atomic orbitals (LCAO) method to determine electron energies of molecular orbitals of \u03c0 electrons in conjugated hydrocarbon systems) of molecules, ranging in complexity from butadiene and benzene to ovalene, were generated on computers at Berkeley and Oxford. These empirical methods were replaced in the 1960s by semi-empirical methods such as CNDO.\nIn the early 1970s, efficient \"ab initio\" computer programs such as ATMOL, Gaussian, IBMOL, and POLYAYTOM, began to be used to speed \"ab initio\" calculations of molecular orbitals. Of these four programs, only Gaussian, now vastly expanded, is still in use, but many other programs are now in use. At the same time, the methods of molecular mechanics, such as MM2 force field, were developed, primarily by Norman Allinger.\nOne of the first mentions of the term \"computational chemistry\" can be found in the 1970 book \"Computers and Their Role in the Physical Sciences\" by Sidney Fernbach and Abraham Haskell Taub, where they state \"It seems, therefore, that 'computational chemistry' can finally be more and more of a reality.\" During the 1970s, widely different methods began to be seen as part of a new emerging discipline of \"computational chemistry\". The \"Journal of Computational Chemistry\" was first published in 1980.\nComputational chemistry has featured in several Nobel Prize awards, most notably in 1998 and 2013. Walter Kohn, \"for his development of the density-functional theory\", and John Pople, \"for his development of computational methods in quantum chemistry\", received the 1998 Nobel Prize in Chemistry. Martin Karplus, Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for \"the development of multiscale models for complex chemical systems\".\nApplications.\nThere are several fields within computational chemistry.\nThese fields can give rise to several applications as shown below.\nCatalysis.\nComputational chemistry is a tool for analyzing catalytic systems without doing experiments. Modern electronic structure theory and density functional theory has allowed researchers to discover and understand catalysts. Computational studies apply theoretical chemistry to catalysis research. Density functional theory methods calculate the energies and orbitals of molecules to give models of those structures. Using these methods, researchers can predict values like activation energy, site reactivity and other thermodynamic properties.\nData that is difficult to obtain experimentally can be found using computational methods to model the mechanisms of catalytic cycles. Skilled computational chemists provide predictions that are close to experimental data with proper considerations of methods and basis sets. With good computational data, researchers can predict how catalysts can be improved to lower the cost and increase the efficiency of these reactions.\nDrug development.\nComputational chemistry is used in drug development to model potentially useful drug molecules and help companies save time and cost in drug development. The drug discovery process involves analyzing data, finding ways to improve current molecules, finding synthetic routes, and testing those molecules. Computational chemistry helps with this process by giving predictions of which experiments would be best to do without conducting other experiments. Computational methods can also find values that are difficult to find experimentally like pKa's of compounds. Methods like density functional theory can be used to model drug molecules and find their properties, like their HOMO and LUMO energies and molecular orbitals. Computational chemists also help companies with developing informatics, infrastructure and designs of drugs.\nAside from drug synthesis, drug carriers are also researched by computational chemists for nanomaterials. It allows researchers to simulate environments to test the effectiveness and stability of drug carriers. Understanding how water interacts with these nanomaterials ensures stability of the material in human bodies. These computational simulations help researchers optimize the material find the best way to structure these nanomaterials before making them.\nComputational chemistry databases.\nDatabases are useful for both computational and non computational chemists in research and verifying the validity of computational methods. Empirical data is used to analyze the error of computational methods against experimental data. Empirical data helps researchers with their methods and basis sets to have greater confidence in the researchers results. Computational chemistry databases are also used in testing software or hardware for computational chemistry.\nDatabases can also use purely calculated data. Purely calculated data uses calculated values over experimental values for databases. Purely calculated data avoids dealing with these adjusting for different experimental conditions like zero-point energy. These calculations can also avoid experimental errors for difficult to test molecules. Though purely calculated data is often not perfect, identifying issues is often easier for calculated data than experimental.\nDatabases also give public access to information for researchers to use. They contain data that other researchers have found and uploaded to these databases so that anyone can search for them. Researchers use these databases to find information on molecules of interest and learn what can be done with those molecules. Some publicly available chemistry databases include the following.\nMethods.\n\"Ab initio\" method.\nThe programs used in computational chemistry are based on many different quantum-chemical methods that solve the molecular Schr\u00f6dinger equation associated with the molecular Hamiltonian. Methods that do not include any empirical or semi-empirical parameters in their equations\u00a0\u2013 being derived directly from theory, with no inclusion of experimental data\u00a0\u2013 are called \"ab initio methods\". A theoretical approximation is rigorously defined on first principles and then solved within an error margin that is qualitatively known beforehand. If numerical iterative methods must be used, the aim is to iterate until full machine accuracy is obtained (the best that is possible with a finite word length on the computer, and within the mathematical and/or physical approximations made).\nAb initio methods need to define a level of theory (the method) and a basis set. A basis set consists of functions centered on the molecule's atoms. These sets are then used to describe molecular orbitals via the linear combination of atomic orbitals (LCAO) molecular orbital method ansatz.\nA common type of \"ab initio\" electronic structure calculation is the Hartree\u2013Fock method (HF), an extension of molecular orbital theory, where electron-electron repulsions in the molecule are not specifically taken into account; only the electrons' average effect is included in the calculation. As the basis set size increases, the energy and wave function tend towards a limit called the Hartree\u2013Fock limit.\nMany types of calculations begin with a Hartree\u2013Fock calculation and subsequently correct for electron-electron repulsion, referred to also as electronic correlation. These types of calculations are termed post-Hartree\u2013Fock methods. By continually improving these methods, scientists can get increasingly closer to perfectly predicting the behavior of atomic and molecular systems under the framework of quantum mechanics, as defined by the Schr\u00f6dinger equation. To obtain exact agreement with the experiment, it is necessary to include specific terms, some of which are far more important for heavy atoms than lighter ones.\nIn most cases, the Hartree\u2013Fock wave function occupies a single configuration or determinant. In some cases, particularly for bond-breaking processes, this is inadequate, and several configurations must be used.\nThe total molecular energy can be evaluated as a function of the molecular geometry; in other words, the potential energy surface. Such a surface can be used for reaction dynamics. The stationary points of the surface lead to predictions of different isomers and the transition structures for conversion between isomers, but these can be determined without full knowledge of the complete surface.\nComputational thermochemistry.\nA particularly important objective, called computational thermochemistry, is to calculate thermochemical quantities such as the enthalpy of formation to chemical accuracy. Chemical accuracy is the accuracy required to make realistic chemical predictions and is generally considered to be 1\u00a0kcal/mol or 4\u00a0kJ/mol. To reach that accuracy in an economic way, it is necessary to use a series of post-Hartree\u2013Fock methods and combine the results. These methods are called quantum chemistry composite methods.\nChemical dynamics.\nAfter the electronic and nuclear variables are separated within the Born\u2013Oppenheimer representation), the wave packet corresponding to the nuclear degrees of freedom is propagated via the time evolution operator (physics) associated to the time-dependent Schr\u00f6dinger equation (for the full molecular Hamiltonian). In the complementary energy-dependent approach, the time-independent Schr\u00f6dinger equation is solved using the scattering theory formalism. The potential representing the interatomic interaction is given by the potential energy surfaces. In general, the potential energy surfaces are coupled via the vibronic coupling terms.\nThe most popular methods for propagating the wave packet associated to the molecular geometry are:\nSplit operator technique.\nHow a computational method solves quantum equations impacts the accuracy and efficiency of the method. The split operator technique is one of these methods for solving differential equations. In computational chemistry, split operator technique reduces computational costs of simulating chemical systems. Computational costs are about how much time it takes for computers to calculate these chemical systems, as it can take days for more complex systems. Quantum systems are difficult and time-consuming to solve for humans. Split operator methods help computers calculate these systems quickly by solving the sub problems in a quantum differential equation. The method does this by separating the differential equation into two different equations, like when there are more than two operators. Once solved, the split equations are combined into one equation again to give an easily calculable solution.\nThis method is used in many fields that require solving differential equations, such as biology. However, the technique comes with a splitting error. For example, with the following solution for a differential equation.\nformula_1\nThe equation can be split, but the solutions will not be exact, only similar. This is an example of first order splitting.\nformula_2\nThere are ways to reduce this error, which include taking an average of two split equations.\nAnother way to increase accuracy is to use higher order splitting. Usually, second order splitting is the most that is done because higher order splitting requires much more time to calculate and is not worth the cost. Higher order methods become too difficult to implement, and are not useful for solving differential equations despite the higher accuracy.\nComputational chemists spend much time making systems calculated with split operator technique more accurate while minimizing the computational cost. Calculating methods is a massive challenge for many chemists trying to simulate molecules or chemical environments.\nDensity functional methods.\nDensity functional theory (DFT) methods are often considered to be \"ab initio methods\" for determining the molecular electronic structure, even though many of the most common functionals use parameters derived from empirical data, or from more complex calculations. In DFT, the total energy is expressed in terms of the total one-electron density rather than the wave function. In this type of calculation, there is an approximate Hamiltonian and an approximate expression for the total electron density. DFT methods can be very accurate for little computational cost. Some methods combine the density functional exchange functional with the Hartree\u2013Fock exchange term and are termed hybrid functional methods.\nSemi-empirical methods.\nSemi-empirical quantum chemistry methods are based on the Hartree\u2013Fock method formalism, but make many approximations and obtain some parameters from empirical data. They were very important in computational chemistry from the 60s to the 90s, especially for treating large molecules where the full Hartree\u2013Fock method without the approximations were too costly. The use of empirical parameters appears to allow some inclusion of correlation effects into the methods.\nPrimitive semi-empirical methods were designed even before, where the two-electron part of the Hamiltonian is not explicitly included. For \u03c0-electron systems, this was the H\u00fcckel method proposed by Erich H\u00fcckel, and for all valence electron systems, the extended H\u00fcckel method proposed by Roald Hoffmann. Sometimes, H\u00fcckel methods are referred to as \"completely empirical\" because they do not derive from a Hamiltonian. Yet, the term \"empirical methods\", or \"empirical force fields\" is usually used to describe molecular mechanics.\nMolecular mechanics.\nIn many cases, large molecular systems can be modeled successfully while avoiding quantum mechanical calculations entirely. Molecular mechanics simulations, for example, use one classical expression for the energy of a compound, for instance, the harmonic oscillator. All constants appearing in the equations must be obtained beforehand from experimental data or \"ab initio\" calculations.\nThe database of compounds used for parameterization, i.e. the resulting set of parameters and functions is called the force field, is crucial to the success of molecular mechanics calculations. A force field parameterized against a specific class of molecules, for instance, proteins, would be expected to only have any relevance when describing other molecules of the same class. These methods can be applied to proteins and other large biological molecules, and allow studies of the approach and interaction (docking) of potential drug molecules.\nMolecular dynamics.\nMolecular dynamics (MD) use either quantum mechanics, molecular mechanics or a mixture of both to calculate forces which are then used to solve Newton's laws of motion to examine the time-dependent behavior of systems. The result of a molecular dynamics simulation is a trajectory that describes how the position and velocity of particles varies with time. The phase point of a system described by the positions and momenta of all its particles on a previous time point will determine the next phase point in time by integrating over Newton's laws of motion.\nMonte Carlo.\nMonte Carlo (MC) generates configurations of a system by making random changes to the positions of its particles, together with their orientations and conformations where appropriate. It is a random sampling method, which makes use of the so-called \"importance sampling\". Importance sampling methods are able to generate low energy states, as this enables properties to be calculated accurately. The potential energy of each configuration of the system can be calculated, together with the values of other properties, from the positions of the atoms.\nQuantum mechanics/molecular mechanics (QM/MM).\nQM/MM is a hybrid method that attempts to combine the accuracy of quantum mechanics with the speed of molecular mechanics. It is useful for simulating very large molecules such as enzymes.\nQuantum Computational Chemistry.\nQuantum computational chemistry aims to exploit quantum computing to simulate chemical systems, distinguishing itself from the QM/MM (Quantum Mechanics/Molecular Mechanics) approach. While QM/MM uses a hybrid approach, combining quantum mechanics for a portion of the system with classical mechanics for the remainder, quantum computational chemistry exclusively uses quantum computing methods to represent and process information, such as Hamiltonian operators.\nConventional computational chemistry methods often struggle with the complex quantum mechanical equations, particularly due to the exponential growth of a quantum system's wave function. Quantum computational chemistry addresses these challenges using quantum computing methods, such as qubitization and quantum phase estimation, which are believed to offer scalable solutions.\nQubitization involves adapting the Hamiltonian operator for more efficient processing on quantum computers, enhancing the simulation's efficiency. Quantum phase estimation, on the other hand, assists in accurately determining energy eigenstates, which are critical for understanding the quantum system's behavior.\nWhile these techniques have advanced the field of computational chemistry, especially in the simulation of chemical systems, their practical application is currently limited mainly to smaller systems due to technological constraints. Nevertheless, these developments may lead to significant progress towards achieving more precise and resource-efficient quantum chemistry simulations.\nComputational costs in chemistry algorithms.\nThe computational cost and algorithmic complexity in chemistry are used to help understand and predict chemical phenomena. They help determine which algorithms/computational methods to use when solving chemical problems. This section focuses on the scaling of computational complexity with molecule size and details the algorithms commonly used in both domains.\nIn quantum chemistry, particularly, the complexity can grow exponentially with the number of electrons involved in the system. This exponential growth is a significant barrier to simulating large or complex systems accurately.\nAdvanced algorithms in both fields strive to balance accuracy with computational efficiency. For instance, in MD, methods like Verlet integration or Beeman's algorithm are employed for their computational efficiency. In quantum chemistry, hybrid methods combining different computational approaches (like QM/MM) are increasingly used to tackle large biomolecular systems.\nAlgorithmic complexity examples.\nThe following list illustrates the impact of computational complexity on algorithms used in chemical computations. It is important to note that while this list provides key examples, it is not comprehensive and serves as a guide to understanding how computational demands influence the selection of specific computational methods in chemistry.\nMolecular dynamics.\nAlgorithm.\nSolves Newton's equations of motion for atoms and molecules.\nComplexity.\nThe standard pairwise interaction calculation in MD leads to an formula_3complexity for formula_4 particles. This is because each particle interacts with every other particle, resulting in formula_5 interactions. Advanced algorithms, such as the Ewald summation or Fast Multipole Method, reduce this to formula_6 or even formula_7 by grouping distant particles and treating them as a single entity or using clever mathematical approximations.\nQuantum mechanics/molecular mechanics (QM/MM).\nAlgorithm.\nCombines quantum mechanical calculations for a small region with molecular mechanics for the larger environment.\nComplexity.\nThe complexity of QM/MM methods depends on both the size of the quantum region and the method used for quantum calculations. For example, if a Hartree-Fock method is used for the quantum part, the complexity can be approximated as formula_8, where formula_9 is the number of basis functions in the quantum region. This complexity arises from the need to solve a set of coupled equations iteratively until self-consistency is achieved.\nHartree-Fock method.\nAlgorithm.\nFinds a single Fock state that minimizes the energy.\nComplexity.\nNP-hard or NP-complete as demonstrated by embedding instances of the Ising model into Hartree-Fock calculations. The Hartree-Fock method involves solving the Roothaan-Hall equations, which scales as formula_10 to formula_7 depending on implementation, with formula_4 being the number of basis functions. The computational cost mainly comes from evaluating and transforming the two-electron integrals. This proof of NP-hardness or NP-completeness comes from embedding problems like the Ising model into the Hartree-Fock formalism.\nDensity functional theory.\nAlgorithm.\nInvestigates the electronic structure or nuclear structure of many-body systems such as atoms, molecules, and the condensed phases.\nComplexity.\nTraditional implementations of DFT typically scale as formula_10, mainly due to the need to diagonalize the Kohn-Sham matrix. The diagonalization step, which finds the eigenvalues and eigenvectors of the matrix, contributes most to this scaling. Recent advances in DFT aim to reduce this complexity through various approximations and algorithmic improvements.\nStandard CCSD and CCSD(T) method.\nAlgorithm.\nCCSD and CCSD(T) methods are advanced electronic structure techniques involving single, double, and in the case of CCSD(T), perturbative triple excitations for calculating electronic correlation effects.\nComplexity.\nCCSD.\nScales as formula_14 where formula_9 is the number of basis functions. This intense computational demand arises from the inclusion of single and double excitations in the electron correlation calculation.\nCCSD(T).\nWith the addition of perturbative triples, the complexity increases to formula_16. This elevated complexity restricts practical usage to smaller systems, typically up to 20-25 atoms in conventional implementations.\nLinear-scaling CCSD(T) method.\nAlgorithm.\nAn adaptation of the standard CCSD(T) method using local natural orbitals (NOs) to significantly reduce the computational burden and enable application to larger systems.\nComplexity.\nAchieves linear scaling with the system size, a major improvement over the traditional fifth-power scaling of CCSD. This advancement allows for practical applications to molecules of up to 100 atoms with reasonable basis sets, marking a significant step forward in computational chemistry's capability to handle larger systems with high accuracy.\nProving the complexity classes for algorithms involves a combination of mathematical proof and computational experiments. For example, in the case of the Hartree-Fock method, the proof of NP-hardness is a theoretical result derived from complexity theory, specifically through reductions from known NP-hard problems.\nFor other methods like MD or DFT, the computational complexity is often empirically observed and supported by algorithm analysis. In these cases, the proof of correctness is less about formal mathematical proofs and more about consistently observing the computational behaviour across various systems and implementations.\nAccuracy.\nComputational chemistry is not an \"exact\" description of real-life chemistry, as the mathematical and physical models of nature can only provide an approximation. However, the majority of chemical phenomena can be described to a certain degree in a qualitative or approximate quantitative computational scheme.\nMolecules consist of nuclei and electrons, so the methods of quantum mechanics apply. Computational chemists often attempt to solve the non-relativistic Schr\u00f6dinger equation, with relativistic corrections added, although some progress has been made in solving the fully relativistic Dirac equation. In principle, it is possible to solve the Schr\u00f6dinger equation in either its time-dependent or time-independent form, as appropriate for the problem in hand; in practice, this is not possible except for very small systems. Therefore, a great number of approximate methods strive to achieve the best trade-off between accuracy and computational cost.\nAccuracy can always be improved with greater computational cost. Significant errors can present themselves in ab initio models comprising many electrons, due to the computational cost of full relativistic-inclusive methods. This complicates the study of molecules interacting with high atomic mass unit atoms, such as transitional metals and their catalytic properties. Present algorithms in computational chemistry can routinely calculate the properties of small molecules that contain up to about 40 electrons with errors for energies less than a few kJ/mol. For geometries, bond lengths can be predicted within a few picometers and bond angles within 0.5 degrees. The treatment of larger molecules that contain a few dozen atoms is computationally tractable by more approximate methods such as density functional theory (DFT).\nThere is some dispute within the field whether or not the latter methods are sufficient to describe complex chemical reactions, such as those in biochemistry. Large molecules can be studied by semi-empirical approximate methods. Even larger molecules are treated by classical mechanics methods that use what are called molecular mechanics (MM).In QM-MM methods, small parts of large complexes are treated quantum mechanically (QM), and the remainder is treated approximately (MM).\nSoftware packages.\nMany self-sufficient exist. Some include many methods covering a wide range, while others concentrate on a very specific range or even on one method. Details of most of them can be found in:"}
{"id": "6020", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=6020", "title": "Crash (Ballard novel)", "text": "Crash is a novel by British author J. G. Ballard, first published in 1973 with cover designed by Bill Botten. It follows a group of car-crash fetishists who, inspired by the famous crashes of celebrities, become sexually aroused by staging and participating in car accidents.\nThe novel was released to divided critical reception, with many reviewers horrified by its provocative content. It was adapted into a controversial 1996 film of the same name by David Cronenberg.\nSynopsis.\nThe story is told through the eyes of narrator James Ballard, named after the author himself, but it centers on the sinister figure of Dr. Robert Vaughan, a former TV scientist turned \"nightmare angel of the highways\". James meets Vaughan after being injured in a car crash near London Airport. Gathering around Vaughan is a group of alienated people, all of them former crash victims, who follow him in his pursuit to re-enact the crashes of Hollywood celebrities such as Jayne Mansfield and James Dean, in order to experience what the narrator calls \"a new sexuality, born from a perverse technology\". Vaughan's ultimate fantasy is to die in a head-on collision with movie star Elizabeth Taylor.\nDevelopment.\nThe Papers of J. G. Ballard at the British Library include two revised drafts of \"Crash\" (Add MS 88938/3/8). Scanned extracts from Ballard's drafts are included in \"Crash: The Collector's Edition,\" ed. Chris Beckett.\nIn 1971, Harley Cokeliss directed a short film entitled \"Crash!\" based on a chapter in J. G. Ballard's book \"The Atrocity Exhibition\", where Ballard is featured, talking about the ideas in his book. British actress Gabrielle Drake appeared as a passenger and car-crash victim. Ballard later developed the idea, resulting in \"Crash\". In his draft of the novel he mentioned Drake by name, but references to her were removed from the published version. \nInterpretation.\n\"Crash\" has been difficult to characterize as a novel. At some points in his career, Ballard claimed that \"Crash\" was a \"cautionary tale\", a view that he would later regret, asserting that it is in fact \"a psychopathic hymn. But it is a psychopathic hymn which has a point\". Likewise, Ballard previously characterized it a science fiction novel, a position he would later take back.\nJean Baudrillard wrote an analysis of \"Crash\" in \"Simulacra and Simulation\" in which he declared it \"the first great novel of the universe of simulation\". He made note of how the fetish in the story conflates the functionality of the automobiles with that of the human body and how the characters' injuries and the damage to the vehicles are used as equivalent signs. To him, the hyperfunctionality leads to the dysfunction in the story. Quotes were used extensively to illustrate that the language of the novel employs plain, mechanical terms for the parts of the automobile and proper, medical language for human sex organs and acts. The story is interpreted as showing a merger between technology, sexuality, and death, and he further argued that by pointing out Vaughan's character takes and keeps photos of the car crashes and the mutilated bodies involved. Baudrillard stated that there is no moral judgment about the events within the novel but that Ballard himself intended it as a warning against a cultural trend.\nThe story can be classed as dystopic.\nCritical reception.\nThe novel received divided reviews when originally published. One publisher's reader returned the verdict \"This author is beyond psychiatric help. Do Not Publish!\" A 1973 review in \"The New York Times\" was equally horrified: \"\"Crash\" is, hands-down, the most repulsive book I've yet to come across.\"\nHowever, retrospective opinion now considers \"Crash\" to be one of Ballard's best and most challenging works. Reassessing \"Crash\" in \"The Guardian\", Zadie Smith wrote, \"C\"rash\" is an existential book about how \"everybody uses everything\". How everything uses everybody. And yet it is not a hopeless vision.\" On Ballard's legacy, she writes: \"In Ballard's work there is always this mix of futuristic dread and excitement, a sweet spot where dystopia and utopia converge. For we cannot say we haven't got precisely what we dreamed of, what we always wanted, so badly.\"\nReferences in popular art.\nMusic.\nThe Normal's 1978 song \"Warm Leatherette\" was inspired by the novel as was \"Miss the Girl,\" a 1983 single by The Creatures.\nThe Manic Street Preachers' song \"Mausoleum\" from 1994's \"The Holy Bible\" contains the famous Ballard quote about his reasons for writing the book, \"I wanted to rub the human face in its own vomit. I wanted to force it to look in the mirror.\" John Foxx's album \"Metamatic\" contains songs that have Ballardian themes, such as \"No-one Driving\".\nOther film adaptations.\nAn apparently unauthorized adaptation of \"Crash\" called \"Nightmare Angel\" was filmed in 1986 by Susan Emerling and Zoe Beloff. This short film bears the credit \"Inspired by J. G. Ballard\"."}
{"id": "6021", "revid": "6036800", "url": "https://en.wikipedia.org/wiki?curid=6021", "title": "C (programming language)", "text": "C (\"pronounced\" \" \u2013 like the letter c\") is a general-purpose programming language. It was created in the 1970s by Dennis Ritchie and remains very widely used and influential. By design, C's features cleanly reflect the capabilities of the targeted CPUs. It has found lasting use in operating systems code (especially in kernels), device drivers, and protocol stacks, but its use in application software has been decreasing. C is commonly used on computer architectures that range from the largest supercomputers to the smallest microcontrollers and embedded systems.\nA successor to the programming language B, C was originally developed at Bell Labs by Ritchie between 1972 and 1973 to construct utilities running on Unix. It was applied to re-implementing the kernel of the Unix operating system. During the 1980s, C gradually gained popularity. It has become one of the most widely used programming languages, with C compilers available for practically all modern computer architectures and operating systems. The book \"The C Programming Language\", co-authored by the original language designer, served for many years as the \"de facto\" standard for the language. C has been standardized since 1989 by the American National Standards Institute (ANSI) and, subsequently, jointly by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC).\nC is an imperative procedural language, supporting structured programming, lexical variable scope, and recursion, with a static type system. It was designed to be compiled to provide low-level access to memory and language constructs that map efficiently to machine instructions, all with minimal runtime support. Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant C program written with portability in mind can be compiled for a wide variety of computer platforms and operating systems with few changes to its source code.\nSince 2000, C has consistently ranked among the top four languages in the TIOBE index, a measure of the popularity of programming languages.\nOverview.\nC is an imperative, procedural language in the ALGOL tradition. It has a static type system. In C, all executable code is contained within subroutines (also called \"functions\", though not in the sense of functional programming). Function parameters are passed by value, although arrays are passed as pointers, i.e. the address of the first item in the array. \"Pass-by-reference\" is simulated in C by explicitly passing pointers to the thing being referenced.\nC program source text is free-form code. Semicolons terminate statements, while curly braces are used to group statements into blocks.\nThe C language also exhibits the following characteristics:\nWhile C does not include certain features found in other languages (such as object orientation and garbage collection), these can be implemented or emulated, often through the use of external libraries (e.g., the GLib Object System or the Boehm garbage collector).\nRelations to other languages.\nMany later languages have borrowed directly or indirectly from C, including C++, C#, Unix's C shell, D, Go, Java, JavaScript (including transpilers), Julia, Limbo, LPC, Objective-C, Perl, PHP, Python, Ruby, Rust, Swift, Verilog and SystemVerilog (hardware description languages). These languages have drawn many of their control structures and other basic features from C. Most of them also express highly similar syntax to C, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.\nHistory.\nEarly developments.\nThe origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was also developed in assembly language.\nB.\nThompson wanted a programming language for developing utilities for the new platform. He first tried writing a Fortran compiler, but he soon gave up the idea and instead created a cut-down version of the recently developed systems programming language called BCPL. The official description of BCPL was not available at the time, and Thompson modified the syntax to be less 'wordy' and similar to a simplified ALGOL known as SMALGOL. He called the result \"B\", describing it as \"BCPL semantics with a lot of SMALGOL syntax\". Like BCPL, B had a bootstrapping compiler to facilitate porting to new machines. Ultimately, few utilities were written in B because it was too slow and could not take advantage of PDP-11 features such as byte addressability.\nNew B and first C release.\nIn 1971 Ritchie started to improve B, to use the features of the more-powerful PDP-11. A significant addition was a character data type. He called this \"New B\" (NB). Thompson started to use NB to write the Unix kernel, and his requirements shaped the direction of the language development. Through to 1972, richer types were added to the NB language: NB had arrays of codice_12 and codice_13. Pointers, the ability to generate pointers to other types, arrays of all types, and types to be returned from functions were all also added. Arrays within expressions became pointers. A new compiler was written, and the language was renamed C.\nThe C compiler and some utilities made with it were included in Version 2 Unix, which is also known as Research Unix.\nStructures and Unix kernel re-write.\nAt Version 4 Unix, released in November 1973, the Unix kernel was extensively re-implemented in C. By this time, the C language had acquired some powerful features such as codice_6 types.\nThe preprocessor was introduced around 1973 at the urging of Alan Snyder and also in recognition of the usefulness of the file-inclusion mechanisms available in BCPL and PL/I. Its original version provided only included files and simple string replacements: codice_15 and codice_16 of parameterless macros. Soon after that, it was extended, mostly by Mike Lesk and then by John Reiser, to incorporate macros with arguments and conditional compilation.\nUnix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system (which was written in PL/I) and Master Control Program (MCP) for the Burroughs B5000 (which was written in ALGOL) in 1961. In around 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.\nK&amp;R C.\nIn 1978 Brian Kernighan and Dennis Ritchie published the first edition of \"The C Programming Language\". Known as \"K&amp;R\" from the initials of its authors, the book served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as \"K&amp;R C\". As this was released in 1978, it is now also referred to as \"C78\". The second edition of the book covers the later ANSI C standard, described below.\n\"K&amp;R\" introduced several language features:\nEven after the publication of the 1989 ANSI standard, for many years K&amp;R C was still considered the \"lowest common denominator\" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&amp;R C code can be legal Standard C as well.\nIn early versions of C, only functions that return types other than codice_12 must be declared if used before the function definition; functions used without prior declaration were presumed to return type codice_12.\nFor example:\nlong some_function(); /* This is a function declaration, so the compiler can know the name and return type of this function. */\n/* int */ other_function(); /* Another function declaration. Because this is an early version of C, there is an implicit 'int' type here. A comment shows where the explicit 'int' type specifier would be required in later versions. */\n/* int */ calling_function() /* This is a function definition, including the body of the code following in the { curly brackets }. Because no return type is specified, the function implicitly returns an 'int' in this early version of C. */\n long test1;\n register /* int */ test2; /* Again, note that 'int' is not required here. The 'int' type specifier */\n /* in the comment would be required in later versions of C. */\n /* The 'register' keyword indicates to the compiler that this variable should */\n /* ideally be stored in a register as opposed to within the stack frame. */\n test1 = some_function();\n if (test1 &gt; 1)\n test2 = 0;\n else\n test2 = other_function();\n return test2;\nThe codice_12 type specifiers which are commented out could be omitted in K&amp;R C, but are required in later standards.\nSince K&amp;R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if different calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.\nIn the years following the publication of K&amp;R C, several features were added to the language, supported by compilers from AT&amp;T (in particular PCC) and some other vendors. These included:\nThe large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&amp;R specification, led to the necessity of standardization.\nANSI C and ISO C.\nDuring the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.\nIn 1983 the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 \"Programming Language C\". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.\nIn 1990 the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms \"C89\" and \"C90\" refer to the same programming language.\nANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.\nOne of the aims of the C standardization process was to produce a superset of K&amp;R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), codice_9 pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&amp;R interface continued to be permitted, for compatibility with existing source code.\nC89 is supported by current C compilers, and most modern C code is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.\nIn cases where code must be compilable by either standard-conforming or K&amp;R C-based compilers, the codice_37 macro can be used to split the code into Standard and K&amp;R sections to prevent the use on a K&amp;R C-based compiler of features available only in Standard C.\nAfter the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.\nC99.\nThe C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as \"C99\". It has since been amended three times by Technical Corrigenda.\nC99 introduced several new features, including inline functions, several new data types (including codice_38 and a codice_39 type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with codice_40, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.\nC99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has codice_12 implicitly assumed. A standard macro codice_42 is defined with value codice_43 to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.\nIn addition, the C99 standard requires support for identifiers using Unicode in the form of escaped characters (e.g. or ) and suggests support for raw Unicode names.\nC11.\nWork began in 2007 on another revision of the C standard, informally called \"C1X\" until its official publication of ISO/IEC 9899:2011 on December 8, 2011. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.\nThe C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro codice_42 is defined as codice_45 to indicate that C11 support is available.\nC17.\nC17 is an informal name for ISO/IEC 9899:2018, a standard for the C programming language published in June 2018. It introduces no new language features, only technical corrections, and clarifications to defects in C11. The standard macro codice_42 is defined as codice_47 to indicate that C17 support is available.\nC23.\nC23 is an informal name for the current major C language standard revision. It was informally known as \"C2X\" through most of its development. C23 was published in October 2024 as ISO/IEC 9899:2024. The standard macro codice_42 is defined as codice_49 to indicate that C23 support is available.\nC2Y.\nC2Y is an informal name for the next major C language standard revision, after C23 (C2X), that is hoped to be released later in the 2020s decade, hence the '2' in \"C2Y\". An early working draft of C2Y was released in February 2024 as N3220 by the working group ISO/IEC JTC1/SC22/WG14.\nEmbedded C.\nHistorically, embedded C programming requires non-standard extensions to the C language to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.\nIn 2008, the C Standards Committee published a technical report extending the C language to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.\nSyntax.\nC has a formal grammar specified by the C standard. Line endings are generally not significant in C; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters codice_50 and codice_51, or (since C99) following codice_40 until the end of the line. Comments delimited by codice_50 and codice_51 do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.\nC source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as codice_6, codice_33, and codice_8, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as codice_13 and codice_12 specify built-in types. Sections of code are enclosed in braces (codice_60 and codice_61, sometimes called \"curly brackets\") to limit the scope of declarations and to act as a single statement for control structures.\nAs an imperative language, C uses \"statements\" to specify actions. The most common statement is an \"expression statement\", consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by codice_62 ... [codice_63] conditional execution and by codice_64 ... codice_4, codice_4, and codice_2 iterative execution (looping). The codice_2 statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. codice_69 and codice_70 can be used within the loop. Break is used to leave the innermost enclosing loop statement and continue is used to skip to its reinitialisation. There is also a non-structured codice_71 statement which branches directly to the designated label within the function. codice_5 selects a codice_73 to be executed based on the value of an integer expression. Different from many other languages, control-flow will fall through to the next codice_73 unless terminated by a codice_69.\nExpressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next \"sequence point\"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (codice_76, codice_77, codice_78 and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.\nKernighan and Ritchie say in the Introduction of \"The C Programming Language\": \"C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better.\" The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.\nCharacter set.\nThe basic C source character set includes the following characters:\nThe \"newline\" character indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as such.\nAdditional multi-byte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multi-national Unicode characters to be embedded portably within C source text by using codice_86 or codice_87 encoding (where codice_88 denotes a hexadecimal character), although this feature is not yet widely implemented.\nThe basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.\nReserved words.\nThe following reserved words are case sensitive.\nC89 has 32 reserved words, also known as 'keywords', which cannot be used for any purposes other than those for which they are predefined:\nC99 added five more reserved words: (\u2021 indicates an alternative spelling alias for a C23 keyword)\nC11 added seven more reserved words: (\u2021 indicates an alternative spelling alias for a C23 keyword)\nC23 reserved fifteen more words:\nMost of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. Some of those words were added as keywords with their conventional spelling in C23 and the corresponding macros were removed.\nPrior to C89, codice_148 was reserved as a keyword. In the second edition of their book \"The C Programming Language\", which describes what became known as C89, Kernighan and Ritchie wrote, \"The ... [keyword] codice_148, formerly reserved but never used, is no longer reserved.\" and \"The stillborn codice_148 keyword is withdrawn.\"\nOperators.\nC supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:\nC uses the operator codice_156 (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator codice_166 to test for equality. The similarity between the operators for assignment and equality may result in the accidental use of one in place of the other, and in many cases the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression codice_188 might mistakenly be written as codice_189, which will be evaluated as codice_141 unless the value of codice_79 is codice_83 after the assignment.\nThe C operator precedence is not always intuitive. For example, the operator codice_166 binds more tightly than (is executed prior to) the operators codice_158 (bitwise AND) and codice_159 (bitwise OR) in expressions such as codice_196, which must be written as codice_197 if that is the coder's intent.\n\"Hello, world\" example.\nThe \"hello, world\" example that appeared in the first edition of \"K&amp;R\" has become the model for an introductory program in most programming textbooks. The program prints \"hello, world\" to the standard output, which is usually a terminal or screen display.\nThe original version was:\nmain()\n printf(\"hello, world\\n\");\nA standard-conforming \"hello, world\" program is:\nint main(void)\n printf(\"hello, world\\n\");\nThe first line of the program contains a preprocessing directive, indicated by codice_15. This causes the compiler to replace that line of code with the entire text of the codice_199 header file, which contains declarations for standard input and output functions such as codice_200 and codice_201. The angle brackets surrounding codice_199 indicate that the header file can be located using a search strategy that prefers headers provided with the compiler to other headers having the same name (as opposed to double quotes which typically include local or project-specific header files).\nThe second line indicates that a function named codice_203 is being defined. The codice_203 function serves a special purpose in C programs; the run-time environment calls the codice_203 function to begin program execution. The type specifier codice_12 indicates that the value returned to the invoker (in this case the run-time environment) as a result of evaluating the codice_203 function, is an integer. The keyword codice_9 as a parameter list indicates that the codice_203 function takes no arguments.\nThe opening curly brace indicates the beginning of the code that defines the codice_203 function.\nThe next line of the program is a statement that \"calls\" (i.e. diverts execution to) a function named codice_200, which in this case is supplied from a system library. In this call, the codice_200 function is \"passed\" (i.e. provided with) a single argument, which is the address of the first character in the string literal codice_213. The string literal is an unnamed array set up automatically by the compiler, with elements of type codice_13 and a final NULL character (ASCII value 0) marking the end of the array (to allow codice_200 to determine the length of the string). The NULL character can also be written as the escape sequence codice_216. The codice_217 is a standard escape sequence that C translates to a \"newline\" character, which, on output, signifies the end of the current line. The return value of the codice_200 function is of type codice_12, but it is silently discarded since it is not used. (A more careful program might test the return value to check that the codice_200 function succeeded.) The semicolon codice_221 terminates the statement.\nThe closing curly brace indicates the end of the code for the codice_203 function. According to the C99 specification and newer, the codice_203 function (unlike any other function) will implicitly return a value of codice_83 upon reaching the codice_61 that terminates the function. The return value of codice_83 is interpreted by the run-time system as an exit code indicating successful execution of the function.\nData types.\nThe type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal. There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (codice_8). Integer type codice_13 is often used for single-byte characters. C99 added a Boolean data type. There are also derived types including arrays, pointers, records (codice_6), and unions (codice_33).\nC is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a \"type cast\" to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.\nSome find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: \"declaration reflects use\".)\nC's \"usual arithmetic conversions\" allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.\nPointers.\nC supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be \"dereferenced\" to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type.\nPointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers; the result of a codice_231 is usually cast to the data type of the data to be stored. Many data types, such as trees, are commonly implemented as dynamically allocated codice_6 objects linked together using pointers. Pointers to other pointers are often used in multi-dimensional arrays and arrays of codice_6 objects. Pointers to functions (\"function pointers\") are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch), in dispatch tables, or as callbacks to event handlers.\nA \"null pointer value\" explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no \"next\" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a \"null pointer constant\" can be written as codice_83, with or without explicit casting to a pointer type, as the codice_235 macro defined by several standard headers or, since C23 with the constant codice_138. In conditional contexts, null pointer values evaluate to codice_137, while all other pointer values evaluate to codice_141.\nVoid pointers (codice_239) point to objects of unspecified type, and can therefore be used as \"generic\" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.\nCareless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.\nArrays.\nArray types in C are traditionally of a fixed, static size specified at compile time. The more recent C99 standard also allows a form of variable-length arrays. However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's codice_231 function, and treat it as an array.\nSince arrays are always accessed (in effect) via pointers, array accesses are typically \"not\" checked against the underlying array size, although some compilers may provide bounds checking as an option. Array bounds violations are therefore possible and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions.\nC does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting \"multi-dimensional array\" can be thought of as increasing in row-major order. Multi-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, in early versions of C the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this was to allocate the array with an additional \"row vector\" of pointers to the columns.) C99 introduced \"variable-length arrays\" which address this issue.\nThe following example using modern C (C99 or later) shows allocation of a two-dimensional array on the heap and the use of multi-dimensional array indexing for accesses (which can use bounds-checking on many C compilers):\nint func(int N, int M)\n float (*p)[N] [M] = malloc(sizeof *p);\n if (p == 0)\n return -1;\n for (int i = 0; i &lt; N; i++)\n for (int j = 0; j &lt; M; j++)\n (*p)[i] [j] = i + j;\n print_array(N, M, p);\n free(p);\n return 1;\nAnd here is a similar implementation using C99's \"Auto VLA\" feature:\nint func(int N, int M)\n // Caution: checks should be made to ensure N*M*sizeof(float) does NOT exceed limitations for auto VLAs and is within available size of stack.\n float p[N] [M]; // auto VLA is held on the stack, and sized when the function is invoked\n for (int i = 0; i &lt; N; i++)\n for (int j = 0; j &lt; M; j++)\n p[i] [j] = i + j;\n print_array(N, M, p);\n // no need to free(p) since it will disappear when the function exits, along with the rest of the stack frame\n return 1;\nArray\u2013pointer interchangeability.\nThe subscript notation codice_241 (where codice_242 designates a pointer) is syntactic sugar for codice_243. Taking advantage of the compiler's knowledge of the pointer type, the address that codice_244 points to is not the base address (pointed to by codice_242) incremented by codice_25 bytes, but rather is defined to be the base address incremented by codice_25 multiplied by the size of an element that codice_242 points to. Thus, codice_241 designates the codice_250th element of the array.\nFurthermore, in most expression contexts (a notable exception is as operand of codice_111), an expression of array type is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.\nThe total size of an array codice_242 can be determined by applying codice_111 to an expression of array type. The size of an element can be determined by applying the operator codice_111 to any dereferenced element of an array codice_81, as in codice_256. Thus, the number of elements in a declared array codice_81 can be determined as codice_258. Note, that if only a pointer to the first element is available as it is often the case in C code because of the automatic conversion described above, the information about the full type of the array and its length are lost.\nMemory management.\nOne of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three principal ways to allocate memory for objects:\nThese three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.\nWhere possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary. Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on C dynamic memory allocation for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)\nUnless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.\nHeap memory allocation has to be synchronized with its actual usage in any program to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before it is deallocated explicitly, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a \"memory leak.\" Conversely, it is possible for memory to be freed, but is referenced subsequently, leading to unpredictable results. Typically, the failure symptoms appear in a portion of the program unrelated to the code that causes the error, making it difficult to diagnose the failure. Such issues are ameliorated in languages with automatic garbage collection.\nLibraries.\nThe C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single \"archive\" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. For a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., codice_262, shorthand for \"link the math library\").\nThe most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, codice_199) specify the interfaces for these and other standard library facilities.\nAnother common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.\nSince many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.\nFile handling and streams.\nFile input and output (I/O) is not part of the C language itself but instead is handled by libraries (such as the C standard library) and their associated header files (e.g. codice_199). File handling is generally implemented through high-level I/O which works through streams. A stream is from this perspective a data flow that is independent of devices, while a file is a concrete device. The high-level I/O is done through the association of a stream to a file. In the C standard library, a buffer (a memory area or queue) is temporarily used to store data before it is sent to the final destination. This reduces the time spent waiting for slower devices, for example a hard drive or solid-state drive. Low-level I/O functions are not part of the standard C library but are generally part of \"bare metal\" programming (programming that is independent of any operating system such as most embedded programming). With few exceptions, implementations include low-level I/O.\nLanguage tools.\nA number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler.\nAutomated source code checking and auditing tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.\nThere are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.\nMemory management checking tools like Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.\nUses.\nRationale for use in systems programming.\nC is widely used for systems programming in implementing operating systems and embedded system applications. This is for several reasons:\nUsed for computationally-intensive libraries.\nC enables programmers to create efficient implementations of algorithms and data structures, because the layer of abstraction from hardware is thin, and its overhead is low, an important criterion for computationally intensive programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C. Many languages support calling library functions in C, for example, the Python-based framework NumPy uses C for the high-performance and hardware-interacting aspects.\nC as an intermediate language.\nC is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--. Also, contemporary major compilers GCC and LLVM both feature an intermediate representation that is not C, and those compilers support front ends for many languages including C.\nOther languages written in C.\nA consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. For example, the reference implementations of Python, Perl, Ruby, and PHP are written in C.\nOnce used for web development.\nHistorically, C was sometimes used for web development using the Common Gateway Interface (CGI) as a \"gateway\" for information between the web application, the server, and the browser. C may have been chosen over interpreted languages because of its speed, stability, and near-universal availability. It is no longer common practice for web development to be done in C, and many other web development languages are popular. Applications where C-based web development continues include the HTTP configuration pages on routers, IoT devices and similar, although even here some projects have parts in higher-level languages e.g. the use of Lua within OpenWRT.\nWeb servers.\nThe two most popular web servers, Apache HTTP Server and Nginx, are both written in C. These web servers interact with the operating system, listen on TCP ports for HTTP requests, and then serve up static web content, or cause the execution of other languages handling to 'render' content such as PHP, which is itself primarily written in C. C's close-to-the-metal approach allows for the construction of these high-performance software systems.\nEnd-user applications.\nC has also been widely used to implement end-user applications. However, such applications can also be written in newer, higher-level languages.\nLimitations.\nWhile C has been popular, influential and hugely successful, it has drawbacks, including:\nFor some purposes, restricted styles of C have been adopted, e.g. MISRA C or CERT C, in an attempt to reduce the opportunity for bugs. Databases such as CWE attempt to count the ways C etc. has vulnerabilities, along with recommendations for mitigation.\nThere are tools that can mitigate against some of the drawbacks. Contemporary C compilers include checks which may generate warnings to help identify many potential bugs.\nRelated languages.\nC has both directly and indirectly influenced many later languages such as C++ and Java. The most pervasive influence has been syntactical; all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models or large-scale program structures that differ from those of C, sometimes radically.\nSeveral C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.\nWhen object-oriented programming languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.\nThe C++ programming language (originally named \"C with Classes\") was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax. C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.\nObjective-C was originally a very \"thin\" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.\nIn addition to C++ and Objective-C, Ch, Cilk, and Unified Parallel C are nearly supersets of C."}
{"id": "6022", "revid": "10044298", "url": "https://en.wikipedia.org/wiki?curid=6022", "title": "Cytology", "text": ""}
{"id": "6023", "revid": "36449898", "url": "https://en.wikipedia.org/wiki?curid=6023", "title": "Castle of the Winds", "text": "Castle of the Winds is a tile-based roguelike video game for Microsoft Windows. It was developed by Rick Saada in 1989 and distributed by Epic MegaGames in 1993. The game was released around 1998 as a freeware download by the author. Though it is secondary to its hack and slash gameplay, \"Castle of the Winds\" has a plot loosely based on Norse mythology, told with setting changes, unique items, and occasional passages of text. The game is composed of two parts: A Question of Vengeance, released as shareware, and Lifthransir's Bane, sold commercially. A combined license for both parts was also sold.\nGameplay.\nThe game differs from most roguelikes in a number of ways. Its interface is mouse-dependent, but supports keyboard shortcuts (such as 'g' to get an item). \"Castle of the Winds\" also allows the player to restore saved games after dying.\nThe game favors the use of magic in combat, as spells are the only weapons that work from a distance. The player character automatically gains a spell with each experience level, and can permanently gain others using corresponding books, until all thirty spells available are learned. There are two opposing pairs of elements: cold vs. fire and lightning vs. acid/poison. Spells are divided into six categories: attack, defense, healing, movement, divination, and miscellaneous.\n\"Castle of the Winds\" possesses an inventory system that limits a player's load based on weight and bulk, rather than by number of items. It allows the character to use different containers, including packs, belts, chests, and bags. Other items include weapons, armor, protective clothing, purses, and ornamental jewellery. Almost every item in the game can be normal, cursed, or enchanted, with curses and enchantments working in a manner similar to \"NetHack\". Although items do not break with use, they may already be broken or rusted when found. Most objects that the character currently carries can be renamed.\nWherever the player goes before entering the dungeon, there is always a town which offers the basic services of a temple for healing and curing curses, a junk store where anything can be sold for a few copper coins, a sage who can identify items and (from the second town onwards) a bank for storing the total capacity of coins to lighten the player's load. Other services that differ and vary in what they sell are outfitters, weaponsmiths, armoursmiths, magic shops and general stores.\nThe game tracks how much time has been spent playing the game. Although story events are not triggered by the passage of time, it does determine when merchants rotate their stock. Victorious players are listed as \"Valhalla's Champions\" in the order of time taken, from fastest to slowest. If the player dies, they are still put on the list, but are categorized as \"Dead\", with their experience point total listed as at the final killing blow. The amount of time spent also determines the difficulty of the last boss.\nPlot.\nThe player begins in a tiny hamlet, near which they used to live. Their farm has been destroyed and godparents killed. After clearing out an abandoned mine, the player finds a scrap of parchment that reveals the death of the player's godparents was ordered by an unknown enemy. The player then returns to the hamlet to find it pillaged, and decides to travel to Bjarnarhaven.\nOnce in Bjarnarhaven, the player explores the levels beneath a nearby fortress, eventually facing Hrungnir, the Hill Giant Lord, responsible for ordering the player's godparents' death. Hrungnir carries the Enchanted Amulet of Kings. Upon activating the amulet, the player is informed of their past by their dead father, after which the player is transported to the town of Crossroads, and \"Part I\" ends. The game can be imported or started over in \"Part II\".\nThe town of Crossroads is run by a Jarl who at first does not admit the player, but later (on up to three occasions) provides advice and rewards. The player then enters the nearby ruined titular Castle of the Winds. There the player meets his/her deceased grandfather, who instructs them to venture into the dungeons below, defeat Surtur, and reclaim their birthright. Venturing deeper, the player encounters monsters run rampant, a desecrated crypt, a necromancer, and the installation of various special rooms for elementals. The player eventually meets and defeats the Wolf-Man leader, Bear-Man leader, the four Jotun kings, a Demon Lord, and finally Surtur. Upon defeating Surtur and escaping the dungeons, the player sits upon the throne, completing the game.\nDevelopment.\nInspired by his love of RPGs and while learning Windows programming in the 80s, Rick Saada designed and completed \"Castle of the Winds\". The game sold 13,500 copies. By 1998, the game's author, Rick Saada, decided to distribute the entirety of \"Castle of the Winds\" free of charge.\nThe game is public domain per Rick Saada's words: \nGraphics.\nAll terrain tiles, some landscape features, all monsters and objects, and some spell/effect graphics take the form of Windows 3.1 icons and were done by Paul Canniff. Multi-tile graphics, such as ball spells and town buildings, are bitmaps included in the executable file. No graphics use colors other than the Windows-standard 16-color palette, plus transparency. They exist in monochrome versions as well, meaning that the game will display well on monochrome monitors.\nThe map view is identical to the playing-field view, except for scaling to fit on one screen. A simplified map view is available to improve performance on slower computers. The latter functionality also presents a cleaner display, as the aforementioned scaling routine does not always work correctly.\nReception.\n\"Computer Gaming World\" rated the gameplay as good and the graphics simple but effective, while noticing the lack of audio, but regarded the game itself enjoyable."}
